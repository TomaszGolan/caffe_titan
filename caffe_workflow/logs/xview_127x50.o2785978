I0506 15:40:04.118079   548 caffe.cpp:184] Using GPUs 0
I0506 15:40:04.544867   548 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1500
test_interval: 500
base_lr: 0.0025
display: 100
max_iter: 15000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 2500
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt"
I0506 15:40:04.546703   548 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0506 15:40:04.550905   548 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0506 15:40:04.550962   548 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0506 15:40:04.551339   548 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0506 15:40:04.551539   548 layer_factory.hpp:77] Creating layer data_hdf5
I0506 15:40:04.551571   548 net.cpp:106] Creating Layer data_hdf5
I0506 15:40:04.551590   548 net.cpp:411] data_hdf5 -> data
I0506 15:40:04.551627   548 net.cpp:411] data_hdf5 -> label
I0506 15:40:04.551666   548 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0506 15:40:04.553884   548 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0506 15:40:04.569049   548 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0506 15:40:26.193478   548 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0506 15:40:26.198796   548 net.cpp:150] Setting up data_hdf5
I0506 15:40:26.198850   548 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0506 15:40:26.198868   548 net.cpp:157] Top shape: 100 (100)
I0506 15:40:26.198884   548 net.cpp:165] Memory required for data: 2540400
I0506 15:40:26.198911   548 layer_factory.hpp:77] Creating layer conv1
I0506 15:40:26.198948   548 net.cpp:106] Creating Layer conv1
I0506 15:40:26.198962   548 net.cpp:454] conv1 <- data
I0506 15:40:26.198988   548 net.cpp:411] conv1 -> conv1
I0506 15:40:29.253542   548 net.cpp:150] Setting up conv1
I0506 15:40:29.253599   548 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 15:40:29.253615   548 net.cpp:165] Memory required for data: 30188400
I0506 15:40:29.253648   548 layer_factory.hpp:77] Creating layer relu1
I0506 15:40:29.253669   548 net.cpp:106] Creating Layer relu1
I0506 15:40:29.253690   548 net.cpp:454] relu1 <- conv1
I0506 15:40:29.253726   548 net.cpp:397] relu1 -> conv1 (in-place)
I0506 15:40:29.254256   548 net.cpp:150] Setting up relu1
I0506 15:40:29.254278   548 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 15:40:29.254292   548 net.cpp:165] Memory required for data: 57836400
I0506 15:40:29.254305   548 layer_factory.hpp:77] Creating layer pool1
I0506 15:40:29.254334   548 net.cpp:106] Creating Layer pool1
I0506 15:40:29.254348   548 net.cpp:454] pool1 <- conv1
I0506 15:40:29.254364   548 net.cpp:411] pool1 -> pool1
I0506 15:40:29.254457   548 net.cpp:150] Setting up pool1
I0506 15:40:29.254474   548 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0506 15:40:29.254487   548 net.cpp:165] Memory required for data: 71660400
I0506 15:40:29.254501   548 layer_factory.hpp:77] Creating layer conv2
I0506 15:40:29.254534   548 net.cpp:106] Creating Layer conv2
I0506 15:40:29.254546   548 net.cpp:454] conv2 <- pool1
I0506 15:40:29.254562   548 net.cpp:411] conv2 -> conv2
I0506 15:40:29.257305   548 net.cpp:150] Setting up conv2
I0506 15:40:29.257339   548 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 15:40:29.257354   548 net.cpp:165] Memory required for data: 91532400
I0506 15:40:29.257381   548 layer_factory.hpp:77] Creating layer relu2
I0506 15:40:29.257408   548 net.cpp:106] Creating Layer relu2
I0506 15:40:29.257422   548 net.cpp:454] relu2 <- conv2
I0506 15:40:29.257438   548 net.cpp:397] relu2 -> conv2 (in-place)
I0506 15:40:29.257793   548 net.cpp:150] Setting up relu2
I0506 15:40:29.257813   548 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 15:40:29.257827   548 net.cpp:165] Memory required for data: 111404400
I0506 15:40:29.257839   548 layer_factory.hpp:77] Creating layer pool2
I0506 15:40:29.257864   548 net.cpp:106] Creating Layer pool2
I0506 15:40:29.257879   548 net.cpp:454] pool2 <- conv2
I0506 15:40:29.257894   548 net.cpp:411] pool2 -> pool2
I0506 15:40:29.257992   548 net.cpp:150] Setting up pool2
I0506 15:40:29.258023   548 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0506 15:40:29.258035   548 net.cpp:165] Memory required for data: 121340400
I0506 15:40:29.258050   548 layer_factory.hpp:77] Creating layer conv3
I0506 15:40:29.258076   548 net.cpp:106] Creating Layer conv3
I0506 15:40:29.258090   548 net.cpp:454] conv3 <- pool2
I0506 15:40:29.258106   548 net.cpp:411] conv3 -> conv3
I0506 15:40:29.260040   548 net.cpp:150] Setting up conv3
I0506 15:40:29.260064   548 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 15:40:29.260084   548 net.cpp:165] Memory required for data: 132182000
I0506 15:40:29.260107   548 layer_factory.hpp:77] Creating layer relu3
I0506 15:40:29.260129   548 net.cpp:106] Creating Layer relu3
I0506 15:40:29.260151   548 net.cpp:454] relu3 <- conv3
I0506 15:40:29.260167   548 net.cpp:397] relu3 -> conv3 (in-place)
I0506 15:40:29.260651   548 net.cpp:150] Setting up relu3
I0506 15:40:29.260673   548 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 15:40:29.260687   548 net.cpp:165] Memory required for data: 143023600
I0506 15:40:29.260699   548 layer_factory.hpp:77] Creating layer pool3
I0506 15:40:29.260718   548 net.cpp:106] Creating Layer pool3
I0506 15:40:29.260740   548 net.cpp:454] pool3 <- conv3
I0506 15:40:29.260756   548 net.cpp:411] pool3 -> pool3
I0506 15:40:29.260838   548 net.cpp:150] Setting up pool3
I0506 15:40:29.260855   548 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0506 15:40:29.260869   548 net.cpp:165] Memory required for data: 148444400
I0506 15:40:29.260880   548 layer_factory.hpp:77] Creating layer conv4
I0506 15:40:29.260902   548 net.cpp:106] Creating Layer conv4
I0506 15:40:29.260921   548 net.cpp:454] conv4 <- pool3
I0506 15:40:29.260938   548 net.cpp:411] conv4 -> conv4
I0506 15:40:29.263897   548 net.cpp:150] Setting up conv4
I0506 15:40:29.263932   548 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 15:40:29.263947   548 net.cpp:165] Memory required for data: 152073200
I0506 15:40:29.263967   548 layer_factory.hpp:77] Creating layer relu4
I0506 15:40:29.263988   548 net.cpp:106] Creating Layer relu4
I0506 15:40:29.264014   548 net.cpp:454] relu4 <- conv4
I0506 15:40:29.264031   548 net.cpp:397] relu4 -> conv4 (in-place)
I0506 15:40:29.264519   548 net.cpp:150] Setting up relu4
I0506 15:40:29.264542   548 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 15:40:29.264555   548 net.cpp:165] Memory required for data: 155702000
I0506 15:40:29.264567   548 layer_factory.hpp:77] Creating layer pool4
I0506 15:40:29.264587   548 net.cpp:106] Creating Layer pool4
I0506 15:40:29.264610   548 net.cpp:454] pool4 <- conv4
I0506 15:40:29.264626   548 net.cpp:411] pool4 -> pool4
I0506 15:40:29.264708   548 net.cpp:150] Setting up pool4
I0506 15:40:29.264730   548 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0506 15:40:29.264744   548 net.cpp:165] Memory required for data: 157516400
I0506 15:40:29.264755   548 layer_factory.hpp:77] Creating layer ip1
I0506 15:40:29.264778   548 net.cpp:106] Creating Layer ip1
I0506 15:40:29.264792   548 net.cpp:454] ip1 <- pool4
I0506 15:40:29.264808   548 net.cpp:411] ip1 -> ip1
I0506 15:40:29.280603   548 net.cpp:150] Setting up ip1
I0506 15:40:29.280637   548 net.cpp:157] Top shape: 100 196 (19600)
I0506 15:40:29.280658   548 net.cpp:165] Memory required for data: 157594800
I0506 15:40:29.280685   548 layer_factory.hpp:77] Creating layer relu5
I0506 15:40:29.280707   548 net.cpp:106] Creating Layer relu5
I0506 15:40:29.280720   548 net.cpp:454] relu5 <- ip1
I0506 15:40:29.280748   548 net.cpp:397] relu5 -> ip1 (in-place)
I0506 15:40:29.281105   548 net.cpp:150] Setting up relu5
I0506 15:40:29.281126   548 net.cpp:157] Top shape: 100 196 (19600)
I0506 15:40:29.281138   548 net.cpp:165] Memory required for data: 157673200
I0506 15:40:29.281150   548 layer_factory.hpp:77] Creating layer drop1
I0506 15:40:29.281183   548 net.cpp:106] Creating Layer drop1
I0506 15:40:29.281195   548 net.cpp:454] drop1 <- ip1
I0506 15:40:29.281211   548 net.cpp:397] drop1 -> ip1 (in-place)
I0506 15:40:29.281265   548 net.cpp:150] Setting up drop1
I0506 15:40:29.281298   548 net.cpp:157] Top shape: 100 196 (19600)
I0506 15:40:29.281313   548 net.cpp:165] Memory required for data: 157751600
I0506 15:40:29.281325   548 layer_factory.hpp:77] Creating layer ip2
I0506 15:40:29.281348   548 net.cpp:106] Creating Layer ip2
I0506 15:40:29.281368   548 net.cpp:454] ip2 <- ip1
I0506 15:40:29.281385   548 net.cpp:411] ip2 -> ip2
I0506 15:40:29.281873   548 net.cpp:150] Setting up ip2
I0506 15:40:29.281893   548 net.cpp:157] Top shape: 100 98 (9800)
I0506 15:40:29.281904   548 net.cpp:165] Memory required for data: 157790800
I0506 15:40:29.281925   548 layer_factory.hpp:77] Creating layer relu6
I0506 15:40:29.281940   548 net.cpp:106] Creating Layer relu6
I0506 15:40:29.281960   548 net.cpp:454] relu6 <- ip2
I0506 15:40:29.281981   548 net.cpp:397] relu6 -> ip2 (in-place)
I0506 15:40:29.282526   548 net.cpp:150] Setting up relu6
I0506 15:40:29.282548   548 net.cpp:157] Top shape: 100 98 (9800)
I0506 15:40:29.282562   548 net.cpp:165] Memory required for data: 157830000
I0506 15:40:29.282577   548 layer_factory.hpp:77] Creating layer drop2
I0506 15:40:29.282593   548 net.cpp:106] Creating Layer drop2
I0506 15:40:29.282613   548 net.cpp:454] drop2 <- ip2
I0506 15:40:29.282629   548 net.cpp:397] drop2 -> ip2 (in-place)
I0506 15:40:29.282678   548 net.cpp:150] Setting up drop2
I0506 15:40:29.282701   548 net.cpp:157] Top shape: 100 98 (9800)
I0506 15:40:29.282714   548 net.cpp:165] Memory required for data: 157869200
I0506 15:40:29.282733   548 layer_factory.hpp:77] Creating layer ip3
I0506 15:40:29.282749   548 net.cpp:106] Creating Layer ip3
I0506 15:40:29.282762   548 net.cpp:454] ip3 <- ip2
I0506 15:40:29.282780   548 net.cpp:411] ip3 -> ip3
I0506 15:40:29.283004   548 net.cpp:150] Setting up ip3
I0506 15:40:29.283022   548 net.cpp:157] Top shape: 100 11 (1100)
I0506 15:40:29.283035   548 net.cpp:165] Memory required for data: 157873600
I0506 15:40:29.283056   548 layer_factory.hpp:77] Creating layer drop3
I0506 15:40:29.283077   548 net.cpp:106] Creating Layer drop3
I0506 15:40:29.283090   548 net.cpp:454] drop3 <- ip3
I0506 15:40:29.283107   548 net.cpp:397] drop3 -> ip3 (in-place)
I0506 15:40:29.283152   548 net.cpp:150] Setting up drop3
I0506 15:40:29.283174   548 net.cpp:157] Top shape: 100 11 (1100)
I0506 15:40:29.283186   548 net.cpp:165] Memory required for data: 157878000
I0506 15:40:29.283200   548 layer_factory.hpp:77] Creating layer loss
I0506 15:40:29.283218   548 net.cpp:106] Creating Layer loss
I0506 15:40:29.283233   548 net.cpp:454] loss <- ip3
I0506 15:40:29.283247   548 net.cpp:454] loss <- label
I0506 15:40:29.283270   548 net.cpp:411] loss -> loss
I0506 15:40:29.283299   548 layer_factory.hpp:77] Creating layer loss
I0506 15:40:29.283958   548 net.cpp:150] Setting up loss
I0506 15:40:29.283979   548 net.cpp:157] Top shape: (1)
I0506 15:40:29.283998   548 net.cpp:160]     with loss weight 1
I0506 15:40:29.284046   548 net.cpp:165] Memory required for data: 157878004
I0506 15:40:29.284067   548 net.cpp:226] loss needs backward computation.
I0506 15:40:29.284081   548 net.cpp:226] drop3 needs backward computation.
I0506 15:40:29.284095   548 net.cpp:226] ip3 needs backward computation.
I0506 15:40:29.284111   548 net.cpp:226] drop2 needs backward computation.
I0506 15:40:29.284123   548 net.cpp:226] relu6 needs backward computation.
I0506 15:40:29.284135   548 net.cpp:226] ip2 needs backward computation.
I0506 15:40:29.284150   548 net.cpp:226] drop1 needs backward computation.
I0506 15:40:29.284162   548 net.cpp:226] relu5 needs backward computation.
I0506 15:40:29.284181   548 net.cpp:226] ip1 needs backward computation.
I0506 15:40:29.284195   548 net.cpp:226] pool4 needs backward computation.
I0506 15:40:29.284211   548 net.cpp:226] relu4 needs backward computation.
I0506 15:40:29.284225   548 net.cpp:226] conv4 needs backward computation.
I0506 15:40:29.284238   548 net.cpp:226] pool3 needs backward computation.
I0506 15:40:29.284251   548 net.cpp:226] relu3 needs backward computation.
I0506 15:40:29.284263   548 net.cpp:226] conv3 needs backward computation.
I0506 15:40:29.284296   548 net.cpp:226] pool2 needs backward computation.
I0506 15:40:29.284309   548 net.cpp:226] relu2 needs backward computation.
I0506 15:40:29.284322   548 net.cpp:226] conv2 needs backward computation.
I0506 15:40:29.284335   548 net.cpp:226] pool1 needs backward computation.
I0506 15:40:29.284348   548 net.cpp:226] relu1 needs backward computation.
I0506 15:40:29.284360   548 net.cpp:226] conv1 needs backward computation.
I0506 15:40:29.284376   548 net.cpp:228] data_hdf5 does not need backward computation.
I0506 15:40:29.284394   548 net.cpp:270] This network produces output loss
I0506 15:40:29.284421   548 net.cpp:283] Network initialization done.
I0506 15:40:29.286169   548 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0506 15:40:29.286244   548 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0506 15:40:29.286618   548 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0506 15:40:29.286836   548 layer_factory.hpp:77] Creating layer data_hdf5
I0506 15:40:29.286856   548 net.cpp:106] Creating Layer data_hdf5
I0506 15:40:29.286871   548 net.cpp:411] data_hdf5 -> data
I0506 15:40:29.286891   548 net.cpp:411] data_hdf5 -> label
I0506 15:40:29.286912   548 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0506 15:40:29.289252   548 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0506 15:40:50.675746   548 net.cpp:150] Setting up data_hdf5
I0506 15:40:50.675914   548 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0506 15:40:50.675931   548 net.cpp:157] Top shape: 100 (100)
I0506 15:40:50.675945   548 net.cpp:165] Memory required for data: 2540400
I0506 15:40:50.675959   548 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0506 15:40:50.675987   548 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0506 15:40:50.676007   548 net.cpp:454] label_data_hdf5_1_split <- label
I0506 15:40:50.676023   548 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0506 15:40:50.676065   548 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0506 15:40:50.676144   548 net.cpp:150] Setting up label_data_hdf5_1_split
I0506 15:40:50.676162   548 net.cpp:157] Top shape: 100 (100)
I0506 15:40:50.676178   548 net.cpp:157] Top shape: 100 (100)
I0506 15:40:50.676199   548 net.cpp:165] Memory required for data: 2541200
I0506 15:40:50.676213   548 layer_factory.hpp:77] Creating layer conv1
I0506 15:40:50.676236   548 net.cpp:106] Creating Layer conv1
I0506 15:40:50.676249   548 net.cpp:454] conv1 <- data
I0506 15:40:50.676265   548 net.cpp:411] conv1 -> conv1
I0506 15:40:50.678200   548 net.cpp:150] Setting up conv1
I0506 15:40:50.678226   548 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 15:40:50.678249   548 net.cpp:165] Memory required for data: 30189200
I0506 15:40:50.678274   548 layer_factory.hpp:77] Creating layer relu1
I0506 15:40:50.678294   548 net.cpp:106] Creating Layer relu1
I0506 15:40:50.678316   548 net.cpp:454] relu1 <- conv1
I0506 15:40:50.678333   548 net.cpp:397] relu1 -> conv1 (in-place)
I0506 15:40:50.678845   548 net.cpp:150] Setting up relu1
I0506 15:40:50.678869   548 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 15:40:50.678881   548 net.cpp:165] Memory required for data: 57837200
I0506 15:40:50.678894   548 layer_factory.hpp:77] Creating layer pool1
I0506 15:40:50.678925   548 net.cpp:106] Creating Layer pool1
I0506 15:40:50.678938   548 net.cpp:454] pool1 <- conv1
I0506 15:40:50.678954   548 net.cpp:411] pool1 -> pool1
I0506 15:40:50.679042   548 net.cpp:150] Setting up pool1
I0506 15:40:50.679059   548 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0506 15:40:50.679075   548 net.cpp:165] Memory required for data: 71661200
I0506 15:40:50.679093   548 layer_factory.hpp:77] Creating layer conv2
I0506 15:40:50.679113   548 net.cpp:106] Creating Layer conv2
I0506 15:40:50.679126   548 net.cpp:454] conv2 <- pool1
I0506 15:40:50.679144   548 net.cpp:411] conv2 -> conv2
I0506 15:40:50.681104   548 net.cpp:150] Setting up conv2
I0506 15:40:50.681131   548 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 15:40:50.681146   548 net.cpp:165] Memory required for data: 91533200
I0506 15:40:50.681167   548 layer_factory.hpp:77] Creating layer relu2
I0506 15:40:50.681196   548 net.cpp:106] Creating Layer relu2
I0506 15:40:50.681210   548 net.cpp:454] relu2 <- conv2
I0506 15:40:50.681226   548 net.cpp:397] relu2 -> conv2 (in-place)
I0506 15:40:50.681576   548 net.cpp:150] Setting up relu2
I0506 15:40:50.681596   548 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 15:40:50.681608   548 net.cpp:165] Memory required for data: 111405200
I0506 15:40:50.681622   548 layer_factory.hpp:77] Creating layer pool2
I0506 15:40:50.681648   548 net.cpp:106] Creating Layer pool2
I0506 15:40:50.681660   548 net.cpp:454] pool2 <- conv2
I0506 15:40:50.681676   548 net.cpp:411] pool2 -> pool2
I0506 15:40:50.681769   548 net.cpp:150] Setting up pool2
I0506 15:40:50.681787   548 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0506 15:40:50.681800   548 net.cpp:165] Memory required for data: 121341200
I0506 15:40:50.681813   548 layer_factory.hpp:77] Creating layer conv3
I0506 15:40:50.681838   548 net.cpp:106] Creating Layer conv3
I0506 15:40:50.681857   548 net.cpp:454] conv3 <- pool2
I0506 15:40:50.681874   548 net.cpp:411] conv3 -> conv3
I0506 15:40:50.683883   548 net.cpp:150] Setting up conv3
I0506 15:40:50.683908   548 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 15:40:50.683928   548 net.cpp:165] Memory required for data: 132182800
I0506 15:40:50.683962   548 layer_factory.hpp:77] Creating layer relu3
I0506 15:40:50.683984   548 net.cpp:106] Creating Layer relu3
I0506 15:40:50.684007   548 net.cpp:454] relu3 <- conv3
I0506 15:40:50.684023   548 net.cpp:397] relu3 -> conv3 (in-place)
I0506 15:40:50.684517   548 net.cpp:150] Setting up relu3
I0506 15:40:50.684540   548 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 15:40:50.684553   548 net.cpp:165] Memory required for data: 143024400
I0506 15:40:50.684569   548 layer_factory.hpp:77] Creating layer pool3
I0506 15:40:50.684586   548 net.cpp:106] Creating Layer pool3
I0506 15:40:50.684607   548 net.cpp:454] pool3 <- conv3
I0506 15:40:50.684624   548 net.cpp:411] pool3 -> pool3
I0506 15:40:50.684710   548 net.cpp:150] Setting up pool3
I0506 15:40:50.684731   548 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0506 15:40:50.684743   548 net.cpp:165] Memory required for data: 148445200
I0506 15:40:50.684758   548 layer_factory.hpp:77] Creating layer conv4
I0506 15:40:50.684785   548 net.cpp:106] Creating Layer conv4
I0506 15:40:50.684798   548 net.cpp:454] conv4 <- pool3
I0506 15:40:50.684814   548 net.cpp:411] conv4 -> conv4
I0506 15:40:50.686934   548 net.cpp:150] Setting up conv4
I0506 15:40:50.686959   548 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 15:40:50.686978   548 net.cpp:165] Memory required for data: 152074000
I0506 15:40:50.686998   548 layer_factory.hpp:77] Creating layer relu4
I0506 15:40:50.687017   548 net.cpp:106] Creating Layer relu4
I0506 15:40:50.687031   548 net.cpp:454] relu4 <- conv4
I0506 15:40:50.687054   548 net.cpp:397] relu4 -> conv4 (in-place)
I0506 15:40:50.687543   548 net.cpp:150] Setting up relu4
I0506 15:40:50.687567   548 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 15:40:50.687580   548 net.cpp:165] Memory required for data: 155702800
I0506 15:40:50.687597   548 layer_factory.hpp:77] Creating layer pool4
I0506 15:40:50.687613   548 net.cpp:106] Creating Layer pool4
I0506 15:40:50.687634   548 net.cpp:454] pool4 <- conv4
I0506 15:40:50.687651   548 net.cpp:411] pool4 -> pool4
I0506 15:40:50.687737   548 net.cpp:150] Setting up pool4
I0506 15:40:50.687755   548 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0506 15:40:50.687772   548 net.cpp:165] Memory required for data: 157517200
I0506 15:40:50.687783   548 layer_factory.hpp:77] Creating layer ip1
I0506 15:40:50.687801   548 net.cpp:106] Creating Layer ip1
I0506 15:40:50.687820   548 net.cpp:454] ip1 <- pool4
I0506 15:40:50.687844   548 net.cpp:411] ip1 -> ip1
I0506 15:40:50.703711   548 net.cpp:150] Setting up ip1
I0506 15:40:50.703743   548 net.cpp:157] Top shape: 100 196 (19600)
I0506 15:40:50.703764   548 net.cpp:165] Memory required for data: 157595600
I0506 15:40:50.703791   548 layer_factory.hpp:77] Creating layer relu5
I0506 15:40:50.703812   548 net.cpp:106] Creating Layer relu5
I0506 15:40:50.703825   548 net.cpp:454] relu5 <- ip1
I0506 15:40:50.703857   548 net.cpp:397] relu5 -> ip1 (in-place)
I0506 15:40:50.704219   548 net.cpp:150] Setting up relu5
I0506 15:40:50.704239   548 net.cpp:157] Top shape: 100 196 (19600)
I0506 15:40:50.704252   548 net.cpp:165] Memory required for data: 157674000
I0506 15:40:50.704264   548 layer_factory.hpp:77] Creating layer drop1
I0506 15:40:50.704296   548 net.cpp:106] Creating Layer drop1
I0506 15:40:50.704310   548 net.cpp:454] drop1 <- ip1
I0506 15:40:50.704326   548 net.cpp:397] drop1 -> ip1 (in-place)
I0506 15:40:50.704376   548 net.cpp:150] Setting up drop1
I0506 15:40:50.704399   548 net.cpp:157] Top shape: 100 196 (19600)
I0506 15:40:50.704412   548 net.cpp:165] Memory required for data: 157752400
I0506 15:40:50.704424   548 layer_factory.hpp:77] Creating layer ip2
I0506 15:40:50.704442   548 net.cpp:106] Creating Layer ip2
I0506 15:40:50.704457   548 net.cpp:454] ip2 <- ip1
I0506 15:40:50.704479   548 net.cpp:411] ip2 -> ip2
I0506 15:40:50.704979   548 net.cpp:150] Setting up ip2
I0506 15:40:50.704998   548 net.cpp:157] Top shape: 100 98 (9800)
I0506 15:40:50.705011   548 net.cpp:165] Memory required for data: 157791600
I0506 15:40:50.705032   548 layer_factory.hpp:77] Creating layer relu6
I0506 15:40:50.705067   548 net.cpp:106] Creating Layer relu6
I0506 15:40:50.705080   548 net.cpp:454] relu6 <- ip2
I0506 15:40:50.705097   548 net.cpp:397] relu6 -> ip2 (in-place)
I0506 15:40:50.705662   548 net.cpp:150] Setting up relu6
I0506 15:40:50.705685   548 net.cpp:157] Top shape: 100 98 (9800)
I0506 15:40:50.705698   548 net.cpp:165] Memory required for data: 157830800
I0506 15:40:50.705710   548 layer_factory.hpp:77] Creating layer drop2
I0506 15:40:50.705730   548 net.cpp:106] Creating Layer drop2
I0506 15:40:50.705752   548 net.cpp:454] drop2 <- ip2
I0506 15:40:50.705768   548 net.cpp:397] drop2 -> ip2 (in-place)
I0506 15:40:50.705819   548 net.cpp:150] Setting up drop2
I0506 15:40:50.705843   548 net.cpp:157] Top shape: 100 98 (9800)
I0506 15:40:50.705855   548 net.cpp:165] Memory required for data: 157870000
I0506 15:40:50.705874   548 layer_factory.hpp:77] Creating layer ip3
I0506 15:40:50.705891   548 net.cpp:106] Creating Layer ip3
I0506 15:40:50.705904   548 net.cpp:454] ip3 <- ip2
I0506 15:40:50.705922   548 net.cpp:411] ip3 -> ip3
I0506 15:40:50.706174   548 net.cpp:150] Setting up ip3
I0506 15:40:50.706194   548 net.cpp:157] Top shape: 100 11 (1100)
I0506 15:40:50.706207   548 net.cpp:165] Memory required for data: 157874400
I0506 15:40:50.706224   548 layer_factory.hpp:77] Creating layer drop3
I0506 15:40:50.706243   548 net.cpp:106] Creating Layer drop3
I0506 15:40:50.706264   548 net.cpp:454] drop3 <- ip3
I0506 15:40:50.706279   548 net.cpp:397] drop3 -> ip3 (in-place)
I0506 15:40:50.706324   548 net.cpp:150] Setting up drop3
I0506 15:40:50.706343   548 net.cpp:157] Top shape: 100 11 (1100)
I0506 15:40:50.706356   548 net.cpp:165] Memory required for data: 157878800
I0506 15:40:50.706374   548 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0506 15:40:50.706393   548 net.cpp:106] Creating Layer ip3_drop3_0_split
I0506 15:40:50.706413   548 net.cpp:454] ip3_drop3_0_split <- ip3
I0506 15:40:50.706429   548 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0506 15:40:50.706449   548 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0506 15:40:50.706531   548 net.cpp:150] Setting up ip3_drop3_0_split
I0506 15:40:50.706550   548 net.cpp:157] Top shape: 100 11 (1100)
I0506 15:40:50.706571   548 net.cpp:157] Top shape: 100 11 (1100)
I0506 15:40:50.706584   548 net.cpp:165] Memory required for data: 157887600
I0506 15:40:50.706600   548 layer_factory.hpp:77] Creating layer accuracy
I0506 15:40:50.706624   548 net.cpp:106] Creating Layer accuracy
I0506 15:40:50.706637   548 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0506 15:40:50.706651   548 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0506 15:40:50.706676   548 net.cpp:411] accuracy -> accuracy
I0506 15:40:50.706701   548 net.cpp:150] Setting up accuracy
I0506 15:40:50.706717   548 net.cpp:157] Top shape: (1)
I0506 15:40:50.706732   548 net.cpp:165] Memory required for data: 157887604
I0506 15:40:50.706750   548 layer_factory.hpp:77] Creating layer loss
I0506 15:40:50.706768   548 net.cpp:106] Creating Layer loss
I0506 15:40:50.706781   548 net.cpp:454] loss <- ip3_drop3_0_split_1
I0506 15:40:50.706795   548 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0506 15:40:50.706810   548 net.cpp:411] loss -> loss
I0506 15:40:50.706832   548 layer_factory.hpp:77] Creating layer loss
I0506 15:40:50.707340   548 net.cpp:150] Setting up loss
I0506 15:40:50.707360   548 net.cpp:157] Top shape: (1)
I0506 15:40:50.707371   548 net.cpp:160]     with loss weight 1
I0506 15:40:50.707396   548 net.cpp:165] Memory required for data: 157887608
I0506 15:40:50.707415   548 net.cpp:226] loss needs backward computation.
I0506 15:40:50.707429   548 net.cpp:228] accuracy does not need backward computation.
I0506 15:40:50.707444   548 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0506 15:40:50.707458   548 net.cpp:226] drop3 needs backward computation.
I0506 15:40:50.707470   548 net.cpp:226] ip3 needs backward computation.
I0506 15:40:50.707486   548 net.cpp:226] drop2 needs backward computation.
I0506 15:40:50.707514   548 net.cpp:226] relu6 needs backward computation.
I0506 15:40:50.707526   548 net.cpp:226] ip2 needs backward computation.
I0506 15:40:50.707542   548 net.cpp:226] drop1 needs backward computation.
I0506 15:40:50.707556   548 net.cpp:226] relu5 needs backward computation.
I0506 15:40:50.707566   548 net.cpp:226] ip1 needs backward computation.
I0506 15:40:50.707579   548 net.cpp:226] pool4 needs backward computation.
I0506 15:40:50.707594   548 net.cpp:226] relu4 needs backward computation.
I0506 15:40:50.707613   548 net.cpp:226] conv4 needs backward computation.
I0506 15:40:50.707628   548 net.cpp:226] pool3 needs backward computation.
I0506 15:40:50.707640   548 net.cpp:226] relu3 needs backward computation.
I0506 15:40:50.707655   548 net.cpp:226] conv3 needs backward computation.
I0506 15:40:50.707669   548 net.cpp:226] pool2 needs backward computation.
I0506 15:40:50.707680   548 net.cpp:226] relu2 needs backward computation.
I0506 15:40:50.707693   548 net.cpp:226] conv2 needs backward computation.
I0506 15:40:50.707708   548 net.cpp:226] pool1 needs backward computation.
I0506 15:40:50.707721   548 net.cpp:226] relu1 needs backward computation.
I0506 15:40:50.707734   548 net.cpp:226] conv1 needs backward computation.
I0506 15:40:50.707754   548 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0506 15:40:50.707769   548 net.cpp:228] data_hdf5 does not need backward computation.
I0506 15:40:50.707782   548 net.cpp:270] This network produces output accuracy
I0506 15:40:50.707793   548 net.cpp:270] This network produces output loss
I0506 15:40:50.707826   548 net.cpp:283] Network initialization done.
I0506 15:40:50.707975   548 solver.cpp:60] Solver scaffolding done.
I0506 15:40:50.709137   548 caffe.cpp:212] Starting Optimization
I0506 15:40:50.709158   548 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0506 15:40:50.709172   548 solver.cpp:289] Learning Rate Policy: fixed
I0506 15:40:50.710289   548 solver.cpp:341] Iteration 0, Testing net (#0)
I0506 15:41:38.516166   548 solver.cpp:409]     Test net output #0: accuracy = 0.0769932
I0506 15:41:38.516327   548 solver.cpp:409]     Test net output #1: loss = 2.39821 (* 1 = 2.39821 loss)
I0506 15:41:38.549216   548 solver.cpp:237] Iteration 0, loss = 2.39816
I0506 15:41:38.549259   548 solver.cpp:253]     Train net output #0: loss = 2.39816 (* 1 = 2.39816 loss)
I0506 15:41:38.549281   548 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0506 15:41:44.358465   548 solver.cpp:237] Iteration 100, loss = 2.33483
I0506 15:41:44.358503   548 solver.cpp:253]     Train net output #0: loss = 2.33483 (* 1 = 2.33483 loss)
I0506 15:41:44.358522   548 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0506 15:41:50.169811   548 solver.cpp:237] Iteration 200, loss = 2.35447
I0506 15:41:50.169847   548 solver.cpp:253]     Train net output #0: loss = 2.35447 (* 1 = 2.35447 loss)
I0506 15:41:50.169867   548 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0506 15:41:55.980514   548 solver.cpp:237] Iteration 300, loss = 2.19867
I0506 15:41:55.980550   548 solver.cpp:253]     Train net output #0: loss = 2.19867 (* 1 = 2.19867 loss)
I0506 15:41:55.980568   548 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0506 15:42:01.794538   548 solver.cpp:237] Iteration 400, loss = 2.19568
I0506 15:42:01.794594   548 solver.cpp:253]     Train net output #0: loss = 2.19568 (* 1 = 2.19568 loss)
I0506 15:42:01.794618   548 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0506 15:42:07.551843   548 solver.cpp:341] Iteration 500, Testing net (#0)
I0506 15:42:54.415079   548 solver.cpp:409]     Test net output #0: accuracy = 0.50798
I0506 15:42:54.415251   548 solver.cpp:409]     Test net output #1: loss = 1.81428 (* 1 = 1.81428 loss)
I0506 15:42:54.433039   548 solver.cpp:237] Iteration 500, loss = 2.15564
I0506 15:42:54.433070   548 solver.cpp:253]     Train net output #0: loss = 2.15564 (* 1 = 2.15564 loss)
I0506 15:42:54.433086   548 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0506 15:43:00.258697   548 solver.cpp:237] Iteration 600, loss = 1.97853
I0506 15:43:00.258733   548 solver.cpp:253]     Train net output #0: loss = 1.97853 (* 1 = 1.97853 loss)
I0506 15:43:00.258757   548 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0506 15:43:06.081939   548 solver.cpp:237] Iteration 700, loss = 2.03957
I0506 15:43:06.081990   548 solver.cpp:253]     Train net output #0: loss = 2.03957 (* 1 = 2.03957 loss)
I0506 15:43:06.082008   548 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0506 15:43:11.908078   548 solver.cpp:237] Iteration 800, loss = 1.97302
I0506 15:43:11.908115   548 solver.cpp:253]     Train net output #0: loss = 1.97302 (* 1 = 1.97302 loss)
I0506 15:43:11.908133   548 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0506 15:43:17.734477   548 solver.cpp:237] Iteration 900, loss = 1.84497
I0506 15:43:17.734522   548 solver.cpp:253]     Train net output #0: loss = 1.84497 (* 1 = 1.84497 loss)
I0506 15:43:17.734549   548 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0506 15:43:23.501453   548 solver.cpp:341] Iteration 1000, Testing net (#0)
I0506 15:44:31.169231   548 solver.cpp:409]     Test net output #0: accuracy = 0.605853
I0506 15:44:31.169389   548 solver.cpp:409]     Test net output #1: loss = 1.42625 (* 1 = 1.42625 loss)
I0506 15:44:53.416998   548 solver.cpp:237] Iteration 1000, loss = 1.83823
I0506 15:44:53.417055   548 solver.cpp:253]     Train net output #0: loss = 1.83823 (* 1 = 1.83823 loss)
I0506 15:44:53.417075   548 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0506 15:44:59.234529   548 solver.cpp:237] Iteration 1100, loss = 1.93635
I0506 15:44:59.234578   548 solver.cpp:253]     Train net output #0: loss = 1.93635 (* 1 = 1.93635 loss)
I0506 15:44:59.234598   548 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0506 15:45:05.055481   548 solver.cpp:237] Iteration 1200, loss = 1.76838
I0506 15:45:05.055639   548 solver.cpp:253]     Train net output #0: loss = 1.76838 (* 1 = 1.76838 loss)
I0506 15:45:05.055656   548 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0506 15:45:10.873268   548 solver.cpp:237] Iteration 1300, loss = 1.78737
I0506 15:45:10.873303   548 solver.cpp:253]     Train net output #0: loss = 1.78737 (* 1 = 1.78737 loss)
I0506 15:45:10.873327   548 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0506 15:45:16.693167   548 solver.cpp:237] Iteration 1400, loss = 1.73201
I0506 15:45:16.693204   548 solver.cpp:253]     Train net output #0: loss = 1.73201 (* 1 = 1.73201 loss)
I0506 15:45:16.693222   548 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0506 15:45:22.454927   548 solver.cpp:341] Iteration 1500, Testing net (#0)
I0506 15:46:08.992502   548 solver.cpp:409]     Test net output #0: accuracy = 0.636539
I0506 15:46:08.992668   548 solver.cpp:409]     Test net output #1: loss = 1.25361 (* 1 = 1.25361 loss)
I0506 15:46:09.010418   548 solver.cpp:237] Iteration 1500, loss = 1.86736
I0506 15:46:09.010447   548 solver.cpp:253]     Train net output #0: loss = 1.86736 (* 1 = 1.86736 loss)
I0506 15:46:09.010464   548 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0506 15:46:14.826295   548 solver.cpp:237] Iteration 1600, loss = 1.62019
I0506 15:46:14.826333   548 solver.cpp:253]     Train net output #0: loss = 1.62019 (* 1 = 1.62019 loss)
I0506 15:46:14.826351   548 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0506 15:46:20.640310   548 solver.cpp:237] Iteration 1700, loss = 1.72314
I0506 15:46:20.640347   548 solver.cpp:253]     Train net output #0: loss = 1.72314 (* 1 = 1.72314 loss)
I0506 15:46:20.640365   548 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0506 15:46:26.452903   548 solver.cpp:237] Iteration 1800, loss = 1.59972
I0506 15:46:26.452939   548 solver.cpp:253]     Train net output #0: loss = 1.59972 (* 1 = 1.59972 loss)
I0506 15:46:26.452956   548 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0506 15:46:32.271397   548 solver.cpp:237] Iteration 1900, loss = 1.74479
I0506 15:46:32.271435   548 solver.cpp:253]     Train net output #0: loss = 1.74479 (* 1 = 1.74479 loss)
I0506 15:46:32.271453   548 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0506 15:46:38.027987   548 solver.cpp:341] Iteration 2000, Testing net (#0)
I0506 15:47:45.789324   548 solver.cpp:409]     Test net output #0: accuracy = 0.663566
I0506 15:47:45.789487   548 solver.cpp:409]     Test net output #1: loss = 1.14404 (* 1 = 1.14404 loss)
I0506 15:48:07.996631   548 solver.cpp:237] Iteration 2000, loss = 1.73497
I0506 15:48:07.996695   548 solver.cpp:253]     Train net output #0: loss = 1.73497 (* 1 = 1.73497 loss)
I0506 15:48:07.996714   548 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0506 15:48:13.802974   548 solver.cpp:237] Iteration 2100, loss = 1.63061
I0506 15:48:13.803022   548 solver.cpp:253]     Train net output #0: loss = 1.63061 (* 1 = 1.63061 loss)
I0506 15:48:13.803040   548 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0506 15:48:19.610055   548 solver.cpp:237] Iteration 2200, loss = 1.80009
I0506 15:48:19.610199   548 solver.cpp:253]     Train net output #0: loss = 1.80009 (* 1 = 1.80009 loss)
I0506 15:48:19.610218   548 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0506 15:48:25.423182   548 solver.cpp:237] Iteration 2300, loss = 1.6593
I0506 15:48:25.423218   548 solver.cpp:253]     Train net output #0: loss = 1.6593 (* 1 = 1.6593 loss)
I0506 15:48:25.423241   548 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0506 15:48:31.228355   548 solver.cpp:237] Iteration 2400, loss = 1.76152
I0506 15:48:31.228392   548 solver.cpp:253]     Train net output #0: loss = 1.76152 (* 1 = 1.76152 loss)
I0506 15:48:31.228411   548 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0506 15:48:36.979917   548 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_2500.caffemodel
I0506 15:48:37.064680   548 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_2500.solverstate
I0506 15:48:37.093950   548 solver.cpp:341] Iteration 2500, Testing net (#0)
I0506 15:49:23.943379   548 solver.cpp:409]     Test net output #0: accuracy = 0.695806
I0506 15:49:23.943537   548 solver.cpp:409]     Test net output #1: loss = 1.18157 (* 1 = 1.18157 loss)
I0506 15:49:23.961385   548 solver.cpp:237] Iteration 2500, loss = 1.62008
I0506 15:49:23.961415   548 solver.cpp:253]     Train net output #0: loss = 1.62008 (* 1 = 1.62008 loss)
I0506 15:49:23.961432   548 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0506 15:49:29.780894   548 solver.cpp:237] Iteration 2600, loss = 1.55636
I0506 15:49:29.780944   548 solver.cpp:253]     Train net output #0: loss = 1.55636 (* 1 = 1.55636 loss)
I0506 15:49:29.780974   548 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0506 15:49:35.601250   548 solver.cpp:237] Iteration 2700, loss = 1.74447
I0506 15:49:35.601286   548 solver.cpp:253]     Train net output #0: loss = 1.74447 (* 1 = 1.74447 loss)
I0506 15:49:35.601305   548 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0506 15:49:41.419250   548 solver.cpp:237] Iteration 2800, loss = 1.82214
I0506 15:49:41.419287   548 solver.cpp:253]     Train net output #0: loss = 1.82214 (* 1 = 1.82214 loss)
I0506 15:49:41.419306   548 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0506 15:49:47.235066   548 solver.cpp:237] Iteration 2900, loss = 1.56127
I0506 15:49:47.235105   548 solver.cpp:253]     Train net output #0: loss = 1.56127 (* 1 = 1.56127 loss)
I0506 15:49:47.235121   548 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0506 15:49:52.993587   548 solver.cpp:341] Iteration 3000, Testing net (#0)
I0506 15:51:00.809728   548 solver.cpp:409]     Test net output #0: accuracy = 0.722967
I0506 15:51:00.809900   548 solver.cpp:409]     Test net output #1: loss = 0.970791 (* 1 = 0.970791 loss)
I0506 15:51:23.057149   548 solver.cpp:237] Iteration 3000, loss = 1.54443
I0506 15:51:23.057210   548 solver.cpp:253]     Train net output #0: loss = 1.54443 (* 1 = 1.54443 loss)
I0506 15:51:23.057231   548 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0506 15:51:28.881696   548 solver.cpp:237] Iteration 3100, loss = 1.45666
I0506 15:51:28.881736   548 solver.cpp:253]     Train net output #0: loss = 1.45666 (* 1 = 1.45666 loss)
I0506 15:51:28.881753   548 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0506 15:51:34.708704   548 solver.cpp:237] Iteration 3200, loss = 1.53414
I0506 15:51:34.708868   548 solver.cpp:253]     Train net output #0: loss = 1.53414 (* 1 = 1.53414 loss)
I0506 15:51:34.708886   548 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0506 15:51:40.530649   548 solver.cpp:237] Iteration 3300, loss = 1.73702
I0506 15:51:40.530685   548 solver.cpp:253]     Train net output #0: loss = 1.73702 (* 1 = 1.73702 loss)
I0506 15:51:40.530705   548 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0506 15:51:46.349517   548 solver.cpp:237] Iteration 3400, loss = 1.72542
I0506 15:51:46.349553   548 solver.cpp:253]     Train net output #0: loss = 1.72542 (* 1 = 1.72542 loss)
I0506 15:51:46.349577   548 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0506 15:51:52.113152   548 solver.cpp:341] Iteration 3500, Testing net (#0)
I0506 15:52:38.746644   548 solver.cpp:409]     Test net output #0: accuracy = 0.714199
I0506 15:52:38.746811   548 solver.cpp:409]     Test net output #1: loss = 1.0262 (* 1 = 1.0262 loss)
I0506 15:52:38.764571   548 solver.cpp:237] Iteration 3500, loss = 1.66282
I0506 15:52:38.764600   548 solver.cpp:253]     Train net output #0: loss = 1.66282 (* 1 = 1.66282 loss)
I0506 15:52:38.764619   548 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0506 15:52:44.590929   548 solver.cpp:237] Iteration 3600, loss = 1.51825
I0506 15:52:44.590965   548 solver.cpp:253]     Train net output #0: loss = 1.51825 (* 1 = 1.51825 loss)
I0506 15:52:44.590984   548 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0506 15:52:50.413837   548 solver.cpp:237] Iteration 3700, loss = 1.55362
I0506 15:52:50.413892   548 solver.cpp:253]     Train net output #0: loss = 1.55362 (* 1 = 1.55362 loss)
I0506 15:52:50.413916   548 sgd_solver.cpp:106] Iteration 3700, lr = 0.0025
I0506 15:52:56.228793   548 solver.cpp:237] Iteration 3800, loss = 1.46161
I0506 15:52:56.228835   548 solver.cpp:253]     Train net output #0: loss = 1.46161 (* 1 = 1.46161 loss)
I0506 15:52:56.228852   548 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0506 15:53:02.043390   548 solver.cpp:237] Iteration 3900, loss = 1.48071
I0506 15:53:02.043426   548 solver.cpp:253]     Train net output #0: loss = 1.48071 (* 1 = 1.48071 loss)
I0506 15:53:02.043445   548 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0506 15:53:07.800524   548 solver.cpp:341] Iteration 4000, Testing net (#0)
I0506 15:54:15.606564   548 solver.cpp:409]     Test net output #0: accuracy = 0.762794
I0506 15:54:15.606737   548 solver.cpp:409]     Test net output #1: loss = 0.897578 (* 1 = 0.897578 loss)
I0506 15:54:37.852254   548 solver.cpp:237] Iteration 4000, loss = 1.46808
I0506 15:54:37.852315   548 solver.cpp:253]     Train net output #0: loss = 1.46808 (* 1 = 1.46808 loss)
I0506 15:54:37.852344   548 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0506 15:54:43.671319   548 solver.cpp:237] Iteration 4100, loss = 1.47031
I0506 15:54:43.671355   548 solver.cpp:253]     Train net output #0: loss = 1.47031 (* 1 = 1.47031 loss)
I0506 15:54:43.671377   548 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0506 15:54:49.493541   548 solver.cpp:237] Iteration 4200, loss = 1.39633
I0506 15:54:49.493687   548 solver.cpp:253]     Train net output #0: loss = 1.39633 (* 1 = 1.39633 loss)
I0506 15:54:49.493705   548 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0506 15:54:55.314126   548 solver.cpp:237] Iteration 4300, loss = 1.45652
I0506 15:54:55.314179   548 solver.cpp:253]     Train net output #0: loss = 1.45652 (* 1 = 1.45652 loss)
I0506 15:54:55.314198   548 sgd_solver.cpp:106] Iteration 4300, lr = 0.0025
I0506 15:55:01.133333   548 solver.cpp:237] Iteration 4400, loss = 1.69114
I0506 15:55:01.133369   548 solver.cpp:253]     Train net output #0: loss = 1.69114 (* 1 = 1.69114 loss)
I0506 15:55:01.133388   548 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0506 15:55:06.895197   548 solver.cpp:341] Iteration 4500, Testing net (#0)
I0506 15:55:53.868835   548 solver.cpp:409]     Test net output #0: accuracy = 0.784568
I0506 15:55:53.868996   548 solver.cpp:409]     Test net output #1: loss = 0.818183 (* 1 = 0.818183 loss)
I0506 15:55:53.886919   548 solver.cpp:237] Iteration 4500, loss = 1.28948
I0506 15:55:53.886950   548 solver.cpp:253]     Train net output #0: loss = 1.28948 (* 1 = 1.28948 loss)
I0506 15:55:53.886967   548 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0506 15:55:59.717465   548 solver.cpp:237] Iteration 4600, loss = 1.47967
I0506 15:55:59.717504   548 solver.cpp:253]     Train net output #0: loss = 1.47967 (* 1 = 1.47967 loss)
I0506 15:55:59.717520   548 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0506 15:56:05.542781   548 solver.cpp:237] Iteration 4700, loss = 1.2869
I0506 15:56:05.542817   548 solver.cpp:253]     Train net output #0: loss = 1.2869 (* 1 = 1.2869 loss)
I0506 15:56:05.542836   548 sgd_solver.cpp:106] Iteration 4700, lr = 0.0025
I0506 15:56:11.375720   548 solver.cpp:237] Iteration 4800, loss = 1.5379
I0506 15:56:11.375772   548 solver.cpp:253]     Train net output #0: loss = 1.5379 (* 1 = 1.5379 loss)
I0506 15:56:11.375798   548 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0506 15:56:17.208030   548 solver.cpp:237] Iteration 4900, loss = 1.45268
I0506 15:56:17.208068   548 solver.cpp:253]     Train net output #0: loss = 1.45268 (* 1 = 1.45268 loss)
I0506 15:56:17.208086   548 sgd_solver.cpp:106] Iteration 4900, lr = 0.0025
I0506 15:56:22.982689   548 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_5000.caffemodel
I0506 15:56:23.070574   548 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_5000.solverstate
I0506 15:56:23.103909   548 solver.cpp:341] Iteration 5000, Testing net (#0)
I0506 15:57:30.857991   548 solver.cpp:409]     Test net output #0: accuracy = 0.791135
I0506 15:57:30.858157   548 solver.cpp:409]     Test net output #1: loss = 0.76186 (* 1 = 0.76186 loss)
I0506 15:57:53.110558   548 solver.cpp:237] Iteration 5000, loss = 1.40894
I0506 15:57:53.110621   548 solver.cpp:253]     Train net output #0: loss = 1.40894 (* 1 = 1.40894 loss)
I0506 15:57:53.110649   548 sgd_solver.cpp:106] Iteration 5000, lr = 0.0025
I0506 15:57:58.923642   548 solver.cpp:237] Iteration 5100, loss = 1.29896
I0506 15:57:58.923681   548 solver.cpp:253]     Train net output #0: loss = 1.29896 (* 1 = 1.29896 loss)
I0506 15:57:58.923697   548 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0506 15:58:04.737725   548 solver.cpp:237] Iteration 5200, loss = 1.36197
I0506 15:58:04.737887   548 solver.cpp:253]     Train net output #0: loss = 1.36197 (* 1 = 1.36197 loss)
I0506 15:58:04.737905   548 sgd_solver.cpp:106] Iteration 5200, lr = 0.0025
I0506 15:58:10.554780   548 solver.cpp:237] Iteration 5300, loss = 1.54357
I0506 15:58:10.554816   548 solver.cpp:253]     Train net output #0: loss = 1.54357 (* 1 = 1.54357 loss)
I0506 15:58:10.554834   548 sgd_solver.cpp:106] Iteration 5300, lr = 0.0025
I0506 15:58:16.367100   548 solver.cpp:237] Iteration 5400, loss = 1.44206
I0506 15:58:16.367153   548 solver.cpp:253]     Train net output #0: loss = 1.44206 (* 1 = 1.44206 loss)
I0506 15:58:16.367180   548 sgd_solver.cpp:106] Iteration 5400, lr = 0.0025
I0506 15:58:22.124595   548 solver.cpp:341] Iteration 5500, Testing net (#0)
I0506 15:59:08.692164   548 solver.cpp:409]     Test net output #0: accuracy = 0.802674
I0506 15:59:08.692330   548 solver.cpp:409]     Test net output #1: loss = 0.756602 (* 1 = 0.756602 loss)
I0506 15:59:08.710191   548 solver.cpp:237] Iteration 5500, loss = 1.51204
I0506 15:59:08.710221   548 solver.cpp:253]     Train net output #0: loss = 1.51204 (* 1 = 1.51204 loss)
I0506 15:59:08.710245   548 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0506 15:59:14.532204   548 solver.cpp:237] Iteration 5600, loss = 1.33727
I0506 15:59:14.532240   548 solver.cpp:253]     Train net output #0: loss = 1.33727 (* 1 = 1.33727 loss)
I0506 15:59:14.532264   548 sgd_solver.cpp:106] Iteration 5600, lr = 0.0025
I0506 15:59:20.351100   548 solver.cpp:237] Iteration 5700, loss = 1.55907
I0506 15:59:20.351136   548 solver.cpp:253]     Train net output #0: loss = 1.55907 (* 1 = 1.55907 loss)
I0506 15:59:20.351155   548 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0506 15:59:26.172072   548 solver.cpp:237] Iteration 5800, loss = 1.50542
I0506 15:59:26.172109   548 solver.cpp:253]     Train net output #0: loss = 1.50542 (* 1 = 1.50542 loss)
I0506 15:59:26.172127   548 sgd_solver.cpp:106] Iteration 5800, lr = 0.0025
I0506 15:59:31.985949   548 solver.cpp:237] Iteration 5900, loss = 1.33652
I0506 15:59:31.986009   548 solver.cpp:253]     Train net output #0: loss = 1.33652 (* 1 = 1.33652 loss)
I0506 15:59:31.986027   548 sgd_solver.cpp:106] Iteration 5900, lr = 0.0025
I0506 15:59:37.736742   548 solver.cpp:341] Iteration 6000, Testing net (#0)
I0506 16:00:45.477917   548 solver.cpp:409]     Test net output #0: accuracy = 0.806968
I0506 16:00:45.478086   548 solver.cpp:409]     Test net output #1: loss = 0.742556 (* 1 = 0.742556 loss)
I0506 16:01:07.667769   548 solver.cpp:237] Iteration 6000, loss = 1.50538
I0506 16:01:07.667829   548 solver.cpp:253]     Train net output #0: loss = 1.50538 (* 1 = 1.50538 loss)
I0506 16:01:07.667856   548 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0506 16:01:13.476333   548 solver.cpp:237] Iteration 6100, loss = 1.29301
I0506 16:01:13.476380   548 solver.cpp:253]     Train net output #0: loss = 1.29301 (* 1 = 1.29301 loss)
I0506 16:01:13.476397   548 sgd_solver.cpp:106] Iteration 6100, lr = 0.0025
I0506 16:01:19.283704   548 solver.cpp:237] Iteration 6200, loss = 1.42905
I0506 16:01:19.283855   548 solver.cpp:253]     Train net output #0: loss = 1.42905 (* 1 = 1.42905 loss)
I0506 16:01:19.283874   548 sgd_solver.cpp:106] Iteration 6200, lr = 0.0025
I0506 16:01:25.089465   548 solver.cpp:237] Iteration 6300, loss = 1.41154
I0506 16:01:25.089503   548 solver.cpp:253]     Train net output #0: loss = 1.41154 (* 1 = 1.41154 loss)
I0506 16:01:25.089520   548 sgd_solver.cpp:106] Iteration 6300, lr = 0.0025
I0506 16:01:30.897541   548 solver.cpp:237] Iteration 6400, loss = 1.56037
I0506 16:01:30.897575   548 solver.cpp:253]     Train net output #0: loss = 1.56037 (* 1 = 1.56037 loss)
I0506 16:01:30.897600   548 sgd_solver.cpp:106] Iteration 6400, lr = 0.0025
I0506 16:01:36.647544   548 solver.cpp:341] Iteration 6500, Testing net (#0)
I0506 16:02:23.541786   548 solver.cpp:409]     Test net output #0: accuracy = 0.809934
I0506 16:02:23.541959   548 solver.cpp:409]     Test net output #1: loss = 0.666829 (* 1 = 0.666829 loss)
I0506 16:02:23.559844   548 solver.cpp:237] Iteration 6500, loss = 1.54876
I0506 16:02:23.559873   548 solver.cpp:253]     Train net output #0: loss = 1.54876 (* 1 = 1.54876 loss)
I0506 16:02:23.559892   548 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0506 16:02:29.377638   548 solver.cpp:237] Iteration 6600, loss = 1.22257
I0506 16:02:29.377686   548 solver.cpp:253]     Train net output #0: loss = 1.22257 (* 1 = 1.22257 loss)
I0506 16:02:29.377715   548 sgd_solver.cpp:106] Iteration 6600, lr = 0.0025
I0506 16:02:35.192780   548 solver.cpp:237] Iteration 6700, loss = 1.46709
I0506 16:02:35.192817   548 solver.cpp:253]     Train net output #0: loss = 1.46709 (* 1 = 1.46709 loss)
I0506 16:02:35.192836   548 sgd_solver.cpp:106] Iteration 6700, lr = 0.0025
I0506 16:02:41.009666   548 solver.cpp:237] Iteration 6800, loss = 1.33756
I0506 16:02:41.009701   548 solver.cpp:253]     Train net output #0: loss = 1.33756 (* 1 = 1.33756 loss)
I0506 16:02:41.009721   548 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0506 16:02:46.829527   548 solver.cpp:237] Iteration 6900, loss = 1.49909
I0506 16:02:46.829563   548 solver.cpp:253]     Train net output #0: loss = 1.49909 (* 1 = 1.49909 loss)
I0506 16:02:46.829582   548 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0506 16:02:52.592562   548 solver.cpp:341] Iteration 7000, Testing net (#0)
I0506 16:04:00.309018   548 solver.cpp:409]     Test net output #0: accuracy = 0.817453
I0506 16:04:00.309186   548 solver.cpp:409]     Test net output #1: loss = 0.636348 (* 1 = 0.636348 loss)
I0506 16:04:22.545805   548 solver.cpp:237] Iteration 7000, loss = 1.26596
I0506 16:04:22.545867   548 solver.cpp:253]     Train net output #0: loss = 1.26596 (* 1 = 1.26596 loss)
I0506 16:04:22.545895   548 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0506 16:04:28.369444   548 solver.cpp:237] Iteration 7100, loss = 1.25631
I0506 16:04:28.369482   548 solver.cpp:253]     Train net output #0: loss = 1.25631 (* 1 = 1.25631 loss)
I0506 16:04:28.369499   548 sgd_solver.cpp:106] Iteration 7100, lr = 0.0025
I0506 16:04:34.195349   548 solver.cpp:237] Iteration 7200, loss = 1.19767
I0506 16:04:34.195513   548 solver.cpp:253]     Train net output #0: loss = 1.19767 (* 1 = 1.19767 loss)
I0506 16:04:34.195531   548 sgd_solver.cpp:106] Iteration 7200, lr = 0.0025
I0506 16:04:40.017757   548 solver.cpp:237] Iteration 7300, loss = 1.37158
I0506 16:04:40.017793   548 solver.cpp:253]     Train net output #0: loss = 1.37158 (* 1 = 1.37158 loss)
I0506 16:04:40.017812   548 sgd_solver.cpp:106] Iteration 7300, lr = 0.0025
I0506 16:04:45.838490   548 solver.cpp:237] Iteration 7400, loss = 1.14779
I0506 16:04:45.838526   548 solver.cpp:253]     Train net output #0: loss = 1.14779 (* 1 = 1.14779 loss)
I0506 16:04:45.838544   548 sgd_solver.cpp:106] Iteration 7400, lr = 0.0025
I0506 16:04:51.601171   548 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_7500.caffemodel
I0506 16:04:51.680433   548 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_7500.solverstate
I0506 16:04:51.710141   548 solver.cpp:341] Iteration 7500, Testing net (#0)
I0506 16:05:38.257648   548 solver.cpp:409]     Test net output #0: accuracy = 0.81974
I0506 16:05:38.257814   548 solver.cpp:409]     Test net output #1: loss = 0.721755 (* 1 = 0.721755 loss)
I0506 16:05:38.275565   548 solver.cpp:237] Iteration 7500, loss = 1.51958
I0506 16:05:38.275595   548 solver.cpp:253]     Train net output #0: loss = 1.51958 (* 1 = 1.51958 loss)
I0506 16:05:38.275614   548 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0506 16:05:44.115907   548 solver.cpp:237] Iteration 7600, loss = 1.33002
I0506 16:05:44.115960   548 solver.cpp:253]     Train net output #0: loss = 1.33002 (* 1 = 1.33002 loss)
I0506 16:05:44.115988   548 sgd_solver.cpp:106] Iteration 7600, lr = 0.0025
I0506 16:05:49.954488   548 solver.cpp:237] Iteration 7700, loss = 1.45699
I0506 16:05:49.954524   548 solver.cpp:253]     Train net output #0: loss = 1.45699 (* 1 = 1.45699 loss)
I0506 16:05:49.954542   548 sgd_solver.cpp:106] Iteration 7700, lr = 0.0025
I0506 16:05:55.786782   548 solver.cpp:237] Iteration 7800, loss = 1.48328
I0506 16:05:55.786819   548 solver.cpp:253]     Train net output #0: loss = 1.48328 (* 1 = 1.48328 loss)
I0506 16:05:55.786839   548 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0506 16:06:01.620959   548 solver.cpp:237] Iteration 7900, loss = 1.36139
I0506 16:06:01.620995   548 solver.cpp:253]     Train net output #0: loss = 1.36139 (* 1 = 1.36139 loss)
I0506 16:06:01.621018   548 sgd_solver.cpp:106] Iteration 7900, lr = 0.0025
I0506 16:06:07.398537   548 solver.cpp:341] Iteration 8000, Testing net (#0)
I0506 16:07:15.263123   548 solver.cpp:409]     Test net output #0: accuracy = 0.823613
I0506 16:07:15.263298   548 solver.cpp:409]     Test net output #1: loss = 0.600406 (* 1 = 0.600406 loss)
I0506 16:07:37.582366   548 solver.cpp:237] Iteration 8000, loss = 1.36887
I0506 16:07:37.582428   548 solver.cpp:253]     Train net output #0: loss = 1.36887 (* 1 = 1.36887 loss)
I0506 16:07:37.582448   548 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0506 16:07:43.391762   548 solver.cpp:237] Iteration 8100, loss = 1.23429
I0506 16:07:43.391799   548 solver.cpp:253]     Train net output #0: loss = 1.23429 (* 1 = 1.23429 loss)
I0506 16:07:43.391824   548 sgd_solver.cpp:106] Iteration 8100, lr = 0.0025
I0506 16:07:49.205124   548 solver.cpp:237] Iteration 8200, loss = 1.56692
I0506 16:07:49.205296   548 solver.cpp:253]     Train net output #0: loss = 1.56692 (* 1 = 1.56692 loss)
I0506 16:07:49.205315   548 sgd_solver.cpp:106] Iteration 8200, lr = 0.0025
I0506 16:07:55.024883   548 solver.cpp:237] Iteration 8300, loss = 1.30024
I0506 16:07:55.024922   548 solver.cpp:253]     Train net output #0: loss = 1.30024 (* 1 = 1.30024 loss)
I0506 16:07:55.024940   548 sgd_solver.cpp:106] Iteration 8300, lr = 0.0025
I0506 16:08:00.838711   548 solver.cpp:237] Iteration 8400, loss = 1.3418
I0506 16:08:00.838749   548 solver.cpp:253]     Train net output #0: loss = 1.3418 (* 1 = 1.3418 loss)
I0506 16:08:00.838767   548 sgd_solver.cpp:106] Iteration 8400, lr = 0.0025
I0506 16:08:06.591619   548 solver.cpp:341] Iteration 8500, Testing net (#0)
I0506 16:08:53.530441   548 solver.cpp:409]     Test net output #0: accuracy = 0.825
I0506 16:08:53.530606   548 solver.cpp:409]     Test net output #1: loss = 0.592821 (* 1 = 0.592821 loss)
I0506 16:08:53.548450   548 solver.cpp:237] Iteration 8500, loss = 1.27005
I0506 16:08:53.548480   548 solver.cpp:253]     Train net output #0: loss = 1.27005 (* 1 = 1.27005 loss)
I0506 16:08:53.548498   548 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0506 16:08:59.356678   548 solver.cpp:237] Iteration 8600, loss = 1.2854
I0506 16:08:59.356714   548 solver.cpp:253]     Train net output #0: loss = 1.2854 (* 1 = 1.2854 loss)
I0506 16:08:59.356732   548 sgd_solver.cpp:106] Iteration 8600, lr = 0.0025
I0506 16:09:05.165848   548 solver.cpp:237] Iteration 8700, loss = 1.19343
I0506 16:09:05.165900   548 solver.cpp:253]     Train net output #0: loss = 1.19343 (* 1 = 1.19343 loss)
I0506 16:09:05.165928   548 sgd_solver.cpp:106] Iteration 8700, lr = 0.0025
I0506 16:09:11.049909   548 solver.cpp:237] Iteration 8800, loss = 1.3226
I0506 16:09:11.049947   548 solver.cpp:253]     Train net output #0: loss = 1.3226 (* 1 = 1.3226 loss)
I0506 16:09:11.049969   548 sgd_solver.cpp:106] Iteration 8800, lr = 0.0025
I0506 16:09:16.858685   548 solver.cpp:237] Iteration 8900, loss = 1.3239
I0506 16:09:16.858721   548 solver.cpp:253]     Train net output #0: loss = 1.3239 (* 1 = 1.3239 loss)
I0506 16:09:16.858739   548 sgd_solver.cpp:106] Iteration 8900, lr = 0.0025
I0506 16:09:22.610368   548 solver.cpp:341] Iteration 9000, Testing net (#0)
I0506 16:10:30.444774   548 solver.cpp:409]     Test net output #0: accuracy = 0.822827
I0506 16:10:30.444952   548 solver.cpp:409]     Test net output #1: loss = 0.602679 (* 1 = 0.602679 loss)
I0506 16:10:52.660856   548 solver.cpp:237] Iteration 9000, loss = 1.42539
I0506 16:10:52.660915   548 solver.cpp:253]     Train net output #0: loss = 1.42539 (* 1 = 1.42539 loss)
I0506 16:10:52.660941   548 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0506 16:10:58.479305   548 solver.cpp:237] Iteration 9100, loss = 1.43828
I0506 16:10:58.479341   548 solver.cpp:253]     Train net output #0: loss = 1.43828 (* 1 = 1.43828 loss)
I0506 16:10:58.479359   548 sgd_solver.cpp:106] Iteration 9100, lr = 0.0025
I0506 16:11:04.297082   548 solver.cpp:237] Iteration 9200, loss = 1.37262
I0506 16:11:04.297240   548 solver.cpp:253]     Train net output #0: loss = 1.37262 (* 1 = 1.37262 loss)
I0506 16:11:04.297257   548 sgd_solver.cpp:106] Iteration 9200, lr = 0.0025
I0506 16:11:10.114672   548 solver.cpp:237] Iteration 9300, loss = 1.3546
I0506 16:11:10.114723   548 solver.cpp:253]     Train net output #0: loss = 1.3546 (* 1 = 1.3546 loss)
I0506 16:11:10.114740   548 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0506 16:11:15.921877   548 solver.cpp:237] Iteration 9400, loss = 1.2582
I0506 16:11:15.921912   548 solver.cpp:253]     Train net output #0: loss = 1.2582 (* 1 = 1.2582 loss)
I0506 16:11:15.921937   548 sgd_solver.cpp:106] Iteration 9400, lr = 0.0025
I0506 16:11:21.669190   548 solver.cpp:341] Iteration 9500, Testing net (#0)
I0506 16:12:08.239485   548 solver.cpp:409]     Test net output #0: accuracy = 0.827333
I0506 16:12:08.239651   548 solver.cpp:409]     Test net output #1: loss = 0.567011 (* 1 = 0.567011 loss)
I0506 16:12:08.257406   548 solver.cpp:237] Iteration 9500, loss = 1.37674
I0506 16:12:08.257436   548 solver.cpp:253]     Train net output #0: loss = 1.37674 (* 1 = 1.37674 loss)
I0506 16:12:08.257453   548 sgd_solver.cpp:106] Iteration 9500, lr = 0.0025
I0506 16:12:14.068553   548 solver.cpp:237] Iteration 9600, loss = 1.44874
I0506 16:12:14.068591   548 solver.cpp:253]     Train net output #0: loss = 1.44874 (* 1 = 1.44874 loss)
I0506 16:12:14.068610   548 sgd_solver.cpp:106] Iteration 9600, lr = 0.0025
I0506 16:12:19.880837   548 solver.cpp:237] Iteration 9700, loss = 1.32188
I0506 16:12:19.880873   548 solver.cpp:253]     Train net output #0: loss = 1.32188 (* 1 = 1.32188 loss)
I0506 16:12:19.880892   548 sgd_solver.cpp:106] Iteration 9700, lr = 0.0025
I0506 16:12:25.698726   548 solver.cpp:237] Iteration 9800, loss = 1.59335
I0506 16:12:25.698781   548 solver.cpp:253]     Train net output #0: loss = 1.59335 (* 1 = 1.59335 loss)
I0506 16:12:25.698807   548 sgd_solver.cpp:106] Iteration 9800, lr = 0.0025
I0506 16:12:31.517710   548 solver.cpp:237] Iteration 9900, loss = 1.40329
I0506 16:12:31.517746   548 solver.cpp:253]     Train net output #0: loss = 1.40329 (* 1 = 1.40329 loss)
I0506 16:12:31.517765   548 sgd_solver.cpp:106] Iteration 9900, lr = 0.0025
I0506 16:12:37.280623   548 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_10000.caffemodel
I0506 16:12:37.360203   548 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_10000.solverstate
I0506 16:12:37.386656   548 solver.cpp:341] Iteration 10000, Testing net (#0)
I0506 16:13:45.130892   548 solver.cpp:409]     Test net output #0: accuracy = 0.827727
I0506 16:13:45.131072   548 solver.cpp:409]     Test net output #1: loss = 0.545871 (* 1 = 0.545871 loss)
I0506 16:14:07.430380   548 solver.cpp:237] Iteration 10000, loss = 1.24696
I0506 16:14:07.430443   548 solver.cpp:253]     Train net output #0: loss = 1.24696 (* 1 = 1.24696 loss)
I0506 16:14:07.430469   548 sgd_solver.cpp:106] Iteration 10000, lr = 0.0025
I0506 16:14:13.241179   548 solver.cpp:237] Iteration 10100, loss = 1.33532
I0506 16:14:13.241215   548 solver.cpp:253]     Train net output #0: loss = 1.33532 (* 1 = 1.33532 loss)
I0506 16:14:13.241240   548 sgd_solver.cpp:106] Iteration 10100, lr = 0.0025
I0506 16:14:19.056813   548 solver.cpp:237] Iteration 10200, loss = 1.39891
I0506 16:14:19.056978   548 solver.cpp:253]     Train net output #0: loss = 1.39891 (* 1 = 1.39891 loss)
I0506 16:14:19.056995   548 sgd_solver.cpp:106] Iteration 10200, lr = 0.0025
I0506 16:14:24.871577   548 solver.cpp:237] Iteration 10300, loss = 1.21974
I0506 16:14:24.871614   548 solver.cpp:253]     Train net output #0: loss = 1.21974 (* 1 = 1.21974 loss)
I0506 16:14:24.871630   548 sgd_solver.cpp:106] Iteration 10300, lr = 0.0025
I0506 16:14:30.681838   548 solver.cpp:237] Iteration 10400, loss = 1.28744
I0506 16:14:30.681888   548 solver.cpp:253]     Train net output #0: loss = 1.28744 (* 1 = 1.28744 loss)
I0506 16:14:30.681905   548 sgd_solver.cpp:106] Iteration 10400, lr = 0.0025
I0506 16:14:36.435816   548 solver.cpp:341] Iteration 10500, Testing net (#0)
I0506 16:15:23.402174   548 solver.cpp:409]     Test net output #0: accuracy = 0.82918
I0506 16:15:23.402340   548 solver.cpp:409]     Test net output #1: loss = 0.536471 (* 1 = 0.536471 loss)
I0506 16:15:23.420083   548 solver.cpp:237] Iteration 10500, loss = 1.2783
I0506 16:15:23.420114   548 solver.cpp:253]     Train net output #0: loss = 1.2783 (* 1 = 1.2783 loss)
I0506 16:15:23.420131   548 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0506 16:15:29.245985   548 solver.cpp:237] Iteration 10600, loss = 1.40094
I0506 16:15:29.246021   548 solver.cpp:253]     Train net output #0: loss = 1.40094 (* 1 = 1.40094 loss)
I0506 16:15:29.246039   548 sgd_solver.cpp:106] Iteration 10600, lr = 0.0025
I0506 16:15:35.068011   548 solver.cpp:237] Iteration 10700, loss = 1.24667
I0506 16:15:35.068048   548 solver.cpp:253]     Train net output #0: loss = 1.24667 (* 1 = 1.24667 loss)
I0506 16:15:35.068066   548 sgd_solver.cpp:106] Iteration 10700, lr = 0.0025
I0506 16:15:40.891441   548 solver.cpp:237] Iteration 10800, loss = 1.3615
I0506 16:15:40.891479   548 solver.cpp:253]     Train net output #0: loss = 1.3615 (* 1 = 1.3615 loss)
I0506 16:15:40.891495   548 sgd_solver.cpp:106] Iteration 10800, lr = 0.0025
I0506 16:15:46.715351   548 solver.cpp:237] Iteration 10900, loss = 1.42414
I0506 16:15:46.715404   548 solver.cpp:253]     Train net output #0: loss = 1.42414 (* 1 = 1.42414 loss)
I0506 16:15:46.715430   548 sgd_solver.cpp:106] Iteration 10900, lr = 0.0025
I0506 16:15:52.481824   548 solver.cpp:341] Iteration 11000, Testing net (#0)
I0506 16:17:00.342389   548 solver.cpp:409]     Test net output #0: accuracy = 0.834094
I0506 16:17:00.342562   548 solver.cpp:409]     Test net output #1: loss = 0.538419 (* 1 = 0.538419 loss)
I0506 16:17:22.627645   548 solver.cpp:237] Iteration 11000, loss = 1.26009
I0506 16:17:22.627707   548 solver.cpp:253]     Train net output #0: loss = 1.26009 (* 1 = 1.26009 loss)
I0506 16:17:22.627734   548 sgd_solver.cpp:106] Iteration 11000, lr = 0.0025
I0506 16:17:28.456904   548 solver.cpp:237] Iteration 11100, loss = 1.25347
I0506 16:17:28.456951   548 solver.cpp:253]     Train net output #0: loss = 1.25347 (* 1 = 1.25347 loss)
I0506 16:17:28.456971   548 sgd_solver.cpp:106] Iteration 11100, lr = 0.0025
I0506 16:17:34.283823   548 solver.cpp:237] Iteration 11200, loss = 1.24496
I0506 16:17:34.283982   548 solver.cpp:253]     Train net output #0: loss = 1.24496 (* 1 = 1.24496 loss)
I0506 16:17:34.283998   548 sgd_solver.cpp:106] Iteration 11200, lr = 0.0025
I0506 16:17:40.109338   548 solver.cpp:237] Iteration 11300, loss = 1.49536
I0506 16:17:40.109375   548 solver.cpp:253]     Train net output #0: loss = 1.49536 (* 1 = 1.49536 loss)
I0506 16:17:40.109392   548 sgd_solver.cpp:106] Iteration 11300, lr = 0.0025
I0506 16:17:45.932538   548 solver.cpp:237] Iteration 11400, loss = 1.42005
I0506 16:17:45.932575   548 solver.cpp:253]     Train net output #0: loss = 1.42005 (* 1 = 1.42005 loss)
I0506 16:17:45.932593   548 sgd_solver.cpp:106] Iteration 11400, lr = 0.0025
I0506 16:17:51.706428   548 solver.cpp:341] Iteration 11500, Testing net (#0)
I0506 16:18:38.309242   548 solver.cpp:409]     Test net output #0: accuracy = 0.836318
I0506 16:18:38.309422   548 solver.cpp:409]     Test net output #1: loss = 0.512949 (* 1 = 0.512949 loss)
I0506 16:18:38.327289   548 solver.cpp:237] Iteration 11500, loss = 1.33759
I0506 16:18:38.327318   548 solver.cpp:253]     Train net output #0: loss = 1.33759 (* 1 = 1.33759 loss)
I0506 16:18:38.327337   548 sgd_solver.cpp:106] Iteration 11500, lr = 0.0025
I0506 16:18:44.152560   548 solver.cpp:237] Iteration 11600, loss = 1.09632
I0506 16:18:44.152611   548 solver.cpp:253]     Train net output #0: loss = 1.09632 (* 1 = 1.09632 loss)
I0506 16:18:44.152628   548 sgd_solver.cpp:106] Iteration 11600, lr = 0.0025
I0506 16:18:49.976650   548 solver.cpp:237] Iteration 11700, loss = 1.23103
I0506 16:18:49.976687   548 solver.cpp:253]     Train net output #0: loss = 1.23103 (* 1 = 1.23103 loss)
I0506 16:18:49.976704   548 sgd_solver.cpp:106] Iteration 11700, lr = 0.0025
I0506 16:18:55.796926   548 solver.cpp:237] Iteration 11800, loss = 1.2179
I0506 16:18:55.796962   548 solver.cpp:253]     Train net output #0: loss = 1.2179 (* 1 = 1.2179 loss)
I0506 16:18:55.796980   548 sgd_solver.cpp:106] Iteration 11800, lr = 0.0025
I0506 16:19:01.620651   548 solver.cpp:237] Iteration 11900, loss = 1.23447
I0506 16:19:01.620687   548 solver.cpp:253]     Train net output #0: loss = 1.23447 (* 1 = 1.23447 loss)
I0506 16:19:01.620704   548 sgd_solver.cpp:106] Iteration 11900, lr = 0.0025
I0506 16:19:07.390178   548 solver.cpp:341] Iteration 12000, Testing net (#0)
I0506 16:20:15.149406   548 solver.cpp:409]     Test net output #0: accuracy = 0.842526
I0506 16:20:15.149580   548 solver.cpp:409]     Test net output #1: loss = 0.534933 (* 1 = 0.534933 loss)
I0506 16:20:37.431279   548 solver.cpp:237] Iteration 12000, loss = 1.25403
I0506 16:20:37.431339   548 solver.cpp:253]     Train net output #0: loss = 1.25403 (* 1 = 1.25403 loss)
I0506 16:20:37.431360   548 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0506 16:20:43.246585   548 solver.cpp:237] Iteration 12100, loss = 1.16828
I0506 16:20:43.246623   548 solver.cpp:253]     Train net output #0: loss = 1.16828 (* 1 = 1.16828 loss)
I0506 16:20:43.246641   548 sgd_solver.cpp:106] Iteration 12100, lr = 0.0025
I0506 16:20:49.063912   548 solver.cpp:237] Iteration 12200, loss = 1.23949
I0506 16:20:49.064100   548 solver.cpp:253]     Train net output #0: loss = 1.23949 (* 1 = 1.23949 loss)
I0506 16:20:49.064116   548 sgd_solver.cpp:106] Iteration 12200, lr = 0.0025
I0506 16:20:54.885149   548 solver.cpp:237] Iteration 12300, loss = 1.2463
I0506 16:20:54.885185   548 solver.cpp:253]     Train net output #0: loss = 1.2463 (* 1 = 1.2463 loss)
I0506 16:20:54.885205   548 sgd_solver.cpp:106] Iteration 12300, lr = 0.0025
I0506 16:21:00.708277   548 solver.cpp:237] Iteration 12400, loss = 1.4613
I0506 16:21:00.708313   548 solver.cpp:253]     Train net output #0: loss = 1.4613 (* 1 = 1.4613 loss)
I0506 16:21:00.708331   548 sgd_solver.cpp:106] Iteration 12400, lr = 0.0025
I0506 16:21:06.469702   548 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_12500.caffemodel
I0506 16:21:06.548701   548 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_12500.solverstate
I0506 16:21:06.575796   548 solver.cpp:341] Iteration 12500, Testing net (#0)
I0506 16:21:53.436216   548 solver.cpp:409]     Test net output #0: accuracy = 0.844311
I0506 16:21:53.436388   548 solver.cpp:409]     Test net output #1: loss = 0.517175 (* 1 = 0.517175 loss)
I0506 16:21:53.454263   548 solver.cpp:237] Iteration 12500, loss = 1.42857
I0506 16:21:53.454293   548 solver.cpp:253]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0506 16:21:53.454309   548 sgd_solver.cpp:106] Iteration 12500, lr = 0.0025
I0506 16:21:59.280594   548 solver.cpp:237] Iteration 12600, loss = 1.18096
I0506 16:21:59.280648   548 solver.cpp:253]     Train net output #0: loss = 1.18096 (* 1 = 1.18096 loss)
I0506 16:21:59.280674   548 sgd_solver.cpp:106] Iteration 12600, lr = 0.0025
I0506 16:22:05.102397   548 solver.cpp:237] Iteration 12700, loss = 1.24826
I0506 16:22:05.102440   548 solver.cpp:253]     Train net output #0: loss = 1.24826 (* 1 = 1.24826 loss)
I0506 16:22:05.102458   548 sgd_solver.cpp:106] Iteration 12700, lr = 0.0025
I0506 16:22:10.922969   548 solver.cpp:237] Iteration 12800, loss = 1.07821
I0506 16:22:10.923005   548 solver.cpp:253]     Train net output #0: loss = 1.07821 (* 1 = 1.07821 loss)
I0506 16:22:10.923029   548 sgd_solver.cpp:106] Iteration 12800, lr = 0.0025
I0506 16:22:16.747934   548 solver.cpp:237] Iteration 12900, loss = 1.25292
I0506 16:22:16.747970   548 solver.cpp:253]     Train net output #0: loss = 1.25292 (* 1 = 1.25292 loss)
I0506 16:22:16.747993   548 sgd_solver.cpp:106] Iteration 12900, lr = 0.0025
I0506 16:22:22.513257   548 solver.cpp:341] Iteration 13000, Testing net (#0)
I0506 16:23:30.277555   548 solver.cpp:409]     Test net output #0: accuracy = 0.846959
I0506 16:23:30.277736   548 solver.cpp:409]     Test net output #1: loss = 0.500917 (* 1 = 0.500917 loss)
I0506 16:23:52.534056   548 solver.cpp:237] Iteration 13000, loss = 1.35757
I0506 16:23:52.534119   548 solver.cpp:253]     Train net output #0: loss = 1.35757 (* 1 = 1.35757 loss)
I0506 16:23:52.534147   548 sgd_solver.cpp:106] Iteration 13000, lr = 0.0025
I0506 16:23:58.345049   548 solver.cpp:237] Iteration 13100, loss = 1.29016
I0506 16:23:58.345088   548 solver.cpp:253]     Train net output #0: loss = 1.29016 (* 1 = 1.29016 loss)
I0506 16:23:58.345104   548 sgd_solver.cpp:106] Iteration 13100, lr = 0.0025
I0506 16:24:04.159137   548 solver.cpp:237] Iteration 13200, loss = 1.29135
I0506 16:24:04.159299   548 solver.cpp:253]     Train net output #0: loss = 1.29135 (* 1 = 1.29135 loss)
I0506 16:24:04.159317   548 sgd_solver.cpp:106] Iteration 13200, lr = 0.0025
I0506 16:24:09.972823   548 solver.cpp:237] Iteration 13300, loss = 1.38244
I0506 16:24:09.972859   548 solver.cpp:253]     Train net output #0: loss = 1.38244 (* 1 = 1.38244 loss)
I0506 16:24:09.972878   548 sgd_solver.cpp:106] Iteration 13300, lr = 0.0025
I0506 16:24:15.789536   548 solver.cpp:237] Iteration 13400, loss = 1.59579
I0506 16:24:15.789573   548 solver.cpp:253]     Train net output #0: loss = 1.59579 (* 1 = 1.59579 loss)
I0506 16:24:15.789597   548 sgd_solver.cpp:106] Iteration 13400, lr = 0.0025
I0506 16:24:21.547251   548 solver.cpp:341] Iteration 13500, Testing net (#0)
I0506 16:25:08.102816   548 solver.cpp:409]     Test net output #0: accuracy = 0.847632
I0506 16:25:08.102987   548 solver.cpp:409]     Test net output #1: loss = 0.518331 (* 1 = 0.518331 loss)
I0506 16:25:08.120903   548 solver.cpp:237] Iteration 13500, loss = 1.51964
I0506 16:25:08.120932   548 solver.cpp:253]     Train net output #0: loss = 1.51964 (* 1 = 1.51964 loss)
I0506 16:25:08.120951   548 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0506 16:25:13.940176   548 solver.cpp:237] Iteration 13600, loss = 1.34405
I0506 16:25:13.940212   548 solver.cpp:253]     Train net output #0: loss = 1.34405 (* 1 = 1.34405 loss)
I0506 16:25:13.940232   548 sgd_solver.cpp:106] Iteration 13600, lr = 0.0025
I0506 16:25:19.761471   548 solver.cpp:237] Iteration 13700, loss = 1.40458
I0506 16:25:19.761523   548 solver.cpp:253]     Train net output #0: loss = 1.40458 (* 1 = 1.40458 loss)
I0506 16:25:19.761541   548 sgd_solver.cpp:106] Iteration 13700, lr = 0.0025
I0506 16:25:25.579388   548 solver.cpp:237] Iteration 13800, loss = 1.21827
I0506 16:25:25.579427   548 solver.cpp:253]     Train net output #0: loss = 1.21827 (* 1 = 1.21827 loss)
I0506 16:25:25.579443   548 sgd_solver.cpp:106] Iteration 13800, lr = 0.0025
I0506 16:25:31.398376   548 solver.cpp:237] Iteration 13900, loss = 1.1757
I0506 16:25:31.398412   548 solver.cpp:253]     Train net output #0: loss = 1.1757 (* 1 = 1.1757 loss)
I0506 16:25:31.398430   548 sgd_solver.cpp:106] Iteration 13900, lr = 0.0025
I0506 16:25:37.156718   548 solver.cpp:341] Iteration 14000, Testing net (#0)
I0506 16:26:44.846881   548 solver.cpp:409]     Test net output #0: accuracy = 0.852533
I0506 16:26:44.847064   548 solver.cpp:409]     Test net output #1: loss = 0.529765 (* 1 = 0.529765 loss)
I0506 16:27:07.079217   548 solver.cpp:237] Iteration 14000, loss = 1.16461
I0506 16:27:07.079277   548 solver.cpp:253]     Train net output #0: loss = 1.16461 (* 1 = 1.16461 loss)
I0506 16:27:07.079298   548 sgd_solver.cpp:106] Iteration 14000, lr = 0.0025
I0506 16:27:12.904476   548 solver.cpp:237] Iteration 14100, loss = 1.05362
I0506 16:27:12.904515   548 solver.cpp:253]     Train net output #0: loss = 1.05362 (* 1 = 1.05362 loss)
I0506 16:27:12.904533   548 sgd_solver.cpp:106] Iteration 14100, lr = 0.0025
I0506 16:27:18.730464   548 solver.cpp:237] Iteration 14200, loss = 1.15463
I0506 16:27:18.730620   548 solver.cpp:253]     Train net output #0: loss = 1.15463 (* 1 = 1.15463 loss)
I0506 16:27:18.730638   548 sgd_solver.cpp:106] Iteration 14200, lr = 0.0025
I0506 16:27:24.556928   548 solver.cpp:237] Iteration 14300, loss = 1.56828
I0506 16:27:24.556983   548 solver.cpp:253]     Train net output #0: loss = 1.56828 (* 1 = 1.56828 loss)
I0506 16:27:24.557000   548 sgd_solver.cpp:106] Iteration 14300, lr = 0.0025
I0506 16:27:30.382599   548 solver.cpp:237] Iteration 14400, loss = 1.39944
I0506 16:27:30.382634   548 solver.cpp:253]     Train net output #0: loss = 1.39944 (* 1 = 1.39944 loss)
I0506 16:27:30.382653   548 sgd_solver.cpp:106] Iteration 14400, lr = 0.0025
I0506 16:27:36.151885   548 solver.cpp:341] Iteration 14500, Testing net (#0)
I0506 16:28:23.104828   548 solver.cpp:409]     Test net output #0: accuracy = 0.850592
I0506 16:28:23.105000   548 solver.cpp:409]     Test net output #1: loss = 0.500073 (* 1 = 0.500073 loss)
I0506 16:28:23.122782   548 solver.cpp:237] Iteration 14500, loss = 1.34192
I0506 16:28:23.122812   548 solver.cpp:253]     Train net output #0: loss = 1.34192 (* 1 = 1.34192 loss)
I0506 16:28:23.122829   548 sgd_solver.cpp:106] Iteration 14500, lr = 0.0025
I0506 16:28:28.942841   548 solver.cpp:237] Iteration 14600, loss = 1.46885
I0506 16:28:28.942878   548 solver.cpp:253]     Train net output #0: loss = 1.46885 (* 1 = 1.46885 loss)
I0506 16:28:28.942895   548 sgd_solver.cpp:106] Iteration 14600, lr = 0.0025
I0506 16:28:34.764613   548 solver.cpp:237] Iteration 14700, loss = 1.27031
I0506 16:28:34.764650   548 solver.cpp:253]     Train net output #0: loss = 1.27031 (* 1 = 1.27031 loss)
I0506 16:28:34.764673   548 sgd_solver.cpp:106] Iteration 14700, lr = 0.0025
I0506 16:28:40.583461   548 solver.cpp:237] Iteration 14800, loss = 1.18888
I0506 16:28:40.583514   548 solver.cpp:253]     Train net output #0: loss = 1.18888 (* 1 = 1.18888 loss)
I0506 16:28:40.583539   548 sgd_solver.cpp:106] Iteration 14800, lr = 0.0025
I0506 16:28:46.405230   548 solver.cpp:237] Iteration 14900, loss = 1.31307
I0506 16:28:46.405268   548 solver.cpp:253]     Train net output #0: loss = 1.31307 (* 1 = 1.31307 loss)
I0506 16:28:46.405287   548 sgd_solver.cpp:106] Iteration 14900, lr = 0.0025
I0506 16:28:52.171754   548 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_15000.caffemodel
I0506 16:28:52.251173   548 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_15000.solverstate
I0506 16:29:13.215730   548 solver.cpp:321] Iteration 15000, loss = 1.1433
I0506 16:29:13.215904   548 solver.cpp:341] Iteration 15000, Testing net (#0)
I0506 16:30:21.003849   548 solver.cpp:409]     Test net output #0: accuracy = 0.853372
I0506 16:30:21.004019   548 solver.cpp:409]     Test net output #1: loss = 0.521832 (* 1 = 0.521832 loss)
I0506 16:30:21.004037   548 solver.cpp:326] Optimization Done.
I0506 16:30:21.004050   548 caffe.cpp:215] Optimization Done.
Application 11148610 network throttled: 1 node throttled, 00:00:20 node-seconds
Application 11148610 balanced injection 100, after throttle 100
Application 11148610 resources: utime ~2590s, stime ~425s, Rss ~5331112, inblocks ~9437758, outblocks ~120930
