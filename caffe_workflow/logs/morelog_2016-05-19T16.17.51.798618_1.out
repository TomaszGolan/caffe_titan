2803975
I0519 16:18:10.808965 22864 caffe.cpp:184] Using GPUs 0
I0519 16:18:11.231449 22864 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1500
test_interval: 3000
base_lr: 0.0025
display: 100
max_iter: 15000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt"
I0519 16:18:11.233836 22864 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0519 16:18:11.236940 22864 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0519 16:18:11.237004 22864 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0519 16:18:11.237383 22864 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0519 16:18:11.237586 22864 layer_factory.hpp:77] Creating layer data_hdf5
I0519 16:18:11.237622 22864 net.cpp:106] Creating Layer data_hdf5
I0519 16:18:11.237639 22864 net.cpp:411] data_hdf5 -> data
I0519 16:18:11.237673 22864 net.cpp:411] data_hdf5 -> label
I0519 16:18:11.237716 22864 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0519 16:18:11.239506 22864 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0519 16:18:11.242112 22864 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0519 16:18:32.750263 22864 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0519 16:18:32.755537 22864 net.cpp:150] Setting up data_hdf5
I0519 16:18:32.755578 22864 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0519 16:18:32.755595 22864 net.cpp:157] Top shape: 100 (100)
I0519 16:18:32.755607 22864 net.cpp:165] Memory required for data: 2540400
I0519 16:18:32.755627 22864 layer_factory.hpp:77] Creating layer conv1
I0519 16:18:32.755673 22864 net.cpp:106] Creating Layer conv1
I0519 16:18:32.755687 22864 net.cpp:454] conv1 <- data
I0519 16:18:32.755712 22864 net.cpp:411] conv1 -> conv1
I0519 16:18:34.771158 22864 net.cpp:150] Setting up conv1
I0519 16:18:34.771212 22864 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 16:18:34.771227 22864 net.cpp:165] Memory required for data: 30188400
I0519 16:18:34.771258 22864 layer_factory.hpp:77] Creating layer relu1
I0519 16:18:34.771280 22864 net.cpp:106] Creating Layer relu1
I0519 16:18:34.771301 22864 net.cpp:454] relu1 <- conv1
I0519 16:18:34.771337 22864 net.cpp:397] relu1 -> conv1 (in-place)
I0519 16:18:34.771869 22864 net.cpp:150] Setting up relu1
I0519 16:18:34.771893 22864 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 16:18:34.771906 22864 net.cpp:165] Memory required for data: 57836400
I0519 16:18:34.771922 22864 layer_factory.hpp:77] Creating layer pool1
I0519 16:18:34.771950 22864 net.cpp:106] Creating Layer pool1
I0519 16:18:34.771963 22864 net.cpp:454] pool1 <- conv1
I0519 16:18:34.771980 22864 net.cpp:411] pool1 -> pool1
I0519 16:18:34.772073 22864 net.cpp:150] Setting up pool1
I0519 16:18:34.772091 22864 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0519 16:18:34.772106 22864 net.cpp:165] Memory required for data: 71660400
I0519 16:18:34.772126 22864 layer_factory.hpp:77] Creating layer conv2
I0519 16:18:34.772151 22864 net.cpp:106] Creating Layer conv2
I0519 16:18:34.772164 22864 net.cpp:454] conv2 <- pool1
I0519 16:18:34.772181 22864 net.cpp:411] conv2 -> conv2
I0519 16:18:34.774938 22864 net.cpp:150] Setting up conv2
I0519 16:18:34.774969 22864 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 16:18:34.774984 22864 net.cpp:165] Memory required for data: 91532400
I0519 16:18:34.775012 22864 layer_factory.hpp:77] Creating layer relu2
I0519 16:18:34.775040 22864 net.cpp:106] Creating Layer relu2
I0519 16:18:34.775054 22864 net.cpp:454] relu2 <- conv2
I0519 16:18:34.775070 22864 net.cpp:397] relu2 -> conv2 (in-place)
I0519 16:18:34.775421 22864 net.cpp:150] Setting up relu2
I0519 16:18:34.775441 22864 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 16:18:34.775454 22864 net.cpp:165] Memory required for data: 111404400
I0519 16:18:34.775467 22864 layer_factory.hpp:77] Creating layer pool2
I0519 16:18:34.775493 22864 net.cpp:106] Creating Layer pool2
I0519 16:18:34.775506 22864 net.cpp:454] pool2 <- conv2
I0519 16:18:34.775522 22864 net.cpp:411] pool2 -> pool2
I0519 16:18:34.775617 22864 net.cpp:150] Setting up pool2
I0519 16:18:34.775635 22864 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0519 16:18:34.775650 22864 net.cpp:165] Memory required for data: 121340400
I0519 16:18:34.775668 22864 layer_factory.hpp:77] Creating layer conv3
I0519 16:18:34.775689 22864 net.cpp:106] Creating Layer conv3
I0519 16:18:34.775712 22864 net.cpp:454] conv3 <- pool2
I0519 16:18:34.775728 22864 net.cpp:411] conv3 -> conv3
I0519 16:18:34.777680 22864 net.cpp:150] Setting up conv3
I0519 16:18:34.777705 22864 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 16:18:34.777725 22864 net.cpp:165] Memory required for data: 132182000
I0519 16:18:34.777748 22864 layer_factory.hpp:77] Creating layer relu3
I0519 16:18:34.777770 22864 net.cpp:106] Creating Layer relu3
I0519 16:18:34.777793 22864 net.cpp:454] relu3 <- conv3
I0519 16:18:34.777809 22864 net.cpp:397] relu3 -> conv3 (in-place)
I0519 16:18:34.778296 22864 net.cpp:150] Setting up relu3
I0519 16:18:34.778321 22864 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 16:18:34.778333 22864 net.cpp:165] Memory required for data: 143023600
I0519 16:18:34.778349 22864 layer_factory.hpp:77] Creating layer pool3
I0519 16:18:34.778365 22864 net.cpp:106] Creating Layer pool3
I0519 16:18:34.778388 22864 net.cpp:454] pool3 <- conv3
I0519 16:18:34.778405 22864 net.cpp:411] pool3 -> pool3
I0519 16:18:34.778488 22864 net.cpp:150] Setting up pool3
I0519 16:18:34.778511 22864 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0519 16:18:34.778523 22864 net.cpp:165] Memory required for data: 148444400
I0519 16:18:34.778538 22864 layer_factory.hpp:77] Creating layer conv4
I0519 16:18:34.778558 22864 net.cpp:106] Creating Layer conv4
I0519 16:18:34.778578 22864 net.cpp:454] conv4 <- pool3
I0519 16:18:34.778594 22864 net.cpp:411] conv4 -> conv4
I0519 16:18:34.781575 22864 net.cpp:150] Setting up conv4
I0519 16:18:34.781607 22864 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 16:18:34.781622 22864 net.cpp:165] Memory required for data: 152073200
I0519 16:18:34.781641 22864 layer_factory.hpp:77] Creating layer relu4
I0519 16:18:34.781663 22864 net.cpp:106] Creating Layer relu4
I0519 16:18:34.781677 22864 net.cpp:454] relu4 <- conv4
I0519 16:18:34.781704 22864 net.cpp:397] relu4 -> conv4 (in-place)
I0519 16:18:34.782196 22864 net.cpp:150] Setting up relu4
I0519 16:18:34.782219 22864 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 16:18:34.782232 22864 net.cpp:165] Memory required for data: 155702000
I0519 16:18:34.782248 22864 layer_factory.hpp:77] Creating layer pool4
I0519 16:18:34.782263 22864 net.cpp:106] Creating Layer pool4
I0519 16:18:34.782285 22864 net.cpp:454] pool4 <- conv4
I0519 16:18:34.782301 22864 net.cpp:411] pool4 -> pool4
I0519 16:18:34.782385 22864 net.cpp:150] Setting up pool4
I0519 16:18:34.782403 22864 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0519 16:18:34.782418 22864 net.cpp:165] Memory required for data: 157516400
I0519 16:18:34.782430 22864 layer_factory.hpp:77] Creating layer ip1
I0519 16:18:34.782459 22864 net.cpp:106] Creating Layer ip1
I0519 16:18:34.782471 22864 net.cpp:454] ip1 <- pool4
I0519 16:18:34.782487 22864 net.cpp:411] ip1 -> ip1
I0519 16:18:34.798008 22864 net.cpp:150] Setting up ip1
I0519 16:18:34.798038 22864 net.cpp:157] Top shape: 100 196 (19600)
I0519 16:18:34.798059 22864 net.cpp:165] Memory required for data: 157594800
I0519 16:18:34.798085 22864 layer_factory.hpp:77] Creating layer relu5
I0519 16:18:34.798107 22864 net.cpp:106] Creating Layer relu5
I0519 16:18:34.798120 22864 net.cpp:454] relu5 <- ip1
I0519 16:18:34.798149 22864 net.cpp:397] relu5 -> ip1 (in-place)
I0519 16:18:34.798506 22864 net.cpp:150] Setting up relu5
I0519 16:18:34.798526 22864 net.cpp:157] Top shape: 100 196 (19600)
I0519 16:18:34.798538 22864 net.cpp:165] Memory required for data: 157673200
I0519 16:18:34.798553 22864 layer_factory.hpp:77] Creating layer drop1
I0519 16:18:34.798583 22864 net.cpp:106] Creating Layer drop1
I0519 16:18:34.798598 22864 net.cpp:454] drop1 <- ip1
I0519 16:18:34.798614 22864 net.cpp:397] drop1 -> ip1 (in-place)
I0519 16:18:34.798684 22864 net.cpp:150] Setting up drop1
I0519 16:18:34.798702 22864 net.cpp:157] Top shape: 100 196 (19600)
I0519 16:18:34.798714 22864 net.cpp:165] Memory required for data: 157751600
I0519 16:18:34.798730 22864 layer_factory.hpp:77] Creating layer ip2
I0519 16:18:34.798751 22864 net.cpp:106] Creating Layer ip2
I0519 16:18:34.798770 22864 net.cpp:454] ip2 <- ip1
I0519 16:18:34.798794 22864 net.cpp:411] ip2 -> ip2
I0519 16:18:34.799281 22864 net.cpp:150] Setting up ip2
I0519 16:18:34.799300 22864 net.cpp:157] Top shape: 100 98 (9800)
I0519 16:18:34.799314 22864 net.cpp:165] Memory required for data: 157790800
I0519 16:18:34.799335 22864 layer_factory.hpp:77] Creating layer relu6
I0519 16:18:34.799348 22864 net.cpp:106] Creating Layer relu6
I0519 16:18:34.799367 22864 net.cpp:454] relu6 <- ip2
I0519 16:18:34.799383 22864 net.cpp:397] relu6 -> ip2 (in-place)
I0519 16:18:34.799932 22864 net.cpp:150] Setting up relu6
I0519 16:18:34.799955 22864 net.cpp:157] Top shape: 100 98 (9800)
I0519 16:18:34.799968 22864 net.cpp:165] Memory required for data: 157830000
I0519 16:18:34.799983 22864 layer_factory.hpp:77] Creating layer drop2
I0519 16:18:34.799999 22864 net.cpp:106] Creating Layer drop2
I0519 16:18:34.800021 22864 net.cpp:454] drop2 <- ip2
I0519 16:18:34.800037 22864 net.cpp:397] drop2 -> ip2 (in-place)
I0519 16:18:34.800087 22864 net.cpp:150] Setting up drop2
I0519 16:18:34.800110 22864 net.cpp:157] Top shape: 100 98 (9800)
I0519 16:18:34.800122 22864 net.cpp:165] Memory required for data: 157869200
I0519 16:18:34.800135 22864 layer_factory.hpp:77] Creating layer ip3
I0519 16:18:34.800151 22864 net.cpp:106] Creating Layer ip3
I0519 16:18:34.800166 22864 net.cpp:454] ip3 <- ip2
I0519 16:18:34.800181 22864 net.cpp:411] ip3 -> ip3
I0519 16:18:34.800410 22864 net.cpp:150] Setting up ip3
I0519 16:18:34.800429 22864 net.cpp:157] Top shape: 100 11 (1100)
I0519 16:18:34.800442 22864 net.cpp:165] Memory required for data: 157873600
I0519 16:18:34.800462 22864 layer_factory.hpp:77] Creating layer drop3
I0519 16:18:34.800483 22864 net.cpp:106] Creating Layer drop3
I0519 16:18:34.800496 22864 net.cpp:454] drop3 <- ip3
I0519 16:18:34.800519 22864 net.cpp:397] drop3 -> ip3 (in-place)
I0519 16:18:34.800566 22864 net.cpp:150] Setting up drop3
I0519 16:18:34.800581 22864 net.cpp:157] Top shape: 100 11 (1100)
I0519 16:18:34.800601 22864 net.cpp:165] Memory required for data: 157878000
I0519 16:18:34.800614 22864 layer_factory.hpp:77] Creating layer loss
I0519 16:18:34.800638 22864 net.cpp:106] Creating Layer loss
I0519 16:18:34.800650 22864 net.cpp:454] loss <- ip3
I0519 16:18:34.800665 22864 net.cpp:454] loss <- label
I0519 16:18:34.800686 22864 net.cpp:411] loss -> loss
I0519 16:18:34.800707 22864 layer_factory.hpp:77] Creating layer loss
I0519 16:18:34.801375 22864 net.cpp:150] Setting up loss
I0519 16:18:34.801398 22864 net.cpp:157] Top shape: (1)
I0519 16:18:34.801414 22864 net.cpp:160]     with loss weight 1
I0519 16:18:34.801463 22864 net.cpp:165] Memory required for data: 157878004
I0519 16:18:34.801477 22864 net.cpp:226] loss needs backward computation.
I0519 16:18:34.801491 22864 net.cpp:226] drop3 needs backward computation.
I0519 16:18:34.801504 22864 net.cpp:226] ip3 needs backward computation.
I0519 16:18:34.801519 22864 net.cpp:226] drop2 needs backward computation.
I0519 16:18:34.801532 22864 net.cpp:226] relu6 needs backward computation.
I0519 16:18:34.801551 22864 net.cpp:226] ip2 needs backward computation.
I0519 16:18:34.801564 22864 net.cpp:226] drop1 needs backward computation.
I0519 16:18:34.801578 22864 net.cpp:226] relu5 needs backward computation.
I0519 16:18:34.801589 22864 net.cpp:226] ip1 needs backward computation.
I0519 16:18:34.801602 22864 net.cpp:226] pool4 needs backward computation.
I0519 16:18:34.801616 22864 net.cpp:226] relu4 needs backward computation.
I0519 16:18:34.801628 22864 net.cpp:226] conv4 needs backward computation.
I0519 16:18:34.801640 22864 net.cpp:226] pool3 needs backward computation.
I0519 16:18:34.801653 22864 net.cpp:226] relu3 needs backward computation.
I0519 16:18:34.801666 22864 net.cpp:226] conv3 needs backward computation.
I0519 16:18:34.801712 22864 net.cpp:226] pool2 needs backward computation.
I0519 16:18:34.801733 22864 net.cpp:226] relu2 needs backward computation.
I0519 16:18:34.801745 22864 net.cpp:226] conv2 needs backward computation.
I0519 16:18:34.801761 22864 net.cpp:226] pool1 needs backward computation.
I0519 16:18:34.801774 22864 net.cpp:226] relu1 needs backward computation.
I0519 16:18:34.801786 22864 net.cpp:226] conv1 needs backward computation.
I0519 16:18:34.801800 22864 net.cpp:228] data_hdf5 does not need backward computation.
I0519 16:18:34.801816 22864 net.cpp:270] This network produces output loss
I0519 16:18:34.801848 22864 net.cpp:283] Network initialization done.
I0519 16:18:34.803504 22864 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0519 16:18:34.803585 22864 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0519 16:18:34.803963 22864 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0519 16:18:34.804183 22864 layer_factory.hpp:77] Creating layer data_hdf5
I0519 16:18:34.804204 22864 net.cpp:106] Creating Layer data_hdf5
I0519 16:18:34.804220 22864 net.cpp:411] data_hdf5 -> data
I0519 16:18:34.804241 22864 net.cpp:411] data_hdf5 -> label
I0519 16:18:34.804260 22864 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0519 16:18:34.819537 22864 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0519 16:18:56.135190 22864 net.cpp:150] Setting up data_hdf5
I0519 16:18:56.135361 22864 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0519 16:18:56.135381 22864 net.cpp:157] Top shape: 100 (100)
I0519 16:18:56.135393 22864 net.cpp:165] Memory required for data: 2540400
I0519 16:18:56.135408 22864 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0519 16:18:56.135442 22864 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0519 16:18:56.135457 22864 net.cpp:454] label_data_hdf5_1_split <- label
I0519 16:18:56.135493 22864 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0519 16:18:56.135515 22864 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0519 16:18:56.135602 22864 net.cpp:150] Setting up label_data_hdf5_1_split
I0519 16:18:56.135620 22864 net.cpp:157] Top shape: 100 (100)
I0519 16:18:56.135635 22864 net.cpp:157] Top shape: 100 (100)
I0519 16:18:56.135648 22864 net.cpp:165] Memory required for data: 2541200
I0519 16:18:56.135659 22864 layer_factory.hpp:77] Creating layer conv1
I0519 16:18:56.135692 22864 net.cpp:106] Creating Layer conv1
I0519 16:18:56.135705 22864 net.cpp:454] conv1 <- data
I0519 16:18:56.135722 22864 net.cpp:411] conv1 -> conv1
I0519 16:18:56.137730 22864 net.cpp:150] Setting up conv1
I0519 16:18:56.137755 22864 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 16:18:56.137778 22864 net.cpp:165] Memory required for data: 30189200
I0519 16:18:56.137801 22864 layer_factory.hpp:77] Creating layer relu1
I0519 16:18:56.137822 22864 net.cpp:106] Creating Layer relu1
I0519 16:18:56.137845 22864 net.cpp:454] relu1 <- conv1
I0519 16:18:56.137863 22864 net.cpp:397] relu1 -> conv1 (in-place)
I0519 16:18:56.138381 22864 net.cpp:150] Setting up relu1
I0519 16:18:56.138404 22864 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 16:18:56.138417 22864 net.cpp:165] Memory required for data: 57837200
I0519 16:18:56.138429 22864 layer_factory.hpp:77] Creating layer pool1
I0519 16:18:56.138460 22864 net.cpp:106] Creating Layer pool1
I0519 16:18:56.138473 22864 net.cpp:454] pool1 <- conv1
I0519 16:18:56.138490 22864 net.cpp:411] pool1 -> pool1
I0519 16:18:56.138579 22864 net.cpp:150] Setting up pool1
I0519 16:18:56.138597 22864 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0519 16:18:56.138612 22864 net.cpp:165] Memory required for data: 71661200
I0519 16:18:56.138630 22864 layer_factory.hpp:77] Creating layer conv2
I0519 16:18:56.138650 22864 net.cpp:106] Creating Layer conv2
I0519 16:18:56.138664 22864 net.cpp:454] conv2 <- pool1
I0519 16:18:56.138680 22864 net.cpp:411] conv2 -> conv2
I0519 16:18:56.140647 22864 net.cpp:150] Setting up conv2
I0519 16:18:56.140672 22864 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 16:18:56.140693 22864 net.cpp:165] Memory required for data: 91533200
I0519 16:18:56.140714 22864 layer_factory.hpp:77] Creating layer relu2
I0519 16:18:56.140734 22864 net.cpp:106] Creating Layer relu2
I0519 16:18:56.140756 22864 net.cpp:454] relu2 <- conv2
I0519 16:18:56.140772 22864 net.cpp:397] relu2 -> conv2 (in-place)
I0519 16:18:56.141118 22864 net.cpp:150] Setting up relu2
I0519 16:18:56.141137 22864 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 16:18:56.141150 22864 net.cpp:165] Memory required for data: 111405200
I0519 16:18:56.141162 22864 layer_factory.hpp:77] Creating layer pool2
I0519 16:18:56.141180 22864 net.cpp:106] Creating Layer pool2
I0519 16:18:56.141193 22864 net.cpp:454] pool2 <- conv2
I0519 16:18:56.141209 22864 net.cpp:411] pool2 -> pool2
I0519 16:18:56.141326 22864 net.cpp:150] Setting up pool2
I0519 16:18:56.141345 22864 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0519 16:18:56.141358 22864 net.cpp:165] Memory required for data: 121341200
I0519 16:18:56.141373 22864 layer_factory.hpp:77] Creating layer conv3
I0519 16:18:56.141396 22864 net.cpp:106] Creating Layer conv3
I0519 16:18:56.141408 22864 net.cpp:454] conv3 <- pool2
I0519 16:18:56.141425 22864 net.cpp:411] conv3 -> conv3
I0519 16:18:56.143460 22864 net.cpp:150] Setting up conv3
I0519 16:18:56.143488 22864 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 16:18:56.143501 22864 net.cpp:165] Memory required for data: 132182800
I0519 16:18:56.143540 22864 layer_factory.hpp:77] Creating layer relu3
I0519 16:18:56.143566 22864 net.cpp:106] Creating Layer relu3
I0519 16:18:56.143580 22864 net.cpp:454] relu3 <- conv3
I0519 16:18:56.143604 22864 net.cpp:397] relu3 -> conv3 (in-place)
I0519 16:18:56.144103 22864 net.cpp:150] Setting up relu3
I0519 16:18:56.144125 22864 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 16:18:56.144139 22864 net.cpp:165] Memory required for data: 143024400
I0519 16:18:56.144155 22864 layer_factory.hpp:77] Creating layer pool3
I0519 16:18:56.144170 22864 net.cpp:106] Creating Layer pool3
I0519 16:18:56.144192 22864 net.cpp:454] pool3 <- conv3
I0519 16:18:56.144209 22864 net.cpp:411] pool3 -> pool3
I0519 16:18:56.144294 22864 net.cpp:150] Setting up pool3
I0519 16:18:56.144312 22864 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0519 16:18:56.144323 22864 net.cpp:165] Memory required for data: 148445200
I0519 16:18:56.144338 22864 layer_factory.hpp:77] Creating layer conv4
I0519 16:18:56.144366 22864 net.cpp:106] Creating Layer conv4
I0519 16:18:56.144378 22864 net.cpp:454] conv4 <- pool3
I0519 16:18:56.144402 22864 net.cpp:411] conv4 -> conv4
I0519 16:18:56.146564 22864 net.cpp:150] Setting up conv4
I0519 16:18:56.146587 22864 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 16:18:56.146608 22864 net.cpp:165] Memory required for data: 152074000
I0519 16:18:56.146627 22864 layer_factory.hpp:77] Creating layer relu4
I0519 16:18:56.146646 22864 net.cpp:106] Creating Layer relu4
I0519 16:18:56.146659 22864 net.cpp:454] relu4 <- conv4
I0519 16:18:56.146683 22864 net.cpp:397] relu4 -> conv4 (in-place)
I0519 16:18:56.147178 22864 net.cpp:150] Setting up relu4
I0519 16:18:56.147200 22864 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 16:18:56.147213 22864 net.cpp:165] Memory required for data: 155702800
I0519 16:18:56.147229 22864 layer_factory.hpp:77] Creating layer pool4
I0519 16:18:56.147245 22864 net.cpp:106] Creating Layer pool4
I0519 16:18:56.147266 22864 net.cpp:454] pool4 <- conv4
I0519 16:18:56.147284 22864 net.cpp:411] pool4 -> pool4
I0519 16:18:56.147370 22864 net.cpp:150] Setting up pool4
I0519 16:18:56.147388 22864 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0519 16:18:56.147403 22864 net.cpp:165] Memory required for data: 157517200
I0519 16:18:56.147415 22864 layer_factory.hpp:77] Creating layer ip1
I0519 16:18:56.147433 22864 net.cpp:106] Creating Layer ip1
I0519 16:18:56.147452 22864 net.cpp:454] ip1 <- pool4
I0519 16:18:56.147475 22864 net.cpp:411] ip1 -> ip1
I0519 16:18:56.162984 22864 net.cpp:150] Setting up ip1
I0519 16:18:56.163017 22864 net.cpp:157] Top shape: 100 196 (19600)
I0519 16:18:56.163036 22864 net.cpp:165] Memory required for data: 157595600
I0519 16:18:56.163064 22864 layer_factory.hpp:77] Creating layer relu5
I0519 16:18:56.163084 22864 net.cpp:106] Creating Layer relu5
I0519 16:18:56.163097 22864 net.cpp:454] relu5 <- ip1
I0519 16:18:56.163126 22864 net.cpp:397] relu5 -> ip1 (in-place)
I0519 16:18:56.163496 22864 net.cpp:150] Setting up relu5
I0519 16:18:56.163516 22864 net.cpp:157] Top shape: 100 196 (19600)
I0519 16:18:56.163528 22864 net.cpp:165] Memory required for data: 157674000
I0519 16:18:56.163543 22864 layer_factory.hpp:77] Creating layer drop1
I0519 16:18:56.163573 22864 net.cpp:106] Creating Layer drop1
I0519 16:18:56.163586 22864 net.cpp:454] drop1 <- ip1
I0519 16:18:56.163604 22864 net.cpp:397] drop1 -> ip1 (in-place)
I0519 16:18:56.163655 22864 net.cpp:150] Setting up drop1
I0519 16:18:56.163679 22864 net.cpp:157] Top shape: 100 196 (19600)
I0519 16:18:56.163691 22864 net.cpp:165] Memory required for data: 157752400
I0519 16:18:56.163704 22864 layer_factory.hpp:77] Creating layer ip2
I0519 16:18:56.163723 22864 net.cpp:106] Creating Layer ip2
I0519 16:18:56.163735 22864 net.cpp:454] ip2 <- ip1
I0519 16:18:56.163758 22864 net.cpp:411] ip2 -> ip2
I0519 16:18:56.164252 22864 net.cpp:150] Setting up ip2
I0519 16:18:56.164270 22864 net.cpp:157] Top shape: 100 98 (9800)
I0519 16:18:56.164283 22864 net.cpp:165] Memory required for data: 157791600
I0519 16:18:56.164304 22864 layer_factory.hpp:77] Creating layer relu6
I0519 16:18:56.164340 22864 net.cpp:106] Creating Layer relu6
I0519 16:18:56.164353 22864 net.cpp:454] relu6 <- ip2
I0519 16:18:56.164369 22864 net.cpp:397] relu6 -> ip2 (in-place)
I0519 16:18:56.164953 22864 net.cpp:150] Setting up relu6
I0519 16:18:56.164976 22864 net.cpp:157] Top shape: 100 98 (9800)
I0519 16:18:56.164989 22864 net.cpp:165] Memory required for data: 157830800
I0519 16:18:56.165005 22864 layer_factory.hpp:77] Creating layer drop2
I0519 16:18:56.165021 22864 net.cpp:106] Creating Layer drop2
I0519 16:18:56.165035 22864 net.cpp:454] drop2 <- ip2
I0519 16:18:56.165052 22864 net.cpp:397] drop2 -> ip2 (in-place)
I0519 16:18:56.165107 22864 net.cpp:150] Setting up drop2
I0519 16:18:56.165141 22864 net.cpp:157] Top shape: 100 98 (9800)
I0519 16:18:56.165154 22864 net.cpp:165] Memory required for data: 157870000
I0519 16:18:56.165176 22864 layer_factory.hpp:77] Creating layer ip3
I0519 16:18:56.165194 22864 net.cpp:106] Creating Layer ip3
I0519 16:18:56.165210 22864 net.cpp:454] ip3 <- ip2
I0519 16:18:56.165225 22864 net.cpp:411] ip3 -> ip3
I0519 16:18:56.165473 22864 net.cpp:150] Setting up ip3
I0519 16:18:56.165493 22864 net.cpp:157] Top shape: 100 11 (1100)
I0519 16:18:56.165505 22864 net.cpp:165] Memory required for data: 157874400
I0519 16:18:56.165526 22864 layer_factory.hpp:77] Creating layer drop3
I0519 16:18:56.165541 22864 net.cpp:106] Creating Layer drop3
I0519 16:18:56.165555 22864 net.cpp:454] drop3 <- ip3
I0519 16:18:56.165577 22864 net.cpp:397] drop3 -> ip3 (in-place)
I0519 16:18:56.165626 22864 net.cpp:150] Setting up drop3
I0519 16:18:56.165647 22864 net.cpp:157] Top shape: 100 11 (1100)
I0519 16:18:56.165660 22864 net.cpp:165] Memory required for data: 157878800
I0519 16:18:56.165676 22864 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0519 16:18:56.165693 22864 net.cpp:106] Creating Layer ip3_drop3_0_split
I0519 16:18:56.165704 22864 net.cpp:454] ip3_drop3_0_split <- ip3
I0519 16:18:56.165722 22864 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0519 16:18:56.165746 22864 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0519 16:18:56.165827 22864 net.cpp:150] Setting up ip3_drop3_0_split
I0519 16:18:56.165844 22864 net.cpp:157] Top shape: 100 11 (1100)
I0519 16:18:56.165869 22864 net.cpp:157] Top shape: 100 11 (1100)
I0519 16:18:56.165882 22864 net.cpp:165] Memory required for data: 157887600
I0519 16:18:56.165896 22864 layer_factory.hpp:77] Creating layer accuracy
I0519 16:18:56.165925 22864 net.cpp:106] Creating Layer accuracy
I0519 16:18:56.165940 22864 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0519 16:18:56.165956 22864 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0519 16:18:56.165973 22864 net.cpp:411] accuracy -> accuracy
I0519 16:18:56.166002 22864 net.cpp:150] Setting up accuracy
I0519 16:18:56.166016 22864 net.cpp:157] Top shape: (1)
I0519 16:18:56.166028 22864 net.cpp:165] Memory required for data: 157887604
I0519 16:18:56.166040 22864 layer_factory.hpp:77] Creating layer loss
I0519 16:18:56.166060 22864 net.cpp:106] Creating Layer loss
I0519 16:18:56.166074 22864 net.cpp:454] loss <- ip3_drop3_0_split_1
I0519 16:18:56.166086 22864 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0519 16:18:56.166101 22864 net.cpp:411] loss -> loss
I0519 16:18:56.166123 22864 layer_factory.hpp:77] Creating layer loss
I0519 16:18:56.166638 22864 net.cpp:150] Setting up loss
I0519 16:18:56.166658 22864 net.cpp:157] Top shape: (1)
I0519 16:18:56.166672 22864 net.cpp:160]     with loss weight 1
I0519 16:18:56.166698 22864 net.cpp:165] Memory required for data: 157887608
I0519 16:18:56.166712 22864 net.cpp:226] loss needs backward computation.
I0519 16:18:56.166733 22864 net.cpp:228] accuracy does not need backward computation.
I0519 16:18:56.166748 22864 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0519 16:18:56.166760 22864 net.cpp:226] drop3 needs backward computation.
I0519 16:18:56.166772 22864 net.cpp:226] ip3 needs backward computation.
I0519 16:18:56.166788 22864 net.cpp:226] drop2 needs backward computation.
I0519 16:18:56.166816 22864 net.cpp:226] relu6 needs backward computation.
I0519 16:18:56.166829 22864 net.cpp:226] ip2 needs backward computation.
I0519 16:18:56.166846 22864 net.cpp:226] drop1 needs backward computation.
I0519 16:18:56.166857 22864 net.cpp:226] relu5 needs backward computation.
I0519 16:18:56.166869 22864 net.cpp:226] ip1 needs backward computation.
I0519 16:18:56.166884 22864 net.cpp:226] pool4 needs backward computation.
I0519 16:18:56.166896 22864 net.cpp:226] relu4 needs backward computation.
I0519 16:18:56.166916 22864 net.cpp:226] conv4 needs backward computation.
I0519 16:18:56.166929 22864 net.cpp:226] pool3 needs backward computation.
I0519 16:18:56.166944 22864 net.cpp:226] relu3 needs backward computation.
I0519 16:18:56.166955 22864 net.cpp:226] conv3 needs backward computation.
I0519 16:18:56.166970 22864 net.cpp:226] pool2 needs backward computation.
I0519 16:18:56.166983 22864 net.cpp:226] relu2 needs backward computation.
I0519 16:18:56.167002 22864 net.cpp:226] conv2 needs backward computation.
I0519 16:18:56.167016 22864 net.cpp:226] pool1 needs backward computation.
I0519 16:18:56.167032 22864 net.cpp:226] relu1 needs backward computation.
I0519 16:18:56.167044 22864 net.cpp:226] conv1 needs backward computation.
I0519 16:18:56.167057 22864 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0519 16:18:56.167073 22864 net.cpp:228] data_hdf5 does not need backward computation.
I0519 16:18:56.167085 22864 net.cpp:270] This network produces output accuracy
I0519 16:18:56.167104 22864 net.cpp:270] This network produces output loss
I0519 16:18:56.167136 22864 net.cpp:283] Network initialization done.
I0519 16:18:56.167290 22864 solver.cpp:60] Solver scaffolding done.
I0519 16:18:56.168444 22864 caffe.cpp:212] Starting Optimization
I0519 16:18:56.168463 22864 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0519 16:18:56.168486 22864 solver.cpp:289] Learning Rate Policy: fixed
I0519 16:18:56.169586 22864 solver.cpp:341] Iteration 0, Testing net (#0)
I0519 16:19:44.035423 22864 solver.cpp:409]     Test net output #0: accuracy = 0.14334
I0519 16:19:44.035591 22864 solver.cpp:409]     Test net output #1: loss = 2.39743 (* 1 = 2.39743 loss)
I0519 16:19:44.068528 22864 solver.cpp:237] Iteration 0, loss = 2.39176
I0519 16:19:44.068567 22864 solver.cpp:253]     Train net output #0: loss = 2.39176 (* 1 = 2.39176 loss)
I0519 16:19:44.068589 22864 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0519 16:19:49.884699 22864 solver.cpp:237] Iteration 100, loss = 2.31569
I0519 16:19:49.884735 22864 solver.cpp:253]     Train net output #0: loss = 2.31569 (* 1 = 2.31569 loss)
I0519 16:19:49.884759 22864 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0519 16:19:55.700698 22864 solver.cpp:237] Iteration 200, loss = 2.30075
I0519 16:19:55.700736 22864 solver.cpp:253]     Train net output #0: loss = 2.30075 (* 1 = 2.30075 loss)
I0519 16:19:55.700752 22864 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0519 16:20:01.511845 22864 solver.cpp:237] Iteration 300, loss = 2.18277
I0519 16:20:01.511883 22864 solver.cpp:253]     Train net output #0: loss = 2.18277 (* 1 = 2.18277 loss)
I0519 16:20:01.511900 22864 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0519 16:20:07.327697 22864 solver.cpp:237] Iteration 400, loss = 2.1544
I0519 16:20:07.327733 22864 solver.cpp:253]     Train net output #0: loss = 2.1544 (* 1 = 2.1544 loss)
I0519 16:20:07.327751 22864 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0519 16:20:13.150080 22864 solver.cpp:237] Iteration 500, loss = 2.03994
I0519 16:20:13.150136 22864 solver.cpp:253]     Train net output #0: loss = 2.03994 (* 1 = 2.03994 loss)
I0519 16:20:13.150161 22864 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0519 16:20:18.974192 22864 solver.cpp:237] Iteration 600, loss = 1.88714
I0519 16:20:18.974342 22864 solver.cpp:253]     Train net output #0: loss = 1.88714 (* 1 = 1.88714 loss)
I0519 16:20:18.974359 22864 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0519 16:20:24.796151 22864 solver.cpp:237] Iteration 700, loss = 2.15079
I0519 16:20:24.796186 22864 solver.cpp:253]     Train net output #0: loss = 2.15079 (* 1 = 2.15079 loss)
I0519 16:20:24.796205 22864 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0519 16:20:30.622202 22864 solver.cpp:237] Iteration 800, loss = 1.78634
I0519 16:20:30.622238 22864 solver.cpp:253]     Train net output #0: loss = 1.78634 (* 1 = 1.78634 loss)
I0519 16:20:30.622262 22864 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0519 16:20:36.445622 22864 solver.cpp:237] Iteration 900, loss = 1.85154
I0519 16:20:36.445680 22864 solver.cpp:253]     Train net output #0: loss = 1.85154 (* 1 = 1.85154 loss)
I0519 16:20:36.445704 22864 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0519 16:20:42.208197 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_1000.caffemodel
I0519 16:20:42.290621 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_1000.solverstate
I0519 16:21:04.495328 22864 solver.cpp:237] Iteration 1000, loss = 1.98913
I0519 16:21:04.495502 22864 solver.cpp:253]     Train net output #0: loss = 1.98913 (* 1 = 1.98913 loss)
I0519 16:21:04.495519 22864 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0519 16:21:10.315718 22864 solver.cpp:237] Iteration 1100, loss = 1.6901
I0519 16:21:10.315757 22864 solver.cpp:253]     Train net output #0: loss = 1.6901 (* 1 = 1.6901 loss)
I0519 16:21:10.315780 22864 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0519 16:21:16.138764 22864 solver.cpp:237] Iteration 1200, loss = 1.87005
I0519 16:21:16.138799 22864 solver.cpp:253]     Train net output #0: loss = 1.87005 (* 1 = 1.87005 loss)
I0519 16:21:16.138823 22864 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0519 16:21:21.963037 22864 solver.cpp:237] Iteration 1300, loss = 1.7698
I0519 16:21:21.963073 22864 solver.cpp:253]     Train net output #0: loss = 1.7698 (* 1 = 1.7698 loss)
I0519 16:21:21.963096 22864 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0519 16:21:27.785485 22864 solver.cpp:237] Iteration 1400, loss = 1.72667
I0519 16:21:27.785542 22864 solver.cpp:253]     Train net output #0: loss = 1.72667 (* 1 = 1.72667 loss)
I0519 16:21:27.785567 22864 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0519 16:21:33.608119 22864 solver.cpp:237] Iteration 1500, loss = 1.96838
I0519 16:21:33.608160 22864 solver.cpp:253]     Train net output #0: loss = 1.96838 (* 1 = 1.96838 loss)
I0519 16:21:33.608177 22864 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0519 16:21:39.432685 22864 solver.cpp:237] Iteration 1600, loss = 1.79324
I0519 16:21:39.432852 22864 solver.cpp:253]     Train net output #0: loss = 1.79324 (* 1 = 1.79324 loss)
I0519 16:21:39.432868 22864 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0519 16:21:45.256810 22864 solver.cpp:237] Iteration 1700, loss = 1.60407
I0519 16:21:45.256846 22864 solver.cpp:253]     Train net output #0: loss = 1.60407 (* 1 = 1.60407 loss)
I0519 16:21:45.256865 22864 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0519 16:21:51.083813 22864 solver.cpp:237] Iteration 1800, loss = 1.57835
I0519 16:21:51.083868 22864 solver.cpp:253]     Train net output #0: loss = 1.57835 (* 1 = 1.57835 loss)
I0519 16:21:51.083895 22864 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0519 16:21:56.901772 22864 solver.cpp:237] Iteration 1900, loss = 1.7806
I0519 16:21:56.901809 22864 solver.cpp:253]     Train net output #0: loss = 1.7806 (* 1 = 1.7806 loss)
I0519 16:21:56.901828 22864 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0519 16:22:02.666752 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_2000.caffemodel
I0519 16:22:02.745370 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_2000.solverstate
I0519 16:22:24.907019 22864 solver.cpp:237] Iteration 2000, loss = 1.77075
I0519 16:22:24.907189 22864 solver.cpp:253]     Train net output #0: loss = 1.77075 (* 1 = 1.77075 loss)
I0519 16:22:24.907207 22864 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0519 16:22:30.724099 22864 solver.cpp:237] Iteration 2100, loss = 1.56336
I0519 16:22:30.724135 22864 solver.cpp:253]     Train net output #0: loss = 1.56336 (* 1 = 1.56336 loss)
I0519 16:22:30.724159 22864 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0519 16:22:36.551007 22864 solver.cpp:237] Iteration 2200, loss = 1.69133
I0519 16:22:36.551043 22864 solver.cpp:253]     Train net output #0: loss = 1.69133 (* 1 = 1.69133 loss)
I0519 16:22:36.551065 22864 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0519 16:22:42.372196 22864 solver.cpp:237] Iteration 2300, loss = 1.68979
I0519 16:22:42.372254 22864 solver.cpp:253]     Train net output #0: loss = 1.68979 (* 1 = 1.68979 loss)
I0519 16:22:42.372279 22864 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0519 16:22:48.195435 22864 solver.cpp:237] Iteration 2400, loss = 1.6656
I0519 16:22:48.195469 22864 solver.cpp:253]     Train net output #0: loss = 1.6656 (* 1 = 1.6656 loss)
I0519 16:22:48.195488 22864 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0519 16:22:54.018363 22864 solver.cpp:237] Iteration 2500, loss = 1.56179
I0519 16:22:54.018399 22864 solver.cpp:253]     Train net output #0: loss = 1.56179 (* 1 = 1.56179 loss)
I0519 16:22:54.018421 22864 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0519 16:22:59.844844 22864 solver.cpp:237] Iteration 2600, loss = 1.61721
I0519 16:22:59.845001 22864 solver.cpp:253]     Train net output #0: loss = 1.61721 (* 1 = 1.61721 loss)
I0519 16:22:59.845018 22864 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0519 16:23:05.672691 22864 solver.cpp:237] Iteration 2700, loss = 1.6398
I0519 16:23:05.672749 22864 solver.cpp:253]     Train net output #0: loss = 1.6398 (* 1 = 1.6398 loss)
I0519 16:23:05.672767 22864 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0519 16:23:11.498625 22864 solver.cpp:237] Iteration 2800, loss = 1.56414
I0519 16:23:11.498662 22864 solver.cpp:253]     Train net output #0: loss = 1.56414 (* 1 = 1.56414 loss)
I0519 16:23:11.498679 22864 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0519 16:23:17.323114 22864 solver.cpp:237] Iteration 2900, loss = 1.56224
I0519 16:23:17.323149 22864 solver.cpp:253]     Train net output #0: loss = 1.56224 (* 1 = 1.56224 loss)
I0519 16:23:17.323168 22864 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0519 16:23:23.088649 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_3000.caffemodel
I0519 16:23:23.166918 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_3000.solverstate
I0519 16:23:23.197808 22864 solver.cpp:341] Iteration 3000, Testing net (#0)
I0519 16:24:10.151739 22864 solver.cpp:409]     Test net output #0: accuracy = 0.688233
I0519 16:24:10.151909 22864 solver.cpp:409]     Test net output #1: loss = 1.07462 (* 1 = 1.07462 loss)
I0519 16:24:32.362923 22864 solver.cpp:237] Iteration 3000, loss = 1.71371
I0519 16:24:32.362988 22864 solver.cpp:253]     Train net output #0: loss = 1.71371 (* 1 = 1.71371 loss)
I0519 16:24:32.363014 22864 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0519 16:24:38.177413 22864 solver.cpp:237] Iteration 3100, loss = 1.64086
I0519 16:24:38.177449 22864 solver.cpp:253]     Train net output #0: loss = 1.64086 (* 1 = 1.64086 loss)
I0519 16:24:38.177469 22864 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0519 16:24:44.000680 22864 solver.cpp:237] Iteration 3200, loss = 1.77285
I0519 16:24:44.000846 22864 solver.cpp:253]     Train net output #0: loss = 1.77285 (* 1 = 1.77285 loss)
I0519 16:24:44.000865 22864 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0519 16:24:49.820358 22864 solver.cpp:237] Iteration 3300, loss = 1.80973
I0519 16:24:49.820395 22864 solver.cpp:253]     Train net output #0: loss = 1.80973 (* 1 = 1.80973 loss)
I0519 16:24:49.820415 22864 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0519 16:24:55.639821 22864 solver.cpp:237] Iteration 3400, loss = 1.65484
I0519 16:24:55.639856 22864 solver.cpp:253]     Train net output #0: loss = 1.65484 (* 1 = 1.65484 loss)
I0519 16:24:55.639874 22864 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0519 16:25:01.457098 22864 solver.cpp:237] Iteration 3500, loss = 1.61408
I0519 16:25:01.457134 22864 solver.cpp:253]     Train net output #0: loss = 1.61408 (* 1 = 1.61408 loss)
I0519 16:25:01.457154 22864 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0519 16:25:07.279270 22864 solver.cpp:237] Iteration 3600, loss = 1.76036
I0519 16:25:07.279305 22864 solver.cpp:253]     Train net output #0: loss = 1.76036 (* 1 = 1.76036 loss)
I0519 16:25:07.279325 22864 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0519 16:25:13.102036 22864 solver.cpp:237] Iteration 3700, loss = 1.58681
I0519 16:25:13.102093 22864 solver.cpp:253]     Train net output #0: loss = 1.58681 (* 1 = 1.58681 loss)
I0519 16:25:13.102118 22864 sgd_solver.cpp:106] Iteration 3700, lr = 0.0025
I0519 16:25:18.919005 22864 solver.cpp:237] Iteration 3800, loss = 1.46872
I0519 16:25:18.919164 22864 solver.cpp:253]     Train net output #0: loss = 1.46872 (* 1 = 1.46872 loss)
I0519 16:25:18.919183 22864 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0519 16:25:24.742697 22864 solver.cpp:237] Iteration 3900, loss = 1.36218
I0519 16:25:24.742732 22864 solver.cpp:253]     Train net output #0: loss = 1.36218 (* 1 = 1.36218 loss)
I0519 16:25:24.742750 22864 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0519 16:25:30.502394 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_4000.caffemodel
I0519 16:25:30.583875 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_4000.solverstate
I0519 16:25:52.792915 22864 solver.cpp:237] Iteration 4000, loss = 1.59038
I0519 16:25:52.793090 22864 solver.cpp:253]     Train net output #0: loss = 1.59038 (* 1 = 1.59038 loss)
I0519 16:25:52.793108 22864 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0519 16:25:58.613016 22864 solver.cpp:237] Iteration 4100, loss = 1.71351
I0519 16:25:58.613070 22864 solver.cpp:253]     Train net output #0: loss = 1.71351 (* 1 = 1.71351 loss)
I0519 16:25:58.613090 22864 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0519 16:26:04.435014 22864 solver.cpp:237] Iteration 4200, loss = 1.36681
I0519 16:26:04.435051 22864 solver.cpp:253]     Train net output #0: loss = 1.36681 (* 1 = 1.36681 loss)
I0519 16:26:04.435075 22864 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0519 16:26:10.254111 22864 solver.cpp:237] Iteration 4300, loss = 1.53797
I0519 16:26:10.254147 22864 solver.cpp:253]     Train net output #0: loss = 1.53797 (* 1 = 1.53797 loss)
I0519 16:26:10.254169 22864 sgd_solver.cpp:106] Iteration 4300, lr = 0.0025
I0519 16:26:16.072166 22864 solver.cpp:237] Iteration 4400, loss = 1.47347
I0519 16:26:16.072202 22864 solver.cpp:253]     Train net output #0: loss = 1.47347 (* 1 = 1.47347 loss)
I0519 16:26:16.072221 22864 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0519 16:26:21.892891 22864 solver.cpp:237] Iteration 4500, loss = 1.47531
I0519 16:26:21.892928 22864 solver.cpp:253]     Train net output #0: loss = 1.47531 (* 1 = 1.47531 loss)
I0519 16:26:21.892947 22864 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0519 16:26:27.716629 22864 solver.cpp:237] Iteration 4600, loss = 1.5428
I0519 16:26:27.716801 22864 solver.cpp:253]     Train net output #0: loss = 1.5428 (* 1 = 1.5428 loss)
I0519 16:26:27.716820 22864 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0519 16:26:33.535135 22864 solver.cpp:237] Iteration 4700, loss = 1.43947
I0519 16:26:33.535172 22864 solver.cpp:253]     Train net output #0: loss = 1.43947 (* 1 = 1.43947 loss)
I0519 16:26:33.535190 22864 sgd_solver.cpp:106] Iteration 4700, lr = 0.0025
I0519 16:26:39.356673 22864 solver.cpp:237] Iteration 4800, loss = 1.64642
I0519 16:26:39.356708 22864 solver.cpp:253]     Train net output #0: loss = 1.64642 (* 1 = 1.64642 loss)
I0519 16:26:39.356724 22864 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0519 16:26:45.179882 22864 solver.cpp:237] Iteration 4900, loss = 1.35195
I0519 16:26:45.179916 22864 solver.cpp:253]     Train net output #0: loss = 1.35195 (* 1 = 1.35195 loss)
I0519 16:26:45.179935 22864 sgd_solver.cpp:106] Iteration 4900, lr = 0.0025
I0519 16:26:50.941205 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_5000.caffemodel
I0519 16:26:51.021400 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_5000.solverstate
I0519 16:27:13.270810 22864 solver.cpp:237] Iteration 5000, loss = 1.30987
I0519 16:27:13.270992 22864 solver.cpp:253]     Train net output #0: loss = 1.30987 (* 1 = 1.30987 loss)
I0519 16:27:13.271009 22864 sgd_solver.cpp:106] Iteration 5000, lr = 0.0025
I0519 16:27:19.091593 22864 solver.cpp:237] Iteration 5100, loss = 1.51195
I0519 16:27:19.091650 22864 solver.cpp:253]     Train net output #0: loss = 1.51195 (* 1 = 1.51195 loss)
I0519 16:27:19.091673 22864 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0519 16:27:24.913245 22864 solver.cpp:237] Iteration 5200, loss = 1.5202
I0519 16:27:24.913280 22864 solver.cpp:253]     Train net output #0: loss = 1.5202 (* 1 = 1.5202 loss)
I0519 16:27:24.913300 22864 sgd_solver.cpp:106] Iteration 5200, lr = 0.0025
I0519 16:27:30.731508 22864 solver.cpp:237] Iteration 5300, loss = 1.58625
I0519 16:27:30.731545 22864 solver.cpp:253]     Train net output #0: loss = 1.58625 (* 1 = 1.58625 loss)
I0519 16:27:30.731564 22864 sgd_solver.cpp:106] Iteration 5300, lr = 0.0025
I0519 16:27:36.541688 22864 solver.cpp:237] Iteration 5400, loss = 1.50119
I0519 16:27:36.541723 22864 solver.cpp:253]     Train net output #0: loss = 1.50119 (* 1 = 1.50119 loss)
I0519 16:27:36.541741 22864 sgd_solver.cpp:106] Iteration 5400, lr = 0.0025
I0519 16:27:42.363673 22864 solver.cpp:237] Iteration 5500, loss = 1.5187
I0519 16:27:42.363730 22864 solver.cpp:253]     Train net output #0: loss = 1.5187 (* 1 = 1.5187 loss)
I0519 16:27:42.363755 22864 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0519 16:27:48.179831 22864 solver.cpp:237] Iteration 5600, loss = 1.48474
I0519 16:27:48.179987 22864 solver.cpp:253]     Train net output #0: loss = 1.48474 (* 1 = 1.48474 loss)
I0519 16:27:48.180004 22864 sgd_solver.cpp:106] Iteration 5600, lr = 0.0025
I0519 16:27:54.001551 22864 solver.cpp:237] Iteration 5700, loss = 1.40824
I0519 16:27:54.001587 22864 solver.cpp:253]     Train net output #0: loss = 1.40824 (* 1 = 1.40824 loss)
I0519 16:27:54.001606 22864 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0519 16:27:59.822463 22864 solver.cpp:237] Iteration 5800, loss = 1.57688
I0519 16:27:59.822499 22864 solver.cpp:253]     Train net output #0: loss = 1.57688 (* 1 = 1.57688 loss)
I0519 16:27:59.822517 22864 sgd_solver.cpp:106] Iteration 5800, lr = 0.0025
I0519 16:28:05.640595 22864 solver.cpp:237] Iteration 5900, loss = 1.46882
I0519 16:28:05.640652 22864 solver.cpp:253]     Train net output #0: loss = 1.46882 (* 1 = 1.46882 loss)
I0519 16:28:05.640678 22864 sgd_solver.cpp:106] Iteration 5900, lr = 0.0025
I0519 16:28:11.398757 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_6000.caffemodel
I0519 16:28:11.479733 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_6000.solverstate
I0519 16:28:11.508260 22864 solver.cpp:341] Iteration 6000, Testing net (#0)
I0519 16:29:19.348326 22864 solver.cpp:409]     Test net output #0: accuracy = 0.786835
I0519 16:29:19.348500 22864 solver.cpp:409]     Test net output #1: loss = 0.763379 (* 1 = 0.763379 loss)
I0519 16:29:41.513200 22864 solver.cpp:237] Iteration 6000, loss = 1.41973
I0519 16:29:41.513265 22864 solver.cpp:253]     Train net output #0: loss = 1.41973 (* 1 = 1.41973 loss)
I0519 16:29:41.513294 22864 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0519 16:29:47.337855 22864 solver.cpp:237] Iteration 6100, loss = 1.24698
I0519 16:29:47.337910 22864 solver.cpp:253]     Train net output #0: loss = 1.24698 (* 1 = 1.24698 loss)
I0519 16:29:47.337939 22864 sgd_solver.cpp:106] Iteration 6100, lr = 0.0025
I0519 16:29:53.162514 22864 solver.cpp:237] Iteration 6200, loss = 1.50578
I0519 16:29:53.162669 22864 solver.cpp:253]     Train net output #0: loss = 1.50578 (* 1 = 1.50578 loss)
I0519 16:29:53.162686 22864 sgd_solver.cpp:106] Iteration 6200, lr = 0.0025
I0519 16:29:58.985774 22864 solver.cpp:237] Iteration 6300, loss = 1.58898
I0519 16:29:58.985811 22864 solver.cpp:253]     Train net output #0: loss = 1.58898 (* 1 = 1.58898 loss)
I0519 16:29:58.985828 22864 sgd_solver.cpp:106] Iteration 6300, lr = 0.0025
I0519 16:30:04.806668 22864 solver.cpp:237] Iteration 6400, loss = 1.74574
I0519 16:30:04.806702 22864 solver.cpp:253]     Train net output #0: loss = 1.74574 (* 1 = 1.74574 loss)
I0519 16:30:04.806726 22864 sgd_solver.cpp:106] Iteration 6400, lr = 0.0025
I0519 16:30:10.625505 22864 solver.cpp:237] Iteration 6500, loss = 1.50835
I0519 16:30:10.625560 22864 solver.cpp:253]     Train net output #0: loss = 1.50835 (* 1 = 1.50835 loss)
I0519 16:30:10.625588 22864 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0519 16:30:16.452343 22864 solver.cpp:237] Iteration 6600, loss = 1.53552
I0519 16:30:16.452380 22864 solver.cpp:253]     Train net output #0: loss = 1.53552 (* 1 = 1.53552 loss)
I0519 16:30:16.452397 22864 sgd_solver.cpp:106] Iteration 6600, lr = 0.0025
I0519 16:30:22.281235 22864 solver.cpp:237] Iteration 6700, loss = 1.49527
I0519 16:30:22.281270 22864 solver.cpp:253]     Train net output #0: loss = 1.49527 (* 1 = 1.49527 loss)
I0519 16:30:22.281289 22864 sgd_solver.cpp:106] Iteration 6700, lr = 0.0025
I0519 16:30:28.101735 22864 solver.cpp:237] Iteration 6800, loss = 1.51927
I0519 16:30:28.101892 22864 solver.cpp:253]     Train net output #0: loss = 1.51927 (* 1 = 1.51927 loss)
I0519 16:30:28.101909 22864 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0519 16:30:33.922904 22864 solver.cpp:237] Iteration 6900, loss = 1.31118
I0519 16:30:33.922961 22864 solver.cpp:253]     Train net output #0: loss = 1.31118 (* 1 = 1.31118 loss)
I0519 16:30:33.922989 22864 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0519 16:30:39.689378 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_7000.caffemodel
I0519 16:30:39.771080 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_7000.solverstate
I0519 16:31:01.966074 22864 solver.cpp:237] Iteration 7000, loss = 1.31537
I0519 16:31:01.966265 22864 solver.cpp:253]     Train net output #0: loss = 1.31537 (* 1 = 1.31537 loss)
I0519 16:31:01.966284 22864 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0519 16:31:07.786834 22864 solver.cpp:237] Iteration 7100, loss = 1.44403
I0519 16:31:07.786870 22864 solver.cpp:253]     Train net output #0: loss = 1.44403 (* 1 = 1.44403 loss)
I0519 16:31:07.786887 22864 sgd_solver.cpp:106] Iteration 7100, lr = 0.0025
I0519 16:31:13.612229 22864 solver.cpp:237] Iteration 7200, loss = 1.25243
I0519 16:31:13.612265 22864 solver.cpp:253]     Train net output #0: loss = 1.25243 (* 1 = 1.25243 loss)
I0519 16:31:13.612289 22864 sgd_solver.cpp:106] Iteration 7200, lr = 0.0025
I0519 16:31:19.438591 22864 solver.cpp:237] Iteration 7300, loss = 1.21528
I0519 16:31:19.438627 22864 solver.cpp:253]     Train net output #0: loss = 1.21528 (* 1 = 1.21528 loss)
I0519 16:31:19.438645 22864 sgd_solver.cpp:106] Iteration 7300, lr = 0.0025
I0519 16:31:25.265202 22864 solver.cpp:237] Iteration 7400, loss = 1.2184
I0519 16:31:25.265259 22864 solver.cpp:253]     Train net output #0: loss = 1.2184 (* 1 = 1.2184 loss)
I0519 16:31:25.265283 22864 sgd_solver.cpp:106] Iteration 7400, lr = 0.0025
I0519 16:31:31.086951 22864 solver.cpp:237] Iteration 7500, loss = 1.42742
I0519 16:31:31.086987 22864 solver.cpp:253]     Train net output #0: loss = 1.42742 (* 1 = 1.42742 loss)
I0519 16:31:31.087007 22864 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0519 16:31:36.909909 22864 solver.cpp:237] Iteration 7600, loss = 1.35337
I0519 16:31:36.910053 22864 solver.cpp:253]     Train net output #0: loss = 1.35337 (* 1 = 1.35337 loss)
I0519 16:31:36.910070 22864 sgd_solver.cpp:106] Iteration 7600, lr = 0.0025
I0519 16:31:42.731696 22864 solver.cpp:237] Iteration 7700, loss = 1.37465
I0519 16:31:42.731734 22864 solver.cpp:253]     Train net output #0: loss = 1.37465 (* 1 = 1.37465 loss)
I0519 16:31:42.731751 22864 sgd_solver.cpp:106] Iteration 7700, lr = 0.0025
I0519 16:31:48.553138 22864 solver.cpp:237] Iteration 7800, loss = 1.44683
I0519 16:31:48.553194 22864 solver.cpp:253]     Train net output #0: loss = 1.44683 (* 1 = 1.44683 loss)
I0519 16:31:48.553213 22864 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0519 16:31:54.376612 22864 solver.cpp:237] Iteration 7900, loss = 1.50421
I0519 16:31:54.376648 22864 solver.cpp:253]     Train net output #0: loss = 1.50421 (* 1 = 1.50421 loss)
I0519 16:31:54.376672 22864 sgd_solver.cpp:106] Iteration 7900, lr = 0.0025
I0519 16:32:00.145064 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_8000.caffemodel
I0519 16:32:00.225147 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_8000.solverstate
I0519 16:32:22.446543 22864 solver.cpp:237] Iteration 8000, loss = 1.28393
I0519 16:32:22.446715 22864 solver.cpp:253]     Train net output #0: loss = 1.28393 (* 1 = 1.28393 loss)
I0519 16:32:22.446733 22864 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0519 16:32:28.272022 22864 solver.cpp:237] Iteration 8100, loss = 1.45334
I0519 16:32:28.272059 22864 solver.cpp:253]     Train net output #0: loss = 1.45334 (* 1 = 1.45334 loss)
I0519 16:32:28.272078 22864 sgd_solver.cpp:106] Iteration 8100, lr = 0.0025
I0519 16:32:34.099274 22864 solver.cpp:237] Iteration 8200, loss = 1.57021
I0519 16:32:34.099310 22864 solver.cpp:253]     Train net output #0: loss = 1.57021 (* 1 = 1.57021 loss)
I0519 16:32:34.099336 22864 sgd_solver.cpp:106] Iteration 8200, lr = 0.0025
I0519 16:32:39.925438 22864 solver.cpp:237] Iteration 8300, loss = 1.42081
I0519 16:32:39.925495 22864 solver.cpp:253]     Train net output #0: loss = 1.42081 (* 1 = 1.42081 loss)
I0519 16:32:39.925521 22864 sgd_solver.cpp:106] Iteration 8300, lr = 0.0025
I0519 16:32:45.747683 22864 solver.cpp:237] Iteration 8400, loss = 1.15454
I0519 16:32:45.747719 22864 solver.cpp:253]     Train net output #0: loss = 1.15454 (* 1 = 1.15454 loss)
I0519 16:32:45.747737 22864 sgd_solver.cpp:106] Iteration 8400, lr = 0.0025
I0519 16:32:51.569869 22864 solver.cpp:237] Iteration 8500, loss = 1.45246
I0519 16:32:51.569905 22864 solver.cpp:253]     Train net output #0: loss = 1.45246 (* 1 = 1.45246 loss)
I0519 16:32:51.569924 22864 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0519 16:32:57.395740 22864 solver.cpp:237] Iteration 8600, loss = 1.37172
I0519 16:32:57.395898 22864 solver.cpp:253]     Train net output #0: loss = 1.37172 (* 1 = 1.37172 loss)
I0519 16:32:57.395915 22864 sgd_solver.cpp:106] Iteration 8600, lr = 0.0025
I0519 16:33:03.211705 22864 solver.cpp:237] Iteration 8700, loss = 1.25327
I0519 16:33:03.211741 22864 solver.cpp:253]     Train net output #0: loss = 1.25327 (* 1 = 1.25327 loss)
I0519 16:33:03.211760 22864 sgd_solver.cpp:106] Iteration 8700, lr = 0.0025
I0519 16:33:09.036610 22864 solver.cpp:237] Iteration 8800, loss = 1.29611
I0519 16:33:09.036658 22864 solver.cpp:253]     Train net output #0: loss = 1.29611 (* 1 = 1.29611 loss)
I0519 16:33:09.036676 22864 sgd_solver.cpp:106] Iteration 8800, lr = 0.0025
I0519 16:33:14.861644 22864 solver.cpp:237] Iteration 8900, loss = 1.367
I0519 16:33:14.861680 22864 solver.cpp:253]     Train net output #0: loss = 1.367 (* 1 = 1.367 loss)
I0519 16:33:14.861703 22864 sgd_solver.cpp:106] Iteration 8900, lr = 0.0025
I0519 16:33:20.627835 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_9000.caffemodel
I0519 16:33:20.706769 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_9000.solverstate
I0519 16:33:20.733400 22864 solver.cpp:341] Iteration 9000, Testing net (#0)
I0519 16:34:07.322384 22864 solver.cpp:409]     Test net output #0: accuracy = 0.821386
I0519 16:34:07.322558 22864 solver.cpp:409]     Test net output #1: loss = 0.600703 (* 1 = 0.600703 loss)
I0519 16:34:29.486336 22864 solver.cpp:237] Iteration 9000, loss = 1.3024
I0519 16:34:29.486402 22864 solver.cpp:253]     Train net output #0: loss = 1.3024 (* 1 = 1.3024 loss)
I0519 16:34:29.486428 22864 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0519 16:34:35.317729 22864 solver.cpp:237] Iteration 9100, loss = 1.26355
I0519 16:34:35.317766 22864 solver.cpp:253]     Train net output #0: loss = 1.26355 (* 1 = 1.26355 loss)
I0519 16:34:35.317790 22864 sgd_solver.cpp:106] Iteration 9100, lr = 0.0025
I0519 16:34:41.149224 22864 solver.cpp:237] Iteration 9200, loss = 1.33074
I0519 16:34:41.149396 22864 solver.cpp:253]     Train net output #0: loss = 1.33074 (* 1 = 1.33074 loss)
I0519 16:34:41.149413 22864 sgd_solver.cpp:106] Iteration 9200, lr = 0.0025
I0519 16:34:46.978806 22864 solver.cpp:237] Iteration 9300, loss = 1.2939
I0519 16:34:46.978859 22864 solver.cpp:253]     Train net output #0: loss = 1.2939 (* 1 = 1.2939 loss)
I0519 16:34:46.978878 22864 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0519 16:34:52.806006 22864 solver.cpp:237] Iteration 9400, loss = 1.39112
I0519 16:34:52.806042 22864 solver.cpp:253]     Train net output #0: loss = 1.39112 (* 1 = 1.39112 loss)
I0519 16:34:52.806066 22864 sgd_solver.cpp:106] Iteration 9400, lr = 0.0025
I0519 16:34:58.630833 22864 solver.cpp:237] Iteration 9500, loss = 1.29092
I0519 16:34:58.630868 22864 solver.cpp:253]     Train net output #0: loss = 1.29092 (* 1 = 1.29092 loss)
I0519 16:34:58.630887 22864 sgd_solver.cpp:106] Iteration 9500, lr = 0.0025
I0519 16:35:04.461357 22864 solver.cpp:237] Iteration 9600, loss = 1.44568
I0519 16:35:04.461393 22864 solver.cpp:253]     Train net output #0: loss = 1.44568 (* 1 = 1.44568 loss)
I0519 16:35:04.461411 22864 sgd_solver.cpp:106] Iteration 9600, lr = 0.0025
I0519 16:35:10.292045 22864 solver.cpp:237] Iteration 9700, loss = 1.34324
I0519 16:35:10.292096 22864 solver.cpp:253]     Train net output #0: loss = 1.34324 (* 1 = 1.34324 loss)
I0519 16:35:10.292124 22864 sgd_solver.cpp:106] Iteration 9700, lr = 0.0025
I0519 16:35:16.121042 22864 solver.cpp:237] Iteration 9800, loss = 1.40008
I0519 16:35:16.121201 22864 solver.cpp:253]     Train net output #0: loss = 1.40008 (* 1 = 1.40008 loss)
I0519 16:35:16.121218 22864 sgd_solver.cpp:106] Iteration 9800, lr = 0.0025
I0519 16:35:21.953040 22864 solver.cpp:237] Iteration 9900, loss = 1.3146
I0519 16:35:21.953076 22864 solver.cpp:253]     Train net output #0: loss = 1.3146 (* 1 = 1.3146 loss)
I0519 16:35:21.953095 22864 sgd_solver.cpp:106] Iteration 9900, lr = 0.0025
I0519 16:35:27.725786 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_10000.caffemodel
I0519 16:35:27.804229 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_10000.solverstate
I0519 16:35:50.015964 22864 solver.cpp:237] Iteration 10000, loss = 1.39654
I0519 16:35:50.016150 22864 solver.cpp:253]     Train net output #0: loss = 1.39654 (* 1 = 1.39654 loss)
I0519 16:35:50.016168 22864 sgd_solver.cpp:106] Iteration 10000, lr = 0.0025
I0519 16:35:55.844140 22864 solver.cpp:237] Iteration 10100, loss = 1.19344
I0519 16:35:55.844175 22864 solver.cpp:253]     Train net output #0: loss = 1.19344 (* 1 = 1.19344 loss)
I0519 16:35:55.844199 22864 sgd_solver.cpp:106] Iteration 10100, lr = 0.0025
I0519 16:36:01.675061 22864 solver.cpp:237] Iteration 10200, loss = 1.28446
I0519 16:36:01.675120 22864 solver.cpp:253]     Train net output #0: loss = 1.28446 (* 1 = 1.28446 loss)
I0519 16:36:01.675145 22864 sgd_solver.cpp:106] Iteration 10200, lr = 0.0025
I0519 16:36:07.506116 22864 solver.cpp:237] Iteration 10300, loss = 1.18719
I0519 16:36:07.506153 22864 solver.cpp:253]     Train net output #0: loss = 1.18719 (* 1 = 1.18719 loss)
I0519 16:36:07.506172 22864 sgd_solver.cpp:106] Iteration 10300, lr = 0.0025
I0519 16:36:13.336340 22864 solver.cpp:237] Iteration 10400, loss = 1.37181
I0519 16:36:13.336377 22864 solver.cpp:253]     Train net output #0: loss = 1.37181 (* 1 = 1.37181 loss)
I0519 16:36:13.336396 22864 sgd_solver.cpp:106] Iteration 10400, lr = 0.0025
I0519 16:36:19.167512 22864 solver.cpp:237] Iteration 10500, loss = 1.40029
I0519 16:36:19.167551 22864 solver.cpp:253]     Train net output #0: loss = 1.40029 (* 1 = 1.40029 loss)
I0519 16:36:19.167567 22864 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0519 16:36:25.000854 22864 solver.cpp:237] Iteration 10600, loss = 1.24256
I0519 16:36:25.001021 22864 solver.cpp:253]     Train net output #0: loss = 1.24256 (* 1 = 1.24256 loss)
I0519 16:36:25.001047 22864 sgd_solver.cpp:106] Iteration 10600, lr = 0.0025
I0519 16:36:30.828943 22864 solver.cpp:237] Iteration 10700, loss = 1.2123
I0519 16:36:30.828979 22864 solver.cpp:253]     Train net output #0: loss = 1.2123 (* 1 = 1.2123 loss)
I0519 16:36:30.829001 22864 sgd_solver.cpp:106] Iteration 10700, lr = 0.0025
I0519 16:36:36.658406 22864 solver.cpp:237] Iteration 10800, loss = 1.19681
I0519 16:36:36.658442 22864 solver.cpp:253]     Train net output #0: loss = 1.19681 (* 1 = 1.19681 loss)
I0519 16:36:36.658462 22864 sgd_solver.cpp:106] Iteration 10800, lr = 0.0025
I0519 16:36:42.483760 22864 solver.cpp:237] Iteration 10900, loss = 1.33759
I0519 16:36:42.483798 22864 solver.cpp:253]     Train net output #0: loss = 1.33759 (* 1 = 1.33759 loss)
I0519 16:36:42.483821 22864 sgd_solver.cpp:106] Iteration 10900, lr = 0.0025
I0519 16:36:48.257100 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_11000.caffemodel
I0519 16:36:48.336197 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_11000.solverstate
I0519 16:37:10.535320 22864 solver.cpp:237] Iteration 11000, loss = 1.29737
I0519 16:37:10.535507 22864 solver.cpp:253]     Train net output #0: loss = 1.29737 (* 1 = 1.29737 loss)
I0519 16:37:10.535526 22864 sgd_solver.cpp:106] Iteration 11000, lr = 0.0025
I0519 16:37:16.365993 22864 solver.cpp:237] Iteration 11100, loss = 1.20328
I0519 16:37:16.366050 22864 solver.cpp:253]     Train net output #0: loss = 1.20328 (* 1 = 1.20328 loss)
I0519 16:37:16.366076 22864 sgd_solver.cpp:106] Iteration 11100, lr = 0.0025
I0519 16:37:22.196316 22864 solver.cpp:237] Iteration 11200, loss = 1.26546
I0519 16:37:22.196352 22864 solver.cpp:253]     Train net output #0: loss = 1.26546 (* 1 = 1.26546 loss)
I0519 16:37:22.196375 22864 sgd_solver.cpp:106] Iteration 11200, lr = 0.0025
I0519 16:37:28.031289 22864 solver.cpp:237] Iteration 11300, loss = 1.38639
I0519 16:37:28.031325 22864 solver.cpp:253]     Train net output #0: loss = 1.38639 (* 1 = 1.38639 loss)
I0519 16:37:28.031345 22864 sgd_solver.cpp:106] Iteration 11300, lr = 0.0025
I0519 16:37:33.858093 22864 solver.cpp:237] Iteration 11400, loss = 1.27732
I0519 16:37:33.858129 22864 solver.cpp:253]     Train net output #0: loss = 1.27732 (* 1 = 1.27732 loss)
I0519 16:37:33.858152 22864 sgd_solver.cpp:106] Iteration 11400, lr = 0.0025
I0519 16:37:39.686081 22864 solver.cpp:237] Iteration 11500, loss = 1.61507
I0519 16:37:39.686137 22864 solver.cpp:253]     Train net output #0: loss = 1.61507 (* 1 = 1.61507 loss)
I0519 16:37:39.686166 22864 sgd_solver.cpp:106] Iteration 11500, lr = 0.0025
I0519 16:37:45.518008 22864 solver.cpp:237] Iteration 11600, loss = 1.38669
I0519 16:37:45.518169 22864 solver.cpp:253]     Train net output #0: loss = 1.38669 (* 1 = 1.38669 loss)
I0519 16:37:45.518187 22864 sgd_solver.cpp:106] Iteration 11600, lr = 0.0025
I0519 16:37:51.344460 22864 solver.cpp:237] Iteration 11700, loss = 1.23346
I0519 16:37:51.344501 22864 solver.cpp:253]     Train net output #0: loss = 1.23346 (* 1 = 1.23346 loss)
I0519 16:37:51.344519 22864 sgd_solver.cpp:106] Iteration 11700, lr = 0.0025
I0519 16:37:57.176121 22864 solver.cpp:237] Iteration 11800, loss = 1.18381
I0519 16:37:57.176156 22864 solver.cpp:253]     Train net output #0: loss = 1.18381 (* 1 = 1.18381 loss)
I0519 16:37:57.176174 22864 sgd_solver.cpp:106] Iteration 11800, lr = 0.0025
I0519 16:38:03.008627 22864 solver.cpp:237] Iteration 11900, loss = 1.29971
I0519 16:38:03.008663 22864 solver.cpp:253]     Train net output #0: loss = 1.29971 (* 1 = 1.29971 loss)
I0519 16:38:03.008682 22864 sgd_solver.cpp:106] Iteration 11900, lr = 0.0025
I0519 16:38:08.777523 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_12000.caffemodel
I0519 16:38:08.856314 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_12000.solverstate
I0519 16:38:08.881667 22864 solver.cpp:341] Iteration 12000, Testing net (#0)
I0519 16:39:16.569465 22864 solver.cpp:409]     Test net output #0: accuracy = 0.838873
I0519 16:39:16.569641 22864 solver.cpp:409]     Test net output #1: loss = 0.551245 (* 1 = 0.551245 loss)
I0519 16:39:38.803715 22864 solver.cpp:237] Iteration 12000, loss = 1.33646
I0519 16:39:38.803781 22864 solver.cpp:253]     Train net output #0: loss = 1.33646 (* 1 = 1.33646 loss)
I0519 16:39:38.803800 22864 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0519 16:39:44.632333 22864 solver.cpp:237] Iteration 12100, loss = 1.33068
I0519 16:39:44.632390 22864 solver.cpp:253]     Train net output #0: loss = 1.33068 (* 1 = 1.33068 loss)
I0519 16:39:44.632406 22864 sgd_solver.cpp:106] Iteration 12100, lr = 0.0025
I0519 16:39:50.465528 22864 solver.cpp:237] Iteration 12200, loss = 1.3516
I0519 16:39:50.465715 22864 solver.cpp:253]     Train net output #0: loss = 1.3516 (* 1 = 1.3516 loss)
I0519 16:39:50.465733 22864 sgd_solver.cpp:106] Iteration 12200, lr = 0.0025
I0519 16:39:56.298409 22864 solver.cpp:237] Iteration 12300, loss = 1.21613
I0519 16:39:56.298445 22864 solver.cpp:253]     Train net output #0: loss = 1.21613 (* 1 = 1.21613 loss)
I0519 16:39:56.298463 22864 sgd_solver.cpp:106] Iteration 12300, lr = 0.0025
I0519 16:40:02.126037 22864 solver.cpp:237] Iteration 12400, loss = 1.42657
I0519 16:40:02.126073 22864 solver.cpp:253]     Train net output #0: loss = 1.42657 (* 1 = 1.42657 loss)
I0519 16:40:02.126091 22864 sgd_solver.cpp:106] Iteration 12400, lr = 0.0025
I0519 16:40:07.953598 22864 solver.cpp:237] Iteration 12500, loss = 1.19557
I0519 16:40:07.953632 22864 solver.cpp:253]     Train net output #0: loss = 1.19557 (* 1 = 1.19557 loss)
I0519 16:40:07.953650 22864 sgd_solver.cpp:106] Iteration 12500, lr = 0.0025
I0519 16:40:13.784684 22864 solver.cpp:237] Iteration 12600, loss = 1.29283
I0519 16:40:13.784737 22864 solver.cpp:253]     Train net output #0: loss = 1.29283 (* 1 = 1.29283 loss)
I0519 16:40:13.784754 22864 sgd_solver.cpp:106] Iteration 12600, lr = 0.0025
I0519 16:40:19.612838 22864 solver.cpp:237] Iteration 12700, loss = 1.43394
I0519 16:40:19.612874 22864 solver.cpp:253]     Train net output #0: loss = 1.43394 (* 1 = 1.43394 loss)
I0519 16:40:19.612891 22864 sgd_solver.cpp:106] Iteration 12700, lr = 0.0025
I0519 16:40:25.435580 22864 solver.cpp:237] Iteration 12800, loss = 1.31932
I0519 16:40:25.435734 22864 solver.cpp:253]     Train net output #0: loss = 1.31932 (* 1 = 1.31932 loss)
I0519 16:40:25.435750 22864 sgd_solver.cpp:106] Iteration 12800, lr = 0.0025
I0519 16:40:31.264612 22864 solver.cpp:237] Iteration 12900, loss = 1.24275
I0519 16:40:31.264648 22864 solver.cpp:253]     Train net output #0: loss = 1.24275 (* 1 = 1.24275 loss)
I0519 16:40:31.264668 22864 sgd_solver.cpp:106] Iteration 12900, lr = 0.0025
I0519 16:40:37.034615 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_13000.caffemodel
I0519 16:40:37.115329 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_13000.solverstate
I0519 16:40:59.373771 22864 solver.cpp:237] Iteration 13000, loss = 1.37161
I0519 16:40:59.373961 22864 solver.cpp:253]     Train net output #0: loss = 1.37161 (* 1 = 1.37161 loss)
I0519 16:40:59.373981 22864 sgd_solver.cpp:106] Iteration 13000, lr = 0.0025
I0519 16:41:05.205391 22864 solver.cpp:237] Iteration 13100, loss = 1.20043
I0519 16:41:05.205428 22864 solver.cpp:253]     Train net output #0: loss = 1.20043 (* 1 = 1.20043 loss)
I0519 16:41:05.205447 22864 sgd_solver.cpp:106] Iteration 13100, lr = 0.0025
I0519 16:41:11.032850 22864 solver.cpp:237] Iteration 13200, loss = 1.21148
I0519 16:41:11.032888 22864 solver.cpp:253]     Train net output #0: loss = 1.21148 (* 1 = 1.21148 loss)
I0519 16:41:11.032907 22864 sgd_solver.cpp:106] Iteration 13200, lr = 0.0025
I0519 16:41:16.866155 22864 solver.cpp:237] Iteration 13300, loss = 1.28525
I0519 16:41:16.866190 22864 solver.cpp:253]     Train net output #0: loss = 1.28525 (* 1 = 1.28525 loss)
I0519 16:41:16.866209 22864 sgd_solver.cpp:106] Iteration 13300, lr = 0.0025
I0519 16:41:22.697147 22864 solver.cpp:237] Iteration 13400, loss = 1.29528
I0519 16:41:22.697185 22864 solver.cpp:253]     Train net output #0: loss = 1.29528 (* 1 = 1.29528 loss)
I0519 16:41:22.697202 22864 sgd_solver.cpp:106] Iteration 13400, lr = 0.0025
I0519 16:41:28.527668 22864 solver.cpp:237] Iteration 13500, loss = 1.3753
I0519 16:41:28.527724 22864 solver.cpp:253]     Train net output #0: loss = 1.3753 (* 1 = 1.3753 loss)
I0519 16:41:28.527748 22864 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0519 16:41:34.357451 22864 solver.cpp:237] Iteration 13600, loss = 1.30518
I0519 16:41:34.357614 22864 solver.cpp:253]     Train net output #0: loss = 1.30518 (* 1 = 1.30518 loss)
I0519 16:41:34.357630 22864 sgd_solver.cpp:106] Iteration 13600, lr = 0.0025
I0519 16:41:40.182191 22864 solver.cpp:237] Iteration 13700, loss = 1.35571
I0519 16:41:40.182229 22864 solver.cpp:253]     Train net output #0: loss = 1.35571 (* 1 = 1.35571 loss)
I0519 16:41:40.182246 22864 sgd_solver.cpp:106] Iteration 13700, lr = 0.0025
I0519 16:41:46.012137 22864 solver.cpp:237] Iteration 13800, loss = 1.47298
I0519 16:41:46.012173 22864 solver.cpp:253]     Train net output #0: loss = 1.47298 (* 1 = 1.47298 loss)
I0519 16:41:46.012192 22864 sgd_solver.cpp:106] Iteration 13800, lr = 0.0025
I0519 16:41:51.841727 22864 solver.cpp:237] Iteration 13900, loss = 1.26765
I0519 16:41:51.841781 22864 solver.cpp:253]     Train net output #0: loss = 1.26765 (* 1 = 1.26765 loss)
I0519 16:41:51.841805 22864 sgd_solver.cpp:106] Iteration 13900, lr = 0.0025
I0519 16:41:57.610910 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_14000.caffemodel
I0519 16:41:57.692139 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_14000.solverstate
I0519 16:42:19.926401 22864 solver.cpp:237] Iteration 14000, loss = 1.35744
I0519 16:42:19.926590 22864 solver.cpp:253]     Train net output #0: loss = 1.35744 (* 1 = 1.35744 loss)
I0519 16:42:19.926609 22864 sgd_solver.cpp:106] Iteration 14000, lr = 0.0025
I0519 16:42:25.757217 22864 solver.cpp:237] Iteration 14100, loss = 1.25485
I0519 16:42:25.757253 22864 solver.cpp:253]     Train net output #0: loss = 1.25485 (* 1 = 1.25485 loss)
I0519 16:42:25.757277 22864 sgd_solver.cpp:106] Iteration 14100, lr = 0.0025
I0519 16:42:31.589002 22864 solver.cpp:237] Iteration 14200, loss = 1.22997
I0519 16:42:31.589038 22864 solver.cpp:253]     Train net output #0: loss = 1.22997 (* 1 = 1.22997 loss)
I0519 16:42:31.589061 22864 sgd_solver.cpp:106] Iteration 14200, lr = 0.0025
I0519 16:42:37.414170 22864 solver.cpp:237] Iteration 14300, loss = 1.38095
I0519 16:42:37.414206 22864 solver.cpp:253]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0519 16:42:37.414229 22864 sgd_solver.cpp:106] Iteration 14300, lr = 0.0025
I0519 16:42:43.244882 22864 solver.cpp:237] Iteration 14400, loss = 1.25844
I0519 16:42:43.244935 22864 solver.cpp:253]     Train net output #0: loss = 1.25844 (* 1 = 1.25844 loss)
I0519 16:42:43.244951 22864 sgd_solver.cpp:106] Iteration 14400, lr = 0.0025
I0519 16:42:49.076160 22864 solver.cpp:237] Iteration 14500, loss = 1.18519
I0519 16:42:49.076196 22864 solver.cpp:253]     Train net output #0: loss = 1.18519 (* 1 = 1.18519 loss)
I0519 16:42:49.076215 22864 sgd_solver.cpp:106] Iteration 14500, lr = 0.0025
I0519 16:42:54.902066 22864 solver.cpp:237] Iteration 14600, loss = 1.38861
I0519 16:42:54.902217 22864 solver.cpp:253]     Train net output #0: loss = 1.38861 (* 1 = 1.38861 loss)
I0519 16:42:54.902233 22864 sgd_solver.cpp:106] Iteration 14600, lr = 0.0025
I0519 16:43:00.731283 22864 solver.cpp:237] Iteration 14700, loss = 1.37221
I0519 16:43:00.731319 22864 solver.cpp:253]     Train net output #0: loss = 1.37221 (* 1 = 1.37221 loss)
I0519 16:43:00.731343 22864 sgd_solver.cpp:106] Iteration 14700, lr = 0.0025
I0519 16:43:06.558221 22864 solver.cpp:237] Iteration 14800, loss = 1.4025
I0519 16:43:06.558280 22864 solver.cpp:253]     Train net output #0: loss = 1.4025 (* 1 = 1.4025 loss)
I0519 16:43:06.558300 22864 sgd_solver.cpp:106] Iteration 14800, lr = 0.0025
I0519 16:43:12.382879 22864 solver.cpp:237] Iteration 14900, loss = 1.2669
I0519 16:43:12.382915 22864 solver.cpp:253]     Train net output #0: loss = 1.2669 (* 1 = 1.2669 loss)
I0519 16:43:12.382938 22864 sgd_solver.cpp:106] Iteration 14900, lr = 0.0025
I0519 16:43:18.155658 22864 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_15000.caffemodel
I0519 16:43:18.236649 22864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/morelog_2016-05-19T16.17.51.798618_iter_15000.solverstate
I0519 16:43:39.154192 22864 solver.cpp:321] Iteration 15000, loss = 1.20252
I0519 16:43:39.154372 22864 solver.cpp:341] Iteration 15000, Testing net (#0)
I0519 16:44:25.975540 22864 solver.cpp:409]     Test net output #0: accuracy = 0.852525
I0519 16:44:25.975718 22864 solver.cpp:409]     Test net output #1: loss = 0.495897 (* 1 = 0.495897 loss)
I0519 16:44:25.975736 22864 solver.cpp:326] Optimization Done.
I0519 16:44:25.975749 22864 caffe.cpp:215] Optimization Done.
Application 11227525 resources: utime ~1348s, stime ~227s, Rss ~5328964, inblocks ~3744348, outblocks ~253585
