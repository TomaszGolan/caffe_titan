2804073
I0519 17:12:52.450144 22126 caffe.cpp:184] Using GPUs 0
I0519 17:12:52.876381 22126 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1500
test_interval: 3000
base_lr: 0.0025
display: 100
max_iter: 15000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt"
I0519 17:12:52.880751 22126 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0519 17:12:52.897280 22126 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0519 17:12:52.897346 22126 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0519 17:12:52.897721 22126 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0519 17:12:52.897924 22126 layer_factory.hpp:77] Creating layer data_hdf5
I0519 17:12:52.897953 22126 net.cpp:106] Creating Layer data_hdf5
I0519 17:12:52.897979 22126 net.cpp:411] data_hdf5 -> data
I0519 17:12:52.898012 22126 net.cpp:411] data_hdf5 -> label
I0519 17:12:52.898056 22126 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0519 17:12:52.899286 22126 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0519 17:12:52.901521 22126 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0519 17:13:14.452775 22126 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0519 17:13:14.458050 22126 net.cpp:150] Setting up data_hdf5
I0519 17:13:14.458092 22126 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0519 17:13:14.458111 22126 net.cpp:157] Top shape: 100 (100)
I0519 17:13:14.458123 22126 net.cpp:165] Memory required for data: 2540400
I0519 17:13:14.458142 22126 layer_factory.hpp:77] Creating layer conv1
I0519 17:13:14.458189 22126 net.cpp:106] Creating Layer conv1
I0519 17:13:14.458204 22126 net.cpp:454] conv1 <- data
I0519 17:13:14.458228 22126 net.cpp:411] conv1 -> conv1
I0519 17:13:15.337805 22126 net.cpp:150] Setting up conv1
I0519 17:13:15.337858 22126 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 17:13:15.337882 22126 net.cpp:165] Memory required for data: 30188400
I0519 17:13:15.337913 22126 layer_factory.hpp:77] Creating layer relu1
I0519 17:13:15.337935 22126 net.cpp:106] Creating Layer relu1
I0519 17:13:15.337955 22126 net.cpp:454] relu1 <- conv1
I0519 17:13:15.337991 22126 net.cpp:397] relu1 -> conv1 (in-place)
I0519 17:13:15.338523 22126 net.cpp:150] Setting up relu1
I0519 17:13:15.338546 22126 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 17:13:15.338560 22126 net.cpp:165] Memory required for data: 57836400
I0519 17:13:15.338577 22126 layer_factory.hpp:77] Creating layer pool1
I0519 17:13:15.338603 22126 net.cpp:106] Creating Layer pool1
I0519 17:13:15.338618 22126 net.cpp:454] pool1 <- conv1
I0519 17:13:15.338634 22126 net.cpp:411] pool1 -> pool1
I0519 17:13:15.338726 22126 net.cpp:150] Setting up pool1
I0519 17:13:15.338745 22126 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0519 17:13:15.338760 22126 net.cpp:165] Memory required for data: 71660400
I0519 17:13:15.338780 22126 layer_factory.hpp:77] Creating layer conv2
I0519 17:13:15.338804 22126 net.cpp:106] Creating Layer conv2
I0519 17:13:15.338819 22126 net.cpp:454] conv2 <- pool1
I0519 17:13:15.338835 22126 net.cpp:411] conv2 -> conv2
I0519 17:13:15.341581 22126 net.cpp:150] Setting up conv2
I0519 17:13:15.341612 22126 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 17:13:15.341629 22126 net.cpp:165] Memory required for data: 91532400
I0519 17:13:15.341656 22126 layer_factory.hpp:77] Creating layer relu2
I0519 17:13:15.341683 22126 net.cpp:106] Creating Layer relu2
I0519 17:13:15.341697 22126 net.cpp:454] relu2 <- conv2
I0519 17:13:15.341713 22126 net.cpp:397] relu2 -> conv2 (in-place)
I0519 17:13:15.342067 22126 net.cpp:150] Setting up relu2
I0519 17:13:15.342088 22126 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 17:13:15.342100 22126 net.cpp:165] Memory required for data: 111404400
I0519 17:13:15.342116 22126 layer_factory.hpp:77] Creating layer pool2
I0519 17:13:15.342138 22126 net.cpp:106] Creating Layer pool2
I0519 17:13:15.342151 22126 net.cpp:454] pool2 <- conv2
I0519 17:13:15.342167 22126 net.cpp:411] pool2 -> pool2
I0519 17:13:15.342263 22126 net.cpp:150] Setting up pool2
I0519 17:13:15.342280 22126 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0519 17:13:15.342295 22126 net.cpp:165] Memory required for data: 121340400
I0519 17:13:15.342316 22126 layer_factory.hpp:77] Creating layer conv3
I0519 17:13:15.342337 22126 net.cpp:106] Creating Layer conv3
I0519 17:13:15.342356 22126 net.cpp:454] conv3 <- pool2
I0519 17:13:15.342373 22126 net.cpp:411] conv3 -> conv3
I0519 17:13:15.344329 22126 net.cpp:150] Setting up conv3
I0519 17:13:15.344353 22126 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 17:13:15.344374 22126 net.cpp:165] Memory required for data: 132182000
I0519 17:13:15.344396 22126 layer_factory.hpp:77] Creating layer relu3
I0519 17:13:15.344419 22126 net.cpp:106] Creating Layer relu3
I0519 17:13:15.344442 22126 net.cpp:454] relu3 <- conv3
I0519 17:13:15.344458 22126 net.cpp:397] relu3 -> conv3 (in-place)
I0519 17:13:15.344944 22126 net.cpp:150] Setting up relu3
I0519 17:13:15.344969 22126 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 17:13:15.344982 22126 net.cpp:165] Memory required for data: 143023600
I0519 17:13:15.344998 22126 layer_factory.hpp:77] Creating layer pool3
I0519 17:13:15.345013 22126 net.cpp:106] Creating Layer pool3
I0519 17:13:15.345036 22126 net.cpp:454] pool3 <- conv3
I0519 17:13:15.345052 22126 net.cpp:411] pool3 -> pool3
I0519 17:13:15.345134 22126 net.cpp:150] Setting up pool3
I0519 17:13:15.345156 22126 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0519 17:13:15.345170 22126 net.cpp:165] Memory required for data: 148444400
I0519 17:13:15.345181 22126 layer_factory.hpp:77] Creating layer conv4
I0519 17:13:15.345211 22126 net.cpp:106] Creating Layer conv4
I0519 17:13:15.345223 22126 net.cpp:454] conv4 <- pool3
I0519 17:13:15.345240 22126 net.cpp:411] conv4 -> conv4
I0519 17:13:15.348212 22126 net.cpp:150] Setting up conv4
I0519 17:13:15.348244 22126 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 17:13:15.348260 22126 net.cpp:165] Memory required for data: 152073200
I0519 17:13:15.348294 22126 layer_factory.hpp:77] Creating layer relu4
I0519 17:13:15.348325 22126 net.cpp:106] Creating Layer relu4
I0519 17:13:15.348340 22126 net.cpp:454] relu4 <- conv4
I0519 17:13:15.348356 22126 net.cpp:397] relu4 -> conv4 (in-place)
I0519 17:13:15.348851 22126 net.cpp:150] Setting up relu4
I0519 17:13:15.348875 22126 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 17:13:15.348887 22126 net.cpp:165] Memory required for data: 155702000
I0519 17:13:15.348903 22126 layer_factory.hpp:77] Creating layer pool4
I0519 17:13:15.348919 22126 net.cpp:106] Creating Layer pool4
I0519 17:13:15.348940 22126 net.cpp:454] pool4 <- conv4
I0519 17:13:15.348956 22126 net.cpp:411] pool4 -> pool4
I0519 17:13:15.349040 22126 net.cpp:150] Setting up pool4
I0519 17:13:15.349061 22126 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0519 17:13:15.349074 22126 net.cpp:165] Memory required for data: 157516400
I0519 17:13:15.349089 22126 layer_factory.hpp:77] Creating layer ip1
I0519 17:13:15.349117 22126 net.cpp:106] Creating Layer ip1
I0519 17:13:15.349130 22126 net.cpp:454] ip1 <- pool4
I0519 17:13:15.349145 22126 net.cpp:411] ip1 -> ip1
I0519 17:13:15.364586 22126 net.cpp:150] Setting up ip1
I0519 17:13:15.364617 22126 net.cpp:157] Top shape: 100 196 (19600)
I0519 17:13:15.364631 22126 net.cpp:165] Memory required for data: 157594800
I0519 17:13:15.364660 22126 layer_factory.hpp:77] Creating layer relu5
I0519 17:13:15.364677 22126 net.cpp:106] Creating Layer relu5
I0519 17:13:15.364691 22126 net.cpp:454] relu5 <- ip1
I0519 17:13:15.364718 22126 net.cpp:397] relu5 -> ip1 (in-place)
I0519 17:13:15.365079 22126 net.cpp:150] Setting up relu5
I0519 17:13:15.365099 22126 net.cpp:157] Top shape: 100 196 (19600)
I0519 17:13:15.365111 22126 net.cpp:165] Memory required for data: 157673200
I0519 17:13:15.365123 22126 layer_factory.hpp:77] Creating layer drop1
I0519 17:13:15.365166 22126 net.cpp:106] Creating Layer drop1
I0519 17:13:15.365180 22126 net.cpp:454] drop1 <- ip1
I0519 17:13:15.365195 22126 net.cpp:397] drop1 -> ip1 (in-place)
I0519 17:13:15.365268 22126 net.cpp:150] Setting up drop1
I0519 17:13:15.365285 22126 net.cpp:157] Top shape: 100 196 (19600)
I0519 17:13:15.365309 22126 net.cpp:165] Memory required for data: 157751600
I0519 17:13:15.365322 22126 layer_factory.hpp:77] Creating layer ip2
I0519 17:13:15.365345 22126 net.cpp:106] Creating Layer ip2
I0519 17:13:15.365358 22126 net.cpp:454] ip2 <- ip1
I0519 17:13:15.365386 22126 net.cpp:411] ip2 -> ip2
I0519 17:13:15.365870 22126 net.cpp:150] Setting up ip2
I0519 17:13:15.365890 22126 net.cpp:157] Top shape: 100 98 (9800)
I0519 17:13:15.365902 22126 net.cpp:165] Memory required for data: 157790800
I0519 17:13:15.365923 22126 layer_factory.hpp:77] Creating layer relu6
I0519 17:13:15.365945 22126 net.cpp:106] Creating Layer relu6
I0519 17:13:15.365958 22126 net.cpp:454] relu6 <- ip2
I0519 17:13:15.365973 22126 net.cpp:397] relu6 -> ip2 (in-place)
I0519 17:13:15.366526 22126 net.cpp:150] Setting up relu6
I0519 17:13:15.366549 22126 net.cpp:157] Top shape: 100 98 (9800)
I0519 17:13:15.366562 22126 net.cpp:165] Memory required for data: 157830000
I0519 17:13:15.366578 22126 layer_factory.hpp:77] Creating layer drop2
I0519 17:13:15.366593 22126 net.cpp:106] Creating Layer drop2
I0519 17:13:15.366614 22126 net.cpp:454] drop2 <- ip2
I0519 17:13:15.366631 22126 net.cpp:397] drop2 -> ip2 (in-place)
I0519 17:13:15.366679 22126 net.cpp:150] Setting up drop2
I0519 17:13:15.366701 22126 net.cpp:157] Top shape: 100 98 (9800)
I0519 17:13:15.366714 22126 net.cpp:165] Memory required for data: 157869200
I0519 17:13:15.366732 22126 layer_factory.hpp:77] Creating layer ip3
I0519 17:13:15.366750 22126 net.cpp:106] Creating Layer ip3
I0519 17:13:15.366761 22126 net.cpp:454] ip3 <- ip2
I0519 17:13:15.366780 22126 net.cpp:411] ip3 -> ip3
I0519 17:13:15.367010 22126 net.cpp:150] Setting up ip3
I0519 17:13:15.367029 22126 net.cpp:157] Top shape: 100 11 (1100)
I0519 17:13:15.367043 22126 net.cpp:165] Memory required for data: 157873600
I0519 17:13:15.367063 22126 layer_factory.hpp:77] Creating layer drop3
I0519 17:13:15.367077 22126 net.cpp:106] Creating Layer drop3
I0519 17:13:15.367097 22126 net.cpp:454] drop3 <- ip3
I0519 17:13:15.367112 22126 net.cpp:397] drop3 -> ip3 (in-place)
I0519 17:13:15.367159 22126 net.cpp:150] Setting up drop3
I0519 17:13:15.367182 22126 net.cpp:157] Top shape: 100 11 (1100)
I0519 17:13:15.367194 22126 net.cpp:165] Memory required for data: 157878000
I0519 17:13:15.367213 22126 layer_factory.hpp:77] Creating layer loss
I0519 17:13:15.367234 22126 net.cpp:106] Creating Layer loss
I0519 17:13:15.367249 22126 net.cpp:454] loss <- ip3
I0519 17:13:15.367269 22126 net.cpp:454] loss <- label
I0519 17:13:15.367285 22126 net.cpp:411] loss -> loss
I0519 17:13:15.367305 22126 layer_factory.hpp:77] Creating layer loss
I0519 17:13:15.367971 22126 net.cpp:150] Setting up loss
I0519 17:13:15.367992 22126 net.cpp:157] Top shape: (1)
I0519 17:13:15.368010 22126 net.cpp:160]     with loss weight 1
I0519 17:13:15.368062 22126 net.cpp:165] Memory required for data: 157878004
I0519 17:13:15.368083 22126 net.cpp:226] loss needs backward computation.
I0519 17:13:15.368098 22126 net.cpp:226] drop3 needs backward computation.
I0519 17:13:15.368110 22126 net.cpp:226] ip3 needs backward computation.
I0519 17:13:15.368124 22126 net.cpp:226] drop2 needs backward computation.
I0519 17:13:15.368135 22126 net.cpp:226] relu6 needs backward computation.
I0519 17:13:15.368151 22126 net.cpp:226] ip2 needs backward computation.
I0519 17:13:15.368172 22126 net.cpp:226] drop1 needs backward computation.
I0519 17:13:15.368185 22126 net.cpp:226] relu5 needs backward computation.
I0519 17:13:15.368198 22126 net.cpp:226] ip1 needs backward computation.
I0519 17:13:15.368211 22126 net.cpp:226] pool4 needs backward computation.
I0519 17:13:15.368224 22126 net.cpp:226] relu4 needs backward computation.
I0519 17:13:15.368239 22126 net.cpp:226] conv4 needs backward computation.
I0519 17:13:15.368252 22126 net.cpp:226] pool3 needs backward computation.
I0519 17:13:15.368278 22126 net.cpp:226] relu3 needs backward computation.
I0519 17:13:15.368293 22126 net.cpp:226] conv3 needs backward computation.
I0519 17:13:15.368315 22126 net.cpp:226] pool2 needs backward computation.
I0519 17:13:15.368331 22126 net.cpp:226] relu2 needs backward computation.
I0519 17:13:15.368345 22126 net.cpp:226] conv2 needs backward computation.
I0519 17:13:15.368366 22126 net.cpp:226] pool1 needs backward computation.
I0519 17:13:15.368379 22126 net.cpp:226] relu1 needs backward computation.
I0519 17:13:15.368396 22126 net.cpp:226] conv1 needs backward computation.
I0519 17:13:15.368410 22126 net.cpp:228] data_hdf5 does not need backward computation.
I0519 17:13:15.368422 22126 net.cpp:270] This network produces output loss
I0519 17:13:15.368451 22126 net.cpp:283] Network initialization done.
I0519 17:13:15.370200 22126 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0519 17:13:15.370275 22126 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0519 17:13:15.370633 22126 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0519 17:13:15.370832 22126 layer_factory.hpp:77] Creating layer data_hdf5
I0519 17:13:15.370851 22126 net.cpp:106] Creating Layer data_hdf5
I0519 17:13:15.370867 22126 net.cpp:411] data_hdf5 -> data
I0519 17:13:15.370887 22126 net.cpp:411] data_hdf5 -> label
I0519 17:13:15.370905 22126 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0519 17:13:15.372318 22126 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0519 17:13:36.723209 22126 net.cpp:150] Setting up data_hdf5
I0519 17:13:36.723381 22126 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0519 17:13:36.723400 22126 net.cpp:157] Top shape: 100 (100)
I0519 17:13:36.723412 22126 net.cpp:165] Memory required for data: 2540400
I0519 17:13:36.723428 22126 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0519 17:13:36.723461 22126 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0519 17:13:36.723474 22126 net.cpp:454] label_data_hdf5_1_split <- label
I0519 17:13:36.723491 22126 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0519 17:13:36.723534 22126 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0519 17:13:36.723613 22126 net.cpp:150] Setting up label_data_hdf5_1_split
I0519 17:13:36.723639 22126 net.cpp:157] Top shape: 100 (100)
I0519 17:13:36.723654 22126 net.cpp:157] Top shape: 100 (100)
I0519 17:13:36.723675 22126 net.cpp:165] Memory required for data: 2541200
I0519 17:13:36.723688 22126 layer_factory.hpp:77] Creating layer conv1
I0519 17:13:36.723714 22126 net.cpp:106] Creating Layer conv1
I0519 17:13:36.723733 22126 net.cpp:454] conv1 <- data
I0519 17:13:36.723752 22126 net.cpp:411] conv1 -> conv1
I0519 17:13:36.725720 22126 net.cpp:150] Setting up conv1
I0519 17:13:36.725746 22126 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 17:13:36.725766 22126 net.cpp:165] Memory required for data: 30189200
I0519 17:13:36.725790 22126 layer_factory.hpp:77] Creating layer relu1
I0519 17:13:36.725811 22126 net.cpp:106] Creating Layer relu1
I0519 17:13:36.725832 22126 net.cpp:454] relu1 <- conv1
I0519 17:13:36.725849 22126 net.cpp:397] relu1 -> conv1 (in-place)
I0519 17:13:36.726366 22126 net.cpp:150] Setting up relu1
I0519 17:13:36.726389 22126 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 17:13:36.726402 22126 net.cpp:165] Memory required for data: 57837200
I0519 17:13:36.726414 22126 layer_factory.hpp:77] Creating layer pool1
I0519 17:13:36.726444 22126 net.cpp:106] Creating Layer pool1
I0519 17:13:36.726459 22126 net.cpp:454] pool1 <- conv1
I0519 17:13:36.726475 22126 net.cpp:411] pool1 -> pool1
I0519 17:13:36.726565 22126 net.cpp:150] Setting up pool1
I0519 17:13:36.726588 22126 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0519 17:13:36.726601 22126 net.cpp:165] Memory required for data: 71661200
I0519 17:13:36.726615 22126 layer_factory.hpp:77] Creating layer conv2
I0519 17:13:36.726642 22126 net.cpp:106] Creating Layer conv2
I0519 17:13:36.726656 22126 net.cpp:454] conv2 <- pool1
I0519 17:13:36.726673 22126 net.cpp:411] conv2 -> conv2
I0519 17:13:36.728652 22126 net.cpp:150] Setting up conv2
I0519 17:13:36.728677 22126 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 17:13:36.728690 22126 net.cpp:165] Memory required for data: 91533200
I0519 17:13:36.728715 22126 layer_factory.hpp:77] Creating layer relu2
I0519 17:13:36.728732 22126 net.cpp:106] Creating Layer relu2
I0519 17:13:36.728755 22126 net.cpp:454] relu2 <- conv2
I0519 17:13:36.728771 22126 net.cpp:397] relu2 -> conv2 (in-place)
I0519 17:13:36.729130 22126 net.cpp:150] Setting up relu2
I0519 17:13:36.729151 22126 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 17:13:36.729163 22126 net.cpp:165] Memory required for data: 111405200
I0519 17:13:36.729178 22126 layer_factory.hpp:77] Creating layer pool2
I0519 17:13:36.729194 22126 net.cpp:106] Creating Layer pool2
I0519 17:13:36.729214 22126 net.cpp:454] pool2 <- conv2
I0519 17:13:36.729230 22126 net.cpp:411] pool2 -> pool2
I0519 17:13:36.729320 22126 net.cpp:150] Setting up pool2
I0519 17:13:36.729338 22126 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0519 17:13:36.729351 22126 net.cpp:165] Memory required for data: 121341200
I0519 17:13:36.729365 22126 layer_factory.hpp:77] Creating layer conv3
I0519 17:13:36.729393 22126 net.cpp:106] Creating Layer conv3
I0519 17:13:36.729406 22126 net.cpp:454] conv3 <- pool2
I0519 17:13:36.729424 22126 net.cpp:411] conv3 -> conv3
I0519 17:13:36.731454 22126 net.cpp:150] Setting up conv3
I0519 17:13:36.731479 22126 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 17:13:36.731498 22126 net.cpp:165] Memory required for data: 132182800
I0519 17:13:36.731536 22126 layer_factory.hpp:77] Creating layer relu3
I0519 17:13:36.731561 22126 net.cpp:106] Creating Layer relu3
I0519 17:13:36.731575 22126 net.cpp:454] relu3 <- conv3
I0519 17:13:36.731591 22126 net.cpp:397] relu3 -> conv3 (in-place)
I0519 17:13:36.732086 22126 net.cpp:150] Setting up relu3
I0519 17:13:36.732110 22126 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 17:13:36.732123 22126 net.cpp:165] Memory required for data: 143024400
I0519 17:13:36.732139 22126 layer_factory.hpp:77] Creating layer pool3
I0519 17:13:36.732163 22126 net.cpp:106] Creating Layer pool3
I0519 17:13:36.732177 22126 net.cpp:454] pool3 <- conv3
I0519 17:13:36.732194 22126 net.cpp:411] pool3 -> pool3
I0519 17:13:36.732287 22126 net.cpp:150] Setting up pool3
I0519 17:13:36.732305 22126 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0519 17:13:36.732319 22126 net.cpp:165] Memory required for data: 148445200
I0519 17:13:36.732331 22126 layer_factory.hpp:77] Creating layer conv4
I0519 17:13:36.732358 22126 net.cpp:106] Creating Layer conv4
I0519 17:13:36.732378 22126 net.cpp:454] conv4 <- pool3
I0519 17:13:36.732394 22126 net.cpp:411] conv4 -> conv4
I0519 17:13:36.734516 22126 net.cpp:150] Setting up conv4
I0519 17:13:36.734540 22126 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 17:13:36.734560 22126 net.cpp:165] Memory required for data: 152074000
I0519 17:13:36.734580 22126 layer_factory.hpp:77] Creating layer relu4
I0519 17:13:36.734599 22126 net.cpp:106] Creating Layer relu4
I0519 17:13:36.734612 22126 net.cpp:454] relu4 <- conv4
I0519 17:13:36.734637 22126 net.cpp:397] relu4 -> conv4 (in-place)
I0519 17:13:36.735131 22126 net.cpp:150] Setting up relu4
I0519 17:13:36.735153 22126 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 17:13:36.735167 22126 net.cpp:165] Memory required for data: 155702800
I0519 17:13:36.735183 22126 layer_factory.hpp:77] Creating layer pool4
I0519 17:13:36.735198 22126 net.cpp:106] Creating Layer pool4
I0519 17:13:36.735213 22126 net.cpp:454] pool4 <- conv4
I0519 17:13:36.735230 22126 net.cpp:411] pool4 -> pool4
I0519 17:13:36.735329 22126 net.cpp:150] Setting up pool4
I0519 17:13:36.735352 22126 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0519 17:13:36.735364 22126 net.cpp:165] Memory required for data: 157517200
I0519 17:13:36.735376 22126 layer_factory.hpp:77] Creating layer ip1
I0519 17:13:36.735394 22126 net.cpp:106] Creating Layer ip1
I0519 17:13:36.735406 22126 net.cpp:454] ip1 <- pool4
I0519 17:13:36.735424 22126 net.cpp:411] ip1 -> ip1
I0519 17:13:36.750910 22126 net.cpp:150] Setting up ip1
I0519 17:13:36.750944 22126 net.cpp:157] Top shape: 100 196 (19600)
I0519 17:13:36.750965 22126 net.cpp:165] Memory required for data: 157595600
I0519 17:13:36.750991 22126 layer_factory.hpp:77] Creating layer relu5
I0519 17:13:36.751013 22126 net.cpp:106] Creating Layer relu5
I0519 17:13:36.751037 22126 net.cpp:454] relu5 <- ip1
I0519 17:13:36.751055 22126 net.cpp:397] relu5 -> ip1 (in-place)
I0519 17:13:36.751421 22126 net.cpp:150] Setting up relu5
I0519 17:13:36.751441 22126 net.cpp:157] Top shape: 100 196 (19600)
I0519 17:13:36.751454 22126 net.cpp:165] Memory required for data: 157674000
I0519 17:13:36.751469 22126 layer_factory.hpp:77] Creating layer drop1
I0519 17:13:36.751492 22126 net.cpp:106] Creating Layer drop1
I0519 17:13:36.751510 22126 net.cpp:454] drop1 <- ip1
I0519 17:13:36.751528 22126 net.cpp:397] drop1 -> ip1 (in-place)
I0519 17:13:36.751579 22126 net.cpp:150] Setting up drop1
I0519 17:13:36.751600 22126 net.cpp:157] Top shape: 100 196 (19600)
I0519 17:13:36.751613 22126 net.cpp:165] Memory required for data: 157752400
I0519 17:13:36.751626 22126 layer_factory.hpp:77] Creating layer ip2
I0519 17:13:36.751646 22126 net.cpp:106] Creating Layer ip2
I0519 17:13:36.751658 22126 net.cpp:454] ip2 <- ip1
I0519 17:13:36.751682 22126 net.cpp:411] ip2 -> ip2
I0519 17:13:36.752178 22126 net.cpp:150] Setting up ip2
I0519 17:13:36.752197 22126 net.cpp:157] Top shape: 100 98 (9800)
I0519 17:13:36.752210 22126 net.cpp:165] Memory required for data: 157791600
I0519 17:13:36.752231 22126 layer_factory.hpp:77] Creating layer relu6
I0519 17:13:36.752265 22126 net.cpp:106] Creating Layer relu6
I0519 17:13:36.752292 22126 net.cpp:454] relu6 <- ip2
I0519 17:13:36.752308 22126 net.cpp:397] relu6 -> ip2 (in-place)
I0519 17:13:36.752877 22126 net.cpp:150] Setting up relu6
I0519 17:13:36.752902 22126 net.cpp:157] Top shape: 100 98 (9800)
I0519 17:13:36.752914 22126 net.cpp:165] Memory required for data: 157830800
I0519 17:13:36.752930 22126 layer_factory.hpp:77] Creating layer drop2
I0519 17:13:36.752955 22126 net.cpp:106] Creating Layer drop2
I0519 17:13:36.752969 22126 net.cpp:454] drop2 <- ip2
I0519 17:13:36.752985 22126 net.cpp:397] drop2 -> ip2 (in-place)
I0519 17:13:36.753036 22126 net.cpp:150] Setting up drop2
I0519 17:13:36.753060 22126 net.cpp:157] Top shape: 100 98 (9800)
I0519 17:13:36.753072 22126 net.cpp:165] Memory required for data: 157870000
I0519 17:13:36.753087 22126 layer_factory.hpp:77] Creating layer ip3
I0519 17:13:36.753103 22126 net.cpp:106] Creating Layer ip3
I0519 17:13:36.753118 22126 net.cpp:454] ip3 <- ip2
I0519 17:13:36.753141 22126 net.cpp:411] ip3 -> ip3
I0519 17:13:36.753381 22126 net.cpp:150] Setting up ip3
I0519 17:13:36.753399 22126 net.cpp:157] Top shape: 100 11 (1100)
I0519 17:13:36.753412 22126 net.cpp:165] Memory required for data: 157874400
I0519 17:13:36.753433 22126 layer_factory.hpp:77] Creating layer drop3
I0519 17:13:36.753455 22126 net.cpp:106] Creating Layer drop3
I0519 17:13:36.753468 22126 net.cpp:454] drop3 <- ip3
I0519 17:13:36.753484 22126 net.cpp:397] drop3 -> ip3 (in-place)
I0519 17:13:36.753533 22126 net.cpp:150] Setting up drop3
I0519 17:13:36.753556 22126 net.cpp:157] Top shape: 100 11 (1100)
I0519 17:13:36.753567 22126 net.cpp:165] Memory required for data: 157878800
I0519 17:13:36.753582 22126 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0519 17:13:36.753597 22126 net.cpp:106] Creating Layer ip3_drop3_0_split
I0519 17:13:36.753612 22126 net.cpp:454] ip3_drop3_0_split <- ip3
I0519 17:13:36.753633 22126 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0519 17:13:36.753653 22126 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0519 17:13:36.753738 22126 net.cpp:150] Setting up ip3_drop3_0_split
I0519 17:13:36.753762 22126 net.cpp:157] Top shape: 100 11 (1100)
I0519 17:13:36.753777 22126 net.cpp:157] Top shape: 100 11 (1100)
I0519 17:13:36.753788 22126 net.cpp:165] Memory required for data: 157887600
I0519 17:13:36.753803 22126 layer_factory.hpp:77] Creating layer accuracy
I0519 17:13:36.753831 22126 net.cpp:106] Creating Layer accuracy
I0519 17:13:36.753845 22126 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0519 17:13:36.753862 22126 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0519 17:13:36.753880 22126 net.cpp:411] accuracy -> accuracy
I0519 17:13:36.753907 22126 net.cpp:150] Setting up accuracy
I0519 17:13:36.753929 22126 net.cpp:157] Top shape: (1)
I0519 17:13:36.753942 22126 net.cpp:165] Memory required for data: 157887604
I0519 17:13:36.753957 22126 layer_factory.hpp:77] Creating layer loss
I0519 17:13:36.753973 22126 net.cpp:106] Creating Layer loss
I0519 17:13:36.753985 22126 net.cpp:454] loss <- ip3_drop3_0_split_1
I0519 17:13:36.754003 22126 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0519 17:13:36.754024 22126 net.cpp:411] loss -> loss
I0519 17:13:36.754045 22126 layer_factory.hpp:77] Creating layer loss
I0519 17:13:36.754559 22126 net.cpp:150] Setting up loss
I0519 17:13:36.754578 22126 net.cpp:157] Top shape: (1)
I0519 17:13:36.754590 22126 net.cpp:160]     with loss weight 1
I0519 17:13:36.754617 22126 net.cpp:165] Memory required for data: 157887608
I0519 17:13:36.754637 22126 net.cpp:226] loss needs backward computation.
I0519 17:13:36.754652 22126 net.cpp:228] accuracy does not need backward computation.
I0519 17:13:36.754665 22126 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0519 17:13:36.754679 22126 net.cpp:226] drop3 needs backward computation.
I0519 17:13:36.754691 22126 net.cpp:226] ip3 needs backward computation.
I0519 17:13:36.754708 22126 net.cpp:226] drop2 needs backward computation.
I0519 17:13:36.754734 22126 net.cpp:226] relu6 needs backward computation.
I0519 17:13:36.754748 22126 net.cpp:226] ip2 needs backward computation.
I0519 17:13:36.754763 22126 net.cpp:226] drop1 needs backward computation.
I0519 17:13:36.754776 22126 net.cpp:226] relu5 needs backward computation.
I0519 17:13:36.754788 22126 net.cpp:226] ip1 needs backward computation.
I0519 17:13:36.754803 22126 net.cpp:226] pool4 needs backward computation.
I0519 17:13:36.754817 22126 net.cpp:226] relu4 needs backward computation.
I0519 17:13:36.754835 22126 net.cpp:226] conv4 needs backward computation.
I0519 17:13:36.754850 22126 net.cpp:226] pool3 needs backward computation.
I0519 17:13:36.754866 22126 net.cpp:226] relu3 needs backward computation.
I0519 17:13:36.754879 22126 net.cpp:226] conv3 needs backward computation.
I0519 17:13:36.754892 22126 net.cpp:226] pool2 needs backward computation.
I0519 17:13:36.754904 22126 net.cpp:226] relu2 needs backward computation.
I0519 17:13:36.754920 22126 net.cpp:226] conv2 needs backward computation.
I0519 17:13:36.754938 22126 net.cpp:226] pool1 needs backward computation.
I0519 17:13:36.754952 22126 net.cpp:226] relu1 needs backward computation.
I0519 17:13:36.754964 22126 net.cpp:226] conv1 needs backward computation.
I0519 17:13:36.754978 22126 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0519 17:13:36.754992 22126 net.cpp:228] data_hdf5 does not need backward computation.
I0519 17:13:36.755004 22126 net.cpp:270] This network produces output accuracy
I0519 17:13:36.755019 22126 net.cpp:270] This network produces output loss
I0519 17:13:36.755049 22126 net.cpp:283] Network initialization done.
I0519 17:13:36.755184 22126 solver.cpp:60] Solver scaffolding done.
I0519 17:13:36.756340 22126 caffe.cpp:212] Starting Optimization
I0519 17:13:36.756356 22126 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0519 17:13:36.756371 22126 solver.cpp:289] Learning Rate Policy: fixed
I0519 17:13:36.757457 22126 solver.cpp:341] Iteration 0, Testing net (#0)
I0519 17:14:24.597373 22126 solver.cpp:409]     Test net output #0: accuracy = 0.0953666
I0519 17:14:24.597549 22126 solver.cpp:409]     Test net output #1: loss = 2.39855 (* 1 = 2.39855 loss)
I0519 17:14:24.630390 22126 solver.cpp:237] Iteration 0, loss = 2.40098
I0519 17:14:24.630432 22126 solver.cpp:253]     Train net output #0: loss = 2.40098 (* 1 = 2.40098 loss)
I0519 17:14:24.630453 22126 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0519 17:14:30.444201 22126 solver.cpp:237] Iteration 100, loss = 2.34178
I0519 17:14:30.444239 22126 solver.cpp:253]     Train net output #0: loss = 2.34178 (* 1 = 2.34178 loss)
I0519 17:14:30.444262 22126 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0519 17:14:36.260084 22126 solver.cpp:237] Iteration 200, loss = 2.2777
I0519 17:14:36.260136 22126 solver.cpp:253]     Train net output #0: loss = 2.2777 (* 1 = 2.2777 loss)
I0519 17:14:36.260154 22126 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0519 17:14:42.074404 22126 solver.cpp:237] Iteration 300, loss = 2.15254
I0519 17:14:42.074440 22126 solver.cpp:253]     Train net output #0: loss = 2.15254 (* 1 = 2.15254 loss)
I0519 17:14:42.074463 22126 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0519 17:14:47.889799 22126 solver.cpp:237] Iteration 400, loss = 2.22306
I0519 17:14:47.889835 22126 solver.cpp:253]     Train net output #0: loss = 2.22306 (* 1 = 2.22306 loss)
I0519 17:14:47.889853 22126 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0519 17:14:53.707268 22126 solver.cpp:237] Iteration 500, loss = 2.03385
I0519 17:14:53.707304 22126 solver.cpp:253]     Train net output #0: loss = 2.03385 (* 1 = 2.03385 loss)
I0519 17:14:53.707327 22126 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0519 17:14:59.523718 22126 solver.cpp:237] Iteration 600, loss = 1.98618
I0519 17:14:59.523886 22126 solver.cpp:253]     Train net output #0: loss = 1.98618 (* 1 = 1.98618 loss)
I0519 17:14:59.523905 22126 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0519 17:15:05.341171 22126 solver.cpp:237] Iteration 700, loss = 2.0703
I0519 17:15:05.341208 22126 solver.cpp:253]     Train net output #0: loss = 2.0703 (* 1 = 2.0703 loss)
I0519 17:15:05.341226 22126 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0519 17:15:11.157016 22126 solver.cpp:237] Iteration 800, loss = 1.84601
I0519 17:15:11.157052 22126 solver.cpp:253]     Train net output #0: loss = 1.84601 (* 1 = 1.84601 loss)
I0519 17:15:11.157076 22126 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0519 17:15:16.972306 22126 solver.cpp:237] Iteration 900, loss = 1.89818
I0519 17:15:16.972342 22126 solver.cpp:253]     Train net output #0: loss = 1.89818 (* 1 = 1.89818 loss)
I0519 17:15:16.972360 22126 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0519 17:15:22.728021 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_1000.caffemodel
I0519 17:15:22.810343 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_1000.solverstate
I0519 17:15:45.026628 22126 solver.cpp:237] Iteration 1000, loss = 1.95227
I0519 17:15:45.026804 22126 solver.cpp:253]     Train net output #0: loss = 1.95227 (* 1 = 1.95227 loss)
I0519 17:15:45.026823 22126 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0519 17:15:50.840981 22126 solver.cpp:237] Iteration 1100, loss = 1.84666
I0519 17:15:50.841035 22126 solver.cpp:253]     Train net output #0: loss = 1.84666 (* 1 = 1.84666 loss)
I0519 17:15:50.841065 22126 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0519 17:15:56.657567 22126 solver.cpp:237] Iteration 1200, loss = 1.72765
I0519 17:15:56.657603 22126 solver.cpp:253]     Train net output #0: loss = 1.72765 (* 1 = 1.72765 loss)
I0519 17:15:56.657627 22126 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0519 17:16:02.473299 22126 solver.cpp:237] Iteration 1300, loss = 1.82133
I0519 17:16:02.473335 22126 solver.cpp:253]     Train net output #0: loss = 1.82133 (* 1 = 1.82133 loss)
I0519 17:16:02.473359 22126 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0519 17:16:08.290470 22126 solver.cpp:237] Iteration 1400, loss = 1.52503
I0519 17:16:08.290506 22126 solver.cpp:253]     Train net output #0: loss = 1.52503 (* 1 = 1.52503 loss)
I0519 17:16:08.290525 22126 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0519 17:16:14.104563 22126 solver.cpp:237] Iteration 1500, loss = 1.74324
I0519 17:16:14.104599 22126 solver.cpp:253]     Train net output #0: loss = 1.74324 (* 1 = 1.74324 loss)
I0519 17:16:14.104624 22126 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0519 17:16:19.922533 22126 solver.cpp:237] Iteration 1600, loss = 1.67018
I0519 17:16:19.922701 22126 solver.cpp:253]     Train net output #0: loss = 1.67018 (* 1 = 1.67018 loss)
I0519 17:16:19.922720 22126 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0519 17:16:25.738134 22126 solver.cpp:237] Iteration 1700, loss = 1.55975
I0519 17:16:25.738171 22126 solver.cpp:253]     Train net output #0: loss = 1.55975 (* 1 = 1.55975 loss)
I0519 17:16:25.738195 22126 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0519 17:16:31.555203 22126 solver.cpp:237] Iteration 1800, loss = 1.68237
I0519 17:16:31.555238 22126 solver.cpp:253]     Train net output #0: loss = 1.68237 (* 1 = 1.68237 loss)
I0519 17:16:31.555263 22126 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0519 17:16:37.372135 22126 solver.cpp:237] Iteration 1900, loss = 1.46506
I0519 17:16:37.372171 22126 solver.cpp:253]     Train net output #0: loss = 1.46506 (* 1 = 1.46506 loss)
I0519 17:16:37.372189 22126 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0519 17:16:43.133498 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_2000.caffemodel
I0519 17:16:43.212196 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_2000.solverstate
I0519 17:17:05.411736 22126 solver.cpp:237] Iteration 2000, loss = 1.69464
I0519 17:17:05.411907 22126 solver.cpp:253]     Train net output #0: loss = 1.69464 (* 1 = 1.69464 loss)
I0519 17:17:05.411928 22126 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0519 17:17:11.226228 22126 solver.cpp:237] Iteration 2100, loss = 1.7176
I0519 17:17:11.226270 22126 solver.cpp:253]     Train net output #0: loss = 1.7176 (* 1 = 1.7176 loss)
I0519 17:17:11.226287 22126 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0519 17:17:17.040392 22126 solver.cpp:237] Iteration 2200, loss = 1.78638
I0519 17:17:17.040427 22126 solver.cpp:253]     Train net output #0: loss = 1.78638 (* 1 = 1.78638 loss)
I0519 17:17:17.040452 22126 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0519 17:17:22.857877 22126 solver.cpp:237] Iteration 2300, loss = 1.55122
I0519 17:17:22.857913 22126 solver.cpp:253]     Train net output #0: loss = 1.55122 (* 1 = 1.55122 loss)
I0519 17:17:22.857935 22126 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0519 17:17:28.677058 22126 solver.cpp:237] Iteration 2400, loss = 1.97085
I0519 17:17:28.677095 22126 solver.cpp:253]     Train net output #0: loss = 1.97085 (* 1 = 1.97085 loss)
I0519 17:17:28.677114 22126 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0519 17:17:34.497797 22126 solver.cpp:237] Iteration 2500, loss = 1.55693
I0519 17:17:34.497853 22126 solver.cpp:253]     Train net output #0: loss = 1.55693 (* 1 = 1.55693 loss)
I0519 17:17:34.497879 22126 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0519 17:17:40.325755 22126 solver.cpp:237] Iteration 2600, loss = 1.58446
I0519 17:17:40.325911 22126 solver.cpp:253]     Train net output #0: loss = 1.58446 (* 1 = 1.58446 loss)
I0519 17:17:40.325928 22126 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0519 17:17:46.151357 22126 solver.cpp:237] Iteration 2700, loss = 1.83886
I0519 17:17:46.151393 22126 solver.cpp:253]     Train net output #0: loss = 1.83886 (* 1 = 1.83886 loss)
I0519 17:17:46.151412 22126 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0519 17:17:51.972427 22126 solver.cpp:237] Iteration 2800, loss = 1.74057
I0519 17:17:51.972462 22126 solver.cpp:253]     Train net output #0: loss = 1.74057 (* 1 = 1.74057 loss)
I0519 17:17:51.972486 22126 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0519 17:17:57.797230 22126 solver.cpp:237] Iteration 2900, loss = 1.56999
I0519 17:17:57.797283 22126 solver.cpp:253]     Train net output #0: loss = 1.56999 (* 1 = 1.56999 loss)
I0519 17:17:57.797312 22126 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0519 17:18:03.565929 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_3000.caffemodel
I0519 17:18:03.643944 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_3000.solverstate
I0519 17:18:03.674669 22126 solver.cpp:341] Iteration 3000, Testing net (#0)
I0519 17:18:50.592103 22126 solver.cpp:409]     Test net output #0: accuracy = 0.682672
I0519 17:18:50.592280 22126 solver.cpp:409]     Test net output #1: loss = 1.10818 (* 1 = 1.10818 loss)
I0519 17:19:12.743813 22126 solver.cpp:237] Iteration 3000, loss = 1.63045
I0519 17:19:12.743873 22126 solver.cpp:253]     Train net output #0: loss = 1.63045 (* 1 = 1.63045 loss)
I0519 17:19:12.743894 22126 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0519 17:19:18.556627 22126 solver.cpp:237] Iteration 3100, loss = 1.61831
I0519 17:19:18.556664 22126 solver.cpp:253]     Train net output #0: loss = 1.61831 (* 1 = 1.61831 loss)
I0519 17:19:18.556684 22126 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0519 17:19:24.375133 22126 solver.cpp:237] Iteration 3200, loss = 1.72629
I0519 17:19:24.375280 22126 solver.cpp:253]     Train net output #0: loss = 1.72629 (* 1 = 1.72629 loss)
I0519 17:19:24.375298 22126 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0519 17:19:30.192407 22126 solver.cpp:237] Iteration 3300, loss = 1.62415
I0519 17:19:30.192443 22126 solver.cpp:253]     Train net output #0: loss = 1.62415 (* 1 = 1.62415 loss)
I0519 17:19:30.192461 22126 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0519 17:19:36.008764 22126 solver.cpp:237] Iteration 3400, loss = 1.55284
I0519 17:19:36.008819 22126 solver.cpp:253]     Train net output #0: loss = 1.55284 (* 1 = 1.55284 loss)
I0519 17:19:36.008836 22126 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0519 17:19:41.821370 22126 solver.cpp:237] Iteration 3500, loss = 1.70669
I0519 17:19:41.821408 22126 solver.cpp:253]     Train net output #0: loss = 1.70669 (* 1 = 1.70669 loss)
I0519 17:19:41.821426 22126 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0519 17:19:47.637683 22126 solver.cpp:237] Iteration 3600, loss = 1.61061
I0519 17:19:47.637719 22126 solver.cpp:253]     Train net output #0: loss = 1.61061 (* 1 = 1.61061 loss)
I0519 17:19:47.637737 22126 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0519 17:19:53.451117 22126 solver.cpp:237] Iteration 3700, loss = 1.63587
I0519 17:19:53.451153 22126 solver.cpp:253]     Train net output #0: loss = 1.63587 (* 1 = 1.63587 loss)
I0519 17:19:53.451177 22126 sgd_solver.cpp:106] Iteration 3700, lr = 0.0025
I0519 17:19:59.265856 22126 solver.cpp:237] Iteration 3800, loss = 1.48378
I0519 17:19:59.266000 22126 solver.cpp:253]     Train net output #0: loss = 1.48378 (* 1 = 1.48378 loss)
I0519 17:19:59.266016 22126 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0519 17:20:05.083178 22126 solver.cpp:237] Iteration 3900, loss = 1.61434
I0519 17:20:05.083230 22126 solver.cpp:253]     Train net output #0: loss = 1.61434 (* 1 = 1.61434 loss)
I0519 17:20:05.083261 22126 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0519 17:20:10.845764 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_4000.caffemodel
I0519 17:20:10.926085 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_4000.solverstate
I0519 17:20:33.180187 22126 solver.cpp:237] Iteration 4000, loss = 1.55034
I0519 17:20:33.180377 22126 solver.cpp:253]     Train net output #0: loss = 1.55034 (* 1 = 1.55034 loss)
I0519 17:20:33.180394 22126 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0519 17:20:38.995900 22126 solver.cpp:237] Iteration 4100, loss = 1.49279
I0519 17:20:38.995937 22126 solver.cpp:253]     Train net output #0: loss = 1.49279 (* 1 = 1.49279 loss)
I0519 17:20:38.995956 22126 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0519 17:20:44.809393 22126 solver.cpp:237] Iteration 4200, loss = 1.48329
I0519 17:20:44.809430 22126 solver.cpp:253]     Train net output #0: loss = 1.48329 (* 1 = 1.48329 loss)
I0519 17:20:44.809454 22126 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0519 17:20:50.627323 22126 solver.cpp:237] Iteration 4300, loss = 1.64476
I0519 17:20:50.627382 22126 solver.cpp:253]     Train net output #0: loss = 1.64476 (* 1 = 1.64476 loss)
I0519 17:20:50.627408 22126 sgd_solver.cpp:106] Iteration 4300, lr = 0.0025
I0519 17:20:56.444921 22126 solver.cpp:237] Iteration 4400, loss = 1.49598
I0519 17:20:56.444957 22126 solver.cpp:253]     Train net output #0: loss = 1.49598 (* 1 = 1.49598 loss)
I0519 17:20:56.444977 22126 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0519 17:21:02.263741 22126 solver.cpp:237] Iteration 4500, loss = 1.68025
I0519 17:21:02.263778 22126 solver.cpp:253]     Train net output #0: loss = 1.68025 (* 1 = 1.68025 loss)
I0519 17:21:02.263797 22126 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0519 17:21:08.082355 22126 solver.cpp:237] Iteration 4600, loss = 1.43683
I0519 17:21:08.082509 22126 solver.cpp:253]     Train net output #0: loss = 1.43683 (* 1 = 1.43683 loss)
I0519 17:21:08.082525 22126 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0519 17:21:13.897886 22126 solver.cpp:237] Iteration 4700, loss = 1.30811
I0519 17:21:13.897920 22126 solver.cpp:253]     Train net output #0: loss = 1.30811 (* 1 = 1.30811 loss)
I0519 17:21:13.897944 22126 sgd_solver.cpp:106] Iteration 4700, lr = 0.0025
I0519 17:21:19.710352 22126 solver.cpp:237] Iteration 4800, loss = 1.54633
I0519 17:21:19.710409 22126 solver.cpp:253]     Train net output #0: loss = 1.54633 (* 1 = 1.54633 loss)
I0519 17:21:19.710433 22126 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0519 17:21:25.527730 22126 solver.cpp:237] Iteration 4900, loss = 1.57961
I0519 17:21:25.527766 22126 solver.cpp:253]     Train net output #0: loss = 1.57961 (* 1 = 1.57961 loss)
I0519 17:21:25.527791 22126 sgd_solver.cpp:106] Iteration 4900, lr = 0.0025
I0519 17:21:31.286298 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_5000.caffemodel
I0519 17:21:31.366940 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_5000.solverstate
I0519 17:21:53.588496 22126 solver.cpp:237] Iteration 5000, loss = 1.49575
I0519 17:21:53.588675 22126 solver.cpp:253]     Train net output #0: loss = 1.49575 (* 1 = 1.49575 loss)
I0519 17:21:53.588693 22126 sgd_solver.cpp:106] Iteration 5000, lr = 0.0025
I0519 17:21:59.406785 22126 solver.cpp:237] Iteration 5100, loss = 1.53116
I0519 17:21:59.406821 22126 solver.cpp:253]     Train net output #0: loss = 1.53116 (* 1 = 1.53116 loss)
I0519 17:21:59.406839 22126 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0519 17:22:05.228922 22126 solver.cpp:237] Iteration 5200, loss = 1.44306
I0519 17:22:05.228979 22126 solver.cpp:253]     Train net output #0: loss = 1.44306 (* 1 = 1.44306 loss)
I0519 17:22:05.229006 22126 sgd_solver.cpp:106] Iteration 5200, lr = 0.0025
I0519 17:22:11.045250 22126 solver.cpp:237] Iteration 5300, loss = 1.694
I0519 17:22:11.045286 22126 solver.cpp:253]     Train net output #0: loss = 1.694 (* 1 = 1.694 loss)
I0519 17:22:11.045312 22126 sgd_solver.cpp:106] Iteration 5300, lr = 0.0025
I0519 17:22:16.865540 22126 solver.cpp:237] Iteration 5400, loss = 1.50336
I0519 17:22:16.865576 22126 solver.cpp:253]     Train net output #0: loss = 1.50336 (* 1 = 1.50336 loss)
I0519 17:22:16.865595 22126 sgd_solver.cpp:106] Iteration 5400, lr = 0.0025
I0519 17:22:22.681010 22126 solver.cpp:237] Iteration 5500, loss = 1.49827
I0519 17:22:22.681046 22126 solver.cpp:253]     Train net output #0: loss = 1.49827 (* 1 = 1.49827 loss)
I0519 17:22:22.681069 22126 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0519 17:22:28.500581 22126 solver.cpp:237] Iteration 5600, loss = 1.47501
I0519 17:22:28.500736 22126 solver.cpp:253]     Train net output #0: loss = 1.47501 (* 1 = 1.47501 loss)
I0519 17:22:28.500753 22126 sgd_solver.cpp:106] Iteration 5600, lr = 0.0025
I0519 17:22:34.319290 22126 solver.cpp:237] Iteration 5700, loss = 1.52992
I0519 17:22:34.319342 22126 solver.cpp:253]     Train net output #0: loss = 1.52992 (* 1 = 1.52992 loss)
I0519 17:22:34.319360 22126 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0519 17:22:40.136802 22126 solver.cpp:237] Iteration 5800, loss = 1.57638
I0519 17:22:40.136839 22126 solver.cpp:253]     Train net output #0: loss = 1.57638 (* 1 = 1.57638 loss)
I0519 17:22:40.136857 22126 sgd_solver.cpp:106] Iteration 5800, lr = 0.0025
I0519 17:22:45.952644 22126 solver.cpp:237] Iteration 5900, loss = 1.53881
I0519 17:22:45.952680 22126 solver.cpp:253]     Train net output #0: loss = 1.53881 (* 1 = 1.53881 loss)
I0519 17:22:45.952703 22126 sgd_solver.cpp:106] Iteration 5900, lr = 0.0025
I0519 17:22:51.714666 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_6000.caffemodel
I0519 17:22:51.795408 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_6000.solverstate
I0519 17:22:51.823673 22126 solver.cpp:341] Iteration 6000, Testing net (#0)
I0519 17:23:59.548895 22126 solver.cpp:409]     Test net output #0: accuracy = 0.782102
I0519 17:23:59.549063 22126 solver.cpp:409]     Test net output #1: loss = 0.734495 (* 1 = 0.734495 loss)
I0519 17:24:21.734833 22126 solver.cpp:237] Iteration 6000, loss = 1.58847
I0519 17:24:21.734900 22126 solver.cpp:253]     Train net output #0: loss = 1.58847 (* 1 = 1.58847 loss)
I0519 17:24:21.734928 22126 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0519 17:24:27.567224 22126 solver.cpp:237] Iteration 6100, loss = 1.48265
I0519 17:24:27.567261 22126 solver.cpp:253]     Train net output #0: loss = 1.48265 (* 1 = 1.48265 loss)
I0519 17:24:27.567281 22126 sgd_solver.cpp:106] Iteration 6100, lr = 0.0025
I0519 17:24:33.395959 22126 solver.cpp:237] Iteration 6200, loss = 1.51615
I0519 17:24:33.396113 22126 solver.cpp:253]     Train net output #0: loss = 1.51615 (* 1 = 1.51615 loss)
I0519 17:24:33.396131 22126 sgd_solver.cpp:106] Iteration 6200, lr = 0.0025
I0519 17:24:39.230367 22126 solver.cpp:237] Iteration 6300, loss = 1.5318
I0519 17:24:39.230413 22126 solver.cpp:253]     Train net output #0: loss = 1.5318 (* 1 = 1.5318 loss)
I0519 17:24:39.230440 22126 sgd_solver.cpp:106] Iteration 6300, lr = 0.0025
I0519 17:24:45.060735 22126 solver.cpp:237] Iteration 6400, loss = 1.50551
I0519 17:24:45.060773 22126 solver.cpp:253]     Train net output #0: loss = 1.50551 (* 1 = 1.50551 loss)
I0519 17:24:45.060791 22126 sgd_solver.cpp:106] Iteration 6400, lr = 0.0025
I0519 17:24:50.895300 22126 solver.cpp:237] Iteration 6500, loss = 1.52085
I0519 17:24:50.895336 22126 solver.cpp:253]     Train net output #0: loss = 1.52085 (* 1 = 1.52085 loss)
I0519 17:24:50.895355 22126 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0519 17:24:56.723784 22126 solver.cpp:237] Iteration 6600, loss = 1.54944
I0519 17:24:56.723821 22126 solver.cpp:253]     Train net output #0: loss = 1.54944 (* 1 = 1.54944 loss)
I0519 17:24:56.723839 22126 sgd_solver.cpp:106] Iteration 6600, lr = 0.0025
I0519 17:25:02.556794 22126 solver.cpp:237] Iteration 6700, loss = 1.44909
I0519 17:25:02.556846 22126 solver.cpp:253]     Train net output #0: loss = 1.44909 (* 1 = 1.44909 loss)
I0519 17:25:02.556872 22126 sgd_solver.cpp:106] Iteration 6700, lr = 0.0025
I0519 17:25:08.385967 22126 solver.cpp:237] Iteration 6800, loss = 1.5392
I0519 17:25:08.386111 22126 solver.cpp:253]     Train net output #0: loss = 1.5392 (* 1 = 1.5392 loss)
I0519 17:25:08.386128 22126 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0519 17:25:14.218448 22126 solver.cpp:237] Iteration 6900, loss = 1.42823
I0519 17:25:14.218483 22126 solver.cpp:253]     Train net output #0: loss = 1.42823 (* 1 = 1.42823 loss)
I0519 17:25:14.218507 22126 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0519 17:25:19.993181 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_7000.caffemodel
I0519 17:25:20.073431 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_7000.solverstate
I0519 17:25:42.312330 22126 solver.cpp:237] Iteration 7000, loss = 1.36473
I0519 17:25:42.312522 22126 solver.cpp:253]     Train net output #0: loss = 1.36473 (* 1 = 1.36473 loss)
I0519 17:25:42.312551 22126 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0519 17:25:48.144433 22126 solver.cpp:237] Iteration 7100, loss = 1.364
I0519 17:25:48.144470 22126 solver.cpp:253]     Train net output #0: loss = 1.364 (* 1 = 1.364 loss)
I0519 17:25:48.144490 22126 sgd_solver.cpp:106] Iteration 7100, lr = 0.0025
I0519 17:25:53.978560 22126 solver.cpp:237] Iteration 7200, loss = 1.40799
I0519 17:25:53.978610 22126 solver.cpp:253]     Train net output #0: loss = 1.40799 (* 1 = 1.40799 loss)
I0519 17:25:53.978637 22126 sgd_solver.cpp:106] Iteration 7200, lr = 0.0025
I0519 17:25:59.811372 22126 solver.cpp:237] Iteration 7300, loss = 1.30326
I0519 17:25:59.811408 22126 solver.cpp:253]     Train net output #0: loss = 1.30326 (* 1 = 1.30326 loss)
I0519 17:25:59.811427 22126 sgd_solver.cpp:106] Iteration 7300, lr = 0.0025
I0519 17:26:05.643152 22126 solver.cpp:237] Iteration 7400, loss = 1.28873
I0519 17:26:05.643188 22126 solver.cpp:253]     Train net output #0: loss = 1.28873 (* 1 = 1.28873 loss)
I0519 17:26:05.643211 22126 sgd_solver.cpp:106] Iteration 7400, lr = 0.0025
I0519 17:26:11.474850 22126 solver.cpp:237] Iteration 7500, loss = 1.36649
I0519 17:26:11.474891 22126 solver.cpp:253]     Train net output #0: loss = 1.36649 (* 1 = 1.36649 loss)
I0519 17:26:11.474908 22126 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0519 17:26:17.304587 22126 solver.cpp:237] Iteration 7600, loss = 1.38617
I0519 17:26:17.304754 22126 solver.cpp:253]     Train net output #0: loss = 1.38617 (* 1 = 1.38617 loss)
I0519 17:26:17.304774 22126 sgd_solver.cpp:106] Iteration 7600, lr = 0.0025
I0519 17:26:23.132921 22126 solver.cpp:237] Iteration 7700, loss = 1.36223
I0519 17:26:23.132957 22126 solver.cpp:253]     Train net output #0: loss = 1.36223 (* 1 = 1.36223 loss)
I0519 17:26:23.132982 22126 sgd_solver.cpp:106] Iteration 7700, lr = 0.0025
I0519 17:26:28.967545 22126 solver.cpp:237] Iteration 7800, loss = 1.16656
I0519 17:26:28.967581 22126 solver.cpp:253]     Train net output #0: loss = 1.16656 (* 1 = 1.16656 loss)
I0519 17:26:28.967599 22126 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0519 17:26:34.800288 22126 solver.cpp:237] Iteration 7900, loss = 1.46568
I0519 17:26:34.800324 22126 solver.cpp:253]     Train net output #0: loss = 1.46568 (* 1 = 1.46568 loss)
I0519 17:26:34.800348 22126 sgd_solver.cpp:106] Iteration 7900, lr = 0.0025
I0519 17:26:40.573072 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_8000.caffemodel
I0519 17:26:40.653040 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_8000.solverstate
I0519 17:27:02.891264 22126 solver.cpp:237] Iteration 8000, loss = 1.36149
I0519 17:27:02.891445 22126 solver.cpp:253]     Train net output #0: loss = 1.36149 (* 1 = 1.36149 loss)
I0519 17:27:02.891463 22126 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0519 17:27:08.725359 22126 solver.cpp:237] Iteration 8100, loss = 1.13476
I0519 17:27:08.725415 22126 solver.cpp:253]     Train net output #0: loss = 1.13476 (* 1 = 1.13476 loss)
I0519 17:27:08.725433 22126 sgd_solver.cpp:106] Iteration 8100, lr = 0.0025
I0519 17:27:14.559206 22126 solver.cpp:237] Iteration 8200, loss = 1.39215
I0519 17:27:14.559242 22126 solver.cpp:253]     Train net output #0: loss = 1.39215 (* 1 = 1.39215 loss)
I0519 17:27:14.559265 22126 sgd_solver.cpp:106] Iteration 8200, lr = 0.0025
I0519 17:27:20.393249 22126 solver.cpp:237] Iteration 8300, loss = 1.43163
I0519 17:27:20.393285 22126 solver.cpp:253]     Train net output #0: loss = 1.43163 (* 1 = 1.43163 loss)
I0519 17:27:20.393303 22126 sgd_solver.cpp:106] Iteration 8300, lr = 0.0025
I0519 17:27:26.226429 22126 solver.cpp:237] Iteration 8400, loss = 1.30031
I0519 17:27:26.226465 22126 solver.cpp:253]     Train net output #0: loss = 1.30031 (* 1 = 1.30031 loss)
I0519 17:27:26.226490 22126 sgd_solver.cpp:106] Iteration 8400, lr = 0.0025
I0519 17:27:32.055379 22126 solver.cpp:237] Iteration 8500, loss = 1.55229
I0519 17:27:32.055439 22126 solver.cpp:253]     Train net output #0: loss = 1.55229 (* 1 = 1.55229 loss)
I0519 17:27:32.055466 22126 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0519 17:27:37.888176 22126 solver.cpp:237] Iteration 8600, loss = 1.2837
I0519 17:27:37.888352 22126 solver.cpp:253]     Train net output #0: loss = 1.2837 (* 1 = 1.2837 loss)
I0519 17:27:37.888370 22126 sgd_solver.cpp:106] Iteration 8600, lr = 0.0025
I0519 17:27:43.721750 22126 solver.cpp:237] Iteration 8700, loss = 1.51504
I0519 17:27:43.721786 22126 solver.cpp:253]     Train net output #0: loss = 1.51504 (* 1 = 1.51504 loss)
I0519 17:27:43.721810 22126 sgd_solver.cpp:106] Iteration 8700, lr = 0.0025
I0519 17:27:49.551255 22126 solver.cpp:237] Iteration 8800, loss = 1.47225
I0519 17:27:49.551292 22126 solver.cpp:253]     Train net output #0: loss = 1.47225 (* 1 = 1.47225 loss)
I0519 17:27:49.551309 22126 sgd_solver.cpp:106] Iteration 8800, lr = 0.0025
I0519 17:27:55.386430 22126 solver.cpp:237] Iteration 8900, loss = 1.47196
I0519 17:27:55.386487 22126 solver.cpp:253]     Train net output #0: loss = 1.47196 (* 1 = 1.47196 loss)
I0519 17:27:55.386503 22126 sgd_solver.cpp:106] Iteration 8900, lr = 0.0025
I0519 17:28:01.159847 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_9000.caffemodel
I0519 17:28:01.238749 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_9000.solverstate
I0519 17:28:01.265642 22126 solver.cpp:341] Iteration 9000, Testing net (#0)
I0519 17:28:47.855931 22126 solver.cpp:409]     Test net output #0: accuracy = 0.816267
I0519 17:28:47.856106 22126 solver.cpp:409]     Test net output #1: loss = 0.61041 (* 1 = 0.61041 loss)
I0519 17:29:10.008679 22126 solver.cpp:237] Iteration 9000, loss = 1.21521
I0519 17:29:10.008741 22126 solver.cpp:253]     Train net output #0: loss = 1.21521 (* 1 = 1.21521 loss)
I0519 17:29:10.008767 22126 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0519 17:29:15.813161 22126 solver.cpp:237] Iteration 9100, loss = 1.291
I0519 17:29:15.813197 22126 solver.cpp:253]     Train net output #0: loss = 1.291 (* 1 = 1.291 loss)
I0519 17:29:15.813220 22126 sgd_solver.cpp:106] Iteration 9100, lr = 0.0025
I0519 17:29:21.618477 22126 solver.cpp:237] Iteration 9200, loss = 1.28533
I0519 17:29:21.618645 22126 solver.cpp:253]     Train net output #0: loss = 1.28533 (* 1 = 1.28533 loss)
I0519 17:29:21.618662 22126 sgd_solver.cpp:106] Iteration 9200, lr = 0.0025
I0519 17:29:27.422997 22126 solver.cpp:237] Iteration 9300, loss = 1.30902
I0519 17:29:27.423034 22126 solver.cpp:253]     Train net output #0: loss = 1.30902 (* 1 = 1.30902 loss)
I0519 17:29:27.423053 22126 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0519 17:29:33.230584 22126 solver.cpp:237] Iteration 9400, loss = 1.40742
I0519 17:29:33.230620 22126 solver.cpp:253]     Train net output #0: loss = 1.40742 (* 1 = 1.40742 loss)
I0519 17:29:33.230639 22126 sgd_solver.cpp:106] Iteration 9400, lr = 0.0025
I0519 17:29:39.039412 22126 solver.cpp:237] Iteration 9500, loss = 1.14385
I0519 17:29:39.039465 22126 solver.cpp:253]     Train net output #0: loss = 1.14385 (* 1 = 1.14385 loss)
I0519 17:29:39.039495 22126 sgd_solver.cpp:106] Iteration 9500, lr = 0.0025
I0519 17:29:44.847430 22126 solver.cpp:237] Iteration 9600, loss = 1.28053
I0519 17:29:44.847466 22126 solver.cpp:253]     Train net output #0: loss = 1.28053 (* 1 = 1.28053 loss)
I0519 17:29:44.847486 22126 sgd_solver.cpp:106] Iteration 9600, lr = 0.0025
I0519 17:29:50.654065 22126 solver.cpp:237] Iteration 9700, loss = 1.32055
I0519 17:29:50.654101 22126 solver.cpp:253]     Train net output #0: loss = 1.32055 (* 1 = 1.32055 loss)
I0519 17:29:50.654119 22126 sgd_solver.cpp:106] Iteration 9700, lr = 0.0025
I0519 17:29:56.462034 22126 solver.cpp:237] Iteration 9800, loss = 1.51335
I0519 17:29:56.462190 22126 solver.cpp:253]     Train net output #0: loss = 1.51335 (* 1 = 1.51335 loss)
I0519 17:29:56.462208 22126 sgd_solver.cpp:106] Iteration 9800, lr = 0.0025
I0519 17:30:02.268715 22126 solver.cpp:237] Iteration 9900, loss = 1.38402
I0519 17:30:02.268774 22126 solver.cpp:253]     Train net output #0: loss = 1.38402 (* 1 = 1.38402 loss)
I0519 17:30:02.268800 22126 sgd_solver.cpp:106] Iteration 9900, lr = 0.0025
I0519 17:30:08.017818 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_10000.caffemodel
I0519 17:30:08.095698 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_10000.solverstate
I0519 17:30:30.322039 22126 solver.cpp:237] Iteration 10000, loss = 1.3747
I0519 17:30:30.322224 22126 solver.cpp:253]     Train net output #0: loss = 1.3747 (* 1 = 1.3747 loss)
I0519 17:30:30.322242 22126 sgd_solver.cpp:106] Iteration 10000, lr = 0.0025
I0519 17:30:36.127507 22126 solver.cpp:237] Iteration 10100, loss = 1.28715
I0519 17:30:36.127543 22126 solver.cpp:253]     Train net output #0: loss = 1.28715 (* 1 = 1.28715 loss)
I0519 17:30:36.127568 22126 sgd_solver.cpp:106] Iteration 10100, lr = 0.0025
I0519 17:30:41.930131 22126 solver.cpp:237] Iteration 10200, loss = 1.46575
I0519 17:30:41.930168 22126 solver.cpp:253]     Train net output #0: loss = 1.46575 (* 1 = 1.46575 loss)
I0519 17:30:41.930191 22126 sgd_solver.cpp:106] Iteration 10200, lr = 0.0025
I0519 17:30:47.735033 22126 solver.cpp:237] Iteration 10300, loss = 1.28094
I0519 17:30:47.735069 22126 solver.cpp:253]     Train net output #0: loss = 1.28094 (* 1 = 1.28094 loss)
I0519 17:30:47.735091 22126 sgd_solver.cpp:106] Iteration 10300, lr = 0.0025
I0519 17:30:53.540716 22126 solver.cpp:237] Iteration 10400, loss = 1.4373
I0519 17:30:53.540773 22126 solver.cpp:253]     Train net output #0: loss = 1.4373 (* 1 = 1.4373 loss)
I0519 17:30:53.540798 22126 sgd_solver.cpp:106] Iteration 10400, lr = 0.0025
I0519 17:30:59.347838 22126 solver.cpp:237] Iteration 10500, loss = 1.3626
I0519 17:30:59.347878 22126 solver.cpp:253]     Train net output #0: loss = 1.3626 (* 1 = 1.3626 loss)
I0519 17:30:59.347895 22126 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0519 17:31:05.153432 22126 solver.cpp:237] Iteration 10600, loss = 1.28945
I0519 17:31:05.153581 22126 solver.cpp:253]     Train net output #0: loss = 1.28945 (* 1 = 1.28945 loss)
I0519 17:31:05.153599 22126 sgd_solver.cpp:106] Iteration 10600, lr = 0.0025
I0519 17:31:10.960968 22126 solver.cpp:237] Iteration 10700, loss = 1.18401
I0519 17:31:10.961005 22126 solver.cpp:253]     Train net output #0: loss = 1.18401 (* 1 = 1.18401 loss)
I0519 17:31:10.961022 22126 sgd_solver.cpp:106] Iteration 10700, lr = 0.0025
I0519 17:31:16.768795 22126 solver.cpp:237] Iteration 10800, loss = 1.28186
I0519 17:31:16.768851 22126 solver.cpp:253]     Train net output #0: loss = 1.28186 (* 1 = 1.28186 loss)
I0519 17:31:16.768874 22126 sgd_solver.cpp:106] Iteration 10800, lr = 0.0025
I0519 17:31:22.575736 22126 solver.cpp:237] Iteration 10900, loss = 1.39536
I0519 17:31:22.575772 22126 solver.cpp:253]     Train net output #0: loss = 1.39536 (* 1 = 1.39536 loss)
I0519 17:31:22.575794 22126 sgd_solver.cpp:106] Iteration 10900, lr = 0.0025
I0519 17:31:28.326939 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_11000.caffemodel
I0519 17:31:28.414499 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_11000.solverstate
I0519 17:31:50.651957 22126 solver.cpp:237] Iteration 11000, loss = 1.25486
I0519 17:31:50.652143 22126 solver.cpp:253]     Train net output #0: loss = 1.25486 (* 1 = 1.25486 loss)
I0519 17:31:50.652163 22126 sgd_solver.cpp:106] Iteration 11000, lr = 0.0025
I0519 17:31:56.455675 22126 solver.cpp:237] Iteration 11100, loss = 1.26887
I0519 17:31:56.455713 22126 solver.cpp:253]     Train net output #0: loss = 1.26887 (* 1 = 1.26887 loss)
I0519 17:31:56.455736 22126 sgd_solver.cpp:106] Iteration 11100, lr = 0.0025
I0519 17:32:02.261659 22126 solver.cpp:237] Iteration 11200, loss = 1.31476
I0519 17:32:02.261696 22126 solver.cpp:253]     Train net output #0: loss = 1.31476 (* 1 = 1.31476 loss)
I0519 17:32:02.261718 22126 sgd_solver.cpp:106] Iteration 11200, lr = 0.0025
I0519 17:32:08.068994 22126 solver.cpp:237] Iteration 11300, loss = 1.22996
I0519 17:32:08.069052 22126 solver.cpp:253]     Train net output #0: loss = 1.22996 (* 1 = 1.22996 loss)
I0519 17:32:08.069079 22126 sgd_solver.cpp:106] Iteration 11300, lr = 0.0025
I0519 17:32:13.878736 22126 solver.cpp:237] Iteration 11400, loss = 1.24847
I0519 17:32:13.878773 22126 solver.cpp:253]     Train net output #0: loss = 1.24847 (* 1 = 1.24847 loss)
I0519 17:32:13.878793 22126 sgd_solver.cpp:106] Iteration 11400, lr = 0.0025
I0519 17:32:19.685600 22126 solver.cpp:237] Iteration 11500, loss = 1.28462
I0519 17:32:19.685636 22126 solver.cpp:253]     Train net output #0: loss = 1.28462 (* 1 = 1.28462 loss)
I0519 17:32:19.685653 22126 sgd_solver.cpp:106] Iteration 11500, lr = 0.0025
I0519 17:32:25.493531 22126 solver.cpp:237] Iteration 11600, loss = 1.27597
I0519 17:32:25.493685 22126 solver.cpp:253]     Train net output #0: loss = 1.27597 (* 1 = 1.27597 loss)
I0519 17:32:25.493702 22126 sgd_solver.cpp:106] Iteration 11600, lr = 0.0025
I0519 17:32:31.306435 22126 solver.cpp:237] Iteration 11700, loss = 1.43006
I0519 17:32:31.306490 22126 solver.cpp:253]     Train net output #0: loss = 1.43006 (* 1 = 1.43006 loss)
I0519 17:32:31.306509 22126 sgd_solver.cpp:106] Iteration 11700, lr = 0.0025
I0519 17:32:37.114048 22126 solver.cpp:237] Iteration 11800, loss = 1.34295
I0519 17:32:37.114084 22126 solver.cpp:253]     Train net output #0: loss = 1.34295 (* 1 = 1.34295 loss)
I0519 17:32:37.114102 22126 sgd_solver.cpp:106] Iteration 11800, lr = 0.0025
I0519 17:32:42.918289 22126 solver.cpp:237] Iteration 11900, loss = 1.16474
I0519 17:32:42.918326 22126 solver.cpp:253]     Train net output #0: loss = 1.16474 (* 1 = 1.16474 loss)
I0519 17:32:42.918350 22126 sgd_solver.cpp:106] Iteration 11900, lr = 0.0025
I0519 17:32:48.669257 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_12000.caffemodel
I0519 17:32:48.747530 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_12000.solverstate
I0519 17:32:48.772822 22126 solver.cpp:341] Iteration 12000, Testing net (#0)
I0519 17:33:56.539311 22126 solver.cpp:409]     Test net output #0: accuracy = 0.841806
I0519 17:33:56.539487 22126 solver.cpp:409]     Test net output #1: loss = 0.559185 (* 1 = 0.559185 loss)
I0519 17:34:18.732576 22126 solver.cpp:237] Iteration 12000, loss = 1.24272
I0519 17:34:18.732637 22126 solver.cpp:253]     Train net output #0: loss = 1.24272 (* 1 = 1.24272 loss)
I0519 17:34:18.732666 22126 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0519 17:34:24.547011 22126 solver.cpp:237] Iteration 12100, loss = 1.33532
I0519 17:34:24.547049 22126 solver.cpp:253]     Train net output #0: loss = 1.33532 (* 1 = 1.33532 loss)
I0519 17:34:24.547072 22126 sgd_solver.cpp:106] Iteration 12100, lr = 0.0025
I0519 17:34:30.363265 22126 solver.cpp:237] Iteration 12200, loss = 1.217
I0519 17:34:30.363443 22126 solver.cpp:253]     Train net output #0: loss = 1.217 (* 1 = 1.217 loss)
I0519 17:34:30.363459 22126 sgd_solver.cpp:106] Iteration 12200, lr = 0.0025
I0519 17:34:36.179294 22126 solver.cpp:237] Iteration 12300, loss = 1.10197
I0519 17:34:36.179353 22126 solver.cpp:253]     Train net output #0: loss = 1.10197 (* 1 = 1.10197 loss)
I0519 17:34:36.179383 22126 sgd_solver.cpp:106] Iteration 12300, lr = 0.0025
I0519 17:34:41.991341 22126 solver.cpp:237] Iteration 12400, loss = 1.12727
I0519 17:34:41.991379 22126 solver.cpp:253]     Train net output #0: loss = 1.12727 (* 1 = 1.12727 loss)
I0519 17:34:41.991396 22126 sgd_solver.cpp:106] Iteration 12400, lr = 0.0025
I0519 17:34:47.806977 22126 solver.cpp:237] Iteration 12500, loss = 1.35102
I0519 17:34:47.807013 22126 solver.cpp:253]     Train net output #0: loss = 1.35102 (* 1 = 1.35102 loss)
I0519 17:34:47.807037 22126 sgd_solver.cpp:106] Iteration 12500, lr = 0.0025
I0519 17:34:53.624403 22126 solver.cpp:237] Iteration 12600, loss = 1.32705
I0519 17:34:53.624439 22126 solver.cpp:253]     Train net output #0: loss = 1.32705 (* 1 = 1.32705 loss)
I0519 17:34:53.624457 22126 sgd_solver.cpp:106] Iteration 12600, lr = 0.0025
I0519 17:34:59.439283 22126 solver.cpp:237] Iteration 12700, loss = 1.40796
I0519 17:34:59.439343 22126 solver.cpp:253]     Train net output #0: loss = 1.40796 (* 1 = 1.40796 loss)
I0519 17:34:59.439370 22126 sgd_solver.cpp:106] Iteration 12700, lr = 0.0025
I0519 17:35:05.259155 22126 solver.cpp:237] Iteration 12800, loss = 1.26293
I0519 17:35:05.259308 22126 solver.cpp:253]     Train net output #0: loss = 1.26293 (* 1 = 1.26293 loss)
I0519 17:35:05.259325 22126 sgd_solver.cpp:106] Iteration 12800, lr = 0.0025
I0519 17:35:11.078402 22126 solver.cpp:237] Iteration 12900, loss = 1.34332
I0519 17:35:11.078438 22126 solver.cpp:253]     Train net output #0: loss = 1.34332 (* 1 = 1.34332 loss)
I0519 17:35:11.078455 22126 sgd_solver.cpp:106] Iteration 12900, lr = 0.0025
I0519 17:35:16.843346 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_13000.caffemodel
I0519 17:35:16.923856 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_13000.solverstate
I0519 17:35:39.118541 22126 solver.cpp:237] Iteration 13000, loss = 1.30143
I0519 17:35:39.118726 22126 solver.cpp:253]     Train net output #0: loss = 1.30143 (* 1 = 1.30143 loss)
I0519 17:35:39.118746 22126 sgd_solver.cpp:106] Iteration 13000, lr = 0.0025
I0519 17:35:44.939930 22126 solver.cpp:237] Iteration 13100, loss = 1.45907
I0519 17:35:44.939967 22126 solver.cpp:253]     Train net output #0: loss = 1.45907 (* 1 = 1.45907 loss)
I0519 17:35:44.939991 22126 sgd_solver.cpp:106] Iteration 13100, lr = 0.0025
I0519 17:35:50.760743 22126 solver.cpp:237] Iteration 13200, loss = 1.24187
I0519 17:35:50.760802 22126 solver.cpp:253]     Train net output #0: loss = 1.24187 (* 1 = 1.24187 loss)
I0519 17:35:50.760828 22126 sgd_solver.cpp:106] Iteration 13200, lr = 0.0025
I0519 17:35:56.576553 22126 solver.cpp:237] Iteration 13300, loss = 1.24823
I0519 17:35:56.576591 22126 solver.cpp:253]     Train net output #0: loss = 1.24823 (* 1 = 1.24823 loss)
I0519 17:35:56.576614 22126 sgd_solver.cpp:106] Iteration 13300, lr = 0.0025
I0519 17:36:02.393929 22126 solver.cpp:237] Iteration 13400, loss = 1.3793
I0519 17:36:02.393965 22126 solver.cpp:253]     Train net output #0: loss = 1.3793 (* 1 = 1.3793 loss)
I0519 17:36:02.393987 22126 sgd_solver.cpp:106] Iteration 13400, lr = 0.0025
I0519 17:36:08.208871 22126 solver.cpp:237] Iteration 13500, loss = 1.40206
I0519 17:36:08.208907 22126 solver.cpp:253]     Train net output #0: loss = 1.40206 (* 1 = 1.40206 loss)
I0519 17:36:08.208925 22126 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0519 17:36:14.027757 22126 solver.cpp:237] Iteration 13600, loss = 1.34702
I0519 17:36:14.027920 22126 solver.cpp:253]     Train net output #0: loss = 1.34702 (* 1 = 1.34702 loss)
I0519 17:36:14.027936 22126 sgd_solver.cpp:106] Iteration 13600, lr = 0.0025
I0519 17:36:19.851400 22126 solver.cpp:237] Iteration 13700, loss = 1.49961
I0519 17:36:19.851451 22126 solver.cpp:253]     Train net output #0: loss = 1.49961 (* 1 = 1.49961 loss)
I0519 17:36:19.851480 22126 sgd_solver.cpp:106] Iteration 13700, lr = 0.0025
I0519 17:36:25.673836 22126 solver.cpp:237] Iteration 13800, loss = 1.52418
I0519 17:36:25.673872 22126 solver.cpp:253]     Train net output #0: loss = 1.52418 (* 1 = 1.52418 loss)
I0519 17:36:25.673892 22126 sgd_solver.cpp:106] Iteration 13800, lr = 0.0025
I0519 17:36:31.496307 22126 solver.cpp:237] Iteration 13900, loss = 1.57152
I0519 17:36:31.496342 22126 solver.cpp:253]     Train net output #0: loss = 1.57152 (* 1 = 1.57152 loss)
I0519 17:36:31.496366 22126 sgd_solver.cpp:106] Iteration 13900, lr = 0.0025
I0519 17:36:37.265147 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_14000.caffemodel
I0519 17:36:37.344945 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_14000.solverstate
I0519 17:36:59.558881 22126 solver.cpp:237] Iteration 14000, loss = 1.24463
I0519 17:36:59.559078 22126 solver.cpp:253]     Train net output #0: loss = 1.24463 (* 1 = 1.24463 loss)
I0519 17:36:59.559097 22126 sgd_solver.cpp:106] Iteration 14000, lr = 0.0025
I0519 17:37:05.386757 22126 solver.cpp:237] Iteration 14100, loss = 1.17225
I0519 17:37:05.386811 22126 solver.cpp:253]     Train net output #0: loss = 1.17225 (* 1 = 1.17225 loss)
I0519 17:37:05.386828 22126 sgd_solver.cpp:106] Iteration 14100, lr = 0.0025
I0519 17:37:11.214457 22126 solver.cpp:237] Iteration 14200, loss = 1.14131
I0519 17:37:11.214494 22126 solver.cpp:253]     Train net output #0: loss = 1.14131 (* 1 = 1.14131 loss)
I0519 17:37:11.214512 22126 sgd_solver.cpp:106] Iteration 14200, lr = 0.0025
I0519 17:37:17.041465 22126 solver.cpp:237] Iteration 14300, loss = 1.4926
I0519 17:37:17.041501 22126 solver.cpp:253]     Train net output #0: loss = 1.4926 (* 1 = 1.4926 loss)
I0519 17:37:17.041524 22126 sgd_solver.cpp:106] Iteration 14300, lr = 0.0025
I0519 17:37:22.865195 22126 solver.cpp:237] Iteration 14400, loss = 1.25498
I0519 17:37:22.865231 22126 solver.cpp:253]     Train net output #0: loss = 1.25498 (* 1 = 1.25498 loss)
I0519 17:37:22.865254 22126 sgd_solver.cpp:106] Iteration 14400, lr = 0.0025
I0519 17:37:28.687448 22126 solver.cpp:237] Iteration 14500, loss = 1.27181
I0519 17:37:28.687484 22126 solver.cpp:253]     Train net output #0: loss = 1.27181 (* 1 = 1.27181 loss)
I0519 17:37:28.687502 22126 sgd_solver.cpp:106] Iteration 14500, lr = 0.0025
I0519 17:37:34.508566 22126 solver.cpp:237] Iteration 14600, loss = 1.34205
I0519 17:37:34.508724 22126 solver.cpp:253]     Train net output #0: loss = 1.34205 (* 1 = 1.34205 loss)
I0519 17:37:34.508743 22126 sgd_solver.cpp:106] Iteration 14600, lr = 0.0025
I0519 17:37:40.338621 22126 solver.cpp:237] Iteration 14700, loss = 1.23439
I0519 17:37:40.338656 22126 solver.cpp:253]     Train net output #0: loss = 1.23439 (* 1 = 1.23439 loss)
I0519 17:37:40.338675 22126 sgd_solver.cpp:106] Iteration 14700, lr = 0.0025
I0519 17:37:46.164048 22126 solver.cpp:237] Iteration 14800, loss = 1.28708
I0519 17:37:46.164084 22126 solver.cpp:253]     Train net output #0: loss = 1.28708 (* 1 = 1.28708 loss)
I0519 17:37:46.164108 22126 sgd_solver.cpp:106] Iteration 14800, lr = 0.0025
I0519 17:37:51.991397 22126 solver.cpp:237] Iteration 14900, loss = 1.28206
I0519 17:37:51.991433 22126 solver.cpp:253]     Train net output #0: loss = 1.28206 (* 1 = 1.28206 loss)
I0519 17:37:51.991452 22126 sgd_solver.cpp:106] Iteration 14900, lr = 0.0025
I0519 17:37:57.758121 22126 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_15000.caffemodel
I0519 17:37:57.838202 22126 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timelog_2016-05-19T17.12.36.226421_iter_15000.solverstate
I0519 17:38:18.763504 22126 solver.cpp:321] Iteration 15000, loss = 1.19631
I0519 17:38:18.763689 22126 solver.cpp:341] Iteration 15000, Testing net (#0)
I0519 17:39:05.599475 22126 solver.cpp:409]     Test net output #0: accuracy = 0.843532
I0519 17:39:05.599658 22126 solver.cpp:409]     Test net output #1: loss = 0.562652 (* 1 = 0.562652 loss)
I0519 17:39:05.599674 22126 solver.cpp:326] Optimization Done.
I0519 17:39:05.599686 22126 caffe.cpp:215] Optimization Done.
Application 11227698 resources: utime ~1347s, stime ~228s, Rss ~5328964, inblocks ~3744348, outblocks ~253585

real	26m20.353s
user	0m0.576s
sys	0m0.200s
