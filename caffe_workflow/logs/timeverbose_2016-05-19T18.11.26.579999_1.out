2804110
I0519 18:11:42.018273 12029 caffe.cpp:184] Using GPUs 0
I0519 18:11:42.445431 12029 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1500
test_interval: 3000
base_lr: 0.0025
display: 100
max_iter: 15000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt"
I0519 18:11:42.447769 12029 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0519 18:11:42.451170 12029 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0519 18:11:42.451233 12029 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0519 18:11:42.451576 12029 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0519 18:11:42.451762 12029 layer_factory.hpp:77] Creating layer data_hdf5
I0519 18:11:42.451786 12029 net.cpp:106] Creating Layer data_hdf5
I0519 18:11:42.451800 12029 net.cpp:411] data_hdf5 -> data
I0519 18:11:42.451833 12029 net.cpp:411] data_hdf5 -> label
I0519 18:11:42.451866 12029 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0519 18:11:42.470351 12029 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0519 18:11:42.508564 12029 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0519 18:12:04.001461 12029 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0519 18:12:04.006662 12029 net.cpp:150] Setting up data_hdf5
I0519 18:12:04.006707 12029 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0519 18:12:04.006722 12029 net.cpp:157] Top shape: 100 (100)
I0519 18:12:04.006733 12029 net.cpp:165] Memory required for data: 2540400
I0519 18:12:04.006747 12029 layer_factory.hpp:77] Creating layer conv1
I0519 18:12:04.006780 12029 net.cpp:106] Creating Layer conv1
I0519 18:12:04.006793 12029 net.cpp:454] conv1 <- data
I0519 18:12:04.006816 12029 net.cpp:411] conv1 -> conv1
I0519 18:12:04.884413 12029 net.cpp:150] Setting up conv1
I0519 18:12:04.884459 12029 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 18:12:04.884469 12029 net.cpp:165] Memory required for data: 30188400
I0519 18:12:04.884500 12029 layer_factory.hpp:77] Creating layer relu1
I0519 18:12:04.884522 12029 net.cpp:106] Creating Layer relu1
I0519 18:12:04.884533 12029 net.cpp:454] relu1 <- conv1
I0519 18:12:04.884546 12029 net.cpp:397] relu1 -> conv1 (in-place)
I0519 18:12:04.885063 12029 net.cpp:150] Setting up relu1
I0519 18:12:04.885081 12029 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 18:12:04.885090 12029 net.cpp:165] Memory required for data: 57836400
I0519 18:12:04.885100 12029 layer_factory.hpp:77] Creating layer pool1
I0519 18:12:04.885118 12029 net.cpp:106] Creating Layer pool1
I0519 18:12:04.885126 12029 net.cpp:454] pool1 <- conv1
I0519 18:12:04.885141 12029 net.cpp:411] pool1 -> pool1
I0519 18:12:04.885221 12029 net.cpp:150] Setting up pool1
I0519 18:12:04.885236 12029 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0519 18:12:04.885246 12029 net.cpp:165] Memory required for data: 71660400
I0519 18:12:04.885253 12029 layer_factory.hpp:77] Creating layer conv2
I0519 18:12:04.885277 12029 net.cpp:106] Creating Layer conv2
I0519 18:12:04.885287 12029 net.cpp:454] conv2 <- pool1
I0519 18:12:04.885298 12029 net.cpp:411] conv2 -> conv2
I0519 18:12:04.888039 12029 net.cpp:150] Setting up conv2
I0519 18:12:04.888062 12029 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 18:12:04.888078 12029 net.cpp:165] Memory required for data: 91532400
I0519 18:12:04.888098 12029 layer_factory.hpp:77] Creating layer relu2
I0519 18:12:04.888113 12029 net.cpp:106] Creating Layer relu2
I0519 18:12:04.888123 12029 net.cpp:454] relu2 <- conv2
I0519 18:12:04.888135 12029 net.cpp:397] relu2 -> conv2 (in-place)
I0519 18:12:04.888465 12029 net.cpp:150] Setting up relu2
I0519 18:12:04.888480 12029 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 18:12:04.888490 12029 net.cpp:165] Memory required for data: 111404400
I0519 18:12:04.888500 12029 layer_factory.hpp:77] Creating layer pool2
I0519 18:12:04.888514 12029 net.cpp:106] Creating Layer pool2
I0519 18:12:04.888523 12029 net.cpp:454] pool2 <- conv2
I0519 18:12:04.888535 12029 net.cpp:411] pool2 -> pool2
I0519 18:12:04.888617 12029 net.cpp:150] Setting up pool2
I0519 18:12:04.888631 12029 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0519 18:12:04.888641 12029 net.cpp:165] Memory required for data: 121340400
I0519 18:12:04.888651 12029 layer_factory.hpp:77] Creating layer conv3
I0519 18:12:04.888669 12029 net.cpp:106] Creating Layer conv3
I0519 18:12:04.888679 12029 net.cpp:454] conv3 <- pool2
I0519 18:12:04.888694 12029 net.cpp:411] conv3 -> conv3
I0519 18:12:04.890619 12029 net.cpp:150] Setting up conv3
I0519 18:12:04.890641 12029 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 18:12:04.890653 12029 net.cpp:165] Memory required for data: 132182000
I0519 18:12:04.890672 12029 layer_factory.hpp:77] Creating layer relu3
I0519 18:12:04.890688 12029 net.cpp:106] Creating Layer relu3
I0519 18:12:04.890698 12029 net.cpp:454] relu3 <- conv3
I0519 18:12:04.890710 12029 net.cpp:397] relu3 -> conv3 (in-place)
I0519 18:12:04.891181 12029 net.cpp:150] Setting up relu3
I0519 18:12:04.891199 12029 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 18:12:04.891209 12029 net.cpp:165] Memory required for data: 143023600
I0519 18:12:04.891219 12029 layer_factory.hpp:77] Creating layer pool3
I0519 18:12:04.891232 12029 net.cpp:106] Creating Layer pool3
I0519 18:12:04.891242 12029 net.cpp:454] pool3 <- conv3
I0519 18:12:04.891254 12029 net.cpp:411] pool3 -> pool3
I0519 18:12:04.891322 12029 net.cpp:150] Setting up pool3
I0519 18:12:04.891335 12029 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0519 18:12:04.891345 12029 net.cpp:165] Memory required for data: 148444400
I0519 18:12:04.891355 12029 layer_factory.hpp:77] Creating layer conv4
I0519 18:12:04.891371 12029 net.cpp:106] Creating Layer conv4
I0519 18:12:04.891381 12029 net.cpp:454] conv4 <- pool3
I0519 18:12:04.891396 12029 net.cpp:411] conv4 -> conv4
I0519 18:12:04.894366 12029 net.cpp:150] Setting up conv4
I0519 18:12:04.894397 12029 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 18:12:04.894407 12029 net.cpp:165] Memory required for data: 152073200
I0519 18:12:04.894423 12029 layer_factory.hpp:77] Creating layer relu4
I0519 18:12:04.894436 12029 net.cpp:106] Creating Layer relu4
I0519 18:12:04.894446 12029 net.cpp:454] relu4 <- conv4
I0519 18:12:04.894459 12029 net.cpp:397] relu4 -> conv4 (in-place)
I0519 18:12:04.894922 12029 net.cpp:150] Setting up relu4
I0519 18:12:04.894937 12029 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 18:12:04.894948 12029 net.cpp:165] Memory required for data: 155702000
I0519 18:12:04.894958 12029 layer_factory.hpp:77] Creating layer pool4
I0519 18:12:04.894971 12029 net.cpp:106] Creating Layer pool4
I0519 18:12:04.894981 12029 net.cpp:454] pool4 <- conv4
I0519 18:12:04.894994 12029 net.cpp:411] pool4 -> pool4
I0519 18:12:04.895061 12029 net.cpp:150] Setting up pool4
I0519 18:12:04.895076 12029 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0519 18:12:04.895086 12029 net.cpp:165] Memory required for data: 157516400
I0519 18:12:04.895094 12029 layer_factory.hpp:77] Creating layer ip1
I0519 18:12:04.895114 12029 net.cpp:106] Creating Layer ip1
I0519 18:12:04.895124 12029 net.cpp:454] ip1 <- pool4
I0519 18:12:04.895138 12029 net.cpp:411] ip1 -> ip1
I0519 18:12:04.910617 12029 net.cpp:150] Setting up ip1
I0519 18:12:04.910646 12029 net.cpp:157] Top shape: 100 196 (19600)
I0519 18:12:04.910660 12029 net.cpp:165] Memory required for data: 157594800
I0519 18:12:04.910681 12029 layer_factory.hpp:77] Creating layer relu5
I0519 18:12:04.910696 12029 net.cpp:106] Creating Layer relu5
I0519 18:12:04.910706 12029 net.cpp:454] relu5 <- ip1
I0519 18:12:04.910719 12029 net.cpp:397] relu5 -> ip1 (in-place)
I0519 18:12:04.911059 12029 net.cpp:150] Setting up relu5
I0519 18:12:04.911072 12029 net.cpp:157] Top shape: 100 196 (19600)
I0519 18:12:04.911082 12029 net.cpp:165] Memory required for data: 157673200
I0519 18:12:04.911092 12029 layer_factory.hpp:77] Creating layer drop1
I0519 18:12:04.911115 12029 net.cpp:106] Creating Layer drop1
I0519 18:12:04.911125 12029 net.cpp:454] drop1 <- ip1
I0519 18:12:04.911139 12029 net.cpp:397] drop1 -> ip1 (in-place)
I0519 18:12:04.911197 12029 net.cpp:150] Setting up drop1
I0519 18:12:04.911211 12029 net.cpp:157] Top shape: 100 196 (19600)
I0519 18:12:04.911221 12029 net.cpp:165] Memory required for data: 157751600
I0519 18:12:04.911229 12029 layer_factory.hpp:77] Creating layer ip2
I0519 18:12:04.911248 12029 net.cpp:106] Creating Layer ip2
I0519 18:12:04.911259 12029 net.cpp:454] ip2 <- ip1
I0519 18:12:04.911273 12029 net.cpp:411] ip2 -> ip2
I0519 18:12:04.911742 12029 net.cpp:150] Setting up ip2
I0519 18:12:04.911756 12029 net.cpp:157] Top shape: 100 98 (9800)
I0519 18:12:04.911767 12029 net.cpp:165] Memory required for data: 157790800
I0519 18:12:04.911782 12029 layer_factory.hpp:77] Creating layer relu6
I0519 18:12:04.911794 12029 net.cpp:106] Creating Layer relu6
I0519 18:12:04.911804 12029 net.cpp:454] relu6 <- ip2
I0519 18:12:04.911816 12029 net.cpp:397] relu6 -> ip2 (in-place)
I0519 18:12:04.912336 12029 net.cpp:150] Setting up relu6
I0519 18:12:04.912353 12029 net.cpp:157] Top shape: 100 98 (9800)
I0519 18:12:04.912364 12029 net.cpp:165] Memory required for data: 157830000
I0519 18:12:04.912372 12029 layer_factory.hpp:77] Creating layer drop2
I0519 18:12:04.912385 12029 net.cpp:106] Creating Layer drop2
I0519 18:12:04.912395 12029 net.cpp:454] drop2 <- ip2
I0519 18:12:04.912408 12029 net.cpp:397] drop2 -> ip2 (in-place)
I0519 18:12:04.912451 12029 net.cpp:150] Setting up drop2
I0519 18:12:04.912464 12029 net.cpp:157] Top shape: 100 98 (9800)
I0519 18:12:04.912474 12029 net.cpp:165] Memory required for data: 157869200
I0519 18:12:04.912484 12029 layer_factory.hpp:77] Creating layer ip3
I0519 18:12:04.912498 12029 net.cpp:106] Creating Layer ip3
I0519 18:12:04.912508 12029 net.cpp:454] ip3 <- ip2
I0519 18:12:04.912520 12029 net.cpp:411] ip3 -> ip3
I0519 18:12:04.912730 12029 net.cpp:150] Setting up ip3
I0519 18:12:04.912744 12029 net.cpp:157] Top shape: 100 11 (1100)
I0519 18:12:04.912753 12029 net.cpp:165] Memory required for data: 157873600
I0519 18:12:04.912768 12029 layer_factory.hpp:77] Creating layer drop3
I0519 18:12:04.912781 12029 net.cpp:106] Creating Layer drop3
I0519 18:12:04.912791 12029 net.cpp:454] drop3 <- ip3
I0519 18:12:04.912802 12029 net.cpp:397] drop3 -> ip3 (in-place)
I0519 18:12:04.912842 12029 net.cpp:150] Setting up drop3
I0519 18:12:04.912854 12029 net.cpp:157] Top shape: 100 11 (1100)
I0519 18:12:04.912865 12029 net.cpp:165] Memory required for data: 157878000
I0519 18:12:04.912874 12029 layer_factory.hpp:77] Creating layer loss
I0519 18:12:04.912894 12029 net.cpp:106] Creating Layer loss
I0519 18:12:04.912904 12029 net.cpp:454] loss <- ip3
I0519 18:12:04.912914 12029 net.cpp:454] loss <- label
I0519 18:12:04.912927 12029 net.cpp:411] loss -> loss
I0519 18:12:04.912945 12029 layer_factory.hpp:77] Creating layer loss
I0519 18:12:04.913589 12029 net.cpp:150] Setting up loss
I0519 18:12:04.913605 12029 net.cpp:157] Top shape: (1)
I0519 18:12:04.913614 12029 net.cpp:160]     with loss weight 1
I0519 18:12:04.913658 12029 net.cpp:165] Memory required for data: 157878004
I0519 18:12:04.913669 12029 net.cpp:226] loss needs backward computation.
I0519 18:12:04.913681 12029 net.cpp:226] drop3 needs backward computation.
I0519 18:12:04.913689 12029 net.cpp:226] ip3 needs backward computation.
I0519 18:12:04.913700 12029 net.cpp:226] drop2 needs backward computation.
I0519 18:12:04.913709 12029 net.cpp:226] relu6 needs backward computation.
I0519 18:12:04.913719 12029 net.cpp:226] ip2 needs backward computation.
I0519 18:12:04.913730 12029 net.cpp:226] drop1 needs backward computation.
I0519 18:12:04.913739 12029 net.cpp:226] relu5 needs backward computation.
I0519 18:12:04.913748 12029 net.cpp:226] ip1 needs backward computation.
I0519 18:12:04.913758 12029 net.cpp:226] pool4 needs backward computation.
I0519 18:12:04.913769 12029 net.cpp:226] relu4 needs backward computation.
I0519 18:12:04.913779 12029 net.cpp:226] conv4 needs backward computation.
I0519 18:12:04.913789 12029 net.cpp:226] pool3 needs backward computation.
I0519 18:12:04.913800 12029 net.cpp:226] relu3 needs backward computation.
I0519 18:12:04.913810 12029 net.cpp:226] conv3 needs backward computation.
I0519 18:12:04.913830 12029 net.cpp:226] pool2 needs backward computation.
I0519 18:12:04.913841 12029 net.cpp:226] relu2 needs backward computation.
I0519 18:12:04.913851 12029 net.cpp:226] conv2 needs backward computation.
I0519 18:12:04.913861 12029 net.cpp:226] pool1 needs backward computation.
I0519 18:12:04.913872 12029 net.cpp:226] relu1 needs backward computation.
I0519 18:12:04.913882 12029 net.cpp:226] conv1 needs backward computation.
I0519 18:12:04.913892 12029 net.cpp:228] data_hdf5 does not need backward computation.
I0519 18:12:04.913902 12029 net.cpp:270] This network produces output loss
I0519 18:12:04.913926 12029 net.cpp:283] Network initialization done.
I0519 18:12:04.915709 12029 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0519 18:12:04.915779 12029 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0519 18:12:04.916137 12029 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0519 18:12:04.916327 12029 layer_factory.hpp:77] Creating layer data_hdf5
I0519 18:12:04.916342 12029 net.cpp:106] Creating Layer data_hdf5
I0519 18:12:04.916354 12029 net.cpp:411] data_hdf5 -> data
I0519 18:12:04.916371 12029 net.cpp:411] data_hdf5 -> label
I0519 18:12:04.916388 12029 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0519 18:12:04.917836 12029 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0519 18:12:26.408649 12029 net.cpp:150] Setting up data_hdf5
I0519 18:12:26.408817 12029 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0519 18:12:26.408831 12029 net.cpp:157] Top shape: 100 (100)
I0519 18:12:26.408843 12029 net.cpp:165] Memory required for data: 2540400
I0519 18:12:26.408857 12029 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0519 18:12:26.408885 12029 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0519 18:12:26.408896 12029 net.cpp:454] label_data_hdf5_1_split <- label
I0519 18:12:26.408911 12029 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0519 18:12:26.408932 12029 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0519 18:12:26.409004 12029 net.cpp:150] Setting up label_data_hdf5_1_split
I0519 18:12:26.409018 12029 net.cpp:157] Top shape: 100 (100)
I0519 18:12:26.409029 12029 net.cpp:157] Top shape: 100 (100)
I0519 18:12:26.409039 12029 net.cpp:165] Memory required for data: 2541200
I0519 18:12:26.409050 12029 layer_factory.hpp:77] Creating layer conv1
I0519 18:12:26.409072 12029 net.cpp:106] Creating Layer conv1
I0519 18:12:26.409082 12029 net.cpp:454] conv1 <- data
I0519 18:12:26.409097 12029 net.cpp:411] conv1 -> conv1
I0519 18:12:26.411031 12029 net.cpp:150] Setting up conv1
I0519 18:12:26.411054 12029 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 18:12:26.411067 12029 net.cpp:165] Memory required for data: 30189200
I0519 18:12:26.411087 12029 layer_factory.hpp:77] Creating layer relu1
I0519 18:12:26.411100 12029 net.cpp:106] Creating Layer relu1
I0519 18:12:26.411110 12029 net.cpp:454] relu1 <- conv1
I0519 18:12:26.411123 12029 net.cpp:397] relu1 -> conv1 (in-place)
I0519 18:12:26.411705 12029 net.cpp:150] Setting up relu1
I0519 18:12:26.411726 12029 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0519 18:12:26.411736 12029 net.cpp:165] Memory required for data: 57837200
I0519 18:12:26.411747 12029 layer_factory.hpp:77] Creating layer pool1
I0519 18:12:26.411763 12029 net.cpp:106] Creating Layer pool1
I0519 18:12:26.411773 12029 net.cpp:454] pool1 <- conv1
I0519 18:12:26.411787 12029 net.cpp:411] pool1 -> pool1
I0519 18:12:26.411862 12029 net.cpp:150] Setting up pool1
I0519 18:12:26.411875 12029 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0519 18:12:26.411885 12029 net.cpp:165] Memory required for data: 71661200
I0519 18:12:26.411895 12029 layer_factory.hpp:77] Creating layer conv2
I0519 18:12:26.411913 12029 net.cpp:106] Creating Layer conv2
I0519 18:12:26.411923 12029 net.cpp:454] conv2 <- pool1
I0519 18:12:26.411937 12029 net.cpp:411] conv2 -> conv2
I0519 18:12:26.413866 12029 net.cpp:150] Setting up conv2
I0519 18:12:26.413882 12029 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 18:12:26.413893 12029 net.cpp:165] Memory required for data: 91533200
I0519 18:12:26.413910 12029 layer_factory.hpp:77] Creating layer relu2
I0519 18:12:26.413924 12029 net.cpp:106] Creating Layer relu2
I0519 18:12:26.413933 12029 net.cpp:454] relu2 <- conv2
I0519 18:12:26.413945 12029 net.cpp:397] relu2 -> conv2 (in-place)
I0519 18:12:26.414281 12029 net.cpp:150] Setting up relu2
I0519 18:12:26.414295 12029 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0519 18:12:26.414305 12029 net.cpp:165] Memory required for data: 111405200
I0519 18:12:26.414315 12029 layer_factory.hpp:77] Creating layer pool2
I0519 18:12:26.414329 12029 net.cpp:106] Creating Layer pool2
I0519 18:12:26.414338 12029 net.cpp:454] pool2 <- conv2
I0519 18:12:26.414350 12029 net.cpp:411] pool2 -> pool2
I0519 18:12:26.414423 12029 net.cpp:150] Setting up pool2
I0519 18:12:26.414436 12029 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0519 18:12:26.414446 12029 net.cpp:165] Memory required for data: 121341200
I0519 18:12:26.414456 12029 layer_factory.hpp:77] Creating layer conv3
I0519 18:12:26.414474 12029 net.cpp:106] Creating Layer conv3
I0519 18:12:26.414484 12029 net.cpp:454] conv3 <- pool2
I0519 18:12:26.414499 12029 net.cpp:411] conv3 -> conv3
I0519 18:12:26.416496 12029 net.cpp:150] Setting up conv3
I0519 18:12:26.416519 12029 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 18:12:26.416532 12029 net.cpp:165] Memory required for data: 132182800
I0519 18:12:26.416563 12029 layer_factory.hpp:77] Creating layer relu3
I0519 18:12:26.416577 12029 net.cpp:106] Creating Layer relu3
I0519 18:12:26.416587 12029 net.cpp:454] relu3 <- conv3
I0519 18:12:26.416600 12029 net.cpp:397] relu3 -> conv3 (in-place)
I0519 18:12:26.417075 12029 net.cpp:150] Setting up relu3
I0519 18:12:26.417091 12029 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0519 18:12:26.417103 12029 net.cpp:165] Memory required for data: 143024400
I0519 18:12:26.417112 12029 layer_factory.hpp:77] Creating layer pool3
I0519 18:12:26.417125 12029 net.cpp:106] Creating Layer pool3
I0519 18:12:26.417135 12029 net.cpp:454] pool3 <- conv3
I0519 18:12:26.417151 12029 net.cpp:411] pool3 -> pool3
I0519 18:12:26.417228 12029 net.cpp:150] Setting up pool3
I0519 18:12:26.417242 12029 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0519 18:12:26.417251 12029 net.cpp:165] Memory required for data: 148445200
I0519 18:12:26.417261 12029 layer_factory.hpp:77] Creating layer conv4
I0519 18:12:26.417279 12029 net.cpp:106] Creating Layer conv4
I0519 18:12:26.417289 12029 net.cpp:454] conv4 <- pool3
I0519 18:12:26.417304 12029 net.cpp:411] conv4 -> conv4
I0519 18:12:26.419394 12029 net.cpp:150] Setting up conv4
I0519 18:12:26.419416 12029 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 18:12:26.419428 12029 net.cpp:165] Memory required for data: 152074000
I0519 18:12:26.419443 12029 layer_factory.hpp:77] Creating layer relu4
I0519 18:12:26.419456 12029 net.cpp:106] Creating Layer relu4
I0519 18:12:26.419466 12029 net.cpp:454] relu4 <- conv4
I0519 18:12:26.419479 12029 net.cpp:397] relu4 -> conv4 (in-place)
I0519 18:12:26.419960 12029 net.cpp:150] Setting up relu4
I0519 18:12:26.419976 12029 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0519 18:12:26.419986 12029 net.cpp:165] Memory required for data: 155702800
I0519 18:12:26.419996 12029 layer_factory.hpp:77] Creating layer pool4
I0519 18:12:26.420011 12029 net.cpp:106] Creating Layer pool4
I0519 18:12:26.420019 12029 net.cpp:454] pool4 <- conv4
I0519 18:12:26.420033 12029 net.cpp:411] pool4 -> pool4
I0519 18:12:26.420104 12029 net.cpp:150] Setting up pool4
I0519 18:12:26.420116 12029 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0519 18:12:26.420126 12029 net.cpp:165] Memory required for data: 157517200
I0519 18:12:26.420136 12029 layer_factory.hpp:77] Creating layer ip1
I0519 18:12:26.420152 12029 net.cpp:106] Creating Layer ip1
I0519 18:12:26.420162 12029 net.cpp:454] ip1 <- pool4
I0519 18:12:26.420176 12029 net.cpp:411] ip1 -> ip1
I0519 18:12:26.435669 12029 net.cpp:150] Setting up ip1
I0519 18:12:26.435693 12029 net.cpp:157] Top shape: 100 196 (19600)
I0519 18:12:26.435711 12029 net.cpp:165] Memory required for data: 157595600
I0519 18:12:26.435734 12029 layer_factory.hpp:77] Creating layer relu5
I0519 18:12:26.435750 12029 net.cpp:106] Creating Layer relu5
I0519 18:12:26.435760 12029 net.cpp:454] relu5 <- ip1
I0519 18:12:26.435777 12029 net.cpp:397] relu5 -> ip1 (in-place)
I0519 18:12:26.436127 12029 net.cpp:150] Setting up relu5
I0519 18:12:26.436141 12029 net.cpp:157] Top shape: 100 196 (19600)
I0519 18:12:26.436151 12029 net.cpp:165] Memory required for data: 157674000
I0519 18:12:26.436161 12029 layer_factory.hpp:77] Creating layer drop1
I0519 18:12:26.436179 12029 net.cpp:106] Creating Layer drop1
I0519 18:12:26.436189 12029 net.cpp:454] drop1 <- ip1
I0519 18:12:26.436203 12029 net.cpp:397] drop1 -> ip1 (in-place)
I0519 18:12:26.436246 12029 net.cpp:150] Setting up drop1
I0519 18:12:26.436259 12029 net.cpp:157] Top shape: 100 196 (19600)
I0519 18:12:26.436270 12029 net.cpp:165] Memory required for data: 157752400
I0519 18:12:26.436280 12029 layer_factory.hpp:77] Creating layer ip2
I0519 18:12:26.436295 12029 net.cpp:106] Creating Layer ip2
I0519 18:12:26.436305 12029 net.cpp:454] ip2 <- ip1
I0519 18:12:26.436318 12029 net.cpp:411] ip2 -> ip2
I0519 18:12:26.436799 12029 net.cpp:150] Setting up ip2
I0519 18:12:26.436811 12029 net.cpp:157] Top shape: 100 98 (9800)
I0519 18:12:26.436822 12029 net.cpp:165] Memory required for data: 157791600
I0519 18:12:26.436837 12029 layer_factory.hpp:77] Creating layer relu6
I0519 18:12:26.436862 12029 net.cpp:106] Creating Layer relu6
I0519 18:12:26.436872 12029 net.cpp:454] relu6 <- ip2
I0519 18:12:26.436885 12029 net.cpp:397] relu6 -> ip2 (in-place)
I0519 18:12:26.437427 12029 net.cpp:150] Setting up relu6
I0519 18:12:26.437448 12029 net.cpp:157] Top shape: 100 98 (9800)
I0519 18:12:26.437458 12029 net.cpp:165] Memory required for data: 157830800
I0519 18:12:26.437468 12029 layer_factory.hpp:77] Creating layer drop2
I0519 18:12:26.437481 12029 net.cpp:106] Creating Layer drop2
I0519 18:12:26.437491 12029 net.cpp:454] drop2 <- ip2
I0519 18:12:26.437505 12029 net.cpp:397] drop2 -> ip2 (in-place)
I0519 18:12:26.437548 12029 net.cpp:150] Setting up drop2
I0519 18:12:26.437561 12029 net.cpp:157] Top shape: 100 98 (9800)
I0519 18:12:26.437571 12029 net.cpp:165] Memory required for data: 157870000
I0519 18:12:26.437580 12029 layer_factory.hpp:77] Creating layer ip3
I0519 18:12:26.437594 12029 net.cpp:106] Creating Layer ip3
I0519 18:12:26.437603 12029 net.cpp:454] ip3 <- ip2
I0519 18:12:26.437618 12029 net.cpp:411] ip3 -> ip3
I0519 18:12:26.437841 12029 net.cpp:150] Setting up ip3
I0519 18:12:26.437855 12029 net.cpp:157] Top shape: 100 11 (1100)
I0519 18:12:26.437863 12029 net.cpp:165] Memory required for data: 157874400
I0519 18:12:26.437880 12029 layer_factory.hpp:77] Creating layer drop3
I0519 18:12:26.437891 12029 net.cpp:106] Creating Layer drop3
I0519 18:12:26.437901 12029 net.cpp:454] drop3 <- ip3
I0519 18:12:26.437914 12029 net.cpp:397] drop3 -> ip3 (in-place)
I0519 18:12:26.437955 12029 net.cpp:150] Setting up drop3
I0519 18:12:26.437968 12029 net.cpp:157] Top shape: 100 11 (1100)
I0519 18:12:26.437978 12029 net.cpp:165] Memory required for data: 157878800
I0519 18:12:26.437988 12029 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0519 18:12:26.438000 12029 net.cpp:106] Creating Layer ip3_drop3_0_split
I0519 18:12:26.438009 12029 net.cpp:454] ip3_drop3_0_split <- ip3
I0519 18:12:26.438022 12029 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0519 18:12:26.438037 12029 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0519 18:12:26.438110 12029 net.cpp:150] Setting up ip3_drop3_0_split
I0519 18:12:26.438123 12029 net.cpp:157] Top shape: 100 11 (1100)
I0519 18:12:26.438135 12029 net.cpp:157] Top shape: 100 11 (1100)
I0519 18:12:26.438145 12029 net.cpp:165] Memory required for data: 157887600
I0519 18:12:26.438158 12029 layer_factory.hpp:77] Creating layer accuracy
I0519 18:12:26.438179 12029 net.cpp:106] Creating Layer accuracy
I0519 18:12:26.438189 12029 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0519 18:12:26.438200 12029 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0519 18:12:26.438213 12029 net.cpp:411] accuracy -> accuracy
I0519 18:12:26.438237 12029 net.cpp:150] Setting up accuracy
I0519 18:12:26.438251 12029 net.cpp:157] Top shape: (1)
I0519 18:12:26.438259 12029 net.cpp:165] Memory required for data: 157887604
I0519 18:12:26.438269 12029 layer_factory.hpp:77] Creating layer loss
I0519 18:12:26.438283 12029 net.cpp:106] Creating Layer loss
I0519 18:12:26.438292 12029 net.cpp:454] loss <- ip3_drop3_0_split_1
I0519 18:12:26.438303 12029 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0519 18:12:26.438316 12029 net.cpp:411] loss -> loss
I0519 18:12:26.438334 12029 layer_factory.hpp:77] Creating layer loss
I0519 18:12:26.438820 12029 net.cpp:150] Setting up loss
I0519 18:12:26.438834 12029 net.cpp:157] Top shape: (1)
I0519 18:12:26.438844 12029 net.cpp:160]     with loss weight 1
I0519 18:12:26.438864 12029 net.cpp:165] Memory required for data: 157887608
I0519 18:12:26.438875 12029 net.cpp:226] loss needs backward computation.
I0519 18:12:26.438886 12029 net.cpp:228] accuracy does not need backward computation.
I0519 18:12:26.438897 12029 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0519 18:12:26.438907 12029 net.cpp:226] drop3 needs backward computation.
I0519 18:12:26.438920 12029 net.cpp:226] ip3 needs backward computation.
I0519 18:12:26.438930 12029 net.cpp:226] drop2 needs backward computation.
I0519 18:12:26.438948 12029 net.cpp:226] relu6 needs backward computation.
I0519 18:12:26.438958 12029 net.cpp:226] ip2 needs backward computation.
I0519 18:12:26.438968 12029 net.cpp:226] drop1 needs backward computation.
I0519 18:12:26.438978 12029 net.cpp:226] relu5 needs backward computation.
I0519 18:12:26.438988 12029 net.cpp:226] ip1 needs backward computation.
I0519 18:12:26.438998 12029 net.cpp:226] pool4 needs backward computation.
I0519 18:12:26.439008 12029 net.cpp:226] relu4 needs backward computation.
I0519 18:12:26.439018 12029 net.cpp:226] conv4 needs backward computation.
I0519 18:12:26.439028 12029 net.cpp:226] pool3 needs backward computation.
I0519 18:12:26.439038 12029 net.cpp:226] relu3 needs backward computation.
I0519 18:12:26.439048 12029 net.cpp:226] conv3 needs backward computation.
I0519 18:12:26.439057 12029 net.cpp:226] pool2 needs backward computation.
I0519 18:12:26.439069 12029 net.cpp:226] relu2 needs backward computation.
I0519 18:12:26.439079 12029 net.cpp:226] conv2 needs backward computation.
I0519 18:12:26.439088 12029 net.cpp:226] pool1 needs backward computation.
I0519 18:12:26.439098 12029 net.cpp:226] relu1 needs backward computation.
I0519 18:12:26.439108 12029 net.cpp:226] conv1 needs backward computation.
I0519 18:12:26.439119 12029 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0519 18:12:26.439131 12029 net.cpp:228] data_hdf5 does not need backward computation.
I0519 18:12:26.439141 12029 net.cpp:270] This network produces output accuracy
I0519 18:12:26.439152 12029 net.cpp:270] This network produces output loss
I0519 18:12:26.439180 12029 net.cpp:283] Network initialization done.
I0519 18:12:26.439311 12029 solver.cpp:60] Solver scaffolding done.
I0519 18:12:26.440462 12029 caffe.cpp:212] Starting Optimization
I0519 18:12:26.440480 12029 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0519 18:12:26.440495 12029 solver.cpp:289] Learning Rate Policy: fixed
I0519 18:12:26.441560 12029 solver.cpp:341] Iteration 0, Testing net (#0)
I0519 18:13:14.297335 12029 solver.cpp:409]     Test net output #0: accuracy = 0.1159
I0519 18:13:14.297502 12029 solver.cpp:409]     Test net output #1: loss = 2.39791 (* 1 = 2.39791 loss)
I0519 18:13:14.330399 12029 solver.cpp:237] Iteration 0, loss = 2.40089
I0519 18:13:14.330436 12029 solver.cpp:253]     Train net output #0: loss = 2.40089 (* 1 = 2.40089 loss)
I0519 18:13:14.330454 12029 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0519 18:13:20.143759 12029 solver.cpp:237] Iteration 100, loss = 2.36875
I0519 18:13:20.143793 12029 solver.cpp:253]     Train net output #0: loss = 2.36875 (* 1 = 2.36875 loss)
I0519 18:13:20.143810 12029 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0519 18:13:25.956804 12029 solver.cpp:237] Iteration 200, loss = 2.32473
I0519 18:13:25.956854 12029 solver.cpp:253]     Train net output #0: loss = 2.32473 (* 1 = 2.32473 loss)
I0519 18:13:25.956867 12029 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0519 18:13:31.774948 12029 solver.cpp:237] Iteration 300, loss = 2.25208
I0519 18:13:31.774982 12029 solver.cpp:253]     Train net output #0: loss = 2.25208 (* 1 = 2.25208 loss)
I0519 18:13:31.774996 12029 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0519 18:13:37.586977 12029 solver.cpp:237] Iteration 400, loss = 2.22553
I0519 18:13:37.587013 12029 solver.cpp:253]     Train net output #0: loss = 2.22553 (* 1 = 2.22553 loss)
I0519 18:13:37.587025 12029 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0519 18:13:43.403463 12029 solver.cpp:237] Iteration 500, loss = 2.13068
I0519 18:13:43.403497 12029 solver.cpp:253]     Train net output #0: loss = 2.13068 (* 1 = 2.13068 loss)
I0519 18:13:43.403512 12029 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0519 18:13:49.220935 12029 solver.cpp:237] Iteration 600, loss = 1.78719
I0519 18:13:49.221098 12029 solver.cpp:253]     Train net output #0: loss = 1.78719 (* 1 = 1.78719 loss)
I0519 18:13:49.221113 12029 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0519 18:13:55.038811 12029 solver.cpp:237] Iteration 700, loss = 2.07741
I0519 18:13:55.038844 12029 solver.cpp:253]     Train net output #0: loss = 2.07741 (* 1 = 2.07741 loss)
I0519 18:13:55.038858 12029 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0519 18:14:00.851410 12029 solver.cpp:237] Iteration 800, loss = 1.84355
I0519 18:14:00.851445 12029 solver.cpp:253]     Train net output #0: loss = 1.84355 (* 1 = 1.84355 loss)
I0519 18:14:00.851461 12029 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0519 18:14:06.666538 12029 solver.cpp:237] Iteration 900, loss = 1.8585
I0519 18:14:06.666573 12029 solver.cpp:253]     Train net output #0: loss = 1.8585 (* 1 = 1.8585 loss)
I0519 18:14:06.666585 12029 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0519 18:14:12.422096 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_1000.caffemodel
I0519 18:14:12.503772 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_1000.solverstate
I0519 18:14:34.763319 12029 solver.cpp:237] Iteration 1000, loss = 1.92747
I0519 18:14:34.763494 12029 solver.cpp:253]     Train net output #0: loss = 1.92747 (* 1 = 1.92747 loss)
I0519 18:14:34.763507 12029 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0519 18:14:40.579649 12029 solver.cpp:237] Iteration 1100, loss = 1.61706
I0519 18:14:40.579694 12029 solver.cpp:253]     Train net output #0: loss = 1.61706 (* 1 = 1.61706 loss)
I0519 18:14:40.579707 12029 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0519 18:14:46.397315 12029 solver.cpp:237] Iteration 1200, loss = 1.71823
I0519 18:14:46.397349 12029 solver.cpp:253]     Train net output #0: loss = 1.71823 (* 1 = 1.71823 loss)
I0519 18:14:46.397363 12029 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0519 18:14:52.213861 12029 solver.cpp:237] Iteration 1300, loss = 1.83147
I0519 18:14:52.213896 12029 solver.cpp:253]     Train net output #0: loss = 1.83147 (* 1 = 1.83147 loss)
I0519 18:14:52.213912 12029 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0519 18:14:58.029997 12029 solver.cpp:237] Iteration 1400, loss = 1.83353
I0519 18:14:58.030032 12029 solver.cpp:253]     Train net output #0: loss = 1.83353 (* 1 = 1.83353 loss)
I0519 18:14:58.030048 12029 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0519 18:15:03.849753 12029 solver.cpp:237] Iteration 1500, loss = 1.76888
I0519 18:15:03.849802 12029 solver.cpp:253]     Train net output #0: loss = 1.76888 (* 1 = 1.76888 loss)
I0519 18:15:03.849820 12029 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0519 18:15:09.666667 12029 solver.cpp:237] Iteration 1600, loss = 1.82963
I0519 18:15:09.666823 12029 solver.cpp:253]     Train net output #0: loss = 1.82963 (* 1 = 1.82963 loss)
I0519 18:15:09.666837 12029 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0519 18:15:15.479959 12029 solver.cpp:237] Iteration 1700, loss = 1.57182
I0519 18:15:15.479993 12029 solver.cpp:253]     Train net output #0: loss = 1.57182 (* 1 = 1.57182 loss)
I0519 18:15:15.480010 12029 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0519 18:15:21.299399 12029 solver.cpp:237] Iteration 1800, loss = 1.61134
I0519 18:15:21.299433 12029 solver.cpp:253]     Train net output #0: loss = 1.61134 (* 1 = 1.61134 loss)
I0519 18:15:21.299448 12029 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0519 18:15:27.119168 12029 solver.cpp:237] Iteration 1900, loss = 1.73532
I0519 18:15:27.119211 12029 solver.cpp:253]     Train net output #0: loss = 1.73532 (* 1 = 1.73532 loss)
I0519 18:15:27.119226 12029 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0519 18:15:32.878814 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_2000.caffemodel
I0519 18:15:32.956509 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_2000.solverstate
I0519 18:15:55.231276 12029 solver.cpp:237] Iteration 2000, loss = 1.7637
I0519 18:15:55.231443 12029 solver.cpp:253]     Train net output #0: loss = 1.7637 (* 1 = 1.7637 loss)
I0519 18:15:55.231458 12029 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0519 18:16:01.045562 12029 solver.cpp:237] Iteration 2100, loss = 1.60412
I0519 18:16:01.045596 12029 solver.cpp:253]     Train net output #0: loss = 1.60412 (* 1 = 1.60412 loss)
I0519 18:16:01.045611 12029 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0519 18:16:06.864748 12029 solver.cpp:237] Iteration 2200, loss = 1.63778
I0519 18:16:06.864781 12029 solver.cpp:253]     Train net output #0: loss = 1.63778 (* 1 = 1.63778 loss)
I0519 18:16:06.864795 12029 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0519 18:16:12.680922 12029 solver.cpp:237] Iteration 2300, loss = 1.46996
I0519 18:16:12.680958 12029 solver.cpp:253]     Train net output #0: loss = 1.46996 (* 1 = 1.46996 loss)
I0519 18:16:12.680971 12029 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0519 18:16:18.498962 12029 solver.cpp:237] Iteration 2400, loss = 1.76197
I0519 18:16:18.499006 12029 solver.cpp:253]     Train net output #0: loss = 1.76197 (* 1 = 1.76197 loss)
I0519 18:16:18.499027 12029 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0519 18:16:24.317140 12029 solver.cpp:237] Iteration 2500, loss = 1.55194
I0519 18:16:24.317174 12029 solver.cpp:253]     Train net output #0: loss = 1.55194 (* 1 = 1.55194 loss)
I0519 18:16:24.317190 12029 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0519 18:16:30.134135 12029 solver.cpp:237] Iteration 2600, loss = 1.86052
I0519 18:16:30.134273 12029 solver.cpp:253]     Train net output #0: loss = 1.86052 (* 1 = 1.86052 loss)
I0519 18:16:30.134285 12029 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0519 18:16:35.949506 12029 solver.cpp:237] Iteration 2700, loss = 1.67631
I0519 18:16:35.949538 12029 solver.cpp:253]     Train net output #0: loss = 1.67631 (* 1 = 1.67631 loss)
I0519 18:16:35.949553 12029 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0519 18:16:41.766068 12029 solver.cpp:237] Iteration 2800, loss = 1.74924
I0519 18:16:41.766118 12029 solver.cpp:253]     Train net output #0: loss = 1.74924 (* 1 = 1.74924 loss)
I0519 18:16:41.766130 12029 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0519 18:16:47.582875 12029 solver.cpp:237] Iteration 2900, loss = 1.46582
I0519 18:16:47.582911 12029 solver.cpp:253]     Train net output #0: loss = 1.46582 (* 1 = 1.46582 loss)
I0519 18:16:47.582924 12029 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0519 18:16:53.339660 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_3000.caffemodel
I0519 18:16:53.475718 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_3000.solverstate
I0519 18:16:53.554219 12029 solver.cpp:341] Iteration 3000, Testing net (#0)
I0519 18:17:40.419751 12029 solver.cpp:409]     Test net output #0: accuracy = 0.702206
I0519 18:17:40.419935 12029 solver.cpp:409]     Test net output #1: loss = 1.05584 (* 1 = 1.05584 loss)
I0519 18:18:02.664276 12029 solver.cpp:237] Iteration 3000, loss = 1.46684
I0519 18:18:02.664330 12029 solver.cpp:253]     Train net output #0: loss = 1.46684 (* 1 = 1.46684 loss)
I0519 18:18:02.664347 12029 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0519 18:18:08.490896 12029 solver.cpp:237] Iteration 3100, loss = 1.63563
I0519 18:18:08.490931 12029 solver.cpp:253]     Train net output #0: loss = 1.63563 (* 1 = 1.63563 loss)
I0519 18:18:08.490947 12029 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0519 18:18:14.316934 12029 solver.cpp:237] Iteration 3200, loss = 1.78639
I0519 18:18:14.317080 12029 solver.cpp:253]     Train net output #0: loss = 1.78639 (* 1 = 1.78639 loss)
I0519 18:18:14.317095 12029 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0519 18:18:20.143806 12029 solver.cpp:237] Iteration 3300, loss = 1.73125
I0519 18:18:20.143839 12029 solver.cpp:253]     Train net output #0: loss = 1.73125 (* 1 = 1.73125 loss)
I0519 18:18:20.143857 12029 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0519 18:18:25.967147 12029 solver.cpp:237] Iteration 3400, loss = 1.53376
I0519 18:18:25.967185 12029 solver.cpp:253]     Train net output #0: loss = 1.53376 (* 1 = 1.53376 loss)
I0519 18:18:25.967206 12029 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0519 18:18:31.791522 12029 solver.cpp:237] Iteration 3500, loss = 1.63343
I0519 18:18:31.791558 12029 solver.cpp:253]     Train net output #0: loss = 1.63343 (* 1 = 1.63343 loss)
I0519 18:18:31.791573 12029 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0519 18:18:37.618584 12029 solver.cpp:237] Iteration 3600, loss = 1.61925
I0519 18:18:37.618618 12029 solver.cpp:253]     Train net output #0: loss = 1.61925 (* 1 = 1.61925 loss)
I0519 18:18:37.618631 12029 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0519 18:18:43.444497 12029 solver.cpp:237] Iteration 3700, loss = 1.58702
I0519 18:18:43.444531 12029 solver.cpp:253]     Train net output #0: loss = 1.58702 (* 1 = 1.58702 loss)
I0519 18:18:43.444547 12029 sgd_solver.cpp:106] Iteration 3700, lr = 0.0025
I0519 18:18:49.272475 12029 solver.cpp:237] Iteration 3800, loss = 1.68301
I0519 18:18:49.272624 12029 solver.cpp:253]     Train net output #0: loss = 1.68301 (* 1 = 1.68301 loss)
I0519 18:18:49.272639 12029 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0519 18:18:55.097724 12029 solver.cpp:237] Iteration 3900, loss = 1.48324
I0519 18:18:55.097759 12029 solver.cpp:253]     Train net output #0: loss = 1.48324 (* 1 = 1.48324 loss)
I0519 18:18:55.097776 12029 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0519 18:19:00.865186 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_4000.caffemodel
I0519 18:19:00.946418 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_4000.solverstate
I0519 18:19:23.252452 12029 solver.cpp:237] Iteration 4000, loss = 1.56997
I0519 18:19:23.252626 12029 solver.cpp:253]     Train net output #0: loss = 1.56997 (* 1 = 1.56997 loss)
I0519 18:19:23.252641 12029 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0519 18:19:29.076643 12029 solver.cpp:237] Iteration 4100, loss = 1.53434
I0519 18:19:29.076678 12029 solver.cpp:253]     Train net output #0: loss = 1.53434 (* 1 = 1.53434 loss)
I0519 18:19:29.076694 12029 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0519 18:19:34.898849 12029 solver.cpp:237] Iteration 4200, loss = 1.55656
I0519 18:19:34.898883 12029 solver.cpp:253]     Train net output #0: loss = 1.55656 (* 1 = 1.55656 loss)
I0519 18:19:34.898900 12029 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0519 18:19:40.726948 12029 solver.cpp:237] Iteration 4300, loss = 1.45899
I0519 18:19:40.726996 12029 solver.cpp:253]     Train net output #0: loss = 1.45899 (* 1 = 1.45899 loss)
I0519 18:19:40.727015 12029 sgd_solver.cpp:106] Iteration 4300, lr = 0.0025
I0519 18:19:46.551688 12029 solver.cpp:237] Iteration 4400, loss = 1.60745
I0519 18:19:46.551723 12029 solver.cpp:253]     Train net output #0: loss = 1.60745 (* 1 = 1.60745 loss)
I0519 18:19:46.551738 12029 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0519 18:19:52.379941 12029 solver.cpp:237] Iteration 4500, loss = 1.36855
I0519 18:19:52.379971 12029 solver.cpp:253]     Train net output #0: loss = 1.36855 (* 1 = 1.36855 loss)
I0519 18:19:52.379983 12029 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0519 18:19:58.205970 12029 solver.cpp:237] Iteration 4600, loss = 1.48487
I0519 18:19:58.206130 12029 solver.cpp:253]     Train net output #0: loss = 1.48487 (* 1 = 1.48487 loss)
I0519 18:19:58.206142 12029 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0519 18:20:04.032002 12029 solver.cpp:237] Iteration 4700, loss = 1.46418
I0519 18:20:04.032044 12029 solver.cpp:253]     Train net output #0: loss = 1.46418 (* 1 = 1.46418 loss)
I0519 18:20:04.032065 12029 sgd_solver.cpp:106] Iteration 4700, lr = 0.0025
I0519 18:20:09.857719 12029 solver.cpp:237] Iteration 4800, loss = 1.52961
I0519 18:20:09.857753 12029 solver.cpp:253]     Train net output #0: loss = 1.52961 (* 1 = 1.52961 loss)
I0519 18:20:09.857769 12029 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0519 18:20:15.687376 12029 solver.cpp:237] Iteration 4900, loss = 1.32813
I0519 18:20:15.687410 12029 solver.cpp:253]     Train net output #0: loss = 1.32813 (* 1 = 1.32813 loss)
I0519 18:20:15.687427 12029 sgd_solver.cpp:106] Iteration 4900, lr = 0.0025
I0519 18:20:21.456521 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_5000.caffemodel
I0519 18:20:21.536873 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_5000.solverstate
I0519 18:20:43.872887 12029 solver.cpp:237] Iteration 5000, loss = 1.31029
I0519 18:20:43.873070 12029 solver.cpp:253]     Train net output #0: loss = 1.31029 (* 1 = 1.31029 loss)
I0519 18:20:43.873085 12029 sgd_solver.cpp:106] Iteration 5000, lr = 0.0025
I0519 18:20:49.696161 12029 solver.cpp:237] Iteration 5100, loss = 1.30436
I0519 18:20:49.696197 12029 solver.cpp:253]     Train net output #0: loss = 1.30436 (* 1 = 1.30436 loss)
I0519 18:20:49.696211 12029 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0519 18:20:55.519794 12029 solver.cpp:237] Iteration 5200, loss = 1.51368
I0519 18:20:55.519840 12029 solver.cpp:253]     Train net output #0: loss = 1.51368 (* 1 = 1.51368 loss)
I0519 18:20:55.519861 12029 sgd_solver.cpp:106] Iteration 5200, lr = 0.0025
I0519 18:21:01.346876 12029 solver.cpp:237] Iteration 5300, loss = 1.68509
I0519 18:21:01.346910 12029 solver.cpp:253]     Train net output #0: loss = 1.68509 (* 1 = 1.68509 loss)
I0519 18:21:01.346928 12029 sgd_solver.cpp:106] Iteration 5300, lr = 0.0025
I0519 18:21:07.175825 12029 solver.cpp:237] Iteration 5400, loss = 1.23893
I0519 18:21:07.175859 12029 solver.cpp:253]     Train net output #0: loss = 1.23893 (* 1 = 1.23893 loss)
I0519 18:21:07.175876 12029 sgd_solver.cpp:106] Iteration 5400, lr = 0.0025
I0519 18:21:13.001229 12029 solver.cpp:237] Iteration 5500, loss = 1.34372
I0519 18:21:13.001262 12029 solver.cpp:253]     Train net output #0: loss = 1.34372 (* 1 = 1.34372 loss)
I0519 18:21:13.001278 12029 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0519 18:21:18.831950 12029 solver.cpp:237] Iteration 5600, loss = 1.32087
I0519 18:21:18.832128 12029 solver.cpp:253]     Train net output #0: loss = 1.32087 (* 1 = 1.32087 loss)
I0519 18:21:18.832141 12029 sgd_solver.cpp:106] Iteration 5600, lr = 0.0025
I0519 18:21:24.658907 12029 solver.cpp:237] Iteration 5700, loss = 1.39231
I0519 18:21:24.658941 12029 solver.cpp:253]     Train net output #0: loss = 1.39231 (* 1 = 1.39231 loss)
I0519 18:21:24.658956 12029 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0519 18:21:30.480505 12029 solver.cpp:237] Iteration 5800, loss = 1.77778
I0519 18:21:30.480540 12029 solver.cpp:253]     Train net output #0: loss = 1.77778 (* 1 = 1.77778 loss)
I0519 18:21:30.480556 12029 sgd_solver.cpp:106] Iteration 5800, lr = 0.0025
I0519 18:21:36.307970 12029 solver.cpp:237] Iteration 5900, loss = 1.40451
I0519 18:21:36.308003 12029 solver.cpp:253]     Train net output #0: loss = 1.40451 (* 1 = 1.40451 loss)
I0519 18:21:36.308019 12029 sgd_solver.cpp:106] Iteration 5900, lr = 0.0025
I0519 18:21:42.076006 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_6000.caffemodel
I0519 18:21:42.166715 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_6000.solverstate
I0519 18:21:42.195412 12029 solver.cpp:341] Iteration 6000, Testing net (#0)
I0519 18:22:50.061791 12029 solver.cpp:409]     Test net output #0: accuracy = 0.803154
I0519 18:22:50.061956 12029 solver.cpp:409]     Test net output #1: loss = 0.712026 (* 1 = 0.712026 loss)
I0519 18:23:12.360589 12029 solver.cpp:237] Iteration 6000, loss = 1.37171
I0519 18:23:12.360647 12029 solver.cpp:253]     Train net output #0: loss = 1.37171 (* 1 = 1.37171 loss)
I0519 18:23:12.360662 12029 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0519 18:23:18.179563 12029 solver.cpp:237] Iteration 6100, loss = 1.3025
I0519 18:23:18.179597 12029 solver.cpp:253]     Train net output #0: loss = 1.3025 (* 1 = 1.3025 loss)
I0519 18:23:18.179613 12029 sgd_solver.cpp:106] Iteration 6100, lr = 0.0025
I0519 18:23:23.999588 12029 solver.cpp:237] Iteration 6200, loss = 1.35224
I0519 18:23:23.999763 12029 solver.cpp:253]     Train net output #0: loss = 1.35224 (* 1 = 1.35224 loss)
I0519 18:23:23.999776 12029 sgd_solver.cpp:106] Iteration 6200, lr = 0.0025
I0519 18:23:29.816040 12029 solver.cpp:237] Iteration 6300, loss = 1.42355
I0519 18:23:29.816069 12029 solver.cpp:253]     Train net output #0: loss = 1.42355 (* 1 = 1.42355 loss)
I0519 18:23:29.816083 12029 sgd_solver.cpp:106] Iteration 6300, lr = 0.0025
I0519 18:23:35.638126 12029 solver.cpp:237] Iteration 6400, loss = 1.45623
I0519 18:23:35.638160 12029 solver.cpp:253]     Train net output #0: loss = 1.45623 (* 1 = 1.45623 loss)
I0519 18:23:35.638177 12029 sgd_solver.cpp:106] Iteration 6400, lr = 0.0025
I0519 18:23:41.458083 12029 solver.cpp:237] Iteration 6500, loss = 1.52291
I0519 18:23:41.458117 12029 solver.cpp:253]     Train net output #0: loss = 1.52291 (* 1 = 1.52291 loss)
I0519 18:23:41.458132 12029 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0519 18:23:47.278749 12029 solver.cpp:237] Iteration 6600, loss = 1.36378
I0519 18:23:47.278795 12029 solver.cpp:253]     Train net output #0: loss = 1.36378 (* 1 = 1.36378 loss)
I0519 18:23:47.278813 12029 sgd_solver.cpp:106] Iteration 6600, lr = 0.0025
I0519 18:23:53.100229 12029 solver.cpp:237] Iteration 6700, loss = 1.40927
I0519 18:23:53.100265 12029 solver.cpp:253]     Train net output #0: loss = 1.40927 (* 1 = 1.40927 loss)
I0519 18:23:53.100281 12029 sgd_solver.cpp:106] Iteration 6700, lr = 0.0025
I0519 18:23:58.917609 12029 solver.cpp:237] Iteration 6800, loss = 1.28094
I0519 18:23:58.917762 12029 solver.cpp:253]     Train net output #0: loss = 1.28094 (* 1 = 1.28094 loss)
I0519 18:23:58.917778 12029 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0519 18:24:04.739460 12029 solver.cpp:237] Iteration 6900, loss = 1.25801
I0519 18:24:04.739493 12029 solver.cpp:253]     Train net output #0: loss = 1.25801 (* 1 = 1.25801 loss)
I0519 18:24:04.739508 12029 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0519 18:24:10.498980 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_7000.caffemodel
I0519 18:24:10.579715 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_7000.solverstate
I0519 18:24:32.947659 12029 solver.cpp:237] Iteration 7000, loss = 1.34571
I0519 18:24:32.947844 12029 solver.cpp:253]     Train net output #0: loss = 1.34571 (* 1 = 1.34571 loss)
I0519 18:24:32.947859 12029 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0519 18:24:38.766978 12029 solver.cpp:237] Iteration 7100, loss = 1.35296
I0519 18:24:38.767025 12029 solver.cpp:253]     Train net output #0: loss = 1.35296 (* 1 = 1.35296 loss)
I0519 18:24:38.767043 12029 sgd_solver.cpp:106] Iteration 7100, lr = 0.0025
I0519 18:24:44.586876 12029 solver.cpp:237] Iteration 7200, loss = 1.23864
I0519 18:24:44.586910 12029 solver.cpp:253]     Train net output #0: loss = 1.23864 (* 1 = 1.23864 loss)
I0519 18:24:44.586926 12029 sgd_solver.cpp:106] Iteration 7200, lr = 0.0025
I0519 18:24:50.408287 12029 solver.cpp:237] Iteration 7300, loss = 1.28346
I0519 18:24:50.408321 12029 solver.cpp:253]     Train net output #0: loss = 1.28346 (* 1 = 1.28346 loss)
I0519 18:24:50.408334 12029 sgd_solver.cpp:106] Iteration 7300, lr = 0.0025
I0519 18:24:56.229192 12029 solver.cpp:237] Iteration 7400, loss = 1.26659
I0519 18:24:56.229225 12029 solver.cpp:253]     Train net output #0: loss = 1.26659 (* 1 = 1.26659 loss)
I0519 18:24:56.229243 12029 sgd_solver.cpp:106] Iteration 7400, lr = 0.0025
I0519 18:25:02.048475 12029 solver.cpp:237] Iteration 7500, loss = 1.50075
I0519 18:25:02.048518 12029 solver.cpp:253]     Train net output #0: loss = 1.50075 (* 1 = 1.50075 loss)
I0519 18:25:02.048537 12029 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0519 18:25:07.864329 12029 solver.cpp:237] Iteration 7600, loss = 1.29749
I0519 18:25:07.864471 12029 solver.cpp:253]     Train net output #0: loss = 1.29749 (* 1 = 1.29749 loss)
I0519 18:25:07.864485 12029 sgd_solver.cpp:106] Iteration 7600, lr = 0.0025
I0519 18:25:13.688568 12029 solver.cpp:237] Iteration 7700, loss = 1.57841
I0519 18:25:13.688601 12029 solver.cpp:253]     Train net output #0: loss = 1.57841 (* 1 = 1.57841 loss)
I0519 18:25:13.688619 12029 sgd_solver.cpp:106] Iteration 7700, lr = 0.0025
I0519 18:25:19.509418 12029 solver.cpp:237] Iteration 7800, loss = 1.32329
I0519 18:25:19.509452 12029 solver.cpp:253]     Train net output #0: loss = 1.32329 (* 1 = 1.32329 loss)
I0519 18:25:19.509467 12029 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0519 18:25:25.329787 12029 solver.cpp:237] Iteration 7900, loss = 1.63004
I0519 18:25:25.329821 12029 solver.cpp:253]     Train net output #0: loss = 1.63004 (* 1 = 1.63004 loss)
I0519 18:25:25.329838 12029 sgd_solver.cpp:106] Iteration 7900, lr = 0.0025
I0519 18:25:31.094902 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_8000.caffemodel
I0519 18:25:31.175582 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_8000.solverstate
I0519 18:25:53.703518 12029 solver.cpp:237] Iteration 8000, loss = 1.22768
I0519 18:25:53.703712 12029 solver.cpp:253]     Train net output #0: loss = 1.22768 (* 1 = 1.22768 loss)
I0519 18:25:53.703727 12029 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0519 18:25:59.521939 12029 solver.cpp:237] Iteration 8100, loss = 1.33042
I0519 18:25:59.521972 12029 solver.cpp:253]     Train net output #0: loss = 1.33042 (* 1 = 1.33042 loss)
I0519 18:25:59.521989 12029 sgd_solver.cpp:106] Iteration 8100, lr = 0.0025
I0519 18:26:05.344398 12029 solver.cpp:237] Iteration 8200, loss = 1.47135
I0519 18:26:05.344432 12029 solver.cpp:253]     Train net output #0: loss = 1.47135 (* 1 = 1.47135 loss)
I0519 18:26:05.344449 12029 sgd_solver.cpp:106] Iteration 8200, lr = 0.0025
I0519 18:26:11.163637 12029 solver.cpp:237] Iteration 8300, loss = 1.44584
I0519 18:26:11.163678 12029 solver.cpp:253]     Train net output #0: loss = 1.44584 (* 1 = 1.44584 loss)
I0519 18:26:11.163691 12029 sgd_solver.cpp:106] Iteration 8300, lr = 0.0025
I0519 18:26:16.985553 12029 solver.cpp:237] Iteration 8400, loss = 1.47864
I0519 18:26:16.985595 12029 solver.cpp:253]     Train net output #0: loss = 1.47864 (* 1 = 1.47864 loss)
I0519 18:26:16.985610 12029 sgd_solver.cpp:106] Iteration 8400, lr = 0.0025
I0519 18:26:22.806802 12029 solver.cpp:237] Iteration 8500, loss = 1.43555
I0519 18:26:22.806836 12029 solver.cpp:253]     Train net output #0: loss = 1.43555 (* 1 = 1.43555 loss)
I0519 18:26:22.806854 12029 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0519 18:26:28.626019 12029 solver.cpp:237] Iteration 8600, loss = 1.38989
I0519 18:26:28.626184 12029 solver.cpp:253]     Train net output #0: loss = 1.38989 (* 1 = 1.38989 loss)
I0519 18:26:28.626199 12029 sgd_solver.cpp:106] Iteration 8600, lr = 0.0025
I0519 18:26:34.441622 12029 solver.cpp:237] Iteration 8700, loss = 1.41982
I0519 18:26:34.441654 12029 solver.cpp:253]     Train net output #0: loss = 1.41982 (* 1 = 1.41982 loss)
I0519 18:26:34.441668 12029 sgd_solver.cpp:106] Iteration 8700, lr = 0.0025
I0519 18:26:40.256114 12029 solver.cpp:237] Iteration 8800, loss = 1.40567
I0519 18:26:40.256148 12029 solver.cpp:253]     Train net output #0: loss = 1.40567 (* 1 = 1.40567 loss)
I0519 18:26:40.256163 12029 sgd_solver.cpp:106] Iteration 8800, lr = 0.0025
I0519 18:26:46.079064 12029 solver.cpp:237] Iteration 8900, loss = 1.23356
I0519 18:26:46.079108 12029 solver.cpp:253]     Train net output #0: loss = 1.23356 (* 1 = 1.23356 loss)
I0519 18:26:46.079128 12029 sgd_solver.cpp:106] Iteration 8900, lr = 0.0025
I0519 18:26:51.846829 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_9000.caffemodel
I0519 18:26:51.925655 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_9000.solverstate
I0519 18:26:51.952074 12029 solver.cpp:341] Iteration 9000, Testing net (#0)
I0519 18:27:38.510610 12029 solver.cpp:409]     Test net output #0: accuracy = 0.82212
I0519 18:27:38.510781 12029 solver.cpp:409]     Test net output #1: loss = 0.612649 (* 1 = 0.612649 loss)
I0519 18:28:00.810842 12029 solver.cpp:237] Iteration 9000, loss = 1.39047
I0519 18:28:00.810899 12029 solver.cpp:253]     Train net output #0: loss = 1.39047 (* 1 = 1.39047 loss)
I0519 18:28:00.810915 12029 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0519 18:28:06.636198 12029 solver.cpp:237] Iteration 9100, loss = 1.30159
I0519 18:28:06.636232 12029 solver.cpp:253]     Train net output #0: loss = 1.30159 (* 1 = 1.30159 loss)
I0519 18:28:06.636250 12029 sgd_solver.cpp:106] Iteration 9100, lr = 0.0025
I0519 18:28:12.459466 12029 solver.cpp:237] Iteration 9200, loss = 1.20307
I0519 18:28:12.459620 12029 solver.cpp:253]     Train net output #0: loss = 1.20307 (* 1 = 1.20307 loss)
I0519 18:28:12.459633 12029 sgd_solver.cpp:106] Iteration 9200, lr = 0.0025
I0519 18:28:18.282594 12029 solver.cpp:237] Iteration 9300, loss = 1.34735
I0519 18:28:18.282629 12029 solver.cpp:253]     Train net output #0: loss = 1.34735 (* 1 = 1.34735 loss)
I0519 18:28:18.282644 12029 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0519 18:28:24.106015 12029 solver.cpp:237] Iteration 9400, loss = 1.21913
I0519 18:28:24.106067 12029 solver.cpp:253]     Train net output #0: loss = 1.21913 (* 1 = 1.21913 loss)
I0519 18:28:24.106082 12029 sgd_solver.cpp:106] Iteration 9400, lr = 0.0025
I0519 18:28:29.930567 12029 solver.cpp:237] Iteration 9500, loss = 1.38204
I0519 18:28:29.930603 12029 solver.cpp:253]     Train net output #0: loss = 1.38204 (* 1 = 1.38204 loss)
I0519 18:28:29.930615 12029 sgd_solver.cpp:106] Iteration 9500, lr = 0.0025
I0519 18:28:35.754851 12029 solver.cpp:237] Iteration 9600, loss = 1.46553
I0519 18:28:35.754885 12029 solver.cpp:253]     Train net output #0: loss = 1.46553 (* 1 = 1.46553 loss)
I0519 18:28:35.754902 12029 sgd_solver.cpp:106] Iteration 9600, lr = 0.0025
I0519 18:28:41.580035 12029 solver.cpp:237] Iteration 9700, loss = 1.38603
I0519 18:28:41.580065 12029 solver.cpp:253]     Train net output #0: loss = 1.38603 (* 1 = 1.38603 loss)
I0519 18:28:41.580078 12029 sgd_solver.cpp:106] Iteration 9700, lr = 0.0025
I0519 18:28:47.404834 12029 solver.cpp:237] Iteration 9800, loss = 1.38516
I0519 18:28:47.405000 12029 solver.cpp:253]     Train net output #0: loss = 1.38516 (* 1 = 1.38516 loss)
I0519 18:28:47.405014 12029 sgd_solver.cpp:106] Iteration 9800, lr = 0.0025
I0519 18:28:53.233110 12029 solver.cpp:237] Iteration 9900, loss = 1.32392
I0519 18:28:53.233144 12029 solver.cpp:253]     Train net output #0: loss = 1.32392 (* 1 = 1.32392 loss)
I0519 18:28:53.233161 12029 sgd_solver.cpp:106] Iteration 9900, lr = 0.0025
I0519 18:28:59.000855 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_10000.caffemodel
I0519 18:28:59.079728 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_10000.solverstate
I0519 18:29:21.429404 12029 solver.cpp:237] Iteration 10000, loss = 1.24987
I0519 18:29:21.429587 12029 solver.cpp:253]     Train net output #0: loss = 1.24987 (* 1 = 1.24987 loss)
I0519 18:29:21.429602 12029 sgd_solver.cpp:106] Iteration 10000, lr = 0.0025
I0519 18:29:27.254931 12029 solver.cpp:237] Iteration 10100, loss = 1.21879
I0519 18:29:27.254966 12029 solver.cpp:253]     Train net output #0: loss = 1.21879 (* 1 = 1.21879 loss)
I0519 18:29:27.254983 12029 sgd_solver.cpp:106] Iteration 10100, lr = 0.0025
I0519 18:29:33.077318 12029 solver.cpp:237] Iteration 10200, loss = 1.39288
I0519 18:29:33.077354 12029 solver.cpp:253]     Train net output #0: loss = 1.39288 (* 1 = 1.39288 loss)
I0519 18:29:33.077370 12029 sgd_solver.cpp:106] Iteration 10200, lr = 0.0025
I0519 18:29:38.903667 12029 solver.cpp:237] Iteration 10300, loss = 1.16209
I0519 18:29:38.903704 12029 solver.cpp:253]     Train net output #0: loss = 1.16209 (* 1 = 1.16209 loss)
I0519 18:29:38.903720 12029 sgd_solver.cpp:106] Iteration 10300, lr = 0.0025
I0519 18:29:44.734694 12029 solver.cpp:237] Iteration 10400, loss = 1.33493
I0519 18:29:44.734729 12029 solver.cpp:253]     Train net output #0: loss = 1.33493 (* 1 = 1.33493 loss)
I0519 18:29:44.734745 12029 sgd_solver.cpp:106] Iteration 10400, lr = 0.0025
I0519 18:29:50.563984 12029 solver.cpp:237] Iteration 10500, loss = 1.33233
I0519 18:29:50.564019 12029 solver.cpp:253]     Train net output #0: loss = 1.33233 (* 1 = 1.33233 loss)
I0519 18:29:50.564034 12029 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0519 18:29:56.388268 12029 solver.cpp:237] Iteration 10600, loss = 1.20082
I0519 18:29:56.388425 12029 solver.cpp:253]     Train net output #0: loss = 1.20082 (* 1 = 1.20082 loss)
I0519 18:29:56.388439 12029 sgd_solver.cpp:106] Iteration 10600, lr = 0.0025
I0519 18:30:02.215644 12029 solver.cpp:237] Iteration 10700, loss = 1.16218
I0519 18:30:02.215685 12029 solver.cpp:253]     Train net output #0: loss = 1.16218 (* 1 = 1.16218 loss)
I0519 18:30:02.215703 12029 sgd_solver.cpp:106] Iteration 10700, lr = 0.0025
I0519 18:30:08.046957 12029 solver.cpp:237] Iteration 10800, loss = 1.2926
I0519 18:30:08.046990 12029 solver.cpp:253]     Train net output #0: loss = 1.2926 (* 1 = 1.2926 loss)
I0519 18:30:08.047005 12029 sgd_solver.cpp:106] Iteration 10800, lr = 0.0025
I0519 18:30:13.868072 12029 solver.cpp:237] Iteration 10900, loss = 1.32136
I0519 18:30:13.868105 12029 solver.cpp:253]     Train net output #0: loss = 1.32136 (* 1 = 1.32136 loss)
I0519 18:30:13.868119 12029 sgd_solver.cpp:106] Iteration 10900, lr = 0.0025
I0519 18:30:19.633043 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_11000.caffemodel
I0519 18:30:19.711963 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_11000.solverstate
I0519 18:30:42.039170 12029 solver.cpp:237] Iteration 11000, loss = 1.34818
I0519 18:30:42.039352 12029 solver.cpp:253]     Train net output #0: loss = 1.34818 (* 1 = 1.34818 loss)
I0519 18:30:42.039367 12029 sgd_solver.cpp:106] Iteration 11000, lr = 0.0025
I0519 18:30:47.858269 12029 solver.cpp:237] Iteration 11100, loss = 1.13461
I0519 18:30:47.858304 12029 solver.cpp:253]     Train net output #0: loss = 1.13461 (* 1 = 1.13461 loss)
I0519 18:30:47.858320 12029 sgd_solver.cpp:106] Iteration 11100, lr = 0.0025
I0519 18:30:53.681695 12029 solver.cpp:237] Iteration 11200, loss = 1.14426
I0519 18:30:53.681738 12029 solver.cpp:253]     Train net output #0: loss = 1.14426 (* 1 = 1.14426 loss)
I0519 18:30:53.681758 12029 sgd_solver.cpp:106] Iteration 11200, lr = 0.0025
I0519 18:30:59.502692 12029 solver.cpp:237] Iteration 11300, loss = 1.3965
I0519 18:30:59.502727 12029 solver.cpp:253]     Train net output #0: loss = 1.3965 (* 1 = 1.3965 loss)
I0519 18:30:59.502743 12029 sgd_solver.cpp:106] Iteration 11300, lr = 0.0025
I0519 18:31:05.329759 12029 solver.cpp:237] Iteration 11400, loss = 1.28687
I0519 18:31:05.329794 12029 solver.cpp:253]     Train net output #0: loss = 1.28687 (* 1 = 1.28687 loss)
I0519 18:31:05.329809 12029 sgd_solver.cpp:106] Iteration 11400, lr = 0.0025
I0519 18:31:11.156281 12029 solver.cpp:237] Iteration 11500, loss = 1.39215
I0519 18:31:11.156314 12029 solver.cpp:253]     Train net output #0: loss = 1.39215 (* 1 = 1.39215 loss)
I0519 18:31:11.156332 12029 sgd_solver.cpp:106] Iteration 11500, lr = 0.0025
I0519 18:31:16.975558 12029 solver.cpp:237] Iteration 11600, loss = 1.27106
I0519 18:31:16.975728 12029 solver.cpp:253]     Train net output #0: loss = 1.27106 (* 1 = 1.27106 loss)
I0519 18:31:16.975742 12029 sgd_solver.cpp:106] Iteration 11600, lr = 0.0025
I0519 18:31:22.796675 12029 solver.cpp:237] Iteration 11700, loss = 1.50688
I0519 18:31:22.796710 12029 solver.cpp:253]     Train net output #0: loss = 1.50688 (* 1 = 1.50688 loss)
I0519 18:31:22.796728 12029 sgd_solver.cpp:106] Iteration 11700, lr = 0.0025
I0519 18:31:28.616698 12029 solver.cpp:237] Iteration 11800, loss = 1.24585
I0519 18:31:28.616731 12029 solver.cpp:253]     Train net output #0: loss = 1.24585 (* 1 = 1.24585 loss)
I0519 18:31:28.616747 12029 sgd_solver.cpp:106] Iteration 11800, lr = 0.0025
I0519 18:31:34.437410 12029 solver.cpp:237] Iteration 11900, loss = 1.29341
I0519 18:31:34.437445 12029 solver.cpp:253]     Train net output #0: loss = 1.29341 (* 1 = 1.29341 loss)
I0519 18:31:34.437458 12029 sgd_solver.cpp:106] Iteration 11900, lr = 0.0025
I0519 18:31:40.205824 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_12000.caffemodel
I0519 18:31:40.283622 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_12000.solverstate
I0519 18:31:40.308957 12029 solver.cpp:341] Iteration 12000, Testing net (#0)
I0519 18:32:48.086206 12029 solver.cpp:409]     Test net output #0: accuracy = 0.838266
I0519 18:32:48.086387 12029 solver.cpp:409]     Test net output #1: loss = 0.525519 (* 1 = 0.525519 loss)
I0519 18:33:10.371419 12029 solver.cpp:237] Iteration 12000, loss = 1.54804
I0519 18:33:10.371474 12029 solver.cpp:253]     Train net output #0: loss = 1.54804 (* 1 = 1.54804 loss)
I0519 18:33:10.371491 12029 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0519 18:33:16.187396 12029 solver.cpp:237] Iteration 12100, loss = 1.36739
I0519 18:33:16.187429 12029 solver.cpp:253]     Train net output #0: loss = 1.36739 (* 1 = 1.36739 loss)
I0519 18:33:16.187446 12029 sgd_solver.cpp:106] Iteration 12100, lr = 0.0025
I0519 18:33:22.007532 12029 solver.cpp:237] Iteration 12200, loss = 1.10265
I0519 18:33:22.007714 12029 solver.cpp:253]     Train net output #0: loss = 1.10265 (* 1 = 1.10265 loss)
I0519 18:33:22.007730 12029 sgd_solver.cpp:106] Iteration 12200, lr = 0.0025
I0519 18:33:27.824872 12029 solver.cpp:237] Iteration 12300, loss = 1.20096
I0519 18:33:27.824908 12029 solver.cpp:253]     Train net output #0: loss = 1.20096 (* 1 = 1.20096 loss)
I0519 18:33:27.824921 12029 sgd_solver.cpp:106] Iteration 12300, lr = 0.0025
I0519 18:33:33.642078 12029 solver.cpp:237] Iteration 12400, loss = 1.27963
I0519 18:33:33.642113 12029 solver.cpp:253]     Train net output #0: loss = 1.27963 (* 1 = 1.27963 loss)
I0519 18:33:33.642129 12029 sgd_solver.cpp:106] Iteration 12400, lr = 0.0025
I0519 18:33:39.456562 12029 solver.cpp:237] Iteration 12500, loss = 1.19165
I0519 18:33:39.456596 12029 solver.cpp:253]     Train net output #0: loss = 1.19165 (* 1 = 1.19165 loss)
I0519 18:33:39.456612 12029 sgd_solver.cpp:106] Iteration 12500, lr = 0.0025
I0519 18:33:45.273968 12029 solver.cpp:237] Iteration 12600, loss = 1.28902
I0519 18:33:45.274001 12029 solver.cpp:253]     Train net output #0: loss = 1.28902 (* 1 = 1.28902 loss)
I0519 18:33:45.274018 12029 sgd_solver.cpp:106] Iteration 12600, lr = 0.0025
I0519 18:33:51.093202 12029 solver.cpp:237] Iteration 12700, loss = 1.37554
I0519 18:33:51.093243 12029 solver.cpp:253]     Train net output #0: loss = 1.37554 (* 1 = 1.37554 loss)
I0519 18:33:51.093260 12029 sgd_solver.cpp:106] Iteration 12700, lr = 0.0025
I0519 18:33:56.909888 12029 solver.cpp:237] Iteration 12800, loss = 1.2733
I0519 18:33:56.910034 12029 solver.cpp:253]     Train net output #0: loss = 1.2733 (* 1 = 1.2733 loss)
I0519 18:33:56.910048 12029 sgd_solver.cpp:106] Iteration 12800, lr = 0.0025
I0519 18:34:02.726317 12029 solver.cpp:237] Iteration 12900, loss = 1.07348
I0519 18:34:02.726351 12029 solver.cpp:253]     Train net output #0: loss = 1.07348 (* 1 = 1.07348 loss)
I0519 18:34:02.726366 12029 sgd_solver.cpp:106] Iteration 12900, lr = 0.0025
I0519 18:34:08.486332 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_13000.caffemodel
I0519 18:34:08.566468 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_13000.solverstate
I0519 18:34:30.885679 12029 solver.cpp:237] Iteration 13000, loss = 1.38416
I0519 18:34:30.885860 12029 solver.cpp:253]     Train net output #0: loss = 1.38416 (* 1 = 1.38416 loss)
I0519 18:34:30.885875 12029 sgd_solver.cpp:106] Iteration 13000, lr = 0.0025
I0519 18:34:36.704128 12029 solver.cpp:237] Iteration 13100, loss = 1.24518
I0519 18:34:36.704170 12029 solver.cpp:253]     Train net output #0: loss = 1.24518 (* 1 = 1.24518 loss)
I0519 18:34:36.704188 12029 sgd_solver.cpp:106] Iteration 13100, lr = 0.0025
I0519 18:34:42.525136 12029 solver.cpp:237] Iteration 13200, loss = 1.37071
I0519 18:34:42.525171 12029 solver.cpp:253]     Train net output #0: loss = 1.37071 (* 1 = 1.37071 loss)
I0519 18:34:42.525187 12029 sgd_solver.cpp:106] Iteration 13200, lr = 0.0025
I0519 18:34:48.341377 12029 solver.cpp:237] Iteration 13300, loss = 1.34898
I0519 18:34:48.341410 12029 solver.cpp:253]     Train net output #0: loss = 1.34898 (* 1 = 1.34898 loss)
I0519 18:34:48.341426 12029 sgd_solver.cpp:106] Iteration 13300, lr = 0.0025
I0519 18:34:54.153606 12029 solver.cpp:237] Iteration 13400, loss = 1.46769
I0519 18:34:54.153640 12029 solver.cpp:253]     Train net output #0: loss = 1.46769 (* 1 = 1.46769 loss)
I0519 18:34:54.153657 12029 sgd_solver.cpp:106] Iteration 13400, lr = 0.0025
I0519 18:34:59.971510 12029 solver.cpp:237] Iteration 13500, loss = 1.35976
I0519 18:34:59.971544 12029 solver.cpp:253]     Train net output #0: loss = 1.35976 (* 1 = 1.35976 loss)
I0519 18:34:59.971557 12029 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0519 18:35:05.791965 12029 solver.cpp:237] Iteration 13600, loss = 1.24837
I0519 18:35:05.792129 12029 solver.cpp:253]     Train net output #0: loss = 1.24837 (* 1 = 1.24837 loss)
I0519 18:35:05.792142 12029 sgd_solver.cpp:106] Iteration 13600, lr = 0.0025
I0519 18:35:11.609740 12029 solver.cpp:237] Iteration 13700, loss = 1.38603
I0519 18:35:11.609772 12029 solver.cpp:253]     Train net output #0: loss = 1.38603 (* 1 = 1.38603 loss)
I0519 18:35:11.609791 12029 sgd_solver.cpp:106] Iteration 13700, lr = 0.0025
I0519 18:35:17.428208 12029 solver.cpp:237] Iteration 13800, loss = 1.27025
I0519 18:35:17.428243 12029 solver.cpp:253]     Train net output #0: loss = 1.27025 (* 1 = 1.27025 loss)
I0519 18:35:17.428259 12029 sgd_solver.cpp:106] Iteration 13800, lr = 0.0025
I0519 18:35:23.243624 12029 solver.cpp:237] Iteration 13900, loss = 1.07664
I0519 18:35:23.243664 12029 solver.cpp:253]     Train net output #0: loss = 1.07664 (* 1 = 1.07664 loss)
I0519 18:35:23.243680 12029 sgd_solver.cpp:106] Iteration 13900, lr = 0.0025
I0519 18:35:29.000846 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_14000.caffemodel
I0519 18:35:29.080855 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_14000.solverstate
I0519 18:35:51.429621 12029 solver.cpp:237] Iteration 14000, loss = 1.19479
I0519 18:35:51.429811 12029 solver.cpp:253]     Train net output #0: loss = 1.19479 (* 1 = 1.19479 loss)
I0519 18:35:51.429827 12029 sgd_solver.cpp:106] Iteration 14000, lr = 0.0025
I0519 18:35:57.250077 12029 solver.cpp:237] Iteration 14100, loss = 1.17992
I0519 18:35:57.250123 12029 solver.cpp:253]     Train net output #0: loss = 1.17992 (* 1 = 1.17992 loss)
I0519 18:35:57.250140 12029 sgd_solver.cpp:106] Iteration 14100, lr = 0.0025
I0519 18:36:03.069098 12029 solver.cpp:237] Iteration 14200, loss = 1.31255
I0519 18:36:03.069133 12029 solver.cpp:253]     Train net output #0: loss = 1.31255 (* 1 = 1.31255 loss)
I0519 18:36:03.069149 12029 sgd_solver.cpp:106] Iteration 14200, lr = 0.0025
I0519 18:36:08.885442 12029 solver.cpp:237] Iteration 14300, loss = 1.35625
I0519 18:36:08.885476 12029 solver.cpp:253]     Train net output #0: loss = 1.35625 (* 1 = 1.35625 loss)
I0519 18:36:08.885493 12029 sgd_solver.cpp:106] Iteration 14300, lr = 0.0025
I0519 18:36:14.703197 12029 solver.cpp:237] Iteration 14400, loss = 1.13356
I0519 18:36:14.703233 12029 solver.cpp:253]     Train net output #0: loss = 1.13356 (* 1 = 1.13356 loss)
I0519 18:36:14.703248 12029 sgd_solver.cpp:106] Iteration 14400, lr = 0.0025
I0519 18:36:20.522660 12029 solver.cpp:237] Iteration 14500, loss = 1.25481
I0519 18:36:20.522706 12029 solver.cpp:253]     Train net output #0: loss = 1.25481 (* 1 = 1.25481 loss)
I0519 18:36:20.522723 12029 sgd_solver.cpp:106] Iteration 14500, lr = 0.0025
I0519 18:36:26.340793 12029 solver.cpp:237] Iteration 14600, loss = 1.34603
I0519 18:36:26.340941 12029 solver.cpp:253]     Train net output #0: loss = 1.34603 (* 1 = 1.34603 loss)
I0519 18:36:26.340955 12029 sgd_solver.cpp:106] Iteration 14600, lr = 0.0025
I0519 18:36:32.161548 12029 solver.cpp:237] Iteration 14700, loss = 1.31968
I0519 18:36:32.161581 12029 solver.cpp:253]     Train net output #0: loss = 1.31968 (* 1 = 1.31968 loss)
I0519 18:36:32.161598 12029 sgd_solver.cpp:106] Iteration 14700, lr = 0.0025
I0519 18:36:37.979905 12029 solver.cpp:237] Iteration 14800, loss = 1.31038
I0519 18:36:37.979939 12029 solver.cpp:253]     Train net output #0: loss = 1.31038 (* 1 = 1.31038 loss)
I0519 18:36:37.979955 12029 sgd_solver.cpp:106] Iteration 14800, lr = 0.0025
I0519 18:36:43.799861 12029 solver.cpp:237] Iteration 14900, loss = 1.23602
I0519 18:36:43.799908 12029 solver.cpp:253]     Train net output #0: loss = 1.23602 (* 1 = 1.23602 loss)
I0519 18:36:43.799926 12029 sgd_solver.cpp:106] Iteration 14900, lr = 0.0025
I0519 18:36:49.559550 12029 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_15000.caffemodel
I0519 18:36:49.640539 12029 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/timeverbose_2016-05-19T18.11.26.579999_iter_15000.solverstate
I0519 18:37:10.655421 12029 solver.cpp:321] Iteration 15000, loss = 1.35743
I0519 18:37:10.655601 12029 solver.cpp:341] Iteration 15000, Testing net (#0)
I0519 18:37:57.567082 12029 solver.cpp:409]     Test net output #0: accuracy = 0.845706
I0519 18:37:57.567260 12029 solver.cpp:409]     Test net output #1: loss = 0.48037 (* 1 = 0.48037 loss)
I0519 18:37:57.567273 12029 solver.cpp:326] Optimization Done.
I0519 18:37:57.567282 12029 caffe.cpp:215] Optimization Done.
Application 11227933 resources: utime ~1347s, stime ~229s, Rss ~5328964, inblocks ~3744348, outblocks ~253586
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_depend/timeverbose_2016-05-19T18.11.26.579999.solver"
	User time (seconds): 0.56
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 26:21.75
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15075
	Voluntary context switches: 2878
	Involuntary context switches: 78
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
