2798046
I0514 13:05:58.672793 16573 caffe.cpp:184] Using GPUs 0
I0514 13:05:59.102229 16573 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1500
test_interval: 500
base_lr: 0.0025
display: 500
max_iter: 10000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt"
I0514 13:05:59.104459 16573 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0514 13:05:59.106787 16573 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0514 13:05:59.106845 16573 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0514 13:05:59.107221 16573 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0514 13:05:59.107414 16573 layer_factory.hpp:77] Creating layer data_hdf5
I0514 13:05:59.107440 16573 net.cpp:106] Creating Layer data_hdf5
I0514 13:05:59.107470 16573 net.cpp:411] data_hdf5 -> data
I0514 13:05:59.107503 16573 net.cpp:411] data_hdf5 -> label
I0514 13:05:59.107534 16573 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0514 13:05:59.108798 16573 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0514 13:05:59.111037 16573 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0514 13:06:20.652624 16573 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0514 13:06:20.658000 16573 net.cpp:150] Setting up data_hdf5
I0514 13:06:20.658057 16573 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0514 13:06:20.658076 16573 net.cpp:157] Top shape: 100 (100)
I0514 13:06:20.658092 16573 net.cpp:165] Memory required for data: 2540400
I0514 13:06:20.658120 16573 layer_factory.hpp:77] Creating layer conv1
I0514 13:06:20.658159 16573 net.cpp:106] Creating Layer conv1
I0514 13:06:20.658174 16573 net.cpp:454] conv1 <- data
I0514 13:06:20.658197 16573 net.cpp:411] conv1 -> conv1
I0514 13:06:21.017159 16573 net.cpp:150] Setting up conv1
I0514 13:06:21.017210 16573 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0514 13:06:21.017225 16573 net.cpp:165] Memory required for data: 30188400
I0514 13:06:21.017258 16573 layer_factory.hpp:77] Creating layer relu1
I0514 13:06:21.017280 16573 net.cpp:106] Creating Layer relu1
I0514 13:06:21.017294 16573 net.cpp:454] relu1 <- conv1
I0514 13:06:21.017336 16573 net.cpp:397] relu1 -> conv1 (in-place)
I0514 13:06:21.017871 16573 net.cpp:150] Setting up relu1
I0514 13:06:21.017894 16573 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0514 13:06:21.017909 16573 net.cpp:165] Memory required for data: 57836400
I0514 13:06:21.017921 16573 layer_factory.hpp:77] Creating layer pool1
I0514 13:06:21.017951 16573 net.cpp:106] Creating Layer pool1
I0514 13:06:21.017966 16573 net.cpp:454] pool1 <- conv1
I0514 13:06:21.017982 16573 net.cpp:411] pool1 -> pool1
I0514 13:06:21.018092 16573 net.cpp:150] Setting up pool1
I0514 13:06:21.018112 16573 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0514 13:06:21.018126 16573 net.cpp:165] Memory required for data: 71660400
I0514 13:06:21.018137 16573 layer_factory.hpp:77] Creating layer conv2
I0514 13:06:21.018172 16573 net.cpp:106] Creating Layer conv2
I0514 13:06:21.018185 16573 net.cpp:454] conv2 <- pool1
I0514 13:06:21.018201 16573 net.cpp:411] conv2 -> conv2
I0514 13:06:21.020975 16573 net.cpp:150] Setting up conv2
I0514 13:06:21.021005 16573 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0514 13:06:21.021021 16573 net.cpp:165] Memory required for data: 91532400
I0514 13:06:21.021044 16573 layer_factory.hpp:77] Creating layer relu2
I0514 13:06:21.021076 16573 net.cpp:106] Creating Layer relu2
I0514 13:06:21.021090 16573 net.cpp:454] relu2 <- conv2
I0514 13:06:21.021107 16573 net.cpp:397] relu2 -> conv2 (in-place)
I0514 13:06:21.021464 16573 net.cpp:150] Setting up relu2
I0514 13:06:21.021486 16573 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0514 13:06:21.021498 16573 net.cpp:165] Memory required for data: 111404400
I0514 13:06:21.021510 16573 layer_factory.hpp:77] Creating layer pool2
I0514 13:06:21.021538 16573 net.cpp:106] Creating Layer pool2
I0514 13:06:21.021551 16573 net.cpp:454] pool2 <- conv2
I0514 13:06:21.021569 16573 net.cpp:411] pool2 -> pool2
I0514 13:06:21.021667 16573 net.cpp:150] Setting up pool2
I0514 13:06:21.021683 16573 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0514 13:06:21.021699 16573 net.cpp:165] Memory required for data: 121340400
I0514 13:06:21.021718 16573 layer_factory.hpp:77] Creating layer conv3
I0514 13:06:21.021739 16573 net.cpp:106] Creating Layer conv3
I0514 13:06:21.021752 16573 net.cpp:454] conv3 <- pool2
I0514 13:06:21.021769 16573 net.cpp:411] conv3 -> conv3
I0514 13:06:21.023751 16573 net.cpp:150] Setting up conv3
I0514 13:06:21.023777 16573 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0514 13:06:21.023797 16573 net.cpp:165] Memory required for data: 132182000
I0514 13:06:21.023819 16573 layer_factory.hpp:77] Creating layer relu3
I0514 13:06:21.023843 16573 net.cpp:106] Creating Layer relu3
I0514 13:06:21.023864 16573 net.cpp:454] relu3 <- conv3
I0514 13:06:21.023881 16573 net.cpp:397] relu3 -> conv3 (in-place)
I0514 13:06:21.024371 16573 net.cpp:150] Setting up relu3
I0514 13:06:21.024394 16573 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0514 13:06:21.024407 16573 net.cpp:165] Memory required for data: 143023600
I0514 13:06:21.024425 16573 layer_factory.hpp:77] Creating layer pool3
I0514 13:06:21.024448 16573 net.cpp:106] Creating Layer pool3
I0514 13:06:21.024462 16573 net.cpp:454] pool3 <- conv3
I0514 13:06:21.024477 16573 net.cpp:411] pool3 -> pool3
I0514 13:06:21.024562 16573 net.cpp:150] Setting up pool3
I0514 13:06:21.024586 16573 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0514 13:06:21.024600 16573 net.cpp:165] Memory required for data: 148444400
I0514 13:06:21.024612 16573 layer_factory.hpp:77] Creating layer conv4
I0514 13:06:21.024641 16573 net.cpp:106] Creating Layer conv4
I0514 13:06:21.024654 16573 net.cpp:454] conv4 <- pool3
I0514 13:06:21.024670 16573 net.cpp:411] conv4 -> conv4
I0514 13:06:21.027693 16573 net.cpp:150] Setting up conv4
I0514 13:06:21.027729 16573 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0514 13:06:21.027743 16573 net.cpp:165] Memory required for data: 152073200
I0514 13:06:21.027762 16573 layer_factory.hpp:77] Creating layer relu4
I0514 13:06:21.027796 16573 net.cpp:106] Creating Layer relu4
I0514 13:06:21.027811 16573 net.cpp:454] relu4 <- conv4
I0514 13:06:21.027827 16573 net.cpp:397] relu4 -> conv4 (in-place)
I0514 13:06:21.028322 16573 net.cpp:150] Setting up relu4
I0514 13:06:21.028347 16573 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0514 13:06:21.028359 16573 net.cpp:165] Memory required for data: 155702000
I0514 13:06:21.028372 16573 layer_factory.hpp:77] Creating layer pool4
I0514 13:06:21.028391 16573 net.cpp:106] Creating Layer pool4
I0514 13:06:21.028414 16573 net.cpp:454] pool4 <- conv4
I0514 13:06:21.028430 16573 net.cpp:411] pool4 -> pool4
I0514 13:06:21.028514 16573 net.cpp:150] Setting up pool4
I0514 13:06:21.028537 16573 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0514 13:06:21.028550 16573 net.cpp:165] Memory required for data: 157516400
I0514 13:06:21.028563 16573 layer_factory.hpp:77] Creating layer ip1
I0514 13:06:21.028592 16573 net.cpp:106] Creating Layer ip1
I0514 13:06:21.028606 16573 net.cpp:454] ip1 <- pool4
I0514 13:06:21.028622 16573 net.cpp:411] ip1 -> ip1
I0514 13:06:21.044512 16573 net.cpp:150] Setting up ip1
I0514 13:06:21.044544 16573 net.cpp:157] Top shape: 100 196 (19600)
I0514 13:06:21.044565 16573 net.cpp:165] Memory required for data: 157594800
I0514 13:06:21.044592 16573 layer_factory.hpp:77] Creating layer relu5
I0514 13:06:21.044613 16573 net.cpp:106] Creating Layer relu5
I0514 13:06:21.044638 16573 net.cpp:454] relu5 <- ip1
I0514 13:06:21.044656 16573 net.cpp:397] relu5 -> ip1 (in-place)
I0514 13:06:21.045020 16573 net.cpp:150] Setting up relu5
I0514 13:06:21.045042 16573 net.cpp:157] Top shape: 100 196 (19600)
I0514 13:06:21.045054 16573 net.cpp:165] Memory required for data: 157673200
I0514 13:06:21.045066 16573 layer_factory.hpp:77] Creating layer drop1
I0514 13:06:21.045097 16573 net.cpp:106] Creating Layer drop1
I0514 13:06:21.045111 16573 net.cpp:454] drop1 <- ip1
I0514 13:06:21.045127 16573 net.cpp:397] drop1 -> ip1 (in-place)
I0514 13:06:21.045202 16573 net.cpp:150] Setting up drop1
I0514 13:06:21.045218 16573 net.cpp:157] Top shape: 100 196 (19600)
I0514 13:06:21.045231 16573 net.cpp:165] Memory required for data: 157751600
I0514 13:06:21.045246 16573 layer_factory.hpp:77] Creating layer ip2
I0514 13:06:21.045267 16573 net.cpp:106] Creating Layer ip2
I0514 13:06:21.045286 16573 net.cpp:454] ip2 <- ip1
I0514 13:06:21.045303 16573 net.cpp:411] ip2 -> ip2
I0514 13:06:21.045804 16573 net.cpp:150] Setting up ip2
I0514 13:06:21.045822 16573 net.cpp:157] Top shape: 100 98 (9800)
I0514 13:06:21.045835 16573 net.cpp:165] Memory required for data: 157790800
I0514 13:06:21.045855 16573 layer_factory.hpp:77] Creating layer relu6
I0514 13:06:21.045876 16573 net.cpp:106] Creating Layer relu6
I0514 13:06:21.045891 16573 net.cpp:454] relu6 <- ip2
I0514 13:06:21.045905 16573 net.cpp:397] relu6 -> ip2 (in-place)
I0514 13:06:21.046463 16573 net.cpp:150] Setting up relu6
I0514 13:06:21.046488 16573 net.cpp:157] Top shape: 100 98 (9800)
I0514 13:06:21.046502 16573 net.cpp:165] Memory required for data: 157830000
I0514 13:06:21.046519 16573 layer_factory.hpp:77] Creating layer drop2
I0514 13:06:21.046541 16573 net.cpp:106] Creating Layer drop2
I0514 13:06:21.046555 16573 net.cpp:454] drop2 <- ip2
I0514 13:06:21.046571 16573 net.cpp:397] drop2 -> ip2 (in-place)
I0514 13:06:21.046622 16573 net.cpp:150] Setting up drop2
I0514 13:06:21.046644 16573 net.cpp:157] Top shape: 100 98 (9800)
I0514 13:06:21.046658 16573 net.cpp:165] Memory required for data: 157869200
I0514 13:06:21.046671 16573 layer_factory.hpp:77] Creating layer ip3
I0514 13:06:21.046686 16573 net.cpp:106] Creating Layer ip3
I0514 13:06:21.046702 16573 net.cpp:454] ip3 <- ip2
I0514 13:06:21.046723 16573 net.cpp:411] ip3 -> ip3
I0514 13:06:21.046952 16573 net.cpp:150] Setting up ip3
I0514 13:06:21.046972 16573 net.cpp:157] Top shape: 100 11 (1100)
I0514 13:06:21.046984 16573 net.cpp:165] Memory required for data: 157873600
I0514 13:06:21.047005 16573 layer_factory.hpp:77] Creating layer drop3
I0514 13:06:21.047026 16573 net.cpp:106] Creating Layer drop3
I0514 13:06:21.047040 16573 net.cpp:454] drop3 <- ip3
I0514 13:06:21.047055 16573 net.cpp:397] drop3 -> ip3 (in-place)
I0514 13:06:21.047103 16573 net.cpp:150] Setting up drop3
I0514 13:06:21.047127 16573 net.cpp:157] Top shape: 100 11 (1100)
I0514 13:06:21.047138 16573 net.cpp:165] Memory required for data: 157878000
I0514 13:06:21.047158 16573 layer_factory.hpp:77] Creating layer loss
I0514 13:06:21.047178 16573 net.cpp:106] Creating Layer loss
I0514 13:06:21.047194 16573 net.cpp:454] loss <- ip3
I0514 13:06:21.047206 16573 net.cpp:454] loss <- label
I0514 13:06:21.047229 16573 net.cpp:411] loss -> loss
I0514 13:06:21.047257 16573 layer_factory.hpp:77] Creating layer loss
I0514 13:06:21.047936 16573 net.cpp:150] Setting up loss
I0514 13:06:21.047958 16573 net.cpp:157] Top shape: (1)
I0514 13:06:21.047981 16573 net.cpp:160]     with loss weight 1
I0514 13:06:21.048040 16573 net.cpp:165] Memory required for data: 157878004
I0514 13:06:21.048056 16573 net.cpp:226] loss needs backward computation.
I0514 13:06:21.048070 16573 net.cpp:226] drop3 needs backward computation.
I0514 13:06:21.048084 16573 net.cpp:226] ip3 needs backward computation.
I0514 13:06:21.048097 16573 net.cpp:226] drop2 needs backward computation.
I0514 13:06:21.048110 16573 net.cpp:226] relu6 needs backward computation.
I0514 13:06:21.048125 16573 net.cpp:226] ip2 needs backward computation.
I0514 13:06:21.048143 16573 net.cpp:226] drop1 needs backward computation.
I0514 13:06:21.048156 16573 net.cpp:226] relu5 needs backward computation.
I0514 13:06:21.048169 16573 net.cpp:226] ip1 needs backward computation.
I0514 13:06:21.048184 16573 net.cpp:226] pool4 needs backward computation.
I0514 13:06:21.048197 16573 net.cpp:226] relu4 needs backward computation.
I0514 13:06:21.048209 16573 net.cpp:226] conv4 needs backward computation.
I0514 13:06:21.048223 16573 net.cpp:226] pool3 needs backward computation.
I0514 13:06:21.048238 16573 net.cpp:226] relu3 needs backward computation.
I0514 13:06:21.048266 16573 net.cpp:226] conv3 needs backward computation.
I0514 13:06:21.048280 16573 net.cpp:226] pool2 needs backward computation.
I0514 13:06:21.048295 16573 net.cpp:226] relu2 needs backward computation.
I0514 13:06:21.048306 16573 net.cpp:226] conv2 needs backward computation.
I0514 13:06:21.048321 16573 net.cpp:226] pool1 needs backward computation.
I0514 13:06:21.048332 16573 net.cpp:226] relu1 needs backward computation.
I0514 13:06:21.048348 16573 net.cpp:226] conv1 needs backward computation.
I0514 13:06:21.048370 16573 net.cpp:228] data_hdf5 does not need backward computation.
I0514 13:06:21.048382 16573 net.cpp:270] This network produces output loss
I0514 13:06:21.048409 16573 net.cpp:283] Network initialization done.
I0514 13:06:21.050148 16573 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0514 13:06:21.050227 16573 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0514 13:06:21.050609 16573 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0514 13:06:21.050818 16573 layer_factory.hpp:77] Creating layer data_hdf5
I0514 13:06:21.050840 16573 net.cpp:106] Creating Layer data_hdf5
I0514 13:06:21.050863 16573 net.cpp:411] data_hdf5 -> data
I0514 13:06:21.050882 16573 net.cpp:411] data_hdf5 -> label
I0514 13:06:21.050904 16573 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0514 13:06:21.052271 16573 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0514 13:06:42.507243 16573 net.cpp:150] Setting up data_hdf5
I0514 13:06:42.507416 16573 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0514 13:06:42.507436 16573 net.cpp:157] Top shape: 100 (100)
I0514 13:06:42.507448 16573 net.cpp:165] Memory required for data: 2540400
I0514 13:06:42.507463 16573 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0514 13:06:42.507491 16573 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0514 13:06:42.507510 16573 net.cpp:454] label_data_hdf5_1_split <- label
I0514 13:06:42.507546 16573 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0514 13:06:42.507570 16573 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0514 13:06:42.507652 16573 net.cpp:150] Setting up label_data_hdf5_1_split
I0514 13:06:42.507673 16573 net.cpp:157] Top shape: 100 (100)
I0514 13:06:42.507694 16573 net.cpp:157] Top shape: 100 (100)
I0514 13:06:42.507707 16573 net.cpp:165] Memory required for data: 2541200
I0514 13:06:42.507720 16573 layer_factory.hpp:77] Creating layer conv1
I0514 13:06:42.507755 16573 net.cpp:106] Creating Layer conv1
I0514 13:06:42.507769 16573 net.cpp:454] conv1 <- data
I0514 13:06:42.507786 16573 net.cpp:411] conv1 -> conv1
I0514 13:06:42.509769 16573 net.cpp:150] Setting up conv1
I0514 13:06:42.509795 16573 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0514 13:06:42.509815 16573 net.cpp:165] Memory required for data: 30189200
I0514 13:06:42.509840 16573 layer_factory.hpp:77] Creating layer relu1
I0514 13:06:42.509860 16573 net.cpp:106] Creating Layer relu1
I0514 13:06:42.509883 16573 net.cpp:454] relu1 <- conv1
I0514 13:06:42.509899 16573 net.cpp:397] relu1 -> conv1 (in-place)
I0514 13:06:42.510437 16573 net.cpp:150] Setting up relu1
I0514 13:06:42.510460 16573 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0514 13:06:42.510473 16573 net.cpp:165] Memory required for data: 57837200
I0514 13:06:42.510490 16573 layer_factory.hpp:77] Creating layer pool1
I0514 13:06:42.510517 16573 net.cpp:106] Creating Layer pool1
I0514 13:06:42.510530 16573 net.cpp:454] pool1 <- conv1
I0514 13:06:42.510546 16573 net.cpp:411] pool1 -> pool1
I0514 13:06:42.510637 16573 net.cpp:150] Setting up pool1
I0514 13:06:42.510653 16573 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0514 13:06:42.510668 16573 net.cpp:165] Memory required for data: 71661200
I0514 13:06:42.510689 16573 layer_factory.hpp:77] Creating layer conv2
I0514 13:06:42.510709 16573 net.cpp:106] Creating Layer conv2
I0514 13:06:42.510722 16573 net.cpp:454] conv2 <- pool1
I0514 13:06:42.510740 16573 net.cpp:411] conv2 -> conv2
I0514 13:06:42.512787 16573 net.cpp:150] Setting up conv2
I0514 13:06:42.512816 16573 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0514 13:06:42.512830 16573 net.cpp:165] Memory required for data: 91533200
I0514 13:06:42.512852 16573 layer_factory.hpp:77] Creating layer relu2
I0514 13:06:42.512881 16573 net.cpp:106] Creating Layer relu2
I0514 13:06:42.512897 16573 net.cpp:454] relu2 <- conv2
I0514 13:06:42.512912 16573 net.cpp:397] relu2 -> conv2 (in-place)
I0514 13:06:42.513280 16573 net.cpp:150] Setting up relu2
I0514 13:06:42.513300 16573 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0514 13:06:42.513314 16573 net.cpp:165] Memory required for data: 111405200
I0514 13:06:42.513326 16573 layer_factory.hpp:77] Creating layer pool2
I0514 13:06:42.513351 16573 net.cpp:106] Creating Layer pool2
I0514 13:06:42.513365 16573 net.cpp:454] pool2 <- conv2
I0514 13:06:42.513381 16573 net.cpp:411] pool2 -> pool2
I0514 13:06:42.513473 16573 net.cpp:150] Setting up pool2
I0514 13:06:42.513491 16573 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0514 13:06:42.513504 16573 net.cpp:165] Memory required for data: 121341200
I0514 13:06:42.513517 16573 layer_factory.hpp:77] Creating layer conv3
I0514 13:06:42.513542 16573 net.cpp:106] Creating Layer conv3
I0514 13:06:42.513563 16573 net.cpp:454] conv3 <- pool2
I0514 13:06:42.513579 16573 net.cpp:411] conv3 -> conv3
I0514 13:06:42.515627 16573 net.cpp:150] Setting up conv3
I0514 13:06:42.515653 16573 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0514 13:06:42.515673 16573 net.cpp:165] Memory required for data: 132182800
I0514 13:06:42.515709 16573 layer_factory.hpp:77] Creating layer relu3
I0514 13:06:42.515739 16573 net.cpp:106] Creating Layer relu3
I0514 13:06:42.515754 16573 net.cpp:454] relu3 <- conv3
I0514 13:06:42.515771 16573 net.cpp:397] relu3 -> conv3 (in-place)
I0514 13:06:42.516270 16573 net.cpp:150] Setting up relu3
I0514 13:06:42.516294 16573 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0514 13:06:42.516306 16573 net.cpp:165] Memory required for data: 143024400
I0514 13:06:42.516322 16573 layer_factory.hpp:77] Creating layer pool3
I0514 13:06:42.516346 16573 net.cpp:106] Creating Layer pool3
I0514 13:06:42.516360 16573 net.cpp:454] pool3 <- conv3
I0514 13:06:42.516376 16573 net.cpp:411] pool3 -> pool3
I0514 13:06:42.516463 16573 net.cpp:150] Setting up pool3
I0514 13:06:42.516482 16573 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0514 13:06:42.516494 16573 net.cpp:165] Memory required for data: 148445200
I0514 13:06:42.516510 16573 layer_factory.hpp:77] Creating layer conv4
I0514 13:06:42.516535 16573 net.cpp:106] Creating Layer conv4
I0514 13:06:42.516549 16573 net.cpp:454] conv4 <- pool3
I0514 13:06:42.516572 16573 net.cpp:411] conv4 -> conv4
I0514 13:06:42.518734 16573 net.cpp:150] Setting up conv4
I0514 13:06:42.518762 16573 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0514 13:06:42.518776 16573 net.cpp:165] Memory required for data: 152074000
I0514 13:06:42.518795 16573 layer_factory.hpp:77] Creating layer relu4
I0514 13:06:42.518815 16573 net.cpp:106] Creating Layer relu4
I0514 13:06:42.518838 16573 net.cpp:454] relu4 <- conv4
I0514 13:06:42.518854 16573 net.cpp:397] relu4 -> conv4 (in-place)
I0514 13:06:42.519351 16573 net.cpp:150] Setting up relu4
I0514 13:06:42.519376 16573 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0514 13:06:42.519388 16573 net.cpp:165] Memory required for data: 155702800
I0514 13:06:42.519404 16573 layer_factory.hpp:77] Creating layer pool4
I0514 13:06:42.519428 16573 net.cpp:106] Creating Layer pool4
I0514 13:06:42.519443 16573 net.cpp:454] pool4 <- conv4
I0514 13:06:42.519459 16573 net.cpp:411] pool4 -> pool4
I0514 13:06:42.519546 16573 net.cpp:150] Setting up pool4
I0514 13:06:42.519563 16573 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0514 13:06:42.519577 16573 net.cpp:165] Memory required for data: 157517200
I0514 13:06:42.519590 16573 layer_factory.hpp:77] Creating layer ip1
I0514 13:06:42.519613 16573 net.cpp:106] Creating Layer ip1
I0514 13:06:42.519628 16573 net.cpp:454] ip1 <- pool4
I0514 13:06:42.519650 16573 net.cpp:411] ip1 -> ip1
I0514 13:06:42.535531 16573 net.cpp:150] Setting up ip1
I0514 13:06:42.535563 16573 net.cpp:157] Top shape: 100 196 (19600)
I0514 13:06:42.535585 16573 net.cpp:165] Memory required for data: 157595600
I0514 13:06:42.535612 16573 layer_factory.hpp:77] Creating layer relu5
I0514 13:06:42.535634 16573 net.cpp:106] Creating Layer relu5
I0514 13:06:42.535660 16573 net.cpp:454] relu5 <- ip1
I0514 13:06:42.535676 16573 net.cpp:397] relu5 -> ip1 (in-place)
I0514 13:06:42.536046 16573 net.cpp:150] Setting up relu5
I0514 13:06:42.536065 16573 net.cpp:157] Top shape: 100 196 (19600)
I0514 13:06:42.536078 16573 net.cpp:165] Memory required for data: 157674000
I0514 13:06:42.536090 16573 layer_factory.hpp:77] Creating layer drop1
I0514 13:06:42.536123 16573 net.cpp:106] Creating Layer drop1
I0514 13:06:42.536137 16573 net.cpp:454] drop1 <- ip1
I0514 13:06:42.536154 16573 net.cpp:397] drop1 -> ip1 (in-place)
I0514 13:06:42.536211 16573 net.cpp:150] Setting up drop1
I0514 13:06:42.536227 16573 net.cpp:157] Top shape: 100 196 (19600)
I0514 13:06:42.536239 16573 net.cpp:165] Memory required for data: 157752400
I0514 13:06:42.536252 16573 layer_factory.hpp:77] Creating layer ip2
I0514 13:06:42.536269 16573 net.cpp:106] Creating Layer ip2
I0514 13:06:42.536285 16573 net.cpp:454] ip2 <- ip1
I0514 13:06:42.536309 16573 net.cpp:411] ip2 -> ip2
I0514 13:06:42.536814 16573 net.cpp:150] Setting up ip2
I0514 13:06:42.536834 16573 net.cpp:157] Top shape: 100 98 (9800)
I0514 13:06:42.536846 16573 net.cpp:165] Memory required for data: 157791600
I0514 13:06:42.536867 16573 layer_factory.hpp:77] Creating layer relu6
I0514 13:06:42.536902 16573 net.cpp:106] Creating Layer relu6
I0514 13:06:42.536916 16573 net.cpp:454] relu6 <- ip2
I0514 13:06:42.536932 16573 net.cpp:397] relu6 -> ip2 (in-place)
I0514 13:06:42.537498 16573 net.cpp:150] Setting up relu6
I0514 13:06:42.537523 16573 net.cpp:157] Top shape: 100 98 (9800)
I0514 13:06:42.537535 16573 net.cpp:165] Memory required for data: 157830800
I0514 13:06:42.537549 16573 layer_factory.hpp:77] Creating layer drop2
I0514 13:06:42.537567 16573 net.cpp:106] Creating Layer drop2
I0514 13:06:42.537590 16573 net.cpp:454] drop2 <- ip2
I0514 13:06:42.537606 16573 net.cpp:397] drop2 -> ip2 (in-place)
I0514 13:06:42.537659 16573 net.cpp:150] Setting up drop2
I0514 13:06:42.537681 16573 net.cpp:157] Top shape: 100 98 (9800)
I0514 13:06:42.537694 16573 net.cpp:165] Memory required for data: 157870000
I0514 13:06:42.537714 16573 layer_factory.hpp:77] Creating layer ip3
I0514 13:06:42.537730 16573 net.cpp:106] Creating Layer ip3
I0514 13:06:42.537744 16573 net.cpp:454] ip3 <- ip2
I0514 13:06:42.537762 16573 net.cpp:411] ip3 -> ip3
I0514 13:06:42.538008 16573 net.cpp:150] Setting up ip3
I0514 13:06:42.538038 16573 net.cpp:157] Top shape: 100 11 (1100)
I0514 13:06:42.538053 16573 net.cpp:165] Memory required for data: 157874400
I0514 13:06:42.538079 16573 layer_factory.hpp:77] Creating layer drop3
I0514 13:06:42.538103 16573 net.cpp:106] Creating Layer drop3
I0514 13:06:42.538116 16573 net.cpp:454] drop3 <- ip3
I0514 13:06:42.538141 16573 net.cpp:397] drop3 -> ip3 (in-place)
I0514 13:06:42.538197 16573 net.cpp:150] Setting up drop3
I0514 13:06:42.538213 16573 net.cpp:157] Top shape: 100 11 (1100)
I0514 13:06:42.538225 16573 net.cpp:165] Memory required for data: 157878800
I0514 13:06:42.538239 16573 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0514 13:06:42.538254 16573 net.cpp:106] Creating Layer ip3_drop3_0_split
I0514 13:06:42.538269 16573 net.cpp:454] ip3_drop3_0_split <- ip3
I0514 13:06:42.538290 16573 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0514 13:06:42.538308 16573 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0514 13:06:42.538388 16573 net.cpp:150] Setting up ip3_drop3_0_split
I0514 13:06:42.538413 16573 net.cpp:157] Top shape: 100 11 (1100)
I0514 13:06:42.538429 16573 net.cpp:157] Top shape: 100 11 (1100)
I0514 13:06:42.538442 16573 net.cpp:165] Memory required for data: 157887600
I0514 13:06:42.538455 16573 layer_factory.hpp:77] Creating layer accuracy
I0514 13:06:42.538480 16573 net.cpp:106] Creating Layer accuracy
I0514 13:06:42.538499 16573 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0514 13:06:42.538514 16573 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0514 13:06:42.538532 16573 net.cpp:411] accuracy -> accuracy
I0514 13:06:42.538559 16573 net.cpp:150] Setting up accuracy
I0514 13:06:42.538578 16573 net.cpp:157] Top shape: (1)
I0514 13:06:42.538595 16573 net.cpp:165] Memory required for data: 157887604
I0514 13:06:42.538609 16573 layer_factory.hpp:77] Creating layer loss
I0514 13:06:42.538625 16573 net.cpp:106] Creating Layer loss
I0514 13:06:42.538640 16573 net.cpp:454] loss <- ip3_drop3_0_split_1
I0514 13:06:42.538653 16573 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0514 13:06:42.538671 16573 net.cpp:411] loss -> loss
I0514 13:06:42.538697 16573 layer_factory.hpp:77] Creating layer loss
I0514 13:06:42.539206 16573 net.cpp:150] Setting up loss
I0514 13:06:42.539227 16573 net.cpp:157] Top shape: (1)
I0514 13:06:42.539240 16573 net.cpp:160]     with loss weight 1
I0514 13:06:42.539268 16573 net.cpp:165] Memory required for data: 157887608
I0514 13:06:42.539288 16573 net.cpp:226] loss needs backward computation.
I0514 13:06:42.539304 16573 net.cpp:228] accuracy does not need backward computation.
I0514 13:06:42.539317 16573 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0514 13:06:42.539331 16573 net.cpp:226] drop3 needs backward computation.
I0514 13:06:42.539343 16573 net.cpp:226] ip3 needs backward computation.
I0514 13:06:42.539360 16573 net.cpp:226] drop2 needs backward computation.
I0514 13:06:42.539387 16573 net.cpp:226] relu6 needs backward computation.
I0514 13:06:42.539402 16573 net.cpp:226] ip2 needs backward computation.
I0514 13:06:42.539417 16573 net.cpp:226] drop1 needs backward computation.
I0514 13:06:42.539430 16573 net.cpp:226] relu5 needs backward computation.
I0514 13:06:42.539443 16573 net.cpp:226] ip1 needs backward computation.
I0514 13:06:42.539454 16573 net.cpp:226] pool4 needs backward computation.
I0514 13:06:42.539470 16573 net.cpp:226] relu4 needs backward computation.
I0514 13:06:42.539489 16573 net.cpp:226] conv4 needs backward computation.
I0514 13:06:42.539504 16573 net.cpp:226] pool3 needs backward computation.
I0514 13:06:42.539517 16573 net.cpp:226] relu3 needs backward computation.
I0514 13:06:42.539531 16573 net.cpp:226] conv3 needs backward computation.
I0514 13:06:42.539544 16573 net.cpp:226] pool2 needs backward computation.
I0514 13:06:42.539557 16573 net.cpp:226] relu2 needs backward computation.
I0514 13:06:42.539569 16573 net.cpp:226] conv2 needs backward computation.
I0514 13:06:42.539585 16573 net.cpp:226] pool1 needs backward computation.
I0514 13:06:42.539604 16573 net.cpp:226] relu1 needs backward computation.
I0514 13:06:42.539618 16573 net.cpp:226] conv1 needs backward computation.
I0514 13:06:42.539633 16573 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0514 13:06:42.539646 16573 net.cpp:228] data_hdf5 does not need backward computation.
I0514 13:06:42.539659 16573 net.cpp:270] This network produces output accuracy
I0514 13:06:42.539671 16573 net.cpp:270] This network produces output loss
I0514 13:06:42.539710 16573 net.cpp:283] Network initialization done.
I0514 13:06:42.539855 16573 solver.cpp:60] Solver scaffolding done.
I0514 13:06:42.541034 16573 caffe.cpp:202] Resuming from /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_6000.solverstate
I0514 13:06:42.589628 16573 sgd_solver.cpp:318] SGDSolver: restoring history
I0514 13:06:42.595268 16573 caffe.cpp:212] Starting Optimization
I0514 13:06:42.595314 16573 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0514 13:06:42.595329 16573 solver.cpp:289] Learning Rate Policy: fixed
I0514 13:06:42.596587 16573 solver.cpp:341] Iteration 6000, Testing net (#0)
I0514 13:07:30.442428 16573 solver.cpp:409]     Test net output #0: accuracy = 0.790834
I0514 13:07:30.442600 16573 solver.cpp:409]     Test net output #1: loss = 0.739321 (* 1 = 0.739321 loss)
I0514 13:07:30.475572 16573 solver.cpp:237] Iteration 6000, loss = 1.44975
I0514 13:07:30.475615 16573 solver.cpp:253]     Train net output #0: loss = 1.44975 (* 1 = 1.44975 loss)
I0514 13:07:30.475637 16573 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0514 13:07:59.506923 16573 solver.cpp:341] Iteration 6500, Testing net (#0)
I0514 13:08:46.481132 16573 solver.cpp:409]     Test net output #0: accuracy = 0.800255
I0514 13:08:46.481297 16573 solver.cpp:409]     Test net output #1: loss = 0.731681 (* 1 = 0.731681 loss)
I0514 13:08:46.499230 16573 solver.cpp:237] Iteration 6500, loss = 1.47055
I0514 13:08:46.499263 16573 solver.cpp:253]     Train net output #0: loss = 1.47055 (* 1 = 1.47055 loss)
I0514 13:08:46.499279 16573 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0514 13:09:15.580569 16573 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_7000.caffemodel
I0514 13:09:15.662259 16573 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_7000.solverstate
I0514 13:09:15.690320 16573 solver.cpp:341] Iteration 7000, Testing net (#0)
I0514 13:10:23.503240 16573 solver.cpp:409]     Test net output #0: accuracy = 0.800881
I0514 13:10:23.503406 16573 solver.cpp:409]     Test net output #1: loss = 0.707518 (* 1 = 0.707518 loss)
I0514 13:10:45.777602 16573 solver.cpp:237] Iteration 7000, loss = 1.54878
I0514 13:10:45.777673 16573 solver.cpp:253]     Train net output #0: loss = 1.54878 (* 1 = 1.54878 loss)
I0514 13:10:45.777693 16573 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0514 13:11:14.818068 16573 solver.cpp:341] Iteration 7500, Testing net (#0)
I0514 13:12:01.488458 16573 solver.cpp:409]     Test net output #0: accuracy = 0.803187
I0514 13:12:01.488622 16573 solver.cpp:409]     Test net output #1: loss = 0.64035 (* 1 = 0.64035 loss)
I0514 13:12:01.506472 16573 solver.cpp:237] Iteration 7500, loss = 1.22975
I0514 13:12:01.506505 16573 solver.cpp:253]     Train net output #0: loss = 1.22975 (* 1 = 1.22975 loss)
I0514 13:12:01.506521 16573 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0514 13:12:30.545855 16573 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_8000.caffemodel
I0514 13:12:30.627581 16573 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_8000.solverstate
I0514 13:12:30.656262 16573 solver.cpp:341] Iteration 8000, Testing net (#0)
I0514 13:13:38.614874 16573 solver.cpp:409]     Test net output #0: accuracy = 0.808181
I0514 13:13:38.615041 16573 solver.cpp:409]     Test net output #1: loss = 0.635202 (* 1 = 0.635202 loss)
I0514 13:14:00.860873 16573 solver.cpp:237] Iteration 8000, loss = 1.25606
I0514 13:14:00.860939 16573 solver.cpp:253]     Train net output #0: loss = 1.25606 (* 1 = 1.25606 loss)
I0514 13:14:00.860967 16573 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0514 13:14:29.875885 16573 solver.cpp:341] Iteration 8500, Testing net (#0)
I0514 13:15:16.874900 16573 solver.cpp:409]     Test net output #0: accuracy = 0.815034
I0514 13:15:16.875066 16573 solver.cpp:409]     Test net output #1: loss = 0.690454 (* 1 = 0.690454 loss)
I0514 13:15:16.892928 16573 solver.cpp:237] Iteration 8500, loss = 1.34422
I0514 13:15:16.892957 16573 solver.cpp:253]     Train net output #0: loss = 1.34422 (* 1 = 1.34422 loss)
I0514 13:15:16.892976 16573 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0514 13:15:45.925678 16573 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_9000.caffemodel
I0514 13:15:46.006835 16573 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_depend/dep_check_10min_2016-05-14T12.44.04.956355_iter_9000.solverstate
I0514 13:15:46.035377 16573 solver.cpp:341] Iteration 9000, Testing net (#0)
aprun: Apid 11199441: Caught signal Terminated, sending to application
*** Aborted at 1463246175 (unix time) try "date -d @1463246175" if you are using GNU date ***
aprun: Apid 11199441: Caught signal Terminated, sending to application
PC: @     0x2aaaaaaca834 ([vdso]+0x833)
aprun: Apid 11199441: Caught signal Terminated, sending to application
*** SIGTERM (@0x40ba) received by PID 16573 (TID 0x2aaac746f900) from PID 16570; stack trace: ***
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab7c78850 (unknown)
    @     0x2aaaaaaca834 ([vdso]+0x833)
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab82072d0 maybe_syscall_gettime_cpu
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab82074b0 __GI_clock_gettime
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab9898f3e (unknown)
    @     0x2aaab928ec5b (unknown)
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab926d723 (unknown)
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab92655e1 (unknown)
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab9266356 (unknown)
    @     0x2aaab91d5562 (unknown)
=>> PBS: job killed: walltime 626 exceeded limit 600
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab91d56ba (unknown)
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaab91b8715 cuMemcpy
    @     0x2aaaaacf9e92 (unknown)
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @     0x2aaaaacde306 (unknown)
    @     0x2aaaaad00328 cudaMemcpy
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x637300 caffe::caffe_gpu_memcpy()
    @           0x606cf0 caffe::SyncedMemory::to_gpu()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x605ef9 caffe::SyncedMemory::gpu_data()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x5f0302 caffe::Blob<>::gpu_data()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x612c57 caffe::InnerProductLayer<>::Forward_gpu()
    @           0x4a6872 caffe::Net<>::ForwardFromTo()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x4a6987 caffe::Net<>::ForwardPrefilled()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x59f48f caffe::Solver<>::Test()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x59fdde caffe::Solver<>::TestAll()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x59ff21 caffe::Solver<>::Step()
    @           0x5a0ac5 caffe::Solver<>::Solve()
aprun: Apid 11199441: Caught signal Terminated, sending to application
    @           0x43b3b8 train()
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
    @           0x438669 (unknown)
