2806414
I0521 10:22:48.091956 11110 caffe.cpp:184] Using GPUs 0
I0521 10:22:48.520615 11110 solver.cpp:48] Initializing solver from parameters: 
test_iter: 157
test_interval: 315
base_lr: 0.0025
display: 15
max_iter: 1578
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 157
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951.prototxt"
I0521 10:22:48.522358 11110 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951.prototxt
I0521 10:22:48.538063 11110 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 10:22:48.538121 11110 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 10:22:48.538466 11110 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 950
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 10:22:48.538643 11110 layer_factory.hpp:77] Creating layer data_hdf5
I0521 10:22:48.538667 11110 net.cpp:106] Creating Layer data_hdf5
I0521 10:22:48.538681 11110 net.cpp:411] data_hdf5 -> data
I0521 10:22:48.538715 11110 net.cpp:411] data_hdf5 -> label
I0521 10:22:48.538748 11110 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 10:22:48.540024 11110 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 10:22:48.542259 11110 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 10:23:10.031934 11110 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 10:23:10.037086 11110 net.cpp:150] Setting up data_hdf5
I0521 10:23:10.037125 11110 net.cpp:157] Top shape: 950 1 127 50 (6032500)
I0521 10:23:10.037140 11110 net.cpp:157] Top shape: 950 (950)
I0521 10:23:10.037153 11110 net.cpp:165] Memory required for data: 24133800
I0521 10:23:10.037165 11110 layer_factory.hpp:77] Creating layer conv1
I0521 10:23:10.037199 11110 net.cpp:106] Creating Layer conv1
I0521 10:23:10.037210 11110 net.cpp:454] conv1 <- data
I0521 10:23:10.037232 11110 net.cpp:411] conv1 -> conv1
I0521 10:23:10.401049 11110 net.cpp:150] Setting up conv1
I0521 10:23:10.401096 11110 net.cpp:157] Top shape: 950 12 120 48 (65664000)
I0521 10:23:10.401108 11110 net.cpp:165] Memory required for data: 286789800
I0521 10:23:10.401136 11110 layer_factory.hpp:77] Creating layer relu1
I0521 10:23:10.401157 11110 net.cpp:106] Creating Layer relu1
I0521 10:23:10.401168 11110 net.cpp:454] relu1 <- conv1
I0521 10:23:10.401181 11110 net.cpp:397] relu1 -> conv1 (in-place)
I0521 10:23:10.401695 11110 net.cpp:150] Setting up relu1
I0521 10:23:10.401712 11110 net.cpp:157] Top shape: 950 12 120 48 (65664000)
I0521 10:23:10.401722 11110 net.cpp:165] Memory required for data: 549445800
I0521 10:23:10.401733 11110 layer_factory.hpp:77] Creating layer pool1
I0521 10:23:10.401749 11110 net.cpp:106] Creating Layer pool1
I0521 10:23:10.401759 11110 net.cpp:454] pool1 <- conv1
I0521 10:23:10.401772 11110 net.cpp:411] pool1 -> pool1
I0521 10:23:10.401852 11110 net.cpp:150] Setting up pool1
I0521 10:23:10.401866 11110 net.cpp:157] Top shape: 950 12 60 48 (32832000)
I0521 10:23:10.401876 11110 net.cpp:165] Memory required for data: 680773800
I0521 10:23:10.401886 11110 layer_factory.hpp:77] Creating layer conv2
I0521 10:23:10.401907 11110 net.cpp:106] Creating Layer conv2
I0521 10:23:10.401917 11110 net.cpp:454] conv2 <- pool1
I0521 10:23:10.401931 11110 net.cpp:411] conv2 -> conv2
I0521 10:23:10.404597 11110 net.cpp:150] Setting up conv2
I0521 10:23:10.404624 11110 net.cpp:157] Top shape: 950 20 54 46 (47196000)
I0521 10:23:10.404635 11110 net.cpp:165] Memory required for data: 869557800
I0521 10:23:10.404655 11110 layer_factory.hpp:77] Creating layer relu2
I0521 10:23:10.404670 11110 net.cpp:106] Creating Layer relu2
I0521 10:23:10.404680 11110 net.cpp:454] relu2 <- conv2
I0521 10:23:10.404692 11110 net.cpp:397] relu2 -> conv2 (in-place)
I0521 10:23:10.405021 11110 net.cpp:150] Setting up relu2
I0521 10:23:10.405035 11110 net.cpp:157] Top shape: 950 20 54 46 (47196000)
I0521 10:23:10.405045 11110 net.cpp:165] Memory required for data: 1058341800
I0521 10:23:10.405056 11110 layer_factory.hpp:77] Creating layer pool2
I0521 10:23:10.405068 11110 net.cpp:106] Creating Layer pool2
I0521 10:23:10.405079 11110 net.cpp:454] pool2 <- conv2
I0521 10:23:10.405103 11110 net.cpp:411] pool2 -> pool2
I0521 10:23:10.405172 11110 net.cpp:150] Setting up pool2
I0521 10:23:10.405185 11110 net.cpp:157] Top shape: 950 20 27 46 (23598000)
I0521 10:23:10.405195 11110 net.cpp:165] Memory required for data: 1152733800
I0521 10:23:10.405205 11110 layer_factory.hpp:77] Creating layer conv3
I0521 10:23:10.405220 11110 net.cpp:106] Creating Layer conv3
I0521 10:23:10.405231 11110 net.cpp:454] conv3 <- pool2
I0521 10:23:10.405244 11110 net.cpp:411] conv3 -> conv3
I0521 10:23:10.407155 11110 net.cpp:150] Setting up conv3
I0521 10:23:10.407178 11110 net.cpp:157] Top shape: 950 28 22 44 (25748800)
I0521 10:23:10.407199 11110 net.cpp:165] Memory required for data: 1255729000
I0521 10:23:10.407218 11110 layer_factory.hpp:77] Creating layer relu3
I0521 10:23:10.407234 11110 net.cpp:106] Creating Layer relu3
I0521 10:23:10.407244 11110 net.cpp:454] relu3 <- conv3
I0521 10:23:10.407258 11110 net.cpp:397] relu3 -> conv3 (in-place)
I0521 10:23:10.407726 11110 net.cpp:150] Setting up relu3
I0521 10:23:10.407742 11110 net.cpp:157] Top shape: 950 28 22 44 (25748800)
I0521 10:23:10.407753 11110 net.cpp:165] Memory required for data: 1358724200
I0521 10:23:10.407763 11110 layer_factory.hpp:77] Creating layer pool3
I0521 10:23:10.407776 11110 net.cpp:106] Creating Layer pool3
I0521 10:23:10.407786 11110 net.cpp:454] pool3 <- conv3
I0521 10:23:10.407799 11110 net.cpp:411] pool3 -> pool3
I0521 10:23:10.407866 11110 net.cpp:150] Setting up pool3
I0521 10:23:10.407879 11110 net.cpp:157] Top shape: 950 28 11 44 (12874400)
I0521 10:23:10.407889 11110 net.cpp:165] Memory required for data: 1410221800
I0521 10:23:10.407899 11110 layer_factory.hpp:77] Creating layer conv4
I0521 10:23:10.407914 11110 net.cpp:106] Creating Layer conv4
I0521 10:23:10.407925 11110 net.cpp:454] conv4 <- pool3
I0521 10:23:10.407938 11110 net.cpp:411] conv4 -> conv4
I0521 10:23:10.410645 11110 net.cpp:150] Setting up conv4
I0521 10:23:10.410672 11110 net.cpp:157] Top shape: 950 36 6 42 (8618400)
I0521 10:23:10.410682 11110 net.cpp:165] Memory required for data: 1444695400
I0521 10:23:10.410697 11110 layer_factory.hpp:77] Creating layer relu4
I0521 10:23:10.410712 11110 net.cpp:106] Creating Layer relu4
I0521 10:23:10.410722 11110 net.cpp:454] relu4 <- conv4
I0521 10:23:10.410735 11110 net.cpp:397] relu4 -> conv4 (in-place)
I0521 10:23:10.411209 11110 net.cpp:150] Setting up relu4
I0521 10:23:10.411226 11110 net.cpp:157] Top shape: 950 36 6 42 (8618400)
I0521 10:23:10.411237 11110 net.cpp:165] Memory required for data: 1479169000
I0521 10:23:10.411247 11110 layer_factory.hpp:77] Creating layer pool4
I0521 10:23:10.411259 11110 net.cpp:106] Creating Layer pool4
I0521 10:23:10.411269 11110 net.cpp:454] pool4 <- conv4
I0521 10:23:10.411283 11110 net.cpp:411] pool4 -> pool4
I0521 10:23:10.411350 11110 net.cpp:150] Setting up pool4
I0521 10:23:10.411365 11110 net.cpp:157] Top shape: 950 36 3 42 (4309200)
I0521 10:23:10.411375 11110 net.cpp:165] Memory required for data: 1496405800
I0521 10:23:10.411386 11110 layer_factory.hpp:77] Creating layer ip1
I0521 10:23:10.411403 11110 net.cpp:106] Creating Layer ip1
I0521 10:23:10.411413 11110 net.cpp:454] ip1 <- pool4
I0521 10:23:10.411427 11110 net.cpp:411] ip1 -> ip1
I0521 10:23:10.426805 11110 net.cpp:150] Setting up ip1
I0521 10:23:10.426834 11110 net.cpp:157] Top shape: 950 196 (186200)
I0521 10:23:10.426846 11110 net.cpp:165] Memory required for data: 1497150600
I0521 10:23:10.426868 11110 layer_factory.hpp:77] Creating layer relu5
I0521 10:23:10.426882 11110 net.cpp:106] Creating Layer relu5
I0521 10:23:10.426893 11110 net.cpp:454] relu5 <- ip1
I0521 10:23:10.426906 11110 net.cpp:397] relu5 -> ip1 (in-place)
I0521 10:23:10.427256 11110 net.cpp:150] Setting up relu5
I0521 10:23:10.427270 11110 net.cpp:157] Top shape: 950 196 (186200)
I0521 10:23:10.427280 11110 net.cpp:165] Memory required for data: 1497895400
I0521 10:23:10.427290 11110 layer_factory.hpp:77] Creating layer drop1
I0521 10:23:10.427311 11110 net.cpp:106] Creating Layer drop1
I0521 10:23:10.427322 11110 net.cpp:454] drop1 <- ip1
I0521 10:23:10.427347 11110 net.cpp:397] drop1 -> ip1 (in-place)
I0521 10:23:10.427393 11110 net.cpp:150] Setting up drop1
I0521 10:23:10.427407 11110 net.cpp:157] Top shape: 950 196 (186200)
I0521 10:23:10.427417 11110 net.cpp:165] Memory required for data: 1498640200
I0521 10:23:10.427428 11110 layer_factory.hpp:77] Creating layer ip2
I0521 10:23:10.427445 11110 net.cpp:106] Creating Layer ip2
I0521 10:23:10.427456 11110 net.cpp:454] ip2 <- ip1
I0521 10:23:10.427469 11110 net.cpp:411] ip2 -> ip2
I0521 10:23:10.427934 11110 net.cpp:150] Setting up ip2
I0521 10:23:10.427948 11110 net.cpp:157] Top shape: 950 98 (93100)
I0521 10:23:10.427958 11110 net.cpp:165] Memory required for data: 1499012600
I0521 10:23:10.427973 11110 layer_factory.hpp:77] Creating layer relu6
I0521 10:23:10.427985 11110 net.cpp:106] Creating Layer relu6
I0521 10:23:10.427995 11110 net.cpp:454] relu6 <- ip2
I0521 10:23:10.428007 11110 net.cpp:397] relu6 -> ip2 (in-place)
I0521 10:23:10.428522 11110 net.cpp:150] Setting up relu6
I0521 10:23:10.428539 11110 net.cpp:157] Top shape: 950 98 (93100)
I0521 10:23:10.428550 11110 net.cpp:165] Memory required for data: 1499385000
I0521 10:23:10.428560 11110 layer_factory.hpp:77] Creating layer drop2
I0521 10:23:10.428572 11110 net.cpp:106] Creating Layer drop2
I0521 10:23:10.428581 11110 net.cpp:454] drop2 <- ip2
I0521 10:23:10.428594 11110 net.cpp:397] drop2 -> ip2 (in-place)
I0521 10:23:10.428635 11110 net.cpp:150] Setting up drop2
I0521 10:23:10.428648 11110 net.cpp:157] Top shape: 950 98 (93100)
I0521 10:23:10.428659 11110 net.cpp:165] Memory required for data: 1499757400
I0521 10:23:10.428669 11110 layer_factory.hpp:77] Creating layer ip3
I0521 10:23:10.428683 11110 net.cpp:106] Creating Layer ip3
I0521 10:23:10.428692 11110 net.cpp:454] ip3 <- ip2
I0521 10:23:10.428705 11110 net.cpp:411] ip3 -> ip3
I0521 10:23:10.428915 11110 net.cpp:150] Setting up ip3
I0521 10:23:10.428928 11110 net.cpp:157] Top shape: 950 11 (10450)
I0521 10:23:10.428939 11110 net.cpp:165] Memory required for data: 1499799200
I0521 10:23:10.428954 11110 layer_factory.hpp:77] Creating layer drop3
I0521 10:23:10.428966 11110 net.cpp:106] Creating Layer drop3
I0521 10:23:10.428977 11110 net.cpp:454] drop3 <- ip3
I0521 10:23:10.428988 11110 net.cpp:397] drop3 -> ip3 (in-place)
I0521 10:23:10.429028 11110 net.cpp:150] Setting up drop3
I0521 10:23:10.429039 11110 net.cpp:157] Top shape: 950 11 (10450)
I0521 10:23:10.429050 11110 net.cpp:165] Memory required for data: 1499841000
I0521 10:23:10.429059 11110 layer_factory.hpp:77] Creating layer loss
I0521 10:23:10.429080 11110 net.cpp:106] Creating Layer loss
I0521 10:23:10.429090 11110 net.cpp:454] loss <- ip3
I0521 10:23:10.429101 11110 net.cpp:454] loss <- label
I0521 10:23:10.429113 11110 net.cpp:411] loss -> loss
I0521 10:23:10.429131 11110 layer_factory.hpp:77] Creating layer loss
I0521 10:23:10.429786 11110 net.cpp:150] Setting up loss
I0521 10:23:10.429807 11110 net.cpp:157] Top shape: (1)
I0521 10:23:10.429821 11110 net.cpp:160]     with loss weight 1
I0521 10:23:10.429865 11110 net.cpp:165] Memory required for data: 1499841004
I0521 10:23:10.429877 11110 net.cpp:226] loss needs backward computation.
I0521 10:23:10.429888 11110 net.cpp:226] drop3 needs backward computation.
I0521 10:23:10.429898 11110 net.cpp:226] ip3 needs backward computation.
I0521 10:23:10.429909 11110 net.cpp:226] drop2 needs backward computation.
I0521 10:23:10.429919 11110 net.cpp:226] relu6 needs backward computation.
I0521 10:23:10.429929 11110 net.cpp:226] ip2 needs backward computation.
I0521 10:23:10.429939 11110 net.cpp:226] drop1 needs backward computation.
I0521 10:23:10.429949 11110 net.cpp:226] relu5 needs backward computation.
I0521 10:23:10.429958 11110 net.cpp:226] ip1 needs backward computation.
I0521 10:23:10.429968 11110 net.cpp:226] pool4 needs backward computation.
I0521 10:23:10.429978 11110 net.cpp:226] relu4 needs backward computation.
I0521 10:23:10.429988 11110 net.cpp:226] conv4 needs backward computation.
I0521 10:23:10.429999 11110 net.cpp:226] pool3 needs backward computation.
I0521 10:23:10.430018 11110 net.cpp:226] relu3 needs backward computation.
I0521 10:23:10.430028 11110 net.cpp:226] conv3 needs backward computation.
I0521 10:23:10.430039 11110 net.cpp:226] pool2 needs backward computation.
I0521 10:23:10.430049 11110 net.cpp:226] relu2 needs backward computation.
I0521 10:23:10.430058 11110 net.cpp:226] conv2 needs backward computation.
I0521 10:23:10.430068 11110 net.cpp:226] pool1 needs backward computation.
I0521 10:23:10.430078 11110 net.cpp:226] relu1 needs backward computation.
I0521 10:23:10.430088 11110 net.cpp:226] conv1 needs backward computation.
I0521 10:23:10.430099 11110 net.cpp:228] data_hdf5 does not need backward computation.
I0521 10:23:10.430109 11110 net.cpp:270] This network produces output loss
I0521 10:23:10.430132 11110 net.cpp:283] Network initialization done.
I0521 10:23:10.431759 11110 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951.prototxt
I0521 10:23:10.431831 11110 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 10:23:10.432190 11110 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 950
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 10:23:10.432379 11110 layer_factory.hpp:77] Creating layer data_hdf5
I0521 10:23:10.432394 11110 net.cpp:106] Creating Layer data_hdf5
I0521 10:23:10.432407 11110 net.cpp:411] data_hdf5 -> data
I0521 10:23:10.432423 11110 net.cpp:411] data_hdf5 -> label
I0521 10:23:10.432440 11110 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 10:23:10.433624 11110 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 10:23:31.765667 11110 net.cpp:150] Setting up data_hdf5
I0521 10:23:31.765835 11110 net.cpp:157] Top shape: 950 1 127 50 (6032500)
I0521 10:23:31.765849 11110 net.cpp:157] Top shape: 950 (950)
I0521 10:23:31.765861 11110 net.cpp:165] Memory required for data: 24133800
I0521 10:23:31.765874 11110 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 10:23:31.765902 11110 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 10:23:31.765913 11110 net.cpp:454] label_data_hdf5_1_split <- label
I0521 10:23:31.765928 11110 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 10:23:31.765949 11110 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 10:23:31.766021 11110 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 10:23:31.766036 11110 net.cpp:157] Top shape: 950 (950)
I0521 10:23:31.766047 11110 net.cpp:157] Top shape: 950 (950)
I0521 10:23:31.766057 11110 net.cpp:165] Memory required for data: 24141400
I0521 10:23:31.766067 11110 layer_factory.hpp:77] Creating layer conv1
I0521 10:23:31.766088 11110 net.cpp:106] Creating Layer conv1
I0521 10:23:31.766099 11110 net.cpp:454] conv1 <- data
I0521 10:23:31.766113 11110 net.cpp:411] conv1 -> conv1
I0521 10:23:31.768059 11110 net.cpp:150] Setting up conv1
I0521 10:23:31.768084 11110 net.cpp:157] Top shape: 950 12 120 48 (65664000)
I0521 10:23:31.768095 11110 net.cpp:165] Memory required for data: 286797400
I0521 10:23:31.768116 11110 layer_factory.hpp:77] Creating layer relu1
I0521 10:23:31.768131 11110 net.cpp:106] Creating Layer relu1
I0521 10:23:31.768141 11110 net.cpp:454] relu1 <- conv1
I0521 10:23:31.768153 11110 net.cpp:397] relu1 -> conv1 (in-place)
I0521 10:23:31.768654 11110 net.cpp:150] Setting up relu1
I0521 10:23:31.768671 11110 net.cpp:157] Top shape: 950 12 120 48 (65664000)
I0521 10:23:31.768682 11110 net.cpp:165] Memory required for data: 549453400
I0521 10:23:31.768692 11110 layer_factory.hpp:77] Creating layer pool1
I0521 10:23:31.768707 11110 net.cpp:106] Creating Layer pool1
I0521 10:23:31.768718 11110 net.cpp:454] pool1 <- conv1
I0521 10:23:31.768730 11110 net.cpp:411] pool1 -> pool1
I0521 10:23:31.768806 11110 net.cpp:150] Setting up pool1
I0521 10:23:31.768820 11110 net.cpp:157] Top shape: 950 12 60 48 (32832000)
I0521 10:23:31.768828 11110 net.cpp:165] Memory required for data: 680781400
I0521 10:23:31.768838 11110 layer_factory.hpp:77] Creating layer conv2
I0521 10:23:31.768857 11110 net.cpp:106] Creating Layer conv2
I0521 10:23:31.768867 11110 net.cpp:454] conv2 <- pool1
I0521 10:23:31.768882 11110 net.cpp:411] conv2 -> conv2
I0521 10:23:31.770782 11110 net.cpp:150] Setting up conv2
I0521 10:23:31.770804 11110 net.cpp:157] Top shape: 950 20 54 46 (47196000)
I0521 10:23:31.770817 11110 net.cpp:165] Memory required for data: 869565400
I0521 10:23:31.770834 11110 layer_factory.hpp:77] Creating layer relu2
I0521 10:23:31.770848 11110 net.cpp:106] Creating Layer relu2
I0521 10:23:31.770858 11110 net.cpp:454] relu2 <- conv2
I0521 10:23:31.770870 11110 net.cpp:397] relu2 -> conv2 (in-place)
I0521 10:23:31.771210 11110 net.cpp:150] Setting up relu2
I0521 10:23:31.771224 11110 net.cpp:157] Top shape: 950 20 54 46 (47196000)
I0521 10:23:31.771234 11110 net.cpp:165] Memory required for data: 1058349400
I0521 10:23:31.771245 11110 layer_factory.hpp:77] Creating layer pool2
I0521 10:23:31.771257 11110 net.cpp:106] Creating Layer pool2
I0521 10:23:31.771267 11110 net.cpp:454] pool2 <- conv2
I0521 10:23:31.771281 11110 net.cpp:411] pool2 -> pool2
I0521 10:23:31.771352 11110 net.cpp:150] Setting up pool2
I0521 10:23:31.771364 11110 net.cpp:157] Top shape: 950 20 27 46 (23598000)
I0521 10:23:31.771373 11110 net.cpp:165] Memory required for data: 1152741400
I0521 10:23:31.771384 11110 layer_factory.hpp:77] Creating layer conv3
I0521 10:23:31.771401 11110 net.cpp:106] Creating Layer conv3
I0521 10:23:31.771411 11110 net.cpp:454] conv3 <- pool2
I0521 10:23:31.771425 11110 net.cpp:411] conv3 -> conv3
I0521 10:23:31.773397 11110 net.cpp:150] Setting up conv3
I0521 10:23:31.773416 11110 net.cpp:157] Top shape: 950 28 22 44 (25748800)
I0521 10:23:31.773425 11110 net.cpp:165] Memory required for data: 1255736600
I0521 10:23:31.773458 11110 layer_factory.hpp:77] Creating layer relu3
I0521 10:23:31.773471 11110 net.cpp:106] Creating Layer relu3
I0521 10:23:31.773481 11110 net.cpp:454] relu3 <- conv3
I0521 10:23:31.773495 11110 net.cpp:397] relu3 -> conv3 (in-place)
I0521 10:23:31.773967 11110 net.cpp:150] Setting up relu3
I0521 10:23:31.773983 11110 net.cpp:157] Top shape: 950 28 22 44 (25748800)
I0521 10:23:31.773993 11110 net.cpp:165] Memory required for data: 1358731800
I0521 10:23:31.774003 11110 layer_factory.hpp:77] Creating layer pool3
I0521 10:23:31.774015 11110 net.cpp:106] Creating Layer pool3
I0521 10:23:31.774025 11110 net.cpp:454] pool3 <- conv3
I0521 10:23:31.774039 11110 net.cpp:411] pool3 -> pool3
I0521 10:23:31.774109 11110 net.cpp:150] Setting up pool3
I0521 10:23:31.774123 11110 net.cpp:157] Top shape: 950 28 11 44 (12874400)
I0521 10:23:31.774133 11110 net.cpp:165] Memory required for data: 1410229400
I0521 10:23:31.774143 11110 layer_factory.hpp:77] Creating layer conv4
I0521 10:23:31.774160 11110 net.cpp:106] Creating Layer conv4
I0521 10:23:31.774171 11110 net.cpp:454] conv4 <- pool3
I0521 10:23:31.774186 11110 net.cpp:411] conv4 -> conv4
I0521 10:23:31.776242 11110 net.cpp:150] Setting up conv4
I0521 10:23:31.776264 11110 net.cpp:157] Top shape: 950 36 6 42 (8618400)
I0521 10:23:31.776276 11110 net.cpp:165] Memory required for data: 1444703000
I0521 10:23:31.776291 11110 layer_factory.hpp:77] Creating layer relu4
I0521 10:23:31.776305 11110 net.cpp:106] Creating Layer relu4
I0521 10:23:31.776315 11110 net.cpp:454] relu4 <- conv4
I0521 10:23:31.776327 11110 net.cpp:397] relu4 -> conv4 (in-place)
I0521 10:23:31.776798 11110 net.cpp:150] Setting up relu4
I0521 10:23:31.776814 11110 net.cpp:157] Top shape: 950 36 6 42 (8618400)
I0521 10:23:31.776824 11110 net.cpp:165] Memory required for data: 1479176600
I0521 10:23:31.776834 11110 layer_factory.hpp:77] Creating layer pool4
I0521 10:23:31.776847 11110 net.cpp:106] Creating Layer pool4
I0521 10:23:31.776857 11110 net.cpp:454] pool4 <- conv4
I0521 10:23:31.776870 11110 net.cpp:411] pool4 -> pool4
I0521 10:23:31.776942 11110 net.cpp:150] Setting up pool4
I0521 10:23:31.776955 11110 net.cpp:157] Top shape: 950 36 3 42 (4309200)
I0521 10:23:31.776964 11110 net.cpp:165] Memory required for data: 1496413400
I0521 10:23:31.776974 11110 layer_factory.hpp:77] Creating layer ip1
I0521 10:23:31.776990 11110 net.cpp:106] Creating Layer ip1
I0521 10:23:31.777000 11110 net.cpp:454] ip1 <- pool4
I0521 10:23:31.777014 11110 net.cpp:411] ip1 -> ip1
I0521 10:23:31.792454 11110 net.cpp:150] Setting up ip1
I0521 10:23:31.792481 11110 net.cpp:157] Top shape: 950 196 (186200)
I0521 10:23:31.792492 11110 net.cpp:165] Memory required for data: 1497158200
I0521 10:23:31.792515 11110 layer_factory.hpp:77] Creating layer relu5
I0521 10:23:31.792529 11110 net.cpp:106] Creating Layer relu5
I0521 10:23:31.792541 11110 net.cpp:454] relu5 <- ip1
I0521 10:23:31.792553 11110 net.cpp:397] relu5 -> ip1 (in-place)
I0521 10:23:31.792898 11110 net.cpp:150] Setting up relu5
I0521 10:23:31.792912 11110 net.cpp:157] Top shape: 950 196 (186200)
I0521 10:23:31.792922 11110 net.cpp:165] Memory required for data: 1497903000
I0521 10:23:31.792934 11110 layer_factory.hpp:77] Creating layer drop1
I0521 10:23:31.792953 11110 net.cpp:106] Creating Layer drop1
I0521 10:23:31.792963 11110 net.cpp:454] drop1 <- ip1
I0521 10:23:31.792975 11110 net.cpp:397] drop1 -> ip1 (in-place)
I0521 10:23:31.793020 11110 net.cpp:150] Setting up drop1
I0521 10:23:31.793032 11110 net.cpp:157] Top shape: 950 196 (186200)
I0521 10:23:31.793043 11110 net.cpp:165] Memory required for data: 1498647800
I0521 10:23:31.793052 11110 layer_factory.hpp:77] Creating layer ip2
I0521 10:23:31.793067 11110 net.cpp:106] Creating Layer ip2
I0521 10:23:31.793076 11110 net.cpp:454] ip2 <- ip1
I0521 10:23:31.793090 11110 net.cpp:411] ip2 -> ip2
I0521 10:23:31.793571 11110 net.cpp:150] Setting up ip2
I0521 10:23:31.793584 11110 net.cpp:157] Top shape: 950 98 (93100)
I0521 10:23:31.793594 11110 net.cpp:165] Memory required for data: 1499020200
I0521 10:23:31.793622 11110 layer_factory.hpp:77] Creating layer relu6
I0521 10:23:31.793635 11110 net.cpp:106] Creating Layer relu6
I0521 10:23:31.793645 11110 net.cpp:454] relu6 <- ip2
I0521 10:23:31.793658 11110 net.cpp:397] relu6 -> ip2 (in-place)
I0521 10:23:31.794186 11110 net.cpp:150] Setting up relu6
I0521 10:23:31.794203 11110 net.cpp:157] Top shape: 950 98 (93100)
I0521 10:23:31.794212 11110 net.cpp:165] Memory required for data: 1499392600
I0521 10:23:31.794222 11110 layer_factory.hpp:77] Creating layer drop2
I0521 10:23:31.794236 11110 net.cpp:106] Creating Layer drop2
I0521 10:23:31.794246 11110 net.cpp:454] drop2 <- ip2
I0521 10:23:31.794260 11110 net.cpp:397] drop2 -> ip2 (in-place)
I0521 10:23:31.794303 11110 net.cpp:150] Setting up drop2
I0521 10:23:31.794315 11110 net.cpp:157] Top shape: 950 98 (93100)
I0521 10:23:31.794325 11110 net.cpp:165] Memory required for data: 1499765000
I0521 10:23:31.794335 11110 layer_factory.hpp:77] Creating layer ip3
I0521 10:23:31.794350 11110 net.cpp:106] Creating Layer ip3
I0521 10:23:31.794360 11110 net.cpp:454] ip3 <- ip2
I0521 10:23:31.794374 11110 net.cpp:411] ip3 -> ip3
I0521 10:23:31.794596 11110 net.cpp:150] Setting up ip3
I0521 10:23:31.794610 11110 net.cpp:157] Top shape: 950 11 (10450)
I0521 10:23:31.794620 11110 net.cpp:165] Memory required for data: 1499806800
I0521 10:23:31.794634 11110 layer_factory.hpp:77] Creating layer drop3
I0521 10:23:31.794647 11110 net.cpp:106] Creating Layer drop3
I0521 10:23:31.794657 11110 net.cpp:454] drop3 <- ip3
I0521 10:23:31.794670 11110 net.cpp:397] drop3 -> ip3 (in-place)
I0521 10:23:31.794713 11110 net.cpp:150] Setting up drop3
I0521 10:23:31.794725 11110 net.cpp:157] Top shape: 950 11 (10450)
I0521 10:23:31.794734 11110 net.cpp:165] Memory required for data: 1499848600
I0521 10:23:31.794744 11110 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 10:23:31.794757 11110 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 10:23:31.794767 11110 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 10:23:31.794780 11110 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 10:23:31.794795 11110 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 10:23:31.794869 11110 net.cpp:150] Setting up ip3_drop3_0_split
I0521 10:23:31.794883 11110 net.cpp:157] Top shape: 950 11 (10450)
I0521 10:23:31.794894 11110 net.cpp:157] Top shape: 950 11 (10450)
I0521 10:23:31.794904 11110 net.cpp:165] Memory required for data: 1499932200
I0521 10:23:31.794914 11110 layer_factory.hpp:77] Creating layer accuracy
I0521 10:23:31.794934 11110 net.cpp:106] Creating Layer accuracy
I0521 10:23:31.794945 11110 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 10:23:31.794956 11110 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 10:23:31.794970 11110 net.cpp:411] accuracy -> accuracy
I0521 10:23:31.794993 11110 net.cpp:150] Setting up accuracy
I0521 10:23:31.795006 11110 net.cpp:157] Top shape: (1)
I0521 10:23:31.795016 11110 net.cpp:165] Memory required for data: 1499932204
I0521 10:23:31.795024 11110 layer_factory.hpp:77] Creating layer loss
I0521 10:23:31.795038 11110 net.cpp:106] Creating Layer loss
I0521 10:23:31.795047 11110 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 10:23:31.795059 11110 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 10:23:31.795073 11110 net.cpp:411] loss -> loss
I0521 10:23:31.795089 11110 layer_factory.hpp:77] Creating layer loss
I0521 10:23:31.795595 11110 net.cpp:150] Setting up loss
I0521 10:23:31.795609 11110 net.cpp:157] Top shape: (1)
I0521 10:23:31.795619 11110 net.cpp:160]     with loss weight 1
I0521 10:23:31.795639 11110 net.cpp:165] Memory required for data: 1499932208
I0521 10:23:31.795650 11110 net.cpp:226] loss needs backward computation.
I0521 10:23:31.795660 11110 net.cpp:228] accuracy does not need backward computation.
I0521 10:23:31.795671 11110 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 10:23:31.795682 11110 net.cpp:226] drop3 needs backward computation.
I0521 10:23:31.795693 11110 net.cpp:226] ip3 needs backward computation.
I0521 10:23:31.795711 11110 net.cpp:226] drop2 needs backward computation.
I0521 10:23:31.795722 11110 net.cpp:226] relu6 needs backward computation.
I0521 10:23:31.795732 11110 net.cpp:226] ip2 needs backward computation.
I0521 10:23:31.795742 11110 net.cpp:226] drop1 needs backward computation.
I0521 10:23:31.795750 11110 net.cpp:226] relu5 needs backward computation.
I0521 10:23:31.795759 11110 net.cpp:226] ip1 needs backward computation.
I0521 10:23:31.795769 11110 net.cpp:226] pool4 needs backward computation.
I0521 10:23:31.795779 11110 net.cpp:226] relu4 needs backward computation.
I0521 10:23:31.795789 11110 net.cpp:226] conv4 needs backward computation.
I0521 10:23:31.795799 11110 net.cpp:226] pool3 needs backward computation.
I0521 10:23:31.795810 11110 net.cpp:226] relu3 needs backward computation.
I0521 10:23:31.795820 11110 net.cpp:226] conv3 needs backward computation.
I0521 10:23:31.795831 11110 net.cpp:226] pool2 needs backward computation.
I0521 10:23:31.795841 11110 net.cpp:226] relu2 needs backward computation.
I0521 10:23:31.795851 11110 net.cpp:226] conv2 needs backward computation.
I0521 10:23:31.795861 11110 net.cpp:226] pool1 needs backward computation.
I0521 10:23:31.795871 11110 net.cpp:226] relu1 needs backward computation.
I0521 10:23:31.795881 11110 net.cpp:226] conv1 needs backward computation.
I0521 10:23:31.795892 11110 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 10:23:31.795904 11110 net.cpp:228] data_hdf5 does not need backward computation.
I0521 10:23:31.795912 11110 net.cpp:270] This network produces output accuracy
I0521 10:23:31.795923 11110 net.cpp:270] This network produces output loss
I0521 10:23:31.795953 11110 net.cpp:283] Network initialization done.
I0521 10:23:31.796085 11110 solver.cpp:60] Solver scaffolding done.
I0521 10:23:31.797212 11110 caffe.cpp:212] Starting Optimization
I0521 10:23:31.797226 11110 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 10:23:31.797237 11110 solver.cpp:289] Learning Rate Policy: fixed
I0521 10:23:31.798447 11110 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 10:24:17.622853 11110 solver.cpp:409]     Test net output #0: accuracy = 0.090419
I0521 10:24:17.623016 11110 solver.cpp:409]     Test net output #1: loss = 2.39745 (* 1 = 2.39745 loss)
I0521 10:24:17.796425 11110 solver.cpp:237] Iteration 0, loss = 2.39718
I0521 10:24:17.796461 11110 solver.cpp:253]     Train net output #0: loss = 2.39718 (* 1 = 2.39718 loss)
I0521 10:24:17.796480 11110 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 10:24:25.426421 11110 solver.cpp:237] Iteration 15, loss = 2.39029
I0521 10:24:25.426456 11110 solver.cpp:253]     Train net output #0: loss = 2.39029 (* 1 = 2.39029 loss)
I0521 10:24:25.426470 11110 sgd_solver.cpp:106] Iteration 15, lr = 0.0025
I0521 10:24:33.059370 11110 solver.cpp:237] Iteration 30, loss = 2.37624
I0521 10:24:33.059401 11110 solver.cpp:253]     Train net output #0: loss = 2.37624 (* 1 = 2.37624 loss)
I0521 10:24:33.059417 11110 sgd_solver.cpp:106] Iteration 30, lr = 0.0025
I0521 10:24:40.699558 11110 solver.cpp:237] Iteration 45, loss = 2.3602
I0521 10:24:40.699596 11110 solver.cpp:253]     Train net output #0: loss = 2.3602 (* 1 = 2.3602 loss)
I0521 10:24:40.699610 11110 sgd_solver.cpp:106] Iteration 45, lr = 0.0025
I0521 10:24:48.334504 11110 solver.cpp:237] Iteration 60, loss = 2.35213
I0521 10:24:48.334645 11110 solver.cpp:253]     Train net output #0: loss = 2.35213 (* 1 = 2.35213 loss)
I0521 10:24:48.334658 11110 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 10:24:55.965639 11110 solver.cpp:237] Iteration 75, loss = 2.33308
I0521 10:24:55.965670 11110 solver.cpp:253]     Train net output #0: loss = 2.33308 (* 1 = 2.33308 loss)
I0521 10:24:55.965688 11110 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 10:25:03.600977 11110 solver.cpp:237] Iteration 90, loss = 2.34344
I0521 10:25:03.601027 11110 solver.cpp:253]     Train net output #0: loss = 2.34344 (* 1 = 2.34344 loss)
I0521 10:25:03.601042 11110 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 10:25:33.337483 11110 solver.cpp:237] Iteration 105, loss = 2.32107
I0521 10:25:33.337651 11110 solver.cpp:253]     Train net output #0: loss = 2.32107 (* 1 = 2.32107 loss)
I0521 10:25:33.337666 11110 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0521 10:25:40.968313 11110 solver.cpp:237] Iteration 120, loss = 2.33745
I0521 10:25:40.968346 11110 solver.cpp:253]     Train net output #0: loss = 2.33745 (* 1 = 2.33745 loss)
I0521 10:25:40.968364 11110 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 10:25:48.601006 11110 solver.cpp:237] Iteration 135, loss = 2.33421
I0521 10:25:48.601039 11110 solver.cpp:253]     Train net output #0: loss = 2.33421 (* 1 = 2.33421 loss)
I0521 10:25:48.601057 11110 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 10:25:56.235818 11110 solver.cpp:237] Iteration 150, loss = 2.32332
I0521 10:25:56.235860 11110 solver.cpp:253]     Train net output #0: loss = 2.32332 (* 1 = 2.32332 loss)
I0521 10:25:56.235878 11110 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 10:25:59.290496 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_157.caffemodel
I0521 10:25:59.690623 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_157.solverstate
I0521 10:26:03.937435 11110 solver.cpp:237] Iteration 165, loss = 2.32726
I0521 10:26:03.937590 11110 solver.cpp:253]     Train net output #0: loss = 2.32726 (* 1 = 2.32726 loss)
I0521 10:26:03.937604 11110 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0521 10:26:11.575523 11110 solver.cpp:237] Iteration 180, loss = 2.32722
I0521 10:26:11.575556 11110 solver.cpp:253]     Train net output #0: loss = 2.32722 (* 1 = 2.32722 loss)
I0521 10:26:11.575570 11110 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 10:26:19.209849 11110 solver.cpp:237] Iteration 195, loss = 2.31642
I0521 10:26:19.209902 11110 solver.cpp:253]     Train net output #0: loss = 2.31642 (* 1 = 2.31642 loss)
I0521 10:26:19.209914 11110 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0521 10:26:48.926657 11110 solver.cpp:237] Iteration 210, loss = 2.3102
I0521 10:26:48.926828 11110 solver.cpp:253]     Train net output #0: loss = 2.3102 (* 1 = 2.3102 loss)
I0521 10:26:48.926842 11110 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 10:26:56.561118 11110 solver.cpp:237] Iteration 225, loss = 2.32675
I0521 10:26:56.561149 11110 solver.cpp:253]     Train net output #0: loss = 2.32675 (* 1 = 2.32675 loss)
I0521 10:26:56.561164 11110 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 10:27:04.187960 11110 solver.cpp:237] Iteration 240, loss = 2.30546
I0521 10:27:04.187994 11110 solver.cpp:253]     Train net output #0: loss = 2.30546 (* 1 = 2.30546 loss)
I0521 10:27:04.188010 11110 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 10:27:11.826222 11110 solver.cpp:237] Iteration 255, loss = 2.30101
I0521 10:27:11.826258 11110 solver.cpp:253]     Train net output #0: loss = 2.30101 (* 1 = 2.30101 loss)
I0521 10:27:11.826277 11110 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 10:27:19.459149 11110 solver.cpp:237] Iteration 270, loss = 2.27813
I0521 10:27:19.459286 11110 solver.cpp:253]     Train net output #0: loss = 2.27813 (* 1 = 2.27813 loss)
I0521 10:27:19.459300 11110 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 10:27:27.093050 11110 solver.cpp:237] Iteration 285, loss = 2.30418
I0521 10:27:27.093078 11110 solver.cpp:253]     Train net output #0: loss = 2.30418 (* 1 = 2.30418 loss)
I0521 10:27:27.093091 11110 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 10:27:34.726707 11110 solver.cpp:237] Iteration 300, loss = 2.25983
I0521 10:27:34.726742 11110 solver.cpp:253]     Train net output #0: loss = 2.25983 (* 1 = 2.25983 loss)
I0521 10:27:34.726763 11110 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 10:27:41.344889 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_314.caffemodel
I0521 10:27:41.739246 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_314.solverstate
I0521 10:27:41.916492 11110 solver.cpp:341] Iteration 315, Testing net (#0)
I0521 10:28:27.171172 11110 solver.cpp:409]     Test net output #0: accuracy = 0.380556
I0521 10:28:27.171342 11110 solver.cpp:409]     Test net output #1: loss = 2.18695 (* 1 = 2.18695 loss)
I0521 10:28:49.380435 11110 solver.cpp:237] Iteration 315, loss = 2.26484
I0521 10:28:49.380492 11110 solver.cpp:253]     Train net output #0: loss = 2.26484 (* 1 = 2.26484 loss)
I0521 10:28:49.380509 11110 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0521 10:28:57.007578 11110 solver.cpp:237] Iteration 330, loss = 2.2295
I0521 10:28:57.007611 11110 solver.cpp:253]     Train net output #0: loss = 2.2295 (* 1 = 2.2295 loss)
I0521 10:28:57.007628 11110 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 10:29:04.631959 11110 solver.cpp:237] Iteration 345, loss = 2.23168
I0521 10:29:04.632097 11110 solver.cpp:253]     Train net output #0: loss = 2.23168 (* 1 = 2.23168 loss)
I0521 10:29:04.632109 11110 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 10:29:12.263541 11110 solver.cpp:237] Iteration 360, loss = 2.21783
I0521 10:29:12.263581 11110 solver.cpp:253]     Train net output #0: loss = 2.21783 (* 1 = 2.21783 loss)
I0521 10:29:12.263599 11110 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 10:29:19.892266 11110 solver.cpp:237] Iteration 375, loss = 2.19023
I0521 10:29:19.892297 11110 solver.cpp:253]     Train net output #0: loss = 2.19023 (* 1 = 2.19023 loss)
I0521 10:29:19.892314 11110 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 10:29:27.520887 11110 solver.cpp:237] Iteration 390, loss = 2.15268
I0521 10:29:27.520920 11110 solver.cpp:253]     Train net output #0: loss = 2.15268 (* 1 = 2.15268 loss)
I0521 10:29:27.520936 11110 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 10:29:35.143028 11110 solver.cpp:237] Iteration 405, loss = 2.16912
I0521 10:29:35.143167 11110 solver.cpp:253]     Train net output #0: loss = 2.16912 (* 1 = 2.16912 loss)
I0521 10:29:35.143180 11110 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 10:29:42.775035 11110 solver.cpp:237] Iteration 420, loss = 2.09378
I0521 10:29:42.775070 11110 solver.cpp:253]     Train net output #0: loss = 2.09378 (* 1 = 2.09378 loss)
I0521 10:29:42.775092 11110 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 10:30:12.487051 11110 solver.cpp:237] Iteration 435, loss = 2.10437
I0521 10:30:12.487227 11110 solver.cpp:253]     Train net output #0: loss = 2.10437 (* 1 = 2.10437 loss)
I0521 10:30:12.487242 11110 sgd_solver.cpp:106] Iteration 435, lr = 0.0025
I0521 10:30:20.110345 11110 solver.cpp:237] Iteration 450, loss = 2.07897
I0521 10:30:20.110378 11110 solver.cpp:253]     Train net output #0: loss = 2.07897 (* 1 = 2.07897 loss)
I0521 10:30:20.110391 11110 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 10:30:27.739852 11110 solver.cpp:237] Iteration 465, loss = 2.10124
I0521 10:30:27.739888 11110 solver.cpp:253]     Train net output #0: loss = 2.10124 (* 1 = 2.10124 loss)
I0521 10:30:27.739910 11110 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0521 10:30:30.286515 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_471.caffemodel
I0521 10:30:30.684381 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_471.solverstate
I0521 10:30:35.440798 11110 solver.cpp:237] Iteration 480, loss = 2.06624
I0521 10:30:35.440850 11110 solver.cpp:253]     Train net output #0: loss = 2.06624 (* 1 = 2.06624 loss)
I0521 10:30:35.440865 11110 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 10:30:43.071696 11110 solver.cpp:237] Iteration 495, loss = 2.02995
I0521 10:30:43.071835 11110 solver.cpp:253]     Train net output #0: loss = 2.02995 (* 1 = 2.02995 loss)
I0521 10:30:43.071848 11110 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0521 10:30:50.703819 11110 solver.cpp:237] Iteration 510, loss = 2.05365
I0521 10:30:50.703861 11110 solver.cpp:253]     Train net output #0: loss = 2.05365 (* 1 = 2.05365 loss)
I0521 10:30:50.703879 11110 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 10:30:58.335377 11110 solver.cpp:237] Iteration 525, loss = 2.07326
I0521 10:30:58.335410 11110 solver.cpp:253]     Train net output #0: loss = 2.07326 (* 1 = 2.07326 loss)
I0521 10:30:58.335429 11110 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 10:31:28.052597 11110 solver.cpp:237] Iteration 540, loss = 1.99421
I0521 10:31:28.052763 11110 solver.cpp:253]     Train net output #0: loss = 1.99421 (* 1 = 1.99421 loss)
I0521 10:31:28.052778 11110 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 10:31:35.680214 11110 solver.cpp:237] Iteration 555, loss = 1.98851
I0521 10:31:35.680245 11110 solver.cpp:253]     Train net output #0: loss = 1.98851 (* 1 = 1.98851 loss)
I0521 10:31:35.680264 11110 sgd_solver.cpp:106] Iteration 555, lr = 0.0025
I0521 10:31:43.307363 11110 solver.cpp:237] Iteration 570, loss = 1.96801
I0521 10:31:43.307391 11110 solver.cpp:253]     Train net output #0: loss = 1.96801 (* 1 = 1.96801 loss)
I0521 10:31:43.307410 11110 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 10:31:50.935684 11110 solver.cpp:237] Iteration 585, loss = 1.93458
I0521 10:31:50.935716 11110 solver.cpp:253]     Train net output #0: loss = 1.93458 (* 1 = 1.93458 loss)
I0521 10:31:50.935732 11110 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0521 10:31:58.562755 11110 solver.cpp:237] Iteration 600, loss = 1.95838
I0521 10:31:58.562894 11110 solver.cpp:253]     Train net output #0: loss = 1.95838 (* 1 = 1.95838 loss)
I0521 10:31:58.562907 11110 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 10:32:06.191249 11110 solver.cpp:237] Iteration 615, loss = 1.93655
I0521 10:32:06.191290 11110 solver.cpp:253]     Train net output #0: loss = 1.93655 (* 1 = 1.93655 loss)
I0521 10:32:06.191308 11110 sgd_solver.cpp:106] Iteration 615, lr = 0.0025
I0521 10:32:12.292304 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_628.caffemodel
I0521 10:32:12.690042 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_628.solverstate
I0521 10:32:13.378336 11110 solver.cpp:341] Iteration 630, Testing net (#0)
I0521 10:33:19.503202 11110 solver.cpp:409]     Test net output #0: accuracy = 0.552625
I0521 10:33:19.503376 11110 solver.cpp:409]     Test net output #1: loss = 1.62623 (* 1 = 1.62623 loss)
I0521 10:33:19.654738 11110 solver.cpp:237] Iteration 630, loss = 1.95167
I0521 10:33:19.654767 11110 solver.cpp:253]     Train net output #0: loss = 1.95167 (* 1 = 1.95167 loss)
I0521 10:33:19.654785 11110 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 10:33:49.405989 11110 solver.cpp:237] Iteration 645, loss = 1.97298
I0521 10:33:49.406044 11110 solver.cpp:253]     Train net output #0: loss = 1.97298 (* 1 = 1.97298 loss)
I0521 10:33:49.406057 11110 sgd_solver.cpp:106] Iteration 645, lr = 0.0025
I0521 10:33:57.031569 11110 solver.cpp:237] Iteration 660, loss = 1.91735
I0521 10:33:57.031714 11110 solver.cpp:253]     Train net output #0: loss = 1.91735 (* 1 = 1.91735 loss)
I0521 10:33:57.031728 11110 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 10:34:04.654661 11110 solver.cpp:237] Iteration 675, loss = 1.88047
I0521 10:34:04.654692 11110 solver.cpp:253]     Train net output #0: loss = 1.88047 (* 1 = 1.88047 loss)
I0521 10:34:04.654711 11110 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 10:34:12.285048 11110 solver.cpp:237] Iteration 690, loss = 1.96411
I0521 10:34:12.285092 11110 solver.cpp:253]     Train net output #0: loss = 1.96411 (* 1 = 1.96411 loss)
I0521 10:34:12.285112 11110 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 10:34:19.915786 11110 solver.cpp:237] Iteration 705, loss = 1.90253
I0521 10:34:19.915818 11110 solver.cpp:253]     Train net output #0: loss = 1.90253 (* 1 = 1.90253 loss)
I0521 10:34:19.915832 11110 sgd_solver.cpp:106] Iteration 705, lr = 0.0025
I0521 10:34:27.540868 11110 solver.cpp:237] Iteration 720, loss = 1.91104
I0521 10:34:27.541007 11110 solver.cpp:253]     Train net output #0: loss = 1.91104 (* 1 = 1.91104 loss)
I0521 10:34:27.541020 11110 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 10:34:35.171608 11110 solver.cpp:237] Iteration 735, loss = 1.90977
I0521 10:34:35.171639 11110 solver.cpp:253]     Train net output #0: loss = 1.90977 (* 1 = 1.90977 loss)
I0521 10:34:35.171658 11110 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0521 10:35:04.912372 11110 solver.cpp:237] Iteration 750, loss = 1.95877
I0521 10:35:04.912607 11110 solver.cpp:253]     Train net output #0: loss = 1.95877 (* 1 = 1.95877 loss)
I0521 10:35:04.912623 11110 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 10:35:12.535966 11110 solver.cpp:237] Iteration 765, loss = 1.86946
I0521 10:35:12.535998 11110 solver.cpp:253]     Train net output #0: loss = 1.86946 (* 1 = 1.86946 loss)
I0521 10:35:12.536015 11110 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 10:35:20.161433 11110 solver.cpp:237] Iteration 780, loss = 1.8897
I0521 10:35:20.161465 11110 solver.cpp:253]     Train net output #0: loss = 1.8897 (* 1 = 1.8897 loss)
I0521 10:35:20.161481 11110 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 10:35:22.195253 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_785.caffemodel
I0521 10:35:22.592891 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_785.solverstate
I0521 10:35:27.854921 11110 solver.cpp:237] Iteration 795, loss = 1.86045
I0521 10:35:27.854970 11110 solver.cpp:253]     Train net output #0: loss = 1.86045 (* 1 = 1.86045 loss)
I0521 10:35:27.854990 11110 sgd_solver.cpp:106] Iteration 795, lr = 0.0025
I0521 10:35:35.478765 11110 solver.cpp:237] Iteration 810, loss = 1.85071
I0521 10:35:35.478914 11110 solver.cpp:253]     Train net output #0: loss = 1.85071 (* 1 = 1.85071 loss)
I0521 10:35:35.478929 11110 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 10:35:43.103001 11110 solver.cpp:237] Iteration 825, loss = 1.85958
I0521 10:35:43.103032 11110 solver.cpp:253]     Train net output #0: loss = 1.85958 (* 1 = 1.85958 loss)
I0521 10:35:43.103049 11110 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 10:35:50.726274 11110 solver.cpp:237] Iteration 840, loss = 1.83891
I0521 10:35:50.726328 11110 solver.cpp:253]     Train net output #0: loss = 1.83891 (* 1 = 1.83891 loss)
I0521 10:35:50.726344 11110 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 10:36:20.452330 11110 solver.cpp:237] Iteration 855, loss = 1.84383
I0521 10:36:20.452502 11110 solver.cpp:253]     Train net output #0: loss = 1.84383 (* 1 = 1.84383 loss)
I0521 10:36:20.452517 11110 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 10:36:28.080235 11110 solver.cpp:237] Iteration 870, loss = 1.80602
I0521 10:36:28.080268 11110 solver.cpp:253]     Train net output #0: loss = 1.80602 (* 1 = 1.80602 loss)
I0521 10:36:28.080286 11110 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 10:36:35.703702 11110 solver.cpp:237] Iteration 885, loss = 1.88459
I0521 10:36:35.703735 11110 solver.cpp:253]     Train net output #0: loss = 1.88459 (* 1 = 1.88459 loss)
I0521 10:36:35.703749 11110 sgd_solver.cpp:106] Iteration 885, lr = 0.0025
I0521 10:36:43.327836 11110 solver.cpp:237] Iteration 900, loss = 1.75735
I0521 10:36:43.327877 11110 solver.cpp:253]     Train net output #0: loss = 1.75735 (* 1 = 1.75735 loss)
I0521 10:36:43.327898 11110 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 10:36:50.952529 11110 solver.cpp:237] Iteration 915, loss = 1.87483
I0521 10:36:50.952667 11110 solver.cpp:253]     Train net output #0: loss = 1.87483 (* 1 = 1.87483 loss)
I0521 10:36:50.952682 11110 sgd_solver.cpp:106] Iteration 915, lr = 0.0025
I0521 10:36:58.577457 11110 solver.cpp:237] Iteration 930, loss = 1.79298
I0521 10:36:58.577489 11110 solver.cpp:253]     Train net output #0: loss = 1.79298 (* 1 = 1.79298 loss)
I0521 10:36:58.577507 11110 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 10:37:04.167512 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_942.caffemodel
I0521 10:37:04.564383 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_942.solverstate
I0521 10:37:05.758126 11110 solver.cpp:341] Iteration 945, Testing net (#0)
I0521 10:37:50.663516 11110 solver.cpp:409]     Test net output #0: accuracy = 0.62055
I0521 10:37:50.663689 11110 solver.cpp:409]     Test net output #1: loss = 1.41342 (* 1 = 1.41342 loss)
I0521 10:37:50.814558 11110 solver.cpp:237] Iteration 945, loss = 1.81285
I0521 10:37:50.814584 11110 solver.cpp:253]     Train net output #0: loss = 1.81285 (* 1 = 1.81285 loss)
I0521 10:37:50.814599 11110 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 10:38:20.530670 11110 solver.cpp:237] Iteration 960, loss = 1.83007
I0521 10:38:20.530725 11110 solver.cpp:253]     Train net output #0: loss = 1.83007 (* 1 = 1.83007 loss)
I0521 10:38:20.530742 11110 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 10:38:28.162348 11110 solver.cpp:237] Iteration 975, loss = 1.79212
I0521 10:38:28.162488 11110 solver.cpp:253]     Train net output #0: loss = 1.79212 (* 1 = 1.79212 loss)
I0521 10:38:28.162502 11110 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 10:38:35.793462 11110 solver.cpp:237] Iteration 990, loss = 1.80264
I0521 10:38:35.793493 11110 solver.cpp:253]     Train net output #0: loss = 1.80264 (* 1 = 1.80264 loss)
I0521 10:38:35.793510 11110 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 10:38:43.425134 11110 solver.cpp:237] Iteration 1005, loss = 1.82703
I0521 10:38:43.425166 11110 solver.cpp:253]     Train net output #0: loss = 1.82703 (* 1 = 1.82703 loss)
I0521 10:38:43.425184 11110 sgd_solver.cpp:106] Iteration 1005, lr = 0.0025
I0521 10:38:51.049429 11110 solver.cpp:237] Iteration 1020, loss = 1.82797
I0521 10:38:51.049470 11110 solver.cpp:253]     Train net output #0: loss = 1.82797 (* 1 = 1.82797 loss)
I0521 10:38:51.049490 11110 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 10:38:58.673728 11110 solver.cpp:237] Iteration 1035, loss = 1.7942
I0521 10:38:58.673877 11110 solver.cpp:253]     Train net output #0: loss = 1.7942 (* 1 = 1.7942 loss)
I0521 10:38:58.673892 11110 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 10:39:06.302556 11110 solver.cpp:237] Iteration 1050, loss = 1.8371
I0521 10:39:06.302587 11110 solver.cpp:253]     Train net output #0: loss = 1.8371 (* 1 = 1.8371 loss)
I0521 10:39:06.302604 11110 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 10:39:36.111199 11110 solver.cpp:237] Iteration 1065, loss = 1.8452
I0521 10:39:36.111366 11110 solver.cpp:253]     Train net output #0: loss = 1.8452 (* 1 = 1.8452 loss)
I0521 10:39:36.111379 11110 sgd_solver.cpp:106] Iteration 1065, lr = 0.0025
I0521 10:39:43.742789 11110 solver.cpp:237] Iteration 1080, loss = 1.76281
I0521 10:39:43.742821 11110 solver.cpp:253]     Train net output #0: loss = 1.76281 (* 1 = 1.76281 loss)
I0521 10:39:43.742841 11110 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 10:39:51.370970 11110 solver.cpp:237] Iteration 1095, loss = 1.78136
I0521 10:39:51.371001 11110 solver.cpp:253]     Train net output #0: loss = 1.78136 (* 1 = 1.78136 loss)
I0521 10:39:51.371016 11110 sgd_solver.cpp:106] Iteration 1095, lr = 0.0025
I0521 10:39:52.897215 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1099.caffemodel
I0521 10:39:53.292568 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1099.solverstate
I0521 10:39:59.065516 11110 solver.cpp:237] Iteration 1110, loss = 1.84588
I0521 10:39:59.065567 11110 solver.cpp:253]     Train net output #0: loss = 1.84588 (* 1 = 1.84588 loss)
I0521 10:39:59.065582 11110 sgd_solver.cpp:106] Iteration 1110, lr = 0.0025
I0521 10:40:06.693045 11110 solver.cpp:237] Iteration 1125, loss = 1.75353
I0521 10:40:06.693197 11110 solver.cpp:253]     Train net output #0: loss = 1.75353 (* 1 = 1.75353 loss)
I0521 10:40:06.693212 11110 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 10:40:14.321455 11110 solver.cpp:237] Iteration 1140, loss = 1.78817
I0521 10:40:14.321486 11110 solver.cpp:253]     Train net output #0: loss = 1.78817 (* 1 = 1.78817 loss)
I0521 10:40:14.321502 11110 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 10:40:21.946121 11110 solver.cpp:237] Iteration 1155, loss = 1.73644
I0521 10:40:21.946154 11110 solver.cpp:253]     Train net output #0: loss = 1.73644 (* 1 = 1.73644 loss)
I0521 10:40:21.946169 11110 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 10:40:51.713048 11110 solver.cpp:237] Iteration 1170, loss = 1.80866
I0521 10:40:51.713218 11110 solver.cpp:253]     Train net output #0: loss = 1.80866 (* 1 = 1.80866 loss)
I0521 10:40:51.713232 11110 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 10:40:59.346422 11110 solver.cpp:237] Iteration 1185, loss = 1.83097
I0521 10:40:59.346454 11110 solver.cpp:253]     Train net output #0: loss = 1.83097 (* 1 = 1.83097 loss)
I0521 10:40:59.346473 11110 sgd_solver.cpp:106] Iteration 1185, lr = 0.0025
I0521 10:41:06.978740 11110 solver.cpp:237] Iteration 1200, loss = 1.82858
I0521 10:41:06.978775 11110 solver.cpp:253]     Train net output #0: loss = 1.82858 (* 1 = 1.82858 loss)
I0521 10:41:06.978787 11110 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 10:41:14.606187 11110 solver.cpp:237] Iteration 1215, loss = 1.77074
I0521 10:41:14.606220 11110 solver.cpp:253]     Train net output #0: loss = 1.77074 (* 1 = 1.77074 loss)
I0521 10:41:14.606233 11110 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 10:41:22.234024 11110 solver.cpp:237] Iteration 1230, loss = 1.71612
I0521 10:41:22.234176 11110 solver.cpp:253]     Train net output #0: loss = 1.71612 (* 1 = 1.71612 loss)
I0521 10:41:22.234189 11110 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0521 10:41:29.862351 11110 solver.cpp:237] Iteration 1245, loss = 1.78231
I0521 10:41:29.862383 11110 solver.cpp:253]     Train net output #0: loss = 1.78231 (* 1 = 1.78231 loss)
I0521 10:41:29.862401 11110 sgd_solver.cpp:106] Iteration 1245, lr = 0.0025
I0521 10:41:34.950476 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1256.caffemodel
I0521 10:41:35.345584 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1256.solverstate
I0521 10:41:37.049685 11110 solver.cpp:341] Iteration 1260, Testing net (#0)
I0521 10:42:43.207664 11110 solver.cpp:409]     Test net output #0: accuracy = 0.647925
I0521 10:42:43.207835 11110 solver.cpp:409]     Test net output #1: loss = 1.33974 (* 1 = 1.33974 loss)
I0521 10:42:43.359088 11110 solver.cpp:237] Iteration 1260, loss = 1.72721
I0521 10:42:43.359117 11110 solver.cpp:253]     Train net output #0: loss = 1.72721 (* 1 = 1.72721 loss)
I0521 10:42:43.359135 11110 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 10:43:13.143723 11110 solver.cpp:237] Iteration 1275, loss = 1.85856
I0521 10:43:13.143777 11110 solver.cpp:253]     Train net output #0: loss = 1.85856 (* 1 = 1.85856 loss)
I0521 10:43:13.143793 11110 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 10:43:20.776971 11110 solver.cpp:237] Iteration 1290, loss = 1.86976
I0521 10:43:20.777115 11110 solver.cpp:253]     Train net output #0: loss = 1.86976 (* 1 = 1.86976 loss)
I0521 10:43:20.777129 11110 sgd_solver.cpp:106] Iteration 1290, lr = 0.0025
I0521 10:43:28.412376 11110 solver.cpp:237] Iteration 1305, loss = 1.80007
I0521 10:43:28.412410 11110 solver.cpp:253]     Train net output #0: loss = 1.80007 (* 1 = 1.80007 loss)
I0521 10:43:28.412423 11110 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0521 10:43:36.045490 11110 solver.cpp:237] Iteration 1320, loss = 1.83964
I0521 10:43:36.045523 11110 solver.cpp:253]     Train net output #0: loss = 1.83964 (* 1 = 1.83964 loss)
I0521 10:43:36.045539 11110 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 10:43:43.686673 11110 solver.cpp:237] Iteration 1335, loss = 1.745
I0521 10:43:43.686705 11110 solver.cpp:253]     Train net output #0: loss = 1.745 (* 1 = 1.745 loss)
I0521 10:43:43.686722 11110 sgd_solver.cpp:106] Iteration 1335, lr = 0.0025
I0521 10:43:51.321249 11110 solver.cpp:237] Iteration 1350, loss = 1.79186
I0521 10:43:51.321396 11110 solver.cpp:253]     Train net output #0: loss = 1.79186 (* 1 = 1.79186 loss)
I0521 10:43:51.321410 11110 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 10:43:58.956001 11110 solver.cpp:237] Iteration 1365, loss = 1.75619
I0521 10:43:58.956032 11110 solver.cpp:253]     Train net output #0: loss = 1.75619 (* 1 = 1.75619 loss)
I0521 10:43:58.956049 11110 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0521 10:44:28.721331 11110 solver.cpp:237] Iteration 1380, loss = 1.70556
I0521 10:44:28.721499 11110 solver.cpp:253]     Train net output #0: loss = 1.70556 (* 1 = 1.70556 loss)
I0521 10:44:28.721513 11110 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 10:44:36.353011 11110 solver.cpp:237] Iteration 1395, loss = 1.72581
I0521 10:44:36.353050 11110 solver.cpp:253]     Train net output #0: loss = 1.72581 (* 1 = 1.72581 loss)
I0521 10:44:36.353065 11110 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0521 10:44:43.990852 11110 solver.cpp:237] Iteration 1410, loss = 1.89135
I0521 10:44:43.990885 11110 solver.cpp:253]     Train net output #0: loss = 1.89135 (* 1 = 1.89135 loss)
I0521 10:44:43.990900 11110 sgd_solver.cpp:106] Iteration 1410, lr = 0.0025
I0521 10:44:45.008843 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1413.caffemodel
I0521 10:44:45.408507 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1413.solverstate
I0521 10:44:51.696216 11110 solver.cpp:237] Iteration 1425, loss = 1.82589
I0521 10:44:51.696269 11110 solver.cpp:253]     Train net output #0: loss = 1.82589 (* 1 = 1.82589 loss)
I0521 10:44:51.696285 11110 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 10:44:59.335296 11110 solver.cpp:237] Iteration 1440, loss = 1.736
I0521 10:44:59.335460 11110 solver.cpp:253]     Train net output #0: loss = 1.736 (* 1 = 1.736 loss)
I0521 10:44:59.335474 11110 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 10:45:06.971421 11110 solver.cpp:237] Iteration 1455, loss = 1.73618
I0521 10:45:06.971470 11110 solver.cpp:253]     Train net output #0: loss = 1.73618 (* 1 = 1.73618 loss)
I0521 10:45:06.971487 11110 sgd_solver.cpp:106] Iteration 1455, lr = 0.0025
I0521 10:45:14.611012 11110 solver.cpp:237] Iteration 1470, loss = 1.74546
I0521 10:45:14.611045 11110 solver.cpp:253]     Train net output #0: loss = 1.74546 (* 1 = 1.74546 loss)
I0521 10:45:14.611063 11110 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 10:45:44.402721 11110 solver.cpp:237] Iteration 1485, loss = 1.78228
I0521 10:45:44.402894 11110 solver.cpp:253]     Train net output #0: loss = 1.78228 (* 1 = 1.78228 loss)
I0521 10:45:44.402909 11110 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 10:45:52.039808 11110 solver.cpp:237] Iteration 1500, loss = 1.74029
I0521 10:45:52.039856 11110 solver.cpp:253]     Train net output #0: loss = 1.74029 (* 1 = 1.74029 loss)
I0521 10:45:52.039876 11110 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 10:45:59.674623 11110 solver.cpp:237] Iteration 1515, loss = 1.68973
I0521 10:45:59.674655 11110 solver.cpp:253]     Train net output #0: loss = 1.68973 (* 1 = 1.68973 loss)
I0521 10:45:59.674670 11110 sgd_solver.cpp:106] Iteration 1515, lr = 0.0025
I0521 10:46:07.307663 11110 solver.cpp:237] Iteration 1530, loss = 1.87094
I0521 10:46:07.307695 11110 solver.cpp:253]     Train net output #0: loss = 1.87094 (* 1 = 1.87094 loss)
I0521 10:46:07.307709 11110 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 10:46:14.940904 11110 solver.cpp:237] Iteration 1545, loss = 1.7471
I0521 10:46:14.941046 11110 solver.cpp:253]     Train net output #0: loss = 1.7471 (* 1 = 1.7471 loss)
I0521 10:46:14.941059 11110 sgd_solver.cpp:106] Iteration 1545, lr = 0.0025
I0521 10:46:22.578409 11110 solver.cpp:237] Iteration 1560, loss = 1.66428
I0521 10:46:22.578440 11110 solver.cpp:253]     Train net output #0: loss = 1.66428 (* 1 = 1.66428 loss)
I0521 10:46:22.578459 11110 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 10:46:27.159417 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1570.caffemodel
I0521 10:46:27.556646 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1570.solverstate
I0521 10:46:29.773942 11110 solver.cpp:341] Iteration 1575, Testing net (#0)
I0521 10:47:15.095497 11110 solver.cpp:409]     Test net output #0: accuracy = 0.654375
I0521 10:47:15.095665 11110 solver.cpp:409]     Test net output #1: loss = 1.24574 (* 1 = 1.24574 loss)
I0521 10:47:15.246979 11110 solver.cpp:237] Iteration 1575, loss = 1.73402
I0521 10:47:15.247009 11110 solver.cpp:253]     Train net output #0: loss = 1.73402 (* 1 = 1.73402 loss)
I0521 10:47:15.247026 11110 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0521 10:47:16.262960 11110 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1578.caffemodel
I0521 10:47:16.662076 11110 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951_iter_1578.solverstate
I0521 10:47:16.690734 11110 solver.cpp:326] Optimization Done.
I0521 10:47:16.690763 11110 caffe.cpp:215] Optimization Done.
Application 11237648 resources: utime ~1248s, stime ~223s, Rss ~5329192, inblocks ~3594475, outblocks ~194566
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_950_2016-05-20T11.21.07.347951.solver"
	User time (seconds): 0.58
	System time (seconds): 0.25
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:34.46
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 57
	Minor (reclaiming a frame) page faults: 15788
	Voluntary context switches: 3155
	Involuntary context switches: 231
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
