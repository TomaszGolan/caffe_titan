2805959
I0520 21:37:36.316721 31157 caffe.cpp:184] Using GPUs 0
I0520 21:37:36.743178 31157 solver.cpp:48] Initializing solver from parameters: 
test_iter: 441
test_interval: 882
base_lr: 0.0025
display: 44
max_iter: 4411
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 441
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022.prototxt"
I0520 21:37:36.745012 31157 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022.prototxt
I0520 21:37:36.761991 31157 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 21:37:36.762049 31157 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 21:37:36.762392 31157 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 340
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:37:36.762572 31157 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:37:36.762595 31157 net.cpp:106] Creating Layer data_hdf5
I0520 21:37:36.762609 31157 net.cpp:411] data_hdf5 -> data
I0520 21:37:36.762642 31157 net.cpp:411] data_hdf5 -> label
I0520 21:37:36.762673 31157 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 21:37:36.763963 31157 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 21:37:36.766170 31157 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 21:37:58.291708 31157 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 21:37:58.296813 31157 net.cpp:150] Setting up data_hdf5
I0520 21:37:58.296854 31157 net.cpp:157] Top shape: 340 1 127 50 (2159000)
I0520 21:37:58.296869 31157 net.cpp:157] Top shape: 340 (340)
I0520 21:37:58.296880 31157 net.cpp:165] Memory required for data: 8637360
I0520 21:37:58.296895 31157 layer_factory.hpp:77] Creating layer conv1
I0520 21:37:58.296927 31157 net.cpp:106] Creating Layer conv1
I0520 21:37:58.296939 31157 net.cpp:454] conv1 <- data
I0520 21:37:58.296959 31157 net.cpp:411] conv1 -> conv1
I0520 21:37:58.684463 31157 net.cpp:150] Setting up conv1
I0520 21:37:58.684505 31157 net.cpp:157] Top shape: 340 12 120 48 (23500800)
I0520 21:37:58.684519 31157 net.cpp:165] Memory required for data: 102640560
I0520 21:37:58.684548 31157 layer_factory.hpp:77] Creating layer relu1
I0520 21:37:58.684569 31157 net.cpp:106] Creating Layer relu1
I0520 21:37:58.684581 31157 net.cpp:454] relu1 <- conv1
I0520 21:37:58.684594 31157 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:37:58.685111 31157 net.cpp:150] Setting up relu1
I0520 21:37:58.685127 31157 net.cpp:157] Top shape: 340 12 120 48 (23500800)
I0520 21:37:58.685138 31157 net.cpp:165] Memory required for data: 196643760
I0520 21:37:58.685148 31157 layer_factory.hpp:77] Creating layer pool1
I0520 21:37:58.685164 31157 net.cpp:106] Creating Layer pool1
I0520 21:37:58.685174 31157 net.cpp:454] pool1 <- conv1
I0520 21:37:58.685187 31157 net.cpp:411] pool1 -> pool1
I0520 21:37:58.685267 31157 net.cpp:150] Setting up pool1
I0520 21:37:58.685281 31157 net.cpp:157] Top shape: 340 12 60 48 (11750400)
I0520 21:37:58.685292 31157 net.cpp:165] Memory required for data: 243645360
I0520 21:37:58.685302 31157 layer_factory.hpp:77] Creating layer conv2
I0520 21:37:58.685324 31157 net.cpp:106] Creating Layer conv2
I0520 21:37:58.685335 31157 net.cpp:454] conv2 <- pool1
I0520 21:37:58.685348 31157 net.cpp:411] conv2 -> conv2
I0520 21:37:58.688031 31157 net.cpp:150] Setting up conv2
I0520 21:37:58.688058 31157 net.cpp:157] Top shape: 340 20 54 46 (16891200)
I0520 21:37:58.688069 31157 net.cpp:165] Memory required for data: 311210160
I0520 21:37:58.688088 31157 layer_factory.hpp:77] Creating layer relu2
I0520 21:37:58.688103 31157 net.cpp:106] Creating Layer relu2
I0520 21:37:58.688113 31157 net.cpp:454] relu2 <- conv2
I0520 21:37:58.688127 31157 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:37:58.688457 31157 net.cpp:150] Setting up relu2
I0520 21:37:58.688472 31157 net.cpp:157] Top shape: 340 20 54 46 (16891200)
I0520 21:37:58.688482 31157 net.cpp:165] Memory required for data: 378774960
I0520 21:37:58.688493 31157 layer_factory.hpp:77] Creating layer pool2
I0520 21:37:58.688504 31157 net.cpp:106] Creating Layer pool2
I0520 21:37:58.688514 31157 net.cpp:454] pool2 <- conv2
I0520 21:37:58.688539 31157 net.cpp:411] pool2 -> pool2
I0520 21:37:58.688607 31157 net.cpp:150] Setting up pool2
I0520 21:37:58.688621 31157 net.cpp:157] Top shape: 340 20 27 46 (8445600)
I0520 21:37:58.688630 31157 net.cpp:165] Memory required for data: 412557360
I0520 21:37:58.688639 31157 layer_factory.hpp:77] Creating layer conv3
I0520 21:37:58.688658 31157 net.cpp:106] Creating Layer conv3
I0520 21:37:58.688668 31157 net.cpp:454] conv3 <- pool2
I0520 21:37:58.688681 31157 net.cpp:411] conv3 -> conv3
I0520 21:37:58.690592 31157 net.cpp:150] Setting up conv3
I0520 21:37:58.690614 31157 net.cpp:157] Top shape: 340 28 22 44 (9215360)
I0520 21:37:58.690628 31157 net.cpp:165] Memory required for data: 449418800
I0520 21:37:58.690645 31157 layer_factory.hpp:77] Creating layer relu3
I0520 21:37:58.690661 31157 net.cpp:106] Creating Layer relu3
I0520 21:37:58.690671 31157 net.cpp:454] relu3 <- conv3
I0520 21:37:58.690685 31157 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:37:58.691155 31157 net.cpp:150] Setting up relu3
I0520 21:37:58.691174 31157 net.cpp:157] Top shape: 340 28 22 44 (9215360)
I0520 21:37:58.691184 31157 net.cpp:165] Memory required for data: 486280240
I0520 21:37:58.691193 31157 layer_factory.hpp:77] Creating layer pool3
I0520 21:37:58.691206 31157 net.cpp:106] Creating Layer pool3
I0520 21:37:58.691216 31157 net.cpp:454] pool3 <- conv3
I0520 21:37:58.691228 31157 net.cpp:411] pool3 -> pool3
I0520 21:37:58.691295 31157 net.cpp:150] Setting up pool3
I0520 21:37:58.691308 31157 net.cpp:157] Top shape: 340 28 11 44 (4607680)
I0520 21:37:58.691318 31157 net.cpp:165] Memory required for data: 504710960
I0520 21:37:58.691329 31157 layer_factory.hpp:77] Creating layer conv4
I0520 21:37:58.691345 31157 net.cpp:106] Creating Layer conv4
I0520 21:37:58.691356 31157 net.cpp:454] conv4 <- pool3
I0520 21:37:58.691370 31157 net.cpp:411] conv4 -> conv4
I0520 21:37:58.694159 31157 net.cpp:150] Setting up conv4
I0520 21:37:58.694186 31157 net.cpp:157] Top shape: 340 36 6 42 (3084480)
I0520 21:37:58.694197 31157 net.cpp:165] Memory required for data: 517048880
I0520 21:37:58.694212 31157 layer_factory.hpp:77] Creating layer relu4
I0520 21:37:58.694226 31157 net.cpp:106] Creating Layer relu4
I0520 21:37:58.694237 31157 net.cpp:454] relu4 <- conv4
I0520 21:37:58.694250 31157 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:37:58.694721 31157 net.cpp:150] Setting up relu4
I0520 21:37:58.694737 31157 net.cpp:157] Top shape: 340 36 6 42 (3084480)
I0520 21:37:58.694748 31157 net.cpp:165] Memory required for data: 529386800
I0520 21:37:58.694758 31157 layer_factory.hpp:77] Creating layer pool4
I0520 21:37:58.694772 31157 net.cpp:106] Creating Layer pool4
I0520 21:37:58.694782 31157 net.cpp:454] pool4 <- conv4
I0520 21:37:58.694794 31157 net.cpp:411] pool4 -> pool4
I0520 21:37:58.694862 31157 net.cpp:150] Setting up pool4
I0520 21:37:58.694876 31157 net.cpp:157] Top shape: 340 36 3 42 (1542240)
I0520 21:37:58.694886 31157 net.cpp:165] Memory required for data: 535555760
I0520 21:37:58.694896 31157 layer_factory.hpp:77] Creating layer ip1
I0520 21:37:58.694916 31157 net.cpp:106] Creating Layer ip1
I0520 21:37:58.694927 31157 net.cpp:454] ip1 <- pool4
I0520 21:37:58.694939 31157 net.cpp:411] ip1 -> ip1
I0520 21:37:58.710417 31157 net.cpp:150] Setting up ip1
I0520 21:37:58.710445 31157 net.cpp:157] Top shape: 340 196 (66640)
I0520 21:37:58.710458 31157 net.cpp:165] Memory required for data: 535822320
I0520 21:37:58.710479 31157 layer_factory.hpp:77] Creating layer relu5
I0520 21:37:58.710494 31157 net.cpp:106] Creating Layer relu5
I0520 21:37:58.710505 31157 net.cpp:454] relu5 <- ip1
I0520 21:37:58.710517 31157 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:37:58.710860 31157 net.cpp:150] Setting up relu5
I0520 21:37:58.710873 31157 net.cpp:157] Top shape: 340 196 (66640)
I0520 21:37:58.710883 31157 net.cpp:165] Memory required for data: 536088880
I0520 21:37:58.710894 31157 layer_factory.hpp:77] Creating layer drop1
I0520 21:37:58.710916 31157 net.cpp:106] Creating Layer drop1
I0520 21:37:58.710927 31157 net.cpp:454] drop1 <- ip1
I0520 21:37:58.710952 31157 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:37:58.710997 31157 net.cpp:150] Setting up drop1
I0520 21:37:58.711010 31157 net.cpp:157] Top shape: 340 196 (66640)
I0520 21:37:58.711021 31157 net.cpp:165] Memory required for data: 536355440
I0520 21:37:58.711030 31157 layer_factory.hpp:77] Creating layer ip2
I0520 21:37:58.711050 31157 net.cpp:106] Creating Layer ip2
I0520 21:37:58.711060 31157 net.cpp:454] ip2 <- ip1
I0520 21:37:58.711072 31157 net.cpp:411] ip2 -> ip2
I0520 21:37:58.711537 31157 net.cpp:150] Setting up ip2
I0520 21:37:58.711550 31157 net.cpp:157] Top shape: 340 98 (33320)
I0520 21:37:58.711560 31157 net.cpp:165] Memory required for data: 536488720
I0520 21:37:58.711575 31157 layer_factory.hpp:77] Creating layer relu6
I0520 21:37:58.711587 31157 net.cpp:106] Creating Layer relu6
I0520 21:37:58.711597 31157 net.cpp:454] relu6 <- ip2
I0520 21:37:58.711609 31157 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:37:58.712136 31157 net.cpp:150] Setting up relu6
I0520 21:37:58.712152 31157 net.cpp:157] Top shape: 340 98 (33320)
I0520 21:37:58.712162 31157 net.cpp:165] Memory required for data: 536622000
I0520 21:37:58.712172 31157 layer_factory.hpp:77] Creating layer drop2
I0520 21:37:58.712184 31157 net.cpp:106] Creating Layer drop2
I0520 21:37:58.712194 31157 net.cpp:454] drop2 <- ip2
I0520 21:37:58.712208 31157 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:37:58.712249 31157 net.cpp:150] Setting up drop2
I0520 21:37:58.712262 31157 net.cpp:157] Top shape: 340 98 (33320)
I0520 21:37:58.712272 31157 net.cpp:165] Memory required for data: 536755280
I0520 21:37:58.712282 31157 layer_factory.hpp:77] Creating layer ip3
I0520 21:37:58.712296 31157 net.cpp:106] Creating Layer ip3
I0520 21:37:58.712306 31157 net.cpp:454] ip3 <- ip2
I0520 21:37:58.712318 31157 net.cpp:411] ip3 -> ip3
I0520 21:37:58.712529 31157 net.cpp:150] Setting up ip3
I0520 21:37:58.712543 31157 net.cpp:157] Top shape: 340 11 (3740)
I0520 21:37:58.712553 31157 net.cpp:165] Memory required for data: 536770240
I0520 21:37:58.712568 31157 layer_factory.hpp:77] Creating layer drop3
I0520 21:37:58.712580 31157 net.cpp:106] Creating Layer drop3
I0520 21:37:58.712589 31157 net.cpp:454] drop3 <- ip3
I0520 21:37:58.712601 31157 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:37:58.712641 31157 net.cpp:150] Setting up drop3
I0520 21:37:58.712653 31157 net.cpp:157] Top shape: 340 11 (3740)
I0520 21:37:58.712663 31157 net.cpp:165] Memory required for data: 536785200
I0520 21:37:58.712673 31157 layer_factory.hpp:77] Creating layer loss
I0520 21:37:58.712692 31157 net.cpp:106] Creating Layer loss
I0520 21:37:58.712702 31157 net.cpp:454] loss <- ip3
I0520 21:37:58.712713 31157 net.cpp:454] loss <- label
I0520 21:37:58.712725 31157 net.cpp:411] loss -> loss
I0520 21:37:58.712743 31157 layer_factory.hpp:77] Creating layer loss
I0520 21:37:58.713387 31157 net.cpp:150] Setting up loss
I0520 21:37:58.713408 31157 net.cpp:157] Top shape: (1)
I0520 21:37:58.713418 31157 net.cpp:160]     with loss weight 1
I0520 21:37:58.713461 31157 net.cpp:165] Memory required for data: 536785204
I0520 21:37:58.713471 31157 net.cpp:226] loss needs backward computation.
I0520 21:37:58.713482 31157 net.cpp:226] drop3 needs backward computation.
I0520 21:37:58.713495 31157 net.cpp:226] ip3 needs backward computation.
I0520 21:37:58.713505 31157 net.cpp:226] drop2 needs backward computation.
I0520 21:37:58.713515 31157 net.cpp:226] relu6 needs backward computation.
I0520 21:37:58.713521 31157 net.cpp:226] ip2 needs backward computation.
I0520 21:37:58.713532 31157 net.cpp:226] drop1 needs backward computation.
I0520 21:37:58.713542 31157 net.cpp:226] relu5 needs backward computation.
I0520 21:37:58.713551 31157 net.cpp:226] ip1 needs backward computation.
I0520 21:37:58.713562 31157 net.cpp:226] pool4 needs backward computation.
I0520 21:37:58.713572 31157 net.cpp:226] relu4 needs backward computation.
I0520 21:37:58.713582 31157 net.cpp:226] conv4 needs backward computation.
I0520 21:37:58.713593 31157 net.cpp:226] pool3 needs backward computation.
I0520 21:37:58.713611 31157 net.cpp:226] relu3 needs backward computation.
I0520 21:37:58.713623 31157 net.cpp:226] conv3 needs backward computation.
I0520 21:37:58.713634 31157 net.cpp:226] pool2 needs backward computation.
I0520 21:37:58.713644 31157 net.cpp:226] relu2 needs backward computation.
I0520 21:37:58.713654 31157 net.cpp:226] conv2 needs backward computation.
I0520 21:37:58.713665 31157 net.cpp:226] pool1 needs backward computation.
I0520 21:37:58.713675 31157 net.cpp:226] relu1 needs backward computation.
I0520 21:37:58.713683 31157 net.cpp:226] conv1 needs backward computation.
I0520 21:37:58.713696 31157 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:37:58.713704 31157 net.cpp:270] This network produces output loss
I0520 21:37:58.713728 31157 net.cpp:283] Network initialization done.
I0520 21:37:58.715320 31157 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022.prototxt
I0520 21:37:58.715391 31157 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 21:37:58.715759 31157 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 340
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:37:58.715948 31157 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:37:58.715963 31157 net.cpp:106] Creating Layer data_hdf5
I0520 21:37:58.715975 31157 net.cpp:411] data_hdf5 -> data
I0520 21:37:58.715991 31157 net.cpp:411] data_hdf5 -> label
I0520 21:37:58.716007 31157 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 21:37:58.717188 31157 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 21:38:20.031278 31157 net.cpp:150] Setting up data_hdf5
I0520 21:38:20.031445 31157 net.cpp:157] Top shape: 340 1 127 50 (2159000)
I0520 21:38:20.031460 31157 net.cpp:157] Top shape: 340 (340)
I0520 21:38:20.031469 31157 net.cpp:165] Memory required for data: 8637360
I0520 21:38:20.031483 31157 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 21:38:20.031512 31157 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 21:38:20.031523 31157 net.cpp:454] label_data_hdf5_1_split <- label
I0520 21:38:20.031538 31157 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 21:38:20.031558 31157 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 21:38:20.031631 31157 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 21:38:20.031656 31157 net.cpp:157] Top shape: 340 (340)
I0520 21:38:20.031667 31157 net.cpp:157] Top shape: 340 (340)
I0520 21:38:20.031677 31157 net.cpp:165] Memory required for data: 8640080
I0520 21:38:20.031687 31157 layer_factory.hpp:77] Creating layer conv1
I0520 21:38:20.031708 31157 net.cpp:106] Creating Layer conv1
I0520 21:38:20.031719 31157 net.cpp:454] conv1 <- data
I0520 21:38:20.031734 31157 net.cpp:411] conv1 -> conv1
I0520 21:38:20.033676 31157 net.cpp:150] Setting up conv1
I0520 21:38:20.033696 31157 net.cpp:157] Top shape: 340 12 120 48 (23500800)
I0520 21:38:20.033706 31157 net.cpp:165] Memory required for data: 102643280
I0520 21:38:20.033727 31157 layer_factory.hpp:77] Creating layer relu1
I0520 21:38:20.033741 31157 net.cpp:106] Creating Layer relu1
I0520 21:38:20.033751 31157 net.cpp:454] relu1 <- conv1
I0520 21:38:20.033764 31157 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:38:20.034266 31157 net.cpp:150] Setting up relu1
I0520 21:38:20.034283 31157 net.cpp:157] Top shape: 340 12 120 48 (23500800)
I0520 21:38:20.034293 31157 net.cpp:165] Memory required for data: 196646480
I0520 21:38:20.034303 31157 layer_factory.hpp:77] Creating layer pool1
I0520 21:38:20.034319 31157 net.cpp:106] Creating Layer pool1
I0520 21:38:20.034329 31157 net.cpp:454] pool1 <- conv1
I0520 21:38:20.034343 31157 net.cpp:411] pool1 -> pool1
I0520 21:38:20.034416 31157 net.cpp:150] Setting up pool1
I0520 21:38:20.034430 31157 net.cpp:157] Top shape: 340 12 60 48 (11750400)
I0520 21:38:20.034440 31157 net.cpp:165] Memory required for data: 243648080
I0520 21:38:20.034451 31157 layer_factory.hpp:77] Creating layer conv2
I0520 21:38:20.034467 31157 net.cpp:106] Creating Layer conv2
I0520 21:38:20.034478 31157 net.cpp:454] conv2 <- pool1
I0520 21:38:20.034493 31157 net.cpp:411] conv2 -> conv2
I0520 21:38:20.036417 31157 net.cpp:150] Setting up conv2
I0520 21:38:20.036439 31157 net.cpp:157] Top shape: 340 20 54 46 (16891200)
I0520 21:38:20.036453 31157 net.cpp:165] Memory required for data: 311212880
I0520 21:38:20.036469 31157 layer_factory.hpp:77] Creating layer relu2
I0520 21:38:20.036484 31157 net.cpp:106] Creating Layer relu2
I0520 21:38:20.036494 31157 net.cpp:454] relu2 <- conv2
I0520 21:38:20.036505 31157 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:38:20.036839 31157 net.cpp:150] Setting up relu2
I0520 21:38:20.036852 31157 net.cpp:157] Top shape: 340 20 54 46 (16891200)
I0520 21:38:20.036862 31157 net.cpp:165] Memory required for data: 378777680
I0520 21:38:20.036872 31157 layer_factory.hpp:77] Creating layer pool2
I0520 21:38:20.036885 31157 net.cpp:106] Creating Layer pool2
I0520 21:38:20.036895 31157 net.cpp:454] pool2 <- conv2
I0520 21:38:20.036907 31157 net.cpp:411] pool2 -> pool2
I0520 21:38:20.036978 31157 net.cpp:150] Setting up pool2
I0520 21:38:20.036991 31157 net.cpp:157] Top shape: 340 20 27 46 (8445600)
I0520 21:38:20.037001 31157 net.cpp:165] Memory required for data: 412560080
I0520 21:38:20.037010 31157 layer_factory.hpp:77] Creating layer conv3
I0520 21:38:20.037029 31157 net.cpp:106] Creating Layer conv3
I0520 21:38:20.037039 31157 net.cpp:454] conv3 <- pool2
I0520 21:38:20.037052 31157 net.cpp:411] conv3 -> conv3
I0520 21:38:20.039013 31157 net.cpp:150] Setting up conv3
I0520 21:38:20.039036 31157 net.cpp:157] Top shape: 340 28 22 44 (9215360)
I0520 21:38:20.039047 31157 net.cpp:165] Memory required for data: 449421520
I0520 21:38:20.039080 31157 layer_factory.hpp:77] Creating layer relu3
I0520 21:38:20.039094 31157 net.cpp:106] Creating Layer relu3
I0520 21:38:20.039104 31157 net.cpp:454] relu3 <- conv3
I0520 21:38:20.039118 31157 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:38:20.039587 31157 net.cpp:150] Setting up relu3
I0520 21:38:20.039603 31157 net.cpp:157] Top shape: 340 28 22 44 (9215360)
I0520 21:38:20.039613 31157 net.cpp:165] Memory required for data: 486282960
I0520 21:38:20.039623 31157 layer_factory.hpp:77] Creating layer pool3
I0520 21:38:20.039636 31157 net.cpp:106] Creating Layer pool3
I0520 21:38:20.039654 31157 net.cpp:454] pool3 <- conv3
I0520 21:38:20.039666 31157 net.cpp:411] pool3 -> pool3
I0520 21:38:20.039739 31157 net.cpp:150] Setting up pool3
I0520 21:38:20.039752 31157 net.cpp:157] Top shape: 340 28 11 44 (4607680)
I0520 21:38:20.039762 31157 net.cpp:165] Memory required for data: 504713680
I0520 21:38:20.039770 31157 layer_factory.hpp:77] Creating layer conv4
I0520 21:38:20.039788 31157 net.cpp:106] Creating Layer conv4
I0520 21:38:20.039798 31157 net.cpp:454] conv4 <- pool3
I0520 21:38:20.039813 31157 net.cpp:411] conv4 -> conv4
I0520 21:38:20.041863 31157 net.cpp:150] Setting up conv4
I0520 21:38:20.041880 31157 net.cpp:157] Top shape: 340 36 6 42 (3084480)
I0520 21:38:20.041893 31157 net.cpp:165] Memory required for data: 517051600
I0520 21:38:20.041908 31157 layer_factory.hpp:77] Creating layer relu4
I0520 21:38:20.041921 31157 net.cpp:106] Creating Layer relu4
I0520 21:38:20.041930 31157 net.cpp:454] relu4 <- conv4
I0520 21:38:20.041944 31157 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:38:20.042413 31157 net.cpp:150] Setting up relu4
I0520 21:38:20.042429 31157 net.cpp:157] Top shape: 340 36 6 42 (3084480)
I0520 21:38:20.042439 31157 net.cpp:165] Memory required for data: 529389520
I0520 21:38:20.042449 31157 layer_factory.hpp:77] Creating layer pool4
I0520 21:38:20.042462 31157 net.cpp:106] Creating Layer pool4
I0520 21:38:20.042472 31157 net.cpp:454] pool4 <- conv4
I0520 21:38:20.042485 31157 net.cpp:411] pool4 -> pool4
I0520 21:38:20.042556 31157 net.cpp:150] Setting up pool4
I0520 21:38:20.042568 31157 net.cpp:157] Top shape: 340 36 3 42 (1542240)
I0520 21:38:20.042577 31157 net.cpp:165] Memory required for data: 535558480
I0520 21:38:20.042587 31157 layer_factory.hpp:77] Creating layer ip1
I0520 21:38:20.042603 31157 net.cpp:106] Creating Layer ip1
I0520 21:38:20.042613 31157 net.cpp:454] ip1 <- pool4
I0520 21:38:20.042628 31157 net.cpp:411] ip1 -> ip1
I0520 21:38:20.058123 31157 net.cpp:150] Setting up ip1
I0520 21:38:20.058151 31157 net.cpp:157] Top shape: 340 196 (66640)
I0520 21:38:20.058167 31157 net.cpp:165] Memory required for data: 535825040
I0520 21:38:20.058188 31157 layer_factory.hpp:77] Creating layer relu5
I0520 21:38:20.058203 31157 net.cpp:106] Creating Layer relu5
I0520 21:38:20.058213 31157 net.cpp:454] relu5 <- ip1
I0520 21:38:20.058228 31157 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:38:20.058574 31157 net.cpp:150] Setting up relu5
I0520 21:38:20.058588 31157 net.cpp:157] Top shape: 340 196 (66640)
I0520 21:38:20.058598 31157 net.cpp:165] Memory required for data: 536091600
I0520 21:38:20.058607 31157 layer_factory.hpp:77] Creating layer drop1
I0520 21:38:20.058626 31157 net.cpp:106] Creating Layer drop1
I0520 21:38:20.058635 31157 net.cpp:454] drop1 <- ip1
I0520 21:38:20.058648 31157 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:38:20.058692 31157 net.cpp:150] Setting up drop1
I0520 21:38:20.058706 31157 net.cpp:157] Top shape: 340 196 (66640)
I0520 21:38:20.058715 31157 net.cpp:165] Memory required for data: 536358160
I0520 21:38:20.058725 31157 layer_factory.hpp:77] Creating layer ip2
I0520 21:38:20.058739 31157 net.cpp:106] Creating Layer ip2
I0520 21:38:20.058749 31157 net.cpp:454] ip2 <- ip1
I0520 21:38:20.058763 31157 net.cpp:411] ip2 -> ip2
I0520 21:38:20.059245 31157 net.cpp:150] Setting up ip2
I0520 21:38:20.059258 31157 net.cpp:157] Top shape: 340 98 (33320)
I0520 21:38:20.059268 31157 net.cpp:165] Memory required for data: 536491440
I0520 21:38:20.059296 31157 layer_factory.hpp:77] Creating layer relu6
I0520 21:38:20.059309 31157 net.cpp:106] Creating Layer relu6
I0520 21:38:20.059319 31157 net.cpp:454] relu6 <- ip2
I0520 21:38:20.059331 31157 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:38:20.059875 31157 net.cpp:150] Setting up relu6
I0520 21:38:20.059896 31157 net.cpp:157] Top shape: 340 98 (33320)
I0520 21:38:20.059906 31157 net.cpp:165] Memory required for data: 536624720
I0520 21:38:20.059916 31157 layer_factory.hpp:77] Creating layer drop2
I0520 21:38:20.059929 31157 net.cpp:106] Creating Layer drop2
I0520 21:38:20.059939 31157 net.cpp:454] drop2 <- ip2
I0520 21:38:20.059952 31157 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:38:20.059996 31157 net.cpp:150] Setting up drop2
I0520 21:38:20.060009 31157 net.cpp:157] Top shape: 340 98 (33320)
I0520 21:38:20.060019 31157 net.cpp:165] Memory required for data: 536758000
I0520 21:38:20.060029 31157 layer_factory.hpp:77] Creating layer ip3
I0520 21:38:20.060042 31157 net.cpp:106] Creating Layer ip3
I0520 21:38:20.060052 31157 net.cpp:454] ip3 <- ip2
I0520 21:38:20.060065 31157 net.cpp:411] ip3 -> ip3
I0520 21:38:20.060291 31157 net.cpp:150] Setting up ip3
I0520 21:38:20.060304 31157 net.cpp:157] Top shape: 340 11 (3740)
I0520 21:38:20.060313 31157 net.cpp:165] Memory required for data: 536772960
I0520 21:38:20.060328 31157 layer_factory.hpp:77] Creating layer drop3
I0520 21:38:20.060341 31157 net.cpp:106] Creating Layer drop3
I0520 21:38:20.060351 31157 net.cpp:454] drop3 <- ip3
I0520 21:38:20.060364 31157 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:38:20.060405 31157 net.cpp:150] Setting up drop3
I0520 21:38:20.060417 31157 net.cpp:157] Top shape: 340 11 (3740)
I0520 21:38:20.060427 31157 net.cpp:165] Memory required for data: 536787920
I0520 21:38:20.060436 31157 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 21:38:20.060449 31157 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 21:38:20.060459 31157 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 21:38:20.060472 31157 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 21:38:20.060487 31157 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 21:38:20.060560 31157 net.cpp:150] Setting up ip3_drop3_0_split
I0520 21:38:20.060573 31157 net.cpp:157] Top shape: 340 11 (3740)
I0520 21:38:20.060585 31157 net.cpp:157] Top shape: 340 11 (3740)
I0520 21:38:20.060595 31157 net.cpp:165] Memory required for data: 536817840
I0520 21:38:20.060603 31157 layer_factory.hpp:77] Creating layer accuracy
I0520 21:38:20.060626 31157 net.cpp:106] Creating Layer accuracy
I0520 21:38:20.060636 31157 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 21:38:20.060647 31157 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 21:38:20.060662 31157 net.cpp:411] accuracy -> accuracy
I0520 21:38:20.060684 31157 net.cpp:150] Setting up accuracy
I0520 21:38:20.060698 31157 net.cpp:157] Top shape: (1)
I0520 21:38:20.060708 31157 net.cpp:165] Memory required for data: 536817844
I0520 21:38:20.060716 31157 layer_factory.hpp:77] Creating layer loss
I0520 21:38:20.060730 31157 net.cpp:106] Creating Layer loss
I0520 21:38:20.060741 31157 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 21:38:20.060752 31157 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 21:38:20.060765 31157 net.cpp:411] loss -> loss
I0520 21:38:20.060783 31157 layer_factory.hpp:77] Creating layer loss
I0520 21:38:20.061269 31157 net.cpp:150] Setting up loss
I0520 21:38:20.061282 31157 net.cpp:157] Top shape: (1)
I0520 21:38:20.061292 31157 net.cpp:160]     with loss weight 1
I0520 21:38:20.061311 31157 net.cpp:165] Memory required for data: 536817848
I0520 21:38:20.061321 31157 net.cpp:226] loss needs backward computation.
I0520 21:38:20.061332 31157 net.cpp:228] accuracy does not need backward computation.
I0520 21:38:20.061342 31157 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 21:38:20.061353 31157 net.cpp:226] drop3 needs backward computation.
I0520 21:38:20.061362 31157 net.cpp:226] ip3 needs backward computation.
I0520 21:38:20.061372 31157 net.cpp:226] drop2 needs backward computation.
I0520 21:38:20.061390 31157 net.cpp:226] relu6 needs backward computation.
I0520 21:38:20.061400 31157 net.cpp:226] ip2 needs backward computation.
I0520 21:38:20.061411 31157 net.cpp:226] drop1 needs backward computation.
I0520 21:38:20.061420 31157 net.cpp:226] relu5 needs backward computation.
I0520 21:38:20.061430 31157 net.cpp:226] ip1 needs backward computation.
I0520 21:38:20.061440 31157 net.cpp:226] pool4 needs backward computation.
I0520 21:38:20.061450 31157 net.cpp:226] relu4 needs backward computation.
I0520 21:38:20.061460 31157 net.cpp:226] conv4 needs backward computation.
I0520 21:38:20.061470 31157 net.cpp:226] pool3 needs backward computation.
I0520 21:38:20.061480 31157 net.cpp:226] relu3 needs backward computation.
I0520 21:38:20.061491 31157 net.cpp:226] conv3 needs backward computation.
I0520 21:38:20.061501 31157 net.cpp:226] pool2 needs backward computation.
I0520 21:38:20.061512 31157 net.cpp:226] relu2 needs backward computation.
I0520 21:38:20.061522 31157 net.cpp:226] conv2 needs backward computation.
I0520 21:38:20.061533 31157 net.cpp:226] pool1 needs backward computation.
I0520 21:38:20.061543 31157 net.cpp:226] relu1 needs backward computation.
I0520 21:38:20.061553 31157 net.cpp:226] conv1 needs backward computation.
I0520 21:38:20.061565 31157 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 21:38:20.061576 31157 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:38:20.061586 31157 net.cpp:270] This network produces output accuracy
I0520 21:38:20.061596 31157 net.cpp:270] This network produces output loss
I0520 21:38:20.061625 31157 net.cpp:283] Network initialization done.
I0520 21:38:20.061758 31157 solver.cpp:60] Solver scaffolding done.
I0520 21:38:20.062882 31157 caffe.cpp:212] Starting Optimization
I0520 21:38:20.062901 31157 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 21:38:20.062914 31157 solver.cpp:289] Learning Rate Policy: fixed
I0520 21:38:20.064143 31157 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 21:39:06.292569 31157 solver.cpp:409]     Test net output #0: accuracy = 0.105102
I0520 21:39:06.292729 31157 solver.cpp:409]     Test net output #1: loss = 2.39944 (* 1 = 2.39944 loss)
I0520 21:39:06.364974 31157 solver.cpp:237] Iteration 0, loss = 2.40178
I0520 21:39:06.365010 31157 solver.cpp:253]     Train net output #0: loss = 2.40178 (* 1 = 2.40178 loss)
I0520 21:39:06.365030 31157 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 21:39:14.501060 31157 solver.cpp:237] Iteration 44, loss = 2.36513
I0520 21:39:14.501106 31157 solver.cpp:253]     Train net output #0: loss = 2.36513 (* 1 = 2.36513 loss)
I0520 21:39:14.501121 31157 sgd_solver.cpp:106] Iteration 44, lr = 0.0025
I0520 21:39:22.629276 31157 solver.cpp:237] Iteration 88, loss = 2.34091
I0520 21:39:22.629309 31157 solver.cpp:253]     Train net output #0: loss = 2.34091 (* 1 = 2.34091 loss)
I0520 21:39:22.629328 31157 sgd_solver.cpp:106] Iteration 88, lr = 0.0025
I0520 21:39:30.757294 31157 solver.cpp:237] Iteration 132, loss = 2.34554
I0520 21:39:30.757328 31157 solver.cpp:253]     Train net output #0: loss = 2.34554 (* 1 = 2.34554 loss)
I0520 21:39:30.757345 31157 sgd_solver.cpp:106] Iteration 132, lr = 0.0025
I0520 21:39:38.881759 31157 solver.cpp:237] Iteration 176, loss = 2.29918
I0520 21:39:38.881921 31157 solver.cpp:253]     Train net output #0: loss = 2.29918 (* 1 = 2.29918 loss)
I0520 21:39:38.881935 31157 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0520 21:39:47.008266 31157 solver.cpp:237] Iteration 220, loss = 2.27249
I0520 21:39:47.008299 31157 solver.cpp:253]     Train net output #0: loss = 2.27249 (* 1 = 2.27249 loss)
I0520 21:39:47.008316 31157 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0520 21:39:55.136881 31157 solver.cpp:237] Iteration 264, loss = 2.19327
I0520 21:39:55.136915 31157 solver.cpp:253]     Train net output #0: loss = 2.19327 (* 1 = 2.19327 loss)
I0520 21:39:55.136934 31157 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0520 21:40:25.371743 31157 solver.cpp:237] Iteration 308, loss = 2.22721
I0520 21:40:25.371906 31157 solver.cpp:253]     Train net output #0: loss = 2.22721 (* 1 = 2.22721 loss)
I0520 21:40:25.371919 31157 sgd_solver.cpp:106] Iteration 308, lr = 0.0025
I0520 21:40:33.497639 31157 solver.cpp:237] Iteration 352, loss = 2.11882
I0520 21:40:33.497684 31157 solver.cpp:253]     Train net output #0: loss = 2.11882 (* 1 = 2.11882 loss)
I0520 21:40:33.497699 31157 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0520 21:40:41.627727 31157 solver.cpp:237] Iteration 396, loss = 2.01043
I0520 21:40:41.627761 31157 solver.cpp:253]     Train net output #0: loss = 2.01043 (* 1 = 2.01043 loss)
I0520 21:40:41.627779 31157 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0520 21:40:49.759554 31157 solver.cpp:237] Iteration 440, loss = 2.01706
I0520 21:40:49.759588 31157 solver.cpp:253]     Train net output #0: loss = 2.01706 (* 1 = 2.01706 loss)
I0520 21:40:49.759605 31157 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0520 21:40:49.759984 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_441.caffemodel
I0520 21:40:49.931118 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_441.solverstate
I0520 21:40:57.957003 31157 solver.cpp:237] Iteration 484, loss = 2.02282
I0520 21:40:57.957160 31157 solver.cpp:253]     Train net output #0: loss = 2.02282 (* 1 = 2.02282 loss)
I0520 21:40:57.957175 31157 sgd_solver.cpp:106] Iteration 484, lr = 0.0025
I0520 21:41:06.086879 31157 solver.cpp:237] Iteration 528, loss = 1.99825
I0520 21:41:06.086911 31157 solver.cpp:253]     Train net output #0: loss = 1.99825 (* 1 = 1.99825 loss)
I0520 21:41:06.086928 31157 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0520 21:41:14.213825 31157 solver.cpp:237] Iteration 572, loss = 1.9581
I0520 21:41:14.213858 31157 solver.cpp:253]     Train net output #0: loss = 1.9581 (* 1 = 1.9581 loss)
I0520 21:41:14.213876 31157 sgd_solver.cpp:106] Iteration 572, lr = 0.0025
I0520 21:41:44.466563 31157 solver.cpp:237] Iteration 616, loss = 2.01314
I0520 21:41:44.466717 31157 solver.cpp:253]     Train net output #0: loss = 2.01314 (* 1 = 2.01314 loss)
I0520 21:41:44.466732 31157 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0520 21:41:52.597105 31157 solver.cpp:237] Iteration 660, loss = 1.92735
I0520 21:41:52.597136 31157 solver.cpp:253]     Train net output #0: loss = 1.92735 (* 1 = 1.92735 loss)
I0520 21:41:52.597154 31157 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0520 21:42:00.728824 31157 solver.cpp:237] Iteration 704, loss = 1.8629
I0520 21:42:00.728858 31157 solver.cpp:253]     Train net output #0: loss = 1.8629 (* 1 = 1.8629 loss)
I0520 21:42:00.728874 31157 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0520 21:42:08.863500 31157 solver.cpp:237] Iteration 748, loss = 1.9684
I0520 21:42:08.863540 31157 solver.cpp:253]     Train net output #0: loss = 1.9684 (* 1 = 1.9684 loss)
I0520 21:42:08.863560 31157 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0520 21:42:16.995736 31157 solver.cpp:237] Iteration 792, loss = 1.84231
I0520 21:42:16.995882 31157 solver.cpp:253]     Train net output #0: loss = 1.84231 (* 1 = 1.84231 loss)
I0520 21:42:16.995895 31157 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0520 21:42:25.127549 31157 solver.cpp:237] Iteration 836, loss = 1.78163
I0520 21:42:25.127583 31157 solver.cpp:253]     Train net output #0: loss = 1.78163 (* 1 = 1.78163 loss)
I0520 21:42:25.127596 31157 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0520 21:42:33.262812 31157 solver.cpp:237] Iteration 880, loss = 1.76481
I0520 21:42:33.262853 31157 solver.cpp:253]     Train net output #0: loss = 1.76481 (* 1 = 1.76481 loss)
I0520 21:42:33.262869 31157 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0520 21:42:33.447141 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_882.caffemodel
I0520 21:42:33.614857 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_882.solverstate
I0520 21:42:33.640518 31157 solver.cpp:341] Iteration 882, Testing net (#0)
I0520 21:43:18.947427 31157 solver.cpp:409]     Test net output #0: accuracy = 0.597719
I0520 21:43:18.947587 31157 solver.cpp:409]     Test net output #1: loss = 1.42289 (* 1 = 1.42289 loss)
I0520 21:43:48.907809 31157 solver.cpp:237] Iteration 924, loss = 1.8676
I0520 21:43:48.907857 31157 solver.cpp:253]     Train net output #0: loss = 1.8676 (* 1 = 1.8676 loss)
I0520 21:43:48.907874 31157 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0520 21:43:57.033344 31157 solver.cpp:237] Iteration 968, loss = 1.78021
I0520 21:43:57.033489 31157 solver.cpp:253]     Train net output #0: loss = 1.78021 (* 1 = 1.78021 loss)
I0520 21:43:57.033502 31157 sgd_solver.cpp:106] Iteration 968, lr = 0.0025
I0520 21:44:05.158705 31157 solver.cpp:237] Iteration 1012, loss = 1.8296
I0520 21:44:05.158737 31157 solver.cpp:253]     Train net output #0: loss = 1.8296 (* 1 = 1.8296 loss)
I0520 21:44:05.158754 31157 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0520 21:44:13.283370 31157 solver.cpp:237] Iteration 1056, loss = 1.83695
I0520 21:44:13.283411 31157 solver.cpp:253]     Train net output #0: loss = 1.83695 (* 1 = 1.83695 loss)
I0520 21:44:13.283429 31157 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0520 21:44:21.406816 31157 solver.cpp:237] Iteration 1100, loss = 1.75045
I0520 21:44:21.406849 31157 solver.cpp:253]     Train net output #0: loss = 1.75045 (* 1 = 1.75045 loss)
I0520 21:44:21.406865 31157 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0520 21:44:29.532896 31157 solver.cpp:237] Iteration 1144, loss = 1.72368
I0520 21:44:29.533030 31157 solver.cpp:253]     Train net output #0: loss = 1.72368 (* 1 = 1.72368 loss)
I0520 21:44:29.533044 31157 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0520 21:44:59.818578 31157 solver.cpp:237] Iteration 1188, loss = 1.78764
I0520 21:44:59.818740 31157 solver.cpp:253]     Train net output #0: loss = 1.78764 (* 1 = 1.78764 loss)
I0520 21:44:59.818755 31157 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0520 21:45:07.943560 31157 solver.cpp:237] Iteration 1232, loss = 1.66901
I0520 21:45:07.943603 31157 solver.cpp:253]     Train net output #0: loss = 1.66901 (* 1 = 1.66901 loss)
I0520 21:45:07.943619 31157 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0520 21:45:16.068810 31157 solver.cpp:237] Iteration 1276, loss = 1.83948
I0520 21:45:16.068845 31157 solver.cpp:253]     Train net output #0: loss = 1.83948 (* 1 = 1.83948 loss)
I0520 21:45:16.068861 31157 sgd_solver.cpp:106] Iteration 1276, lr = 0.0025
I0520 21:45:24.192683 31157 solver.cpp:237] Iteration 1320, loss = 1.81187
I0520 21:45:24.192716 31157 solver.cpp:253]     Train net output #0: loss = 1.81187 (* 1 = 1.81187 loss)
I0520 21:45:24.192731 31157 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0520 21:45:24.561890 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_1323.caffemodel
I0520 21:45:24.732389 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_1323.solverstate
I0520 21:45:32.387871 31157 solver.cpp:237] Iteration 1364, loss = 1.68363
I0520 21:45:32.388046 31157 solver.cpp:253]     Train net output #0: loss = 1.68363 (* 1 = 1.68363 loss)
I0520 21:45:32.388059 31157 sgd_solver.cpp:106] Iteration 1364, lr = 0.0025
I0520 21:45:40.512670 31157 solver.cpp:237] Iteration 1408, loss = 1.69061
I0520 21:45:40.512704 31157 solver.cpp:253]     Train net output #0: loss = 1.69061 (* 1 = 1.69061 loss)
I0520 21:45:40.512722 31157 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0520 21:45:48.638401 31157 solver.cpp:237] Iteration 1452, loss = 1.86091
I0520 21:45:48.638433 31157 solver.cpp:253]     Train net output #0: loss = 1.86091 (* 1 = 1.86091 loss)
I0520 21:45:48.638451 31157 sgd_solver.cpp:106] Iteration 1452, lr = 0.0025
I0520 21:46:18.910290 31157 solver.cpp:237] Iteration 1496, loss = 1.71546
I0520 21:46:18.910450 31157 solver.cpp:253]     Train net output #0: loss = 1.71546 (* 1 = 1.71546 loss)
I0520 21:46:18.910465 31157 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 21:46:27.029970 31157 solver.cpp:237] Iteration 1540, loss = 1.73246
I0520 21:46:27.030002 31157 solver.cpp:253]     Train net output #0: loss = 1.73246 (* 1 = 1.73246 loss)
I0520 21:46:27.030020 31157 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0520 21:46:35.153394 31157 solver.cpp:237] Iteration 1584, loss = 1.68198
I0520 21:46:35.153429 31157 solver.cpp:253]     Train net output #0: loss = 1.68198 (* 1 = 1.68198 loss)
I0520 21:46:35.153445 31157 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0520 21:46:43.281960 31157 solver.cpp:237] Iteration 1628, loss = 1.75221
I0520 21:46:43.281991 31157 solver.cpp:253]     Train net output #0: loss = 1.75221 (* 1 = 1.75221 loss)
I0520 21:46:43.282013 31157 sgd_solver.cpp:106] Iteration 1628, lr = 0.0025
I0520 21:46:51.405844 31157 solver.cpp:237] Iteration 1672, loss = 1.67646
I0520 21:46:51.405977 31157 solver.cpp:253]     Train net output #0: loss = 1.67646 (* 1 = 1.67646 loss)
I0520 21:46:51.405990 31157 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0520 21:46:59.529325 31157 solver.cpp:237] Iteration 1716, loss = 1.68609
I0520 21:46:59.529357 31157 solver.cpp:253]     Train net output #0: loss = 1.68609 (* 1 = 1.68609 loss)
I0520 21:46:59.529376 31157 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0520 21:47:07.647838 31157 solver.cpp:237] Iteration 1760, loss = 1.71242
I0520 21:47:07.647881 31157 solver.cpp:253]     Train net output #0: loss = 1.71242 (* 1 = 1.71242 loss)
I0520 21:47:07.647896 31157 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0520 21:47:08.202564 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_1764.caffemodel
I0520 21:47:08.371778 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_1764.solverstate
I0520 21:47:08.400624 31157 solver.cpp:341] Iteration 1764, Testing net (#0)
I0520 21:48:14.533579 31157 solver.cpp:409]     Test net output #0: accuracy = 0.65014
I0520 21:48:14.533749 31157 solver.cpp:409]     Test net output #1: loss = 1.21898 (* 1 = 1.21898 loss)
I0520 21:48:44.152918 31157 solver.cpp:237] Iteration 1804, loss = 1.72559
I0520 21:48:44.152969 31157 solver.cpp:253]     Train net output #0: loss = 1.72559 (* 1 = 1.72559 loss)
I0520 21:48:44.152984 31157 sgd_solver.cpp:106] Iteration 1804, lr = 0.0025
I0520 21:48:52.292012 31157 solver.cpp:237] Iteration 1848, loss = 1.85486
I0520 21:48:52.292173 31157 solver.cpp:253]     Train net output #0: loss = 1.85486 (* 1 = 1.85486 loss)
I0520 21:48:52.292187 31157 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0520 21:49:00.427953 31157 solver.cpp:237] Iteration 1892, loss = 1.76163
I0520 21:49:00.427986 31157 solver.cpp:253]     Train net output #0: loss = 1.76163 (* 1 = 1.76163 loss)
I0520 21:49:00.428004 31157 sgd_solver.cpp:106] Iteration 1892, lr = 0.0025
I0520 21:49:08.560976 31157 solver.cpp:237] Iteration 1936, loss = 1.70256
I0520 21:49:08.561010 31157 solver.cpp:253]     Train net output #0: loss = 1.70256 (* 1 = 1.70256 loss)
I0520 21:49:08.561025 31157 sgd_solver.cpp:106] Iteration 1936, lr = 0.0025
I0520 21:49:16.694428 31157 solver.cpp:237] Iteration 1980, loss = 1.67163
I0520 21:49:16.694468 31157 solver.cpp:253]     Train net output #0: loss = 1.67163 (* 1 = 1.67163 loss)
I0520 21:49:16.694488 31157 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0520 21:49:24.833376 31157 solver.cpp:237] Iteration 2024, loss = 1.67543
I0520 21:49:24.833515 31157 solver.cpp:253]     Train net output #0: loss = 1.67543 (* 1 = 1.67543 loss)
I0520 21:49:24.833528 31157 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0520 21:49:55.123567 31157 solver.cpp:237] Iteration 2068, loss = 1.64202
I0520 21:49:55.123745 31157 solver.cpp:253]     Train net output #0: loss = 1.64202 (* 1 = 1.64202 loss)
I0520 21:49:55.123760 31157 sgd_solver.cpp:106] Iteration 2068, lr = 0.0025
I0520 21:50:03.254945 31157 solver.cpp:237] Iteration 2112, loss = 1.68552
I0520 21:50:03.254992 31157 solver.cpp:253]     Train net output #0: loss = 1.68552 (* 1 = 1.68552 loss)
I0520 21:50:03.255008 31157 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0520 21:50:11.388383 31157 solver.cpp:237] Iteration 2156, loss = 1.66846
I0520 21:50:11.388417 31157 solver.cpp:253]     Train net output #0: loss = 1.66846 (* 1 = 1.66846 loss)
I0520 21:50:11.388433 31157 sgd_solver.cpp:106] Iteration 2156, lr = 0.0025
I0520 21:50:19.514458 31157 solver.cpp:237] Iteration 2200, loss = 1.59868
I0520 21:50:19.514492 31157 solver.cpp:253]     Train net output #0: loss = 1.59868 (* 1 = 1.59868 loss)
I0520 21:50:19.514508 31157 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0520 21:50:20.253744 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_2205.caffemodel
I0520 21:50:20.425319 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_2205.solverstate
I0520 21:50:27.724098 31157 solver.cpp:237] Iteration 2244, loss = 1.65994
I0520 21:50:27.724261 31157 solver.cpp:253]     Train net output #0: loss = 1.65994 (* 1 = 1.65994 loss)
I0520 21:50:27.724275 31157 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0520 21:50:35.848278 31157 solver.cpp:237] Iteration 2288, loss = 1.59731
I0520 21:50:35.848310 31157 solver.cpp:253]     Train net output #0: loss = 1.59731 (* 1 = 1.59731 loss)
I0520 21:50:35.848325 31157 sgd_solver.cpp:106] Iteration 2288, lr = 0.0025
I0520 21:50:43.976337 31157 solver.cpp:237] Iteration 2332, loss = 1.65301
I0520 21:50:43.976371 31157 solver.cpp:253]     Train net output #0: loss = 1.65301 (* 1 = 1.65301 loss)
I0520 21:50:43.976387 31157 sgd_solver.cpp:106] Iteration 2332, lr = 0.0025
I0520 21:51:14.292471 31157 solver.cpp:237] Iteration 2376, loss = 1.62669
I0520 21:51:14.292639 31157 solver.cpp:253]     Train net output #0: loss = 1.62669 (* 1 = 1.62669 loss)
I0520 21:51:14.292655 31157 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0520 21:51:22.427265 31157 solver.cpp:237] Iteration 2420, loss = 1.68992
I0520 21:51:22.427312 31157 solver.cpp:253]     Train net output #0: loss = 1.68992 (* 1 = 1.68992 loss)
I0520 21:51:22.427330 31157 sgd_solver.cpp:106] Iteration 2420, lr = 0.0025
I0520 21:51:30.555202 31157 solver.cpp:237] Iteration 2464, loss = 1.63139
I0520 21:51:30.555235 31157 solver.cpp:253]     Train net output #0: loss = 1.63139 (* 1 = 1.63139 loss)
I0520 21:51:30.555253 31157 sgd_solver.cpp:106] Iteration 2464, lr = 0.0025
I0520 21:51:38.687968 31157 solver.cpp:237] Iteration 2508, loss = 1.50549
I0520 21:51:38.688000 31157 solver.cpp:253]     Train net output #0: loss = 1.50549 (* 1 = 1.50549 loss)
I0520 21:51:38.688019 31157 sgd_solver.cpp:106] Iteration 2508, lr = 0.0025
I0520 21:51:46.821293 31157 solver.cpp:237] Iteration 2552, loss = 1.68926
I0520 21:51:46.821451 31157 solver.cpp:253]     Train net output #0: loss = 1.68926 (* 1 = 1.68926 loss)
I0520 21:51:46.821465 31157 sgd_solver.cpp:106] Iteration 2552, lr = 0.0025
I0520 21:51:54.946246 31157 solver.cpp:237] Iteration 2596, loss = 1.67621
I0520 21:51:54.946280 31157 solver.cpp:253]     Train net output #0: loss = 1.67621 (* 1 = 1.67621 loss)
I0520 21:51:54.946296 31157 sgd_solver.cpp:106] Iteration 2596, lr = 0.0025
I0520 21:52:03.082675 31157 solver.cpp:237] Iteration 2640, loss = 1.52881
I0520 21:52:03.082708 31157 solver.cpp:253]     Train net output #0: loss = 1.52881 (* 1 = 1.52881 loss)
I0520 21:52:03.082725 31157 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0520 21:52:04.006901 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_2646.caffemodel
I0520 21:52:04.175309 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_2646.solverstate
I0520 21:52:04.201997 31157 solver.cpp:341] Iteration 2646, Testing net (#0)
I0520 21:52:49.213245 31157 solver.cpp:409]     Test net output #0: accuracy = 0.683366
I0520 21:52:49.213407 31157 solver.cpp:409]     Test net output #1: loss = 1.05265 (* 1 = 1.05265 loss)
I0520 21:53:18.440839 31157 solver.cpp:237] Iteration 2684, loss = 1.65763
I0520 21:53:18.440888 31157 solver.cpp:253]     Train net output #0: loss = 1.65763 (* 1 = 1.65763 loss)
I0520 21:53:18.440901 31157 sgd_solver.cpp:106] Iteration 2684, lr = 0.0025
I0520 21:53:26.583032 31157 solver.cpp:237] Iteration 2728, loss = 1.67482
I0520 21:53:26.583204 31157 solver.cpp:253]     Train net output #0: loss = 1.67482 (* 1 = 1.67482 loss)
I0520 21:53:26.583217 31157 sgd_solver.cpp:106] Iteration 2728, lr = 0.0025
I0520 21:53:34.726819 31157 solver.cpp:237] Iteration 2772, loss = 1.64645
I0520 21:53:34.726850 31157 solver.cpp:253]     Train net output #0: loss = 1.64645 (* 1 = 1.64645 loss)
I0520 21:53:34.726868 31157 sgd_solver.cpp:106] Iteration 2772, lr = 0.0025
I0520 21:53:42.867386 31157 solver.cpp:237] Iteration 2816, loss = 1.61533
I0520 21:53:42.867420 31157 solver.cpp:253]     Train net output #0: loss = 1.61533 (* 1 = 1.61533 loss)
I0520 21:53:42.867437 31157 sgd_solver.cpp:106] Iteration 2816, lr = 0.0025
I0520 21:53:51.013595 31157 solver.cpp:237] Iteration 2860, loss = 1.63923
I0520 21:53:51.013644 31157 solver.cpp:253]     Train net output #0: loss = 1.63923 (* 1 = 1.63923 loss)
I0520 21:53:51.013659 31157 sgd_solver.cpp:106] Iteration 2860, lr = 0.0025
I0520 21:53:59.153777 31157 solver.cpp:237] Iteration 2904, loss = 1.61498
I0520 21:53:59.153930 31157 solver.cpp:253]     Train net output #0: loss = 1.61498 (* 1 = 1.61498 loss)
I0520 21:53:59.153944 31157 sgd_solver.cpp:106] Iteration 2904, lr = 0.0025
I0520 21:54:29.431306 31157 solver.cpp:237] Iteration 2948, loss = 1.54658
I0520 21:54:29.431476 31157 solver.cpp:253]     Train net output #0: loss = 1.54658 (* 1 = 1.54658 loss)
I0520 21:54:29.431491 31157 sgd_solver.cpp:106] Iteration 2948, lr = 0.0025
I0520 21:54:37.577857 31157 solver.cpp:237] Iteration 2992, loss = 1.53842
I0520 21:54:37.577898 31157 solver.cpp:253]     Train net output #0: loss = 1.53842 (* 1 = 1.53842 loss)
I0520 21:54:37.577915 31157 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0520 21:54:45.718868 31157 solver.cpp:237] Iteration 3036, loss = 1.71892
I0520 21:54:45.718900 31157 solver.cpp:253]     Train net output #0: loss = 1.71892 (* 1 = 1.71892 loss)
I0520 21:54:45.718914 31157 sgd_solver.cpp:106] Iteration 3036, lr = 0.0025
I0520 21:54:53.862771 31157 solver.cpp:237] Iteration 3080, loss = 1.56434
I0520 21:54:53.862804 31157 solver.cpp:253]     Train net output #0: loss = 1.56434 (* 1 = 1.56434 loss)
I0520 21:54:53.862820 31157 sgd_solver.cpp:106] Iteration 3080, lr = 0.0025
I0520 21:54:54.972091 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_3087.caffemodel
I0520 21:54:55.139657 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_3087.solverstate
I0520 21:55:02.069739 31157 solver.cpp:237] Iteration 3124, loss = 1.64461
I0520 21:55:02.069900 31157 solver.cpp:253]     Train net output #0: loss = 1.64461 (* 1 = 1.64461 loss)
I0520 21:55:02.069913 31157 sgd_solver.cpp:106] Iteration 3124, lr = 0.0025
I0520 21:55:10.210374 31157 solver.cpp:237] Iteration 3168, loss = 1.64039
I0520 21:55:10.210407 31157 solver.cpp:253]     Train net output #0: loss = 1.64039 (* 1 = 1.64039 loss)
I0520 21:55:10.210424 31157 sgd_solver.cpp:106] Iteration 3168, lr = 0.0025
I0520 21:55:18.352957 31157 solver.cpp:237] Iteration 3212, loss = 1.60812
I0520 21:55:18.352990 31157 solver.cpp:253]     Train net output #0: loss = 1.60812 (* 1 = 1.60812 loss)
I0520 21:55:18.353006 31157 sgd_solver.cpp:106] Iteration 3212, lr = 0.0025
I0520 21:55:48.675945 31157 solver.cpp:237] Iteration 3256, loss = 1.65946
I0520 21:55:48.676110 31157 solver.cpp:253]     Train net output #0: loss = 1.65946 (* 1 = 1.65946 loss)
I0520 21:55:48.676126 31157 sgd_solver.cpp:106] Iteration 3256, lr = 0.0025
I0520 21:55:56.821298 31157 solver.cpp:237] Iteration 3300, loss = 1.59457
I0520 21:55:56.821343 31157 solver.cpp:253]     Train net output #0: loss = 1.59457 (* 1 = 1.59457 loss)
I0520 21:55:56.821360 31157 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0520 21:56:04.966156 31157 solver.cpp:237] Iteration 3344, loss = 1.63956
I0520 21:56:04.966190 31157 solver.cpp:253]     Train net output #0: loss = 1.63956 (* 1 = 1.63956 loss)
I0520 21:56:04.966207 31157 sgd_solver.cpp:106] Iteration 3344, lr = 0.0025
I0520 21:56:13.107846 31157 solver.cpp:237] Iteration 3388, loss = 1.50837
I0520 21:56:13.107878 31157 solver.cpp:253]     Train net output #0: loss = 1.50837 (* 1 = 1.50837 loss)
I0520 21:56:13.107894 31157 sgd_solver.cpp:106] Iteration 3388, lr = 0.0025
I0520 21:56:21.251691 31157 solver.cpp:237] Iteration 3432, loss = 1.60191
I0520 21:56:21.251844 31157 solver.cpp:253]     Train net output #0: loss = 1.60191 (* 1 = 1.60191 loss)
I0520 21:56:21.251858 31157 sgd_solver.cpp:106] Iteration 3432, lr = 0.0025
I0520 21:56:29.390254 31157 solver.cpp:237] Iteration 3476, loss = 1.65885
I0520 21:56:29.390287 31157 solver.cpp:253]     Train net output #0: loss = 1.65885 (* 1 = 1.65885 loss)
I0520 21:56:29.390305 31157 sgd_solver.cpp:106] Iteration 3476, lr = 0.0025
I0520 21:56:37.527979 31157 solver.cpp:237] Iteration 3520, loss = 1.45666
I0520 21:56:37.528012 31157 solver.cpp:253]     Train net output #0: loss = 1.45666 (* 1 = 1.45666 loss)
I0520 21:56:37.528028 31157 sgd_solver.cpp:106] Iteration 3520, lr = 0.0025
I0520 21:56:38.824247 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_3528.caffemodel
I0520 21:56:38.993505 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_3528.solverstate
I0520 21:56:39.020072 31157 solver.cpp:341] Iteration 3528, Testing net (#0)
I0520 21:57:45.131980 31157 solver.cpp:409]     Test net output #0: accuracy = 0.707569
I0520 21:57:45.132166 31157 solver.cpp:409]     Test net output #1: loss = 0.977973 (* 1 = 0.977973 loss)
I0520 21:58:13.929936 31157 solver.cpp:237] Iteration 3564, loss = 1.71713
I0520 21:58:13.929986 31157 solver.cpp:253]     Train net output #0: loss = 1.71713 (* 1 = 1.71713 loss)
I0520 21:58:13.930001 31157 sgd_solver.cpp:106] Iteration 3564, lr = 0.0025
I0520 21:58:22.057611 31157 solver.cpp:237] Iteration 3608, loss = 1.65775
I0520 21:58:22.057762 31157 solver.cpp:253]     Train net output #0: loss = 1.65775 (* 1 = 1.65775 loss)
I0520 21:58:22.057776 31157 sgd_solver.cpp:106] Iteration 3608, lr = 0.0025
I0520 21:58:30.188340 31157 solver.cpp:237] Iteration 3652, loss = 1.56598
I0520 21:58:30.188374 31157 solver.cpp:253]     Train net output #0: loss = 1.56598 (* 1 = 1.56598 loss)
I0520 21:58:30.188388 31157 sgd_solver.cpp:106] Iteration 3652, lr = 0.0025
I0520 21:58:38.322899 31157 solver.cpp:237] Iteration 3696, loss = 1.586
I0520 21:58:38.322933 31157 solver.cpp:253]     Train net output #0: loss = 1.586 (* 1 = 1.586 loss)
I0520 21:58:38.322949 31157 sgd_solver.cpp:106] Iteration 3696, lr = 0.0025
I0520 21:58:46.450525 31157 solver.cpp:237] Iteration 3740, loss = 1.59068
I0520 21:58:46.450558 31157 solver.cpp:253]     Train net output #0: loss = 1.59068 (* 1 = 1.59068 loss)
I0520 21:58:46.450575 31157 sgd_solver.cpp:106] Iteration 3740, lr = 0.0025
I0520 21:58:54.574708 31157 solver.cpp:237] Iteration 3784, loss = 1.59426
I0520 21:58:54.574856 31157 solver.cpp:253]     Train net output #0: loss = 1.59426 (* 1 = 1.59426 loss)
I0520 21:58:54.574870 31157 sgd_solver.cpp:106] Iteration 3784, lr = 0.0025
I0520 21:59:24.952625 31157 solver.cpp:237] Iteration 3828, loss = 1.54467
I0520 21:59:24.952793 31157 solver.cpp:253]     Train net output #0: loss = 1.54467 (* 1 = 1.54467 loss)
I0520 21:59:24.952810 31157 sgd_solver.cpp:106] Iteration 3828, lr = 0.0025
I0520 21:59:33.085681 31157 solver.cpp:237] Iteration 3872, loss = 1.49689
I0520 21:59:33.085714 31157 solver.cpp:253]     Train net output #0: loss = 1.49689 (* 1 = 1.49689 loss)
I0520 21:59:33.085731 31157 sgd_solver.cpp:106] Iteration 3872, lr = 0.0025
I0520 21:59:41.215962 31157 solver.cpp:237] Iteration 3916, loss = 1.74684
I0520 21:59:41.216002 31157 solver.cpp:253]     Train net output #0: loss = 1.74684 (* 1 = 1.74684 loss)
I0520 21:59:41.216019 31157 sgd_solver.cpp:106] Iteration 3916, lr = 0.0025
I0520 21:59:49.347584 31157 solver.cpp:237] Iteration 3960, loss = 1.58229
I0520 21:59:49.347618 31157 solver.cpp:253]     Train net output #0: loss = 1.58229 (* 1 = 1.58229 loss)
I0520 21:59:49.347635 31157 sgd_solver.cpp:106] Iteration 3960, lr = 0.0025
I0520 21:59:50.824038 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_3969.caffemodel
I0520 21:59:51.005015 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_3969.solverstate
I0520 21:59:57.565414 31157 solver.cpp:237] Iteration 4004, loss = 1.58903
I0520 21:59:57.565578 31157 solver.cpp:253]     Train net output #0: loss = 1.58903 (* 1 = 1.58903 loss)
I0520 21:59:57.565593 31157 sgd_solver.cpp:106] Iteration 4004, lr = 0.0025
I0520 22:00:05.694370 31157 solver.cpp:237] Iteration 4048, loss = 1.51951
I0520 22:00:05.694417 31157 solver.cpp:253]     Train net output #0: loss = 1.51951 (* 1 = 1.51951 loss)
I0520 22:00:05.694434 31157 sgd_solver.cpp:106] Iteration 4048, lr = 0.0025
I0520 22:00:13.817566 31157 solver.cpp:237] Iteration 4092, loss = 1.57691
I0520 22:00:13.817598 31157 solver.cpp:253]     Train net output #0: loss = 1.57691 (* 1 = 1.57691 loss)
I0520 22:00:13.817615 31157 sgd_solver.cpp:106] Iteration 4092, lr = 0.0025
I0520 22:00:44.131841 31157 solver.cpp:237] Iteration 4136, loss = 1.60653
I0520 22:00:44.132014 31157 solver.cpp:253]     Train net output #0: loss = 1.60653 (* 1 = 1.60653 loss)
I0520 22:00:44.132028 31157 sgd_solver.cpp:106] Iteration 4136, lr = 0.0025
I0520 22:00:52.259671 31157 solver.cpp:237] Iteration 4180, loss = 1.54412
I0520 22:00:52.259716 31157 solver.cpp:253]     Train net output #0: loss = 1.54412 (* 1 = 1.54412 loss)
I0520 22:00:52.259732 31157 sgd_solver.cpp:106] Iteration 4180, lr = 0.0025
I0520 22:01:00.393396 31157 solver.cpp:237] Iteration 4224, loss = 1.48502
I0520 22:01:00.393429 31157 solver.cpp:253]     Train net output #0: loss = 1.48502 (* 1 = 1.48502 loss)
I0520 22:01:00.393446 31157 sgd_solver.cpp:106] Iteration 4224, lr = 0.0025
I0520 22:01:08.523931 31157 solver.cpp:237] Iteration 4268, loss = 1.50209
I0520 22:01:08.523965 31157 solver.cpp:253]     Train net output #0: loss = 1.50209 (* 1 = 1.50209 loss)
I0520 22:01:08.523982 31157 sgd_solver.cpp:106] Iteration 4268, lr = 0.0025
I0520 22:01:16.658102 31157 solver.cpp:237] Iteration 4312, loss = 1.50039
I0520 22:01:16.658257 31157 solver.cpp:253]     Train net output #0: loss = 1.50039 (* 1 = 1.50039 loss)
I0520 22:01:16.658270 31157 sgd_solver.cpp:106] Iteration 4312, lr = 0.0025
I0520 22:01:24.791473 31157 solver.cpp:237] Iteration 4356, loss = 1.48657
I0520 22:01:24.791507 31157 solver.cpp:253]     Train net output #0: loss = 1.48657 (* 1 = 1.48657 loss)
I0520 22:01:24.791525 31157 sgd_solver.cpp:106] Iteration 4356, lr = 0.0025
I0520 22:01:32.924804 31157 solver.cpp:237] Iteration 4400, loss = 1.63278
I0520 22:01:32.924839 31157 solver.cpp:253]     Train net output #0: loss = 1.63278 (* 1 = 1.63278 loss)
I0520 22:01:32.924854 31157 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0520 22:01:34.589885 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_4410.caffemodel
I0520 22:01:34.766160 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_4410.solverstate
I0520 22:01:34.802839 31157 solver.cpp:341] Iteration 4410, Testing net (#0)
I0520 22:02:20.123953 31157 solver.cpp:409]     Test net output #0: accuracy = 0.728411
I0520 22:02:20.124119 31157 solver.cpp:409]     Test net output #1: loss = 0.923568 (* 1 = 0.923568 loss)
I0520 22:02:20.179703 31157 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_4411.caffemodel
I0520 22:02:20.354467 31157 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022_iter_4411.solverstate
I0520 22:02:20.391479 31157 solver.cpp:326] Optimization Done.
I0520 22:02:20.391506 31157 caffe.cpp:215] Optimization Done.
Application 11235531 resources: utime ~1260s, stime ~226s, Rss ~5329176, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_340_2016-05-20T11.20.45.113022.solver"
	User time (seconds): 0.54
	System time (seconds): 0.13
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:49.96
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15080
	Voluntary context switches: 2919
	Involuntary context switches: 188
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
