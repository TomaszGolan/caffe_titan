2806431
I0521 10:47:43.506222 19762 caffe.cpp:184] Using GPUs 0
I0521 10:47:43.930723 19762 solver.cpp:48] Initializing solver from parameters: 
test_iter: 154
test_interval: 309
base_lr: 0.0025
display: 15
max_iter: 1546
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 154
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487.prototxt"
I0521 10:47:43.932299 19762 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487.prototxt
I0521 10:47:43.947422 19762 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 10:47:43.947482 19762 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 10:47:43.947835 19762 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 970
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 10:47:43.948017 19762 layer_factory.hpp:77] Creating layer data_hdf5
I0521 10:47:43.948041 19762 net.cpp:106] Creating Layer data_hdf5
I0521 10:47:43.948056 19762 net.cpp:411] data_hdf5 -> data
I0521 10:47:43.948089 19762 net.cpp:411] data_hdf5 -> label
I0521 10:47:43.948123 19762 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 10:47:43.949512 19762 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 10:47:43.951746 19762 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 10:48:05.499980 19762 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 10:48:05.505091 19762 net.cpp:150] Setting up data_hdf5
I0521 10:48:05.505132 19762 net.cpp:157] Top shape: 970 1 127 50 (6159500)
I0521 10:48:05.505147 19762 net.cpp:157] Top shape: 970 (970)
I0521 10:48:05.505159 19762 net.cpp:165] Memory required for data: 24641880
I0521 10:48:05.505172 19762 layer_factory.hpp:77] Creating layer conv1
I0521 10:48:05.505208 19762 net.cpp:106] Creating Layer conv1
I0521 10:48:05.505218 19762 net.cpp:454] conv1 <- data
I0521 10:48:05.505240 19762 net.cpp:411] conv1 -> conv1
I0521 10:48:05.871984 19762 net.cpp:150] Setting up conv1
I0521 10:48:05.872030 19762 net.cpp:157] Top shape: 970 12 120 48 (67046400)
I0521 10:48:05.872042 19762 net.cpp:165] Memory required for data: 292827480
I0521 10:48:05.872072 19762 layer_factory.hpp:77] Creating layer relu1
I0521 10:48:05.872094 19762 net.cpp:106] Creating Layer relu1
I0521 10:48:05.872105 19762 net.cpp:454] relu1 <- conv1
I0521 10:48:05.872119 19762 net.cpp:397] relu1 -> conv1 (in-place)
I0521 10:48:05.872639 19762 net.cpp:150] Setting up relu1
I0521 10:48:05.872656 19762 net.cpp:157] Top shape: 970 12 120 48 (67046400)
I0521 10:48:05.872666 19762 net.cpp:165] Memory required for data: 561013080
I0521 10:48:05.872678 19762 layer_factory.hpp:77] Creating layer pool1
I0521 10:48:05.872694 19762 net.cpp:106] Creating Layer pool1
I0521 10:48:05.872704 19762 net.cpp:454] pool1 <- conv1
I0521 10:48:05.872716 19762 net.cpp:411] pool1 -> pool1
I0521 10:48:05.872797 19762 net.cpp:150] Setting up pool1
I0521 10:48:05.872812 19762 net.cpp:157] Top shape: 970 12 60 48 (33523200)
I0521 10:48:05.872822 19762 net.cpp:165] Memory required for data: 695105880
I0521 10:48:05.872833 19762 layer_factory.hpp:77] Creating layer conv2
I0521 10:48:05.872855 19762 net.cpp:106] Creating Layer conv2
I0521 10:48:05.872865 19762 net.cpp:454] conv2 <- pool1
I0521 10:48:05.872879 19762 net.cpp:411] conv2 -> conv2
I0521 10:48:05.875542 19762 net.cpp:150] Setting up conv2
I0521 10:48:05.875571 19762 net.cpp:157] Top shape: 970 20 54 46 (48189600)
I0521 10:48:05.875581 19762 net.cpp:165] Memory required for data: 887864280
I0521 10:48:05.875599 19762 layer_factory.hpp:77] Creating layer relu2
I0521 10:48:05.875614 19762 net.cpp:106] Creating Layer relu2
I0521 10:48:05.875624 19762 net.cpp:454] relu2 <- conv2
I0521 10:48:05.875638 19762 net.cpp:397] relu2 -> conv2 (in-place)
I0521 10:48:05.875974 19762 net.cpp:150] Setting up relu2
I0521 10:48:05.875989 19762 net.cpp:157] Top shape: 970 20 54 46 (48189600)
I0521 10:48:05.875999 19762 net.cpp:165] Memory required for data: 1080622680
I0521 10:48:05.876009 19762 layer_factory.hpp:77] Creating layer pool2
I0521 10:48:05.876021 19762 net.cpp:106] Creating Layer pool2
I0521 10:48:05.876031 19762 net.cpp:454] pool2 <- conv2
I0521 10:48:05.876057 19762 net.cpp:411] pool2 -> pool2
I0521 10:48:05.876126 19762 net.cpp:150] Setting up pool2
I0521 10:48:05.876139 19762 net.cpp:157] Top shape: 970 20 27 46 (24094800)
I0521 10:48:05.876149 19762 net.cpp:165] Memory required for data: 1177001880
I0521 10:48:05.876157 19762 layer_factory.hpp:77] Creating layer conv3
I0521 10:48:05.876175 19762 net.cpp:106] Creating Layer conv3
I0521 10:48:05.876185 19762 net.cpp:454] conv3 <- pool2
I0521 10:48:05.876199 19762 net.cpp:411] conv3 -> conv3
I0521 10:48:05.878110 19762 net.cpp:150] Setting up conv3
I0521 10:48:05.878129 19762 net.cpp:157] Top shape: 970 28 22 44 (26290880)
I0521 10:48:05.878144 19762 net.cpp:165] Memory required for data: 1282165400
I0521 10:48:05.878161 19762 layer_factory.hpp:77] Creating layer relu3
I0521 10:48:05.878177 19762 net.cpp:106] Creating Layer relu3
I0521 10:48:05.878187 19762 net.cpp:454] relu3 <- conv3
I0521 10:48:05.878201 19762 net.cpp:397] relu3 -> conv3 (in-place)
I0521 10:48:05.878670 19762 net.cpp:150] Setting up relu3
I0521 10:48:05.878689 19762 net.cpp:157] Top shape: 970 28 22 44 (26290880)
I0521 10:48:05.878698 19762 net.cpp:165] Memory required for data: 1387328920
I0521 10:48:05.878710 19762 layer_factory.hpp:77] Creating layer pool3
I0521 10:48:05.878722 19762 net.cpp:106] Creating Layer pool3
I0521 10:48:05.878732 19762 net.cpp:454] pool3 <- conv3
I0521 10:48:05.878746 19762 net.cpp:411] pool3 -> pool3
I0521 10:48:05.878813 19762 net.cpp:150] Setting up pool3
I0521 10:48:05.878825 19762 net.cpp:157] Top shape: 970 28 11 44 (13145440)
I0521 10:48:05.878836 19762 net.cpp:165] Memory required for data: 1439910680
I0521 10:48:05.878844 19762 layer_factory.hpp:77] Creating layer conv4
I0521 10:48:05.878861 19762 net.cpp:106] Creating Layer conv4
I0521 10:48:05.878872 19762 net.cpp:454] conv4 <- pool3
I0521 10:48:05.878885 19762 net.cpp:411] conv4 -> conv4
I0521 10:48:05.881623 19762 net.cpp:150] Setting up conv4
I0521 10:48:05.881645 19762 net.cpp:157] Top shape: 970 36 6 42 (8799840)
I0521 10:48:05.881656 19762 net.cpp:165] Memory required for data: 1475110040
I0521 10:48:05.881674 19762 layer_factory.hpp:77] Creating layer relu4
I0521 10:48:05.881687 19762 net.cpp:106] Creating Layer relu4
I0521 10:48:05.881697 19762 net.cpp:454] relu4 <- conv4
I0521 10:48:05.881711 19762 net.cpp:397] relu4 -> conv4 (in-place)
I0521 10:48:05.882174 19762 net.cpp:150] Setting up relu4
I0521 10:48:05.882189 19762 net.cpp:157] Top shape: 970 36 6 42 (8799840)
I0521 10:48:05.882200 19762 net.cpp:165] Memory required for data: 1510309400
I0521 10:48:05.882210 19762 layer_factory.hpp:77] Creating layer pool4
I0521 10:48:05.882225 19762 net.cpp:106] Creating Layer pool4
I0521 10:48:05.882235 19762 net.cpp:454] pool4 <- conv4
I0521 10:48:05.882247 19762 net.cpp:411] pool4 -> pool4
I0521 10:48:05.882315 19762 net.cpp:150] Setting up pool4
I0521 10:48:05.882328 19762 net.cpp:157] Top shape: 970 36 3 42 (4399920)
I0521 10:48:05.882339 19762 net.cpp:165] Memory required for data: 1527909080
I0521 10:48:05.882349 19762 layer_factory.hpp:77] Creating layer ip1
I0521 10:48:05.882370 19762 net.cpp:106] Creating Layer ip1
I0521 10:48:05.882380 19762 net.cpp:454] ip1 <- pool4
I0521 10:48:05.882395 19762 net.cpp:411] ip1 -> ip1
I0521 10:48:05.897815 19762 net.cpp:150] Setting up ip1
I0521 10:48:05.897840 19762 net.cpp:157] Top shape: 970 196 (190120)
I0521 10:48:05.897852 19762 net.cpp:165] Memory required for data: 1528669560
I0521 10:48:05.897879 19762 layer_factory.hpp:77] Creating layer relu5
I0521 10:48:05.897894 19762 net.cpp:106] Creating Layer relu5
I0521 10:48:05.897905 19762 net.cpp:454] relu5 <- ip1
I0521 10:48:05.897919 19762 net.cpp:397] relu5 -> ip1 (in-place)
I0521 10:48:05.898262 19762 net.cpp:150] Setting up relu5
I0521 10:48:05.898275 19762 net.cpp:157] Top shape: 970 196 (190120)
I0521 10:48:05.898286 19762 net.cpp:165] Memory required for data: 1529430040
I0521 10:48:05.898296 19762 layer_factory.hpp:77] Creating layer drop1
I0521 10:48:05.898319 19762 net.cpp:106] Creating Layer drop1
I0521 10:48:05.898329 19762 net.cpp:454] drop1 <- ip1
I0521 10:48:05.898355 19762 net.cpp:397] drop1 -> ip1 (in-place)
I0521 10:48:05.898401 19762 net.cpp:150] Setting up drop1
I0521 10:48:05.898413 19762 net.cpp:157] Top shape: 970 196 (190120)
I0521 10:48:05.898424 19762 net.cpp:165] Memory required for data: 1530190520
I0521 10:48:05.898434 19762 layer_factory.hpp:77] Creating layer ip2
I0521 10:48:05.898453 19762 net.cpp:106] Creating Layer ip2
I0521 10:48:05.898464 19762 net.cpp:454] ip2 <- ip1
I0521 10:48:05.898478 19762 net.cpp:411] ip2 -> ip2
I0521 10:48:05.898973 19762 net.cpp:150] Setting up ip2
I0521 10:48:05.898986 19762 net.cpp:157] Top shape: 970 98 (95060)
I0521 10:48:05.898998 19762 net.cpp:165] Memory required for data: 1530570760
I0521 10:48:05.899013 19762 layer_factory.hpp:77] Creating layer relu6
I0521 10:48:05.899024 19762 net.cpp:106] Creating Layer relu6
I0521 10:48:05.899034 19762 net.cpp:454] relu6 <- ip2
I0521 10:48:05.899046 19762 net.cpp:397] relu6 -> ip2 (in-place)
I0521 10:48:05.899567 19762 net.cpp:150] Setting up relu6
I0521 10:48:05.899583 19762 net.cpp:157] Top shape: 970 98 (95060)
I0521 10:48:05.899595 19762 net.cpp:165] Memory required for data: 1530951000
I0521 10:48:05.899605 19762 layer_factory.hpp:77] Creating layer drop2
I0521 10:48:05.899617 19762 net.cpp:106] Creating Layer drop2
I0521 10:48:05.899627 19762 net.cpp:454] drop2 <- ip2
I0521 10:48:05.899641 19762 net.cpp:397] drop2 -> ip2 (in-place)
I0521 10:48:05.899682 19762 net.cpp:150] Setting up drop2
I0521 10:48:05.899694 19762 net.cpp:157] Top shape: 970 98 (95060)
I0521 10:48:05.899705 19762 net.cpp:165] Memory required for data: 1531331240
I0521 10:48:05.899715 19762 layer_factory.hpp:77] Creating layer ip3
I0521 10:48:05.899729 19762 net.cpp:106] Creating Layer ip3
I0521 10:48:05.899739 19762 net.cpp:454] ip3 <- ip2
I0521 10:48:05.899750 19762 net.cpp:411] ip3 -> ip3
I0521 10:48:05.899969 19762 net.cpp:150] Setting up ip3
I0521 10:48:05.899982 19762 net.cpp:157] Top shape: 970 11 (10670)
I0521 10:48:05.899992 19762 net.cpp:165] Memory required for data: 1531373920
I0521 10:48:05.900007 19762 layer_factory.hpp:77] Creating layer drop3
I0521 10:48:05.900020 19762 net.cpp:106] Creating Layer drop3
I0521 10:48:05.900030 19762 net.cpp:454] drop3 <- ip3
I0521 10:48:05.900043 19762 net.cpp:397] drop3 -> ip3 (in-place)
I0521 10:48:05.900081 19762 net.cpp:150] Setting up drop3
I0521 10:48:05.900094 19762 net.cpp:157] Top shape: 970 11 (10670)
I0521 10:48:05.900105 19762 net.cpp:165] Memory required for data: 1531416600
I0521 10:48:05.900115 19762 layer_factory.hpp:77] Creating layer loss
I0521 10:48:05.900135 19762 net.cpp:106] Creating Layer loss
I0521 10:48:05.900144 19762 net.cpp:454] loss <- ip3
I0521 10:48:05.900156 19762 net.cpp:454] loss <- label
I0521 10:48:05.900168 19762 net.cpp:411] loss -> loss
I0521 10:48:05.900185 19762 layer_factory.hpp:77] Creating layer loss
I0521 10:48:05.900840 19762 net.cpp:150] Setting up loss
I0521 10:48:05.900862 19762 net.cpp:157] Top shape: (1)
I0521 10:48:05.900874 19762 net.cpp:160]     with loss weight 1
I0521 10:48:05.900919 19762 net.cpp:165] Memory required for data: 1531416604
I0521 10:48:05.900930 19762 net.cpp:226] loss needs backward computation.
I0521 10:48:05.900941 19762 net.cpp:226] drop3 needs backward computation.
I0521 10:48:05.900950 19762 net.cpp:226] ip3 needs backward computation.
I0521 10:48:05.900961 19762 net.cpp:226] drop2 needs backward computation.
I0521 10:48:05.900971 19762 net.cpp:226] relu6 needs backward computation.
I0521 10:48:05.900980 19762 net.cpp:226] ip2 needs backward computation.
I0521 10:48:05.900991 19762 net.cpp:226] drop1 needs backward computation.
I0521 10:48:05.901000 19762 net.cpp:226] relu5 needs backward computation.
I0521 10:48:05.901010 19762 net.cpp:226] ip1 needs backward computation.
I0521 10:48:05.901021 19762 net.cpp:226] pool4 needs backward computation.
I0521 10:48:05.901031 19762 net.cpp:226] relu4 needs backward computation.
I0521 10:48:05.901041 19762 net.cpp:226] conv4 needs backward computation.
I0521 10:48:05.901051 19762 net.cpp:226] pool3 needs backward computation.
I0521 10:48:05.901069 19762 net.cpp:226] relu3 needs backward computation.
I0521 10:48:05.901079 19762 net.cpp:226] conv3 needs backward computation.
I0521 10:48:05.901089 19762 net.cpp:226] pool2 needs backward computation.
I0521 10:48:05.901100 19762 net.cpp:226] relu2 needs backward computation.
I0521 10:48:05.901110 19762 net.cpp:226] conv2 needs backward computation.
I0521 10:48:05.901121 19762 net.cpp:226] pool1 needs backward computation.
I0521 10:48:05.901131 19762 net.cpp:226] relu1 needs backward computation.
I0521 10:48:05.901141 19762 net.cpp:226] conv1 needs backward computation.
I0521 10:48:05.901154 19762 net.cpp:228] data_hdf5 does not need backward computation.
I0521 10:48:05.901162 19762 net.cpp:270] This network produces output loss
I0521 10:48:05.901186 19762 net.cpp:283] Network initialization done.
I0521 10:48:05.902820 19762 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487.prototxt
I0521 10:48:05.902890 19762 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 10:48:05.903246 19762 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 970
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 10:48:05.903434 19762 layer_factory.hpp:77] Creating layer data_hdf5
I0521 10:48:05.903450 19762 net.cpp:106] Creating Layer data_hdf5
I0521 10:48:05.903460 19762 net.cpp:411] data_hdf5 -> data
I0521 10:48:05.903477 19762 net.cpp:411] data_hdf5 -> label
I0521 10:48:05.903493 19762 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 10:48:05.904809 19762 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 10:48:27.249950 19762 net.cpp:150] Setting up data_hdf5
I0521 10:48:27.250119 19762 net.cpp:157] Top shape: 970 1 127 50 (6159500)
I0521 10:48:27.250134 19762 net.cpp:157] Top shape: 970 (970)
I0521 10:48:27.250145 19762 net.cpp:165] Memory required for data: 24641880
I0521 10:48:27.250159 19762 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 10:48:27.250188 19762 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 10:48:27.250200 19762 net.cpp:454] label_data_hdf5_1_split <- label
I0521 10:48:27.250214 19762 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 10:48:27.250236 19762 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 10:48:27.250309 19762 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 10:48:27.250324 19762 net.cpp:157] Top shape: 970 (970)
I0521 10:48:27.250335 19762 net.cpp:157] Top shape: 970 (970)
I0521 10:48:27.250344 19762 net.cpp:165] Memory required for data: 24649640
I0521 10:48:27.250355 19762 layer_factory.hpp:77] Creating layer conv1
I0521 10:48:27.250375 19762 net.cpp:106] Creating Layer conv1
I0521 10:48:27.250386 19762 net.cpp:454] conv1 <- data
I0521 10:48:27.250401 19762 net.cpp:411] conv1 -> conv1
I0521 10:48:27.252349 19762 net.cpp:150] Setting up conv1
I0521 10:48:27.252373 19762 net.cpp:157] Top shape: 970 12 120 48 (67046400)
I0521 10:48:27.252385 19762 net.cpp:165] Memory required for data: 292835240
I0521 10:48:27.252405 19762 layer_factory.hpp:77] Creating layer relu1
I0521 10:48:27.252420 19762 net.cpp:106] Creating Layer relu1
I0521 10:48:27.252430 19762 net.cpp:454] relu1 <- conv1
I0521 10:48:27.252444 19762 net.cpp:397] relu1 -> conv1 (in-place)
I0521 10:48:27.252941 19762 net.cpp:150] Setting up relu1
I0521 10:48:27.252959 19762 net.cpp:157] Top shape: 970 12 120 48 (67046400)
I0521 10:48:27.252969 19762 net.cpp:165] Memory required for data: 561020840
I0521 10:48:27.252979 19762 layer_factory.hpp:77] Creating layer pool1
I0521 10:48:27.252995 19762 net.cpp:106] Creating Layer pool1
I0521 10:48:27.253005 19762 net.cpp:454] pool1 <- conv1
I0521 10:48:27.253017 19762 net.cpp:411] pool1 -> pool1
I0521 10:48:27.253092 19762 net.cpp:150] Setting up pool1
I0521 10:48:27.253105 19762 net.cpp:157] Top shape: 970 12 60 48 (33523200)
I0521 10:48:27.253114 19762 net.cpp:165] Memory required for data: 695113640
I0521 10:48:27.253125 19762 layer_factory.hpp:77] Creating layer conv2
I0521 10:48:27.253144 19762 net.cpp:106] Creating Layer conv2
I0521 10:48:27.253154 19762 net.cpp:454] conv2 <- pool1
I0521 10:48:27.253167 19762 net.cpp:411] conv2 -> conv2
I0521 10:48:27.255074 19762 net.cpp:150] Setting up conv2
I0521 10:48:27.255096 19762 net.cpp:157] Top shape: 970 20 54 46 (48189600)
I0521 10:48:27.255110 19762 net.cpp:165] Memory required for data: 887872040
I0521 10:48:27.255126 19762 layer_factory.hpp:77] Creating layer relu2
I0521 10:48:27.255141 19762 net.cpp:106] Creating Layer relu2
I0521 10:48:27.255149 19762 net.cpp:454] relu2 <- conv2
I0521 10:48:27.255162 19762 net.cpp:397] relu2 -> conv2 (in-place)
I0521 10:48:27.255496 19762 net.cpp:150] Setting up relu2
I0521 10:48:27.255509 19762 net.cpp:157] Top shape: 970 20 54 46 (48189600)
I0521 10:48:27.255519 19762 net.cpp:165] Memory required for data: 1080630440
I0521 10:48:27.255529 19762 layer_factory.hpp:77] Creating layer pool2
I0521 10:48:27.255542 19762 net.cpp:106] Creating Layer pool2
I0521 10:48:27.255553 19762 net.cpp:454] pool2 <- conv2
I0521 10:48:27.255565 19762 net.cpp:411] pool2 -> pool2
I0521 10:48:27.255636 19762 net.cpp:150] Setting up pool2
I0521 10:48:27.255650 19762 net.cpp:157] Top shape: 970 20 27 46 (24094800)
I0521 10:48:27.255659 19762 net.cpp:165] Memory required for data: 1177009640
I0521 10:48:27.255669 19762 layer_factory.hpp:77] Creating layer conv3
I0521 10:48:27.255686 19762 net.cpp:106] Creating Layer conv3
I0521 10:48:27.255697 19762 net.cpp:454] conv3 <- pool2
I0521 10:48:27.255712 19762 net.cpp:411] conv3 -> conv3
I0521 10:48:27.257690 19762 net.cpp:150] Setting up conv3
I0521 10:48:27.257709 19762 net.cpp:157] Top shape: 970 28 22 44 (26290880)
I0521 10:48:27.257719 19762 net.cpp:165] Memory required for data: 1282173160
I0521 10:48:27.257751 19762 layer_factory.hpp:77] Creating layer relu3
I0521 10:48:27.257766 19762 net.cpp:106] Creating Layer relu3
I0521 10:48:27.257776 19762 net.cpp:454] relu3 <- conv3
I0521 10:48:27.257788 19762 net.cpp:397] relu3 -> conv3 (in-place)
I0521 10:48:27.258261 19762 net.cpp:150] Setting up relu3
I0521 10:48:27.258277 19762 net.cpp:157] Top shape: 970 28 22 44 (26290880)
I0521 10:48:27.258288 19762 net.cpp:165] Memory required for data: 1387336680
I0521 10:48:27.258298 19762 layer_factory.hpp:77] Creating layer pool3
I0521 10:48:27.258311 19762 net.cpp:106] Creating Layer pool3
I0521 10:48:27.258322 19762 net.cpp:454] pool3 <- conv3
I0521 10:48:27.258335 19762 net.cpp:411] pool3 -> pool3
I0521 10:48:27.258406 19762 net.cpp:150] Setting up pool3
I0521 10:48:27.258420 19762 net.cpp:157] Top shape: 970 28 11 44 (13145440)
I0521 10:48:27.258430 19762 net.cpp:165] Memory required for data: 1439918440
I0521 10:48:27.258440 19762 layer_factory.hpp:77] Creating layer conv4
I0521 10:48:27.258458 19762 net.cpp:106] Creating Layer conv4
I0521 10:48:27.258468 19762 net.cpp:454] conv4 <- pool3
I0521 10:48:27.258483 19762 net.cpp:411] conv4 -> conv4
I0521 10:48:27.260546 19762 net.cpp:150] Setting up conv4
I0521 10:48:27.260570 19762 net.cpp:157] Top shape: 970 36 6 42 (8799840)
I0521 10:48:27.260581 19762 net.cpp:165] Memory required for data: 1475117800
I0521 10:48:27.260596 19762 layer_factory.hpp:77] Creating layer relu4
I0521 10:48:27.260610 19762 net.cpp:106] Creating Layer relu4
I0521 10:48:27.260620 19762 net.cpp:454] relu4 <- conv4
I0521 10:48:27.260633 19762 net.cpp:397] relu4 -> conv4 (in-place)
I0521 10:48:27.261103 19762 net.cpp:150] Setting up relu4
I0521 10:48:27.261119 19762 net.cpp:157] Top shape: 970 36 6 42 (8799840)
I0521 10:48:27.261131 19762 net.cpp:165] Memory required for data: 1510317160
I0521 10:48:27.261140 19762 layer_factory.hpp:77] Creating layer pool4
I0521 10:48:27.261153 19762 net.cpp:106] Creating Layer pool4
I0521 10:48:27.261163 19762 net.cpp:454] pool4 <- conv4
I0521 10:48:27.261178 19762 net.cpp:411] pool4 -> pool4
I0521 10:48:27.261248 19762 net.cpp:150] Setting up pool4
I0521 10:48:27.261261 19762 net.cpp:157] Top shape: 970 36 3 42 (4399920)
I0521 10:48:27.261271 19762 net.cpp:165] Memory required for data: 1527916840
I0521 10:48:27.261281 19762 layer_factory.hpp:77] Creating layer ip1
I0521 10:48:27.261297 19762 net.cpp:106] Creating Layer ip1
I0521 10:48:27.261307 19762 net.cpp:454] ip1 <- pool4
I0521 10:48:27.261324 19762 net.cpp:411] ip1 -> ip1
I0521 10:48:27.276816 19762 net.cpp:150] Setting up ip1
I0521 10:48:27.276846 19762 net.cpp:157] Top shape: 970 196 (190120)
I0521 10:48:27.276857 19762 net.cpp:165] Memory required for data: 1528677320
I0521 10:48:27.276880 19762 layer_factory.hpp:77] Creating layer relu5
I0521 10:48:27.276895 19762 net.cpp:106] Creating Layer relu5
I0521 10:48:27.276906 19762 net.cpp:454] relu5 <- ip1
I0521 10:48:27.276918 19762 net.cpp:397] relu5 -> ip1 (in-place)
I0521 10:48:27.277263 19762 net.cpp:150] Setting up relu5
I0521 10:48:27.277277 19762 net.cpp:157] Top shape: 970 196 (190120)
I0521 10:48:27.277287 19762 net.cpp:165] Memory required for data: 1529437800
I0521 10:48:27.277297 19762 layer_factory.hpp:77] Creating layer drop1
I0521 10:48:27.277318 19762 net.cpp:106] Creating Layer drop1
I0521 10:48:27.277329 19762 net.cpp:454] drop1 <- ip1
I0521 10:48:27.277345 19762 net.cpp:397] drop1 -> ip1 (in-place)
I0521 10:48:27.277391 19762 net.cpp:150] Setting up drop1
I0521 10:48:27.277405 19762 net.cpp:157] Top shape: 970 196 (190120)
I0521 10:48:27.277413 19762 net.cpp:165] Memory required for data: 1530198280
I0521 10:48:27.277425 19762 layer_factory.hpp:77] Creating layer ip2
I0521 10:48:27.277439 19762 net.cpp:106] Creating Layer ip2
I0521 10:48:27.277449 19762 net.cpp:454] ip2 <- ip1
I0521 10:48:27.277462 19762 net.cpp:411] ip2 -> ip2
I0521 10:48:27.277942 19762 net.cpp:150] Setting up ip2
I0521 10:48:27.277956 19762 net.cpp:157] Top shape: 970 98 (95060)
I0521 10:48:27.277966 19762 net.cpp:165] Memory required for data: 1530578520
I0521 10:48:27.277993 19762 layer_factory.hpp:77] Creating layer relu6
I0521 10:48:27.278007 19762 net.cpp:106] Creating Layer relu6
I0521 10:48:27.278017 19762 net.cpp:454] relu6 <- ip2
I0521 10:48:27.278029 19762 net.cpp:397] relu6 -> ip2 (in-place)
I0521 10:48:27.278564 19762 net.cpp:150] Setting up relu6
I0521 10:48:27.278585 19762 net.cpp:157] Top shape: 970 98 (95060)
I0521 10:48:27.278595 19762 net.cpp:165] Memory required for data: 1530958760
I0521 10:48:27.278606 19762 layer_factory.hpp:77] Creating layer drop2
I0521 10:48:27.278620 19762 net.cpp:106] Creating Layer drop2
I0521 10:48:27.278630 19762 net.cpp:454] drop2 <- ip2
I0521 10:48:27.278642 19762 net.cpp:397] drop2 -> ip2 (in-place)
I0521 10:48:27.278686 19762 net.cpp:150] Setting up drop2
I0521 10:48:27.278698 19762 net.cpp:157] Top shape: 970 98 (95060)
I0521 10:48:27.278708 19762 net.cpp:165] Memory required for data: 1531339000
I0521 10:48:27.278718 19762 layer_factory.hpp:77] Creating layer ip3
I0521 10:48:27.278733 19762 net.cpp:106] Creating Layer ip3
I0521 10:48:27.278741 19762 net.cpp:454] ip3 <- ip2
I0521 10:48:27.278755 19762 net.cpp:411] ip3 -> ip3
I0521 10:48:27.278977 19762 net.cpp:150] Setting up ip3
I0521 10:48:27.278990 19762 net.cpp:157] Top shape: 970 11 (10670)
I0521 10:48:27.279000 19762 net.cpp:165] Memory required for data: 1531381680
I0521 10:48:27.279016 19762 layer_factory.hpp:77] Creating layer drop3
I0521 10:48:27.279028 19762 net.cpp:106] Creating Layer drop3
I0521 10:48:27.279038 19762 net.cpp:454] drop3 <- ip3
I0521 10:48:27.279050 19762 net.cpp:397] drop3 -> ip3 (in-place)
I0521 10:48:27.279093 19762 net.cpp:150] Setting up drop3
I0521 10:48:27.279105 19762 net.cpp:157] Top shape: 970 11 (10670)
I0521 10:48:27.279115 19762 net.cpp:165] Memory required for data: 1531424360
I0521 10:48:27.279124 19762 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 10:48:27.279139 19762 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 10:48:27.279147 19762 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 10:48:27.279160 19762 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 10:48:27.279175 19762 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 10:48:27.279248 19762 net.cpp:150] Setting up ip3_drop3_0_split
I0521 10:48:27.279261 19762 net.cpp:157] Top shape: 970 11 (10670)
I0521 10:48:27.279273 19762 net.cpp:157] Top shape: 970 11 (10670)
I0521 10:48:27.279284 19762 net.cpp:165] Memory required for data: 1531509720
I0521 10:48:27.279294 19762 layer_factory.hpp:77] Creating layer accuracy
I0521 10:48:27.279315 19762 net.cpp:106] Creating Layer accuracy
I0521 10:48:27.279325 19762 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 10:48:27.279336 19762 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 10:48:27.279350 19762 net.cpp:411] accuracy -> accuracy
I0521 10:48:27.279373 19762 net.cpp:150] Setting up accuracy
I0521 10:48:27.279386 19762 net.cpp:157] Top shape: (1)
I0521 10:48:27.279395 19762 net.cpp:165] Memory required for data: 1531509724
I0521 10:48:27.279407 19762 layer_factory.hpp:77] Creating layer loss
I0521 10:48:27.279420 19762 net.cpp:106] Creating Layer loss
I0521 10:48:27.279430 19762 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 10:48:27.279441 19762 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 10:48:27.279453 19762 net.cpp:411] loss -> loss
I0521 10:48:27.279471 19762 layer_factory.hpp:77] Creating layer loss
I0521 10:48:27.279978 19762 net.cpp:150] Setting up loss
I0521 10:48:27.279991 19762 net.cpp:157] Top shape: (1)
I0521 10:48:27.280000 19762 net.cpp:160]     with loss weight 1
I0521 10:48:27.280022 19762 net.cpp:165] Memory required for data: 1531509728
I0521 10:48:27.280032 19762 net.cpp:226] loss needs backward computation.
I0521 10:48:27.280043 19762 net.cpp:228] accuracy does not need backward computation.
I0521 10:48:27.280055 19762 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 10:48:27.280064 19762 net.cpp:226] drop3 needs backward computation.
I0521 10:48:27.280076 19762 net.cpp:226] ip3 needs backward computation.
I0521 10:48:27.280093 19762 net.cpp:226] drop2 needs backward computation.
I0521 10:48:27.280104 19762 net.cpp:226] relu6 needs backward computation.
I0521 10:48:27.280114 19762 net.cpp:226] ip2 needs backward computation.
I0521 10:48:27.280124 19762 net.cpp:226] drop1 needs backward computation.
I0521 10:48:27.280134 19762 net.cpp:226] relu5 needs backward computation.
I0521 10:48:27.280143 19762 net.cpp:226] ip1 needs backward computation.
I0521 10:48:27.280154 19762 net.cpp:226] pool4 needs backward computation.
I0521 10:48:27.280164 19762 net.cpp:226] relu4 needs backward computation.
I0521 10:48:27.280172 19762 net.cpp:226] conv4 needs backward computation.
I0521 10:48:27.280184 19762 net.cpp:226] pool3 needs backward computation.
I0521 10:48:27.280194 19762 net.cpp:226] relu3 needs backward computation.
I0521 10:48:27.280203 19762 net.cpp:226] conv3 needs backward computation.
I0521 10:48:27.280215 19762 net.cpp:226] pool2 needs backward computation.
I0521 10:48:27.280225 19762 net.cpp:226] relu2 needs backward computation.
I0521 10:48:27.280235 19762 net.cpp:226] conv2 needs backward computation.
I0521 10:48:27.280246 19762 net.cpp:226] pool1 needs backward computation.
I0521 10:48:27.280256 19762 net.cpp:226] relu1 needs backward computation.
I0521 10:48:27.280266 19762 net.cpp:226] conv1 needs backward computation.
I0521 10:48:27.280277 19762 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 10:48:27.280288 19762 net.cpp:228] data_hdf5 does not need backward computation.
I0521 10:48:27.280298 19762 net.cpp:270] This network produces output accuracy
I0521 10:48:27.280309 19762 net.cpp:270] This network produces output loss
I0521 10:48:27.280338 19762 net.cpp:283] Network initialization done.
I0521 10:48:27.280470 19762 solver.cpp:60] Solver scaffolding done.
I0521 10:48:27.281605 19762 caffe.cpp:212] Starting Optimization
I0521 10:48:27.281623 19762 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 10:48:27.281632 19762 solver.cpp:289] Learning Rate Policy: fixed
I0521 10:48:27.282857 19762 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 10:49:13.229784 19762 solver.cpp:409]     Test net output #0: accuracy = 0.130064
I0521 10:49:13.229949 19762 solver.cpp:409]     Test net output #1: loss = 2.39775 (* 1 = 2.39775 loss)
I0521 10:49:13.406759 19762 solver.cpp:237] Iteration 0, loss = 2.39824
I0521 10:49:13.406795 19762 solver.cpp:253]     Train net output #0: loss = 2.39824 (* 1 = 2.39824 loss)
I0521 10:49:13.406813 19762 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 10:49:21.229182 19762 solver.cpp:237] Iteration 15, loss = 2.38998
I0521 10:49:21.229228 19762 solver.cpp:253]     Train net output #0: loss = 2.38998 (* 1 = 2.38998 loss)
I0521 10:49:21.229245 19762 sgd_solver.cpp:106] Iteration 15, lr = 0.0025
I0521 10:49:29.056174 19762 solver.cpp:237] Iteration 30, loss = 2.38061
I0521 10:49:29.056205 19762 solver.cpp:253]     Train net output #0: loss = 2.38061 (* 1 = 2.38061 loss)
I0521 10:49:29.056222 19762 sgd_solver.cpp:106] Iteration 30, lr = 0.0025
I0521 10:49:36.878653 19762 solver.cpp:237] Iteration 45, loss = 2.36336
I0521 10:49:36.878684 19762 solver.cpp:253]     Train net output #0: loss = 2.36336 (* 1 = 2.36336 loss)
I0521 10:49:36.878701 19762 sgd_solver.cpp:106] Iteration 45, lr = 0.0025
I0521 10:49:44.698920 19762 solver.cpp:237] Iteration 60, loss = 2.35304
I0521 10:49:44.699072 19762 solver.cpp:253]     Train net output #0: loss = 2.35304 (* 1 = 2.35304 loss)
I0521 10:49:44.699086 19762 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 10:49:52.520333 19762 solver.cpp:237] Iteration 75, loss = 2.3434
I0521 10:49:52.520364 19762 solver.cpp:253]     Train net output #0: loss = 2.3434 (* 1 = 2.3434 loss)
I0521 10:49:52.520380 19762 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 10:50:00.342149 19762 solver.cpp:237] Iteration 90, loss = 2.33971
I0521 10:50:00.342181 19762 solver.cpp:253]     Train net output #0: loss = 2.33971 (* 1 = 2.33971 loss)
I0521 10:50:00.342197 19762 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 10:50:30.278991 19762 solver.cpp:237] Iteration 105, loss = 2.33509
I0521 10:50:30.279155 19762 solver.cpp:253]     Train net output #0: loss = 2.33509 (* 1 = 2.33509 loss)
I0521 10:50:30.279237 19762 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0521 10:50:38.101433 19762 solver.cpp:237] Iteration 120, loss = 2.31955
I0521 10:50:38.101466 19762 solver.cpp:253]     Train net output #0: loss = 2.31955 (* 1 = 2.31955 loss)
I0521 10:50:38.101480 19762 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 10:50:45.922696 19762 solver.cpp:237] Iteration 135, loss = 2.31837
I0521 10:50:45.922730 19762 solver.cpp:253]     Train net output #0: loss = 2.31837 (* 1 = 2.31837 loss)
I0521 10:50:45.922744 19762 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 10:50:53.738376 19762 solver.cpp:237] Iteration 150, loss = 2.30824
I0521 10:50:53.738425 19762 solver.cpp:253]     Train net output #0: loss = 2.30824 (* 1 = 2.30824 loss)
I0521 10:50:53.738441 19762 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 10:50:55.305358 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_154.caffemodel
I0521 10:50:55.712128 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_154.solverstate
I0521 10:51:01.624084 19762 solver.cpp:237] Iteration 165, loss = 2.29895
I0521 10:51:01.624238 19762 solver.cpp:253]     Train net output #0: loss = 2.29895 (* 1 = 2.29895 loss)
I0521 10:51:01.624253 19762 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0521 10:51:09.445760 19762 solver.cpp:237] Iteration 180, loss = 2.30952
I0521 10:51:09.445791 19762 solver.cpp:253]     Train net output #0: loss = 2.30952 (* 1 = 2.30952 loss)
I0521 10:51:09.445808 19762 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 10:51:17.260020 19762 solver.cpp:237] Iteration 195, loss = 2.29064
I0521 10:51:17.260052 19762 solver.cpp:253]     Train net output #0: loss = 2.29064 (* 1 = 2.29064 loss)
I0521 10:51:17.260071 19762 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0521 10:51:47.178023 19762 solver.cpp:237] Iteration 210, loss = 2.28526
I0521 10:51:47.178192 19762 solver.cpp:253]     Train net output #0: loss = 2.28526 (* 1 = 2.28526 loss)
I0521 10:51:47.178206 19762 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 10:51:54.995932 19762 solver.cpp:237] Iteration 225, loss = 2.26646
I0521 10:51:54.995965 19762 solver.cpp:253]     Train net output #0: loss = 2.26646 (* 1 = 2.26646 loss)
I0521 10:51:54.995982 19762 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 10:52:02.819274 19762 solver.cpp:237] Iteration 240, loss = 2.26936
I0521 10:52:02.819308 19762 solver.cpp:253]     Train net output #0: loss = 2.26936 (* 1 = 2.26936 loss)
I0521 10:52:02.819324 19762 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 10:52:10.646855 19762 solver.cpp:237] Iteration 255, loss = 2.24616
I0521 10:52:10.646904 19762 solver.cpp:253]     Train net output #0: loss = 2.24616 (* 1 = 2.24616 loss)
I0521 10:52:10.646921 19762 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 10:52:18.476426 19762 solver.cpp:237] Iteration 270, loss = 2.24083
I0521 10:52:18.476560 19762 solver.cpp:253]     Train net output #0: loss = 2.24083 (* 1 = 2.24083 loss)
I0521 10:52:18.476573 19762 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 10:52:26.300487 19762 solver.cpp:237] Iteration 285, loss = 2.2
I0521 10:52:26.300519 19762 solver.cpp:253]     Train net output #0: loss = 2.2 (* 1 = 2.2 loss)
I0521 10:52:26.300537 19762 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 10:52:34.123971 19762 solver.cpp:237] Iteration 300, loss = 2.19994
I0521 10:52:34.124013 19762 solver.cpp:253]     Train net output #0: loss = 2.19994 (* 1 = 2.19994 loss)
I0521 10:52:34.124032 19762 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 10:52:37.773803 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_308.caffemodel
I0521 10:52:38.179731 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_308.solverstate
I0521 10:52:38.360924 19762 solver.cpp:341] Iteration 309, Testing net (#0)
I0521 10:53:23.738577 19762 solver.cpp:409]     Test net output #0: accuracy = 0.409165
I0521 10:53:23.738732 19762 solver.cpp:409]     Test net output #1: loss = 2.06949 (* 1 = 2.06949 loss)
I0521 10:53:49.145545 19762 solver.cpp:237] Iteration 315, loss = 2.16552
I0521 10:53:49.145596 19762 solver.cpp:253]     Train net output #0: loss = 2.16552 (* 1 = 2.16552 loss)
I0521 10:53:49.145609 19762 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0521 10:53:56.956305 19762 solver.cpp:237] Iteration 330, loss = 2.18845
I0521 10:53:56.956444 19762 solver.cpp:253]     Train net output #0: loss = 2.18845 (* 1 = 2.18845 loss)
I0521 10:53:56.956456 19762 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 10:54:04.770416 19762 solver.cpp:237] Iteration 345, loss = 2.1342
I0521 10:54:04.770447 19762 solver.cpp:253]     Train net output #0: loss = 2.1342 (* 1 = 2.1342 loss)
I0521 10:54:04.770465 19762 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 10:54:12.590065 19762 solver.cpp:237] Iteration 360, loss = 2.13795
I0521 10:54:12.590098 19762 solver.cpp:253]     Train net output #0: loss = 2.13795 (* 1 = 2.13795 loss)
I0521 10:54:12.590116 19762 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 10:54:20.409247 19762 solver.cpp:237] Iteration 375, loss = 2.12741
I0521 10:54:20.409286 19762 solver.cpp:253]     Train net output #0: loss = 2.12741 (* 1 = 2.12741 loss)
I0521 10:54:20.409307 19762 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 10:54:28.221586 19762 solver.cpp:237] Iteration 390, loss = 2.14384
I0521 10:54:28.221719 19762 solver.cpp:253]     Train net output #0: loss = 2.14384 (* 1 = 2.14384 loss)
I0521 10:54:28.221734 19762 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 10:54:36.043437 19762 solver.cpp:237] Iteration 405, loss = 2.08782
I0521 10:54:36.043468 19762 solver.cpp:253]     Train net output #0: loss = 2.08782 (* 1 = 2.08782 loss)
I0521 10:54:36.043486 19762 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 10:55:06.010534 19762 solver.cpp:237] Iteration 420, loss = 2.07476
I0521 10:55:06.010704 19762 solver.cpp:253]     Train net output #0: loss = 2.07476 (* 1 = 2.07476 loss)
I0521 10:55:06.010717 19762 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 10:55:13.831151 19762 solver.cpp:237] Iteration 435, loss = 2.08001
I0521 10:55:13.831184 19762 solver.cpp:253]     Train net output #0: loss = 2.08001 (* 1 = 2.08001 loss)
I0521 10:55:13.831202 19762 sgd_solver.cpp:106] Iteration 435, lr = 0.0025
I0521 10:55:21.643056 19762 solver.cpp:237] Iteration 450, loss = 2.01585
I0521 10:55:21.643090 19762 solver.cpp:253]     Train net output #0: loss = 2.01585 (* 1 = 2.01585 loss)
I0521 10:55:21.643106 19762 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 10:55:27.371922 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_462.caffemodel
I0521 10:55:27.778056 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_462.solverstate
I0521 10:55:29.522251 19762 solver.cpp:237] Iteration 465, loss = 2.02576
I0521 10:55:29.522303 19762 solver.cpp:253]     Train net output #0: loss = 2.02576 (* 1 = 2.02576 loss)
I0521 10:55:29.522323 19762 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0521 10:55:37.335850 19762 solver.cpp:237] Iteration 480, loss = 2.02902
I0521 10:55:37.335985 19762 solver.cpp:253]     Train net output #0: loss = 2.02902 (* 1 = 2.02902 loss)
I0521 10:55:37.335999 19762 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 10:55:45.156920 19762 solver.cpp:237] Iteration 495, loss = 2.00196
I0521 10:55:45.156952 19762 solver.cpp:253]     Train net output #0: loss = 2.00196 (* 1 = 2.00196 loss)
I0521 10:55:45.156970 19762 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0521 10:55:52.974364 19762 solver.cpp:237] Iteration 510, loss = 2.03216
I0521 10:55:52.974396 19762 solver.cpp:253]     Train net output #0: loss = 2.03216 (* 1 = 2.03216 loss)
I0521 10:55:52.974413 19762 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 10:56:22.966308 19762 solver.cpp:237] Iteration 525, loss = 1.99205
I0521 10:56:22.966476 19762 solver.cpp:253]     Train net output #0: loss = 1.99205 (* 1 = 1.99205 loss)
I0521 10:56:22.966491 19762 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 10:56:30.775444 19762 solver.cpp:237] Iteration 540, loss = 2.03348
I0521 10:56:30.775476 19762 solver.cpp:253]     Train net output #0: loss = 2.03348 (* 1 = 2.03348 loss)
I0521 10:56:30.775495 19762 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 10:56:38.589668 19762 solver.cpp:237] Iteration 555, loss = 1.99335
I0521 10:56:38.589700 19762 solver.cpp:253]     Train net output #0: loss = 1.99335 (* 1 = 1.99335 loss)
I0521 10:56:38.589717 19762 sgd_solver.cpp:106] Iteration 555, lr = 0.0025
I0521 10:56:46.410012 19762 solver.cpp:237] Iteration 570, loss = 1.98239
I0521 10:56:46.410066 19762 solver.cpp:253]     Train net output #0: loss = 1.98239 (* 1 = 1.98239 loss)
I0521 10:56:46.410080 19762 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 10:56:54.224553 19762 solver.cpp:237] Iteration 585, loss = 1.9425
I0521 10:56:54.224692 19762 solver.cpp:253]     Train net output #0: loss = 1.9425 (* 1 = 1.9425 loss)
I0521 10:56:54.224705 19762 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0521 10:57:02.044217 19762 solver.cpp:237] Iteration 600, loss = 1.9451
I0521 10:57:02.044250 19762 solver.cpp:253]     Train net output #0: loss = 1.9451 (* 1 = 1.9451 loss)
I0521 10:57:02.044266 19762 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 10:57:09.863164 19762 solver.cpp:237] Iteration 615, loss = 1.91621
I0521 10:57:09.863194 19762 solver.cpp:253]     Train net output #0: loss = 1.91621 (* 1 = 1.91621 loss)
I0521 10:57:09.863211 19762 sgd_solver.cpp:106] Iteration 615, lr = 0.0025
I0521 10:57:09.863590 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_616.caffemodel
I0521 10:57:10.270869 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_616.solverstate
I0521 10:57:10.976238 19762 solver.cpp:341] Iteration 618, Testing net (#0)
I0521 10:58:17.292039 19762 solver.cpp:409]     Test net output #0: accuracy = 0.562311
I0521 10:58:17.292215 19762 solver.cpp:409]     Test net output #1: loss = 1.5907 (* 1 = 1.5907 loss)
I0521 10:58:45.860198 19762 solver.cpp:237] Iteration 630, loss = 1.932
I0521 10:58:45.860255 19762 solver.cpp:253]     Train net output #0: loss = 1.932 (* 1 = 1.932 loss)
I0521 10:58:45.860270 19762 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 10:58:53.674412 19762 solver.cpp:237] Iteration 645, loss = 1.87075
I0521 10:58:53.674562 19762 solver.cpp:253]     Train net output #0: loss = 1.87075 (* 1 = 1.87075 loss)
I0521 10:58:53.674576 19762 sgd_solver.cpp:106] Iteration 645, lr = 0.0025
I0521 10:59:01.485668 19762 solver.cpp:237] Iteration 660, loss = 1.88976
I0521 10:59:01.485699 19762 solver.cpp:253]     Train net output #0: loss = 1.88976 (* 1 = 1.88976 loss)
I0521 10:59:01.485710 19762 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 10:59:09.297766 19762 solver.cpp:237] Iteration 675, loss = 1.89231
I0521 10:59:09.297798 19762 solver.cpp:253]     Train net output #0: loss = 1.89231 (* 1 = 1.89231 loss)
I0521 10:59:09.297813 19762 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 10:59:17.111027 19762 solver.cpp:237] Iteration 690, loss = 1.87896
I0521 10:59:17.111066 19762 solver.cpp:253]     Train net output #0: loss = 1.87896 (* 1 = 1.87896 loss)
I0521 10:59:17.111081 19762 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 10:59:24.922021 19762 solver.cpp:237] Iteration 705, loss = 1.90441
I0521 10:59:24.922166 19762 solver.cpp:253]     Train net output #0: loss = 1.90441 (* 1 = 1.90441 loss)
I0521 10:59:24.922179 19762 sgd_solver.cpp:106] Iteration 705, lr = 0.0025
I0521 10:59:32.735579 19762 solver.cpp:237] Iteration 720, loss = 1.90218
I0521 10:59:32.735611 19762 solver.cpp:253]     Train net output #0: loss = 1.90218 (* 1 = 1.90218 loss)
I0521 10:59:32.735627 19762 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 11:00:02.670214 19762 solver.cpp:237] Iteration 735, loss = 1.88726
I0521 11:00:02.670384 19762 solver.cpp:253]     Train net output #0: loss = 1.88726 (* 1 = 1.88726 loss)
I0521 11:00:02.670398 19762 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0521 11:00:10.485710 19762 solver.cpp:237] Iteration 750, loss = 1.87797
I0521 11:00:10.485755 19762 solver.cpp:253]     Train net output #0: loss = 1.87797 (* 1 = 1.87797 loss)
I0521 11:00:10.485769 19762 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 11:00:18.294839 19762 solver.cpp:237] Iteration 765, loss = 1.89235
I0521 11:00:18.294872 19762 solver.cpp:253]     Train net output #0: loss = 1.89235 (* 1 = 1.89235 loss)
I0521 11:00:18.294886 19762 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 11:00:20.378010 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_770.caffemodel
I0521 11:00:20.784749 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_770.solverstate
I0521 11:00:26.177677 19762 solver.cpp:237] Iteration 780, loss = 1.91495
I0521 11:00:26.177731 19762 solver.cpp:253]     Train net output #0: loss = 1.91495 (* 1 = 1.91495 loss)
I0521 11:00:26.177745 19762 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 11:00:33.992707 19762 solver.cpp:237] Iteration 795, loss = 1.82863
I0521 11:00:33.992880 19762 solver.cpp:253]     Train net output #0: loss = 1.82863 (* 1 = 1.82863 loss)
I0521 11:00:33.992894 19762 sgd_solver.cpp:106] Iteration 795, lr = 0.0025
I0521 11:00:41.807009 19762 solver.cpp:237] Iteration 810, loss = 1.84936
I0521 11:00:41.807042 19762 solver.cpp:253]     Train net output #0: loss = 1.84936 (* 1 = 1.84936 loss)
I0521 11:00:41.807057 19762 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 11:01:11.764559 19762 solver.cpp:237] Iteration 825, loss = 1.81804
I0521 11:01:11.764727 19762 solver.cpp:253]     Train net output #0: loss = 1.81804 (* 1 = 1.81804 loss)
I0521 11:01:11.764741 19762 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 11:01:19.579030 19762 solver.cpp:237] Iteration 840, loss = 1.85784
I0521 11:01:19.579090 19762 solver.cpp:253]     Train net output #0: loss = 1.85784 (* 1 = 1.85784 loss)
I0521 11:01:19.579104 19762 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 11:01:27.390426 19762 solver.cpp:237] Iteration 855, loss = 1.87712
I0521 11:01:27.390460 19762 solver.cpp:253]     Train net output #0: loss = 1.87712 (* 1 = 1.87712 loss)
I0521 11:01:27.390475 19762 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 11:01:35.197887 19762 solver.cpp:237] Iteration 870, loss = 1.8603
I0521 11:01:35.197919 19762 solver.cpp:253]     Train net output #0: loss = 1.8603 (* 1 = 1.8603 loss)
I0521 11:01:35.197935 19762 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 11:01:43.009649 19762 solver.cpp:237] Iteration 885, loss = 1.82319
I0521 11:01:43.009786 19762 solver.cpp:253]     Train net output #0: loss = 1.82319 (* 1 = 1.82319 loss)
I0521 11:01:43.009799 19762 sgd_solver.cpp:106] Iteration 885, lr = 0.0025
I0521 11:01:50.816715 19762 solver.cpp:237] Iteration 900, loss = 1.82056
I0521 11:01:50.816758 19762 solver.cpp:253]     Train net output #0: loss = 1.82056 (* 1 = 1.82056 loss)
I0521 11:01:50.816774 19762 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 11:01:58.623998 19762 solver.cpp:237] Iteration 915, loss = 1.79893
I0521 11:01:58.624029 19762 solver.cpp:253]     Train net output #0: loss = 1.79893 (* 1 = 1.79893 loss)
I0521 11:01:58.624044 19762 sgd_solver.cpp:106] Iteration 915, lr = 0.0025
I0521 11:02:02.788313 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_924.caffemodel
I0521 11:02:03.191928 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_924.solverstate
I0521 11:02:04.418287 19762 solver.cpp:341] Iteration 927, Testing net (#0)
I0521 11:02:49.531718 19762 solver.cpp:409]     Test net output #0: accuracy = 0.602611
I0521 11:02:49.531886 19762 solver.cpp:409]     Test net output #1: loss = 1.39219 (* 1 = 1.39219 loss)
I0521 11:03:13.407855 19762 solver.cpp:237] Iteration 930, loss = 1.81237
I0521 11:03:13.407912 19762 solver.cpp:253]     Train net output #0: loss = 1.81237 (* 1 = 1.81237 loss)
I0521 11:03:13.407925 19762 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 11:03:21.220691 19762 solver.cpp:237] Iteration 945, loss = 1.7813
I0521 11:03:21.220842 19762 solver.cpp:253]     Train net output #0: loss = 1.7813 (* 1 = 1.7813 loss)
I0521 11:03:21.220855 19762 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 11:03:29.032415 19762 solver.cpp:237] Iteration 960, loss = 1.81982
I0521 11:03:29.032459 19762 solver.cpp:253]     Train net output #0: loss = 1.81982 (* 1 = 1.81982 loss)
I0521 11:03:29.032475 19762 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 11:03:36.852367 19762 solver.cpp:237] Iteration 975, loss = 1.86174
I0521 11:03:36.852399 19762 solver.cpp:253]     Train net output #0: loss = 1.86174 (* 1 = 1.86174 loss)
I0521 11:03:36.852414 19762 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 11:03:44.660769 19762 solver.cpp:237] Iteration 990, loss = 1.82035
I0521 11:03:44.660802 19762 solver.cpp:253]     Train net output #0: loss = 1.82035 (* 1 = 1.82035 loss)
I0521 11:03:44.660817 19762 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 11:03:52.471335 19762 solver.cpp:237] Iteration 1005, loss = 1.80613
I0521 11:03:52.471503 19762 solver.cpp:253]     Train net output #0: loss = 1.80613 (* 1 = 1.80613 loss)
I0521 11:03:52.471516 19762 sgd_solver.cpp:106] Iteration 1005, lr = 0.0025
I0521 11:04:00.288516 19762 solver.cpp:237] Iteration 1020, loss = 1.78527
I0521 11:04:00.288547 19762 solver.cpp:253]     Train net output #0: loss = 1.78527 (* 1 = 1.78527 loss)
I0521 11:04:00.288563 19762 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 11:04:30.215631 19762 solver.cpp:237] Iteration 1035, loss = 1.82068
I0521 11:04:30.215806 19762 solver.cpp:253]     Train net output #0: loss = 1.82068 (* 1 = 1.82068 loss)
I0521 11:04:30.215829 19762 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 11:04:38.030207 19762 solver.cpp:237] Iteration 1050, loss = 1.83718
I0521 11:04:38.030239 19762 solver.cpp:253]     Train net output #0: loss = 1.83718 (* 1 = 1.83718 loss)
I0521 11:04:38.030254 19762 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 11:04:45.851532 19762 solver.cpp:237] Iteration 1065, loss = 1.82269
I0521 11:04:45.851572 19762 solver.cpp:253]     Train net output #0: loss = 1.82269 (* 1 = 1.82269 loss)
I0521 11:04:45.851590 19762 sgd_solver.cpp:106] Iteration 1065, lr = 0.0025
I0521 11:04:52.100042 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1078.caffemodel
I0521 11:04:52.503547 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1078.solverstate
I0521 11:04:53.728611 19762 solver.cpp:237] Iteration 1080, loss = 1.83914
I0521 11:04:53.728657 19762 solver.cpp:253]     Train net output #0: loss = 1.83914 (* 1 = 1.83914 loss)
I0521 11:04:53.728677 19762 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 11:05:01.545742 19762 solver.cpp:237] Iteration 1095, loss = 1.77692
I0521 11:05:01.545893 19762 solver.cpp:253]     Train net output #0: loss = 1.77692 (* 1 = 1.77692 loss)
I0521 11:05:01.545907 19762 sgd_solver.cpp:106] Iteration 1095, lr = 0.0025
I0521 11:05:09.362143 19762 solver.cpp:237] Iteration 1110, loss = 1.74931
I0521 11:05:09.362186 19762 solver.cpp:253]     Train net output #0: loss = 1.74931 (* 1 = 1.74931 loss)
I0521 11:05:09.362201 19762 sgd_solver.cpp:106] Iteration 1110, lr = 0.0025
I0521 11:05:17.177394 19762 solver.cpp:237] Iteration 1125, loss = 1.78325
I0521 11:05:17.177428 19762 solver.cpp:253]     Train net output #0: loss = 1.78325 (* 1 = 1.78325 loss)
I0521 11:05:17.177443 19762 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 11:05:47.162382 19762 solver.cpp:237] Iteration 1140, loss = 1.7566
I0521 11:05:47.162557 19762 solver.cpp:253]     Train net output #0: loss = 1.7566 (* 1 = 1.7566 loss)
I0521 11:05:47.162572 19762 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 11:05:54.978602 19762 solver.cpp:237] Iteration 1155, loss = 1.78771
I0521 11:05:54.978647 19762 solver.cpp:253]     Train net output #0: loss = 1.78771 (* 1 = 1.78771 loss)
I0521 11:05:54.978667 19762 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 11:06:02.794173 19762 solver.cpp:237] Iteration 1170, loss = 1.85744
I0521 11:06:02.794206 19762 solver.cpp:253]     Train net output #0: loss = 1.85744 (* 1 = 1.85744 loss)
I0521 11:06:02.794221 19762 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 11:06:10.608381 19762 solver.cpp:237] Iteration 1185, loss = 1.78004
I0521 11:06:10.608413 19762 solver.cpp:253]     Train net output #0: loss = 1.78004 (* 1 = 1.78004 loss)
I0521 11:06:10.608428 19762 sgd_solver.cpp:106] Iteration 1185, lr = 0.0025
I0521 11:06:18.420084 19762 solver.cpp:237] Iteration 1200, loss = 1.72775
I0521 11:06:18.420223 19762 solver.cpp:253]     Train net output #0: loss = 1.72775 (* 1 = 1.72775 loss)
I0521 11:06:18.420238 19762 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 11:06:26.239460 19762 solver.cpp:237] Iteration 1215, loss = 1.75975
I0521 11:06:26.239495 19762 solver.cpp:253]     Train net output #0: loss = 1.75975 (* 1 = 1.75975 loss)
I0521 11:06:26.239507 19762 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 11:06:34.056679 19762 solver.cpp:237] Iteration 1230, loss = 1.79655
I0521 11:06:34.056711 19762 solver.cpp:253]     Train net output #0: loss = 1.79655 (* 1 = 1.79655 loss)
I0521 11:06:34.056727 19762 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0521 11:06:34.576187 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1232.caffemodel
I0521 11:06:34.979305 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1232.solverstate
I0521 11:06:36.724761 19762 solver.cpp:341] Iteration 1236, Testing net (#0)
I0521 11:07:43.066149 19762 solver.cpp:409]     Test net output #0: accuracy = 0.641358
I0521 11:07:43.066329 19762 solver.cpp:409]     Test net output #1: loss = 1.25014 (* 1 = 1.25014 loss)
I0521 11:08:10.060619 19762 solver.cpp:237] Iteration 1245, loss = 1.76434
I0521 11:08:10.060675 19762 solver.cpp:253]     Train net output #0: loss = 1.76434 (* 1 = 1.76434 loss)
I0521 11:08:10.060689 19762 sgd_solver.cpp:106] Iteration 1245, lr = 0.0025
I0521 11:08:17.875614 19762 solver.cpp:237] Iteration 1260, loss = 1.76972
I0521 11:08:17.875766 19762 solver.cpp:253]     Train net output #0: loss = 1.76972 (* 1 = 1.76972 loss)
I0521 11:08:17.875778 19762 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 11:08:25.696461 19762 solver.cpp:237] Iteration 1275, loss = 1.7639
I0521 11:08:25.696506 19762 solver.cpp:253]     Train net output #0: loss = 1.7639 (* 1 = 1.7639 loss)
I0521 11:08:25.696521 19762 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 11:08:33.511756 19762 solver.cpp:237] Iteration 1290, loss = 1.74188
I0521 11:08:33.511790 19762 solver.cpp:253]     Train net output #0: loss = 1.74188 (* 1 = 1.74188 loss)
I0521 11:08:33.511803 19762 sgd_solver.cpp:106] Iteration 1290, lr = 0.0025
I0521 11:08:41.332628 19762 solver.cpp:237] Iteration 1305, loss = 1.73164
I0521 11:08:41.332660 19762 solver.cpp:253]     Train net output #0: loss = 1.73164 (* 1 = 1.73164 loss)
I0521 11:08:41.332675 19762 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0521 11:08:49.148005 19762 solver.cpp:237] Iteration 1320, loss = 1.68538
I0521 11:08:49.148154 19762 solver.cpp:253]     Train net output #0: loss = 1.68538 (* 1 = 1.68538 loss)
I0521 11:08:49.148169 19762 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 11:08:56.966359 19762 solver.cpp:237] Iteration 1335, loss = 1.75733
I0521 11:08:56.966390 19762 solver.cpp:253]     Train net output #0: loss = 1.75733 (* 1 = 1.75733 loss)
I0521 11:08:56.966406 19762 sgd_solver.cpp:106] Iteration 1335, lr = 0.0025
I0521 11:09:26.958788 19762 solver.cpp:237] Iteration 1350, loss = 1.75037
I0521 11:09:26.958958 19762 solver.cpp:253]     Train net output #0: loss = 1.75037 (* 1 = 1.75037 loss)
I0521 11:09:26.958972 19762 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 11:09:34.784940 19762 solver.cpp:237] Iteration 1365, loss = 1.8273
I0521 11:09:34.784973 19762 solver.cpp:253]     Train net output #0: loss = 1.8273 (* 1 = 1.8273 loss)
I0521 11:09:34.784988 19762 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0521 11:09:42.591706 19762 solver.cpp:237] Iteration 1380, loss = 1.75628
I0521 11:09:42.591747 19762 solver.cpp:253]     Train net output #0: loss = 1.75628 (* 1 = 1.75628 loss)
I0521 11:09:42.591766 19762 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 11:09:45.199717 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1386.caffemodel
I0521 11:09:45.606626 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1386.solverstate
I0521 11:09:50.476181 19762 solver.cpp:237] Iteration 1395, loss = 1.79327
I0521 11:09:50.476232 19762 solver.cpp:253]     Train net output #0: loss = 1.79327 (* 1 = 1.79327 loss)
I0521 11:09:50.476244 19762 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0521 11:09:58.293493 19762 solver.cpp:237] Iteration 1410, loss = 1.7147
I0521 11:09:58.293648 19762 solver.cpp:253]     Train net output #0: loss = 1.7147 (* 1 = 1.7147 loss)
I0521 11:09:58.293661 19762 sgd_solver.cpp:106] Iteration 1410, lr = 0.0025
I0521 11:10:06.113307 19762 solver.cpp:237] Iteration 1425, loss = 1.72806
I0521 11:10:06.113353 19762 solver.cpp:253]     Train net output #0: loss = 1.72806 (* 1 = 1.72806 loss)
I0521 11:10:06.113373 19762 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 11:10:13.926986 19762 solver.cpp:237] Iteration 1440, loss = 1.71767
I0521 11:10:13.927019 19762 solver.cpp:253]     Train net output #0: loss = 1.71767 (* 1 = 1.71767 loss)
I0521 11:10:13.927034 19762 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 11:10:43.899145 19762 solver.cpp:237] Iteration 1455, loss = 1.71438
I0521 11:10:43.899322 19762 solver.cpp:253]     Train net output #0: loss = 1.71438 (* 1 = 1.71438 loss)
I0521 11:10:43.899336 19762 sgd_solver.cpp:106] Iteration 1455, lr = 0.0025
I0521 11:10:51.714658 19762 solver.cpp:237] Iteration 1470, loss = 1.75453
I0521 11:10:51.714691 19762 solver.cpp:253]     Train net output #0: loss = 1.75453 (* 1 = 1.75453 loss)
I0521 11:10:51.714707 19762 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 11:10:59.534011 19762 solver.cpp:237] Iteration 1485, loss = 1.7727
I0521 11:10:59.534054 19762 solver.cpp:253]     Train net output #0: loss = 1.7727 (* 1 = 1.7727 loss)
I0521 11:10:59.534070 19762 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 11:11:07.358225 19762 solver.cpp:237] Iteration 1500, loss = 1.7901
I0521 11:11:07.358259 19762 solver.cpp:253]     Train net output #0: loss = 1.7901 (* 1 = 1.7901 loss)
I0521 11:11:07.358275 19762 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 11:11:15.174023 19762 solver.cpp:237] Iteration 1515, loss = 1.7129
I0521 11:11:15.174163 19762 solver.cpp:253]     Train net output #0: loss = 1.7129 (* 1 = 1.7129 loss)
I0521 11:11:15.174177 19762 sgd_solver.cpp:106] Iteration 1515, lr = 0.0025
I0521 11:11:22.986201 19762 solver.cpp:237] Iteration 1530, loss = 1.70417
I0521 11:11:22.986239 19762 solver.cpp:253]     Train net output #0: loss = 1.70417 (* 1 = 1.70417 loss)
I0521 11:11:22.986259 19762 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 11:11:27.678918 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1540.caffemodel
I0521 11:11:28.083878 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1540.solverstate
I0521 11:11:30.350371 19762 solver.cpp:341] Iteration 1545, Testing net (#0)
I0521 11:12:15.724762 19762 solver.cpp:409]     Test net output #0: accuracy = 0.646506
I0521 11:12:15.724939 19762 solver.cpp:409]     Test net output #1: loss = 1.20956 (* 1 = 1.20956 loss)
I0521 11:12:15.879403 19762 solver.cpp:237] Iteration 1545, loss = 1.73741
I0521 11:12:15.879432 19762 solver.cpp:253]     Train net output #0: loss = 1.73741 (* 1 = 1.73741 loss)
I0521 11:12:15.879452 19762 sgd_solver.cpp:106] Iteration 1545, lr = 0.0025
I0521 11:12:15.879844 19762 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1546.caffemodel
I0521 11:12:16.286366 19762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487_iter_1546.solverstate
I0521 11:12:16.314581 19762 solver.cpp:326] Optimization Done.
I0521 11:12:16.314610 19762 caffe.cpp:215] Optimization Done.
Application 11237719 resources: utime ~1250s, stime ~225s, Rss ~5329612, inblocks ~3594475, outblocks ~194564
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_970_2016-05-20T11.21.08.067487.solver"
	User time (seconds): 0.58
	System time (seconds): 0.10
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:38.57
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15088
	Voluntary context switches: 2721
	Involuntary context switches: 70
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
