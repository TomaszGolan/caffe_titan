2805954
I0520 21:30:28.327661 20835 caffe.cpp:184] Using GPUs 0
I0520 21:30:28.754441 20835 solver.cpp:48] Initializing solver from parameters: 
test_iter: 454
test_interval: 909
base_lr: 0.0025
display: 45
max_iter: 4545
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 454
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450.prototxt"
I0520 21:30:28.756214 20835 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450.prototxt
I0520 21:30:28.772009 20835 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 21:30:28.772068 20835 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 21:30:28.772413 20835 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 330
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:30:28.772593 20835 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:30:28.772617 20835 net.cpp:106] Creating Layer data_hdf5
I0520 21:30:28.772631 20835 net.cpp:411] data_hdf5 -> data
I0520 21:30:28.772665 20835 net.cpp:411] data_hdf5 -> label
I0520 21:30:28.772696 20835 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 21:30:28.788795 20835 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 21:30:28.790967 20835 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 21:30:50.341680 20835 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 21:30:50.346752 20835 net.cpp:150] Setting up data_hdf5
I0520 21:30:50.346792 20835 net.cpp:157] Top shape: 330 1 127 50 (2095500)
I0520 21:30:50.346807 20835 net.cpp:157] Top shape: 330 (330)
I0520 21:30:50.346818 20835 net.cpp:165] Memory required for data: 8383320
I0520 21:30:50.346832 20835 layer_factory.hpp:77] Creating layer conv1
I0520 21:30:50.346866 20835 net.cpp:106] Creating Layer conv1
I0520 21:30:50.346879 20835 net.cpp:454] conv1 <- data
I0520 21:30:50.346899 20835 net.cpp:411] conv1 -> conv1
I0520 21:30:52.713712 20835 net.cpp:150] Setting up conv1
I0520 21:30:52.713758 20835 net.cpp:157] Top shape: 330 12 120 48 (22809600)
I0520 21:30:52.713769 20835 net.cpp:165] Memory required for data: 99621720
I0520 21:30:52.713799 20835 layer_factory.hpp:77] Creating layer relu1
I0520 21:30:52.713820 20835 net.cpp:106] Creating Layer relu1
I0520 21:30:52.713831 20835 net.cpp:454] relu1 <- conv1
I0520 21:30:52.713845 20835 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:30:52.714361 20835 net.cpp:150] Setting up relu1
I0520 21:30:52.714378 20835 net.cpp:157] Top shape: 330 12 120 48 (22809600)
I0520 21:30:52.714390 20835 net.cpp:165] Memory required for data: 190860120
I0520 21:30:52.714398 20835 layer_factory.hpp:77] Creating layer pool1
I0520 21:30:52.714414 20835 net.cpp:106] Creating Layer pool1
I0520 21:30:52.714424 20835 net.cpp:454] pool1 <- conv1
I0520 21:30:52.714437 20835 net.cpp:411] pool1 -> pool1
I0520 21:30:52.714517 20835 net.cpp:150] Setting up pool1
I0520 21:30:52.714531 20835 net.cpp:157] Top shape: 330 12 60 48 (11404800)
I0520 21:30:52.714541 20835 net.cpp:165] Memory required for data: 236479320
I0520 21:30:52.714552 20835 layer_factory.hpp:77] Creating layer conv2
I0520 21:30:52.714575 20835 net.cpp:106] Creating Layer conv2
I0520 21:30:52.714586 20835 net.cpp:454] conv2 <- pool1
I0520 21:30:52.714599 20835 net.cpp:411] conv2 -> conv2
I0520 21:30:52.717283 20835 net.cpp:150] Setting up conv2
I0520 21:30:52.717310 20835 net.cpp:157] Top shape: 330 20 54 46 (16394400)
I0520 21:30:52.717321 20835 net.cpp:165] Memory required for data: 302056920
I0520 21:30:52.717340 20835 layer_factory.hpp:77] Creating layer relu2
I0520 21:30:52.717355 20835 net.cpp:106] Creating Layer relu2
I0520 21:30:52.717365 20835 net.cpp:454] relu2 <- conv2
I0520 21:30:52.717376 20835 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:30:52.717707 20835 net.cpp:150] Setting up relu2
I0520 21:30:52.717722 20835 net.cpp:157] Top shape: 330 20 54 46 (16394400)
I0520 21:30:52.717732 20835 net.cpp:165] Memory required for data: 367634520
I0520 21:30:52.717742 20835 layer_factory.hpp:77] Creating layer pool2
I0520 21:30:52.717756 20835 net.cpp:106] Creating Layer pool2
I0520 21:30:52.717764 20835 net.cpp:454] pool2 <- conv2
I0520 21:30:52.717789 20835 net.cpp:411] pool2 -> pool2
I0520 21:30:52.717859 20835 net.cpp:150] Setting up pool2
I0520 21:30:52.717871 20835 net.cpp:157] Top shape: 330 20 27 46 (8197200)
I0520 21:30:52.717881 20835 net.cpp:165] Memory required for data: 400423320
I0520 21:30:52.717892 20835 layer_factory.hpp:77] Creating layer conv3
I0520 21:30:52.717910 20835 net.cpp:106] Creating Layer conv3
I0520 21:30:52.717921 20835 net.cpp:454] conv3 <- pool2
I0520 21:30:52.717934 20835 net.cpp:411] conv3 -> conv3
I0520 21:30:52.719856 20835 net.cpp:150] Setting up conv3
I0520 21:30:52.719879 20835 net.cpp:157] Top shape: 330 28 22 44 (8944320)
I0520 21:30:52.719892 20835 net.cpp:165] Memory required for data: 436200600
I0520 21:30:52.719909 20835 layer_factory.hpp:77] Creating layer relu3
I0520 21:30:52.719925 20835 net.cpp:106] Creating Layer relu3
I0520 21:30:52.719935 20835 net.cpp:454] relu3 <- conv3
I0520 21:30:52.719949 20835 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:30:52.720417 20835 net.cpp:150] Setting up relu3
I0520 21:30:52.720435 20835 net.cpp:157] Top shape: 330 28 22 44 (8944320)
I0520 21:30:52.720445 20835 net.cpp:165] Memory required for data: 471977880
I0520 21:30:52.720455 20835 layer_factory.hpp:77] Creating layer pool3
I0520 21:30:52.720468 20835 net.cpp:106] Creating Layer pool3
I0520 21:30:52.720477 20835 net.cpp:454] pool3 <- conv3
I0520 21:30:52.720490 20835 net.cpp:411] pool3 -> pool3
I0520 21:30:52.720557 20835 net.cpp:150] Setting up pool3
I0520 21:30:52.720571 20835 net.cpp:157] Top shape: 330 28 11 44 (4472160)
I0520 21:30:52.720580 20835 net.cpp:165] Memory required for data: 489866520
I0520 21:30:52.720589 20835 layer_factory.hpp:77] Creating layer conv4
I0520 21:30:52.720607 20835 net.cpp:106] Creating Layer conv4
I0520 21:30:52.720618 20835 net.cpp:454] conv4 <- pool3
I0520 21:30:52.720631 20835 net.cpp:411] conv4 -> conv4
I0520 21:30:52.723381 20835 net.cpp:150] Setting up conv4
I0520 21:30:52.723409 20835 net.cpp:157] Top shape: 330 36 6 42 (2993760)
I0520 21:30:52.723419 20835 net.cpp:165] Memory required for data: 501841560
I0520 21:30:52.723434 20835 layer_factory.hpp:77] Creating layer relu4
I0520 21:30:52.723448 20835 net.cpp:106] Creating Layer relu4
I0520 21:30:52.723459 20835 net.cpp:454] relu4 <- conv4
I0520 21:30:52.723471 20835 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:30:52.723956 20835 net.cpp:150] Setting up relu4
I0520 21:30:52.723973 20835 net.cpp:157] Top shape: 330 36 6 42 (2993760)
I0520 21:30:52.723984 20835 net.cpp:165] Memory required for data: 513816600
I0520 21:30:52.723994 20835 layer_factory.hpp:77] Creating layer pool4
I0520 21:30:52.724007 20835 net.cpp:106] Creating Layer pool4
I0520 21:30:52.724017 20835 net.cpp:454] pool4 <- conv4
I0520 21:30:52.724030 20835 net.cpp:411] pool4 -> pool4
I0520 21:30:52.724098 20835 net.cpp:150] Setting up pool4
I0520 21:30:52.724112 20835 net.cpp:157] Top shape: 330 36 3 42 (1496880)
I0520 21:30:52.724123 20835 net.cpp:165] Memory required for data: 519804120
I0520 21:30:52.724133 20835 layer_factory.hpp:77] Creating layer ip1
I0520 21:30:52.724151 20835 net.cpp:106] Creating Layer ip1
I0520 21:30:52.724161 20835 net.cpp:454] ip1 <- pool4
I0520 21:30:52.724174 20835 net.cpp:411] ip1 -> ip1
I0520 21:30:52.739562 20835 net.cpp:150] Setting up ip1
I0520 21:30:52.739590 20835 net.cpp:157] Top shape: 330 196 (64680)
I0520 21:30:52.739604 20835 net.cpp:165] Memory required for data: 520062840
I0520 21:30:52.739631 20835 layer_factory.hpp:77] Creating layer relu5
I0520 21:30:52.739646 20835 net.cpp:106] Creating Layer relu5
I0520 21:30:52.739656 20835 net.cpp:454] relu5 <- ip1
I0520 21:30:52.739670 20835 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:30:52.740018 20835 net.cpp:150] Setting up relu5
I0520 21:30:52.740032 20835 net.cpp:157] Top shape: 330 196 (64680)
I0520 21:30:52.740043 20835 net.cpp:165] Memory required for data: 520321560
I0520 21:30:52.740053 20835 layer_factory.hpp:77] Creating layer drop1
I0520 21:30:52.740074 20835 net.cpp:106] Creating Layer drop1
I0520 21:30:52.740085 20835 net.cpp:454] drop1 <- ip1
I0520 21:30:52.740110 20835 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:30:52.740157 20835 net.cpp:150] Setting up drop1
I0520 21:30:52.740170 20835 net.cpp:157] Top shape: 330 196 (64680)
I0520 21:30:52.740180 20835 net.cpp:165] Memory required for data: 520580280
I0520 21:30:52.740190 20835 layer_factory.hpp:77] Creating layer ip2
I0520 21:30:52.740208 20835 net.cpp:106] Creating Layer ip2
I0520 21:30:52.740218 20835 net.cpp:454] ip2 <- ip1
I0520 21:30:52.740229 20835 net.cpp:411] ip2 -> ip2
I0520 21:30:52.740695 20835 net.cpp:150] Setting up ip2
I0520 21:30:52.740708 20835 net.cpp:157] Top shape: 330 98 (32340)
I0520 21:30:52.740718 20835 net.cpp:165] Memory required for data: 520709640
I0520 21:30:52.740733 20835 layer_factory.hpp:77] Creating layer relu6
I0520 21:30:52.740746 20835 net.cpp:106] Creating Layer relu6
I0520 21:30:52.740756 20835 net.cpp:454] relu6 <- ip2
I0520 21:30:52.740767 20835 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:30:52.741286 20835 net.cpp:150] Setting up relu6
I0520 21:30:52.741302 20835 net.cpp:157] Top shape: 330 98 (32340)
I0520 21:30:52.741312 20835 net.cpp:165] Memory required for data: 520839000
I0520 21:30:52.741323 20835 layer_factory.hpp:77] Creating layer drop2
I0520 21:30:52.741336 20835 net.cpp:106] Creating Layer drop2
I0520 21:30:52.741346 20835 net.cpp:454] drop2 <- ip2
I0520 21:30:52.741358 20835 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:30:52.741400 20835 net.cpp:150] Setting up drop2
I0520 21:30:52.741412 20835 net.cpp:157] Top shape: 330 98 (32340)
I0520 21:30:52.741423 20835 net.cpp:165] Memory required for data: 520968360
I0520 21:30:52.741432 20835 layer_factory.hpp:77] Creating layer ip3
I0520 21:30:52.741446 20835 net.cpp:106] Creating Layer ip3
I0520 21:30:52.741456 20835 net.cpp:454] ip3 <- ip2
I0520 21:30:52.741468 20835 net.cpp:411] ip3 -> ip3
I0520 21:30:52.741677 20835 net.cpp:150] Setting up ip3
I0520 21:30:52.741690 20835 net.cpp:157] Top shape: 330 11 (3630)
I0520 21:30:52.741699 20835 net.cpp:165] Memory required for data: 520982880
I0520 21:30:52.741715 20835 layer_factory.hpp:77] Creating layer drop3
I0520 21:30:52.741727 20835 net.cpp:106] Creating Layer drop3
I0520 21:30:52.741737 20835 net.cpp:454] drop3 <- ip3
I0520 21:30:52.741750 20835 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:30:52.741788 20835 net.cpp:150] Setting up drop3
I0520 21:30:52.741801 20835 net.cpp:157] Top shape: 330 11 (3630)
I0520 21:30:52.741811 20835 net.cpp:165] Memory required for data: 520997400
I0520 21:30:52.741821 20835 layer_factory.hpp:77] Creating layer loss
I0520 21:30:52.741839 20835 net.cpp:106] Creating Layer loss
I0520 21:30:52.741849 20835 net.cpp:454] loss <- ip3
I0520 21:30:52.741860 20835 net.cpp:454] loss <- label
I0520 21:30:52.741873 20835 net.cpp:411] loss -> loss
I0520 21:30:52.741889 20835 layer_factory.hpp:77] Creating layer loss
I0520 21:30:52.742537 20835 net.cpp:150] Setting up loss
I0520 21:30:52.742558 20835 net.cpp:157] Top shape: (1)
I0520 21:30:52.742571 20835 net.cpp:160]     with loss weight 1
I0520 21:30:52.742614 20835 net.cpp:165] Memory required for data: 520997404
I0520 21:30:52.742624 20835 net.cpp:226] loss needs backward computation.
I0520 21:30:52.742635 20835 net.cpp:226] drop3 needs backward computation.
I0520 21:30:52.742645 20835 net.cpp:226] ip3 needs backward computation.
I0520 21:30:52.742653 20835 net.cpp:226] drop2 needs backward computation.
I0520 21:30:52.742663 20835 net.cpp:226] relu6 needs backward computation.
I0520 21:30:52.742673 20835 net.cpp:226] ip2 needs backward computation.
I0520 21:30:52.742683 20835 net.cpp:226] drop1 needs backward computation.
I0520 21:30:52.742692 20835 net.cpp:226] relu5 needs backward computation.
I0520 21:30:52.742702 20835 net.cpp:226] ip1 needs backward computation.
I0520 21:30:52.742712 20835 net.cpp:226] pool4 needs backward computation.
I0520 21:30:52.742722 20835 net.cpp:226] relu4 needs backward computation.
I0520 21:30:52.742732 20835 net.cpp:226] conv4 needs backward computation.
I0520 21:30:52.742741 20835 net.cpp:226] pool3 needs backward computation.
I0520 21:30:52.742760 20835 net.cpp:226] relu3 needs backward computation.
I0520 21:30:52.742770 20835 net.cpp:226] conv3 needs backward computation.
I0520 21:30:52.742779 20835 net.cpp:226] pool2 needs backward computation.
I0520 21:30:52.742790 20835 net.cpp:226] relu2 needs backward computation.
I0520 21:30:52.742800 20835 net.cpp:226] conv2 needs backward computation.
I0520 21:30:52.742810 20835 net.cpp:226] pool1 needs backward computation.
I0520 21:30:52.742822 20835 net.cpp:226] relu1 needs backward computation.
I0520 21:30:52.742832 20835 net.cpp:226] conv1 needs backward computation.
I0520 21:30:52.742843 20835 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:30:52.742853 20835 net.cpp:270] This network produces output loss
I0520 21:30:52.742877 20835 net.cpp:283] Network initialization done.
I0520 21:30:52.744459 20835 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450.prototxt
I0520 21:30:52.744529 20835 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 21:30:52.744885 20835 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 330
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:30:52.745074 20835 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:30:52.745090 20835 net.cpp:106] Creating Layer data_hdf5
I0520 21:30:52.745102 20835 net.cpp:411] data_hdf5 -> data
I0520 21:30:52.745120 20835 net.cpp:411] data_hdf5 -> label
I0520 21:30:52.745136 20835 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 21:30:52.746284 20835 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 21:31:14.093498 20835 net.cpp:150] Setting up data_hdf5
I0520 21:31:14.093667 20835 net.cpp:157] Top shape: 330 1 127 50 (2095500)
I0520 21:31:14.093680 20835 net.cpp:157] Top shape: 330 (330)
I0520 21:31:14.093690 20835 net.cpp:165] Memory required for data: 8383320
I0520 21:31:14.093704 20835 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 21:31:14.093732 20835 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 21:31:14.093744 20835 net.cpp:454] label_data_hdf5_1_split <- label
I0520 21:31:14.093757 20835 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 21:31:14.093780 20835 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 21:31:14.093852 20835 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 21:31:14.093866 20835 net.cpp:157] Top shape: 330 (330)
I0520 21:31:14.093878 20835 net.cpp:157] Top shape: 330 (330)
I0520 21:31:14.093888 20835 net.cpp:165] Memory required for data: 8385960
I0520 21:31:14.093897 20835 layer_factory.hpp:77] Creating layer conv1
I0520 21:31:14.093919 20835 net.cpp:106] Creating Layer conv1
I0520 21:31:14.093930 20835 net.cpp:454] conv1 <- data
I0520 21:31:14.093945 20835 net.cpp:411] conv1 -> conv1
I0520 21:31:14.095899 20835 net.cpp:150] Setting up conv1
I0520 21:31:14.095923 20835 net.cpp:157] Top shape: 330 12 120 48 (22809600)
I0520 21:31:14.095934 20835 net.cpp:165] Memory required for data: 99624360
I0520 21:31:14.095955 20835 layer_factory.hpp:77] Creating layer relu1
I0520 21:31:14.095970 20835 net.cpp:106] Creating Layer relu1
I0520 21:31:14.095980 20835 net.cpp:454] relu1 <- conv1
I0520 21:31:14.095993 20835 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:31:14.096490 20835 net.cpp:150] Setting up relu1
I0520 21:31:14.096506 20835 net.cpp:157] Top shape: 330 12 120 48 (22809600)
I0520 21:31:14.096516 20835 net.cpp:165] Memory required for data: 190862760
I0520 21:31:14.096527 20835 layer_factory.hpp:77] Creating layer pool1
I0520 21:31:14.096544 20835 net.cpp:106] Creating Layer pool1
I0520 21:31:14.096554 20835 net.cpp:454] pool1 <- conv1
I0520 21:31:14.096566 20835 net.cpp:411] pool1 -> pool1
I0520 21:31:14.096640 20835 net.cpp:150] Setting up pool1
I0520 21:31:14.096654 20835 net.cpp:157] Top shape: 330 12 60 48 (11404800)
I0520 21:31:14.096663 20835 net.cpp:165] Memory required for data: 236481960
I0520 21:31:14.096674 20835 layer_factory.hpp:77] Creating layer conv2
I0520 21:31:14.096691 20835 net.cpp:106] Creating Layer conv2
I0520 21:31:14.096701 20835 net.cpp:454] conv2 <- pool1
I0520 21:31:14.096716 20835 net.cpp:411] conv2 -> conv2
I0520 21:31:14.098654 20835 net.cpp:150] Setting up conv2
I0520 21:31:14.098675 20835 net.cpp:157] Top shape: 330 20 54 46 (16394400)
I0520 21:31:14.098688 20835 net.cpp:165] Memory required for data: 302059560
I0520 21:31:14.098706 20835 layer_factory.hpp:77] Creating layer relu2
I0520 21:31:14.098719 20835 net.cpp:106] Creating Layer relu2
I0520 21:31:14.098731 20835 net.cpp:454] relu2 <- conv2
I0520 21:31:14.098742 20835 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:31:14.099076 20835 net.cpp:150] Setting up relu2
I0520 21:31:14.099089 20835 net.cpp:157] Top shape: 330 20 54 46 (16394400)
I0520 21:31:14.099099 20835 net.cpp:165] Memory required for data: 367637160
I0520 21:31:14.099109 20835 layer_factory.hpp:77] Creating layer pool2
I0520 21:31:14.099123 20835 net.cpp:106] Creating Layer pool2
I0520 21:31:14.099133 20835 net.cpp:454] pool2 <- conv2
I0520 21:31:14.099145 20835 net.cpp:411] pool2 -> pool2
I0520 21:31:14.099217 20835 net.cpp:150] Setting up pool2
I0520 21:31:14.099230 20835 net.cpp:157] Top shape: 330 20 27 46 (8197200)
I0520 21:31:14.099239 20835 net.cpp:165] Memory required for data: 400425960
I0520 21:31:14.099249 20835 layer_factory.hpp:77] Creating layer conv3
I0520 21:31:14.099268 20835 net.cpp:106] Creating Layer conv3
I0520 21:31:14.099279 20835 net.cpp:454] conv3 <- pool2
I0520 21:31:14.099293 20835 net.cpp:411] conv3 -> conv3
I0520 21:31:14.101265 20835 net.cpp:150] Setting up conv3
I0520 21:31:14.101289 20835 net.cpp:157] Top shape: 330 28 22 44 (8944320)
I0520 21:31:14.101299 20835 net.cpp:165] Memory required for data: 436203240
I0520 21:31:14.101330 20835 layer_factory.hpp:77] Creating layer relu3
I0520 21:31:14.101343 20835 net.cpp:106] Creating Layer relu3
I0520 21:31:14.101353 20835 net.cpp:454] relu3 <- conv3
I0520 21:31:14.101366 20835 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:31:14.101837 20835 net.cpp:150] Setting up relu3
I0520 21:31:14.101853 20835 net.cpp:157] Top shape: 330 28 22 44 (8944320)
I0520 21:31:14.101864 20835 net.cpp:165] Memory required for data: 471980520
I0520 21:31:14.101873 20835 layer_factory.hpp:77] Creating layer pool3
I0520 21:31:14.101886 20835 net.cpp:106] Creating Layer pool3
I0520 21:31:14.101897 20835 net.cpp:454] pool3 <- conv3
I0520 21:31:14.101909 20835 net.cpp:411] pool3 -> pool3
I0520 21:31:14.101981 20835 net.cpp:150] Setting up pool3
I0520 21:31:14.101994 20835 net.cpp:157] Top shape: 330 28 11 44 (4472160)
I0520 21:31:14.102004 20835 net.cpp:165] Memory required for data: 489869160
I0520 21:31:14.102013 20835 layer_factory.hpp:77] Creating layer conv4
I0520 21:31:14.102031 20835 net.cpp:106] Creating Layer conv4
I0520 21:31:14.102041 20835 net.cpp:454] conv4 <- pool3
I0520 21:31:14.102057 20835 net.cpp:411] conv4 -> conv4
I0520 21:31:14.104113 20835 net.cpp:150] Setting up conv4
I0520 21:31:14.104135 20835 net.cpp:157] Top shape: 330 36 6 42 (2993760)
I0520 21:31:14.104148 20835 net.cpp:165] Memory required for data: 501844200
I0520 21:31:14.104163 20835 layer_factory.hpp:77] Creating layer relu4
I0520 21:31:14.104176 20835 net.cpp:106] Creating Layer relu4
I0520 21:31:14.104187 20835 net.cpp:454] relu4 <- conv4
I0520 21:31:14.104199 20835 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:31:14.104671 20835 net.cpp:150] Setting up relu4
I0520 21:31:14.104686 20835 net.cpp:157] Top shape: 330 36 6 42 (2993760)
I0520 21:31:14.104696 20835 net.cpp:165] Memory required for data: 513819240
I0520 21:31:14.104707 20835 layer_factory.hpp:77] Creating layer pool4
I0520 21:31:14.104719 20835 net.cpp:106] Creating Layer pool4
I0520 21:31:14.104729 20835 net.cpp:454] pool4 <- conv4
I0520 21:31:14.104742 20835 net.cpp:411] pool4 -> pool4
I0520 21:31:14.104811 20835 net.cpp:150] Setting up pool4
I0520 21:31:14.104825 20835 net.cpp:157] Top shape: 330 36 3 42 (1496880)
I0520 21:31:14.104835 20835 net.cpp:165] Memory required for data: 519806760
I0520 21:31:14.104843 20835 layer_factory.hpp:77] Creating layer ip1
I0520 21:31:14.104856 20835 net.cpp:106] Creating Layer ip1
I0520 21:31:14.104867 20835 net.cpp:454] ip1 <- pool4
I0520 21:31:14.104882 20835 net.cpp:411] ip1 -> ip1
I0520 21:31:14.120342 20835 net.cpp:150] Setting up ip1
I0520 21:31:14.120368 20835 net.cpp:157] Top shape: 330 196 (64680)
I0520 21:31:14.120381 20835 net.cpp:165] Memory required for data: 520065480
I0520 21:31:14.120404 20835 layer_factory.hpp:77] Creating layer relu5
I0520 21:31:14.120419 20835 net.cpp:106] Creating Layer relu5
I0520 21:31:14.120429 20835 net.cpp:454] relu5 <- ip1
I0520 21:31:14.120441 20835 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:31:14.120786 20835 net.cpp:150] Setting up relu5
I0520 21:31:14.120800 20835 net.cpp:157] Top shape: 330 196 (64680)
I0520 21:31:14.120810 20835 net.cpp:165] Memory required for data: 520324200
I0520 21:31:14.120820 20835 layer_factory.hpp:77] Creating layer drop1
I0520 21:31:14.120838 20835 net.cpp:106] Creating Layer drop1
I0520 21:31:14.120848 20835 net.cpp:454] drop1 <- ip1
I0520 21:31:14.120862 20835 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:31:14.120905 20835 net.cpp:150] Setting up drop1
I0520 21:31:14.120918 20835 net.cpp:157] Top shape: 330 196 (64680)
I0520 21:31:14.120926 20835 net.cpp:165] Memory required for data: 520582920
I0520 21:31:14.120936 20835 layer_factory.hpp:77] Creating layer ip2
I0520 21:31:14.120950 20835 net.cpp:106] Creating Layer ip2
I0520 21:31:14.120960 20835 net.cpp:454] ip2 <- ip1
I0520 21:31:14.120973 20835 net.cpp:411] ip2 -> ip2
I0520 21:31:14.121453 20835 net.cpp:150] Setting up ip2
I0520 21:31:14.121465 20835 net.cpp:157] Top shape: 330 98 (32340)
I0520 21:31:14.121475 20835 net.cpp:165] Memory required for data: 520712280
I0520 21:31:14.121503 20835 layer_factory.hpp:77] Creating layer relu6
I0520 21:31:14.121516 20835 net.cpp:106] Creating Layer relu6
I0520 21:31:14.121526 20835 net.cpp:454] relu6 <- ip2
I0520 21:31:14.121539 20835 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:31:14.122071 20835 net.cpp:150] Setting up relu6
I0520 21:31:14.122092 20835 net.cpp:157] Top shape: 330 98 (32340)
I0520 21:31:14.122102 20835 net.cpp:165] Memory required for data: 520841640
I0520 21:31:14.122112 20835 layer_factory.hpp:77] Creating layer drop2
I0520 21:31:14.122125 20835 net.cpp:106] Creating Layer drop2
I0520 21:31:14.122135 20835 net.cpp:454] drop2 <- ip2
I0520 21:31:14.122148 20835 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:31:14.122192 20835 net.cpp:150] Setting up drop2
I0520 21:31:14.122205 20835 net.cpp:157] Top shape: 330 98 (32340)
I0520 21:31:14.122215 20835 net.cpp:165] Memory required for data: 520971000
I0520 21:31:14.122225 20835 layer_factory.hpp:77] Creating layer ip3
I0520 21:31:14.122238 20835 net.cpp:106] Creating Layer ip3
I0520 21:31:14.122248 20835 net.cpp:454] ip3 <- ip2
I0520 21:31:14.122262 20835 net.cpp:411] ip3 -> ip3
I0520 21:31:14.122485 20835 net.cpp:150] Setting up ip3
I0520 21:31:14.122498 20835 net.cpp:157] Top shape: 330 11 (3630)
I0520 21:31:14.122509 20835 net.cpp:165] Memory required for data: 520985520
I0520 21:31:14.122524 20835 layer_factory.hpp:77] Creating layer drop3
I0520 21:31:14.122536 20835 net.cpp:106] Creating Layer drop3
I0520 21:31:14.122546 20835 net.cpp:454] drop3 <- ip3
I0520 21:31:14.122560 20835 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:31:14.122599 20835 net.cpp:150] Setting up drop3
I0520 21:31:14.122612 20835 net.cpp:157] Top shape: 330 11 (3630)
I0520 21:31:14.122622 20835 net.cpp:165] Memory required for data: 521000040
I0520 21:31:14.122632 20835 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 21:31:14.122644 20835 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 21:31:14.122654 20835 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 21:31:14.122666 20835 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 21:31:14.122681 20835 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 21:31:14.122753 20835 net.cpp:150] Setting up ip3_drop3_0_split
I0520 21:31:14.122766 20835 net.cpp:157] Top shape: 330 11 (3630)
I0520 21:31:14.122779 20835 net.cpp:157] Top shape: 330 11 (3630)
I0520 21:31:14.122789 20835 net.cpp:165] Memory required for data: 521029080
I0520 21:31:14.122800 20835 layer_factory.hpp:77] Creating layer accuracy
I0520 21:31:14.122822 20835 net.cpp:106] Creating Layer accuracy
I0520 21:31:14.122833 20835 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 21:31:14.122843 20835 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 21:31:14.122856 20835 net.cpp:411] accuracy -> accuracy
I0520 21:31:14.122880 20835 net.cpp:150] Setting up accuracy
I0520 21:31:14.122892 20835 net.cpp:157] Top shape: (1)
I0520 21:31:14.122902 20835 net.cpp:165] Memory required for data: 521029084
I0520 21:31:14.122912 20835 layer_factory.hpp:77] Creating layer loss
I0520 21:31:14.122925 20835 net.cpp:106] Creating Layer loss
I0520 21:31:14.122936 20835 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 21:31:14.122947 20835 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 21:31:14.122959 20835 net.cpp:411] loss -> loss
I0520 21:31:14.122977 20835 layer_factory.hpp:77] Creating layer loss
I0520 21:31:14.123462 20835 net.cpp:150] Setting up loss
I0520 21:31:14.123476 20835 net.cpp:157] Top shape: (1)
I0520 21:31:14.123486 20835 net.cpp:160]     with loss weight 1
I0520 21:31:14.123503 20835 net.cpp:165] Memory required for data: 521029088
I0520 21:31:14.123513 20835 net.cpp:226] loss needs backward computation.
I0520 21:31:14.123525 20835 net.cpp:228] accuracy does not need backward computation.
I0520 21:31:14.123536 20835 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 21:31:14.123546 20835 net.cpp:226] drop3 needs backward computation.
I0520 21:31:14.123555 20835 net.cpp:226] ip3 needs backward computation.
I0520 21:31:14.123565 20835 net.cpp:226] drop2 needs backward computation.
I0520 21:31:14.123584 20835 net.cpp:226] relu6 needs backward computation.
I0520 21:31:14.123594 20835 net.cpp:226] ip2 needs backward computation.
I0520 21:31:14.123603 20835 net.cpp:226] drop1 needs backward computation.
I0520 21:31:14.123612 20835 net.cpp:226] relu5 needs backward computation.
I0520 21:31:14.123621 20835 net.cpp:226] ip1 needs backward computation.
I0520 21:31:14.123631 20835 net.cpp:226] pool4 needs backward computation.
I0520 21:31:14.123642 20835 net.cpp:226] relu4 needs backward computation.
I0520 21:31:14.123651 20835 net.cpp:226] conv4 needs backward computation.
I0520 21:31:14.123662 20835 net.cpp:226] pool3 needs backward computation.
I0520 21:31:14.123672 20835 net.cpp:226] relu3 needs backward computation.
I0520 21:31:14.123688 20835 net.cpp:226] conv3 needs backward computation.
I0520 21:31:14.123700 20835 net.cpp:226] pool2 needs backward computation.
I0520 21:31:14.123711 20835 net.cpp:226] relu2 needs backward computation.
I0520 21:31:14.123721 20835 net.cpp:226] conv2 needs backward computation.
I0520 21:31:14.123731 20835 net.cpp:226] pool1 needs backward computation.
I0520 21:31:14.123741 20835 net.cpp:226] relu1 needs backward computation.
I0520 21:31:14.123751 20835 net.cpp:226] conv1 needs backward computation.
I0520 21:31:14.123764 20835 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 21:31:14.123775 20835 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:31:14.123785 20835 net.cpp:270] This network produces output accuracy
I0520 21:31:14.123795 20835 net.cpp:270] This network produces output loss
I0520 21:31:14.123821 20835 net.cpp:283] Network initialization done.
I0520 21:31:14.123955 20835 solver.cpp:60] Solver scaffolding done.
I0520 21:31:14.125075 20835 caffe.cpp:212] Starting Optimization
I0520 21:31:14.125093 20835 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 21:31:14.125108 20835 solver.cpp:289] Learning Rate Policy: fixed
I0520 21:31:14.126327 20835 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 21:32:00.389979 20835 solver.cpp:409]     Test net output #0: accuracy = 0.0850622
I0520 21:32:00.390139 20835 solver.cpp:409]     Test net output #1: loss = 2.39866 (* 1 = 2.39866 loss)
I0520 21:32:00.461010 20835 solver.cpp:237] Iteration 0, loss = 2.39825
I0520 21:32:00.461048 20835 solver.cpp:253]     Train net output #0: loss = 2.39825 (* 1 = 2.39825 loss)
I0520 21:32:00.461066 20835 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 21:32:08.546948 20835 solver.cpp:237] Iteration 45, loss = 2.34987
I0520 21:32:08.546984 20835 solver.cpp:253]     Train net output #0: loss = 2.34987 (* 1 = 2.34987 loss)
I0520 21:32:08.547001 20835 sgd_solver.cpp:106] Iteration 45, lr = 0.0025
I0520 21:32:16.629475 20835 solver.cpp:237] Iteration 90, loss = 2.3311
I0520 21:32:16.629508 20835 solver.cpp:253]     Train net output #0: loss = 2.3311 (* 1 = 2.3311 loss)
I0520 21:32:16.629524 20835 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0520 21:32:24.716975 20835 solver.cpp:237] Iteration 135, loss = 2.29768
I0520 21:32:24.717017 20835 solver.cpp:253]     Train net output #0: loss = 2.29768 (* 1 = 2.29768 loss)
I0520 21:32:24.717032 20835 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0520 21:32:32.799839 20835 solver.cpp:237] Iteration 180, loss = 2.3262
I0520 21:32:32.799984 20835 solver.cpp:253]     Train net output #0: loss = 2.3262 (* 1 = 2.3262 loss)
I0520 21:32:32.799998 20835 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0520 21:32:40.881484 20835 solver.cpp:237] Iteration 225, loss = 2.28883
I0520 21:32:40.881516 20835 solver.cpp:253]     Train net output #0: loss = 2.28883 (* 1 = 2.28883 loss)
I0520 21:32:40.881536 20835 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0520 21:32:48.969360 20835 solver.cpp:237] Iteration 270, loss = 2.1807
I0520 21:32:48.969403 20835 solver.cpp:253]     Train net output #0: loss = 2.1807 (* 1 = 2.1807 loss)
I0520 21:32:48.969419 20835 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0520 21:33:19.162231 20835 solver.cpp:237] Iteration 315, loss = 2.10521
I0520 21:33:19.162394 20835 solver.cpp:253]     Train net output #0: loss = 2.10521 (* 1 = 2.10521 loss)
I0520 21:33:19.162410 20835 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0520 21:33:27.248838 20835 solver.cpp:237] Iteration 360, loss = 2.08656
I0520 21:33:27.248872 20835 solver.cpp:253]     Train net output #0: loss = 2.08656 (* 1 = 2.08656 loss)
I0520 21:33:27.248889 20835 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0520 21:33:35.338147 20835 solver.cpp:237] Iteration 405, loss = 2.1001
I0520 21:33:35.338188 20835 solver.cpp:253]     Train net output #0: loss = 2.1001 (* 1 = 2.1001 loss)
I0520 21:33:35.338208 20835 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0520 21:33:43.423869 20835 solver.cpp:237] Iteration 450, loss = 2.00413
I0520 21:33:43.423897 20835 solver.cpp:253]     Train net output #0: loss = 2.00413 (* 1 = 2.00413 loss)
I0520 21:33:43.423909 20835 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0520 21:33:43.962617 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_454.caffemodel
I0520 21:33:44.129844 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_454.solverstate
I0520 21:33:51.577184 20835 solver.cpp:237] Iteration 495, loss = 1.98046
I0520 21:33:51.577337 20835 solver.cpp:253]     Train net output #0: loss = 1.98046 (* 1 = 1.98046 loss)
I0520 21:33:51.577350 20835 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0520 21:33:59.664332 20835 solver.cpp:237] Iteration 540, loss = 1.96379
I0520 21:33:59.664376 20835 solver.cpp:253]     Train net output #0: loss = 1.96379 (* 1 = 1.96379 loss)
I0520 21:33:59.664392 20835 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0520 21:34:07.752965 20835 solver.cpp:237] Iteration 585, loss = 1.85319
I0520 21:34:07.753000 20835 solver.cpp:253]     Train net output #0: loss = 1.85319 (* 1 = 1.85319 loss)
I0520 21:34:07.753015 20835 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0520 21:34:38.012387 20835 solver.cpp:237] Iteration 630, loss = 1.91062
I0520 21:34:38.012542 20835 solver.cpp:253]     Train net output #0: loss = 1.91062 (* 1 = 1.91062 loss)
I0520 21:34:38.012557 20835 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0520 21:34:46.102053 20835 solver.cpp:237] Iteration 675, loss = 1.87346
I0520 21:34:46.102087 20835 solver.cpp:253]     Train net output #0: loss = 1.87346 (* 1 = 1.87346 loss)
I0520 21:34:46.102104 20835 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0520 21:34:54.195539 20835 solver.cpp:237] Iteration 720, loss = 1.76322
I0520 21:34:54.195585 20835 solver.cpp:253]     Train net output #0: loss = 1.76322 (* 1 = 1.76322 loss)
I0520 21:34:54.195601 20835 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0520 21:35:02.281429 20835 solver.cpp:237] Iteration 765, loss = 1.81482
I0520 21:35:02.281463 20835 solver.cpp:253]     Train net output #0: loss = 1.81482 (* 1 = 1.81482 loss)
I0520 21:35:02.281479 20835 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0520 21:35:10.371485 20835 solver.cpp:237] Iteration 810, loss = 1.90069
I0520 21:35:10.371633 20835 solver.cpp:253]     Train net output #0: loss = 1.90069 (* 1 = 1.90069 loss)
I0520 21:35:10.371646 20835 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0520 21:35:18.460566 20835 solver.cpp:237] Iteration 855, loss = 1.78607
I0520 21:35:18.460605 20835 solver.cpp:253]     Train net output #0: loss = 1.78607 (* 1 = 1.78607 loss)
I0520 21:35:18.460623 20835 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0520 21:35:26.552961 20835 solver.cpp:237] Iteration 900, loss = 1.81482
I0520 21:35:26.552996 20835 solver.cpp:253]     Train net output #0: loss = 1.81482 (* 1 = 1.81482 loss)
I0520 21:35:26.553012 20835 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0520 21:35:27.811715 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_908.caffemodel
I0520 21:35:27.979126 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_908.solverstate
I0520 21:35:28.060420 20835 solver.cpp:341] Iteration 909, Testing net (#0)
I0520 21:36:13.441028 20835 solver.cpp:409]     Test net output #0: accuracy = 0.624857
I0520 21:36:13.441187 20835 solver.cpp:409]     Test net output #1: loss = 1.36223 (* 1 = 1.36223 loss)
I0520 21:36:42.121598 20835 solver.cpp:237] Iteration 945, loss = 1.83593
I0520 21:36:42.121646 20835 solver.cpp:253]     Train net output #0: loss = 1.83593 (* 1 = 1.83593 loss)
I0520 21:36:42.121662 20835 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0520 21:36:50.216383 20835 solver.cpp:237] Iteration 990, loss = 1.72746
I0520 21:36:50.216531 20835 solver.cpp:253]     Train net output #0: loss = 1.72746 (* 1 = 1.72746 loss)
I0520 21:36:50.216544 20835 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0520 21:36:58.304142 20835 solver.cpp:237] Iteration 1035, loss = 1.81456
I0520 21:36:58.304182 20835 solver.cpp:253]     Train net output #0: loss = 1.81456 (* 1 = 1.81456 loss)
I0520 21:36:58.304204 20835 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0520 21:37:06.392036 20835 solver.cpp:237] Iteration 1080, loss = 1.77041
I0520 21:37:06.392071 20835 solver.cpp:253]     Train net output #0: loss = 1.77041 (* 1 = 1.77041 loss)
I0520 21:37:06.392086 20835 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0520 21:37:14.479646 20835 solver.cpp:237] Iteration 1125, loss = 1.80332
I0520 21:37:14.479679 20835 solver.cpp:253]     Train net output #0: loss = 1.80332 (* 1 = 1.80332 loss)
I0520 21:37:14.479703 20835 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0520 21:37:22.570046 20835 solver.cpp:237] Iteration 1170, loss = 1.82713
I0520 21:37:22.570197 20835 solver.cpp:253]     Train net output #0: loss = 1.82713 (* 1 = 1.82713 loss)
I0520 21:37:22.570212 20835 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0520 21:37:52.814991 20835 solver.cpp:237] Iteration 1215, loss = 1.72812
I0520 21:37:52.815155 20835 solver.cpp:253]     Train net output #0: loss = 1.72812 (* 1 = 1.72812 loss)
I0520 21:37:52.815171 20835 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0520 21:38:00.901368 20835 solver.cpp:237] Iteration 1260, loss = 1.66221
I0520 21:38:00.901401 20835 solver.cpp:253]     Train net output #0: loss = 1.66221 (* 1 = 1.66221 loss)
I0520 21:38:00.901418 20835 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0520 21:38:08.991945 20835 solver.cpp:237] Iteration 1305, loss = 1.78827
I0520 21:38:08.991991 20835 solver.cpp:253]     Train net output #0: loss = 1.78827 (* 1 = 1.78827 loss)
I0520 21:38:08.992007 20835 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0520 21:38:17.080339 20835 solver.cpp:237] Iteration 1350, loss = 1.81142
I0520 21:38:17.080374 20835 solver.cpp:253]     Train net output #0: loss = 1.81142 (* 1 = 1.81142 loss)
I0520 21:38:17.080389 20835 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0520 21:38:19.056970 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_1362.caffemodel
I0520 21:38:19.223000 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_1362.solverstate
I0520 21:38:25.240574 20835 solver.cpp:237] Iteration 1395, loss = 1.70833
I0520 21:38:25.240738 20835 solver.cpp:253]     Train net output #0: loss = 1.70833 (* 1 = 1.70833 loss)
I0520 21:38:25.240751 20835 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0520 21:38:33.334570 20835 solver.cpp:237] Iteration 1440, loss = 1.69448
I0520 21:38:33.334604 20835 solver.cpp:253]     Train net output #0: loss = 1.69448 (* 1 = 1.69448 loss)
I0520 21:38:33.334617 20835 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0520 21:38:41.426695 20835 solver.cpp:237] Iteration 1485, loss = 1.73246
I0520 21:38:41.426741 20835 solver.cpp:253]     Train net output #0: loss = 1.73246 (* 1 = 1.73246 loss)
I0520 21:38:41.426755 20835 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0520 21:39:11.698207 20835 solver.cpp:237] Iteration 1530, loss = 1.72865
I0520 21:39:11.698369 20835 solver.cpp:253]     Train net output #0: loss = 1.72865 (* 1 = 1.72865 loss)
I0520 21:39:11.698385 20835 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0520 21:39:19.797761 20835 solver.cpp:237] Iteration 1575, loss = 1.67696
I0520 21:39:19.797793 20835 solver.cpp:253]     Train net output #0: loss = 1.67696 (* 1 = 1.67696 loss)
I0520 21:39:19.797811 20835 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0520 21:39:27.890357 20835 solver.cpp:237] Iteration 1620, loss = 1.70088
I0520 21:39:27.890393 20835 solver.cpp:253]     Train net output #0: loss = 1.70088 (* 1 = 1.70088 loss)
I0520 21:39:27.890415 20835 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0520 21:39:35.978140 20835 solver.cpp:237] Iteration 1665, loss = 1.75135
I0520 21:39:35.978174 20835 solver.cpp:253]     Train net output #0: loss = 1.75135 (* 1 = 1.75135 loss)
I0520 21:39:35.978190 20835 sgd_solver.cpp:106] Iteration 1665, lr = 0.0025
I0520 21:39:44.067471 20835 solver.cpp:237] Iteration 1710, loss = 1.70007
I0520 21:39:44.067615 20835 solver.cpp:253]     Train net output #0: loss = 1.70007 (* 1 = 1.70007 loss)
I0520 21:39:44.067628 20835 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0520 21:39:52.158778 20835 solver.cpp:237] Iteration 1755, loss = 1.66388
I0520 21:39:52.158815 20835 solver.cpp:253]     Train net output #0: loss = 1.66388 (* 1 = 1.66388 loss)
I0520 21:39:52.158836 20835 sgd_solver.cpp:106] Iteration 1755, lr = 0.0025
I0520 21:40:00.245775 20835 solver.cpp:237] Iteration 1800, loss = 1.61442
I0520 21:40:00.245803 20835 solver.cpp:253]     Train net output #0: loss = 1.61442 (* 1 = 1.61442 loss)
I0520 21:40:00.245815 20835 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0520 21:40:02.942507 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_1816.caffemodel
I0520 21:40:03.117779 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_1816.solverstate
I0520 21:40:03.380148 20835 solver.cpp:341] Iteration 1818, Testing net (#0)
I0520 21:41:09.678629 20835 solver.cpp:409]     Test net output #0: accuracy = 0.675437
I0520 21:41:09.678792 20835 solver.cpp:409]     Test net output #1: loss = 1.19763 (* 1 = 1.19763 loss)
I0520 21:41:36.713701 20835 solver.cpp:237] Iteration 1845, loss = 1.69892
I0520 21:41:36.713749 20835 solver.cpp:253]     Train net output #0: loss = 1.69892 (* 1 = 1.69892 loss)
I0520 21:41:36.713767 20835 sgd_solver.cpp:106] Iteration 1845, lr = 0.0025
I0520 21:41:44.794037 20835 solver.cpp:237] Iteration 1890, loss = 1.67196
I0520 21:41:44.794188 20835 solver.cpp:253]     Train net output #0: loss = 1.67196 (* 1 = 1.67196 loss)
I0520 21:41:44.794200 20835 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0520 21:41:52.877521 20835 solver.cpp:237] Iteration 1935, loss = 1.68645
I0520 21:41:52.877552 20835 solver.cpp:253]     Train net output #0: loss = 1.68645 (* 1 = 1.68645 loss)
I0520 21:41:52.877565 20835 sgd_solver.cpp:106] Iteration 1935, lr = 0.0025
I0520 21:42:00.958935 20835 solver.cpp:237] Iteration 1980, loss = 1.65461
I0520 21:42:00.958972 20835 solver.cpp:253]     Train net output #0: loss = 1.65461 (* 1 = 1.65461 loss)
I0520 21:42:00.958994 20835 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0520 21:42:09.035213 20835 solver.cpp:237] Iteration 2025, loss = 1.73212
I0520 21:42:09.035248 20835 solver.cpp:253]     Train net output #0: loss = 1.73212 (* 1 = 1.73212 loss)
I0520 21:42:09.035264 20835 sgd_solver.cpp:106] Iteration 2025, lr = 0.0025
I0520 21:42:17.118623 20835 solver.cpp:237] Iteration 2070, loss = 1.54296
I0520 21:42:17.118762 20835 solver.cpp:253]     Train net output #0: loss = 1.54296 (* 1 = 1.54296 loss)
I0520 21:42:17.118774 20835 sgd_solver.cpp:106] Iteration 2070, lr = 0.0025
I0520 21:42:25.197556 20835 solver.cpp:237] Iteration 2115, loss = 1.70117
I0520 21:42:25.197600 20835 solver.cpp:253]     Train net output #0: loss = 1.70117 (* 1 = 1.70117 loss)
I0520 21:42:25.197618 20835 sgd_solver.cpp:106] Iteration 2115, lr = 0.0025
I0520 21:42:55.443837 20835 solver.cpp:237] Iteration 2160, loss = 1.69444
I0520 21:42:55.444002 20835 solver.cpp:253]     Train net output #0: loss = 1.69444 (* 1 = 1.69444 loss)
I0520 21:42:55.444018 20835 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0520 21:43:03.523587 20835 solver.cpp:237] Iteration 2205, loss = 1.6542
I0520 21:43:03.523620 20835 solver.cpp:253]     Train net output #0: loss = 1.6542 (* 1 = 1.6542 loss)
I0520 21:43:03.523634 20835 sgd_solver.cpp:106] Iteration 2205, lr = 0.0025
I0520 21:43:11.602272 20835 solver.cpp:237] Iteration 2250, loss = 1.63932
I0520 21:43:11.602314 20835 solver.cpp:253]     Train net output #0: loss = 1.63932 (* 1 = 1.63932 loss)
I0520 21:43:11.602331 20835 sgd_solver.cpp:106] Iteration 2250, lr = 0.0025
I0520 21:43:15.013931 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_2270.caffemodel
I0520 21:43:15.179644 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_2270.solverstate
I0520 21:43:19.754714 20835 solver.cpp:237] Iteration 2295, loss = 1.68126
I0520 21:43:19.754763 20835 solver.cpp:253]     Train net output #0: loss = 1.68126 (* 1 = 1.68126 loss)
I0520 21:43:19.754777 20835 sgd_solver.cpp:106] Iteration 2295, lr = 0.0025
I0520 21:43:27.832870 20835 solver.cpp:237] Iteration 2340, loss = 1.59613
I0520 21:43:27.833014 20835 solver.cpp:253]     Train net output #0: loss = 1.59613 (* 1 = 1.59613 loss)
I0520 21:43:27.833027 20835 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0520 21:43:35.908396 20835 solver.cpp:237] Iteration 2385, loss = 1.59374
I0520 21:43:35.908437 20835 solver.cpp:253]     Train net output #0: loss = 1.59374 (* 1 = 1.59374 loss)
I0520 21:43:35.908455 20835 sgd_solver.cpp:106] Iteration 2385, lr = 0.0025
I0520 21:44:06.148525 20835 solver.cpp:237] Iteration 2430, loss = 1.69414
I0520 21:44:06.148692 20835 solver.cpp:253]     Train net output #0: loss = 1.69414 (* 1 = 1.69414 loss)
I0520 21:44:06.148708 20835 sgd_solver.cpp:106] Iteration 2430, lr = 0.0025
I0520 21:44:14.228171 20835 solver.cpp:237] Iteration 2475, loss = 1.56023
I0520 21:44:14.228204 20835 solver.cpp:253]     Train net output #0: loss = 1.56023 (* 1 = 1.56023 loss)
I0520 21:44:14.228222 20835 sgd_solver.cpp:106] Iteration 2475, lr = 0.0025
I0520 21:44:22.307644 20835 solver.cpp:237] Iteration 2520, loss = 1.57548
I0520 21:44:22.307678 20835 solver.cpp:253]     Train net output #0: loss = 1.57548 (* 1 = 1.57548 loss)
I0520 21:44:22.307700 20835 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0520 21:44:30.388623 20835 solver.cpp:237] Iteration 2565, loss = 1.62662
I0520 21:44:30.388664 20835 solver.cpp:253]     Train net output #0: loss = 1.62662 (* 1 = 1.62662 loss)
I0520 21:44:30.388684 20835 sgd_solver.cpp:106] Iteration 2565, lr = 0.0025
I0520 21:44:38.463488 20835 solver.cpp:237] Iteration 2610, loss = 1.58512
I0520 21:44:38.463629 20835 solver.cpp:253]     Train net output #0: loss = 1.58512 (* 1 = 1.58512 loss)
I0520 21:44:38.463642 20835 sgd_solver.cpp:106] Iteration 2610, lr = 0.0025
I0520 21:44:46.542881 20835 solver.cpp:237] Iteration 2655, loss = 1.60714
I0520 21:44:46.542913 20835 solver.cpp:253]     Train net output #0: loss = 1.60714 (* 1 = 1.60714 loss)
I0520 21:44:46.542930 20835 sgd_solver.cpp:106] Iteration 2655, lr = 0.0025
I0520 21:44:54.621405 20835 solver.cpp:237] Iteration 2700, loss = 1.46868
I0520 21:44:54.621445 20835 solver.cpp:253]     Train net output #0: loss = 1.46868 (* 1 = 1.46868 loss)
I0520 21:44:54.621466 20835 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0520 21:44:58.752082 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_2724.caffemodel
I0520 21:44:58.915557 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_2724.solverstate
I0520 21:44:59.355355 20835 solver.cpp:341] Iteration 2727, Testing net (#0)
I0520 21:45:44.485450 20835 solver.cpp:409]     Test net output #0: accuracy = 0.731471
I0520 21:45:44.485610 20835 solver.cpp:409]     Test net output #1: loss = 0.946705 (* 1 = 0.946705 loss)
I0520 21:46:09.936676 20835 solver.cpp:237] Iteration 2745, loss = 1.51443
I0520 21:46:09.936727 20835 solver.cpp:253]     Train net output #0: loss = 1.51443 (* 1 = 1.51443 loss)
I0520 21:46:09.936741 20835 sgd_solver.cpp:106] Iteration 2745, lr = 0.0025
I0520 21:46:18.019930 20835 solver.cpp:237] Iteration 2790, loss = 1.61062
I0520 21:46:18.020082 20835 solver.cpp:253]     Train net output #0: loss = 1.61062 (* 1 = 1.61062 loss)
I0520 21:46:18.020097 20835 sgd_solver.cpp:106] Iteration 2790, lr = 0.0025
I0520 21:46:26.104557 20835 solver.cpp:237] Iteration 2835, loss = 1.61794
I0520 21:46:26.104589 20835 solver.cpp:253]     Train net output #0: loss = 1.61794 (* 1 = 1.61794 loss)
I0520 21:46:26.104606 20835 sgd_solver.cpp:106] Iteration 2835, lr = 0.0025
I0520 21:46:34.191215 20835 solver.cpp:237] Iteration 2880, loss = 1.4989
I0520 21:46:34.191251 20835 solver.cpp:253]     Train net output #0: loss = 1.4989 (* 1 = 1.4989 loss)
I0520 21:46:34.191272 20835 sgd_solver.cpp:106] Iteration 2880, lr = 0.0025
I0520 21:46:42.275120 20835 solver.cpp:237] Iteration 2925, loss = 1.52379
I0520 21:46:42.275153 20835 solver.cpp:253]     Train net output #0: loss = 1.52379 (* 1 = 1.52379 loss)
I0520 21:46:42.275169 20835 sgd_solver.cpp:106] Iteration 2925, lr = 0.0025
I0520 21:46:50.360803 20835 solver.cpp:237] Iteration 2970, loss = 1.40927
I0520 21:46:50.360954 20835 solver.cpp:253]     Train net output #0: loss = 1.40927 (* 1 = 1.40927 loss)
I0520 21:46:50.360966 20835 sgd_solver.cpp:106] Iteration 2970, lr = 0.0025
I0520 21:46:58.446760 20835 solver.cpp:237] Iteration 3015, loss = 1.54554
I0520 21:46:58.446801 20835 solver.cpp:253]     Train net output #0: loss = 1.54554 (* 1 = 1.54554 loss)
I0520 21:46:58.446818 20835 sgd_solver.cpp:106] Iteration 3015, lr = 0.0025
I0520 21:47:28.684075 20835 solver.cpp:237] Iteration 3060, loss = 1.54637
I0520 21:47:28.684245 20835 solver.cpp:253]     Train net output #0: loss = 1.54637 (* 1 = 1.54637 loss)
I0520 21:47:28.684260 20835 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0520 21:47:36.764725 20835 solver.cpp:237] Iteration 3105, loss = 1.51853
I0520 21:47:36.764757 20835 solver.cpp:253]     Train net output #0: loss = 1.51853 (* 1 = 1.51853 loss)
I0520 21:47:36.764772 20835 sgd_solver.cpp:106] Iteration 3105, lr = 0.0025
I0520 21:47:44.849022 20835 solver.cpp:237] Iteration 3150, loss = 1.53513
I0520 21:47:44.849069 20835 solver.cpp:253]     Train net output #0: loss = 1.53513 (* 1 = 1.53513 loss)
I0520 21:47:44.849086 20835 sgd_solver.cpp:106] Iteration 3150, lr = 0.0025
I0520 21:47:49.698448 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_3178.caffemodel
I0520 21:47:49.862103 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_3178.solverstate
I0520 21:47:52.995163 20835 solver.cpp:237] Iteration 3195, loss = 1.56358
I0520 21:47:52.995205 20835 solver.cpp:253]     Train net output #0: loss = 1.56358 (* 1 = 1.56358 loss)
I0520 21:47:52.995225 20835 sgd_solver.cpp:106] Iteration 3195, lr = 0.0025
I0520 21:48:01.078706 20835 solver.cpp:237] Iteration 3240, loss = 1.46417
I0520 21:48:01.078850 20835 solver.cpp:253]     Train net output #0: loss = 1.46417 (* 1 = 1.46417 loss)
I0520 21:48:01.078863 20835 sgd_solver.cpp:106] Iteration 3240, lr = 0.0025
I0520 21:48:09.163466 20835 solver.cpp:237] Iteration 3285, loss = 1.43254
I0520 21:48:09.163504 20835 solver.cpp:253]     Train net output #0: loss = 1.43254 (* 1 = 1.43254 loss)
I0520 21:48:09.163522 20835 sgd_solver.cpp:106] Iteration 3285, lr = 0.0025
I0520 21:48:17.248294 20835 solver.cpp:237] Iteration 3330, loss = 1.38235
I0520 21:48:17.248328 20835 solver.cpp:253]     Train net output #0: loss = 1.38235 (* 1 = 1.38235 loss)
I0520 21:48:17.248345 20835 sgd_solver.cpp:106] Iteration 3330, lr = 0.0025
I0520 21:48:47.559425 20835 solver.cpp:237] Iteration 3375, loss = 1.54315
I0520 21:48:47.559593 20835 solver.cpp:253]     Train net output #0: loss = 1.54315 (* 1 = 1.54315 loss)
I0520 21:48:47.559607 20835 sgd_solver.cpp:106] Iteration 3375, lr = 0.0025
I0520 21:48:55.643076 20835 solver.cpp:237] Iteration 3420, loss = 1.47594
I0520 21:48:55.643108 20835 solver.cpp:253]     Train net output #0: loss = 1.47594 (* 1 = 1.47594 loss)
I0520 21:48:55.643126 20835 sgd_solver.cpp:106] Iteration 3420, lr = 0.0025
I0520 21:49:03.725399 20835 solver.cpp:237] Iteration 3465, loss = 1.5459
I0520 21:49:03.725445 20835 solver.cpp:253]     Train net output #0: loss = 1.5459 (* 1 = 1.5459 loss)
I0520 21:49:03.725461 20835 sgd_solver.cpp:106] Iteration 3465, lr = 0.0025
I0520 21:49:11.811365 20835 solver.cpp:237] Iteration 3510, loss = 1.49713
I0520 21:49:11.811398 20835 solver.cpp:253]     Train net output #0: loss = 1.49713 (* 1 = 1.49713 loss)
I0520 21:49:11.811415 20835 sgd_solver.cpp:106] Iteration 3510, lr = 0.0025
I0520 21:49:19.894798 20835 solver.cpp:237] Iteration 3555, loss = 1.49988
I0520 21:49:19.894937 20835 solver.cpp:253]     Train net output #0: loss = 1.49988 (* 1 = 1.49988 loss)
I0520 21:49:19.894950 20835 sgd_solver.cpp:106] Iteration 3555, lr = 0.0025
I0520 21:49:27.980957 20835 solver.cpp:237] Iteration 3600, loss = 1.50648
I0520 21:49:27.980996 20835 solver.cpp:253]     Train net output #0: loss = 1.50648 (* 1 = 1.50648 loss)
I0520 21:49:27.981017 20835 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0520 21:49:33.551143 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_3632.caffemodel
I0520 21:49:33.715039 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_3632.solverstate
I0520 21:49:34.333169 20835 solver.cpp:341] Iteration 3636, Testing net (#0)
I0520 21:50:40.686457 20835 solver.cpp:409]     Test net output #0: accuracy = 0.770945
I0520 21:50:40.686630 20835 solver.cpp:409]     Test net output #1: loss = 0.83347 (* 1 = 0.83347 loss)
I0520 21:51:04.550150 20835 solver.cpp:237] Iteration 3645, loss = 1.48894
I0520 21:51:04.550199 20835 solver.cpp:253]     Train net output #0: loss = 1.48894 (* 1 = 1.48894 loss)
I0520 21:51:04.550216 20835 sgd_solver.cpp:106] Iteration 3645, lr = 0.0025
I0520 21:51:12.632768 20835 solver.cpp:237] Iteration 3690, loss = 1.48715
I0520 21:51:12.632921 20835 solver.cpp:253]     Train net output #0: loss = 1.48715 (* 1 = 1.48715 loss)
I0520 21:51:12.632935 20835 sgd_solver.cpp:106] Iteration 3690, lr = 0.0025
I0520 21:51:20.717471 20835 solver.cpp:237] Iteration 3735, loss = 1.49923
I0520 21:51:20.717505 20835 solver.cpp:253]     Train net output #0: loss = 1.49923 (* 1 = 1.49923 loss)
I0520 21:51:20.717520 20835 sgd_solver.cpp:106] Iteration 3735, lr = 0.0025
I0520 21:51:28.797394 20835 solver.cpp:237] Iteration 3780, loss = 1.6172
I0520 21:51:28.797441 20835 solver.cpp:253]     Train net output #0: loss = 1.6172 (* 1 = 1.6172 loss)
I0520 21:51:28.797456 20835 sgd_solver.cpp:106] Iteration 3780, lr = 0.0025
I0520 21:51:36.881739 20835 solver.cpp:237] Iteration 3825, loss = 1.37491
I0520 21:51:36.881773 20835 solver.cpp:253]     Train net output #0: loss = 1.37491 (* 1 = 1.37491 loss)
I0520 21:51:36.881789 20835 sgd_solver.cpp:106] Iteration 3825, lr = 0.0025
I0520 21:51:44.962653 20835 solver.cpp:237] Iteration 3870, loss = 1.4516
I0520 21:51:44.962795 20835 solver.cpp:253]     Train net output #0: loss = 1.4516 (* 1 = 1.4516 loss)
I0520 21:51:44.962807 20835 sgd_solver.cpp:106] Iteration 3870, lr = 0.0025
I0520 21:51:53.045742 20835 solver.cpp:237] Iteration 3915, loss = 1.44687
I0520 21:51:53.045774 20835 solver.cpp:253]     Train net output #0: loss = 1.44687 (* 1 = 1.44687 loss)
I0520 21:51:53.045794 20835 sgd_solver.cpp:106] Iteration 3915, lr = 0.0025
I0520 21:52:23.258826 20835 solver.cpp:237] Iteration 3960, loss = 1.61382
I0520 21:52:23.258998 20835 solver.cpp:253]     Train net output #0: loss = 1.61382 (* 1 = 1.61382 loss)
I0520 21:52:23.259014 20835 sgd_solver.cpp:106] Iteration 3960, lr = 0.0025
I0520 21:52:31.339756 20835 solver.cpp:237] Iteration 4005, loss = 1.36969
I0520 21:52:31.339787 20835 solver.cpp:253]     Train net output #0: loss = 1.36969 (* 1 = 1.36969 loss)
I0520 21:52:31.339802 20835 sgd_solver.cpp:106] Iteration 4005, lr = 0.0025
I0520 21:52:39.421903 20835 solver.cpp:237] Iteration 4050, loss = 1.50776
I0520 21:52:39.421937 20835 solver.cpp:253]     Train net output #0: loss = 1.50776 (* 1 = 1.50776 loss)
I0520 21:52:39.421954 20835 sgd_solver.cpp:106] Iteration 4050, lr = 0.0025
I0520 21:52:45.714303 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_4086.caffemodel
I0520 21:52:45.879292 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_4086.solverstate
I0520 21:52:47.577536 20835 solver.cpp:237] Iteration 4095, loss = 1.46765
I0520 21:52:47.577584 20835 solver.cpp:253]     Train net output #0: loss = 1.46765 (* 1 = 1.46765 loss)
I0520 21:52:47.577596 20835 sgd_solver.cpp:106] Iteration 4095, lr = 0.0025
I0520 21:52:55.664996 20835 solver.cpp:237] Iteration 4140, loss = 1.5472
I0520 21:52:55.665155 20835 solver.cpp:253]     Train net output #0: loss = 1.5472 (* 1 = 1.5472 loss)
I0520 21:52:55.665169 20835 sgd_solver.cpp:106] Iteration 4140, lr = 0.0025
I0520 21:53:03.748644 20835 solver.cpp:237] Iteration 4185, loss = 1.3434
I0520 21:53:03.748677 20835 solver.cpp:253]     Train net output #0: loss = 1.3434 (* 1 = 1.3434 loss)
I0520 21:53:03.748693 20835 sgd_solver.cpp:106] Iteration 4185, lr = 0.0025
I0520 21:53:11.827272 20835 solver.cpp:237] Iteration 4230, loss = 1.34065
I0520 21:53:11.827322 20835 solver.cpp:253]     Train net output #0: loss = 1.34065 (* 1 = 1.34065 loss)
I0520 21:53:11.827337 20835 sgd_solver.cpp:106] Iteration 4230, lr = 0.0025
I0520 21:53:42.074283 20835 solver.cpp:237] Iteration 4275, loss = 1.46106
I0520 21:53:42.074456 20835 solver.cpp:253]     Train net output #0: loss = 1.46106 (* 1 = 1.46106 loss)
I0520 21:53:42.074472 20835 sgd_solver.cpp:106] Iteration 4275, lr = 0.0025
I0520 21:53:50.157361 20835 solver.cpp:237] Iteration 4320, loss = 1.41899
I0520 21:53:50.157393 20835 solver.cpp:253]     Train net output #0: loss = 1.41899 (* 1 = 1.41899 loss)
I0520 21:53:50.157407 20835 sgd_solver.cpp:106] Iteration 4320, lr = 0.0025
I0520 21:53:58.245273 20835 solver.cpp:237] Iteration 4365, loss = 1.2986
I0520 21:53:58.245308 20835 solver.cpp:253]     Train net output #0: loss = 1.2986 (* 1 = 1.2986 loss)
I0520 21:53:58.245324 20835 sgd_solver.cpp:106] Iteration 4365, lr = 0.0025
I0520 21:54:06.328204 20835 solver.cpp:237] Iteration 4410, loss = 1.47086
I0520 21:54:06.328233 20835 solver.cpp:253]     Train net output #0: loss = 1.47086 (* 1 = 1.47086 loss)
I0520 21:54:06.328246 20835 sgd_solver.cpp:106] Iteration 4410, lr = 0.0025
I0520 21:54:14.408993 20835 solver.cpp:237] Iteration 4455, loss = 1.49377
I0520 21:54:14.409132 20835 solver.cpp:253]     Train net output #0: loss = 1.49377 (* 1 = 1.49377 loss)
I0520 21:54:14.409145 20835 sgd_solver.cpp:106] Iteration 4455, lr = 0.0025
I0520 21:54:22.493947 20835 solver.cpp:237] Iteration 4500, loss = 1.42978
I0520 21:54:22.493980 20835 solver.cpp:253]     Train net output #0: loss = 1.42978 (* 1 = 1.42978 loss)
I0520 21:54:22.493993 20835 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0520 21:54:29.498975 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_4540.caffemodel
I0520 21:54:29.664618 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_4540.solverstate
I0520 21:54:30.465545 20835 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_4545.caffemodel
I0520 21:54:30.631259 20835 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450_iter_4545.solverstate
I0520 21:54:51.599970 20835 solver.cpp:321] Iteration 4545, loss = 1.38053
I0520 21:54:51.600134 20835 solver.cpp:341] Iteration 4545, Testing net (#0)
I0520 21:55:36.948032 20835 solver.cpp:409]     Test net output #0: accuracy = 0.793786
I0520 21:55:36.948199 20835 solver.cpp:409]     Test net output #1: loss = 0.8047 (* 1 = 0.8047 loss)
I0520 21:55:36.948213 20835 solver.cpp:326] Optimization Done.
I0520 21:55:36.948225 20835 caffe.cpp:215] Optimization Done.
Application 11235516 resources: utime ~1279s, stime ~229s, Rss ~5329476, inblocks ~3744348, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_330_2016-05-20T11.20.44.760450.solver"
	User time (seconds): 0.54
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:15.40
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 2
	Minor (reclaiming a frame) page faults: 15073
	Voluntary context switches: 2780
	Involuntary context switches: 80
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
