2805234
I0520 15:02:45.327749 13563 caffe.cpp:184] Using GPUs 0
I0520 15:02:45.759351 13563 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 2000
base_lr: 0.0025
display: 100
max_iter: 10000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657.prototxt"
I0520 15:02:45.760885 13563 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657.prototxt
I0520 15:02:45.781107 13563 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 15:02:45.781167 13563 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 15:02:45.781512 13563 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 150
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:02:45.781693 13563 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:02:45.781718 13563 net.cpp:106] Creating Layer data_hdf5
I0520 15:02:45.781733 13563 net.cpp:411] data_hdf5 -> data
I0520 15:02:45.781766 13563 net.cpp:411] data_hdf5 -> label
I0520 15:02:45.781798 13563 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 15:02:45.783026 13563 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 15:02:45.785159 13563 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 15:03:07.351069 13563 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 15:03:07.356191 13563 net.cpp:150] Setting up data_hdf5
I0520 15:03:07.356231 13563 net.cpp:157] Top shape: 150 1 127 50 (952500)
I0520 15:03:07.356246 13563 net.cpp:157] Top shape: 150 (150)
I0520 15:03:07.356259 13563 net.cpp:165] Memory required for data: 3810600
I0520 15:03:07.356272 13563 layer_factory.hpp:77] Creating layer conv1
I0520 15:03:07.356307 13563 net.cpp:106] Creating Layer conv1
I0520 15:03:07.356318 13563 net.cpp:454] conv1 <- data
I0520 15:03:07.356339 13563 net.cpp:411] conv1 -> conv1
I0520 15:03:08.798861 13563 net.cpp:150] Setting up conv1
I0520 15:03:08.798908 13563 net.cpp:157] Top shape: 150 12 120 48 (10368000)
I0520 15:03:08.798919 13563 net.cpp:165] Memory required for data: 45282600
I0520 15:03:08.798949 13563 layer_factory.hpp:77] Creating layer relu1
I0520 15:03:08.798971 13563 net.cpp:106] Creating Layer relu1
I0520 15:03:08.798981 13563 net.cpp:454] relu1 <- conv1
I0520 15:03:08.798995 13563 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:03:08.799518 13563 net.cpp:150] Setting up relu1
I0520 15:03:08.799535 13563 net.cpp:157] Top shape: 150 12 120 48 (10368000)
I0520 15:03:08.799546 13563 net.cpp:165] Memory required for data: 86754600
I0520 15:03:08.799556 13563 layer_factory.hpp:77] Creating layer pool1
I0520 15:03:08.799572 13563 net.cpp:106] Creating Layer pool1
I0520 15:03:08.799583 13563 net.cpp:454] pool1 <- conv1
I0520 15:03:08.799595 13563 net.cpp:411] pool1 -> pool1
I0520 15:03:08.799675 13563 net.cpp:150] Setting up pool1
I0520 15:03:08.799690 13563 net.cpp:157] Top shape: 150 12 60 48 (5184000)
I0520 15:03:08.799700 13563 net.cpp:165] Memory required for data: 107490600
I0520 15:03:08.799710 13563 layer_factory.hpp:77] Creating layer conv2
I0520 15:03:08.799731 13563 net.cpp:106] Creating Layer conv2
I0520 15:03:08.799742 13563 net.cpp:454] conv2 <- pool1
I0520 15:03:08.799757 13563 net.cpp:411] conv2 -> conv2
I0520 15:03:08.802435 13563 net.cpp:150] Setting up conv2
I0520 15:03:08.802464 13563 net.cpp:157] Top shape: 150 20 54 46 (7452000)
I0520 15:03:08.802474 13563 net.cpp:165] Memory required for data: 137298600
I0520 15:03:08.802494 13563 layer_factory.hpp:77] Creating layer relu2
I0520 15:03:08.802507 13563 net.cpp:106] Creating Layer relu2
I0520 15:03:08.802517 13563 net.cpp:454] relu2 <- conv2
I0520 15:03:08.802531 13563 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:03:08.802860 13563 net.cpp:150] Setting up relu2
I0520 15:03:08.802873 13563 net.cpp:157] Top shape: 150 20 54 46 (7452000)
I0520 15:03:08.802883 13563 net.cpp:165] Memory required for data: 167106600
I0520 15:03:08.802893 13563 layer_factory.hpp:77] Creating layer pool2
I0520 15:03:08.802906 13563 net.cpp:106] Creating Layer pool2
I0520 15:03:08.802917 13563 net.cpp:454] pool2 <- conv2
I0520 15:03:08.802942 13563 net.cpp:411] pool2 -> pool2
I0520 15:03:08.803010 13563 net.cpp:150] Setting up pool2
I0520 15:03:08.803023 13563 net.cpp:157] Top shape: 150 20 27 46 (3726000)
I0520 15:03:08.803033 13563 net.cpp:165] Memory required for data: 182010600
I0520 15:03:08.803041 13563 layer_factory.hpp:77] Creating layer conv3
I0520 15:03:08.803061 13563 net.cpp:106] Creating Layer conv3
I0520 15:03:08.803071 13563 net.cpp:454] conv3 <- pool2
I0520 15:03:08.803084 13563 net.cpp:411] conv3 -> conv3
I0520 15:03:08.805047 13563 net.cpp:150] Setting up conv3
I0520 15:03:08.805069 13563 net.cpp:157] Top shape: 150 28 22 44 (4065600)
I0520 15:03:08.805080 13563 net.cpp:165] Memory required for data: 198273000
I0520 15:03:08.805099 13563 layer_factory.hpp:77] Creating layer relu3
I0520 15:03:08.805115 13563 net.cpp:106] Creating Layer relu3
I0520 15:03:08.805125 13563 net.cpp:454] relu3 <- conv3
I0520 15:03:08.805138 13563 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:03:08.805608 13563 net.cpp:150] Setting up relu3
I0520 15:03:08.805625 13563 net.cpp:157] Top shape: 150 28 22 44 (4065600)
I0520 15:03:08.805635 13563 net.cpp:165] Memory required for data: 214535400
I0520 15:03:08.805645 13563 layer_factory.hpp:77] Creating layer pool3
I0520 15:03:08.805658 13563 net.cpp:106] Creating Layer pool3
I0520 15:03:08.805668 13563 net.cpp:454] pool3 <- conv3
I0520 15:03:08.805681 13563 net.cpp:411] pool3 -> pool3
I0520 15:03:08.805748 13563 net.cpp:150] Setting up pool3
I0520 15:03:08.805763 13563 net.cpp:157] Top shape: 150 28 11 44 (2032800)
I0520 15:03:08.805773 13563 net.cpp:165] Memory required for data: 222666600
I0520 15:03:08.805780 13563 layer_factory.hpp:77] Creating layer conv4
I0520 15:03:08.805799 13563 net.cpp:106] Creating Layer conv4
I0520 15:03:08.805809 13563 net.cpp:454] conv4 <- pool3
I0520 15:03:08.805822 13563 net.cpp:411] conv4 -> conv4
I0520 15:03:08.808771 13563 net.cpp:150] Setting up conv4
I0520 15:03:08.808800 13563 net.cpp:157] Top shape: 150 36 6 42 (1360800)
I0520 15:03:08.808810 13563 net.cpp:165] Memory required for data: 228109800
I0520 15:03:08.808826 13563 layer_factory.hpp:77] Creating layer relu4
I0520 15:03:08.808841 13563 net.cpp:106] Creating Layer relu4
I0520 15:03:08.808851 13563 net.cpp:454] relu4 <- conv4
I0520 15:03:08.808864 13563 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:03:08.809329 13563 net.cpp:150] Setting up relu4
I0520 15:03:08.809345 13563 net.cpp:157] Top shape: 150 36 6 42 (1360800)
I0520 15:03:08.809355 13563 net.cpp:165] Memory required for data: 233553000
I0520 15:03:08.809365 13563 layer_factory.hpp:77] Creating layer pool4
I0520 15:03:08.809378 13563 net.cpp:106] Creating Layer pool4
I0520 15:03:08.809388 13563 net.cpp:454] pool4 <- conv4
I0520 15:03:08.809401 13563 net.cpp:411] pool4 -> pool4
I0520 15:03:08.809469 13563 net.cpp:150] Setting up pool4
I0520 15:03:08.809484 13563 net.cpp:157] Top shape: 150 36 3 42 (680400)
I0520 15:03:08.809492 13563 net.cpp:165] Memory required for data: 236274600
I0520 15:03:08.809502 13563 layer_factory.hpp:77] Creating layer ip1
I0520 15:03:08.809523 13563 net.cpp:106] Creating Layer ip1
I0520 15:03:08.809535 13563 net.cpp:454] ip1 <- pool4
I0520 15:03:08.809545 13563 net.cpp:411] ip1 -> ip1
I0520 15:03:08.824982 13563 net.cpp:150] Setting up ip1
I0520 15:03:08.825012 13563 net.cpp:157] Top shape: 150 196 (29400)
I0520 15:03:08.825023 13563 net.cpp:165] Memory required for data: 236392200
I0520 15:03:08.825050 13563 layer_factory.hpp:77] Creating layer relu5
I0520 15:03:08.825065 13563 net.cpp:106] Creating Layer relu5
I0520 15:03:08.825075 13563 net.cpp:454] relu5 <- ip1
I0520 15:03:08.825089 13563 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:03:08.825472 13563 net.cpp:150] Setting up relu5
I0520 15:03:08.825486 13563 net.cpp:157] Top shape: 150 196 (29400)
I0520 15:03:08.825496 13563 net.cpp:165] Memory required for data: 236509800
I0520 15:03:08.825506 13563 layer_factory.hpp:77] Creating layer drop1
I0520 15:03:08.825527 13563 net.cpp:106] Creating Layer drop1
I0520 15:03:08.825538 13563 net.cpp:454] drop1 <- ip1
I0520 15:03:08.825562 13563 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:03:08.825610 13563 net.cpp:150] Setting up drop1
I0520 15:03:08.825623 13563 net.cpp:157] Top shape: 150 196 (29400)
I0520 15:03:08.825634 13563 net.cpp:165] Memory required for data: 236627400
I0520 15:03:08.825642 13563 layer_factory.hpp:77] Creating layer ip2
I0520 15:03:08.825662 13563 net.cpp:106] Creating Layer ip2
I0520 15:03:08.825672 13563 net.cpp:454] ip2 <- ip1
I0520 15:03:08.825685 13563 net.cpp:411] ip2 -> ip2
I0520 15:03:08.826151 13563 net.cpp:150] Setting up ip2
I0520 15:03:08.826164 13563 net.cpp:157] Top shape: 150 98 (14700)
I0520 15:03:08.826174 13563 net.cpp:165] Memory required for data: 236686200
I0520 15:03:08.826190 13563 layer_factory.hpp:77] Creating layer relu6
I0520 15:03:08.826202 13563 net.cpp:106] Creating Layer relu6
I0520 15:03:08.826212 13563 net.cpp:454] relu6 <- ip2
I0520 15:03:08.826225 13563 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:03:08.826745 13563 net.cpp:150] Setting up relu6
I0520 15:03:08.826761 13563 net.cpp:157] Top shape: 150 98 (14700)
I0520 15:03:08.826771 13563 net.cpp:165] Memory required for data: 236745000
I0520 15:03:08.826781 13563 layer_factory.hpp:77] Creating layer drop2
I0520 15:03:08.826794 13563 net.cpp:106] Creating Layer drop2
I0520 15:03:08.826804 13563 net.cpp:454] drop2 <- ip2
I0520 15:03:08.826817 13563 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:03:08.826859 13563 net.cpp:150] Setting up drop2
I0520 15:03:08.826872 13563 net.cpp:157] Top shape: 150 98 (14700)
I0520 15:03:08.826884 13563 net.cpp:165] Memory required for data: 236803800
I0520 15:03:08.826894 13563 layer_factory.hpp:77] Creating layer ip3
I0520 15:03:08.826907 13563 net.cpp:106] Creating Layer ip3
I0520 15:03:08.826917 13563 net.cpp:454] ip3 <- ip2
I0520 15:03:08.826930 13563 net.cpp:411] ip3 -> ip3
I0520 15:03:08.827139 13563 net.cpp:150] Setting up ip3
I0520 15:03:08.827152 13563 net.cpp:157] Top shape: 150 11 (1650)
I0520 15:03:08.827162 13563 net.cpp:165] Memory required for data: 236810400
I0520 15:03:08.827177 13563 layer_factory.hpp:77] Creating layer drop3
I0520 15:03:08.827189 13563 net.cpp:106] Creating Layer drop3
I0520 15:03:08.827198 13563 net.cpp:454] drop3 <- ip3
I0520 15:03:08.827210 13563 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:03:08.827250 13563 net.cpp:150] Setting up drop3
I0520 15:03:08.827262 13563 net.cpp:157] Top shape: 150 11 (1650)
I0520 15:03:08.827272 13563 net.cpp:165] Memory required for data: 236817000
I0520 15:03:08.827282 13563 layer_factory.hpp:77] Creating layer loss
I0520 15:03:08.827302 13563 net.cpp:106] Creating Layer loss
I0520 15:03:08.827312 13563 net.cpp:454] loss <- ip3
I0520 15:03:08.827323 13563 net.cpp:454] loss <- label
I0520 15:03:08.827335 13563 net.cpp:411] loss -> loss
I0520 15:03:08.827359 13563 layer_factory.hpp:77] Creating layer loss
I0520 15:03:08.827999 13563 net.cpp:150] Setting up loss
I0520 15:03:08.828021 13563 net.cpp:157] Top shape: (1)
I0520 15:03:08.828033 13563 net.cpp:160]     with loss weight 1
I0520 15:03:08.828075 13563 net.cpp:165] Memory required for data: 236817004
I0520 15:03:08.828085 13563 net.cpp:226] loss needs backward computation.
I0520 15:03:08.828096 13563 net.cpp:226] drop3 needs backward computation.
I0520 15:03:08.828106 13563 net.cpp:226] ip3 needs backward computation.
I0520 15:03:08.828117 13563 net.cpp:226] drop2 needs backward computation.
I0520 15:03:08.828126 13563 net.cpp:226] relu6 needs backward computation.
I0520 15:03:08.828136 13563 net.cpp:226] ip2 needs backward computation.
I0520 15:03:08.828147 13563 net.cpp:226] drop1 needs backward computation.
I0520 15:03:08.828156 13563 net.cpp:226] relu5 needs backward computation.
I0520 15:03:08.828166 13563 net.cpp:226] ip1 needs backward computation.
I0520 15:03:08.828176 13563 net.cpp:226] pool4 needs backward computation.
I0520 15:03:08.828186 13563 net.cpp:226] relu4 needs backward computation.
I0520 15:03:08.828197 13563 net.cpp:226] conv4 needs backward computation.
I0520 15:03:08.828207 13563 net.cpp:226] pool3 needs backward computation.
I0520 15:03:08.828225 13563 net.cpp:226] relu3 needs backward computation.
I0520 15:03:08.828235 13563 net.cpp:226] conv3 needs backward computation.
I0520 15:03:08.828246 13563 net.cpp:226] pool2 needs backward computation.
I0520 15:03:08.828258 13563 net.cpp:226] relu2 needs backward computation.
I0520 15:03:08.828268 13563 net.cpp:226] conv2 needs backward computation.
I0520 15:03:08.828279 13563 net.cpp:226] pool1 needs backward computation.
I0520 15:03:08.828289 13563 net.cpp:226] relu1 needs backward computation.
I0520 15:03:08.828299 13563 net.cpp:226] conv1 needs backward computation.
I0520 15:03:08.828310 13563 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:03:08.828320 13563 net.cpp:270] This network produces output loss
I0520 15:03:08.828344 13563 net.cpp:283] Network initialization done.
I0520 15:03:08.829905 13563 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657.prototxt
I0520 15:03:08.829975 13563 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 15:03:08.830329 13563 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 150
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:03:08.830516 13563 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:03:08.830533 13563 net.cpp:106] Creating Layer data_hdf5
I0520 15:03:08.830544 13563 net.cpp:411] data_hdf5 -> data
I0520 15:03:08.830561 13563 net.cpp:411] data_hdf5 -> label
I0520 15:03:08.830577 13563 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 15:03:08.831817 13563 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 15:03:30.201679 13563 net.cpp:150] Setting up data_hdf5
I0520 15:03:30.201844 13563 net.cpp:157] Top shape: 150 1 127 50 (952500)
I0520 15:03:30.201858 13563 net.cpp:157] Top shape: 150 (150)
I0520 15:03:30.201869 13563 net.cpp:165] Memory required for data: 3810600
I0520 15:03:30.201884 13563 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 15:03:30.201910 13563 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 15:03:30.201921 13563 net.cpp:454] label_data_hdf5_1_split <- label
I0520 15:03:30.201936 13563 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 15:03:30.201957 13563 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 15:03:30.202030 13563 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 15:03:30.202044 13563 net.cpp:157] Top shape: 150 (150)
I0520 15:03:30.202056 13563 net.cpp:157] Top shape: 150 (150)
I0520 15:03:30.202066 13563 net.cpp:165] Memory required for data: 3811800
I0520 15:03:30.202076 13563 layer_factory.hpp:77] Creating layer conv1
I0520 15:03:30.202098 13563 net.cpp:106] Creating Layer conv1
I0520 15:03:30.202110 13563 net.cpp:454] conv1 <- data
I0520 15:03:30.202123 13563 net.cpp:411] conv1 -> conv1
I0520 15:03:30.204057 13563 net.cpp:150] Setting up conv1
I0520 15:03:30.204082 13563 net.cpp:157] Top shape: 150 12 120 48 (10368000)
I0520 15:03:30.204094 13563 net.cpp:165] Memory required for data: 45283800
I0520 15:03:30.204114 13563 layer_factory.hpp:77] Creating layer relu1
I0520 15:03:30.204128 13563 net.cpp:106] Creating Layer relu1
I0520 15:03:30.204138 13563 net.cpp:454] relu1 <- conv1
I0520 15:03:30.204151 13563 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:03:30.204650 13563 net.cpp:150] Setting up relu1
I0520 15:03:30.204666 13563 net.cpp:157] Top shape: 150 12 120 48 (10368000)
I0520 15:03:30.204676 13563 net.cpp:165] Memory required for data: 86755800
I0520 15:03:30.204687 13563 layer_factory.hpp:77] Creating layer pool1
I0520 15:03:30.204702 13563 net.cpp:106] Creating Layer pool1
I0520 15:03:30.204712 13563 net.cpp:454] pool1 <- conv1
I0520 15:03:30.204725 13563 net.cpp:411] pool1 -> pool1
I0520 15:03:30.204800 13563 net.cpp:150] Setting up pool1
I0520 15:03:30.204813 13563 net.cpp:157] Top shape: 150 12 60 48 (5184000)
I0520 15:03:30.204823 13563 net.cpp:165] Memory required for data: 107491800
I0520 15:03:30.204833 13563 layer_factory.hpp:77] Creating layer conv2
I0520 15:03:30.204849 13563 net.cpp:106] Creating Layer conv2
I0520 15:03:30.204859 13563 net.cpp:454] conv2 <- pool1
I0520 15:03:30.204874 13563 net.cpp:411] conv2 -> conv2
I0520 15:03:30.206784 13563 net.cpp:150] Setting up conv2
I0520 15:03:30.206805 13563 net.cpp:157] Top shape: 150 20 54 46 (7452000)
I0520 15:03:30.206820 13563 net.cpp:165] Memory required for data: 137299800
I0520 15:03:30.206836 13563 layer_factory.hpp:77] Creating layer relu2
I0520 15:03:30.206850 13563 net.cpp:106] Creating Layer relu2
I0520 15:03:30.206859 13563 net.cpp:454] relu2 <- conv2
I0520 15:03:30.206871 13563 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:03:30.207206 13563 net.cpp:150] Setting up relu2
I0520 15:03:30.207221 13563 net.cpp:157] Top shape: 150 20 54 46 (7452000)
I0520 15:03:30.207231 13563 net.cpp:165] Memory required for data: 167107800
I0520 15:03:30.207240 13563 layer_factory.hpp:77] Creating layer pool2
I0520 15:03:30.207253 13563 net.cpp:106] Creating Layer pool2
I0520 15:03:30.207263 13563 net.cpp:454] pool2 <- conv2
I0520 15:03:30.207276 13563 net.cpp:411] pool2 -> pool2
I0520 15:03:30.207355 13563 net.cpp:150] Setting up pool2
I0520 15:03:30.207368 13563 net.cpp:157] Top shape: 150 20 27 46 (3726000)
I0520 15:03:30.207378 13563 net.cpp:165] Memory required for data: 182011800
I0520 15:03:30.207388 13563 layer_factory.hpp:77] Creating layer conv3
I0520 15:03:30.207406 13563 net.cpp:106] Creating Layer conv3
I0520 15:03:30.207417 13563 net.cpp:454] conv3 <- pool2
I0520 15:03:30.207430 13563 net.cpp:411] conv3 -> conv3
I0520 15:03:30.209414 13563 net.cpp:150] Setting up conv3
I0520 15:03:30.209434 13563 net.cpp:157] Top shape: 150 28 22 44 (4065600)
I0520 15:03:30.209444 13563 net.cpp:165] Memory required for data: 198274200
I0520 15:03:30.209476 13563 layer_factory.hpp:77] Creating layer relu3
I0520 15:03:30.209491 13563 net.cpp:106] Creating Layer relu3
I0520 15:03:30.209501 13563 net.cpp:454] relu3 <- conv3
I0520 15:03:30.209513 13563 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:03:30.209981 13563 net.cpp:150] Setting up relu3
I0520 15:03:30.209997 13563 net.cpp:157] Top shape: 150 28 22 44 (4065600)
I0520 15:03:30.210008 13563 net.cpp:165] Memory required for data: 214536600
I0520 15:03:30.210018 13563 layer_factory.hpp:77] Creating layer pool3
I0520 15:03:30.210031 13563 net.cpp:106] Creating Layer pool3
I0520 15:03:30.210041 13563 net.cpp:454] pool3 <- conv3
I0520 15:03:30.210057 13563 net.cpp:411] pool3 -> pool3
I0520 15:03:30.210127 13563 net.cpp:150] Setting up pool3
I0520 15:03:30.210140 13563 net.cpp:157] Top shape: 150 28 11 44 (2032800)
I0520 15:03:30.210150 13563 net.cpp:165] Memory required for data: 222667800
I0520 15:03:30.210160 13563 layer_factory.hpp:77] Creating layer conv4
I0520 15:03:30.210177 13563 net.cpp:106] Creating Layer conv4
I0520 15:03:30.210188 13563 net.cpp:454] conv4 <- pool3
I0520 15:03:30.210202 13563 net.cpp:411] conv4 -> conv4
I0520 15:03:30.212287 13563 net.cpp:150] Setting up conv4
I0520 15:03:30.212309 13563 net.cpp:157] Top shape: 150 36 6 42 (1360800)
I0520 15:03:30.212322 13563 net.cpp:165] Memory required for data: 228111000
I0520 15:03:30.212337 13563 layer_factory.hpp:77] Creating layer relu4
I0520 15:03:30.212350 13563 net.cpp:106] Creating Layer relu4
I0520 15:03:30.212360 13563 net.cpp:454] relu4 <- conv4
I0520 15:03:30.212373 13563 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:03:30.212847 13563 net.cpp:150] Setting up relu4
I0520 15:03:30.212863 13563 net.cpp:157] Top shape: 150 36 6 42 (1360800)
I0520 15:03:30.212873 13563 net.cpp:165] Memory required for data: 233554200
I0520 15:03:30.212883 13563 layer_factory.hpp:77] Creating layer pool4
I0520 15:03:30.212898 13563 net.cpp:106] Creating Layer pool4
I0520 15:03:30.212906 13563 net.cpp:454] pool4 <- conv4
I0520 15:03:30.212919 13563 net.cpp:411] pool4 -> pool4
I0520 15:03:30.212990 13563 net.cpp:150] Setting up pool4
I0520 15:03:30.213003 13563 net.cpp:157] Top shape: 150 36 3 42 (680400)
I0520 15:03:30.213013 13563 net.cpp:165] Memory required for data: 236275800
I0520 15:03:30.213023 13563 layer_factory.hpp:77] Creating layer ip1
I0520 15:03:30.213039 13563 net.cpp:106] Creating Layer ip1
I0520 15:03:30.213049 13563 net.cpp:454] ip1 <- pool4
I0520 15:03:30.213063 13563 net.cpp:411] ip1 -> ip1
I0520 15:03:30.228479 13563 net.cpp:150] Setting up ip1
I0520 15:03:30.228508 13563 net.cpp:157] Top shape: 150 196 (29400)
I0520 15:03:30.228519 13563 net.cpp:165] Memory required for data: 236393400
I0520 15:03:30.228541 13563 layer_factory.hpp:77] Creating layer relu5
I0520 15:03:30.228556 13563 net.cpp:106] Creating Layer relu5
I0520 15:03:30.228567 13563 net.cpp:454] relu5 <- ip1
I0520 15:03:30.228580 13563 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:03:30.228929 13563 net.cpp:150] Setting up relu5
I0520 15:03:30.228942 13563 net.cpp:157] Top shape: 150 196 (29400)
I0520 15:03:30.228952 13563 net.cpp:165] Memory required for data: 236511000
I0520 15:03:30.228962 13563 layer_factory.hpp:77] Creating layer drop1
I0520 15:03:30.228982 13563 net.cpp:106] Creating Layer drop1
I0520 15:03:30.228992 13563 net.cpp:454] drop1 <- ip1
I0520 15:03:30.229006 13563 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:03:30.229050 13563 net.cpp:150] Setting up drop1
I0520 15:03:30.229064 13563 net.cpp:157] Top shape: 150 196 (29400)
I0520 15:03:30.229074 13563 net.cpp:165] Memory required for data: 236628600
I0520 15:03:30.229084 13563 layer_factory.hpp:77] Creating layer ip2
I0520 15:03:30.229099 13563 net.cpp:106] Creating Layer ip2
I0520 15:03:30.229109 13563 net.cpp:454] ip2 <- ip1
I0520 15:03:30.229120 13563 net.cpp:411] ip2 -> ip2
I0520 15:03:30.229599 13563 net.cpp:150] Setting up ip2
I0520 15:03:30.229614 13563 net.cpp:157] Top shape: 150 98 (14700)
I0520 15:03:30.229622 13563 net.cpp:165] Memory required for data: 236687400
I0520 15:03:30.229650 13563 layer_factory.hpp:77] Creating layer relu6
I0520 15:03:30.229665 13563 net.cpp:106] Creating Layer relu6
I0520 15:03:30.229674 13563 net.cpp:454] relu6 <- ip2
I0520 15:03:30.229686 13563 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:03:30.230222 13563 net.cpp:150] Setting up relu6
I0520 15:03:30.230243 13563 net.cpp:157] Top shape: 150 98 (14700)
I0520 15:03:30.230253 13563 net.cpp:165] Memory required for data: 236746200
I0520 15:03:30.230263 13563 layer_factory.hpp:77] Creating layer drop2
I0520 15:03:30.230278 13563 net.cpp:106] Creating Layer drop2
I0520 15:03:30.230288 13563 net.cpp:454] drop2 <- ip2
I0520 15:03:30.230300 13563 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:03:30.230345 13563 net.cpp:150] Setting up drop2
I0520 15:03:30.230357 13563 net.cpp:157] Top shape: 150 98 (14700)
I0520 15:03:30.230368 13563 net.cpp:165] Memory required for data: 236805000
I0520 15:03:30.230378 13563 layer_factory.hpp:77] Creating layer ip3
I0520 15:03:30.230391 13563 net.cpp:106] Creating Layer ip3
I0520 15:03:30.230401 13563 net.cpp:454] ip3 <- ip2
I0520 15:03:30.230414 13563 net.cpp:411] ip3 -> ip3
I0520 15:03:30.230635 13563 net.cpp:150] Setting up ip3
I0520 15:03:30.230648 13563 net.cpp:157] Top shape: 150 11 (1650)
I0520 15:03:30.230657 13563 net.cpp:165] Memory required for data: 236811600
I0520 15:03:30.230674 13563 layer_factory.hpp:77] Creating layer drop3
I0520 15:03:30.230687 13563 net.cpp:106] Creating Layer drop3
I0520 15:03:30.230696 13563 net.cpp:454] drop3 <- ip3
I0520 15:03:30.230710 13563 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:03:30.230751 13563 net.cpp:150] Setting up drop3
I0520 15:03:30.230762 13563 net.cpp:157] Top shape: 150 11 (1650)
I0520 15:03:30.230772 13563 net.cpp:165] Memory required for data: 236818200
I0520 15:03:30.230782 13563 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 15:03:30.230795 13563 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 15:03:30.230805 13563 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 15:03:30.230818 13563 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 15:03:30.230832 13563 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 15:03:30.230906 13563 net.cpp:150] Setting up ip3_drop3_0_split
I0520 15:03:30.230918 13563 net.cpp:157] Top shape: 150 11 (1650)
I0520 15:03:30.230931 13563 net.cpp:157] Top shape: 150 11 (1650)
I0520 15:03:30.230940 13563 net.cpp:165] Memory required for data: 236831400
I0520 15:03:30.230948 13563 layer_factory.hpp:77] Creating layer accuracy
I0520 15:03:30.230970 13563 net.cpp:106] Creating Layer accuracy
I0520 15:03:30.230981 13563 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 15:03:30.230993 13563 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 15:03:30.231006 13563 net.cpp:411] accuracy -> accuracy
I0520 15:03:30.231030 13563 net.cpp:150] Setting up accuracy
I0520 15:03:30.231042 13563 net.cpp:157] Top shape: (1)
I0520 15:03:30.231052 13563 net.cpp:165] Memory required for data: 236831404
I0520 15:03:30.231062 13563 layer_factory.hpp:77] Creating layer loss
I0520 15:03:30.231077 13563 net.cpp:106] Creating Layer loss
I0520 15:03:30.231086 13563 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 15:03:30.231097 13563 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 15:03:30.231112 13563 net.cpp:411] loss -> loss
I0520 15:03:30.231129 13563 layer_factory.hpp:77] Creating layer loss
I0520 15:03:30.231622 13563 net.cpp:150] Setting up loss
I0520 15:03:30.231637 13563 net.cpp:157] Top shape: (1)
I0520 15:03:30.231645 13563 net.cpp:160]     with loss weight 1
I0520 15:03:30.231664 13563 net.cpp:165] Memory required for data: 236831408
I0520 15:03:30.231674 13563 net.cpp:226] loss needs backward computation.
I0520 15:03:30.231685 13563 net.cpp:228] accuracy does not need backward computation.
I0520 15:03:30.231696 13563 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 15:03:30.231706 13563 net.cpp:226] drop3 needs backward computation.
I0520 15:03:30.231714 13563 net.cpp:226] ip3 needs backward computation.
I0520 15:03:30.231724 13563 net.cpp:226] drop2 needs backward computation.
I0520 15:03:30.231744 13563 net.cpp:226] relu6 needs backward computation.
I0520 15:03:30.231753 13563 net.cpp:226] ip2 needs backward computation.
I0520 15:03:30.231763 13563 net.cpp:226] drop1 needs backward computation.
I0520 15:03:30.231772 13563 net.cpp:226] relu5 needs backward computation.
I0520 15:03:30.231782 13563 net.cpp:226] ip1 needs backward computation.
I0520 15:03:30.231792 13563 net.cpp:226] pool4 needs backward computation.
I0520 15:03:30.231802 13563 net.cpp:226] relu4 needs backward computation.
I0520 15:03:30.231812 13563 net.cpp:226] conv4 needs backward computation.
I0520 15:03:30.231822 13563 net.cpp:226] pool3 needs backward computation.
I0520 15:03:30.231832 13563 net.cpp:226] relu3 needs backward computation.
I0520 15:03:30.231840 13563 net.cpp:226] conv3 needs backward computation.
I0520 15:03:30.231851 13563 net.cpp:226] pool2 needs backward computation.
I0520 15:03:30.231861 13563 net.cpp:226] relu2 needs backward computation.
I0520 15:03:30.231873 13563 net.cpp:226] conv2 needs backward computation.
I0520 15:03:30.231883 13563 net.cpp:226] pool1 needs backward computation.
I0520 15:03:30.231892 13563 net.cpp:226] relu1 needs backward computation.
I0520 15:03:30.231902 13563 net.cpp:226] conv1 needs backward computation.
I0520 15:03:30.231914 13563 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 15:03:30.231925 13563 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:03:30.231935 13563 net.cpp:270] This network produces output accuracy
I0520 15:03:30.231946 13563 net.cpp:270] This network produces output loss
I0520 15:03:30.231976 13563 net.cpp:283] Network initialization done.
I0520 15:03:30.232108 13563 solver.cpp:60] Solver scaffolding done.
I0520 15:03:30.233248 13563 caffe.cpp:212] Starting Optimization
I0520 15:03:30.233261 13563 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 15:03:30.233271 13563 solver.cpp:289] Learning Rate Policy: fixed
I0520 15:03:30.234336 13563 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 15:04:17.243312 13563 solver.cpp:409]     Test net output #0: accuracy = 0.0872001
I0520 15:04:17.243477 13563 solver.cpp:409]     Test net output #1: loss = 2.39771 (* 1 = 2.39771 loss)
I0520 15:04:17.284448 13563 solver.cpp:237] Iteration 0, loss = 2.39794
I0520 15:04:17.284484 13563 solver.cpp:253]     Train net output #0: loss = 2.39794 (* 1 = 2.39794 loss)
I0520 15:04:17.284502 13563 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 15:04:25.700520 13563 solver.cpp:237] Iteration 100, loss = 2.29301
I0520 15:04:25.700556 13563 solver.cpp:253]     Train net output #0: loss = 2.29301 (* 1 = 2.29301 loss)
I0520 15:04:25.700572 13563 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0520 15:04:34.115658 13563 solver.cpp:237] Iteration 200, loss = 2.33707
I0520 15:04:34.115694 13563 solver.cpp:253]     Train net output #0: loss = 2.33707 (* 1 = 2.33707 loss)
I0520 15:04:34.115715 13563 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0520 15:04:42.535995 13563 solver.cpp:237] Iteration 300, loss = 2.22691
I0520 15:04:42.536031 13563 solver.cpp:253]     Train net output #0: loss = 2.22691 (* 1 = 2.22691 loss)
I0520 15:04:42.536047 13563 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0520 15:04:50.961874 13563 solver.cpp:237] Iteration 400, loss = 2.10135
I0520 15:04:50.962023 13563 solver.cpp:253]     Train net output #0: loss = 2.10135 (* 1 = 2.10135 loss)
I0520 15:04:50.962038 13563 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0520 15:04:59.390455 13563 solver.cpp:237] Iteration 500, loss = 2.12262
I0520 15:04:59.390498 13563 solver.cpp:253]     Train net output #0: loss = 2.12262 (* 1 = 2.12262 loss)
I0520 15:04:59.390517 13563 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0520 15:05:07.813591 13563 solver.cpp:237] Iteration 600, loss = 1.83562
I0520 15:05:07.813627 13563 solver.cpp:253]     Train net output #0: loss = 1.83562 (* 1 = 1.83562 loss)
I0520 15:05:07.813643 13563 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0520 15:05:38.366004 13563 solver.cpp:237] Iteration 700, loss = 1.94527
I0520 15:05:38.366165 13563 solver.cpp:253]     Train net output #0: loss = 1.94527 (* 1 = 1.94527 loss)
I0520 15:05:38.366183 13563 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0520 15:05:46.792670 13563 solver.cpp:237] Iteration 800, loss = 1.80332
I0520 15:05:46.792704 13563 solver.cpp:253]     Train net output #0: loss = 1.80332 (* 1 = 1.80332 loss)
I0520 15:05:46.792722 13563 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0520 15:05:55.202123 13563 solver.cpp:237] Iteration 900, loss = 2.00379
I0520 15:05:55.202167 13563 solver.cpp:253]     Train net output #0: loss = 2.00379 (* 1 = 2.00379 loss)
I0520 15:05:55.202184 13563 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0520 15:06:03.537199 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_1000.caffemodel
I0520 15:06:03.637282 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_1000.solverstate
I0520 15:06:03.689134 13563 solver.cpp:237] Iteration 1000, loss = 1.74741
I0520 15:06:03.689180 13563 solver.cpp:253]     Train net output #0: loss = 1.74741 (* 1 = 1.74741 loss)
I0520 15:06:03.689194 13563 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0520 15:06:12.096563 13563 solver.cpp:237] Iteration 1100, loss = 1.88252
I0520 15:06:12.096700 13563 solver.cpp:253]     Train net output #0: loss = 1.88252 (* 1 = 1.88252 loss)
I0520 15:06:12.096714 13563 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0520 15:06:20.513949 13563 solver.cpp:237] Iteration 1200, loss = 1.74552
I0520 15:06:20.513984 13563 solver.cpp:253]     Train net output #0: loss = 1.74552 (* 1 = 1.74552 loss)
I0520 15:06:20.514003 13563 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0520 15:06:28.923324 13563 solver.cpp:237] Iteration 1300, loss = 1.81179
I0520 15:06:28.923374 13563 solver.cpp:253]     Train net output #0: loss = 1.81179 (* 1 = 1.81179 loss)
I0520 15:06:28.923387 13563 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0520 15:06:59.449527 13563 solver.cpp:237] Iteration 1400, loss = 1.7738
I0520 15:06:59.449683 13563 solver.cpp:253]     Train net output #0: loss = 1.7738 (* 1 = 1.7738 loss)
I0520 15:06:59.449699 13563 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0520 15:07:07.865728 13563 solver.cpp:237] Iteration 1500, loss = 1.79147
I0520 15:07:07.865775 13563 solver.cpp:253]     Train net output #0: loss = 1.79147 (* 1 = 1.79147 loss)
I0520 15:07:07.865793 13563 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0520 15:07:16.286160 13563 solver.cpp:237] Iteration 1600, loss = 1.80949
I0520 15:07:16.286196 13563 solver.cpp:253]     Train net output #0: loss = 1.80949 (* 1 = 1.80949 loss)
I0520 15:07:16.286212 13563 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0520 15:07:24.702903 13563 solver.cpp:237] Iteration 1700, loss = 1.77324
I0520 15:07:24.702937 13563 solver.cpp:253]     Train net output #0: loss = 1.77324 (* 1 = 1.77324 loss)
I0520 15:07:24.702955 13563 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0520 15:07:33.127197 13563 solver.cpp:237] Iteration 1800, loss = 1.71626
I0520 15:07:33.127365 13563 solver.cpp:253]     Train net output #0: loss = 1.71626 (* 1 = 1.71626 loss)
I0520 15:07:33.127380 13563 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0520 15:07:41.537911 13563 solver.cpp:237] Iteration 1900, loss = 1.64477
I0520 15:07:41.537945 13563 solver.cpp:253]     Train net output #0: loss = 1.64477 (* 1 = 1.64477 loss)
I0520 15:07:41.537963 13563 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0520 15:07:49.862808 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_2000.caffemodel
I0520 15:07:49.959403 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_2000.solverstate
I0520 15:07:49.985605 13563 solver.cpp:341] Iteration 2000, Testing net (#0)
I0520 15:08:36.043236 13563 solver.cpp:409]     Test net output #0: accuracy = 0.663734
I0520 15:08:36.043406 13563 solver.cpp:409]     Test net output #1: loss = 1.14422 (* 1 = 1.14422 loss)
I0520 15:08:58.203465 13563 solver.cpp:237] Iteration 2000, loss = 1.59468
I0520 15:08:58.203518 13563 solver.cpp:253]     Train net output #0: loss = 1.59468 (* 1 = 1.59468 loss)
I0520 15:08:58.203533 13563 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0520 15:09:06.615957 13563 solver.cpp:237] Iteration 2100, loss = 1.7549
I0520 15:09:06.616111 13563 solver.cpp:253]     Train net output #0: loss = 1.7549 (* 1 = 1.7549 loss)
I0520 15:09:06.616124 13563 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0520 15:09:15.039230 13563 solver.cpp:237] Iteration 2200, loss = 1.7335
I0520 15:09:15.039268 13563 solver.cpp:253]     Train net output #0: loss = 1.7335 (* 1 = 1.7335 loss)
I0520 15:09:15.039289 13563 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0520 15:09:23.460099 13563 solver.cpp:237] Iteration 2300, loss = 1.72928
I0520 15:09:23.460135 13563 solver.cpp:253]     Train net output #0: loss = 1.72928 (* 1 = 1.72928 loss)
I0520 15:09:23.460151 13563 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0520 15:09:31.883201 13563 solver.cpp:237] Iteration 2400, loss = 1.72
I0520 15:09:31.883235 13563 solver.cpp:253]     Train net output #0: loss = 1.72 (* 1 = 1.72 loss)
I0520 15:09:31.883252 13563 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0520 15:09:40.305086 13563 solver.cpp:237] Iteration 2500, loss = 1.8752
I0520 15:09:40.305233 13563 solver.cpp:253]     Train net output #0: loss = 1.8752 (* 1 = 1.8752 loss)
I0520 15:09:40.305248 13563 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0520 15:09:48.727154 13563 solver.cpp:237] Iteration 2600, loss = 1.66231
I0520 15:09:48.727188 13563 solver.cpp:253]     Train net output #0: loss = 1.66231 (* 1 = 1.66231 loss)
I0520 15:09:48.727206 13563 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0520 15:10:19.239176 13563 solver.cpp:237] Iteration 2700, loss = 1.63348
I0520 15:10:19.239347 13563 solver.cpp:253]     Train net output #0: loss = 1.63348 (* 1 = 1.63348 loss)
I0520 15:10:19.239362 13563 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0520 15:10:27.647433 13563 solver.cpp:237] Iteration 2800, loss = 1.55833
I0520 15:10:27.647469 13563 solver.cpp:253]     Train net output #0: loss = 1.55833 (* 1 = 1.55833 loss)
I0520 15:10:27.647488 13563 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0520 15:10:36.054597 13563 solver.cpp:237] Iteration 2900, loss = 1.7502
I0520 15:10:36.054632 13563 solver.cpp:253]     Train net output #0: loss = 1.7502 (* 1 = 1.7502 loss)
I0520 15:10:36.054649 13563 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0520 15:10:44.385944 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_3000.caffemodel
I0520 15:10:44.488744 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_3000.solverstate
I0520 15:10:44.542809 13563 solver.cpp:237] Iteration 3000, loss = 1.56935
I0520 15:10:44.542855 13563 solver.cpp:253]     Train net output #0: loss = 1.56935 (* 1 = 1.56935 loss)
I0520 15:10:44.542876 13563 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0520 15:10:52.962903 13563 solver.cpp:237] Iteration 3100, loss = 1.67101
I0520 15:10:52.963062 13563 solver.cpp:253]     Train net output #0: loss = 1.67101 (* 1 = 1.67101 loss)
I0520 15:10:52.963076 13563 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0520 15:11:01.381428 13563 solver.cpp:237] Iteration 3200, loss = 1.55571
I0520 15:11:01.381463 13563 solver.cpp:253]     Train net output #0: loss = 1.55571 (* 1 = 1.55571 loss)
I0520 15:11:01.381479 13563 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0520 15:11:09.792275 13563 solver.cpp:237] Iteration 3300, loss = 1.63142
I0520 15:11:09.792311 13563 solver.cpp:253]     Train net output #0: loss = 1.63142 (* 1 = 1.63142 loss)
I0520 15:11:09.792325 13563 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0520 15:11:40.365351 13563 solver.cpp:237] Iteration 3400, loss = 1.53718
I0520 15:11:40.365523 13563 solver.cpp:253]     Train net output #0: loss = 1.53718 (* 1 = 1.53718 loss)
I0520 15:11:40.365540 13563 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0520 15:11:48.780036 13563 solver.cpp:237] Iteration 3500, loss = 1.58233
I0520 15:11:48.780071 13563 solver.cpp:253]     Train net output #0: loss = 1.58233 (* 1 = 1.58233 loss)
I0520 15:11:48.780088 13563 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0520 15:11:57.201421 13563 solver.cpp:237] Iteration 3600, loss = 1.64472
I0520 15:11:57.201457 13563 solver.cpp:253]     Train net output #0: loss = 1.64472 (* 1 = 1.64472 loss)
I0520 15:11:57.201472 13563 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0520 15:12:05.622309 13563 solver.cpp:237] Iteration 3700, loss = 1.65518
I0520 15:12:05.622347 13563 solver.cpp:253]     Train net output #0: loss = 1.65518 (* 1 = 1.65518 loss)
I0520 15:12:05.622370 13563 sgd_solver.cpp:106] Iteration 3700, lr = 0.0025
I0520 15:12:14.038457 13563 solver.cpp:237] Iteration 3800, loss = 1.45128
I0520 15:12:14.038595 13563 solver.cpp:253]     Train net output #0: loss = 1.45128 (* 1 = 1.45128 loss)
I0520 15:12:14.038609 13563 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0520 15:12:22.456524 13563 solver.cpp:237] Iteration 3900, loss = 1.48788
I0520 15:12:22.456557 13563 solver.cpp:253]     Train net output #0: loss = 1.48788 (* 1 = 1.48788 loss)
I0520 15:12:22.456575 13563 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0520 15:12:30.796537 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_4000.caffemodel
I0520 15:12:30.894695 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_4000.solverstate
I0520 15:12:30.923146 13563 solver.cpp:341] Iteration 4000, Testing net (#0)
I0520 15:13:37.776020 13563 solver.cpp:409]     Test net output #0: accuracy = 0.745634
I0520 15:13:37.776190 13563 solver.cpp:409]     Test net output #1: loss = 0.83789 (* 1 = 0.83789 loss)
I0520 15:13:59.967499 13563 solver.cpp:237] Iteration 4000, loss = 1.59945
I0520 15:13:59.967551 13563 solver.cpp:253]     Train net output #0: loss = 1.59945 (* 1 = 1.59945 loss)
I0520 15:13:59.967569 13563 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0520 15:14:08.393174 13563 solver.cpp:237] Iteration 4100, loss = 1.33667
I0520 15:14:08.393332 13563 solver.cpp:253]     Train net output #0: loss = 1.33667 (* 1 = 1.33667 loss)
I0520 15:14:08.393347 13563 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0520 15:14:16.809588 13563 solver.cpp:237] Iteration 4200, loss = 1.60197
I0520 15:14:16.809623 13563 solver.cpp:253]     Train net output #0: loss = 1.60197 (* 1 = 1.60197 loss)
I0520 15:14:16.809640 13563 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0520 15:14:25.225920 13563 solver.cpp:237] Iteration 4300, loss = 1.62684
I0520 15:14:25.225956 13563 solver.cpp:253]     Train net output #0: loss = 1.62684 (* 1 = 1.62684 loss)
I0520 15:14:25.225971 13563 sgd_solver.cpp:106] Iteration 4300, lr = 0.0025
I0520 15:14:33.650941 13563 solver.cpp:237] Iteration 4400, loss = 1.39158
I0520 15:14:33.650980 13563 solver.cpp:253]     Train net output #0: loss = 1.39158 (* 1 = 1.39158 loss)
I0520 15:14:33.651000 13563 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0520 15:14:42.073786 13563 solver.cpp:237] Iteration 4500, loss = 1.46111
I0520 15:14:42.073916 13563 solver.cpp:253]     Train net output #0: loss = 1.46111 (* 1 = 1.46111 loss)
I0520 15:14:42.073930 13563 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0520 15:14:50.488925 13563 solver.cpp:237] Iteration 4600, loss = 1.38516
I0520 15:14:50.488958 13563 solver.cpp:253]     Train net output #0: loss = 1.38516 (* 1 = 1.38516 loss)
I0520 15:14:50.488977 13563 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0520 15:15:21.083770 13563 solver.cpp:237] Iteration 4700, loss = 1.46537
I0520 15:15:21.083935 13563 solver.cpp:253]     Train net output #0: loss = 1.46537 (* 1 = 1.46537 loss)
I0520 15:15:21.083950 13563 sgd_solver.cpp:106] Iteration 4700, lr = 0.0025
I0520 15:15:29.502607 13563 solver.cpp:237] Iteration 4800, loss = 1.44642
I0520 15:15:29.502650 13563 solver.cpp:253]     Train net output #0: loss = 1.44642 (* 1 = 1.44642 loss)
I0520 15:15:29.502668 13563 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0520 15:15:37.934273 13563 solver.cpp:237] Iteration 4900, loss = 1.44192
I0520 15:15:37.934309 13563 solver.cpp:253]     Train net output #0: loss = 1.44192 (* 1 = 1.44192 loss)
I0520 15:15:37.934325 13563 sgd_solver.cpp:106] Iteration 4900, lr = 0.0025
I0520 15:15:46.270145 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_5000.caffemodel
I0520 15:15:46.368765 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_5000.solverstate
I0520 15:15:46.422889 13563 solver.cpp:237] Iteration 5000, loss = 1.41413
I0520 15:15:46.422936 13563 solver.cpp:253]     Train net output #0: loss = 1.41413 (* 1 = 1.41413 loss)
I0520 15:15:46.422956 13563 sgd_solver.cpp:106] Iteration 5000, lr = 0.0025
I0520 15:15:54.851296 13563 solver.cpp:237] Iteration 5100, loss = 1.3645
I0520 15:15:54.851459 13563 solver.cpp:253]     Train net output #0: loss = 1.3645 (* 1 = 1.3645 loss)
I0520 15:15:54.851472 13563 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0520 15:16:03.271334 13563 solver.cpp:237] Iteration 5200, loss = 1.35525
I0520 15:16:03.271373 13563 solver.cpp:253]     Train net output #0: loss = 1.35525 (* 1 = 1.35525 loss)
I0520 15:16:03.271390 13563 sgd_solver.cpp:106] Iteration 5200, lr = 0.0025
I0520 15:16:11.686257 13563 solver.cpp:237] Iteration 5300, loss = 1.60544
I0520 15:16:11.686292 13563 solver.cpp:253]     Train net output #0: loss = 1.60544 (* 1 = 1.60544 loss)
I0520 15:16:11.686310 13563 sgd_solver.cpp:106] Iteration 5300, lr = 0.0025
I0520 15:16:42.281846 13563 solver.cpp:237] Iteration 5400, loss = 1.42165
I0520 15:16:42.282016 13563 solver.cpp:253]     Train net output #0: loss = 1.42165 (* 1 = 1.42165 loss)
I0520 15:16:42.282032 13563 sgd_solver.cpp:106] Iteration 5400, lr = 0.0025
I0520 15:16:50.706993 13563 solver.cpp:237] Iteration 5500, loss = 1.50542
I0520 15:16:50.707027 13563 solver.cpp:253]     Train net output #0: loss = 1.50542 (* 1 = 1.50542 loss)
I0520 15:16:50.707044 13563 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0520 15:16:59.120987 13563 solver.cpp:237] Iteration 5600, loss = 1.27944
I0520 15:16:59.121022 13563 solver.cpp:253]     Train net output #0: loss = 1.27944 (* 1 = 1.27944 loss)
I0520 15:16:59.121037 13563 sgd_solver.cpp:106] Iteration 5600, lr = 0.0025
I0520 15:17:07.539810 13563 solver.cpp:237] Iteration 5700, loss = 1.61946
I0520 15:17:07.539854 13563 solver.cpp:253]     Train net output #0: loss = 1.61946 (* 1 = 1.61946 loss)
I0520 15:17:07.539872 13563 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0520 15:17:15.967473 13563 solver.cpp:237] Iteration 5800, loss = 1.61203
I0520 15:17:15.967617 13563 solver.cpp:253]     Train net output #0: loss = 1.61203 (* 1 = 1.61203 loss)
I0520 15:17:15.967629 13563 sgd_solver.cpp:106] Iteration 5800, lr = 0.0025
I0520 15:17:24.389185 13563 solver.cpp:237] Iteration 5900, loss = 1.35771
I0520 15:17:24.389219 13563 solver.cpp:253]     Train net output #0: loss = 1.35771 (* 1 = 1.35771 loss)
I0520 15:17:24.389238 13563 sgd_solver.cpp:106] Iteration 5900, lr = 0.0025
I0520 15:17:32.720927 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_6000.caffemodel
I0520 15:17:32.818125 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_6000.solverstate
I0520 15:17:32.844295 13563 solver.cpp:341] Iteration 6000, Testing net (#0)
I0520 15:18:18.566238 13563 solver.cpp:409]     Test net output #0: accuracy = 0.780581
I0520 15:18:18.566411 13563 solver.cpp:409]     Test net output #1: loss = 0.690546 (* 1 = 0.690546 loss)
I0520 15:18:40.722980 13563 solver.cpp:237] Iteration 6000, loss = 1.33795
I0520 15:18:40.723032 13563 solver.cpp:253]     Train net output #0: loss = 1.33795 (* 1 = 1.33795 loss)
I0520 15:18:40.723047 13563 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0520 15:18:49.137776 13563 solver.cpp:237] Iteration 6100, loss = 1.345
I0520 15:18:49.137935 13563 solver.cpp:253]     Train net output #0: loss = 1.345 (* 1 = 1.345 loss)
I0520 15:18:49.137949 13563 sgd_solver.cpp:106] Iteration 6100, lr = 0.0025
I0520 15:18:57.558794 13563 solver.cpp:237] Iteration 6200, loss = 1.44964
I0520 15:18:57.558828 13563 solver.cpp:253]     Train net output #0: loss = 1.44964 (* 1 = 1.44964 loss)
I0520 15:18:57.558846 13563 sgd_solver.cpp:106] Iteration 6200, lr = 0.0025
I0520 15:19:05.983197 13563 solver.cpp:237] Iteration 6300, loss = 1.4633
I0520 15:19:05.983233 13563 solver.cpp:253]     Train net output #0: loss = 1.4633 (* 1 = 1.4633 loss)
I0520 15:19:05.983247 13563 sgd_solver.cpp:106] Iteration 6300, lr = 0.0025
I0520 15:19:14.393671 13563 solver.cpp:237] Iteration 6400, loss = 1.46157
I0520 15:19:14.393707 13563 solver.cpp:253]     Train net output #0: loss = 1.46157 (* 1 = 1.46157 loss)
I0520 15:19:14.393723 13563 sgd_solver.cpp:106] Iteration 6400, lr = 0.0025
I0520 15:19:22.819249 13563 solver.cpp:237] Iteration 6500, loss = 1.47225
I0520 15:19:22.819408 13563 solver.cpp:253]     Train net output #0: loss = 1.47225 (* 1 = 1.47225 loss)
I0520 15:19:22.819424 13563 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0520 15:19:31.235566 13563 solver.cpp:237] Iteration 6600, loss = 1.35671
I0520 15:19:31.235601 13563 solver.cpp:253]     Train net output #0: loss = 1.35671 (* 1 = 1.35671 loss)
I0520 15:19:31.235620 13563 sgd_solver.cpp:106] Iteration 6600, lr = 0.0025
I0520 15:20:01.808466 13563 solver.cpp:237] Iteration 6700, loss = 1.47761
I0520 15:20:01.808643 13563 solver.cpp:253]     Train net output #0: loss = 1.47761 (* 1 = 1.47761 loss)
I0520 15:20:01.808660 13563 sgd_solver.cpp:106] Iteration 6700, lr = 0.0025
I0520 15:20:10.224678 13563 solver.cpp:237] Iteration 6800, loss = 1.48069
I0520 15:20:10.224710 13563 solver.cpp:253]     Train net output #0: loss = 1.48069 (* 1 = 1.48069 loss)
I0520 15:20:10.224728 13563 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0520 15:20:18.651384 13563 solver.cpp:237] Iteration 6900, loss = 1.26907
I0520 15:20:18.651420 13563 solver.cpp:253]     Train net output #0: loss = 1.26907 (* 1 = 1.26907 loss)
I0520 15:20:18.651437 13563 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0520 15:20:26.979192 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_7000.caffemodel
I0520 15:20:27.076134 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_7000.solverstate
I0520 15:20:27.128123 13563 solver.cpp:237] Iteration 7000, loss = 1.45273
I0520 15:20:27.128168 13563 solver.cpp:253]     Train net output #0: loss = 1.45273 (* 1 = 1.45273 loss)
I0520 15:20:27.128185 13563 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0520 15:20:35.538084 13563 solver.cpp:237] Iteration 7100, loss = 1.3774
I0520 15:20:35.538239 13563 solver.cpp:253]     Train net output #0: loss = 1.3774 (* 1 = 1.3774 loss)
I0520 15:20:35.538254 13563 sgd_solver.cpp:106] Iteration 7100, lr = 0.0025
I0520 15:20:43.953028 13563 solver.cpp:237] Iteration 7200, loss = 1.42961
I0520 15:20:43.953063 13563 solver.cpp:253]     Train net output #0: loss = 1.42961 (* 1 = 1.42961 loss)
I0520 15:20:43.953079 13563 sgd_solver.cpp:106] Iteration 7200, lr = 0.0025
I0520 15:20:52.371182 13563 solver.cpp:237] Iteration 7300, loss = 1.48058
I0520 15:20:52.371220 13563 solver.cpp:253]     Train net output #0: loss = 1.48058 (* 1 = 1.48058 loss)
I0520 15:20:52.371242 13563 sgd_solver.cpp:106] Iteration 7300, lr = 0.0025
I0520 15:21:22.888608 13563 solver.cpp:237] Iteration 7400, loss = 1.33988
I0520 15:21:22.888779 13563 solver.cpp:253]     Train net output #0: loss = 1.33988 (* 1 = 1.33988 loss)
I0520 15:21:22.888794 13563 sgd_solver.cpp:106] Iteration 7400, lr = 0.0025
I0520 15:21:31.312623 13563 solver.cpp:237] Iteration 7500, loss = 1.33436
I0520 15:21:31.312657 13563 solver.cpp:253]     Train net output #0: loss = 1.33436 (* 1 = 1.33436 loss)
I0520 15:21:31.312675 13563 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0520 15:21:39.737191 13563 solver.cpp:237] Iteration 7600, loss = 1.42034
I0520 15:21:39.737241 13563 solver.cpp:253]     Train net output #0: loss = 1.42034 (* 1 = 1.42034 loss)
I0520 15:21:39.737256 13563 sgd_solver.cpp:106] Iteration 7600, lr = 0.0025
I0520 15:21:48.159737 13563 solver.cpp:237] Iteration 7700, loss = 1.33903
I0520 15:21:48.159772 13563 solver.cpp:253]     Train net output #0: loss = 1.33903 (* 1 = 1.33903 loss)
I0520 15:21:48.159788 13563 sgd_solver.cpp:106] Iteration 7700, lr = 0.0025
I0520 15:21:56.572108 13563 solver.cpp:237] Iteration 7800, loss = 1.39725
I0520 15:21:56.572252 13563 solver.cpp:253]     Train net output #0: loss = 1.39725 (* 1 = 1.39725 loss)
I0520 15:21:56.572265 13563 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0520 15:22:04.987020 13563 solver.cpp:237] Iteration 7900, loss = 1.40127
I0520 15:22:04.987064 13563 solver.cpp:253]     Train net output #0: loss = 1.40127 (* 1 = 1.40127 loss)
I0520 15:22:04.987082 13563 sgd_solver.cpp:106] Iteration 7900, lr = 0.0025
I0520 15:22:13.321243 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_8000.caffemodel
I0520 15:22:13.417860 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_8000.solverstate
I0520 15:22:13.444140 13563 solver.cpp:341] Iteration 8000, Testing net (#0)
I0520 15:23:20.216042 13563 solver.cpp:409]     Test net output #0: accuracy = 0.816526
I0520 15:23:20.216217 13563 solver.cpp:409]     Test net output #1: loss = 0.616314 (* 1 = 0.616314 loss)
I0520 15:23:42.389565 13563 solver.cpp:237] Iteration 8000, loss = 1.22709
I0520 15:23:42.389619 13563 solver.cpp:253]     Train net output #0: loss = 1.22709 (* 1 = 1.22709 loss)
I0520 15:23:42.389636 13563 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0520 15:23:50.800696 13563 solver.cpp:237] Iteration 8100, loss = 1.34108
I0520 15:23:50.800843 13563 solver.cpp:253]     Train net output #0: loss = 1.34108 (* 1 = 1.34108 loss)
I0520 15:23:50.800858 13563 sgd_solver.cpp:106] Iteration 8100, lr = 0.0025
I0520 15:23:59.215399 13563 solver.cpp:237] Iteration 8200, loss = 1.33636
I0520 15:23:59.215432 13563 solver.cpp:253]     Train net output #0: loss = 1.33636 (* 1 = 1.33636 loss)
I0520 15:23:59.215450 13563 sgd_solver.cpp:106] Iteration 8200, lr = 0.0025
I0520 15:24:07.638677 13563 solver.cpp:237] Iteration 8300, loss = 1.23996
I0520 15:24:07.638723 13563 solver.cpp:253]     Train net output #0: loss = 1.23996 (* 1 = 1.23996 loss)
I0520 15:24:07.638738 13563 sgd_solver.cpp:106] Iteration 8300, lr = 0.0025
I0520 15:24:16.057063 13563 solver.cpp:237] Iteration 8400, loss = 1.36213
I0520 15:24:16.057099 13563 solver.cpp:253]     Train net output #0: loss = 1.36213 (* 1 = 1.36213 loss)
I0520 15:24:16.057114 13563 sgd_solver.cpp:106] Iteration 8400, lr = 0.0025
I0520 15:24:24.475713 13563 solver.cpp:237] Iteration 8500, loss = 1.33109
I0520 15:24:24.475867 13563 solver.cpp:253]     Train net output #0: loss = 1.33109 (* 1 = 1.33109 loss)
I0520 15:24:24.475883 13563 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0520 15:24:32.890791 13563 solver.cpp:237] Iteration 8600, loss = 1.17109
I0520 15:24:32.890836 13563 solver.cpp:253]     Train net output #0: loss = 1.17109 (* 1 = 1.17109 loss)
I0520 15:24:32.890851 13563 sgd_solver.cpp:106] Iteration 8600, lr = 0.0025
I0520 15:25:03.444149 13563 solver.cpp:237] Iteration 8700, loss = 1.27539
I0520 15:25:03.444322 13563 solver.cpp:253]     Train net output #0: loss = 1.27539 (* 1 = 1.27539 loss)
I0520 15:25:03.444339 13563 sgd_solver.cpp:106] Iteration 8700, lr = 0.0025
I0520 15:25:11.866653 13563 solver.cpp:237] Iteration 8800, loss = 1.42665
I0520 15:25:11.866688 13563 solver.cpp:253]     Train net output #0: loss = 1.42665 (* 1 = 1.42665 loss)
I0520 15:25:11.866704 13563 sgd_solver.cpp:106] Iteration 8800, lr = 0.0025
I0520 15:25:20.289329 13563 solver.cpp:237] Iteration 8900, loss = 1.25549
I0520 15:25:20.289363 13563 solver.cpp:253]     Train net output #0: loss = 1.25549 (* 1 = 1.25549 loss)
I0520 15:25:20.289379 13563 sgd_solver.cpp:106] Iteration 8900, lr = 0.0025
I0520 15:25:28.624562 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_9000.caffemodel
I0520 15:25:28.728643 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_9000.solverstate
I0520 15:25:28.782760 13563 solver.cpp:237] Iteration 9000, loss = 1.45502
I0520 15:25:28.782809 13563 solver.cpp:253]     Train net output #0: loss = 1.45502 (* 1 = 1.45502 loss)
I0520 15:25:28.782824 13563 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0520 15:25:37.196298 13563 solver.cpp:237] Iteration 9100, loss = 1.32988
I0520 15:25:37.196449 13563 solver.cpp:253]     Train net output #0: loss = 1.32988 (* 1 = 1.32988 loss)
I0520 15:25:37.196462 13563 sgd_solver.cpp:106] Iteration 9100, lr = 0.0025
I0520 15:25:45.624884 13563 solver.cpp:237] Iteration 9200, loss = 1.44008
I0520 15:25:45.624918 13563 solver.cpp:253]     Train net output #0: loss = 1.44008 (* 1 = 1.44008 loss)
I0520 15:25:45.624935 13563 sgd_solver.cpp:106] Iteration 9200, lr = 0.0025
I0520 15:25:54.041600 13563 solver.cpp:237] Iteration 9300, loss = 1.336
I0520 15:25:54.041637 13563 solver.cpp:253]     Train net output #0: loss = 1.336 (* 1 = 1.336 loss)
I0520 15:25:54.041658 13563 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0520 15:26:24.639920 13563 solver.cpp:237] Iteration 9400, loss = 1.27379
I0520 15:26:24.640102 13563 solver.cpp:253]     Train net output #0: loss = 1.27379 (* 1 = 1.27379 loss)
I0520 15:26:24.640117 13563 sgd_solver.cpp:106] Iteration 9400, lr = 0.0025
I0520 15:26:33.064679 13563 solver.cpp:237] Iteration 9500, loss = 1.42934
I0520 15:26:33.064714 13563 solver.cpp:253]     Train net output #0: loss = 1.42934 (* 1 = 1.42934 loss)
I0520 15:26:33.064731 13563 sgd_solver.cpp:106] Iteration 9500, lr = 0.0025
I0520 15:26:41.483177 13563 solver.cpp:237] Iteration 9600, loss = 1.32296
I0520 15:26:41.483225 13563 solver.cpp:253]     Train net output #0: loss = 1.32296 (* 1 = 1.32296 loss)
I0520 15:26:41.483242 13563 sgd_solver.cpp:106] Iteration 9600, lr = 0.0025
I0520 15:26:49.909240 13563 solver.cpp:237] Iteration 9700, loss = 1.39651
I0520 15:26:49.909276 13563 solver.cpp:253]     Train net output #0: loss = 1.39651 (* 1 = 1.39651 loss)
I0520 15:26:49.909292 13563 sgd_solver.cpp:106] Iteration 9700, lr = 0.0025
I0520 15:26:58.328683 13563 solver.cpp:237] Iteration 9800, loss = 1.26239
I0520 15:26:58.328830 13563 solver.cpp:253]     Train net output #0: loss = 1.26239 (* 1 = 1.26239 loss)
I0520 15:26:58.328843 13563 sgd_solver.cpp:106] Iteration 9800, lr = 0.0025
I0520 15:27:06.750120 13563 solver.cpp:237] Iteration 9900, loss = 1.30567
I0520 15:27:06.750154 13563 solver.cpp:253]     Train net output #0: loss = 1.30567 (* 1 = 1.30567 loss)
I0520 15:27:06.750176 13563 sgd_solver.cpp:106] Iteration 9900, lr = 0.0025
I0520 15:27:15.081338 13563 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_10000.caffemodel
I0520 15:27:15.183297 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657_iter_10000.solverstate
I0520 15:27:36.118368 13563 solver.cpp:321] Iteration 10000, loss = 1.10615
I0520 15:27:36.118540 13563 solver.cpp:341] Iteration 10000, Testing net (#0)
I0520 15:28:22.174934 13563 solver.cpp:409]     Test net output #0: accuracy = 0.81946
I0520 15:28:22.175098 13563 solver.cpp:409]     Test net output #1: loss = 0.55931 (* 1 = 0.55931 loss)
I0520 15:28:22.175112 13563 solver.cpp:326] Optimization Done.
I0520 15:28:22.175123 13563 caffe.cpp:215] Optimization Done.
Application 11232751 resources: utime ~1305s, stime ~233s, Rss ~5329284, inblocks ~3744348, outblocks ~179816
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_150_2016-05-20T11.20.38.121657.solver"
	User time (seconds): 0.56
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:46.36
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15112
	Voluntary context switches: 3071
	Involuntary context switches: 158
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
