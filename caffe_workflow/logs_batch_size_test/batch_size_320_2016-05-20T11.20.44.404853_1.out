2805940
I0520 21:12:24.955018  8812 caffe.cpp:184] Using GPUs 0
I0520 21:12:25.381989  8812 solver.cpp:48] Initializing solver from parameters: 
test_iter: 468
test_interval: 937
base_lr: 0.0025
display: 46
max_iter: 4687
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 468
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853.prototxt"
I0520 21:12:25.383579  8812 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853.prototxt
I0520 21:12:25.406631  8812 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 21:12:25.406692  8812 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 21:12:25.407034  8812 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 320
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:12:25.407217  8812 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:12:25.407241  8812 net.cpp:106] Creating Layer data_hdf5
I0520 21:12:25.407256  8812 net.cpp:411] data_hdf5 -> data
I0520 21:12:25.407290  8812 net.cpp:411] data_hdf5 -> label
I0520 21:12:25.407322  8812 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 21:12:25.408591  8812 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 21:12:25.410784  8812 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 21:12:46.934432  8812 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 21:12:46.939551  8812 net.cpp:150] Setting up data_hdf5
I0520 21:12:46.939591  8812 net.cpp:157] Top shape: 320 1 127 50 (2032000)
I0520 21:12:46.939606  8812 net.cpp:157] Top shape: 320 (320)
I0520 21:12:46.939618  8812 net.cpp:165] Memory required for data: 8129280
I0520 21:12:46.939631  8812 layer_factory.hpp:77] Creating layer conv1
I0520 21:12:46.939664  8812 net.cpp:106] Creating Layer conv1
I0520 21:12:46.939676  8812 net.cpp:454] conv1 <- data
I0520 21:12:46.939695  8812 net.cpp:411] conv1 -> conv1
I0520 21:12:47.657472  8812 net.cpp:150] Setting up conv1
I0520 21:12:47.657518  8812 net.cpp:157] Top shape: 320 12 120 48 (22118400)
I0520 21:12:47.657532  8812 net.cpp:165] Memory required for data: 96602880
I0520 21:12:47.657562  8812 layer_factory.hpp:77] Creating layer relu1
I0520 21:12:47.657582  8812 net.cpp:106] Creating Layer relu1
I0520 21:12:47.657593  8812 net.cpp:454] relu1 <- conv1
I0520 21:12:47.657608  8812 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:12:47.658131  8812 net.cpp:150] Setting up relu1
I0520 21:12:47.658149  8812 net.cpp:157] Top shape: 320 12 120 48 (22118400)
I0520 21:12:47.658159  8812 net.cpp:165] Memory required for data: 185076480
I0520 21:12:47.658169  8812 layer_factory.hpp:77] Creating layer pool1
I0520 21:12:47.658185  8812 net.cpp:106] Creating Layer pool1
I0520 21:12:47.658195  8812 net.cpp:454] pool1 <- conv1
I0520 21:12:47.658208  8812 net.cpp:411] pool1 -> pool1
I0520 21:12:47.658288  8812 net.cpp:150] Setting up pool1
I0520 21:12:47.658303  8812 net.cpp:157] Top shape: 320 12 60 48 (11059200)
I0520 21:12:47.658313  8812 net.cpp:165] Memory required for data: 229313280
I0520 21:12:47.658321  8812 layer_factory.hpp:77] Creating layer conv2
I0520 21:12:47.658342  8812 net.cpp:106] Creating Layer conv2
I0520 21:12:47.658354  8812 net.cpp:454] conv2 <- pool1
I0520 21:12:47.658366  8812 net.cpp:411] conv2 -> conv2
I0520 21:12:47.661056  8812 net.cpp:150] Setting up conv2
I0520 21:12:47.661082  8812 net.cpp:157] Top shape: 320 20 54 46 (15897600)
I0520 21:12:47.661093  8812 net.cpp:165] Memory required for data: 292903680
I0520 21:12:47.661113  8812 layer_factory.hpp:77] Creating layer relu2
I0520 21:12:47.661128  8812 net.cpp:106] Creating Layer relu2
I0520 21:12:47.661137  8812 net.cpp:454] relu2 <- conv2
I0520 21:12:47.661150  8812 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:12:47.661481  8812 net.cpp:150] Setting up relu2
I0520 21:12:47.661495  8812 net.cpp:157] Top shape: 320 20 54 46 (15897600)
I0520 21:12:47.661506  8812 net.cpp:165] Memory required for data: 356494080
I0520 21:12:47.661516  8812 layer_factory.hpp:77] Creating layer pool2
I0520 21:12:47.661528  8812 net.cpp:106] Creating Layer pool2
I0520 21:12:47.661538  8812 net.cpp:454] pool2 <- conv2
I0520 21:12:47.661563  8812 net.cpp:411] pool2 -> pool2
I0520 21:12:47.661633  8812 net.cpp:150] Setting up pool2
I0520 21:12:47.661645  8812 net.cpp:157] Top shape: 320 20 27 46 (7948800)
I0520 21:12:47.661655  8812 net.cpp:165] Memory required for data: 388289280
I0520 21:12:47.661664  8812 layer_factory.hpp:77] Creating layer conv3
I0520 21:12:47.661682  8812 net.cpp:106] Creating Layer conv3
I0520 21:12:47.661694  8812 net.cpp:454] conv3 <- pool2
I0520 21:12:47.661706  8812 net.cpp:411] conv3 -> conv3
I0520 21:12:47.663630  8812 net.cpp:150] Setting up conv3
I0520 21:12:47.663653  8812 net.cpp:157] Top shape: 320 28 22 44 (8673280)
I0520 21:12:47.663666  8812 net.cpp:165] Memory required for data: 422982400
I0520 21:12:47.663683  8812 layer_factory.hpp:77] Creating layer relu3
I0520 21:12:47.663699  8812 net.cpp:106] Creating Layer relu3
I0520 21:12:47.663709  8812 net.cpp:454] relu3 <- conv3
I0520 21:12:47.663722  8812 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:12:47.664192  8812 net.cpp:150] Setting up relu3
I0520 21:12:47.664209  8812 net.cpp:157] Top shape: 320 28 22 44 (8673280)
I0520 21:12:47.664219  8812 net.cpp:165] Memory required for data: 457675520
I0520 21:12:47.664229  8812 layer_factory.hpp:77] Creating layer pool3
I0520 21:12:47.664242  8812 net.cpp:106] Creating Layer pool3
I0520 21:12:47.664252  8812 net.cpp:454] pool3 <- conv3
I0520 21:12:47.664265  8812 net.cpp:411] pool3 -> pool3
I0520 21:12:47.664332  8812 net.cpp:150] Setting up pool3
I0520 21:12:47.664345  8812 net.cpp:157] Top shape: 320 28 11 44 (4336640)
I0520 21:12:47.664355  8812 net.cpp:165] Memory required for data: 475022080
I0520 21:12:47.664366  8812 layer_factory.hpp:77] Creating layer conv4
I0520 21:12:47.664382  8812 net.cpp:106] Creating Layer conv4
I0520 21:12:47.664393  8812 net.cpp:454] conv4 <- pool3
I0520 21:12:47.664407  8812 net.cpp:411] conv4 -> conv4
I0520 21:12:47.667181  8812 net.cpp:150] Setting up conv4
I0520 21:12:47.667203  8812 net.cpp:157] Top shape: 320 36 6 42 (2903040)
I0520 21:12:47.667218  8812 net.cpp:165] Memory required for data: 486634240
I0520 21:12:47.667234  8812 layer_factory.hpp:77] Creating layer relu4
I0520 21:12:47.667248  8812 net.cpp:106] Creating Layer relu4
I0520 21:12:47.667258  8812 net.cpp:454] relu4 <- conv4
I0520 21:12:47.667271  8812 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:12:47.667744  8812 net.cpp:150] Setting up relu4
I0520 21:12:47.667760  8812 net.cpp:157] Top shape: 320 36 6 42 (2903040)
I0520 21:12:47.667770  8812 net.cpp:165] Memory required for data: 498246400
I0520 21:12:47.667781  8812 layer_factory.hpp:77] Creating layer pool4
I0520 21:12:47.667794  8812 net.cpp:106] Creating Layer pool4
I0520 21:12:47.667804  8812 net.cpp:454] pool4 <- conv4
I0520 21:12:47.667816  8812 net.cpp:411] pool4 -> pool4
I0520 21:12:47.667884  8812 net.cpp:150] Setting up pool4
I0520 21:12:47.667898  8812 net.cpp:157] Top shape: 320 36 3 42 (1451520)
I0520 21:12:47.667909  8812 net.cpp:165] Memory required for data: 504052480
I0520 21:12:47.667919  8812 layer_factory.hpp:77] Creating layer ip1
I0520 21:12:47.667939  8812 net.cpp:106] Creating Layer ip1
I0520 21:12:47.667950  8812 net.cpp:454] ip1 <- pool4
I0520 21:12:47.667963  8812 net.cpp:411] ip1 -> ip1
I0520 21:12:47.683429  8812 net.cpp:150] Setting up ip1
I0520 21:12:47.683459  8812 net.cpp:157] Top shape: 320 196 (62720)
I0520 21:12:47.683471  8812 net.cpp:165] Memory required for data: 504303360
I0520 21:12:47.683493  8812 layer_factory.hpp:77] Creating layer relu5
I0520 21:12:47.683509  8812 net.cpp:106] Creating Layer relu5
I0520 21:12:47.683519  8812 net.cpp:454] relu5 <- ip1
I0520 21:12:47.683532  8812 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:12:47.683876  8812 net.cpp:150] Setting up relu5
I0520 21:12:47.683890  8812 net.cpp:157] Top shape: 320 196 (62720)
I0520 21:12:47.683902  8812 net.cpp:165] Memory required for data: 504554240
I0520 21:12:47.683912  8812 layer_factory.hpp:77] Creating layer drop1
I0520 21:12:47.683933  8812 net.cpp:106] Creating Layer drop1
I0520 21:12:47.683943  8812 net.cpp:454] drop1 <- ip1
I0520 21:12:47.683969  8812 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:12:47.684015  8812 net.cpp:150] Setting up drop1
I0520 21:12:47.684028  8812 net.cpp:157] Top shape: 320 196 (62720)
I0520 21:12:47.684038  8812 net.cpp:165] Memory required for data: 504805120
I0520 21:12:47.684048  8812 layer_factory.hpp:77] Creating layer ip2
I0520 21:12:47.684067  8812 net.cpp:106] Creating Layer ip2
I0520 21:12:47.684077  8812 net.cpp:454] ip2 <- ip1
I0520 21:12:47.684090  8812 net.cpp:411] ip2 -> ip2
I0520 21:12:47.684561  8812 net.cpp:150] Setting up ip2
I0520 21:12:47.684574  8812 net.cpp:157] Top shape: 320 98 (31360)
I0520 21:12:47.684586  8812 net.cpp:165] Memory required for data: 504930560
I0520 21:12:47.684600  8812 layer_factory.hpp:77] Creating layer relu6
I0520 21:12:47.684612  8812 net.cpp:106] Creating Layer relu6
I0520 21:12:47.684623  8812 net.cpp:454] relu6 <- ip2
I0520 21:12:47.684634  8812 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:12:47.685158  8812 net.cpp:150] Setting up relu6
I0520 21:12:47.685173  8812 net.cpp:157] Top shape: 320 98 (31360)
I0520 21:12:47.685184  8812 net.cpp:165] Memory required for data: 505056000
I0520 21:12:47.685194  8812 layer_factory.hpp:77] Creating layer drop2
I0520 21:12:47.685206  8812 net.cpp:106] Creating Layer drop2
I0520 21:12:47.685216  8812 net.cpp:454] drop2 <- ip2
I0520 21:12:47.685228  8812 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:12:47.685271  8812 net.cpp:150] Setting up drop2
I0520 21:12:47.685284  8812 net.cpp:157] Top shape: 320 98 (31360)
I0520 21:12:47.685294  8812 net.cpp:165] Memory required for data: 505181440
I0520 21:12:47.685304  8812 layer_factory.hpp:77] Creating layer ip3
I0520 21:12:47.685317  8812 net.cpp:106] Creating Layer ip3
I0520 21:12:47.685328  8812 net.cpp:454] ip3 <- ip2
I0520 21:12:47.685340  8812 net.cpp:411] ip3 -> ip3
I0520 21:12:47.685551  8812 net.cpp:150] Setting up ip3
I0520 21:12:47.685565  8812 net.cpp:157] Top shape: 320 11 (3520)
I0520 21:12:47.685575  8812 net.cpp:165] Memory required for data: 505195520
I0520 21:12:47.685590  8812 layer_factory.hpp:77] Creating layer drop3
I0520 21:12:47.685602  8812 net.cpp:106] Creating Layer drop3
I0520 21:12:47.685612  8812 net.cpp:454] drop3 <- ip3
I0520 21:12:47.685623  8812 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:12:47.685663  8812 net.cpp:150] Setting up drop3
I0520 21:12:47.685675  8812 net.cpp:157] Top shape: 320 11 (3520)
I0520 21:12:47.685685  8812 net.cpp:165] Memory required for data: 505209600
I0520 21:12:47.685695  8812 layer_factory.hpp:77] Creating layer loss
I0520 21:12:47.685714  8812 net.cpp:106] Creating Layer loss
I0520 21:12:47.685724  8812 net.cpp:454] loss <- ip3
I0520 21:12:47.685735  8812 net.cpp:454] loss <- label
I0520 21:12:47.685747  8812 net.cpp:411] loss -> loss
I0520 21:12:47.685765  8812 layer_factory.hpp:77] Creating layer loss
I0520 21:12:47.686406  8812 net.cpp:150] Setting up loss
I0520 21:12:47.686422  8812 net.cpp:157] Top shape: (1)
I0520 21:12:47.686432  8812 net.cpp:160]     with loss weight 1
I0520 21:12:47.686475  8812 net.cpp:165] Memory required for data: 505209604
I0520 21:12:47.686486  8812 net.cpp:226] loss needs backward computation.
I0520 21:12:47.686496  8812 net.cpp:226] drop3 needs backward computation.
I0520 21:12:47.686504  8812 net.cpp:226] ip3 needs backward computation.
I0520 21:12:47.686516  8812 net.cpp:226] drop2 needs backward computation.
I0520 21:12:47.686524  8812 net.cpp:226] relu6 needs backward computation.
I0520 21:12:47.686537  8812 net.cpp:226] ip2 needs backward computation.
I0520 21:12:47.686545  8812 net.cpp:226] drop1 needs backward computation.
I0520 21:12:47.686555  8812 net.cpp:226] relu5 needs backward computation.
I0520 21:12:47.686565  8812 net.cpp:226] ip1 needs backward computation.
I0520 21:12:47.686575  8812 net.cpp:226] pool4 needs backward computation.
I0520 21:12:47.686585  8812 net.cpp:226] relu4 needs backward computation.
I0520 21:12:47.686595  8812 net.cpp:226] conv4 needs backward computation.
I0520 21:12:47.686606  8812 net.cpp:226] pool3 needs backward computation.
I0520 21:12:47.686625  8812 net.cpp:226] relu3 needs backward computation.
I0520 21:12:47.686636  8812 net.cpp:226] conv3 needs backward computation.
I0520 21:12:47.686646  8812 net.cpp:226] pool2 needs backward computation.
I0520 21:12:47.686657  8812 net.cpp:226] relu2 needs backward computation.
I0520 21:12:47.686666  8812 net.cpp:226] conv2 needs backward computation.
I0520 21:12:47.686677  8812 net.cpp:226] pool1 needs backward computation.
I0520 21:12:47.686687  8812 net.cpp:226] relu1 needs backward computation.
I0520 21:12:47.686697  8812 net.cpp:226] conv1 needs backward computation.
I0520 21:12:47.686708  8812 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:12:47.686718  8812 net.cpp:270] This network produces output loss
I0520 21:12:47.686743  8812 net.cpp:283] Network initialization done.
I0520 21:12:47.688302  8812 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853.prototxt
I0520 21:12:47.688374  8812 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 21:12:47.688743  8812 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 320
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:12:47.688932  8812 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:12:47.688947  8812 net.cpp:106] Creating Layer data_hdf5
I0520 21:12:47.688961  8812 net.cpp:411] data_hdf5 -> data
I0520 21:12:47.688977  8812 net.cpp:411] data_hdf5 -> label
I0520 21:12:47.688993  8812 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 21:12:47.799032  8812 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 21:13:09.195369  8812 net.cpp:150] Setting up data_hdf5
I0520 21:13:09.195525  8812 net.cpp:157] Top shape: 320 1 127 50 (2032000)
I0520 21:13:09.195539  8812 net.cpp:157] Top shape: 320 (320)
I0520 21:13:09.195549  8812 net.cpp:165] Memory required for data: 8129280
I0520 21:13:09.195564  8812 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 21:13:09.195590  8812 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 21:13:09.195601  8812 net.cpp:454] label_data_hdf5_1_split <- label
I0520 21:13:09.195616  8812 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 21:13:09.195638  8812 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 21:13:09.195711  8812 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 21:13:09.195725  8812 net.cpp:157] Top shape: 320 (320)
I0520 21:13:09.195736  8812 net.cpp:157] Top shape: 320 (320)
I0520 21:13:09.195746  8812 net.cpp:165] Memory required for data: 8131840
I0520 21:13:09.195756  8812 layer_factory.hpp:77] Creating layer conv1
I0520 21:13:09.195778  8812 net.cpp:106] Creating Layer conv1
I0520 21:13:09.195790  8812 net.cpp:454] conv1 <- data
I0520 21:13:09.195803  8812 net.cpp:411] conv1 -> conv1
I0520 21:13:09.197760  8812 net.cpp:150] Setting up conv1
I0520 21:13:09.197779  8812 net.cpp:157] Top shape: 320 12 120 48 (22118400)
I0520 21:13:09.197794  8812 net.cpp:165] Memory required for data: 96605440
I0520 21:13:09.197816  8812 layer_factory.hpp:77] Creating layer relu1
I0520 21:13:09.197831  8812 net.cpp:106] Creating Layer relu1
I0520 21:13:09.197841  8812 net.cpp:454] relu1 <- conv1
I0520 21:13:09.197854  8812 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:13:09.198354  8812 net.cpp:150] Setting up relu1
I0520 21:13:09.198369  8812 net.cpp:157] Top shape: 320 12 120 48 (22118400)
I0520 21:13:09.198380  8812 net.cpp:165] Memory required for data: 185079040
I0520 21:13:09.198390  8812 layer_factory.hpp:77] Creating layer pool1
I0520 21:13:09.198405  8812 net.cpp:106] Creating Layer pool1
I0520 21:13:09.198415  8812 net.cpp:454] pool1 <- conv1
I0520 21:13:09.198427  8812 net.cpp:411] pool1 -> pool1
I0520 21:13:09.198503  8812 net.cpp:150] Setting up pool1
I0520 21:13:09.198515  8812 net.cpp:157] Top shape: 320 12 60 48 (11059200)
I0520 21:13:09.198525  8812 net.cpp:165] Memory required for data: 229315840
I0520 21:13:09.198534  8812 layer_factory.hpp:77] Creating layer conv2
I0520 21:13:09.198551  8812 net.cpp:106] Creating Layer conv2
I0520 21:13:09.198561  8812 net.cpp:454] conv2 <- pool1
I0520 21:13:09.198576  8812 net.cpp:411] conv2 -> conv2
I0520 21:13:09.200484  8812 net.cpp:150] Setting up conv2
I0520 21:13:09.200506  8812 net.cpp:157] Top shape: 320 20 54 46 (15897600)
I0520 21:13:09.200525  8812 net.cpp:165] Memory required for data: 292906240
I0520 21:13:09.200543  8812 layer_factory.hpp:77] Creating layer relu2
I0520 21:13:09.200557  8812 net.cpp:106] Creating Layer relu2
I0520 21:13:09.200567  8812 net.cpp:454] relu2 <- conv2
I0520 21:13:09.200579  8812 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:13:09.200909  8812 net.cpp:150] Setting up relu2
I0520 21:13:09.200923  8812 net.cpp:157] Top shape: 320 20 54 46 (15897600)
I0520 21:13:09.200933  8812 net.cpp:165] Memory required for data: 356496640
I0520 21:13:09.200944  8812 layer_factory.hpp:77] Creating layer pool2
I0520 21:13:09.200958  8812 net.cpp:106] Creating Layer pool2
I0520 21:13:09.200968  8812 net.cpp:454] pool2 <- conv2
I0520 21:13:09.200981  8812 net.cpp:411] pool2 -> pool2
I0520 21:13:09.201052  8812 net.cpp:150] Setting up pool2
I0520 21:13:09.201066  8812 net.cpp:157] Top shape: 320 20 27 46 (7948800)
I0520 21:13:09.201076  8812 net.cpp:165] Memory required for data: 388291840
I0520 21:13:09.201086  8812 layer_factory.hpp:77] Creating layer conv3
I0520 21:13:09.201103  8812 net.cpp:106] Creating Layer conv3
I0520 21:13:09.201114  8812 net.cpp:454] conv3 <- pool2
I0520 21:13:09.201128  8812 net.cpp:411] conv3 -> conv3
I0520 21:13:09.203091  8812 net.cpp:150] Setting up conv3
I0520 21:13:09.203114  8812 net.cpp:157] Top shape: 320 28 22 44 (8673280)
I0520 21:13:09.203127  8812 net.cpp:165] Memory required for data: 422984960
I0520 21:13:09.203161  8812 layer_factory.hpp:77] Creating layer relu3
I0520 21:13:09.203173  8812 net.cpp:106] Creating Layer relu3
I0520 21:13:09.203183  8812 net.cpp:454] relu3 <- conv3
I0520 21:13:09.203197  8812 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:13:09.203670  8812 net.cpp:150] Setting up relu3
I0520 21:13:09.203685  8812 net.cpp:157] Top shape: 320 28 22 44 (8673280)
I0520 21:13:09.203696  8812 net.cpp:165] Memory required for data: 457678080
I0520 21:13:09.203706  8812 layer_factory.hpp:77] Creating layer pool3
I0520 21:13:09.203718  8812 net.cpp:106] Creating Layer pool3
I0520 21:13:09.203728  8812 net.cpp:454] pool3 <- conv3
I0520 21:13:09.203743  8812 net.cpp:411] pool3 -> pool3
I0520 21:13:09.203814  8812 net.cpp:150] Setting up pool3
I0520 21:13:09.203829  8812 net.cpp:157] Top shape: 320 28 11 44 (4336640)
I0520 21:13:09.203837  8812 net.cpp:165] Memory required for data: 475024640
I0520 21:13:09.203848  8812 layer_factory.hpp:77] Creating layer conv4
I0520 21:13:09.203866  8812 net.cpp:106] Creating Layer conv4
I0520 21:13:09.203877  8812 net.cpp:454] conv4 <- pool3
I0520 21:13:09.203891  8812 net.cpp:411] conv4 -> conv4
I0520 21:13:09.205955  8812 net.cpp:150] Setting up conv4
I0520 21:13:09.205972  8812 net.cpp:157] Top shape: 320 36 6 42 (2903040)
I0520 21:13:09.205982  8812 net.cpp:165] Memory required for data: 486636800
I0520 21:13:09.206001  8812 layer_factory.hpp:77] Creating layer relu4
I0520 21:13:09.206014  8812 net.cpp:106] Creating Layer relu4
I0520 21:13:09.206025  8812 net.cpp:454] relu4 <- conv4
I0520 21:13:09.206038  8812 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:13:09.206509  8812 net.cpp:150] Setting up relu4
I0520 21:13:09.206526  8812 net.cpp:157] Top shape: 320 36 6 42 (2903040)
I0520 21:13:09.206535  8812 net.cpp:165] Memory required for data: 498248960
I0520 21:13:09.206545  8812 layer_factory.hpp:77] Creating layer pool4
I0520 21:13:09.206558  8812 net.cpp:106] Creating Layer pool4
I0520 21:13:09.206568  8812 net.cpp:454] pool4 <- conv4
I0520 21:13:09.206581  8812 net.cpp:411] pool4 -> pool4
I0520 21:13:09.206653  8812 net.cpp:150] Setting up pool4
I0520 21:13:09.206667  8812 net.cpp:157] Top shape: 320 36 3 42 (1451520)
I0520 21:13:09.206676  8812 net.cpp:165] Memory required for data: 504055040
I0520 21:13:09.206686  8812 layer_factory.hpp:77] Creating layer ip1
I0520 21:13:09.206702  8812 net.cpp:106] Creating Layer ip1
I0520 21:13:09.206712  8812 net.cpp:454] ip1 <- pool4
I0520 21:13:09.206727  8812 net.cpp:411] ip1 -> ip1
I0520 21:13:09.222183  8812 net.cpp:150] Setting up ip1
I0520 21:13:09.222211  8812 net.cpp:157] Top shape: 320 196 (62720)
I0520 21:13:09.222223  8812 net.cpp:165] Memory required for data: 504305920
I0520 21:13:09.222249  8812 layer_factory.hpp:77] Creating layer relu5
I0520 21:13:09.222264  8812 net.cpp:106] Creating Layer relu5
I0520 21:13:09.222273  8812 net.cpp:454] relu5 <- ip1
I0520 21:13:09.222287  8812 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:13:09.222635  8812 net.cpp:150] Setting up relu5
I0520 21:13:09.222648  8812 net.cpp:157] Top shape: 320 196 (62720)
I0520 21:13:09.222658  8812 net.cpp:165] Memory required for data: 504556800
I0520 21:13:09.222668  8812 layer_factory.hpp:77] Creating layer drop1
I0520 21:13:09.222687  8812 net.cpp:106] Creating Layer drop1
I0520 21:13:09.222697  8812 net.cpp:454] drop1 <- ip1
I0520 21:13:09.222709  8812 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:13:09.222754  8812 net.cpp:150] Setting up drop1
I0520 21:13:09.222766  8812 net.cpp:157] Top shape: 320 196 (62720)
I0520 21:13:09.222775  8812 net.cpp:165] Memory required for data: 504807680
I0520 21:13:09.222785  8812 layer_factory.hpp:77] Creating layer ip2
I0520 21:13:09.222800  8812 net.cpp:106] Creating Layer ip2
I0520 21:13:09.222810  8812 net.cpp:454] ip2 <- ip1
I0520 21:13:09.222822  8812 net.cpp:411] ip2 -> ip2
I0520 21:13:09.223302  8812 net.cpp:150] Setting up ip2
I0520 21:13:09.223316  8812 net.cpp:157] Top shape: 320 98 (31360)
I0520 21:13:09.223326  8812 net.cpp:165] Memory required for data: 504933120
I0520 21:13:09.223353  8812 layer_factory.hpp:77] Creating layer relu6
I0520 21:13:09.223366  8812 net.cpp:106] Creating Layer relu6
I0520 21:13:09.223376  8812 net.cpp:454] relu6 <- ip2
I0520 21:13:09.223389  8812 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:13:09.223932  8812 net.cpp:150] Setting up relu6
I0520 21:13:09.223954  8812 net.cpp:157] Top shape: 320 98 (31360)
I0520 21:13:09.223964  8812 net.cpp:165] Memory required for data: 505058560
I0520 21:13:09.223974  8812 layer_factory.hpp:77] Creating layer drop2
I0520 21:13:09.223987  8812 net.cpp:106] Creating Layer drop2
I0520 21:13:09.223997  8812 net.cpp:454] drop2 <- ip2
I0520 21:13:09.224011  8812 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:13:09.224056  8812 net.cpp:150] Setting up drop2
I0520 21:13:09.224068  8812 net.cpp:157] Top shape: 320 98 (31360)
I0520 21:13:09.224078  8812 net.cpp:165] Memory required for data: 505184000
I0520 21:13:09.224087  8812 layer_factory.hpp:77] Creating layer ip3
I0520 21:13:09.224102  8812 net.cpp:106] Creating Layer ip3
I0520 21:13:09.224112  8812 net.cpp:454] ip3 <- ip2
I0520 21:13:09.224125  8812 net.cpp:411] ip3 -> ip3
I0520 21:13:09.224350  8812 net.cpp:150] Setting up ip3
I0520 21:13:09.224364  8812 net.cpp:157] Top shape: 320 11 (3520)
I0520 21:13:09.224373  8812 net.cpp:165] Memory required for data: 505198080
I0520 21:13:09.224388  8812 layer_factory.hpp:77] Creating layer drop3
I0520 21:13:09.224402  8812 net.cpp:106] Creating Layer drop3
I0520 21:13:09.224411  8812 net.cpp:454] drop3 <- ip3
I0520 21:13:09.224424  8812 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:13:09.224465  8812 net.cpp:150] Setting up drop3
I0520 21:13:09.224478  8812 net.cpp:157] Top shape: 320 11 (3520)
I0520 21:13:09.224488  8812 net.cpp:165] Memory required for data: 505212160
I0520 21:13:09.224498  8812 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 21:13:09.224510  8812 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 21:13:09.224526  8812 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 21:13:09.224540  8812 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 21:13:09.224555  8812 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 21:13:09.224629  8812 net.cpp:150] Setting up ip3_drop3_0_split
I0520 21:13:09.224642  8812 net.cpp:157] Top shape: 320 11 (3520)
I0520 21:13:09.224654  8812 net.cpp:157] Top shape: 320 11 (3520)
I0520 21:13:09.224665  8812 net.cpp:165] Memory required for data: 505240320
I0520 21:13:09.224675  8812 layer_factory.hpp:77] Creating layer accuracy
I0520 21:13:09.224696  8812 net.cpp:106] Creating Layer accuracy
I0520 21:13:09.224707  8812 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 21:13:09.224720  8812 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 21:13:09.224732  8812 net.cpp:411] accuracy -> accuracy
I0520 21:13:09.224756  8812 net.cpp:150] Setting up accuracy
I0520 21:13:09.224769  8812 net.cpp:157] Top shape: (1)
I0520 21:13:09.224779  8812 net.cpp:165] Memory required for data: 505240324
I0520 21:13:09.224789  8812 layer_factory.hpp:77] Creating layer loss
I0520 21:13:09.224803  8812 net.cpp:106] Creating Layer loss
I0520 21:13:09.224814  8812 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 21:13:09.224825  8812 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 21:13:09.224838  8812 net.cpp:411] loss -> loss
I0520 21:13:09.224856  8812 layer_factory.hpp:77] Creating layer loss
I0520 21:13:09.225342  8812 net.cpp:150] Setting up loss
I0520 21:13:09.225356  8812 net.cpp:157] Top shape: (1)
I0520 21:13:09.225366  8812 net.cpp:160]     with loss weight 1
I0520 21:13:09.225384  8812 net.cpp:165] Memory required for data: 505240328
I0520 21:13:09.225395  8812 net.cpp:226] loss needs backward computation.
I0520 21:13:09.225406  8812 net.cpp:228] accuracy does not need backward computation.
I0520 21:13:09.225417  8812 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 21:13:09.225427  8812 net.cpp:226] drop3 needs backward computation.
I0520 21:13:09.225437  8812 net.cpp:226] ip3 needs backward computation.
I0520 21:13:09.225448  8812 net.cpp:226] drop2 needs backward computation.
I0520 21:13:09.225466  8812 net.cpp:226] relu6 needs backward computation.
I0520 21:13:09.225477  8812 net.cpp:226] ip2 needs backward computation.
I0520 21:13:09.225487  8812 net.cpp:226] drop1 needs backward computation.
I0520 21:13:09.225497  8812 net.cpp:226] relu5 needs backward computation.
I0520 21:13:09.225507  8812 net.cpp:226] ip1 needs backward computation.
I0520 21:13:09.225517  8812 net.cpp:226] pool4 needs backward computation.
I0520 21:13:09.225527  8812 net.cpp:226] relu4 needs backward computation.
I0520 21:13:09.225536  8812 net.cpp:226] conv4 needs backward computation.
I0520 21:13:09.225548  8812 net.cpp:226] pool3 needs backward computation.
I0520 21:13:09.225558  8812 net.cpp:226] relu3 needs backward computation.
I0520 21:13:09.225569  8812 net.cpp:226] conv3 needs backward computation.
I0520 21:13:09.225579  8812 net.cpp:226] pool2 needs backward computation.
I0520 21:13:09.225589  8812 net.cpp:226] relu2 needs backward computation.
I0520 21:13:09.225600  8812 net.cpp:226] conv2 needs backward computation.
I0520 21:13:09.225610  8812 net.cpp:226] pool1 needs backward computation.
I0520 21:13:09.225620  8812 net.cpp:226] relu1 needs backward computation.
I0520 21:13:09.225630  8812 net.cpp:226] conv1 needs backward computation.
I0520 21:13:09.225641  8812 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 21:13:09.225653  8812 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:13:09.225663  8812 net.cpp:270] This network produces output accuracy
I0520 21:13:09.225674  8812 net.cpp:270] This network produces output loss
I0520 21:13:09.225703  8812 net.cpp:283] Network initialization done.
I0520 21:13:09.225836  8812 solver.cpp:60] Solver scaffolding done.
I0520 21:13:09.226963  8812 caffe.cpp:212] Starting Optimization
I0520 21:13:09.226980  8812 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 21:13:09.226994  8812 solver.cpp:289] Learning Rate Policy: fixed
I0520 21:13:09.228217  8812 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 21:13:55.430970  8812 solver.cpp:409]     Test net output #0: accuracy = 0.0601095
I0520 21:13:55.431141  8812 solver.cpp:409]     Test net output #1: loss = 2.39882 (* 1 = 2.39882 loss)
I0520 21:13:55.499822  8812 solver.cpp:237] Iteration 0, loss = 2.39719
I0520 21:13:55.499857  8812 solver.cpp:253]     Train net output #0: loss = 2.39719 (* 1 = 2.39719 loss)
I0520 21:13:55.499876  8812 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 21:14:03.516926  8812 solver.cpp:237] Iteration 46, loss = 2.36412
I0520 21:14:03.516962  8812 solver.cpp:253]     Train net output #0: loss = 2.36412 (* 1 = 2.36412 loss)
I0520 21:14:03.516974  8812 sgd_solver.cpp:106] Iteration 46, lr = 0.0025
I0520 21:14:11.534593  8812 solver.cpp:237] Iteration 92, loss = 2.34764
I0520 21:14:11.534636  8812 solver.cpp:253]     Train net output #0: loss = 2.34764 (* 1 = 2.34764 loss)
I0520 21:14:11.534652  8812 sgd_solver.cpp:106] Iteration 92, lr = 0.0025
I0520 21:14:19.554980  8812 solver.cpp:237] Iteration 138, loss = 2.33982
I0520 21:14:19.555013  8812 solver.cpp:253]     Train net output #0: loss = 2.33982 (* 1 = 2.33982 loss)
I0520 21:14:19.555030  8812 sgd_solver.cpp:106] Iteration 138, lr = 0.0025
I0520 21:14:27.569070  8812 solver.cpp:237] Iteration 184, loss = 2.31375
I0520 21:14:27.569222  8812 solver.cpp:253]     Train net output #0: loss = 2.31375 (* 1 = 2.31375 loss)
I0520 21:14:27.569236  8812 sgd_solver.cpp:106] Iteration 184, lr = 0.0025
I0520 21:14:35.584749  8812 solver.cpp:237] Iteration 230, loss = 2.31249
I0520 21:14:35.584797  8812 solver.cpp:253]     Train net output #0: loss = 2.31249 (* 1 = 2.31249 loss)
I0520 21:14:35.584811  8812 sgd_solver.cpp:106] Iteration 230, lr = 0.0025
I0520 21:14:43.599864  8812 solver.cpp:237] Iteration 276, loss = 2.28124
I0520 21:14:43.599897  8812 solver.cpp:253]     Train net output #0: loss = 2.28124 (* 1 = 2.28124 loss)
I0520 21:14:43.599913  8812 sgd_solver.cpp:106] Iteration 276, lr = 0.0025
I0520 21:15:13.783392  8812 solver.cpp:237] Iteration 322, loss = 2.25061
I0520 21:15:13.783557  8812 solver.cpp:253]     Train net output #0: loss = 2.25061 (* 1 = 2.25061 loss)
I0520 21:15:13.783573  8812 sgd_solver.cpp:106] Iteration 322, lr = 0.0025
I0520 21:15:21.804117  8812 solver.cpp:237] Iteration 368, loss = 2.23207
I0520 21:15:21.804157  8812 solver.cpp:253]     Train net output #0: loss = 2.23207 (* 1 = 2.23207 loss)
I0520 21:15:21.804180  8812 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0520 21:15:29.823024  8812 solver.cpp:237] Iteration 414, loss = 2.23823
I0520 21:15:29.823057  8812 solver.cpp:253]     Train net output #0: loss = 2.23823 (* 1 = 2.23823 loss)
I0520 21:15:29.823073  8812 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0520 21:15:37.846487  8812 solver.cpp:237] Iteration 460, loss = 2.01254
I0520 21:15:37.846521  8812 solver.cpp:253]     Train net output #0: loss = 2.01254 (* 1 = 2.01254 loss)
I0520 21:15:37.846537  8812 sgd_solver.cpp:106] Iteration 460, lr = 0.0025
I0520 21:15:39.066860  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_468.caffemodel
I0520 21:15:39.230401  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_468.solverstate
I0520 21:15:45.932845  8812 solver.cpp:237] Iteration 506, loss = 1.99283
I0520 21:15:45.932999  8812 solver.cpp:253]     Train net output #0: loss = 1.99283 (* 1 = 1.99283 loss)
I0520 21:15:45.933014  8812 sgd_solver.cpp:106] Iteration 506, lr = 0.0025
I0520 21:15:53.953145  8812 solver.cpp:237] Iteration 552, loss = 2.01253
I0520 21:15:53.953178  8812 solver.cpp:253]     Train net output #0: loss = 2.01253 (* 1 = 2.01253 loss)
I0520 21:15:53.953196  8812 sgd_solver.cpp:106] Iteration 552, lr = 0.0025
I0520 21:16:01.971454  8812 solver.cpp:237] Iteration 598, loss = 2.02391
I0520 21:16:01.971596  8812 solver.cpp:253]     Train net output #0: loss = 2.02391 (* 1 = 2.02391 loss)
I0520 21:16:01.971612  8812 sgd_solver.cpp:106] Iteration 598, lr = 0.0025
I0520 21:16:32.164989  8812 solver.cpp:237] Iteration 644, loss = 1.90814
I0520 21:16:32.165150  8812 solver.cpp:253]     Train net output #0: loss = 1.90814 (* 1 = 1.90814 loss)
I0520 21:16:32.165166  8812 sgd_solver.cpp:106] Iteration 644, lr = 0.0025
I0520 21:16:40.185075  8812 solver.cpp:237] Iteration 690, loss = 1.92195
I0520 21:16:40.185120  8812 solver.cpp:253]     Train net output #0: loss = 1.92195 (* 1 = 1.92195 loss)
I0520 21:16:40.185137  8812 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0520 21:16:48.203001  8812 solver.cpp:237] Iteration 736, loss = 1.97903
I0520 21:16:48.203037  8812 solver.cpp:253]     Train net output #0: loss = 1.97903 (* 1 = 1.97903 loss)
I0520 21:16:48.203050  8812 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0520 21:16:56.222975  8812 solver.cpp:237] Iteration 782, loss = 1.90856
I0520 21:16:56.223009  8812 solver.cpp:253]     Train net output #0: loss = 1.90856 (* 1 = 1.90856 loss)
I0520 21:16:56.223026  8812 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0520 21:17:04.242422  8812 solver.cpp:237] Iteration 828, loss = 1.93152
I0520 21:17:04.242581  8812 solver.cpp:253]     Train net output #0: loss = 1.93152 (* 1 = 1.93152 loss)
I0520 21:17:04.242595  8812 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0520 21:17:12.263659  8812 solver.cpp:237] Iteration 874, loss = 1.84036
I0520 21:17:12.263692  8812 solver.cpp:253]     Train net output #0: loss = 1.84036 (* 1 = 1.84036 loss)
I0520 21:17:12.263710  8812 sgd_solver.cpp:106] Iteration 874, lr = 0.0025
I0520 21:17:20.285621  8812 solver.cpp:237] Iteration 920, loss = 1.88421
I0520 21:17:20.285655  8812 solver.cpp:253]     Train net output #0: loss = 1.88421 (* 1 = 1.88421 loss)
I0520 21:17:20.285671  8812 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0520 21:17:22.900722  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_936.caffemodel
I0520 21:17:23.061323  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_936.solverstate
I0520 21:17:23.138908  8812 solver.cpp:341] Iteration 937, Testing net (#0)
I0520 21:18:08.457672  8812 solver.cpp:409]     Test net output #0: accuracy = 0.614877
I0520 21:18:08.457829  8812 solver.cpp:409]     Test net output #1: loss = 1.37922 (* 1 = 1.37922 loss)
I0520 21:18:35.728180  8812 solver.cpp:237] Iteration 966, loss = 1.82871
I0520 21:18:35.728231  8812 solver.cpp:253]     Train net output #0: loss = 1.82871 (* 1 = 1.82871 loss)
I0520 21:18:35.728246  8812 sgd_solver.cpp:106] Iteration 966, lr = 0.0025
I0520 21:18:43.755580  8812 solver.cpp:237] Iteration 1012, loss = 1.85399
I0520 21:18:43.755739  8812 solver.cpp:253]     Train net output #0: loss = 1.85399 (* 1 = 1.85399 loss)
I0520 21:18:43.755754  8812 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0520 21:18:51.778529  8812 solver.cpp:237] Iteration 1058, loss = 1.77491
I0520 21:18:51.778563  8812 solver.cpp:253]     Train net output #0: loss = 1.77491 (* 1 = 1.77491 loss)
I0520 21:18:51.778580  8812 sgd_solver.cpp:106] Iteration 1058, lr = 0.0025
I0520 21:18:59.803565  8812 solver.cpp:237] Iteration 1104, loss = 1.83008
I0520 21:18:59.803598  8812 solver.cpp:253]     Train net output #0: loss = 1.83008 (* 1 = 1.83008 loss)
I0520 21:18:59.803616  8812 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0520 21:19:07.832835  8812 solver.cpp:237] Iteration 1150, loss = 1.80856
I0520 21:19:07.832880  8812 solver.cpp:253]     Train net output #0: loss = 1.80856 (* 1 = 1.80856 loss)
I0520 21:19:07.832896  8812 sgd_solver.cpp:106] Iteration 1150, lr = 0.0025
I0520 21:19:15.855978  8812 solver.cpp:237] Iteration 1196, loss = 1.81653
I0520 21:19:15.856112  8812 solver.cpp:253]     Train net output #0: loss = 1.81653 (* 1 = 1.81653 loss)
I0520 21:19:15.856127  8812 sgd_solver.cpp:106] Iteration 1196, lr = 0.0025
I0520 21:19:23.880951  8812 solver.cpp:237] Iteration 1242, loss = 1.7866
I0520 21:19:23.880985  8812 solver.cpp:253]     Train net output #0: loss = 1.7866 (* 1 = 1.7866 loss)
I0520 21:19:23.881001  8812 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0520 21:19:54.080696  8812 solver.cpp:237] Iteration 1288, loss = 1.70044
I0520 21:19:54.080865  8812 solver.cpp:253]     Train net output #0: loss = 1.70044 (* 1 = 1.70044 loss)
I0520 21:19:54.080881  8812 sgd_solver.cpp:106] Iteration 1288, lr = 0.0025
I0520 21:20:02.110558  8812 solver.cpp:237] Iteration 1334, loss = 1.8423
I0520 21:20:02.110594  8812 solver.cpp:253]     Train net output #0: loss = 1.8423 (* 1 = 1.8423 loss)
I0520 21:20:02.110612  8812 sgd_solver.cpp:106] Iteration 1334, lr = 0.0025
I0520 21:20:10.132043  8812 solver.cpp:237] Iteration 1380, loss = 1.792
I0520 21:20:10.132077  8812 solver.cpp:253]     Train net output #0: loss = 1.792 (* 1 = 1.792 loss)
I0520 21:20:10.132093  8812 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0520 21:20:14.145591  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_1404.caffemodel
I0520 21:20:14.309243  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_1404.solverstate
I0520 21:20:18.223506  8812 solver.cpp:237] Iteration 1426, loss = 1.82152
I0520 21:20:18.223556  8812 solver.cpp:253]     Train net output #0: loss = 1.82152 (* 1 = 1.82152 loss)
I0520 21:20:18.223570  8812 sgd_solver.cpp:106] Iteration 1426, lr = 0.0025
I0520 21:20:26.249917  8812 solver.cpp:237] Iteration 1472, loss = 1.82241
I0520 21:20:26.250061  8812 solver.cpp:253]     Train net output #0: loss = 1.82241 (* 1 = 1.82241 loss)
I0520 21:20:26.250074  8812 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0520 21:20:34.275077  8812 solver.cpp:237] Iteration 1518, loss = 1.77538
I0520 21:20:34.275110  8812 solver.cpp:253]     Train net output #0: loss = 1.77538 (* 1 = 1.77538 loss)
I0520 21:20:34.275127  8812 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0520 21:21:04.451871  8812 solver.cpp:237] Iteration 1564, loss = 1.71574
I0520 21:21:04.452035  8812 solver.cpp:253]     Train net output #0: loss = 1.71574 (* 1 = 1.71574 loss)
I0520 21:21:04.452050  8812 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0520 21:21:12.476372  8812 solver.cpp:237] Iteration 1610, loss = 1.81905
I0520 21:21:12.476410  8812 solver.cpp:253]     Train net output #0: loss = 1.81905 (* 1 = 1.81905 loss)
I0520 21:21:12.476430  8812 sgd_solver.cpp:106] Iteration 1610, lr = 0.0025
I0520 21:21:20.498747  8812 solver.cpp:237] Iteration 1656, loss = 1.75995
I0520 21:21:20.498782  8812 solver.cpp:253]     Train net output #0: loss = 1.75995 (* 1 = 1.75995 loss)
I0520 21:21:20.498800  8812 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0520 21:21:28.521378  8812 solver.cpp:237] Iteration 1702, loss = 1.7122
I0520 21:21:28.521412  8812 solver.cpp:253]     Train net output #0: loss = 1.7122 (* 1 = 1.7122 loss)
I0520 21:21:28.521425  8812 sgd_solver.cpp:106] Iteration 1702, lr = 0.0025
I0520 21:21:36.539652  8812 solver.cpp:237] Iteration 1748, loss = 1.6857
I0520 21:21:36.539803  8812 solver.cpp:253]     Train net output #0: loss = 1.6857 (* 1 = 1.6857 loss)
I0520 21:21:36.539818  8812 sgd_solver.cpp:106] Iteration 1748, lr = 0.0025
I0520 21:21:44.559062  8812 solver.cpp:237] Iteration 1794, loss = 1.74249
I0520 21:21:44.559095  8812 solver.cpp:253]     Train net output #0: loss = 1.74249 (* 1 = 1.74249 loss)
I0520 21:21:44.559113  8812 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0520 21:21:52.578649  8812 solver.cpp:237] Iteration 1840, loss = 1.69852
I0520 21:21:52.578681  8812 solver.cpp:253]     Train net output #0: loss = 1.69852 (* 1 = 1.69852 loss)
I0520 21:21:52.578698  8812 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0520 21:21:57.987303  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_1872.caffemodel
I0520 21:21:58.163885  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_1872.solverstate
I0520 21:21:58.432829  8812 solver.cpp:341] Iteration 1874, Testing net (#0)
I0520 21:23:04.758065  8812 solver.cpp:409]     Test net output #0: accuracy = 0.626549
I0520 21:23:04.758255  8812 solver.cpp:409]     Test net output #1: loss = 1.23516 (* 1 = 1.23516 loss)
I0520 21:23:29.071667  8812 solver.cpp:237] Iteration 1886, loss = 1.84482
I0520 21:23:29.071719  8812 solver.cpp:253]     Train net output #0: loss = 1.84482 (* 1 = 1.84482 loss)
I0520 21:23:29.071733  8812 sgd_solver.cpp:106] Iteration 1886, lr = 0.0025
I0520 21:23:37.083856  8812 solver.cpp:237] Iteration 1932, loss = 1.64423
I0520 21:23:37.084007  8812 solver.cpp:253]     Train net output #0: loss = 1.64423 (* 1 = 1.64423 loss)
I0520 21:23:37.084022  8812 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0520 21:23:45.095028  8812 solver.cpp:237] Iteration 1978, loss = 1.69126
I0520 21:23:45.095072  8812 solver.cpp:253]     Train net output #0: loss = 1.69126 (* 1 = 1.69126 loss)
I0520 21:23:45.095089  8812 sgd_solver.cpp:106] Iteration 1978, lr = 0.0025
I0520 21:23:53.098978  8812 solver.cpp:237] Iteration 2024, loss = 1.72248
I0520 21:23:53.099011  8812 solver.cpp:253]     Train net output #0: loss = 1.72248 (* 1 = 1.72248 loss)
I0520 21:23:53.099027  8812 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0520 21:24:01.113459  8812 solver.cpp:237] Iteration 2070, loss = 1.67063
I0520 21:24:01.113494  8812 solver.cpp:253]     Train net output #0: loss = 1.67063 (* 1 = 1.67063 loss)
I0520 21:24:01.113509  8812 sgd_solver.cpp:106] Iteration 2070, lr = 0.0025
I0520 21:24:09.123854  8812 solver.cpp:237] Iteration 2116, loss = 1.78195
I0520 21:24:09.124003  8812 solver.cpp:253]     Train net output #0: loss = 1.78195 (* 1 = 1.78195 loss)
I0520 21:24:09.124017  8812 sgd_solver.cpp:106] Iteration 2116, lr = 0.0025
I0520 21:24:17.136435  8812 solver.cpp:237] Iteration 2162, loss = 1.68708
I0520 21:24:17.136468  8812 solver.cpp:253]     Train net output #0: loss = 1.68708 (* 1 = 1.68708 loss)
I0520 21:24:17.136485  8812 sgd_solver.cpp:106] Iteration 2162, lr = 0.0025
I0520 21:24:47.345300  8812 solver.cpp:237] Iteration 2208, loss = 1.63302
I0520 21:24:47.345469  8812 solver.cpp:253]     Train net output #0: loss = 1.63302 (* 1 = 1.63302 loss)
I0520 21:24:47.345484  8812 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0520 21:24:55.355996  8812 solver.cpp:237] Iteration 2254, loss = 1.63831
I0520 21:24:55.356043  8812 solver.cpp:253]     Train net output #0: loss = 1.63831 (* 1 = 1.63831 loss)
I0520 21:24:55.356060  8812 sgd_solver.cpp:106] Iteration 2254, lr = 0.0025
I0520 21:25:03.369334  8812 solver.cpp:237] Iteration 2300, loss = 1.68206
I0520 21:25:03.369369  8812 solver.cpp:253]     Train net output #0: loss = 1.68206 (* 1 = 1.68206 loss)
I0520 21:25:03.369385  8812 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0520 21:25:10.158244  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_2340.caffemodel
I0520 21:25:10.320937  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_2340.solverstate
I0520 21:25:11.446259  8812 solver.cpp:237] Iteration 2346, loss = 1.64416
I0520 21:25:11.446303  8812 solver.cpp:253]     Train net output #0: loss = 1.64416 (* 1 = 1.64416 loss)
I0520 21:25:11.446319  8812 sgd_solver.cpp:106] Iteration 2346, lr = 0.0025
I0520 21:25:19.453838  8812 solver.cpp:237] Iteration 2392, loss = 1.66791
I0520 21:25:19.453994  8812 solver.cpp:253]     Train net output #0: loss = 1.66791 (* 1 = 1.66791 loss)
I0520 21:25:19.454007  8812 sgd_solver.cpp:106] Iteration 2392, lr = 0.0025
I0520 21:25:27.464915  8812 solver.cpp:237] Iteration 2438, loss = 1.66619
I0520 21:25:27.464951  8812 solver.cpp:253]     Train net output #0: loss = 1.66619 (* 1 = 1.66619 loss)
I0520 21:25:27.464967  8812 sgd_solver.cpp:106] Iteration 2438, lr = 0.0025
I0520 21:25:35.477711  8812 solver.cpp:237] Iteration 2484, loss = 1.70287
I0520 21:25:35.477744  8812 solver.cpp:253]     Train net output #0: loss = 1.70287 (* 1 = 1.70287 loss)
I0520 21:25:35.477757  8812 sgd_solver.cpp:106] Iteration 2484, lr = 0.0025
I0520 21:26:05.665602  8812 solver.cpp:237] Iteration 2530, loss = 1.63868
I0520 21:26:05.665777  8812 solver.cpp:253]     Train net output #0: loss = 1.63868 (* 1 = 1.63868 loss)
I0520 21:26:05.665792  8812 sgd_solver.cpp:106] Iteration 2530, lr = 0.0025
I0520 21:26:13.673835  8812 solver.cpp:237] Iteration 2576, loss = 1.68847
I0520 21:26:13.673871  8812 solver.cpp:253]     Train net output #0: loss = 1.68847 (* 1 = 1.68847 loss)
I0520 21:26:13.673894  8812 sgd_solver.cpp:106] Iteration 2576, lr = 0.0025
I0520 21:26:21.684859  8812 solver.cpp:237] Iteration 2622, loss = 1.67508
I0520 21:26:21.684895  8812 solver.cpp:253]     Train net output #0: loss = 1.67508 (* 1 = 1.67508 loss)
I0520 21:26:21.684911  8812 sgd_solver.cpp:106] Iteration 2622, lr = 0.0025
I0520 21:26:29.694542  8812 solver.cpp:237] Iteration 2668, loss = 1.686
I0520 21:26:29.694576  8812 solver.cpp:253]     Train net output #0: loss = 1.686 (* 1 = 1.686 loss)
I0520 21:26:29.694591  8812 sgd_solver.cpp:106] Iteration 2668, lr = 0.0025
I0520 21:26:37.702538  8812 solver.cpp:237] Iteration 2714, loss = 1.69398
I0520 21:26:37.702687  8812 solver.cpp:253]     Train net output #0: loss = 1.69398 (* 1 = 1.69398 loss)
I0520 21:26:37.702700  8812 sgd_solver.cpp:106] Iteration 2714, lr = 0.0025
I0520 21:26:45.712100  8812 solver.cpp:237] Iteration 2760, loss = 1.49992
I0520 21:26:45.712136  8812 solver.cpp:253]     Train net output #0: loss = 1.49992 (* 1 = 1.49992 loss)
I0520 21:26:45.712152  8812 sgd_solver.cpp:106] Iteration 2760, lr = 0.0025
I0520 21:26:53.723522  8812 solver.cpp:237] Iteration 2806, loss = 1.52797
I0520 21:26:53.723556  8812 solver.cpp:253]     Train net output #0: loss = 1.52797 (* 1 = 1.52797 loss)
I0520 21:26:53.723572  8812 sgd_solver.cpp:106] Iteration 2806, lr = 0.0025
I0520 21:26:53.897814  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_2808.caffemodel
I0520 21:26:54.058293  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_2808.solverstate
I0520 21:26:54.486369  8812 solver.cpp:341] Iteration 2811, Testing net (#0)
I0520 21:27:39.527351  8812 solver.cpp:409]     Test net output #0: accuracy = 0.690418
I0520 21:27:39.527510  8812 solver.cpp:409]     Test net output #1: loss = 1.04224 (* 1 = 1.04224 loss)
I0520 21:28:08.863598  8812 solver.cpp:237] Iteration 2852, loss = 1.61795
I0520 21:28:08.863648  8812 solver.cpp:253]     Train net output #0: loss = 1.61795 (* 1 = 1.61795 loss)
I0520 21:28:08.863664  8812 sgd_solver.cpp:106] Iteration 2852, lr = 0.0025
I0520 21:28:16.868846  8812 solver.cpp:237] Iteration 2898, loss = 1.66075
I0520 21:28:16.868995  8812 solver.cpp:253]     Train net output #0: loss = 1.66075 (* 1 = 1.66075 loss)
I0520 21:28:16.869009  8812 sgd_solver.cpp:106] Iteration 2898, lr = 0.0025
I0520 21:28:24.869176  8812 solver.cpp:237] Iteration 2944, loss = 1.58734
I0520 21:28:24.869210  8812 solver.cpp:253]     Train net output #0: loss = 1.58734 (* 1 = 1.58734 loss)
I0520 21:28:24.869226  8812 sgd_solver.cpp:106] Iteration 2944, lr = 0.0025
I0520 21:28:32.871997  8812 solver.cpp:237] Iteration 2990, loss = 1.64384
I0520 21:28:32.872031  8812 solver.cpp:253]     Train net output #0: loss = 1.64384 (* 1 = 1.64384 loss)
I0520 21:28:32.872046  8812 sgd_solver.cpp:106] Iteration 2990, lr = 0.0025
I0520 21:28:40.877213  8812 solver.cpp:237] Iteration 3036, loss = 1.59285
I0520 21:28:40.877251  8812 solver.cpp:253]     Train net output #0: loss = 1.59285 (* 1 = 1.59285 loss)
I0520 21:28:40.877269  8812 sgd_solver.cpp:106] Iteration 3036, lr = 0.0025
I0520 21:28:48.883201  8812 solver.cpp:237] Iteration 3082, loss = 1.61858
I0520 21:28:48.883352  8812 solver.cpp:253]     Train net output #0: loss = 1.61858 (* 1 = 1.61858 loss)
I0520 21:28:48.883365  8812 sgd_solver.cpp:106] Iteration 3082, lr = 0.0025
I0520 21:29:19.080204  8812 solver.cpp:237] Iteration 3128, loss = 1.57568
I0520 21:29:19.080371  8812 solver.cpp:253]     Train net output #0: loss = 1.57568 (* 1 = 1.57568 loss)
I0520 21:29:19.080387  8812 sgd_solver.cpp:106] Iteration 3128, lr = 0.0025
I0520 21:29:27.086309  8812 solver.cpp:237] Iteration 3174, loss = 1.44993
I0520 21:29:27.086344  8812 solver.cpp:253]     Train net output #0: loss = 1.44993 (* 1 = 1.44993 loss)
I0520 21:29:27.086359  8812 sgd_solver.cpp:106] Iteration 3174, lr = 0.0025
I0520 21:29:35.093024  8812 solver.cpp:237] Iteration 3220, loss = 1.72043
I0520 21:29:35.093065  8812 solver.cpp:253]     Train net output #0: loss = 1.72043 (* 1 = 1.72043 loss)
I0520 21:29:35.093081  8812 sgd_solver.cpp:106] Iteration 3220, lr = 0.0025
I0520 21:29:43.097589  8812 solver.cpp:237] Iteration 3266, loss = 1.62286
I0520 21:29:43.097623  8812 solver.cpp:253]     Train net output #0: loss = 1.62286 (* 1 = 1.62286 loss)
I0520 21:29:43.097636  8812 sgd_solver.cpp:106] Iteration 3266, lr = 0.0025
I0520 21:29:44.662994  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_3276.caffemodel
I0520 21:29:44.822952  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_3276.solverstate
I0520 21:29:51.163718  8812 solver.cpp:237] Iteration 3312, loss = 1.64041
I0520 21:29:51.163874  8812 solver.cpp:253]     Train net output #0: loss = 1.64041 (* 1 = 1.64041 loss)
I0520 21:29:51.163888  8812 sgd_solver.cpp:106] Iteration 3312, lr = 0.0025
I0520 21:29:59.168131  8812 solver.cpp:237] Iteration 3358, loss = 1.55766
I0520 21:29:59.168164  8812 solver.cpp:253]     Train net output #0: loss = 1.55766 (* 1 = 1.55766 loss)
I0520 21:29:59.168186  8812 sgd_solver.cpp:106] Iteration 3358, lr = 0.0025
I0520 21:30:07.172442  8812 solver.cpp:237] Iteration 3404, loss = 1.45879
I0520 21:30:07.172477  8812 solver.cpp:253]     Train net output #0: loss = 1.45879 (* 1 = 1.45879 loss)
I0520 21:30:07.172493  8812 sgd_solver.cpp:106] Iteration 3404, lr = 0.0025
I0520 21:30:37.369225  8812 solver.cpp:237] Iteration 3450, loss = 1.48334
I0520 21:30:37.369393  8812 solver.cpp:253]     Train net output #0: loss = 1.48334 (* 1 = 1.48334 loss)
I0520 21:30:37.369410  8812 sgd_solver.cpp:106] Iteration 3450, lr = 0.0025
I0520 21:30:45.377025  8812 solver.cpp:237] Iteration 3496, loss = 1.51364
I0520 21:30:45.377061  8812 solver.cpp:253]     Train net output #0: loss = 1.51364 (* 1 = 1.51364 loss)
I0520 21:30:45.377079  8812 sgd_solver.cpp:106] Iteration 3496, lr = 0.0025
I0520 21:30:53.381585  8812 solver.cpp:237] Iteration 3542, loss = 1.5536
I0520 21:30:53.381619  8812 solver.cpp:253]     Train net output #0: loss = 1.5536 (* 1 = 1.5536 loss)
I0520 21:30:53.381635  8812 sgd_solver.cpp:106] Iteration 3542, lr = 0.0025
I0520 21:31:01.386998  8812 solver.cpp:237] Iteration 3588, loss = 1.66744
I0520 21:31:01.387030  8812 solver.cpp:253]     Train net output #0: loss = 1.66744 (* 1 = 1.66744 loss)
I0520 21:31:01.387048  8812 sgd_solver.cpp:106] Iteration 3588, lr = 0.0025
I0520 21:31:09.393548  8812 solver.cpp:237] Iteration 3634, loss = 1.52921
I0520 21:31:09.393692  8812 solver.cpp:253]     Train net output #0: loss = 1.52921 (* 1 = 1.52921 loss)
I0520 21:31:09.393707  8812 sgd_solver.cpp:106] Iteration 3634, lr = 0.0025
I0520 21:31:17.398118  8812 solver.cpp:237] Iteration 3680, loss = 1.60806
I0520 21:31:17.398151  8812 solver.cpp:253]     Train net output #0: loss = 1.60806 (* 1 = 1.60806 loss)
I0520 21:31:17.398169  8812 sgd_solver.cpp:106] Iteration 3680, lr = 0.0025
I0520 21:31:25.401489  8812 solver.cpp:237] Iteration 3726, loss = 1.42574
I0520 21:31:25.401521  8812 solver.cpp:253]     Train net output #0: loss = 1.42574 (* 1 = 1.42574 loss)
I0520 21:31:25.401538  8812 sgd_solver.cpp:106] Iteration 3726, lr = 0.0025
I0520 21:31:28.361275  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_3744.caffemodel
I0520 21:31:28.522279  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_3744.solverstate
I0520 21:31:29.122174  8812 solver.cpp:341] Iteration 3748, Testing net (#0)
I0520 21:32:35.357820  8812 solver.cpp:409]     Test net output #0: accuracy = 0.719986
I0520 21:32:35.357998  8812 solver.cpp:409]     Test net output #1: loss = 0.942877 (* 1 = 0.942877 loss)
I0520 21:33:01.747586  8812 solver.cpp:237] Iteration 3772, loss = 1.53095
I0520 21:33:01.747637  8812 solver.cpp:253]     Train net output #0: loss = 1.53095 (* 1 = 1.53095 loss)
I0520 21:33:01.747653  8812 sgd_solver.cpp:106] Iteration 3772, lr = 0.0025
I0520 21:33:09.758373  8812 solver.cpp:237] Iteration 3818, loss = 1.53236
I0520 21:33:09.758533  8812 solver.cpp:253]     Train net output #0: loss = 1.53236 (* 1 = 1.53236 loss)
I0520 21:33:09.758546  8812 sgd_solver.cpp:106] Iteration 3818, lr = 0.0025
I0520 21:33:17.766726  8812 solver.cpp:237] Iteration 3864, loss = 1.61373
I0520 21:33:17.766760  8812 solver.cpp:253]     Train net output #0: loss = 1.61373 (* 1 = 1.61373 loss)
I0520 21:33:17.766782  8812 sgd_solver.cpp:106] Iteration 3864, lr = 0.0025
I0520 21:33:25.778038  8812 solver.cpp:237] Iteration 3910, loss = 1.54618
I0520 21:33:25.778072  8812 solver.cpp:253]     Train net output #0: loss = 1.54618 (* 1 = 1.54618 loss)
I0520 21:33:25.778089  8812 sgd_solver.cpp:106] Iteration 3910, lr = 0.0025
I0520 21:33:33.789372  8812 solver.cpp:237] Iteration 3956, loss = 1.56263
I0520 21:33:33.789405  8812 solver.cpp:253]     Train net output #0: loss = 1.56263 (* 1 = 1.56263 loss)
I0520 21:33:33.789422  8812 sgd_solver.cpp:106] Iteration 3956, lr = 0.0025
I0520 21:33:41.798987  8812 solver.cpp:237] Iteration 4002, loss = 1.52333
I0520 21:33:41.799142  8812 solver.cpp:253]     Train net output #0: loss = 1.52333 (* 1 = 1.52333 loss)
I0520 21:33:41.799156  8812 sgd_solver.cpp:106] Iteration 4002, lr = 0.0025
I0520 21:33:49.810160  8812 solver.cpp:237] Iteration 4048, loss = 1.56478
I0520 21:33:49.810194  8812 solver.cpp:253]     Train net output #0: loss = 1.56478 (* 1 = 1.56478 loss)
I0520 21:33:49.810212  8812 sgd_solver.cpp:106] Iteration 4048, lr = 0.0025
I0520 21:34:19.995602  8812 solver.cpp:237] Iteration 4094, loss = 1.56389
I0520 21:34:19.995767  8812 solver.cpp:253]     Train net output #0: loss = 1.56389 (* 1 = 1.56389 loss)
I0520 21:34:19.995784  8812 sgd_solver.cpp:106] Iteration 4094, lr = 0.0025
I0520 21:34:28.002905  8812 solver.cpp:237] Iteration 4140, loss = 1.42662
I0520 21:34:28.002939  8812 solver.cpp:253]     Train net output #0: loss = 1.42662 (* 1 = 1.42662 loss)
I0520 21:34:28.002956  8812 sgd_solver.cpp:106] Iteration 4140, lr = 0.0025
I0520 21:34:36.015748  8812 solver.cpp:237] Iteration 4186, loss = 1.44812
I0520 21:34:36.015781  8812 solver.cpp:253]     Train net output #0: loss = 1.44812 (* 1 = 1.44812 loss)
I0520 21:34:36.015800  8812 sgd_solver.cpp:106] Iteration 4186, lr = 0.0025
I0520 21:34:40.369307  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_4212.caffemodel
I0520 21:34:40.540324  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_4212.solverstate
I0520 21:34:44.104260  8812 solver.cpp:237] Iteration 4232, loss = 1.59546
I0520 21:34:44.104308  8812 solver.cpp:253]     Train net output #0: loss = 1.59546 (* 1 = 1.59546 loss)
I0520 21:34:44.104324  8812 sgd_solver.cpp:106] Iteration 4232, lr = 0.0025
I0520 21:34:52.109020  8812 solver.cpp:237] Iteration 4278, loss = 1.4487
I0520 21:34:52.109179  8812 solver.cpp:253]     Train net output #0: loss = 1.4487 (* 1 = 1.4487 loss)
I0520 21:34:52.109191  8812 sgd_solver.cpp:106] Iteration 4278, lr = 0.0025
I0520 21:35:00.121047  8812 solver.cpp:237] Iteration 4324, loss = 1.43965
I0520 21:35:00.121079  8812 solver.cpp:253]     Train net output #0: loss = 1.43965 (* 1 = 1.43965 loss)
I0520 21:35:00.121098  8812 sgd_solver.cpp:106] Iteration 4324, lr = 0.0025
I0520 21:35:08.130290  8812 solver.cpp:237] Iteration 4370, loss = 1.79998
I0520 21:35:08.130322  8812 solver.cpp:253]     Train net output #0: loss = 1.79998 (* 1 = 1.79998 loss)
I0520 21:35:08.130337  8812 sgd_solver.cpp:106] Iteration 4370, lr = 0.0025
I0520 21:35:38.279825  8812 solver.cpp:237] Iteration 4416, loss = 1.50105
I0520 21:35:38.279997  8812 solver.cpp:253]     Train net output #0: loss = 1.50105 (* 1 = 1.50105 loss)
I0520 21:35:38.280014  8812 sgd_solver.cpp:106] Iteration 4416, lr = 0.0025
I0520 21:35:46.288744  8812 solver.cpp:237] Iteration 4462, loss = 1.53105
I0520 21:35:46.288789  8812 solver.cpp:253]     Train net output #0: loss = 1.53105 (* 1 = 1.53105 loss)
I0520 21:35:46.288805  8812 sgd_solver.cpp:106] Iteration 4462, lr = 0.0025
I0520 21:35:54.298688  8812 solver.cpp:237] Iteration 4508, loss = 1.50197
I0520 21:35:54.298722  8812 solver.cpp:253]     Train net output #0: loss = 1.50197 (* 1 = 1.50197 loss)
I0520 21:35:54.298738  8812 sgd_solver.cpp:106] Iteration 4508, lr = 0.0025
I0520 21:36:02.306641  8812 solver.cpp:237] Iteration 4554, loss = 1.49382
I0520 21:36:02.306674  8812 solver.cpp:253]     Train net output #0: loss = 1.49382 (* 1 = 1.49382 loss)
I0520 21:36:02.306690  8812 sgd_solver.cpp:106] Iteration 4554, lr = 0.0025
I0520 21:36:10.313395  8812 solver.cpp:237] Iteration 4600, loss = 1.51389
I0520 21:36:10.313551  8812 solver.cpp:253]     Train net output #0: loss = 1.51389 (* 1 = 1.51389 loss)
I0520 21:36:10.313565  8812 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0520 21:36:18.317870  8812 solver.cpp:237] Iteration 4646, loss = 1.46181
I0520 21:36:18.317904  8812 solver.cpp:253]     Train net output #0: loss = 1.46181 (* 1 = 1.46181 loss)
I0520 21:36:18.317920  8812 sgd_solver.cpp:106] Iteration 4646, lr = 0.0025
I0520 21:36:24.064026  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_4680.caffemodel
I0520 21:36:24.226728  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_4680.solverstate
I0520 21:36:25.003334  8812 solver.cpp:341] Iteration 4685, Testing net (#0)
I0520 21:37:10.348608  8812 solver.cpp:409]     Test net output #0: accuracy = 0.775581
I0520 21:37:10.348781  8812 solver.cpp:409]     Test net output #1: loss = 0.817354 (* 1 = 0.817354 loss)
I0520 21:37:10.574918  8812 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_4687.caffemodel
I0520 21:37:10.737233  8812 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853_iter_4687.solverstate
I0520 21:37:10.765204  8812 solver.cpp:326] Optimization Done.
I0520 21:37:10.765233  8812 caffe.cpp:215] Optimization Done.
Application 11235424 resources: utime ~1259s, stime ~228s, Rss ~5329344, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_320_2016-05-20T11.20.44.404853.solver"
	User time (seconds): 0.56
	System time (seconds): 0.13
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:52.22
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 1
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 3112
	Involuntary context switches: 180
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
