2805179
I0520 14:10:34.403209 26180 caffe.cpp:184] Using GPUs 0
I0520 14:10:34.835536 26180 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1363
test_interval: 2727
base_lr: 0.0025
display: 136
max_iter: 13636
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1363
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879.prototxt"
I0520 14:10:34.837152 26180 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879.prototxt
I0520 14:10:34.856205 26180 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 14:10:34.856264 26180 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 14:10:34.856607 26180 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 110
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:10:34.856788 26180 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:10:34.856812 26180 net.cpp:106] Creating Layer data_hdf5
I0520 14:10:34.856827 26180 net.cpp:411] data_hdf5 -> data
I0520 14:10:34.856860 26180 net.cpp:411] data_hdf5 -> label
I0520 14:10:34.856892 26180 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 14:10:34.858067 26180 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 14:10:34.860286 26180 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 14:10:56.348991 26180 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 14:10:56.354053 26180 net.cpp:150] Setting up data_hdf5
I0520 14:10:56.354094 26180 net.cpp:157] Top shape: 110 1 127 50 (698500)
I0520 14:10:56.354109 26180 net.cpp:157] Top shape: 110 (110)
I0520 14:10:56.354120 26180 net.cpp:165] Memory required for data: 2794440
I0520 14:10:56.354135 26180 layer_factory.hpp:77] Creating layer conv1
I0520 14:10:56.354167 26180 net.cpp:106] Creating Layer conv1
I0520 14:10:56.354178 26180 net.cpp:454] conv1 <- data
I0520 14:10:56.354199 26180 net.cpp:411] conv1 -> conv1
I0520 14:10:56.917989 26180 net.cpp:150] Setting up conv1
I0520 14:10:56.918036 26180 net.cpp:157] Top shape: 110 12 120 48 (7603200)
I0520 14:10:56.918046 26180 net.cpp:165] Memory required for data: 33207240
I0520 14:10:56.918076 26180 layer_factory.hpp:77] Creating layer relu1
I0520 14:10:56.918097 26180 net.cpp:106] Creating Layer relu1
I0520 14:10:56.918107 26180 net.cpp:454] relu1 <- conv1
I0520 14:10:56.918120 26180 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:10:56.918632 26180 net.cpp:150] Setting up relu1
I0520 14:10:56.918649 26180 net.cpp:157] Top shape: 110 12 120 48 (7603200)
I0520 14:10:56.918660 26180 net.cpp:165] Memory required for data: 63620040
I0520 14:10:56.918670 26180 layer_factory.hpp:77] Creating layer pool1
I0520 14:10:56.918686 26180 net.cpp:106] Creating Layer pool1
I0520 14:10:56.918696 26180 net.cpp:454] pool1 <- conv1
I0520 14:10:56.918710 26180 net.cpp:411] pool1 -> pool1
I0520 14:10:56.918789 26180 net.cpp:150] Setting up pool1
I0520 14:10:56.918803 26180 net.cpp:157] Top shape: 110 12 60 48 (3801600)
I0520 14:10:56.918813 26180 net.cpp:165] Memory required for data: 78826440
I0520 14:10:56.918824 26180 layer_factory.hpp:77] Creating layer conv2
I0520 14:10:56.918846 26180 net.cpp:106] Creating Layer conv2
I0520 14:10:56.918858 26180 net.cpp:454] conv2 <- pool1
I0520 14:10:56.918870 26180 net.cpp:411] conv2 -> conv2
I0520 14:10:56.921608 26180 net.cpp:150] Setting up conv2
I0520 14:10:56.921632 26180 net.cpp:157] Top shape: 110 20 54 46 (5464800)
I0520 14:10:56.921641 26180 net.cpp:165] Memory required for data: 100685640
I0520 14:10:56.921660 26180 layer_factory.hpp:77] Creating layer relu2
I0520 14:10:56.921675 26180 net.cpp:106] Creating Layer relu2
I0520 14:10:56.921685 26180 net.cpp:454] relu2 <- conv2
I0520 14:10:56.921699 26180 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:10:56.922029 26180 net.cpp:150] Setting up relu2
I0520 14:10:56.922044 26180 net.cpp:157] Top shape: 110 20 54 46 (5464800)
I0520 14:10:56.922055 26180 net.cpp:165] Memory required for data: 122544840
I0520 14:10:56.922065 26180 layer_factory.hpp:77] Creating layer pool2
I0520 14:10:56.922076 26180 net.cpp:106] Creating Layer pool2
I0520 14:10:56.922086 26180 net.cpp:454] pool2 <- conv2
I0520 14:10:56.922111 26180 net.cpp:411] pool2 -> pool2
I0520 14:10:56.922180 26180 net.cpp:150] Setting up pool2
I0520 14:10:56.922194 26180 net.cpp:157] Top shape: 110 20 27 46 (2732400)
I0520 14:10:56.922202 26180 net.cpp:165] Memory required for data: 133474440
I0520 14:10:56.922212 26180 layer_factory.hpp:77] Creating layer conv3
I0520 14:10:56.922230 26180 net.cpp:106] Creating Layer conv3
I0520 14:10:56.922240 26180 net.cpp:454] conv3 <- pool2
I0520 14:10:56.922253 26180 net.cpp:411] conv3 -> conv3
I0520 14:10:56.924173 26180 net.cpp:150] Setting up conv3
I0520 14:10:56.924196 26180 net.cpp:157] Top shape: 110 28 22 44 (2981440)
I0520 14:10:56.924209 26180 net.cpp:165] Memory required for data: 145400200
I0520 14:10:56.924227 26180 layer_factory.hpp:77] Creating layer relu3
I0520 14:10:56.924243 26180 net.cpp:106] Creating Layer relu3
I0520 14:10:56.924253 26180 net.cpp:454] relu3 <- conv3
I0520 14:10:56.924266 26180 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:10:56.924736 26180 net.cpp:150] Setting up relu3
I0520 14:10:56.924752 26180 net.cpp:157] Top shape: 110 28 22 44 (2981440)
I0520 14:10:56.924763 26180 net.cpp:165] Memory required for data: 157325960
I0520 14:10:56.924773 26180 layer_factory.hpp:77] Creating layer pool3
I0520 14:10:56.924787 26180 net.cpp:106] Creating Layer pool3
I0520 14:10:56.924796 26180 net.cpp:454] pool3 <- conv3
I0520 14:10:56.924809 26180 net.cpp:411] pool3 -> pool3
I0520 14:10:56.924876 26180 net.cpp:150] Setting up pool3
I0520 14:10:56.924890 26180 net.cpp:157] Top shape: 110 28 11 44 (1490720)
I0520 14:10:56.924899 26180 net.cpp:165] Memory required for data: 163288840
I0520 14:10:56.924908 26180 layer_factory.hpp:77] Creating layer conv4
I0520 14:10:56.924926 26180 net.cpp:106] Creating Layer conv4
I0520 14:10:56.924937 26180 net.cpp:454] conv4 <- pool3
I0520 14:10:56.924950 26180 net.cpp:411] conv4 -> conv4
I0520 14:10:56.927894 26180 net.cpp:150] Setting up conv4
I0520 14:10:56.927923 26180 net.cpp:157] Top shape: 110 36 6 42 (997920)
I0520 14:10:56.927933 26180 net.cpp:165] Memory required for data: 167280520
I0520 14:10:56.927949 26180 layer_factory.hpp:77] Creating layer relu4
I0520 14:10:56.927963 26180 net.cpp:106] Creating Layer relu4
I0520 14:10:56.927973 26180 net.cpp:454] relu4 <- conv4
I0520 14:10:56.927986 26180 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:10:56.928452 26180 net.cpp:150] Setting up relu4
I0520 14:10:56.928468 26180 net.cpp:157] Top shape: 110 36 6 42 (997920)
I0520 14:10:56.928479 26180 net.cpp:165] Memory required for data: 171272200
I0520 14:10:56.928489 26180 layer_factory.hpp:77] Creating layer pool4
I0520 14:10:56.928503 26180 net.cpp:106] Creating Layer pool4
I0520 14:10:56.928513 26180 net.cpp:454] pool4 <- conv4
I0520 14:10:56.928525 26180 net.cpp:411] pool4 -> pool4
I0520 14:10:56.928593 26180 net.cpp:150] Setting up pool4
I0520 14:10:56.928606 26180 net.cpp:157] Top shape: 110 36 3 42 (498960)
I0520 14:10:56.928617 26180 net.cpp:165] Memory required for data: 173268040
I0520 14:10:56.928627 26180 layer_factory.hpp:77] Creating layer ip1
I0520 14:10:56.928647 26180 net.cpp:106] Creating Layer ip1
I0520 14:10:56.928656 26180 net.cpp:454] ip1 <- pool4
I0520 14:10:56.928670 26180 net.cpp:411] ip1 -> ip1
I0520 14:10:56.944097 26180 net.cpp:150] Setting up ip1
I0520 14:10:56.944124 26180 net.cpp:157] Top shape: 110 196 (21560)
I0520 14:10:56.944139 26180 net.cpp:165] Memory required for data: 173354280
I0520 14:10:56.944160 26180 layer_factory.hpp:77] Creating layer relu5
I0520 14:10:56.944175 26180 net.cpp:106] Creating Layer relu5
I0520 14:10:56.944185 26180 net.cpp:454] relu5 <- ip1
I0520 14:10:56.944198 26180 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:10:56.944541 26180 net.cpp:150] Setting up relu5
I0520 14:10:56.944555 26180 net.cpp:157] Top shape: 110 196 (21560)
I0520 14:10:56.944566 26180 net.cpp:165] Memory required for data: 173440520
I0520 14:10:56.944573 26180 layer_factory.hpp:77] Creating layer drop1
I0520 14:10:56.944597 26180 net.cpp:106] Creating Layer drop1
I0520 14:10:56.944607 26180 net.cpp:454] drop1 <- ip1
I0520 14:10:56.944631 26180 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:10:56.944680 26180 net.cpp:150] Setting up drop1
I0520 14:10:56.944694 26180 net.cpp:157] Top shape: 110 196 (21560)
I0520 14:10:56.944703 26180 net.cpp:165] Memory required for data: 173526760
I0520 14:10:56.944713 26180 layer_factory.hpp:77] Creating layer ip2
I0520 14:10:56.944732 26180 net.cpp:106] Creating Layer ip2
I0520 14:10:56.944742 26180 net.cpp:454] ip2 <- ip1
I0520 14:10:56.944756 26180 net.cpp:411] ip2 -> ip2
I0520 14:10:56.945221 26180 net.cpp:150] Setting up ip2
I0520 14:10:56.945235 26180 net.cpp:157] Top shape: 110 98 (10780)
I0520 14:10:56.945245 26180 net.cpp:165] Memory required for data: 173569880
I0520 14:10:56.945261 26180 layer_factory.hpp:77] Creating layer relu6
I0520 14:10:56.945272 26180 net.cpp:106] Creating Layer relu6
I0520 14:10:56.945282 26180 net.cpp:454] relu6 <- ip2
I0520 14:10:56.945294 26180 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:10:56.945891 26180 net.cpp:150] Setting up relu6
I0520 14:10:56.945914 26180 net.cpp:157] Top shape: 110 98 (10780)
I0520 14:10:56.945927 26180 net.cpp:165] Memory required for data: 173613000
I0520 14:10:56.945936 26180 layer_factory.hpp:77] Creating layer drop2
I0520 14:10:56.945950 26180 net.cpp:106] Creating Layer drop2
I0520 14:10:56.945961 26180 net.cpp:454] drop2 <- ip2
I0520 14:10:56.945974 26180 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:10:56.946017 26180 net.cpp:150] Setting up drop2
I0520 14:10:56.946030 26180 net.cpp:157] Top shape: 110 98 (10780)
I0520 14:10:56.946040 26180 net.cpp:165] Memory required for data: 173656120
I0520 14:10:56.946051 26180 layer_factory.hpp:77] Creating layer ip3
I0520 14:10:56.946065 26180 net.cpp:106] Creating Layer ip3
I0520 14:10:56.946075 26180 net.cpp:454] ip3 <- ip2
I0520 14:10:56.946089 26180 net.cpp:411] ip3 -> ip3
I0520 14:10:56.946298 26180 net.cpp:150] Setting up ip3
I0520 14:10:56.946311 26180 net.cpp:157] Top shape: 110 11 (1210)
I0520 14:10:56.946321 26180 net.cpp:165] Memory required for data: 173660960
I0520 14:10:56.946336 26180 layer_factory.hpp:77] Creating layer drop3
I0520 14:10:56.946348 26180 net.cpp:106] Creating Layer drop3
I0520 14:10:56.946358 26180 net.cpp:454] drop3 <- ip3
I0520 14:10:56.946370 26180 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:10:56.946409 26180 net.cpp:150] Setting up drop3
I0520 14:10:56.946422 26180 net.cpp:157] Top shape: 110 11 (1210)
I0520 14:10:56.946434 26180 net.cpp:165] Memory required for data: 173665800
I0520 14:10:56.946444 26180 layer_factory.hpp:77] Creating layer loss
I0520 14:10:56.946462 26180 net.cpp:106] Creating Layer loss
I0520 14:10:56.946472 26180 net.cpp:454] loss <- ip3
I0520 14:10:56.946483 26180 net.cpp:454] loss <- label
I0520 14:10:56.946496 26180 net.cpp:411] loss -> loss
I0520 14:10:56.946513 26180 layer_factory.hpp:77] Creating layer loss
I0520 14:10:56.947151 26180 net.cpp:150] Setting up loss
I0520 14:10:56.947172 26180 net.cpp:157] Top shape: (1)
I0520 14:10:56.947185 26180 net.cpp:160]     with loss weight 1
I0520 14:10:56.947227 26180 net.cpp:165] Memory required for data: 173665804
I0520 14:10:56.947237 26180 net.cpp:226] loss needs backward computation.
I0520 14:10:56.947248 26180 net.cpp:226] drop3 needs backward computation.
I0520 14:10:56.947257 26180 net.cpp:226] ip3 needs backward computation.
I0520 14:10:56.947268 26180 net.cpp:226] drop2 needs backward computation.
I0520 14:10:56.947278 26180 net.cpp:226] relu6 needs backward computation.
I0520 14:10:56.947288 26180 net.cpp:226] ip2 needs backward computation.
I0520 14:10:56.947299 26180 net.cpp:226] drop1 needs backward computation.
I0520 14:10:56.947307 26180 net.cpp:226] relu5 needs backward computation.
I0520 14:10:56.947317 26180 net.cpp:226] ip1 needs backward computation.
I0520 14:10:56.947327 26180 net.cpp:226] pool4 needs backward computation.
I0520 14:10:56.947337 26180 net.cpp:226] relu4 needs backward computation.
I0520 14:10:56.947347 26180 net.cpp:226] conv4 needs backward computation.
I0520 14:10:56.947357 26180 net.cpp:226] pool3 needs backward computation.
I0520 14:10:56.947368 26180 net.cpp:226] relu3 needs backward computation.
I0520 14:10:56.947386 26180 net.cpp:226] conv3 needs backward computation.
I0520 14:10:56.947397 26180 net.cpp:226] pool2 needs backward computation.
I0520 14:10:56.947407 26180 net.cpp:226] relu2 needs backward computation.
I0520 14:10:56.947418 26180 net.cpp:226] conv2 needs backward computation.
I0520 14:10:56.947429 26180 net.cpp:226] pool1 needs backward computation.
I0520 14:10:56.947439 26180 net.cpp:226] relu1 needs backward computation.
I0520 14:10:56.947449 26180 net.cpp:226] conv1 needs backward computation.
I0520 14:10:56.947460 26180 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:10:56.947470 26180 net.cpp:270] This network produces output loss
I0520 14:10:56.947499 26180 net.cpp:283] Network initialization done.
I0520 14:10:56.949077 26180 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879.prototxt
I0520 14:10:56.949148 26180 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 14:10:56.949502 26180 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 110
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:10:56.949690 26180 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:10:56.949705 26180 net.cpp:106] Creating Layer data_hdf5
I0520 14:10:56.949718 26180 net.cpp:411] data_hdf5 -> data
I0520 14:10:56.949734 26180 net.cpp:411] data_hdf5 -> label
I0520 14:10:56.949751 26180 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 14:10:56.951019 26180 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 14:11:18.251652 26180 net.cpp:150] Setting up data_hdf5
I0520 14:11:18.251827 26180 net.cpp:157] Top shape: 110 1 127 50 (698500)
I0520 14:11:18.251842 26180 net.cpp:157] Top shape: 110 (110)
I0520 14:11:18.251853 26180 net.cpp:165] Memory required for data: 2794440
I0520 14:11:18.251866 26180 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 14:11:18.251895 26180 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 14:11:18.251906 26180 net.cpp:454] label_data_hdf5_1_split <- label
I0520 14:11:18.251921 26180 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 14:11:18.251942 26180 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 14:11:18.252015 26180 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 14:11:18.252029 26180 net.cpp:157] Top shape: 110 (110)
I0520 14:11:18.252041 26180 net.cpp:157] Top shape: 110 (110)
I0520 14:11:18.252051 26180 net.cpp:165] Memory required for data: 2795320
I0520 14:11:18.252060 26180 layer_factory.hpp:77] Creating layer conv1
I0520 14:11:18.252086 26180 net.cpp:106] Creating Layer conv1
I0520 14:11:18.252097 26180 net.cpp:454] conv1 <- data
I0520 14:11:18.252110 26180 net.cpp:411] conv1 -> conv1
I0520 14:11:18.254055 26180 net.cpp:150] Setting up conv1
I0520 14:11:18.254078 26180 net.cpp:157] Top shape: 110 12 120 48 (7603200)
I0520 14:11:18.254089 26180 net.cpp:165] Memory required for data: 33208120
I0520 14:11:18.254109 26180 layer_factory.hpp:77] Creating layer relu1
I0520 14:11:18.254124 26180 net.cpp:106] Creating Layer relu1
I0520 14:11:18.254134 26180 net.cpp:454] relu1 <- conv1
I0520 14:11:18.254147 26180 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:11:18.254639 26180 net.cpp:150] Setting up relu1
I0520 14:11:18.254655 26180 net.cpp:157] Top shape: 110 12 120 48 (7603200)
I0520 14:11:18.254667 26180 net.cpp:165] Memory required for data: 63620920
I0520 14:11:18.254678 26180 layer_factory.hpp:77] Creating layer pool1
I0520 14:11:18.254693 26180 net.cpp:106] Creating Layer pool1
I0520 14:11:18.254703 26180 net.cpp:454] pool1 <- conv1
I0520 14:11:18.254717 26180 net.cpp:411] pool1 -> pool1
I0520 14:11:18.254793 26180 net.cpp:150] Setting up pool1
I0520 14:11:18.254806 26180 net.cpp:157] Top shape: 110 12 60 48 (3801600)
I0520 14:11:18.254815 26180 net.cpp:165] Memory required for data: 78827320
I0520 14:11:18.254827 26180 layer_factory.hpp:77] Creating layer conv2
I0520 14:11:18.254843 26180 net.cpp:106] Creating Layer conv2
I0520 14:11:18.254854 26180 net.cpp:454] conv2 <- pool1
I0520 14:11:18.254868 26180 net.cpp:411] conv2 -> conv2
I0520 14:11:18.256796 26180 net.cpp:150] Setting up conv2
I0520 14:11:18.256819 26180 net.cpp:157] Top shape: 110 20 54 46 (5464800)
I0520 14:11:18.256831 26180 net.cpp:165] Memory required for data: 100686520
I0520 14:11:18.256850 26180 layer_factory.hpp:77] Creating layer relu2
I0520 14:11:18.256862 26180 net.cpp:106] Creating Layer relu2
I0520 14:11:18.256872 26180 net.cpp:454] relu2 <- conv2
I0520 14:11:18.256886 26180 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:11:18.257220 26180 net.cpp:150] Setting up relu2
I0520 14:11:18.257233 26180 net.cpp:157] Top shape: 110 20 54 46 (5464800)
I0520 14:11:18.257243 26180 net.cpp:165] Memory required for data: 122545720
I0520 14:11:18.257253 26180 layer_factory.hpp:77] Creating layer pool2
I0520 14:11:18.257267 26180 net.cpp:106] Creating Layer pool2
I0520 14:11:18.257277 26180 net.cpp:454] pool2 <- conv2
I0520 14:11:18.257289 26180 net.cpp:411] pool2 -> pool2
I0520 14:11:18.257361 26180 net.cpp:150] Setting up pool2
I0520 14:11:18.257375 26180 net.cpp:157] Top shape: 110 20 27 46 (2732400)
I0520 14:11:18.257385 26180 net.cpp:165] Memory required for data: 133475320
I0520 14:11:18.257395 26180 layer_factory.hpp:77] Creating layer conv3
I0520 14:11:18.257411 26180 net.cpp:106] Creating Layer conv3
I0520 14:11:18.257422 26180 net.cpp:454] conv3 <- pool2
I0520 14:11:18.257436 26180 net.cpp:411] conv3 -> conv3
I0520 14:11:18.259426 26180 net.cpp:150] Setting up conv3
I0520 14:11:18.259449 26180 net.cpp:157] Top shape: 110 28 22 44 (2981440)
I0520 14:11:18.259461 26180 net.cpp:165] Memory required for data: 145401080
I0520 14:11:18.259501 26180 layer_factory.hpp:77] Creating layer relu3
I0520 14:11:18.259516 26180 net.cpp:106] Creating Layer relu3
I0520 14:11:18.259526 26180 net.cpp:454] relu3 <- conv3
I0520 14:11:18.259538 26180 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:11:18.260010 26180 net.cpp:150] Setting up relu3
I0520 14:11:18.260027 26180 net.cpp:157] Top shape: 110 28 22 44 (2981440)
I0520 14:11:18.260037 26180 net.cpp:165] Memory required for data: 157326840
I0520 14:11:18.260047 26180 layer_factory.hpp:77] Creating layer pool3
I0520 14:11:18.260061 26180 net.cpp:106] Creating Layer pool3
I0520 14:11:18.260071 26180 net.cpp:454] pool3 <- conv3
I0520 14:11:18.260084 26180 net.cpp:411] pool3 -> pool3
I0520 14:11:18.260155 26180 net.cpp:150] Setting up pool3
I0520 14:11:18.260169 26180 net.cpp:157] Top shape: 110 28 11 44 (1490720)
I0520 14:11:18.260179 26180 net.cpp:165] Memory required for data: 163289720
I0520 14:11:18.260186 26180 layer_factory.hpp:77] Creating layer conv4
I0520 14:11:18.260205 26180 net.cpp:106] Creating Layer conv4
I0520 14:11:18.260215 26180 net.cpp:454] conv4 <- pool3
I0520 14:11:18.260231 26180 net.cpp:411] conv4 -> conv4
I0520 14:11:18.262305 26180 net.cpp:150] Setting up conv4
I0520 14:11:18.262326 26180 net.cpp:157] Top shape: 110 36 6 42 (997920)
I0520 14:11:18.262339 26180 net.cpp:165] Memory required for data: 167281400
I0520 14:11:18.262353 26180 layer_factory.hpp:77] Creating layer relu4
I0520 14:11:18.262367 26180 net.cpp:106] Creating Layer relu4
I0520 14:11:18.262377 26180 net.cpp:454] relu4 <- conv4
I0520 14:11:18.262390 26180 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:11:18.262859 26180 net.cpp:150] Setting up relu4
I0520 14:11:18.262876 26180 net.cpp:157] Top shape: 110 36 6 42 (997920)
I0520 14:11:18.262887 26180 net.cpp:165] Memory required for data: 171273080
I0520 14:11:18.262897 26180 layer_factory.hpp:77] Creating layer pool4
I0520 14:11:18.262910 26180 net.cpp:106] Creating Layer pool4
I0520 14:11:18.262920 26180 net.cpp:454] pool4 <- conv4
I0520 14:11:18.262934 26180 net.cpp:411] pool4 -> pool4
I0520 14:11:18.263005 26180 net.cpp:150] Setting up pool4
I0520 14:11:18.263020 26180 net.cpp:157] Top shape: 110 36 3 42 (498960)
I0520 14:11:18.263028 26180 net.cpp:165] Memory required for data: 173268920
I0520 14:11:18.263036 26180 layer_factory.hpp:77] Creating layer ip1
I0520 14:11:18.263052 26180 net.cpp:106] Creating Layer ip1
I0520 14:11:18.263063 26180 net.cpp:454] ip1 <- pool4
I0520 14:11:18.263077 26180 net.cpp:411] ip1 -> ip1
I0520 14:11:18.278535 26180 net.cpp:150] Setting up ip1
I0520 14:11:18.278563 26180 net.cpp:157] Top shape: 110 196 (21560)
I0520 14:11:18.278574 26180 net.cpp:165] Memory required for data: 173355160
I0520 14:11:18.278596 26180 layer_factory.hpp:77] Creating layer relu5
I0520 14:11:18.278611 26180 net.cpp:106] Creating Layer relu5
I0520 14:11:18.278622 26180 net.cpp:454] relu5 <- ip1
I0520 14:11:18.278635 26180 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:11:18.278982 26180 net.cpp:150] Setting up relu5
I0520 14:11:18.278996 26180 net.cpp:157] Top shape: 110 196 (21560)
I0520 14:11:18.279006 26180 net.cpp:165] Memory required for data: 173441400
I0520 14:11:18.279016 26180 layer_factory.hpp:77] Creating layer drop1
I0520 14:11:18.279034 26180 net.cpp:106] Creating Layer drop1
I0520 14:11:18.279044 26180 net.cpp:454] drop1 <- ip1
I0520 14:11:18.279057 26180 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:11:18.279104 26180 net.cpp:150] Setting up drop1
I0520 14:11:18.279116 26180 net.cpp:157] Top shape: 110 196 (21560)
I0520 14:11:18.279126 26180 net.cpp:165] Memory required for data: 173527640
I0520 14:11:18.279135 26180 layer_factory.hpp:77] Creating layer ip2
I0520 14:11:18.279150 26180 net.cpp:106] Creating Layer ip2
I0520 14:11:18.279160 26180 net.cpp:454] ip2 <- ip1
I0520 14:11:18.279173 26180 net.cpp:411] ip2 -> ip2
I0520 14:11:18.279659 26180 net.cpp:150] Setting up ip2
I0520 14:11:18.279671 26180 net.cpp:157] Top shape: 110 98 (10780)
I0520 14:11:18.279682 26180 net.cpp:165] Memory required for data: 173570760
I0520 14:11:18.279698 26180 layer_factory.hpp:77] Creating layer relu6
I0520 14:11:18.279723 26180 net.cpp:106] Creating Layer relu6
I0520 14:11:18.279733 26180 net.cpp:454] relu6 <- ip2
I0520 14:11:18.279747 26180 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:11:18.280282 26180 net.cpp:150] Setting up relu6
I0520 14:11:18.280305 26180 net.cpp:157] Top shape: 110 98 (10780)
I0520 14:11:18.280315 26180 net.cpp:165] Memory required for data: 173613880
I0520 14:11:18.280325 26180 layer_factory.hpp:77] Creating layer drop2
I0520 14:11:18.280339 26180 net.cpp:106] Creating Layer drop2
I0520 14:11:18.280349 26180 net.cpp:454] drop2 <- ip2
I0520 14:11:18.280362 26180 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:11:18.280406 26180 net.cpp:150] Setting up drop2
I0520 14:11:18.280419 26180 net.cpp:157] Top shape: 110 98 (10780)
I0520 14:11:18.280429 26180 net.cpp:165] Memory required for data: 173657000
I0520 14:11:18.280439 26180 layer_factory.hpp:77] Creating layer ip3
I0520 14:11:18.280453 26180 net.cpp:106] Creating Layer ip3
I0520 14:11:18.280462 26180 net.cpp:454] ip3 <- ip2
I0520 14:11:18.280477 26180 net.cpp:411] ip3 -> ip3
I0520 14:11:18.280699 26180 net.cpp:150] Setting up ip3
I0520 14:11:18.280711 26180 net.cpp:157] Top shape: 110 11 (1210)
I0520 14:11:18.280721 26180 net.cpp:165] Memory required for data: 173661840
I0520 14:11:18.280737 26180 layer_factory.hpp:77] Creating layer drop3
I0520 14:11:18.280750 26180 net.cpp:106] Creating Layer drop3
I0520 14:11:18.280761 26180 net.cpp:454] drop3 <- ip3
I0520 14:11:18.280773 26180 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:11:18.280813 26180 net.cpp:150] Setting up drop3
I0520 14:11:18.280827 26180 net.cpp:157] Top shape: 110 11 (1210)
I0520 14:11:18.280836 26180 net.cpp:165] Memory required for data: 173666680
I0520 14:11:18.280845 26180 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 14:11:18.280858 26180 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 14:11:18.280869 26180 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 14:11:18.280881 26180 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 14:11:18.280896 26180 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 14:11:18.280971 26180 net.cpp:150] Setting up ip3_drop3_0_split
I0520 14:11:18.280983 26180 net.cpp:157] Top shape: 110 11 (1210)
I0520 14:11:18.280997 26180 net.cpp:157] Top shape: 110 11 (1210)
I0520 14:11:18.281009 26180 net.cpp:165] Memory required for data: 173676360
I0520 14:11:18.281021 26180 layer_factory.hpp:77] Creating layer accuracy
I0520 14:11:18.281046 26180 net.cpp:106] Creating Layer accuracy
I0520 14:11:18.281056 26180 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 14:11:18.281067 26180 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 14:11:18.281081 26180 net.cpp:411] accuracy -> accuracy
I0520 14:11:18.281105 26180 net.cpp:150] Setting up accuracy
I0520 14:11:18.281117 26180 net.cpp:157] Top shape: (1)
I0520 14:11:18.281126 26180 net.cpp:165] Memory required for data: 173676364
I0520 14:11:18.281137 26180 layer_factory.hpp:77] Creating layer loss
I0520 14:11:18.281150 26180 net.cpp:106] Creating Layer loss
I0520 14:11:18.281160 26180 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 14:11:18.281172 26180 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 14:11:18.281185 26180 net.cpp:411] loss -> loss
I0520 14:11:18.281203 26180 layer_factory.hpp:77] Creating layer loss
I0520 14:11:18.281689 26180 net.cpp:150] Setting up loss
I0520 14:11:18.281702 26180 net.cpp:157] Top shape: (1)
I0520 14:11:18.281713 26180 net.cpp:160]     with loss weight 1
I0520 14:11:18.281730 26180 net.cpp:165] Memory required for data: 173676368
I0520 14:11:18.281741 26180 net.cpp:226] loss needs backward computation.
I0520 14:11:18.281752 26180 net.cpp:228] accuracy does not need backward computation.
I0520 14:11:18.281764 26180 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 14:11:18.281774 26180 net.cpp:226] drop3 needs backward computation.
I0520 14:11:18.281781 26180 net.cpp:226] ip3 needs backward computation.
I0520 14:11:18.281792 26180 net.cpp:226] drop2 needs backward computation.
I0520 14:11:18.281810 26180 net.cpp:226] relu6 needs backward computation.
I0520 14:11:18.281821 26180 net.cpp:226] ip2 needs backward computation.
I0520 14:11:18.281831 26180 net.cpp:226] drop1 needs backward computation.
I0520 14:11:18.281841 26180 net.cpp:226] relu5 needs backward computation.
I0520 14:11:18.281850 26180 net.cpp:226] ip1 needs backward computation.
I0520 14:11:18.281859 26180 net.cpp:226] pool4 needs backward computation.
I0520 14:11:18.281869 26180 net.cpp:226] relu4 needs backward computation.
I0520 14:11:18.281879 26180 net.cpp:226] conv4 needs backward computation.
I0520 14:11:18.281890 26180 net.cpp:226] pool3 needs backward computation.
I0520 14:11:18.281900 26180 net.cpp:226] relu3 needs backward computation.
I0520 14:11:18.281910 26180 net.cpp:226] conv3 needs backward computation.
I0520 14:11:18.281921 26180 net.cpp:226] pool2 needs backward computation.
I0520 14:11:18.281932 26180 net.cpp:226] relu2 needs backward computation.
I0520 14:11:18.281942 26180 net.cpp:226] conv2 needs backward computation.
I0520 14:11:18.281955 26180 net.cpp:226] pool1 needs backward computation.
I0520 14:11:18.281965 26180 net.cpp:226] relu1 needs backward computation.
I0520 14:11:18.281975 26180 net.cpp:226] conv1 needs backward computation.
I0520 14:11:18.281985 26180 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 14:11:18.281996 26180 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:11:18.282006 26180 net.cpp:270] This network produces output accuracy
I0520 14:11:18.282016 26180 net.cpp:270] This network produces output loss
I0520 14:11:18.282044 26180 net.cpp:283] Network initialization done.
I0520 14:11:18.282178 26180 solver.cpp:60] Solver scaffolding done.
I0520 14:11:18.283314 26180 caffe.cpp:212] Starting Optimization
I0520 14:11:18.283334 26180 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 14:11:18.283346 26180 solver.cpp:289] Learning Rate Policy: fixed
I0520 14:11:18.284420 26180 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 14:12:05.882071 26180 solver.cpp:409]     Test net output #0: accuracy = 0.0721072
I0520 14:12:05.882232 26180 solver.cpp:409]     Test net output #1: loss = 2.39893 (* 1 = 2.39893 loss)
I0520 14:12:05.916744 26180 solver.cpp:237] Iteration 0, loss = 2.39913
I0520 14:12:05.916780 26180 solver.cpp:253]     Train net output #0: loss = 2.39913 (* 1 = 2.39913 loss)
I0520 14:12:05.916800 26180 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 14:12:14.507484 26180 solver.cpp:237] Iteration 136, loss = 2.27828
I0520 14:12:14.507537 26180 solver.cpp:253]     Train net output #0: loss = 2.27828 (* 1 = 2.27828 loss)
I0520 14:12:14.507553 26180 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0520 14:12:23.096705 26180 solver.cpp:237] Iteration 272, loss = 2.31754
I0520 14:12:23.096741 26180 solver.cpp:253]     Train net output #0: loss = 2.31754 (* 1 = 2.31754 loss)
I0520 14:12:23.096758 26180 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0520 14:12:31.689070 26180 solver.cpp:237] Iteration 408, loss = 2.10693
I0520 14:12:31.689106 26180 solver.cpp:253]     Train net output #0: loss = 2.10693 (* 1 = 2.10693 loss)
I0520 14:12:31.689122 26180 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0520 14:12:40.278604 26180 solver.cpp:237] Iteration 544, loss = 2.17061
I0520 14:12:40.278765 26180 solver.cpp:253]     Train net output #0: loss = 2.17061 (* 1 = 2.17061 loss)
I0520 14:12:40.278779 26180 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0520 14:12:48.869212 26180 solver.cpp:237] Iteration 680, loss = 1.90245
I0520 14:12:48.869246 26180 solver.cpp:253]     Train net output #0: loss = 1.90245 (* 1 = 1.90245 loss)
I0520 14:12:48.869263 26180 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0520 14:12:57.459383 26180 solver.cpp:237] Iteration 816, loss = 1.88261
I0520 14:12:57.459417 26180 solver.cpp:253]     Train net output #0: loss = 1.88261 (* 1 = 1.88261 loss)
I0520 14:12:57.459434 26180 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0520 14:13:28.158705 26180 solver.cpp:237] Iteration 952, loss = 1.98461
I0520 14:13:28.158869 26180 solver.cpp:253]     Train net output #0: loss = 1.98461 (* 1 = 1.98461 loss)
I0520 14:13:28.158882 26180 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0520 14:13:36.751986 26180 solver.cpp:237] Iteration 1088, loss = 1.79146
I0520 14:13:36.752019 26180 solver.cpp:253]     Train net output #0: loss = 1.79146 (* 1 = 1.79146 loss)
I0520 14:13:36.752037 26180 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0520 14:13:45.343813 26180 solver.cpp:237] Iteration 1224, loss = 1.88941
I0520 14:13:45.343848 26180 solver.cpp:253]     Train net output #0: loss = 1.88941 (* 1 = 1.88941 loss)
I0520 14:13:45.343861 26180 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0520 14:13:53.937464 26180 solver.cpp:237] Iteration 1360, loss = 1.79034
I0520 14:13:53.937505 26180 solver.cpp:253]     Train net output #0: loss = 1.79034 (* 1 = 1.79034 loss)
I0520 14:13:53.937525 26180 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0520 14:13:54.064117 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_1363.caffemodel
I0520 14:13:54.149318 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_1363.solverstate
I0520 14:14:02.595664 26180 solver.cpp:237] Iteration 1496, loss = 1.84064
I0520 14:14:02.595827 26180 solver.cpp:253]     Train net output #0: loss = 1.84064 (* 1 = 1.84064 loss)
I0520 14:14:02.595841 26180 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 14:14:11.187731 26180 solver.cpp:237] Iteration 1632, loss = 1.78675
I0520 14:14:11.187765 26180 solver.cpp:253]     Train net output #0: loss = 1.78675 (* 1 = 1.78675 loss)
I0520 14:14:11.187783 26180 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0520 14:14:19.779198 26180 solver.cpp:237] Iteration 1768, loss = 1.7377
I0520 14:14:19.779237 26180 solver.cpp:253]     Train net output #0: loss = 1.7377 (* 1 = 1.7377 loss)
I0520 14:14:19.779260 26180 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0520 14:14:50.542088 26180 solver.cpp:237] Iteration 1904, loss = 1.69929
I0520 14:14:50.542258 26180 solver.cpp:253]     Train net output #0: loss = 1.69929 (* 1 = 1.69929 loss)
I0520 14:14:50.542273 26180 sgd_solver.cpp:106] Iteration 1904, lr = 0.0025
I0520 14:14:59.136970 26180 solver.cpp:237] Iteration 2040, loss = 1.75291
I0520 14:14:59.137006 26180 solver.cpp:253]     Train net output #0: loss = 1.75291 (* 1 = 1.75291 loss)
I0520 14:14:59.137020 26180 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0520 14:15:07.728556 26180 solver.cpp:237] Iteration 2176, loss = 1.81242
I0520 14:15:07.728603 26180 solver.cpp:253]     Train net output #0: loss = 1.81242 (* 1 = 1.81242 loss)
I0520 14:15:07.728620 26180 sgd_solver.cpp:106] Iteration 2176, lr = 0.0025
I0520 14:15:16.323065 26180 solver.cpp:237] Iteration 2312, loss = 1.8347
I0520 14:15:16.323101 26180 solver.cpp:253]     Train net output #0: loss = 1.8347 (* 1 = 1.8347 loss)
I0520 14:15:16.323117 26180 sgd_solver.cpp:106] Iteration 2312, lr = 0.0025
I0520 14:15:24.917896 26180 solver.cpp:237] Iteration 2448, loss = 1.66314
I0520 14:15:24.918045 26180 solver.cpp:253]     Train net output #0: loss = 1.66314 (* 1 = 1.66314 loss)
I0520 14:15:24.918057 26180 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0520 14:15:33.510349 26180 solver.cpp:237] Iteration 2584, loss = 1.53623
I0520 14:15:33.510393 26180 solver.cpp:253]     Train net output #0: loss = 1.53623 (* 1 = 1.53623 loss)
I0520 14:15:33.510411 26180 sgd_solver.cpp:106] Iteration 2584, lr = 0.0025
I0520 14:15:42.104975 26180 solver.cpp:237] Iteration 2720, loss = 1.65513
I0520 14:15:42.105010 26180 solver.cpp:253]     Train net output #0: loss = 1.65513 (* 1 = 1.65513 loss)
I0520 14:15:42.105026 26180 sgd_solver.cpp:106] Iteration 2720, lr = 0.0025
I0520 14:15:42.420967 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_2726.caffemodel
I0520 14:15:42.503178 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_2726.solverstate
I0520 14:15:42.549372 26180 solver.cpp:341] Iteration 2727, Testing net (#0)
I0520 14:16:29.235404 26180 solver.cpp:409]     Test net output #0: accuracy = 0.673768
I0520 14:16:29.235577 26180 solver.cpp:409]     Test net output #1: loss = 1.11958 (* 1 = 1.11958 loss)
I0520 14:16:59.565780 26180 solver.cpp:237] Iteration 2856, loss = 1.66951
I0520 14:16:59.565943 26180 solver.cpp:253]     Train net output #0: loss = 1.66951 (* 1 = 1.66951 loss)
I0520 14:16:59.565958 26180 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0520 14:17:08.157603 26180 solver.cpp:237] Iteration 2992, loss = 1.67165
I0520 14:17:08.157637 26180 solver.cpp:253]     Train net output #0: loss = 1.67165 (* 1 = 1.67165 loss)
I0520 14:17:08.157655 26180 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0520 14:17:16.751359 26180 solver.cpp:237] Iteration 3128, loss = 1.59474
I0520 14:17:16.751395 26180 solver.cpp:253]     Train net output #0: loss = 1.59474 (* 1 = 1.59474 loss)
I0520 14:17:16.751417 26180 sgd_solver.cpp:106] Iteration 3128, lr = 0.0025
I0520 14:17:25.345039 26180 solver.cpp:237] Iteration 3264, loss = 1.68714
I0520 14:17:25.345074 26180 solver.cpp:253]     Train net output #0: loss = 1.68714 (* 1 = 1.68714 loss)
I0520 14:17:25.345091 26180 sgd_solver.cpp:106] Iteration 3264, lr = 0.0025
I0520 14:17:33.938875 26180 solver.cpp:237] Iteration 3400, loss = 1.62831
I0520 14:17:33.939020 26180 solver.cpp:253]     Train net output #0: loss = 1.62831 (* 1 = 1.62831 loss)
I0520 14:17:33.939033 26180 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0520 14:17:42.532980 26180 solver.cpp:237] Iteration 3536, loss = 1.5513
I0520 14:17:42.533020 26180 solver.cpp:253]     Train net output #0: loss = 1.5513 (* 1 = 1.5513 loss)
I0520 14:17:42.533040 26180 sgd_solver.cpp:106] Iteration 3536, lr = 0.0025
I0520 14:18:13.303319 26180 solver.cpp:237] Iteration 3672, loss = 1.61105
I0520 14:18:13.303483 26180 solver.cpp:253]     Train net output #0: loss = 1.61105 (* 1 = 1.61105 loss)
I0520 14:18:13.303504 26180 sgd_solver.cpp:106] Iteration 3672, lr = 0.0025
I0520 14:18:21.896383 26180 solver.cpp:237] Iteration 3808, loss = 1.42286
I0520 14:18:21.896416 26180 solver.cpp:253]     Train net output #0: loss = 1.42286 (* 1 = 1.42286 loss)
I0520 14:18:21.896430 26180 sgd_solver.cpp:106] Iteration 3808, lr = 0.0025
I0520 14:18:30.490592 26180 solver.cpp:237] Iteration 3944, loss = 1.47727
I0520 14:18:30.490625 26180 solver.cpp:253]     Train net output #0: loss = 1.47727 (* 1 = 1.47727 loss)
I0520 14:18:30.490648 26180 sgd_solver.cpp:106] Iteration 3944, lr = 0.0025
I0520 14:18:39.083209 26180 solver.cpp:237] Iteration 4080, loss = 1.45758
I0520 14:18:39.083245 26180 solver.cpp:253]     Train net output #0: loss = 1.45758 (* 1 = 1.45758 loss)
I0520 14:18:39.083261 26180 sgd_solver.cpp:106] Iteration 4080, lr = 0.0025
I0520 14:18:39.589246 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_4089.caffemodel
I0520 14:18:39.673146 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_4089.solverstate
I0520 14:18:47.745829 26180 solver.cpp:237] Iteration 4216, loss = 1.43173
I0520 14:18:47.745996 26180 solver.cpp:253]     Train net output #0: loss = 1.43173 (* 1 = 1.43173 loss)
I0520 14:18:47.746011 26180 sgd_solver.cpp:106] Iteration 4216, lr = 0.0025
I0520 14:18:56.339795 26180 solver.cpp:237] Iteration 4352, loss = 1.54007
I0520 14:18:56.339840 26180 solver.cpp:253]     Train net output #0: loss = 1.54007 (* 1 = 1.54007 loss)
I0520 14:18:56.339859 26180 sgd_solver.cpp:106] Iteration 4352, lr = 0.0025
I0520 14:19:04.932849 26180 solver.cpp:237] Iteration 4488, loss = 1.55513
I0520 14:19:04.932885 26180 solver.cpp:253]     Train net output #0: loss = 1.55513 (* 1 = 1.55513 loss)
I0520 14:19:04.932903 26180 sgd_solver.cpp:106] Iteration 4488, lr = 0.0025
I0520 14:19:35.745329 26180 solver.cpp:237] Iteration 4624, loss = 1.35659
I0520 14:19:35.745494 26180 solver.cpp:253]     Train net output #0: loss = 1.35659 (* 1 = 1.35659 loss)
I0520 14:19:35.745510 26180 sgd_solver.cpp:106] Iteration 4624, lr = 0.0025
I0520 14:19:44.338171 26180 solver.cpp:237] Iteration 4760, loss = 1.40352
I0520 14:19:44.338208 26180 solver.cpp:253]     Train net output #0: loss = 1.40352 (* 1 = 1.40352 loss)
I0520 14:19:44.338229 26180 sgd_solver.cpp:106] Iteration 4760, lr = 0.0025
I0520 14:19:52.930536 26180 solver.cpp:237] Iteration 4896, loss = 1.36465
I0520 14:19:52.930572 26180 solver.cpp:253]     Train net output #0: loss = 1.36465 (* 1 = 1.36465 loss)
I0520 14:19:52.930588 26180 sgd_solver.cpp:106] Iteration 4896, lr = 0.0025
I0520 14:20:01.523844 26180 solver.cpp:237] Iteration 5032, loss = 1.60604
I0520 14:20:01.523880 26180 solver.cpp:253]     Train net output #0: loss = 1.60604 (* 1 = 1.60604 loss)
I0520 14:20:01.523895 26180 sgd_solver.cpp:106] Iteration 5032, lr = 0.0025
I0520 14:20:10.117481 26180 solver.cpp:237] Iteration 5168, loss = 1.38802
I0520 14:20:10.117637 26180 solver.cpp:253]     Train net output #0: loss = 1.38802 (* 1 = 1.38802 loss)
I0520 14:20:10.117651 26180 sgd_solver.cpp:106] Iteration 5168, lr = 0.0025
I0520 14:20:18.710583 26180 solver.cpp:237] Iteration 5304, loss = 1.54064
I0520 14:20:18.710618 26180 solver.cpp:253]     Train net output #0: loss = 1.54064 (* 1 = 1.54064 loss)
I0520 14:20:18.710635 26180 sgd_solver.cpp:106] Iteration 5304, lr = 0.0025
I0520 14:20:27.304921 26180 solver.cpp:237] Iteration 5440, loss = 1.34714
I0520 14:20:27.304956 26180 solver.cpp:253]     Train net output #0: loss = 1.34714 (* 1 = 1.34714 loss)
I0520 14:20:27.304970 26180 sgd_solver.cpp:106] Iteration 5440, lr = 0.0025
I0520 14:20:28.000107 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_5452.caffemodel
I0520 14:20:28.084014 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_5452.solverstate
I0520 14:20:28.195399 26180 solver.cpp:341] Iteration 5454, Testing net (#0)
I0520 14:21:35.737874 26180 solver.cpp:409]     Test net output #0: accuracy = 0.761534
I0520 14:21:35.738044 26180 solver.cpp:409]     Test net output #1: loss = 0.898958 (* 1 = 0.898958 loss)
I0520 14:22:05.666100 26180 solver.cpp:237] Iteration 5576, loss = 1.47962
I0520 14:22:05.666151 26180 solver.cpp:253]     Train net output #0: loss = 1.47962 (* 1 = 1.47962 loss)
I0520 14:22:05.666167 26180 sgd_solver.cpp:106] Iteration 5576, lr = 0.0025
I0520 14:22:14.257990 26180 solver.cpp:237] Iteration 5712, loss = 1.53112
I0520 14:22:14.258152 26180 solver.cpp:253]     Train net output #0: loss = 1.53112 (* 1 = 1.53112 loss)
I0520 14:22:14.258167 26180 sgd_solver.cpp:106] Iteration 5712, lr = 0.0025
I0520 14:22:22.849495 26180 solver.cpp:237] Iteration 5848, loss = 1.2301
I0520 14:22:22.849531 26180 solver.cpp:253]     Train net output #0: loss = 1.2301 (* 1 = 1.2301 loss)
I0520 14:22:22.849547 26180 sgd_solver.cpp:106] Iteration 5848, lr = 0.0025
I0520 14:22:31.439577 26180 solver.cpp:237] Iteration 5984, loss = 1.43285
I0520 14:22:31.439612 26180 solver.cpp:253]     Train net output #0: loss = 1.43285 (* 1 = 1.43285 loss)
I0520 14:22:31.439630 26180 sgd_solver.cpp:106] Iteration 5984, lr = 0.0025
I0520 14:22:40.032812 26180 solver.cpp:237] Iteration 6120, loss = 1.48866
I0520 14:22:40.032860 26180 solver.cpp:253]     Train net output #0: loss = 1.48866 (* 1 = 1.48866 loss)
I0520 14:22:40.032876 26180 sgd_solver.cpp:106] Iteration 6120, lr = 0.0025
I0520 14:22:48.623551 26180 solver.cpp:237] Iteration 6256, loss = 1.46271
I0520 14:22:48.623695 26180 solver.cpp:253]     Train net output #0: loss = 1.46271 (* 1 = 1.46271 loss)
I0520 14:22:48.623709 26180 sgd_solver.cpp:106] Iteration 6256, lr = 0.0025
I0520 14:23:19.408620 26180 solver.cpp:237] Iteration 6392, loss = 1.46075
I0520 14:23:19.408802 26180 solver.cpp:253]     Train net output #0: loss = 1.46075 (* 1 = 1.46075 loss)
I0520 14:23:19.408818 26180 sgd_solver.cpp:106] Iteration 6392, lr = 0.0025
I0520 14:23:28.003074 26180 solver.cpp:237] Iteration 6528, loss = 1.46958
I0520 14:23:28.003113 26180 solver.cpp:253]     Train net output #0: loss = 1.46958 (* 1 = 1.46958 loss)
I0520 14:23:28.003132 26180 sgd_solver.cpp:106] Iteration 6528, lr = 0.0025
I0520 14:23:36.597712 26180 solver.cpp:237] Iteration 6664, loss = 1.49948
I0520 14:23:36.597748 26180 solver.cpp:253]     Train net output #0: loss = 1.49948 (* 1 = 1.49948 loss)
I0520 14:23:36.597762 26180 sgd_solver.cpp:106] Iteration 6664, lr = 0.0025
I0520 14:23:45.189128 26180 solver.cpp:237] Iteration 6800, loss = 1.25135
I0520 14:23:45.189163 26180 solver.cpp:253]     Train net output #0: loss = 1.25135 (* 1 = 1.25135 loss)
I0520 14:23:45.189179 26180 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0520 14:23:46.074332 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_6815.caffemodel
I0520 14:23:46.158478 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_6815.solverstate
I0520 14:23:53.851902 26180 solver.cpp:237] Iteration 6936, loss = 1.46464
I0520 14:23:53.852062 26180 solver.cpp:253]     Train net output #0: loss = 1.46464 (* 1 = 1.46464 loss)
I0520 14:23:53.852077 26180 sgd_solver.cpp:106] Iteration 6936, lr = 0.0025
I0520 14:24:02.443375 26180 solver.cpp:237] Iteration 7072, loss = 1.30663
I0520 14:24:02.443409 26180 solver.cpp:253]     Train net output #0: loss = 1.30663 (* 1 = 1.30663 loss)
I0520 14:24:02.443426 26180 sgd_solver.cpp:106] Iteration 7072, lr = 0.0025
I0520 14:24:11.037402 26180 solver.cpp:237] Iteration 7208, loss = 1.49761
I0520 14:24:11.037438 26180 solver.cpp:253]     Train net output #0: loss = 1.49761 (* 1 = 1.49761 loss)
I0520 14:24:11.037454 26180 sgd_solver.cpp:106] Iteration 7208, lr = 0.0025
I0520 14:24:41.870929 26180 solver.cpp:237] Iteration 7344, loss = 1.53535
I0520 14:24:41.871096 26180 solver.cpp:253]     Train net output #0: loss = 1.53535 (* 1 = 1.53535 loss)
I0520 14:24:41.871112 26180 sgd_solver.cpp:106] Iteration 7344, lr = 0.0025
I0520 14:24:50.463785 26180 solver.cpp:237] Iteration 7480, loss = 1.51911
I0520 14:24:50.463820 26180 solver.cpp:253]     Train net output #0: loss = 1.51911 (* 1 = 1.51911 loss)
I0520 14:24:50.463837 26180 sgd_solver.cpp:106] Iteration 7480, lr = 0.0025
I0520 14:24:59.056205 26180 solver.cpp:237] Iteration 7616, loss = 1.48518
I0520 14:24:59.056239 26180 solver.cpp:253]     Train net output #0: loss = 1.48518 (* 1 = 1.48518 loss)
I0520 14:24:59.056257 26180 sgd_solver.cpp:106] Iteration 7616, lr = 0.0025
I0520 14:25:07.649052 26180 solver.cpp:237] Iteration 7752, loss = 1.23119
I0520 14:25:07.649101 26180 solver.cpp:253]     Train net output #0: loss = 1.23119 (* 1 = 1.23119 loss)
I0520 14:25:07.649116 26180 sgd_solver.cpp:106] Iteration 7752, lr = 0.0025
I0520 14:25:16.239807 26180 solver.cpp:237] Iteration 7888, loss = 1.35525
I0520 14:25:16.239948 26180 solver.cpp:253]     Train net output #0: loss = 1.35525 (* 1 = 1.35525 loss)
I0520 14:25:16.239961 26180 sgd_solver.cpp:106] Iteration 7888, lr = 0.0025
I0520 14:25:24.832334 26180 solver.cpp:237] Iteration 8024, loss = 1.30944
I0520 14:25:24.832367 26180 solver.cpp:253]     Train net output #0: loss = 1.30944 (* 1 = 1.30944 loss)
I0520 14:25:24.832386 26180 sgd_solver.cpp:106] Iteration 8024, lr = 0.0025
I0520 14:25:33.425932 26180 solver.cpp:237] Iteration 8160, loss = 1.3201
I0520 14:25:33.425978 26180 solver.cpp:253]     Train net output #0: loss = 1.3201 (* 1 = 1.3201 loss)
I0520 14:25:33.425995 26180 sgd_solver.cpp:106] Iteration 8160, lr = 0.0025
I0520 14:25:34.500567 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_8178.caffemodel
I0520 14:25:34.582473 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_8178.solverstate
I0520 14:25:34.754953 26180 solver.cpp:341] Iteration 8181, Testing net (#0)
I0520 14:26:21.126214 26180 solver.cpp:409]     Test net output #0: accuracy = 0.815898
I0520 14:26:21.126373 26180 solver.cpp:409]     Test net output #1: loss = 0.729021 (* 1 = 0.729021 loss)
I0520 14:26:50.568923 26180 solver.cpp:237] Iteration 8296, loss = 1.30908
I0520 14:26:50.568971 26180 solver.cpp:253]     Train net output #0: loss = 1.30908 (* 1 = 1.30908 loss)
I0520 14:26:50.568989 26180 sgd_solver.cpp:106] Iteration 8296, lr = 0.0025
I0520 14:26:59.161885 26180 solver.cpp:237] Iteration 8432, loss = 1.25883
I0520 14:26:59.162035 26180 solver.cpp:253]     Train net output #0: loss = 1.25883 (* 1 = 1.25883 loss)
I0520 14:26:59.162050 26180 sgd_solver.cpp:106] Iteration 8432, lr = 0.0025
I0520 14:27:07.756443 26180 solver.cpp:237] Iteration 8568, loss = 1.36537
I0520 14:27:07.756477 26180 solver.cpp:253]     Train net output #0: loss = 1.36537 (* 1 = 1.36537 loss)
I0520 14:27:07.756494 26180 sgd_solver.cpp:106] Iteration 8568, lr = 0.0025
I0520 14:27:16.352589 26180 solver.cpp:237] Iteration 8704, loss = 1.47284
I0520 14:27:16.352630 26180 solver.cpp:253]     Train net output #0: loss = 1.47284 (* 1 = 1.47284 loss)
I0520 14:27:16.352650 26180 sgd_solver.cpp:106] Iteration 8704, lr = 0.0025
I0520 14:27:24.945003 26180 solver.cpp:237] Iteration 8840, loss = 1.19957
I0520 14:27:24.945036 26180 solver.cpp:253]     Train net output #0: loss = 1.19957 (* 1 = 1.19957 loss)
I0520 14:27:24.945053 26180 sgd_solver.cpp:106] Iteration 8840, lr = 0.0025
I0520 14:27:33.541276 26180 solver.cpp:237] Iteration 8976, loss = 1.30195
I0520 14:27:33.541429 26180 solver.cpp:253]     Train net output #0: loss = 1.30195 (* 1 = 1.30195 loss)
I0520 14:27:33.541442 26180 sgd_solver.cpp:106] Iteration 8976, lr = 0.0025
I0520 14:28:04.282971 26180 solver.cpp:237] Iteration 9112, loss = 1.10371
I0520 14:28:04.283140 26180 solver.cpp:253]     Train net output #0: loss = 1.10371 (* 1 = 1.10371 loss)
I0520 14:28:04.283156 26180 sgd_solver.cpp:106] Iteration 9112, lr = 0.0025
I0520 14:28:12.878038 26180 solver.cpp:237] Iteration 9248, loss = 1.46349
I0520 14:28:12.878073 26180 solver.cpp:253]     Train net output #0: loss = 1.46349 (* 1 = 1.46349 loss)
I0520 14:28:12.878090 26180 sgd_solver.cpp:106] Iteration 9248, lr = 0.0025
I0520 14:28:21.472972 26180 solver.cpp:237] Iteration 9384, loss = 1.36762
I0520 14:28:21.473007 26180 solver.cpp:253]     Train net output #0: loss = 1.36762 (* 1 = 1.36762 loss)
I0520 14:28:21.473023 26180 sgd_solver.cpp:106] Iteration 9384, lr = 0.0025
I0520 14:28:30.067323 26180 solver.cpp:237] Iteration 9520, loss = 1.26132
I0520 14:28:30.067369 26180 solver.cpp:253]     Train net output #0: loss = 1.26132 (* 1 = 1.26132 loss)
I0520 14:28:30.067385 26180 sgd_solver.cpp:106] Iteration 9520, lr = 0.0025
I0520 14:28:31.331863 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_9541.caffemodel
I0520 14:28:31.413295 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_9541.solverstate
I0520 14:28:38.726860 26180 solver.cpp:237] Iteration 9656, loss = 1.25459
I0520 14:28:38.727017 26180 solver.cpp:253]     Train net output #0: loss = 1.25459 (* 1 = 1.25459 loss)
I0520 14:28:38.727031 26180 sgd_solver.cpp:106] Iteration 9656, lr = 0.0025
I0520 14:28:47.319586 26180 solver.cpp:237] Iteration 9792, loss = 1.28792
I0520 14:28:47.319622 26180 solver.cpp:253]     Train net output #0: loss = 1.28792 (* 1 = 1.28792 loss)
I0520 14:28:47.319638 26180 sgd_solver.cpp:106] Iteration 9792, lr = 0.0025
I0520 14:28:55.914103 26180 solver.cpp:237] Iteration 9928, loss = 1.44623
I0520 14:28:55.914145 26180 solver.cpp:253]     Train net output #0: loss = 1.44623 (* 1 = 1.44623 loss)
I0520 14:28:55.914160 26180 sgd_solver.cpp:106] Iteration 9928, lr = 0.0025
I0520 14:29:26.675283 26180 solver.cpp:237] Iteration 10064, loss = 1.30252
I0520 14:29:26.675447 26180 solver.cpp:253]     Train net output #0: loss = 1.30252 (* 1 = 1.30252 loss)
I0520 14:29:26.675463 26180 sgd_solver.cpp:106] Iteration 10064, lr = 0.0025
I0520 14:29:35.270885 26180 solver.cpp:237] Iteration 10200, loss = 1.27652
I0520 14:29:35.270920 26180 solver.cpp:253]     Train net output #0: loss = 1.27652 (* 1 = 1.27652 loss)
I0520 14:29:35.270937 26180 sgd_solver.cpp:106] Iteration 10200, lr = 0.0025
I0520 14:29:43.866420 26180 solver.cpp:237] Iteration 10336, loss = 1.42586
I0520 14:29:43.866464 26180 solver.cpp:253]     Train net output #0: loss = 1.42586 (* 1 = 1.42586 loss)
I0520 14:29:43.866479 26180 sgd_solver.cpp:106] Iteration 10336, lr = 0.0025
I0520 14:29:52.458724 26180 solver.cpp:237] Iteration 10472, loss = 1.36886
I0520 14:29:52.458758 26180 solver.cpp:253]     Train net output #0: loss = 1.36886 (* 1 = 1.36886 loss)
I0520 14:29:52.458775 26180 sgd_solver.cpp:106] Iteration 10472, lr = 0.0025
I0520 14:30:01.052997 26180 solver.cpp:237] Iteration 10608, loss = 1.3389
I0520 14:30:01.053138 26180 solver.cpp:253]     Train net output #0: loss = 1.3389 (* 1 = 1.3389 loss)
I0520 14:30:01.053151 26180 sgd_solver.cpp:106] Iteration 10608, lr = 0.0025
I0520 14:30:09.646291 26180 solver.cpp:237] Iteration 10744, loss = 1.42819
I0520 14:30:09.646335 26180 solver.cpp:253]     Train net output #0: loss = 1.42819 (* 1 = 1.42819 loss)
I0520 14:30:09.646353 26180 sgd_solver.cpp:106] Iteration 10744, lr = 0.0025
I0520 14:30:18.241641 26180 solver.cpp:237] Iteration 10880, loss = 1.29033
I0520 14:30:18.241677 26180 solver.cpp:253]     Train net output #0: loss = 1.29033 (* 1 = 1.29033 loss)
I0520 14:30:18.241691 26180 sgd_solver.cpp:106] Iteration 10880, lr = 0.0025
I0520 14:30:19.694659 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_10904.caffemodel
I0520 14:30:19.776741 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_10904.solverstate
I0520 14:30:20.011612 26180 solver.cpp:341] Iteration 10908, Testing net (#0)
I0520 14:31:27.521410 26180 solver.cpp:409]     Test net output #0: accuracy = 0.831054
I0520 14:31:27.521585 26180 solver.cpp:409]     Test net output #1: loss = 0.583381 (* 1 = 0.583381 loss)
I0520 14:31:56.569416 26180 solver.cpp:237] Iteration 11016, loss = 1.12726
I0520 14:31:56.569466 26180 solver.cpp:253]     Train net output #0: loss = 1.12726 (* 1 = 1.12726 loss)
I0520 14:31:56.569483 26180 sgd_solver.cpp:106] Iteration 11016, lr = 0.0025
I0520 14:32:05.170238 26180 solver.cpp:237] Iteration 11152, loss = 1.24861
I0520 14:32:05.170393 26180 solver.cpp:253]     Train net output #0: loss = 1.24861 (* 1 = 1.24861 loss)
I0520 14:32:05.170408 26180 sgd_solver.cpp:106] Iteration 11152, lr = 0.0025
I0520 14:32:13.769526 26180 solver.cpp:237] Iteration 11288, loss = 1.18123
I0520 14:32:13.769568 26180 solver.cpp:253]     Train net output #0: loss = 1.18123 (* 1 = 1.18123 loss)
I0520 14:32:13.769585 26180 sgd_solver.cpp:106] Iteration 11288, lr = 0.0025
I0520 14:32:22.367126 26180 solver.cpp:237] Iteration 11424, loss = 1.43086
I0520 14:32:22.367162 26180 solver.cpp:253]     Train net output #0: loss = 1.43086 (* 1 = 1.43086 loss)
I0520 14:32:22.367177 26180 sgd_solver.cpp:106] Iteration 11424, lr = 0.0025
I0520 14:32:30.968708 26180 solver.cpp:237] Iteration 11560, loss = 1.4978
I0520 14:32:30.968742 26180 solver.cpp:253]     Train net output #0: loss = 1.4978 (* 1 = 1.4978 loss)
I0520 14:32:30.968758 26180 sgd_solver.cpp:106] Iteration 11560, lr = 0.0025
I0520 14:32:39.569272 26180 solver.cpp:237] Iteration 11696, loss = 1.54719
I0520 14:32:39.569424 26180 solver.cpp:253]     Train net output #0: loss = 1.54719 (* 1 = 1.54719 loss)
I0520 14:32:39.569438 26180 sgd_solver.cpp:106] Iteration 11696, lr = 0.0025
I0520 14:33:10.342833 26180 solver.cpp:237] Iteration 11832, loss = 1.2393
I0520 14:33:10.342998 26180 solver.cpp:253]     Train net output #0: loss = 1.2393 (* 1 = 1.2393 loss)
I0520 14:33:10.343011 26180 sgd_solver.cpp:106] Iteration 11832, lr = 0.0025
I0520 14:33:18.942520 26180 solver.cpp:237] Iteration 11968, loss = 1.25529
I0520 14:33:18.942555 26180 solver.cpp:253]     Train net output #0: loss = 1.25529 (* 1 = 1.25529 loss)
I0520 14:33:18.942574 26180 sgd_solver.cpp:106] Iteration 11968, lr = 0.0025
I0520 14:33:27.542886 26180 solver.cpp:237] Iteration 12104, loss = 1.40523
I0520 14:33:27.542927 26180 solver.cpp:253]     Train net output #0: loss = 1.40523 (* 1 = 1.40523 loss)
I0520 14:33:27.542943 26180 sgd_solver.cpp:106] Iteration 12104, lr = 0.0025
I0520 14:33:36.142057 26180 solver.cpp:237] Iteration 12240, loss = 1.15093
I0520 14:33:36.142093 26180 solver.cpp:253]     Train net output #0: loss = 1.15093 (* 1 = 1.15093 loss)
I0520 14:33:36.142112 26180 sgd_solver.cpp:106] Iteration 12240, lr = 0.0025
I0520 14:33:37.786222 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_12267.caffemodel
I0520 14:33:37.870846 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_12267.solverstate
I0520 14:33:44.809146 26180 solver.cpp:237] Iteration 12376, loss = 1.11835
I0520 14:33:44.809310 26180 solver.cpp:253]     Train net output #0: loss = 1.11835 (* 1 = 1.11835 loss)
I0520 14:33:44.809324 26180 sgd_solver.cpp:106] Iteration 12376, lr = 0.0025
I0520 14:33:53.407135 26180 solver.cpp:237] Iteration 12512, loss = 1.25193
I0520 14:33:53.407181 26180 solver.cpp:253]     Train net output #0: loss = 1.25193 (* 1 = 1.25193 loss)
I0520 14:33:53.407196 26180 sgd_solver.cpp:106] Iteration 12512, lr = 0.0025
I0520 14:34:02.005790 26180 solver.cpp:237] Iteration 12648, loss = 1.39402
I0520 14:34:02.005825 26180 solver.cpp:253]     Train net output #0: loss = 1.39402 (* 1 = 1.39402 loss)
I0520 14:34:02.005841 26180 sgd_solver.cpp:106] Iteration 12648, lr = 0.0025
I0520 14:34:32.870309 26180 solver.cpp:237] Iteration 12784, loss = 1.35916
I0520 14:34:32.870507 26180 solver.cpp:253]     Train net output #0: loss = 1.35916 (* 1 = 1.35916 loss)
I0520 14:34:32.870523 26180 sgd_solver.cpp:106] Iteration 12784, lr = 0.0025
I0520 14:34:41.467782 26180 solver.cpp:237] Iteration 12920, loss = 1.00591
I0520 14:34:41.467886 26180 solver.cpp:253]     Train net output #0: loss = 1.00591 (* 1 = 1.00591 loss)
I0520 14:34:41.467901 26180 sgd_solver.cpp:106] Iteration 12920, lr = 0.0025
I0520 14:34:50.066587 26180 solver.cpp:237] Iteration 13056, loss = 1.1629
I0520 14:34:50.066623 26180 solver.cpp:253]     Train net output #0: loss = 1.1629 (* 1 = 1.1629 loss)
I0520 14:34:50.066638 26180 sgd_solver.cpp:106] Iteration 13056, lr = 0.0025
I0520 14:34:58.664866 26180 solver.cpp:237] Iteration 13192, loss = 1.25085
I0520 14:34:58.664901 26180 solver.cpp:253]     Train net output #0: loss = 1.25085 (* 1 = 1.25085 loss)
I0520 14:34:58.664917 26180 sgd_solver.cpp:106] Iteration 13192, lr = 0.0025
I0520 14:35:07.264152 26180 solver.cpp:237] Iteration 13328, loss = 1.461
I0520 14:35:07.264309 26180 solver.cpp:253]     Train net output #0: loss = 1.461 (* 1 = 1.461 loss)
I0520 14:35:07.264323 26180 sgd_solver.cpp:106] Iteration 13328, lr = 0.0025
I0520 14:35:15.865447 26180 solver.cpp:237] Iteration 13464, loss = 1.29268
I0520 14:35:15.865481 26180 solver.cpp:253]     Train net output #0: loss = 1.29268 (* 1 = 1.29268 loss)
I0520 14:35:15.865499 26180 sgd_solver.cpp:106] Iteration 13464, lr = 0.0025
I0520 14:35:24.464892 26180 solver.cpp:237] Iteration 13600, loss = 1.34194
I0520 14:35:24.464926 26180 solver.cpp:253]     Train net output #0: loss = 1.34194 (* 1 = 1.34194 loss)
I0520 14:35:24.464942 26180 sgd_solver.cpp:106] Iteration 13600, lr = 0.0025
I0520 14:35:26.298205 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_13630.caffemodel
I0520 14:35:26.382223 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_13630.solverstate
I0520 14:35:26.682569 26180 solver.cpp:341] Iteration 13635, Testing net (#0)
I0520 14:36:13.391782 26180 solver.cpp:409]     Test net output #0: accuracy = 0.84124
I0520 14:36:13.391948 26180 solver.cpp:409]     Test net output #1: loss = 0.536414 (* 1 = 0.536414 loss)
I0520 14:36:13.411682 26180 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_13636.caffemodel
I0520 14:36:13.495942 26180 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879_iter_13636.solverstate
I0520 14:36:13.523030 26180 solver.cpp:326] Optimization Done.
I0520 14:36:13.523056 26180 caffe.cpp:215] Optimization Done.
Application 11232485 resources: utime ~1307s, stime ~233s, Rss ~5329508, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_110_2016-05-20T11.20.36.681879.solver"
	User time (seconds): 0.54
	System time (seconds): 0.17
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:45.13
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15078
	Voluntary context switches: 2800
	Involuntary context switches: 92
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
