2806190
I0521 02:39:50.108852  9476 caffe.cpp:184] Using GPUs 0
I0521 02:39:50.536407  9476 solver.cpp:48] Initializing solver from parameters: 
test_iter: 258
test_interval: 517
base_lr: 0.0025
display: 25
max_iter: 2586
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 258
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664.prototxt"
I0521 02:39:50.538130  9476 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664.prototxt
I0521 02:39:50.554762  9476 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 02:39:50.554822  9476 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 02:39:50.555169  9476 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 580
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:39:50.555351  9476 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:39:50.555376  9476 net.cpp:106] Creating Layer data_hdf5
I0521 02:39:50.555389  9476 net.cpp:411] data_hdf5 -> data
I0521 02:39:50.555423  9476 net.cpp:411] data_hdf5 -> label
I0521 02:39:50.555456  9476 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 02:39:50.556707  9476 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 02:39:50.558928  9476 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 02:40:12.115376  9476 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 02:40:12.120473  9476 net.cpp:150] Setting up data_hdf5
I0521 02:40:12.120513  9476 net.cpp:157] Top shape: 580 1 127 50 (3683000)
I0521 02:40:12.120528  9476 net.cpp:157] Top shape: 580 (580)
I0521 02:40:12.120540  9476 net.cpp:165] Memory required for data: 14734320
I0521 02:40:12.120553  9476 layer_factory.hpp:77] Creating layer conv1
I0521 02:40:12.120587  9476 net.cpp:106] Creating Layer conv1
I0521 02:40:12.120599  9476 net.cpp:454] conv1 <- data
I0521 02:40:12.120620  9476 net.cpp:411] conv1 -> conv1
I0521 02:40:13.017328  9476 net.cpp:150] Setting up conv1
I0521 02:40:13.017371  9476 net.cpp:157] Top shape: 580 12 120 48 (40089600)
I0521 02:40:13.017382  9476 net.cpp:165] Memory required for data: 175092720
I0521 02:40:13.017410  9476 layer_factory.hpp:77] Creating layer relu1
I0521 02:40:13.017432  9476 net.cpp:106] Creating Layer relu1
I0521 02:40:13.017443  9476 net.cpp:454] relu1 <- conv1
I0521 02:40:13.017457  9476 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:40:13.017972  9476 net.cpp:150] Setting up relu1
I0521 02:40:13.017989  9476 net.cpp:157] Top shape: 580 12 120 48 (40089600)
I0521 02:40:13.018000  9476 net.cpp:165] Memory required for data: 335451120
I0521 02:40:13.018013  9476 layer_factory.hpp:77] Creating layer pool1
I0521 02:40:13.018028  9476 net.cpp:106] Creating Layer pool1
I0521 02:40:13.018038  9476 net.cpp:454] pool1 <- conv1
I0521 02:40:13.018051  9476 net.cpp:411] pool1 -> pool1
I0521 02:40:13.018133  9476 net.cpp:150] Setting up pool1
I0521 02:40:13.018147  9476 net.cpp:157] Top shape: 580 12 60 48 (20044800)
I0521 02:40:13.018158  9476 net.cpp:165] Memory required for data: 415630320
I0521 02:40:13.018168  9476 layer_factory.hpp:77] Creating layer conv2
I0521 02:40:13.018190  9476 net.cpp:106] Creating Layer conv2
I0521 02:40:13.018200  9476 net.cpp:454] conv2 <- pool1
I0521 02:40:13.018214  9476 net.cpp:411] conv2 -> conv2
I0521 02:40:13.020890  9476 net.cpp:150] Setting up conv2
I0521 02:40:13.020912  9476 net.cpp:157] Top shape: 580 20 54 46 (28814400)
I0521 02:40:13.020928  9476 net.cpp:165] Memory required for data: 530887920
I0521 02:40:13.020946  9476 layer_factory.hpp:77] Creating layer relu2
I0521 02:40:13.020961  9476 net.cpp:106] Creating Layer relu2
I0521 02:40:13.020972  9476 net.cpp:454] relu2 <- conv2
I0521 02:40:13.020984  9476 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:40:13.021314  9476 net.cpp:150] Setting up relu2
I0521 02:40:13.021328  9476 net.cpp:157] Top shape: 580 20 54 46 (28814400)
I0521 02:40:13.021339  9476 net.cpp:165] Memory required for data: 646145520
I0521 02:40:13.021353  9476 layer_factory.hpp:77] Creating layer pool2
I0521 02:40:13.021366  9476 net.cpp:106] Creating Layer pool2
I0521 02:40:13.021376  9476 net.cpp:454] pool2 <- conv2
I0521 02:40:13.021401  9476 net.cpp:411] pool2 -> pool2
I0521 02:40:13.021469  9476 net.cpp:150] Setting up pool2
I0521 02:40:13.021482  9476 net.cpp:157] Top shape: 580 20 27 46 (14407200)
I0521 02:40:13.021492  9476 net.cpp:165] Memory required for data: 703774320
I0521 02:40:13.021503  9476 layer_factory.hpp:77] Creating layer conv3
I0521 02:40:13.021519  9476 net.cpp:106] Creating Layer conv3
I0521 02:40:13.021529  9476 net.cpp:454] conv3 <- pool2
I0521 02:40:13.021543  9476 net.cpp:411] conv3 -> conv3
I0521 02:40:13.023448  9476 net.cpp:150] Setting up conv3
I0521 02:40:13.023485  9476 net.cpp:157] Top shape: 580 28 22 44 (15720320)
I0521 02:40:13.023495  9476 net.cpp:165] Memory required for data: 766655600
I0521 02:40:13.023515  9476 layer_factory.hpp:77] Creating layer relu3
I0521 02:40:13.023531  9476 net.cpp:106] Creating Layer relu3
I0521 02:40:13.023541  9476 net.cpp:454] relu3 <- conv3
I0521 02:40:13.023555  9476 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:40:13.024024  9476 net.cpp:150] Setting up relu3
I0521 02:40:13.024041  9476 net.cpp:157] Top shape: 580 28 22 44 (15720320)
I0521 02:40:13.024052  9476 net.cpp:165] Memory required for data: 829536880
I0521 02:40:13.024062  9476 layer_factory.hpp:77] Creating layer pool3
I0521 02:40:13.024075  9476 net.cpp:106] Creating Layer pool3
I0521 02:40:13.024085  9476 net.cpp:454] pool3 <- conv3
I0521 02:40:13.024097  9476 net.cpp:411] pool3 -> pool3
I0521 02:40:13.024165  9476 net.cpp:150] Setting up pool3
I0521 02:40:13.024178  9476 net.cpp:157] Top shape: 580 28 11 44 (7860160)
I0521 02:40:13.024188  9476 net.cpp:165] Memory required for data: 860977520
I0521 02:40:13.024197  9476 layer_factory.hpp:77] Creating layer conv4
I0521 02:40:13.024215  9476 net.cpp:106] Creating Layer conv4
I0521 02:40:13.024226  9476 net.cpp:454] conv4 <- pool3
I0521 02:40:13.024240  9476 net.cpp:411] conv4 -> conv4
I0521 02:40:13.027005  9476 net.cpp:150] Setting up conv4
I0521 02:40:13.027034  9476 net.cpp:157] Top shape: 580 36 6 42 (5261760)
I0521 02:40:13.027047  9476 net.cpp:165] Memory required for data: 882024560
I0521 02:40:13.027063  9476 layer_factory.hpp:77] Creating layer relu4
I0521 02:40:13.027077  9476 net.cpp:106] Creating Layer relu4
I0521 02:40:13.027087  9476 net.cpp:454] relu4 <- conv4
I0521 02:40:13.027101  9476 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:40:13.027580  9476 net.cpp:150] Setting up relu4
I0521 02:40:13.027597  9476 net.cpp:157] Top shape: 580 36 6 42 (5261760)
I0521 02:40:13.027608  9476 net.cpp:165] Memory required for data: 903071600
I0521 02:40:13.027618  9476 layer_factory.hpp:77] Creating layer pool4
I0521 02:40:13.027631  9476 net.cpp:106] Creating Layer pool4
I0521 02:40:13.027642  9476 net.cpp:454] pool4 <- conv4
I0521 02:40:13.027654  9476 net.cpp:411] pool4 -> pool4
I0521 02:40:13.027722  9476 net.cpp:150] Setting up pool4
I0521 02:40:13.027735  9476 net.cpp:157] Top shape: 580 36 3 42 (2630880)
I0521 02:40:13.027746  9476 net.cpp:165] Memory required for data: 913595120
I0521 02:40:13.027756  9476 layer_factory.hpp:77] Creating layer ip1
I0521 02:40:13.027776  9476 net.cpp:106] Creating Layer ip1
I0521 02:40:13.027786  9476 net.cpp:454] ip1 <- pool4
I0521 02:40:13.027798  9476 net.cpp:411] ip1 -> ip1
I0521 02:40:13.043198  9476 net.cpp:150] Setting up ip1
I0521 02:40:13.043226  9476 net.cpp:157] Top shape: 580 196 (113680)
I0521 02:40:13.043238  9476 net.cpp:165] Memory required for data: 914049840
I0521 02:40:13.043261  9476 layer_factory.hpp:77] Creating layer relu5
I0521 02:40:13.043275  9476 net.cpp:106] Creating Layer relu5
I0521 02:40:13.043285  9476 net.cpp:454] relu5 <- ip1
I0521 02:40:13.043299  9476 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:40:13.043647  9476 net.cpp:150] Setting up relu5
I0521 02:40:13.043660  9476 net.cpp:157] Top shape: 580 196 (113680)
I0521 02:40:13.043671  9476 net.cpp:165] Memory required for data: 914504560
I0521 02:40:13.043681  9476 layer_factory.hpp:77] Creating layer drop1
I0521 02:40:13.043702  9476 net.cpp:106] Creating Layer drop1
I0521 02:40:13.043714  9476 net.cpp:454] drop1 <- ip1
I0521 02:40:13.043738  9476 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:40:13.043787  9476 net.cpp:150] Setting up drop1
I0521 02:40:13.043799  9476 net.cpp:157] Top shape: 580 196 (113680)
I0521 02:40:13.043809  9476 net.cpp:165] Memory required for data: 914959280
I0521 02:40:13.043819  9476 layer_factory.hpp:77] Creating layer ip2
I0521 02:40:13.043838  9476 net.cpp:106] Creating Layer ip2
I0521 02:40:13.043848  9476 net.cpp:454] ip2 <- ip1
I0521 02:40:13.043861  9476 net.cpp:411] ip2 -> ip2
I0521 02:40:13.044327  9476 net.cpp:150] Setting up ip2
I0521 02:40:13.044340  9476 net.cpp:157] Top shape: 580 98 (56840)
I0521 02:40:13.044351  9476 net.cpp:165] Memory required for data: 915186640
I0521 02:40:13.044366  9476 layer_factory.hpp:77] Creating layer relu6
I0521 02:40:13.044378  9476 net.cpp:106] Creating Layer relu6
I0521 02:40:13.044389  9476 net.cpp:454] relu6 <- ip2
I0521 02:40:13.044400  9476 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:40:13.044914  9476 net.cpp:150] Setting up relu6
I0521 02:40:13.044930  9476 net.cpp:157] Top shape: 580 98 (56840)
I0521 02:40:13.044941  9476 net.cpp:165] Memory required for data: 915414000
I0521 02:40:13.044951  9476 layer_factory.hpp:77] Creating layer drop2
I0521 02:40:13.044963  9476 net.cpp:106] Creating Layer drop2
I0521 02:40:13.044973  9476 net.cpp:454] drop2 <- ip2
I0521 02:40:13.044986  9476 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:40:13.045028  9476 net.cpp:150] Setting up drop2
I0521 02:40:13.045042  9476 net.cpp:157] Top shape: 580 98 (56840)
I0521 02:40:13.045052  9476 net.cpp:165] Memory required for data: 915641360
I0521 02:40:13.045061  9476 layer_factory.hpp:77] Creating layer ip3
I0521 02:40:13.045074  9476 net.cpp:106] Creating Layer ip3
I0521 02:40:13.045084  9476 net.cpp:454] ip3 <- ip2
I0521 02:40:13.045096  9476 net.cpp:411] ip3 -> ip3
I0521 02:40:13.045306  9476 net.cpp:150] Setting up ip3
I0521 02:40:13.045320  9476 net.cpp:157] Top shape: 580 11 (6380)
I0521 02:40:13.045331  9476 net.cpp:165] Memory required for data: 915666880
I0521 02:40:13.045348  9476 layer_factory.hpp:77] Creating layer drop3
I0521 02:40:13.045361  9476 net.cpp:106] Creating Layer drop3
I0521 02:40:13.045370  9476 net.cpp:454] drop3 <- ip3
I0521 02:40:13.045382  9476 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:40:13.045421  9476 net.cpp:150] Setting up drop3
I0521 02:40:13.045433  9476 net.cpp:157] Top shape: 580 11 (6380)
I0521 02:40:13.045444  9476 net.cpp:165] Memory required for data: 915692400
I0521 02:40:13.045452  9476 layer_factory.hpp:77] Creating layer loss
I0521 02:40:13.045474  9476 net.cpp:106] Creating Layer loss
I0521 02:40:13.045482  9476 net.cpp:454] loss <- ip3
I0521 02:40:13.045495  9476 net.cpp:454] loss <- label
I0521 02:40:13.045506  9476 net.cpp:411] loss -> loss
I0521 02:40:13.045522  9476 layer_factory.hpp:77] Creating layer loss
I0521 02:40:13.046171  9476 net.cpp:150] Setting up loss
I0521 02:40:13.046192  9476 net.cpp:157] Top shape: (1)
I0521 02:40:13.046205  9476 net.cpp:160]     with loss weight 1
I0521 02:40:13.046250  9476 net.cpp:165] Memory required for data: 915692404
I0521 02:40:13.046262  9476 net.cpp:226] loss needs backward computation.
I0521 02:40:13.046272  9476 net.cpp:226] drop3 needs backward computation.
I0521 02:40:13.046280  9476 net.cpp:226] ip3 needs backward computation.
I0521 02:40:13.046290  9476 net.cpp:226] drop2 needs backward computation.
I0521 02:40:13.046300  9476 net.cpp:226] relu6 needs backward computation.
I0521 02:40:13.046310  9476 net.cpp:226] ip2 needs backward computation.
I0521 02:40:13.046320  9476 net.cpp:226] drop1 needs backward computation.
I0521 02:40:13.046330  9476 net.cpp:226] relu5 needs backward computation.
I0521 02:40:13.046339  9476 net.cpp:226] ip1 needs backward computation.
I0521 02:40:13.046350  9476 net.cpp:226] pool4 needs backward computation.
I0521 02:40:13.046360  9476 net.cpp:226] relu4 needs backward computation.
I0521 02:40:13.046370  9476 net.cpp:226] conv4 needs backward computation.
I0521 02:40:13.046380  9476 net.cpp:226] pool3 needs backward computation.
I0521 02:40:13.046399  9476 net.cpp:226] relu3 needs backward computation.
I0521 02:40:13.046411  9476 net.cpp:226] conv3 needs backward computation.
I0521 02:40:13.046422  9476 net.cpp:226] pool2 needs backward computation.
I0521 02:40:13.046432  9476 net.cpp:226] relu2 needs backward computation.
I0521 02:40:13.046440  9476 net.cpp:226] conv2 needs backward computation.
I0521 02:40:13.046452  9476 net.cpp:226] pool1 needs backward computation.
I0521 02:40:13.046461  9476 net.cpp:226] relu1 needs backward computation.
I0521 02:40:13.046471  9476 net.cpp:226] conv1 needs backward computation.
I0521 02:40:13.046483  9476 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:40:13.046492  9476 net.cpp:270] This network produces output loss
I0521 02:40:13.046516  9476 net.cpp:283] Network initialization done.
I0521 02:40:13.048117  9476 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664.prototxt
I0521 02:40:13.048188  9476 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 02:40:13.048544  9476 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 580
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:40:13.048733  9476 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:40:13.048748  9476 net.cpp:106] Creating Layer data_hdf5
I0521 02:40:13.048760  9476 net.cpp:411] data_hdf5 -> data
I0521 02:40:13.048777  9476 net.cpp:411] data_hdf5 -> label
I0521 02:40:13.048794  9476 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 02:40:13.050132  9476 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 02:40:34.364779  9476 net.cpp:150] Setting up data_hdf5
I0521 02:40:34.364948  9476 net.cpp:157] Top shape: 580 1 127 50 (3683000)
I0521 02:40:34.364962  9476 net.cpp:157] Top shape: 580 (580)
I0521 02:40:34.364974  9476 net.cpp:165] Memory required for data: 14734320
I0521 02:40:34.364989  9476 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 02:40:34.365016  9476 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 02:40:34.365027  9476 net.cpp:454] label_data_hdf5_1_split <- label
I0521 02:40:34.365042  9476 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 02:40:34.365063  9476 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 02:40:34.365136  9476 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 02:40:34.365150  9476 net.cpp:157] Top shape: 580 (580)
I0521 02:40:34.365161  9476 net.cpp:157] Top shape: 580 (580)
I0521 02:40:34.365171  9476 net.cpp:165] Memory required for data: 14738960
I0521 02:40:34.365181  9476 layer_factory.hpp:77] Creating layer conv1
I0521 02:40:34.365203  9476 net.cpp:106] Creating Layer conv1
I0521 02:40:34.365214  9476 net.cpp:454] conv1 <- data
I0521 02:40:34.365226  9476 net.cpp:411] conv1 -> conv1
I0521 02:40:34.367195  9476 net.cpp:150] Setting up conv1
I0521 02:40:34.367220  9476 net.cpp:157] Top shape: 580 12 120 48 (40089600)
I0521 02:40:34.367231  9476 net.cpp:165] Memory required for data: 175097360
I0521 02:40:34.367251  9476 layer_factory.hpp:77] Creating layer relu1
I0521 02:40:34.367266  9476 net.cpp:106] Creating Layer relu1
I0521 02:40:34.367276  9476 net.cpp:454] relu1 <- conv1
I0521 02:40:34.367290  9476 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:40:34.367796  9476 net.cpp:150] Setting up relu1
I0521 02:40:34.367812  9476 net.cpp:157] Top shape: 580 12 120 48 (40089600)
I0521 02:40:34.367823  9476 net.cpp:165] Memory required for data: 335455760
I0521 02:40:34.367835  9476 layer_factory.hpp:77] Creating layer pool1
I0521 02:40:34.367849  9476 net.cpp:106] Creating Layer pool1
I0521 02:40:34.367859  9476 net.cpp:454] pool1 <- conv1
I0521 02:40:34.367872  9476 net.cpp:411] pool1 -> pool1
I0521 02:40:34.367949  9476 net.cpp:150] Setting up pool1
I0521 02:40:34.367961  9476 net.cpp:157] Top shape: 580 12 60 48 (20044800)
I0521 02:40:34.367971  9476 net.cpp:165] Memory required for data: 415634960
I0521 02:40:34.367981  9476 layer_factory.hpp:77] Creating layer conv2
I0521 02:40:34.368000  9476 net.cpp:106] Creating Layer conv2
I0521 02:40:34.368010  9476 net.cpp:454] conv2 <- pool1
I0521 02:40:34.368023  9476 net.cpp:411] conv2 -> conv2
I0521 02:40:34.369937  9476 net.cpp:150] Setting up conv2
I0521 02:40:34.369961  9476 net.cpp:157] Top shape: 580 20 54 46 (28814400)
I0521 02:40:34.369972  9476 net.cpp:165] Memory required for data: 530892560
I0521 02:40:34.369990  9476 layer_factory.hpp:77] Creating layer relu2
I0521 02:40:34.370003  9476 net.cpp:106] Creating Layer relu2
I0521 02:40:34.370013  9476 net.cpp:454] relu2 <- conv2
I0521 02:40:34.370025  9476 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:40:34.370358  9476 net.cpp:150] Setting up relu2
I0521 02:40:34.370373  9476 net.cpp:157] Top shape: 580 20 54 46 (28814400)
I0521 02:40:34.370383  9476 net.cpp:165] Memory required for data: 646150160
I0521 02:40:34.370393  9476 layer_factory.hpp:77] Creating layer pool2
I0521 02:40:34.370406  9476 net.cpp:106] Creating Layer pool2
I0521 02:40:34.370415  9476 net.cpp:454] pool2 <- conv2
I0521 02:40:34.370427  9476 net.cpp:411] pool2 -> pool2
I0521 02:40:34.370498  9476 net.cpp:150] Setting up pool2
I0521 02:40:34.370512  9476 net.cpp:157] Top shape: 580 20 27 46 (14407200)
I0521 02:40:34.370520  9476 net.cpp:165] Memory required for data: 703778960
I0521 02:40:34.370530  9476 layer_factory.hpp:77] Creating layer conv3
I0521 02:40:34.370548  9476 net.cpp:106] Creating Layer conv3
I0521 02:40:34.370559  9476 net.cpp:454] conv3 <- pool2
I0521 02:40:34.370573  9476 net.cpp:411] conv3 -> conv3
I0521 02:40:34.372545  9476 net.cpp:150] Setting up conv3
I0521 02:40:34.372567  9476 net.cpp:157] Top shape: 580 28 22 44 (15720320)
I0521 02:40:34.372581  9476 net.cpp:165] Memory required for data: 766660240
I0521 02:40:34.372613  9476 layer_factory.hpp:77] Creating layer relu3
I0521 02:40:34.372627  9476 net.cpp:106] Creating Layer relu3
I0521 02:40:34.372637  9476 net.cpp:454] relu3 <- conv3
I0521 02:40:34.372650  9476 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:40:34.373124  9476 net.cpp:150] Setting up relu3
I0521 02:40:34.373141  9476 net.cpp:157] Top shape: 580 28 22 44 (15720320)
I0521 02:40:34.373152  9476 net.cpp:165] Memory required for data: 829541520
I0521 02:40:34.373162  9476 layer_factory.hpp:77] Creating layer pool3
I0521 02:40:34.373174  9476 net.cpp:106] Creating Layer pool3
I0521 02:40:34.373184  9476 net.cpp:454] pool3 <- conv3
I0521 02:40:34.373196  9476 net.cpp:411] pool3 -> pool3
I0521 02:40:34.373270  9476 net.cpp:150] Setting up pool3
I0521 02:40:34.373282  9476 net.cpp:157] Top shape: 580 28 11 44 (7860160)
I0521 02:40:34.373292  9476 net.cpp:165] Memory required for data: 860982160
I0521 02:40:34.373302  9476 layer_factory.hpp:77] Creating layer conv4
I0521 02:40:34.373319  9476 net.cpp:106] Creating Layer conv4
I0521 02:40:34.373329  9476 net.cpp:454] conv4 <- pool3
I0521 02:40:34.373343  9476 net.cpp:411] conv4 -> conv4
I0521 02:40:34.375391  9476 net.cpp:150] Setting up conv4
I0521 02:40:34.375412  9476 net.cpp:157] Top shape: 580 36 6 42 (5261760)
I0521 02:40:34.375425  9476 net.cpp:165] Memory required for data: 882029200
I0521 02:40:34.375442  9476 layer_factory.hpp:77] Creating layer relu4
I0521 02:40:34.375455  9476 net.cpp:106] Creating Layer relu4
I0521 02:40:34.375473  9476 net.cpp:454] relu4 <- conv4
I0521 02:40:34.375486  9476 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:40:34.375960  9476 net.cpp:150] Setting up relu4
I0521 02:40:34.375977  9476 net.cpp:157] Top shape: 580 36 6 42 (5261760)
I0521 02:40:34.375988  9476 net.cpp:165] Memory required for data: 903076240
I0521 02:40:34.375998  9476 layer_factory.hpp:77] Creating layer pool4
I0521 02:40:34.376010  9476 net.cpp:106] Creating Layer pool4
I0521 02:40:34.376020  9476 net.cpp:454] pool4 <- conv4
I0521 02:40:34.376034  9476 net.cpp:411] pool4 -> pool4
I0521 02:40:34.376106  9476 net.cpp:150] Setting up pool4
I0521 02:40:34.376118  9476 net.cpp:157] Top shape: 580 36 3 42 (2630880)
I0521 02:40:34.376128  9476 net.cpp:165] Memory required for data: 913599760
I0521 02:40:34.376138  9476 layer_factory.hpp:77] Creating layer ip1
I0521 02:40:34.376153  9476 net.cpp:106] Creating Layer ip1
I0521 02:40:34.376163  9476 net.cpp:454] ip1 <- pool4
I0521 02:40:34.376178  9476 net.cpp:411] ip1 -> ip1
I0521 02:40:34.391626  9476 net.cpp:150] Setting up ip1
I0521 02:40:34.391655  9476 net.cpp:157] Top shape: 580 196 (113680)
I0521 02:40:34.391669  9476 net.cpp:165] Memory required for data: 914054480
I0521 02:40:34.391691  9476 layer_factory.hpp:77] Creating layer relu5
I0521 02:40:34.391706  9476 net.cpp:106] Creating Layer relu5
I0521 02:40:34.391717  9476 net.cpp:454] relu5 <- ip1
I0521 02:40:34.391731  9476 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:40:34.392077  9476 net.cpp:150] Setting up relu5
I0521 02:40:34.392091  9476 net.cpp:157] Top shape: 580 196 (113680)
I0521 02:40:34.392102  9476 net.cpp:165] Memory required for data: 914509200
I0521 02:40:34.392112  9476 layer_factory.hpp:77] Creating layer drop1
I0521 02:40:34.392129  9476 net.cpp:106] Creating Layer drop1
I0521 02:40:34.392139  9476 net.cpp:454] drop1 <- ip1
I0521 02:40:34.392153  9476 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:40:34.392197  9476 net.cpp:150] Setting up drop1
I0521 02:40:34.392210  9476 net.cpp:157] Top shape: 580 196 (113680)
I0521 02:40:34.392218  9476 net.cpp:165] Memory required for data: 914963920
I0521 02:40:34.392230  9476 layer_factory.hpp:77] Creating layer ip2
I0521 02:40:34.392244  9476 net.cpp:106] Creating Layer ip2
I0521 02:40:34.392254  9476 net.cpp:454] ip2 <- ip1
I0521 02:40:34.392267  9476 net.cpp:411] ip2 -> ip2
I0521 02:40:34.392750  9476 net.cpp:150] Setting up ip2
I0521 02:40:34.392762  9476 net.cpp:157] Top shape: 580 98 (56840)
I0521 02:40:34.392772  9476 net.cpp:165] Memory required for data: 915191280
I0521 02:40:34.392802  9476 layer_factory.hpp:77] Creating layer relu6
I0521 02:40:34.392814  9476 net.cpp:106] Creating Layer relu6
I0521 02:40:34.392824  9476 net.cpp:454] relu6 <- ip2
I0521 02:40:34.392837  9476 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:40:34.393374  9476 net.cpp:150] Setting up relu6
I0521 02:40:34.393390  9476 net.cpp:157] Top shape: 580 98 (56840)
I0521 02:40:34.393400  9476 net.cpp:165] Memory required for data: 915418640
I0521 02:40:34.393410  9476 layer_factory.hpp:77] Creating layer drop2
I0521 02:40:34.393424  9476 net.cpp:106] Creating Layer drop2
I0521 02:40:34.393435  9476 net.cpp:454] drop2 <- ip2
I0521 02:40:34.393446  9476 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:40:34.393491  9476 net.cpp:150] Setting up drop2
I0521 02:40:34.393503  9476 net.cpp:157] Top shape: 580 98 (56840)
I0521 02:40:34.393513  9476 net.cpp:165] Memory required for data: 915646000
I0521 02:40:34.393523  9476 layer_factory.hpp:77] Creating layer ip3
I0521 02:40:34.393537  9476 net.cpp:106] Creating Layer ip3
I0521 02:40:34.393548  9476 net.cpp:454] ip3 <- ip2
I0521 02:40:34.393561  9476 net.cpp:411] ip3 -> ip3
I0521 02:40:34.393785  9476 net.cpp:150] Setting up ip3
I0521 02:40:34.393798  9476 net.cpp:157] Top shape: 580 11 (6380)
I0521 02:40:34.393808  9476 net.cpp:165] Memory required for data: 915671520
I0521 02:40:34.393823  9476 layer_factory.hpp:77] Creating layer drop3
I0521 02:40:34.393836  9476 net.cpp:106] Creating Layer drop3
I0521 02:40:34.393846  9476 net.cpp:454] drop3 <- ip3
I0521 02:40:34.393859  9476 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:40:34.393900  9476 net.cpp:150] Setting up drop3
I0521 02:40:34.393913  9476 net.cpp:157] Top shape: 580 11 (6380)
I0521 02:40:34.393923  9476 net.cpp:165] Memory required for data: 915697040
I0521 02:40:34.393932  9476 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 02:40:34.393945  9476 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 02:40:34.393955  9476 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 02:40:34.393968  9476 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 02:40:34.393982  9476 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 02:40:34.394057  9476 net.cpp:150] Setting up ip3_drop3_0_split
I0521 02:40:34.394069  9476 net.cpp:157] Top shape: 580 11 (6380)
I0521 02:40:34.394081  9476 net.cpp:157] Top shape: 580 11 (6380)
I0521 02:40:34.394091  9476 net.cpp:165] Memory required for data: 915748080
I0521 02:40:34.394101  9476 layer_factory.hpp:77] Creating layer accuracy
I0521 02:40:34.394124  9476 net.cpp:106] Creating Layer accuracy
I0521 02:40:34.394134  9476 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 02:40:34.394145  9476 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 02:40:34.394158  9476 net.cpp:411] accuracy -> accuracy
I0521 02:40:34.394182  9476 net.cpp:150] Setting up accuracy
I0521 02:40:34.394194  9476 net.cpp:157] Top shape: (1)
I0521 02:40:34.394204  9476 net.cpp:165] Memory required for data: 915748084
I0521 02:40:34.394215  9476 layer_factory.hpp:77] Creating layer loss
I0521 02:40:34.394228  9476 net.cpp:106] Creating Layer loss
I0521 02:40:34.394238  9476 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 02:40:34.394249  9476 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 02:40:34.394263  9476 net.cpp:411] loss -> loss
I0521 02:40:34.394280  9476 layer_factory.hpp:77] Creating layer loss
I0521 02:40:34.394773  9476 net.cpp:150] Setting up loss
I0521 02:40:34.394788  9476 net.cpp:157] Top shape: (1)
I0521 02:40:34.394798  9476 net.cpp:160]     with loss weight 1
I0521 02:40:34.394817  9476 net.cpp:165] Memory required for data: 915748088
I0521 02:40:34.394827  9476 net.cpp:226] loss needs backward computation.
I0521 02:40:34.394839  9476 net.cpp:228] accuracy does not need backward computation.
I0521 02:40:34.394850  9476 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 02:40:34.394860  9476 net.cpp:226] drop3 needs backward computation.
I0521 02:40:34.394870  9476 net.cpp:226] ip3 needs backward computation.
I0521 02:40:34.394879  9476 net.cpp:226] drop2 needs backward computation.
I0521 02:40:34.394897  9476 net.cpp:226] relu6 needs backward computation.
I0521 02:40:34.394907  9476 net.cpp:226] ip2 needs backward computation.
I0521 02:40:34.394917  9476 net.cpp:226] drop1 needs backward computation.
I0521 02:40:34.394927  9476 net.cpp:226] relu5 needs backward computation.
I0521 02:40:34.394937  9476 net.cpp:226] ip1 needs backward computation.
I0521 02:40:34.394947  9476 net.cpp:226] pool4 needs backward computation.
I0521 02:40:34.394956  9476 net.cpp:226] relu4 needs backward computation.
I0521 02:40:34.394966  9476 net.cpp:226] conv4 needs backward computation.
I0521 02:40:34.394976  9476 net.cpp:226] pool3 needs backward computation.
I0521 02:40:34.394986  9476 net.cpp:226] relu3 needs backward computation.
I0521 02:40:34.394995  9476 net.cpp:226] conv3 needs backward computation.
I0521 02:40:34.395005  9476 net.cpp:226] pool2 needs backward computation.
I0521 02:40:34.395016  9476 net.cpp:226] relu2 needs backward computation.
I0521 02:40:34.395027  9476 net.cpp:226] conv2 needs backward computation.
I0521 02:40:34.395038  9476 net.cpp:226] pool1 needs backward computation.
I0521 02:40:34.395048  9476 net.cpp:226] relu1 needs backward computation.
I0521 02:40:34.395058  9476 net.cpp:226] conv1 needs backward computation.
I0521 02:40:34.395069  9476 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 02:40:34.395081  9476 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:40:34.395089  9476 net.cpp:270] This network produces output accuracy
I0521 02:40:34.395102  9476 net.cpp:270] This network produces output loss
I0521 02:40:34.395129  9476 net.cpp:283] Network initialization done.
I0521 02:40:34.395262  9476 solver.cpp:60] Solver scaffolding done.
I0521 02:40:34.396414  9476 caffe.cpp:212] Starting Optimization
I0521 02:40:34.396432  9476 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 02:40:34.396446  9476 solver.cpp:289] Learning Rate Policy: fixed
I0521 02:40:34.397661  9476 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 02:41:20.359860  9476 solver.cpp:409]     Test net output #0: accuracy = 0.1252
I0521 02:41:20.360026  9476 solver.cpp:409]     Test net output #1: loss = 2.39716 (* 1 = 2.39716 loss)
I0521 02:41:20.471999  9476 solver.cpp:237] Iteration 0, loss = 2.39655
I0521 02:41:20.472036  9476 solver.cpp:253]     Train net output #0: loss = 2.39655 (* 1 = 2.39655 loss)
I0521 02:41:20.472054  9476 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 02:41:28.303876  9476 solver.cpp:237] Iteration 25, loss = 2.38369
I0521 02:41:28.303915  9476 solver.cpp:253]     Train net output #0: loss = 2.38369 (* 1 = 2.38369 loss)
I0521 02:41:28.303936  9476 sgd_solver.cpp:106] Iteration 25, lr = 0.0025
I0521 02:41:36.133225  9476 solver.cpp:237] Iteration 50, loss = 2.37236
I0521 02:41:36.133257  9476 solver.cpp:253]     Train net output #0: loss = 2.37236 (* 1 = 2.37236 loss)
I0521 02:41:36.133275  9476 sgd_solver.cpp:106] Iteration 50, lr = 0.0025
I0521 02:41:43.965116  9476 solver.cpp:237] Iteration 75, loss = 2.34641
I0521 02:41:43.965148  9476 solver.cpp:253]     Train net output #0: loss = 2.34641 (* 1 = 2.34641 loss)
I0521 02:41:43.965164  9476 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 02:41:51.794060  9476 solver.cpp:237] Iteration 100, loss = 2.33355
I0521 02:41:51.794217  9476 solver.cpp:253]     Train net output #0: loss = 2.33355 (* 1 = 2.33355 loss)
I0521 02:41:51.794232  9476 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0521 02:41:59.621510  9476 solver.cpp:237] Iteration 125, loss = 2.33302
I0521 02:41:59.621541  9476 solver.cpp:253]     Train net output #0: loss = 2.33302 (* 1 = 2.33302 loss)
I0521 02:41:59.621559  9476 sgd_solver.cpp:106] Iteration 125, lr = 0.0025
I0521 02:42:07.450949  9476 solver.cpp:237] Iteration 150, loss = 2.33661
I0521 02:42:07.450980  9476 solver.cpp:253]     Train net output #0: loss = 2.33661 (* 1 = 2.33661 loss)
I0521 02:42:07.450999  9476 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 02:42:37.417033  9476 solver.cpp:237] Iteration 175, loss = 2.33353
I0521 02:42:37.417193  9476 solver.cpp:253]     Train net output #0: loss = 2.33353 (* 1 = 2.33353 loss)
I0521 02:42:37.417207  9476 sgd_solver.cpp:106] Iteration 175, lr = 0.0025
I0521 02:42:45.246168  9476 solver.cpp:237] Iteration 200, loss = 2.29706
I0521 02:42:45.246201  9476 solver.cpp:253]     Train net output #0: loss = 2.29706 (* 1 = 2.29706 loss)
I0521 02:42:45.246218  9476 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0521 02:42:53.076655  9476 solver.cpp:237] Iteration 225, loss = 2.2894
I0521 02:42:53.076689  9476 solver.cpp:253]     Train net output #0: loss = 2.2894 (* 1 = 2.2894 loss)
I0521 02:42:53.076704  9476 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 02:43:00.909621  9476 solver.cpp:237] Iteration 250, loss = 2.24651
I0521 02:43:00.909653  9476 solver.cpp:253]     Train net output #0: loss = 2.24651 (* 1 = 2.24651 loss)
I0521 02:43:00.909670  9476 sgd_solver.cpp:106] Iteration 250, lr = 0.0025
I0521 02:43:03.103586  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_258.caffemodel
I0521 02:43:03.407497  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_258.solverstate
I0521 02:43:08.849233  9476 solver.cpp:237] Iteration 275, loss = 2.26879
I0521 02:43:08.849390  9476 solver.cpp:253]     Train net output #0: loss = 2.26879 (* 1 = 2.26879 loss)
I0521 02:43:08.849403  9476 sgd_solver.cpp:106] Iteration 275, lr = 0.0025
I0521 02:43:16.682715  9476 solver.cpp:237] Iteration 300, loss = 2.24267
I0521 02:43:16.682746  9476 solver.cpp:253]     Train net output #0: loss = 2.24267 (* 1 = 2.24267 loss)
I0521 02:43:16.682765  9476 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 02:43:24.508793  9476 solver.cpp:237] Iteration 325, loss = 2.20583
I0521 02:43:24.508826  9476 solver.cpp:253]     Train net output #0: loss = 2.20583 (* 1 = 2.20583 loss)
I0521 02:43:24.508843  9476 sgd_solver.cpp:106] Iteration 325, lr = 0.0025
I0521 02:43:54.487013  9476 solver.cpp:237] Iteration 350, loss = 2.16009
I0521 02:43:54.487174  9476 solver.cpp:253]     Train net output #0: loss = 2.16009 (* 1 = 2.16009 loss)
I0521 02:43:54.487188  9476 sgd_solver.cpp:106] Iteration 350, lr = 0.0025
I0521 02:44:02.316980  9476 solver.cpp:237] Iteration 375, loss = 2.16957
I0521 02:44:02.317013  9476 solver.cpp:253]     Train net output #0: loss = 2.16957 (* 1 = 2.16957 loss)
I0521 02:44:02.317031  9476 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 02:44:10.145212  9476 solver.cpp:237] Iteration 400, loss = 2.10154
I0521 02:44:10.145246  9476 solver.cpp:253]     Train net output #0: loss = 2.10154 (* 1 = 2.10154 loss)
I0521 02:44:10.145262  9476 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 02:44:17.977782  9476 solver.cpp:237] Iteration 425, loss = 2.1034
I0521 02:44:17.977821  9476 solver.cpp:253]     Train net output #0: loss = 2.1034 (* 1 = 2.1034 loss)
I0521 02:44:17.977839  9476 sgd_solver.cpp:106] Iteration 425, lr = 0.0025
I0521 02:44:25.810358  9476 solver.cpp:237] Iteration 450, loss = 2.09975
I0521 02:44:25.810502  9476 solver.cpp:253]     Train net output #0: loss = 2.09975 (* 1 = 2.09975 loss)
I0521 02:44:25.810515  9476 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 02:44:33.641753  9476 solver.cpp:237] Iteration 475, loss = 2.05322
I0521 02:44:33.641784  9476 solver.cpp:253]     Train net output #0: loss = 2.05322 (* 1 = 2.05322 loss)
I0521 02:44:33.641803  9476 sgd_solver.cpp:106] Iteration 475, lr = 0.0025
I0521 02:44:41.474581  9476 solver.cpp:237] Iteration 500, loss = 2.03345
I0521 02:44:41.474614  9476 solver.cpp:253]     Train net output #0: loss = 2.03345 (* 1 = 2.03345 loss)
I0521 02:44:41.474632  9476 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0521 02:44:46.173326  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_516.caffemodel
I0521 02:44:46.432641  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_516.solverstate
I0521 02:44:46.551542  9476 solver.cpp:341] Iteration 517, Testing net (#0)
I0521 02:45:31.834714  9476 solver.cpp:409]     Test net output #0: accuracy = 0.520597
I0521 02:45:31.834882  9476 solver.cpp:409]     Test net output #1: loss = 1.75312 (* 1 = 1.75312 loss)
I0521 02:45:56.603133  9476 solver.cpp:237] Iteration 525, loss = 1.94813
I0521 02:45:56.603188  9476 solver.cpp:253]     Train net output #0: loss = 1.94813 (* 1 = 1.94813 loss)
I0521 02:45:56.603204  9476 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 02:46:04.435515  9476 solver.cpp:237] Iteration 550, loss = 1.94683
I0521 02:46:04.435670  9476 solver.cpp:253]     Train net output #0: loss = 1.94683 (* 1 = 1.94683 loss)
I0521 02:46:04.435684  9476 sgd_solver.cpp:106] Iteration 550, lr = 0.0025
I0521 02:46:12.259845  9476 solver.cpp:237] Iteration 575, loss = 1.95337
I0521 02:46:12.259876  9476 solver.cpp:253]     Train net output #0: loss = 1.95337 (* 1 = 1.95337 loss)
I0521 02:46:12.259894  9476 sgd_solver.cpp:106] Iteration 575, lr = 0.0025
I0521 02:46:20.085986  9476 solver.cpp:237] Iteration 600, loss = 1.93942
I0521 02:46:20.086019  9476 solver.cpp:253]     Train net output #0: loss = 1.93942 (* 1 = 1.93942 loss)
I0521 02:46:20.086035  9476 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 02:46:27.913843  9476 solver.cpp:237] Iteration 625, loss = 1.92393
I0521 02:46:27.913872  9476 solver.cpp:253]     Train net output #0: loss = 1.92393 (* 1 = 1.92393 loss)
I0521 02:46:27.913899  9476 sgd_solver.cpp:106] Iteration 625, lr = 0.0025
I0521 02:46:35.744738  9476 solver.cpp:237] Iteration 650, loss = 1.89145
I0521 02:46:35.744889  9476 solver.cpp:253]     Train net output #0: loss = 1.89145 (* 1 = 1.89145 loss)
I0521 02:46:35.744901  9476 sgd_solver.cpp:106] Iteration 650, lr = 0.0025
I0521 02:46:43.567195  9476 solver.cpp:237] Iteration 675, loss = 1.89738
I0521 02:46:43.567226  9476 solver.cpp:253]     Train net output #0: loss = 1.89738 (* 1 = 1.89738 loss)
I0521 02:46:43.567241  9476 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 02:47:13.522227  9476 solver.cpp:237] Iteration 700, loss = 1.89159
I0521 02:47:13.522398  9476 solver.cpp:253]     Train net output #0: loss = 1.89159 (* 1 = 1.89159 loss)
I0521 02:47:13.522413  9476 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0521 02:47:21.350199  9476 solver.cpp:237] Iteration 725, loss = 1.87128
I0521 02:47:21.350230  9476 solver.cpp:253]     Train net output #0: loss = 1.87128 (* 1 = 1.87128 loss)
I0521 02:47:21.350249  9476 sgd_solver.cpp:106] Iteration 725, lr = 0.0025
I0521 02:47:29.174346  9476 solver.cpp:237] Iteration 750, loss = 1.8375
I0521 02:47:29.174378  9476 solver.cpp:253]     Train net output #0: loss = 1.8375 (* 1 = 1.8375 loss)
I0521 02:47:29.174392  9476 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 02:47:36.372936  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_774.caffemodel
I0521 02:47:36.635273  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_774.solverstate
I0521 02:47:37.068435  9476 solver.cpp:237] Iteration 775, loss = 1.8526
I0521 02:47:37.068486  9476 solver.cpp:253]     Train net output #0: loss = 1.8526 (* 1 = 1.8526 loss)
I0521 02:47:37.068500  9476 sgd_solver.cpp:106] Iteration 775, lr = 0.0025
I0521 02:47:44.900365  9476 solver.cpp:237] Iteration 800, loss = 1.82029
I0521 02:47:44.900533  9476 solver.cpp:253]     Train net output #0: loss = 1.82029 (* 1 = 1.82029 loss)
I0521 02:47:44.900548  9476 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 02:47:52.725425  9476 solver.cpp:237] Iteration 825, loss = 1.82673
I0521 02:47:52.725455  9476 solver.cpp:253]     Train net output #0: loss = 1.82673 (* 1 = 1.82673 loss)
I0521 02:47:52.725474  9476 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 02:48:00.548861  9476 solver.cpp:237] Iteration 850, loss = 1.86843
I0521 02:48:00.548895  9476 solver.cpp:253]     Train net output #0: loss = 1.86843 (* 1 = 1.86843 loss)
I0521 02:48:00.548909  9476 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0521 02:48:30.496718  9476 solver.cpp:237] Iteration 875, loss = 1.78233
I0521 02:48:30.496892  9476 solver.cpp:253]     Train net output #0: loss = 1.78233 (* 1 = 1.78233 loss)
I0521 02:48:30.496906  9476 sgd_solver.cpp:106] Iteration 875, lr = 0.0025
I0521 02:48:38.326494  9476 solver.cpp:237] Iteration 900, loss = 1.83265
I0521 02:48:38.326525  9476 solver.cpp:253]     Train net output #0: loss = 1.83265 (* 1 = 1.83265 loss)
I0521 02:48:38.326545  9476 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 02:48:46.158059  9476 solver.cpp:237] Iteration 925, loss = 1.79349
I0521 02:48:46.158092  9476 solver.cpp:253]     Train net output #0: loss = 1.79349 (* 1 = 1.79349 loss)
I0521 02:48:46.158108  9476 sgd_solver.cpp:106] Iteration 925, lr = 0.0025
I0521 02:48:53.987905  9476 solver.cpp:237] Iteration 950, loss = 1.85864
I0521 02:48:53.987941  9476 solver.cpp:253]     Train net output #0: loss = 1.85864 (* 1 = 1.85864 loss)
I0521 02:48:53.987960  9476 sgd_solver.cpp:106] Iteration 950, lr = 0.0025
I0521 02:49:01.818626  9476 solver.cpp:237] Iteration 975, loss = 1.77313
I0521 02:49:01.818768  9476 solver.cpp:253]     Train net output #0: loss = 1.77313 (* 1 = 1.77313 loss)
I0521 02:49:01.818781  9476 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 02:49:09.648334  9476 solver.cpp:237] Iteration 1000, loss = 1.7949
I0521 02:49:09.648365  9476 solver.cpp:253]     Train net output #0: loss = 1.7949 (* 1 = 1.7949 loss)
I0521 02:49:09.648382  9476 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0521 02:49:17.470054  9476 solver.cpp:237] Iteration 1025, loss = 1.80518
I0521 02:49:17.470088  9476 solver.cpp:253]     Train net output #0: loss = 1.80518 (* 1 = 1.80518 loss)
I0521 02:49:17.470105  9476 sgd_solver.cpp:106] Iteration 1025, lr = 0.0025
I0521 02:49:19.350528  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1032.caffemodel
I0521 02:49:19.610909  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1032.solverstate
I0521 02:49:20.044951  9476 solver.cpp:341] Iteration 1034, Testing net (#0)
I0521 02:50:26.140283  9476 solver.cpp:409]     Test net output #0: accuracy = 0.614107
I0521 02:50:26.140470  9476 solver.cpp:409]     Test net output #1: loss = 1.32731 (* 1 = 1.32731 loss)
I0521 02:50:53.407477  9476 solver.cpp:237] Iteration 1050, loss = 1.77978
I0521 02:50:53.407533  9476 solver.cpp:253]     Train net output #0: loss = 1.77978 (* 1 = 1.77978 loss)
I0521 02:50:53.407549  9476 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 02:51:01.230876  9476 solver.cpp:237] Iteration 1075, loss = 1.86008
I0521 02:51:01.231046  9476 solver.cpp:253]     Train net output #0: loss = 1.86008 (* 1 = 1.86008 loss)
I0521 02:51:01.231060  9476 sgd_solver.cpp:106] Iteration 1075, lr = 0.0025
I0521 02:51:09.053900  9476 solver.cpp:237] Iteration 1100, loss = 1.72329
I0521 02:51:09.053931  9476 solver.cpp:253]     Train net output #0: loss = 1.72329 (* 1 = 1.72329 loss)
I0521 02:51:09.053948  9476 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 02:51:16.875813  9476 solver.cpp:237] Iteration 1125, loss = 1.75475
I0521 02:51:16.875844  9476 solver.cpp:253]     Train net output #0: loss = 1.75475 (* 1 = 1.75475 loss)
I0521 02:51:16.875861  9476 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 02:51:24.696682  9476 solver.cpp:237] Iteration 1150, loss = 1.79212
I0521 02:51:24.696723  9476 solver.cpp:253]     Train net output #0: loss = 1.79212 (* 1 = 1.79212 loss)
I0521 02:51:24.696739  9476 sgd_solver.cpp:106] Iteration 1150, lr = 0.0025
I0521 02:51:32.517968  9476 solver.cpp:237] Iteration 1175, loss = 1.87768
I0521 02:51:32.518112  9476 solver.cpp:253]     Train net output #0: loss = 1.87768 (* 1 = 1.87768 loss)
I0521 02:51:32.518126  9476 sgd_solver.cpp:106] Iteration 1175, lr = 0.0025
I0521 02:51:40.339289  9476 solver.cpp:237] Iteration 1200, loss = 1.77404
I0521 02:51:40.339320  9476 solver.cpp:253]     Train net output #0: loss = 1.77404 (* 1 = 1.77404 loss)
I0521 02:51:40.339339  9476 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 02:52:10.350545  9476 solver.cpp:237] Iteration 1225, loss = 1.82737
I0521 02:52:10.350710  9476 solver.cpp:253]     Train net output #0: loss = 1.82737 (* 1 = 1.82737 loss)
I0521 02:52:10.350724  9476 sgd_solver.cpp:106] Iteration 1225, lr = 0.0025
I0521 02:52:18.173115  9476 solver.cpp:237] Iteration 1250, loss = 1.80082
I0521 02:52:18.173151  9476 solver.cpp:253]     Train net output #0: loss = 1.80082 (* 1 = 1.80082 loss)
I0521 02:52:18.173172  9476 sgd_solver.cpp:106] Iteration 1250, lr = 0.0025
I0521 02:52:25.996531  9476 solver.cpp:237] Iteration 1275, loss = 1.71892
I0521 02:52:25.996565  9476 solver.cpp:253]     Train net output #0: loss = 1.71892 (* 1 = 1.71892 loss)
I0521 02:52:25.996582  9476 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 02:52:30.375778  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1290.caffemodel
I0521 02:52:30.636201  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1290.solverstate
I0521 02:52:33.886219  9476 solver.cpp:237] Iteration 1300, loss = 1.78089
I0521 02:52:33.886270  9476 solver.cpp:253]     Train net output #0: loss = 1.78089 (* 1 = 1.78089 loss)
I0521 02:52:33.886286  9476 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0521 02:52:41.714239  9476 solver.cpp:237] Iteration 1325, loss = 1.71278
I0521 02:52:41.714407  9476 solver.cpp:253]     Train net output #0: loss = 1.71278 (* 1 = 1.71278 loss)
I0521 02:52:41.714421  9476 sgd_solver.cpp:106] Iteration 1325, lr = 0.0025
I0521 02:52:49.538656  9476 solver.cpp:237] Iteration 1350, loss = 1.7415
I0521 02:52:49.538689  9476 solver.cpp:253]     Train net output #0: loss = 1.7415 (* 1 = 1.7415 loss)
I0521 02:52:49.538707  9476 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 02:52:57.361603  9476 solver.cpp:237] Iteration 1375, loss = 1.75969
I0521 02:52:57.361636  9476 solver.cpp:253]     Train net output #0: loss = 1.75969 (* 1 = 1.75969 loss)
I0521 02:52:57.361654  9476 sgd_solver.cpp:106] Iteration 1375, lr = 0.0025
I0521 02:53:27.288794  9476 solver.cpp:237] Iteration 1400, loss = 1.70747
I0521 02:53:27.288964  9476 solver.cpp:253]     Train net output #0: loss = 1.70747 (* 1 = 1.70747 loss)
I0521 02:53:27.288977  9476 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0521 02:53:35.114599  9476 solver.cpp:237] Iteration 1425, loss = 1.68011
I0521 02:53:35.114642  9476 solver.cpp:253]     Train net output #0: loss = 1.68011 (* 1 = 1.68011 loss)
I0521 02:53:35.114657  9476 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 02:53:42.938158  9476 solver.cpp:237] Iteration 1450, loss = 1.77565
I0521 02:53:42.938191  9476 solver.cpp:253]     Train net output #0: loss = 1.77565 (* 1 = 1.77565 loss)
I0521 02:53:42.938208  9476 sgd_solver.cpp:106] Iteration 1450, lr = 0.0025
I0521 02:53:50.759719  9476 solver.cpp:237] Iteration 1475, loss = 1.77017
I0521 02:53:50.759752  9476 solver.cpp:253]     Train net output #0: loss = 1.77017 (* 1 = 1.77017 loss)
I0521 02:53:50.759768  9476 sgd_solver.cpp:106] Iteration 1475, lr = 0.0025
I0521 02:53:58.580902  9476 solver.cpp:237] Iteration 1500, loss = 1.76865
I0521 02:53:58.581053  9476 solver.cpp:253]     Train net output #0: loss = 1.76865 (* 1 = 1.76865 loss)
I0521 02:53:58.581068  9476 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 02:54:06.405632  9476 solver.cpp:237] Iteration 1525, loss = 1.6244
I0521 02:54:06.405663  9476 solver.cpp:253]     Train net output #0: loss = 1.6244 (* 1 = 1.6244 loss)
I0521 02:54:06.405681  9476 sgd_solver.cpp:106] Iteration 1525, lr = 0.0025
I0521 02:54:13.288496  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1548.caffemodel
I0521 02:54:13.545459  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1548.solverstate
I0521 02:54:14.291038  9476 solver.cpp:237] Iteration 1550, loss = 1.65519
I0521 02:54:14.291086  9476 solver.cpp:253]     Train net output #0: loss = 1.65519 (* 1 = 1.65519 loss)
I0521 02:54:14.291101  9476 sgd_solver.cpp:106] Iteration 1550, lr = 0.0025
I0521 02:54:14.291599  9476 solver.cpp:341] Iteration 1551, Testing net (#0)
I0521 02:54:59.231560  9476 solver.cpp:409]     Test net output #0: accuracy = 0.661908
I0521 02:54:59.231737  9476 solver.cpp:409]     Test net output #1: loss = 1.18417 (* 1 = 1.18417 loss)
I0521 02:55:28.983845  9476 solver.cpp:237] Iteration 1575, loss = 1.72339
I0521 02:55:28.983896  9476 solver.cpp:253]     Train net output #0: loss = 1.72339 (* 1 = 1.72339 loss)
I0521 02:55:28.983911  9476 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0521 02:55:36.803005  9476 solver.cpp:237] Iteration 1600, loss = 1.74442
I0521 02:55:36.803169  9476 solver.cpp:253]     Train net output #0: loss = 1.74442 (* 1 = 1.74442 loss)
I0521 02:55:36.803184  9476 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 02:55:44.627612  9476 solver.cpp:237] Iteration 1625, loss = 1.72376
I0521 02:55:44.627645  9476 solver.cpp:253]     Train net output #0: loss = 1.72376 (* 1 = 1.72376 loss)
I0521 02:55:44.627660  9476 sgd_solver.cpp:106] Iteration 1625, lr = 0.0025
I0521 02:55:52.450851  9476 solver.cpp:237] Iteration 1650, loss = 1.67784
I0521 02:55:52.450883  9476 solver.cpp:253]     Train net output #0: loss = 1.67784 (* 1 = 1.67784 loss)
I0521 02:55:52.450899  9476 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 02:56:00.269114  9476 solver.cpp:237] Iteration 1675, loss = 1.74512
I0521 02:56:00.269143  9476 solver.cpp:253]     Train net output #0: loss = 1.74512 (* 1 = 1.74512 loss)
I0521 02:56:00.269168  9476 sgd_solver.cpp:106] Iteration 1675, lr = 0.0025
I0521 02:56:08.087604  9476 solver.cpp:237] Iteration 1700, loss = 1.74598
I0521 02:56:08.087751  9476 solver.cpp:253]     Train net output #0: loss = 1.74598 (* 1 = 1.74598 loss)
I0521 02:56:08.087764  9476 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 02:56:38.073098  9476 solver.cpp:237] Iteration 1725, loss = 1.64319
I0521 02:56:38.073148  9476 solver.cpp:253]     Train net output #0: loss = 1.64319 (* 1 = 1.64319 loss)
I0521 02:56:38.073169  9476 sgd_solver.cpp:106] Iteration 1725, lr = 0.0025
I0521 02:56:45.894098  9476 solver.cpp:237] Iteration 1750, loss = 1.71015
I0521 02:56:45.894248  9476 solver.cpp:253]     Train net output #0: loss = 1.71015 (* 1 = 1.71015 loss)
I0521 02:56:45.894261  9476 sgd_solver.cpp:106] Iteration 1750, lr = 0.0025
I0521 02:56:53.711544  9476 solver.cpp:237] Iteration 1775, loss = 1.70023
I0521 02:56:53.711575  9476 solver.cpp:253]     Train net output #0: loss = 1.70023 (* 1 = 1.70023 loss)
I0521 02:56:53.711592  9476 sgd_solver.cpp:106] Iteration 1775, lr = 0.0025
I0521 02:57:01.529985  9476 solver.cpp:237] Iteration 1800, loss = 1.70853
I0521 02:57:01.530019  9476 solver.cpp:253]     Train net output #0: loss = 1.70853 (* 1 = 1.70853 loss)
I0521 02:57:01.530032  9476 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 02:57:03.094022  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1806.caffemodel
I0521 02:57:03.355283  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_1806.solverstate
I0521 02:57:09.419023  9476 solver.cpp:237] Iteration 1825, loss = 1.57748
I0521 02:57:09.419069  9476 solver.cpp:253]     Train net output #0: loss = 1.57748 (* 1 = 1.57748 loss)
I0521 02:57:09.419085  9476 sgd_solver.cpp:106] Iteration 1825, lr = 0.0025
I0521 02:57:17.239007  9476 solver.cpp:237] Iteration 1850, loss = 1.76596
I0521 02:57:17.239153  9476 solver.cpp:253]     Train net output #0: loss = 1.76596 (* 1 = 1.76596 loss)
I0521 02:57:17.239166  9476 sgd_solver.cpp:106] Iteration 1850, lr = 0.0025
I0521 02:57:25.057014  9476 solver.cpp:237] Iteration 1875, loss = 1.71547
I0521 02:57:25.057045  9476 solver.cpp:253]     Train net output #0: loss = 1.71547 (* 1 = 1.71547 loss)
I0521 02:57:25.057065  9476 sgd_solver.cpp:106] Iteration 1875, lr = 0.0025
I0521 02:57:55.071759  9476 solver.cpp:237] Iteration 1900, loss = 1.69798
I0521 02:57:55.071941  9476 solver.cpp:253]     Train net output #0: loss = 1.69798 (* 1 = 1.69798 loss)
I0521 02:57:55.071955  9476 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0521 02:58:02.892771  9476 solver.cpp:237] Iteration 1925, loss = 1.68444
I0521 02:58:02.892807  9476 solver.cpp:253]     Train net output #0: loss = 1.68444 (* 1 = 1.68444 loss)
I0521 02:58:02.892824  9476 sgd_solver.cpp:106] Iteration 1925, lr = 0.0025
I0521 02:58:10.713323  9476 solver.cpp:237] Iteration 1950, loss = 1.72497
I0521 02:58:10.713356  9476 solver.cpp:253]     Train net output #0: loss = 1.72497 (* 1 = 1.72497 loss)
I0521 02:58:10.713373  9476 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0521 02:58:18.529500  9476 solver.cpp:237] Iteration 1975, loss = 1.66435
I0521 02:58:18.529533  9476 solver.cpp:253]     Train net output #0: loss = 1.66435 (* 1 = 1.66435 loss)
I0521 02:58:18.529551  9476 sgd_solver.cpp:106] Iteration 1975, lr = 0.0025
I0521 02:58:26.353981  9476 solver.cpp:237] Iteration 2000, loss = 1.68248
I0521 02:58:26.354118  9476 solver.cpp:253]     Train net output #0: loss = 1.68248 (* 1 = 1.68248 loss)
I0521 02:58:26.354131  9476 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0521 02:58:34.174175  9476 solver.cpp:237] Iteration 2025, loss = 1.66228
I0521 02:58:34.174213  9476 solver.cpp:253]     Train net output #0: loss = 1.66228 (* 1 = 1.66228 loss)
I0521 02:58:34.174231  9476 sgd_solver.cpp:106] Iteration 2025, lr = 0.0025
I0521 02:58:41.992888  9476 solver.cpp:237] Iteration 2050, loss = 1.64335
I0521 02:58:41.992920  9476 solver.cpp:253]     Train net output #0: loss = 1.64335 (* 1 = 1.64335 loss)
I0521 02:58:41.992936  9476 sgd_solver.cpp:106] Iteration 2050, lr = 0.0025
I0521 02:58:46.057886  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2064.caffemodel
I0521 02:58:46.315130  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2064.solverstate
I0521 02:58:47.373114  9476 solver.cpp:341] Iteration 2068, Testing net (#0)
I0521 02:59:53.504040  9476 solver.cpp:409]     Test net output #0: accuracy = 0.674097
I0521 02:59:53.504218  9476 solver.cpp:409]     Test net output #1: loss = 1.12486 (* 1 = 1.12486 loss)
I0521 03:00:17.902611  9476 solver.cpp:237] Iteration 2075, loss = 1.66381
I0521 03:00:17.902667  9476 solver.cpp:253]     Train net output #0: loss = 1.66381 (* 1 = 1.66381 loss)
I0521 03:00:17.902683  9476 sgd_solver.cpp:106] Iteration 2075, lr = 0.0025
I0521 03:00:25.729882  9476 solver.cpp:237] Iteration 2100, loss = 1.58649
I0521 03:00:25.730029  9476 solver.cpp:253]     Train net output #0: loss = 1.58649 (* 1 = 1.58649 loss)
I0521 03:00:25.730042  9476 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0521 03:00:33.555474  9476 solver.cpp:237] Iteration 2125, loss = 1.63162
I0521 03:00:33.555517  9476 solver.cpp:253]     Train net output #0: loss = 1.63162 (* 1 = 1.63162 loss)
I0521 03:00:33.555538  9476 sgd_solver.cpp:106] Iteration 2125, lr = 0.0025
I0521 03:00:41.377373  9476 solver.cpp:237] Iteration 2150, loss = 1.64441
I0521 03:00:41.377400  9476 solver.cpp:253]     Train net output #0: loss = 1.64441 (* 1 = 1.64441 loss)
I0521 03:00:41.377413  9476 sgd_solver.cpp:106] Iteration 2150, lr = 0.0025
I0521 03:00:49.200255  9476 solver.cpp:237] Iteration 2175, loss = 1.59746
I0521 03:00:49.200287  9476 solver.cpp:253]     Train net output #0: loss = 1.59746 (* 1 = 1.59746 loss)
I0521 03:00:49.200304  9476 sgd_solver.cpp:106] Iteration 2175, lr = 0.0025
I0521 03:00:57.024230  9476 solver.cpp:237] Iteration 2200, loss = 1.74071
I0521 03:00:57.024370  9476 solver.cpp:253]     Train net output #0: loss = 1.74071 (* 1 = 1.74071 loss)
I0521 03:00:57.024384  9476 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0521 03:01:04.844944  9476 solver.cpp:237] Iteration 2225, loss = 1.68108
I0521 03:01:04.844981  9476 solver.cpp:253]     Train net output #0: loss = 1.68108 (* 1 = 1.68108 loss)
I0521 03:01:04.845001  9476 sgd_solver.cpp:106] Iteration 2225, lr = 0.0025
I0521 03:01:34.811254  9476 solver.cpp:237] Iteration 2250, loss = 1.5393
I0521 03:01:34.811444  9476 solver.cpp:253]     Train net output #0: loss = 1.5393 (* 1 = 1.5393 loss)
I0521 03:01:34.811458  9476 sgd_solver.cpp:106] Iteration 2250, lr = 0.0025
I0521 03:01:42.630718  9476 solver.cpp:237] Iteration 2275, loss = 1.67326
I0521 03:01:42.630751  9476 solver.cpp:253]     Train net output #0: loss = 1.67326 (* 1 = 1.67326 loss)
I0521 03:01:42.630769  9476 sgd_solver.cpp:106] Iteration 2275, lr = 0.0025
I0521 03:01:50.455765  9476 solver.cpp:237] Iteration 2300, loss = 1.65278
I0521 03:01:50.455811  9476 solver.cpp:253]     Train net output #0: loss = 1.65278 (* 1 = 1.65278 loss)
I0521 03:01:50.455826  9476 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0521 03:01:57.027768  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2322.caffemodel
I0521 03:01:57.287010  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2322.solverstate
I0521 03:01:58.346168  9476 solver.cpp:237] Iteration 2325, loss = 1.60146
I0521 03:01:58.346220  9476 solver.cpp:253]     Train net output #0: loss = 1.60146 (* 1 = 1.60146 loss)
I0521 03:01:58.346235  9476 sgd_solver.cpp:106] Iteration 2325, lr = 0.0025
I0521 03:02:06.168407  9476 solver.cpp:237] Iteration 2350, loss = 1.61199
I0521 03:02:06.168565  9476 solver.cpp:253]     Train net output #0: loss = 1.61199 (* 1 = 1.61199 loss)
I0521 03:02:06.168578  9476 sgd_solver.cpp:106] Iteration 2350, lr = 0.0025
I0521 03:02:13.991336  9476 solver.cpp:237] Iteration 2375, loss = 1.61125
I0521 03:02:13.991375  9476 solver.cpp:253]     Train net output #0: loss = 1.61125 (* 1 = 1.61125 loss)
I0521 03:02:13.991396  9476 sgd_solver.cpp:106] Iteration 2375, lr = 0.0025
I0521 03:02:21.813781  9476 solver.cpp:237] Iteration 2400, loss = 1.63832
I0521 03:02:21.813813  9476 solver.cpp:253]     Train net output #0: loss = 1.63832 (* 1 = 1.63832 loss)
I0521 03:02:21.813830  9476 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0521 03:02:51.835656  9476 solver.cpp:237] Iteration 2425, loss = 1.60476
I0521 03:02:51.835834  9476 solver.cpp:253]     Train net output #0: loss = 1.60476 (* 1 = 1.60476 loss)
I0521 03:02:51.835849  9476 sgd_solver.cpp:106] Iteration 2425, lr = 0.0025
I0521 03:02:59.657932  9476 solver.cpp:237] Iteration 2450, loss = 1.60658
I0521 03:02:59.657963  9476 solver.cpp:253]     Train net output #0: loss = 1.60658 (* 1 = 1.60658 loss)
I0521 03:02:59.657980  9476 sgd_solver.cpp:106] Iteration 2450, lr = 0.0025
I0521 03:03:07.483455  9476 solver.cpp:237] Iteration 2475, loss = 1.63132
I0521 03:03:07.483513  9476 solver.cpp:253]     Train net output #0: loss = 1.63132 (* 1 = 1.63132 loss)
I0521 03:03:07.483530  9476 sgd_solver.cpp:106] Iteration 2475, lr = 0.0025
I0521 03:03:15.304208  9476 solver.cpp:237] Iteration 2500, loss = 1.59735
I0521 03:03:15.304240  9476 solver.cpp:253]     Train net output #0: loss = 1.59735 (* 1 = 1.59735 loss)
I0521 03:03:15.304255  9476 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0521 03:03:23.129462  9476 solver.cpp:237] Iteration 2525, loss = 1.66188
I0521 03:03:23.129609  9476 solver.cpp:253]     Train net output #0: loss = 1.66188 (* 1 = 1.66188 loss)
I0521 03:03:23.129623  9476 sgd_solver.cpp:106] Iteration 2525, lr = 0.0025
I0521 03:03:30.953775  9476 solver.cpp:237] Iteration 2550, loss = 1.68173
I0521 03:03:30.953820  9476 solver.cpp:253]     Train net output #0: loss = 1.68173 (* 1 = 1.68173 loss)
I0521 03:03:30.953840  9476 sgd_solver.cpp:106] Iteration 2550, lr = 0.0025
I0521 03:03:38.778198  9476 solver.cpp:237] Iteration 2575, loss = 1.60572
I0521 03:03:38.778230  9476 solver.cpp:253]     Train net output #0: loss = 1.60572 (* 1 = 1.60572 loss)
I0521 03:03:38.778246  9476 sgd_solver.cpp:106] Iteration 2575, lr = 0.0025
I0521 03:03:40.030058  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2580.caffemodel
I0521 03:03:40.289840  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2580.solverstate
I0521 03:03:41.664499  9476 solver.cpp:341] Iteration 2585, Testing net (#0)
I0521 03:04:26.967859  9476 solver.cpp:409]     Test net output #0: accuracy = 0.688264
I0521 03:04:26.968029  9476 solver.cpp:409]     Test net output #1: loss = 1.07523 (* 1 = 1.07523 loss)
I0521 03:04:27.061070  9476 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2586.caffemodel
I0521 03:04:27.320011  9476 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664_iter_2586.solverstate
I0521 03:04:27.348227  9476 solver.cpp:326] Optimization Done.
I0521 03:04:27.348258  9476 caffe.cpp:215] Optimization Done.
Application 11236613 resources: utime ~1252s, stime ~227s, Rss ~5332904, inblocks ~3594475, outblocks ~194564
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_580_2016-05-20T11.20.53.689664.solver"
	User time (seconds): 0.54
	System time (seconds): 0.17
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:43.46
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15072
	Voluntary context switches: 3044
	Involuntary context switches: 205
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
