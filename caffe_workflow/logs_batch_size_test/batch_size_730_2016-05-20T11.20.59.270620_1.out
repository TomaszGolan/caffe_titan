2806308
I0521 05:47:30.058926 26727 caffe.cpp:184] Using GPUs 0
I0521 05:47:30.481489 26727 solver.cpp:48] Initializing solver from parameters: 
test_iter: 205
test_interval: 410
base_lr: 0.0025
display: 20
max_iter: 2054
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 205
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620.prototxt"
I0521 05:47:30.483039 26727 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620.prototxt
I0521 05:47:30.501922 26727 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 05:47:30.501982 26727 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 05:47:30.502326 26727 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 730
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 05:47:30.502502 26727 layer_factory.hpp:77] Creating layer data_hdf5
I0521 05:47:30.502526 26727 net.cpp:106] Creating Layer data_hdf5
I0521 05:47:30.502540 26727 net.cpp:411] data_hdf5 -> data
I0521 05:47:30.502573 26727 net.cpp:411] data_hdf5 -> label
I0521 05:47:30.502605 26727 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 05:47:30.503785 26727 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 05:47:30.505967 26727 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 05:47:52.024744 26727 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 05:47:52.029853 26727 net.cpp:150] Setting up data_hdf5
I0521 05:47:52.029892 26727 net.cpp:157] Top shape: 730 1 127 50 (4635500)
I0521 05:47:52.029907 26727 net.cpp:157] Top shape: 730 (730)
I0521 05:47:52.029922 26727 net.cpp:165] Memory required for data: 18544920
I0521 05:47:52.029934 26727 layer_factory.hpp:77] Creating layer conv1
I0521 05:47:52.029968 26727 net.cpp:106] Creating Layer conv1
I0521 05:47:52.029979 26727 net.cpp:454] conv1 <- data
I0521 05:47:52.030000 26727 net.cpp:411] conv1 -> conv1
I0521 05:47:52.397760 26727 net.cpp:150] Setting up conv1
I0521 05:47:52.397807 26727 net.cpp:157] Top shape: 730 12 120 48 (50457600)
I0521 05:47:52.397819 26727 net.cpp:165] Memory required for data: 220375320
I0521 05:47:52.397846 26727 layer_factory.hpp:77] Creating layer relu1
I0521 05:47:52.397867 26727 net.cpp:106] Creating Layer relu1
I0521 05:47:52.397878 26727 net.cpp:454] relu1 <- conv1
I0521 05:47:52.397891 26727 net.cpp:397] relu1 -> conv1 (in-place)
I0521 05:47:52.398406 26727 net.cpp:150] Setting up relu1
I0521 05:47:52.398422 26727 net.cpp:157] Top shape: 730 12 120 48 (50457600)
I0521 05:47:52.398433 26727 net.cpp:165] Memory required for data: 422205720
I0521 05:47:52.398444 26727 layer_factory.hpp:77] Creating layer pool1
I0521 05:47:52.398460 26727 net.cpp:106] Creating Layer pool1
I0521 05:47:52.398470 26727 net.cpp:454] pool1 <- conv1
I0521 05:47:52.398483 26727 net.cpp:411] pool1 -> pool1
I0521 05:47:52.398563 26727 net.cpp:150] Setting up pool1
I0521 05:47:52.398576 26727 net.cpp:157] Top shape: 730 12 60 48 (25228800)
I0521 05:47:52.398586 26727 net.cpp:165] Memory required for data: 523120920
I0521 05:47:52.398596 26727 layer_factory.hpp:77] Creating layer conv2
I0521 05:47:52.398619 26727 net.cpp:106] Creating Layer conv2
I0521 05:47:52.398629 26727 net.cpp:454] conv2 <- pool1
I0521 05:47:52.398643 26727 net.cpp:411] conv2 -> conv2
I0521 05:47:52.401324 26727 net.cpp:150] Setting up conv2
I0521 05:47:52.401352 26727 net.cpp:157] Top shape: 730 20 54 46 (36266400)
I0521 05:47:52.401363 26727 net.cpp:165] Memory required for data: 668186520
I0521 05:47:52.401382 26727 layer_factory.hpp:77] Creating layer relu2
I0521 05:47:52.401397 26727 net.cpp:106] Creating Layer relu2
I0521 05:47:52.401407 26727 net.cpp:454] relu2 <- conv2
I0521 05:47:52.401420 26727 net.cpp:397] relu2 -> conv2 (in-place)
I0521 05:47:52.401749 26727 net.cpp:150] Setting up relu2
I0521 05:47:52.401764 26727 net.cpp:157] Top shape: 730 20 54 46 (36266400)
I0521 05:47:52.401774 26727 net.cpp:165] Memory required for data: 813252120
I0521 05:47:52.401784 26727 layer_factory.hpp:77] Creating layer pool2
I0521 05:47:52.401796 26727 net.cpp:106] Creating Layer pool2
I0521 05:47:52.401806 26727 net.cpp:454] pool2 <- conv2
I0521 05:47:52.401831 26727 net.cpp:411] pool2 -> pool2
I0521 05:47:52.401901 26727 net.cpp:150] Setting up pool2
I0521 05:47:52.401914 26727 net.cpp:157] Top shape: 730 20 27 46 (18133200)
I0521 05:47:52.401924 26727 net.cpp:165] Memory required for data: 885784920
I0521 05:47:52.401932 26727 layer_factory.hpp:77] Creating layer conv3
I0521 05:47:52.401950 26727 net.cpp:106] Creating Layer conv3
I0521 05:47:52.401960 26727 net.cpp:454] conv3 <- pool2
I0521 05:47:52.401974 26727 net.cpp:411] conv3 -> conv3
I0521 05:47:52.403889 26727 net.cpp:150] Setting up conv3
I0521 05:47:52.403913 26727 net.cpp:157] Top shape: 730 28 22 44 (19785920)
I0521 05:47:52.403925 26727 net.cpp:165] Memory required for data: 964928600
I0521 05:47:52.403942 26727 layer_factory.hpp:77] Creating layer relu3
I0521 05:47:52.403959 26727 net.cpp:106] Creating Layer relu3
I0521 05:47:52.403969 26727 net.cpp:454] relu3 <- conv3
I0521 05:47:52.403981 26727 net.cpp:397] relu3 -> conv3 (in-place)
I0521 05:47:52.404450 26727 net.cpp:150] Setting up relu3
I0521 05:47:52.404469 26727 net.cpp:157] Top shape: 730 28 22 44 (19785920)
I0521 05:47:52.404479 26727 net.cpp:165] Memory required for data: 1044072280
I0521 05:47:52.404489 26727 layer_factory.hpp:77] Creating layer pool3
I0521 05:47:52.404501 26727 net.cpp:106] Creating Layer pool3
I0521 05:47:52.404511 26727 net.cpp:454] pool3 <- conv3
I0521 05:47:52.404523 26727 net.cpp:411] pool3 -> pool3
I0521 05:47:52.404592 26727 net.cpp:150] Setting up pool3
I0521 05:47:52.404604 26727 net.cpp:157] Top shape: 730 28 11 44 (9892960)
I0521 05:47:52.404614 26727 net.cpp:165] Memory required for data: 1083644120
I0521 05:47:52.404623 26727 layer_factory.hpp:77] Creating layer conv4
I0521 05:47:52.404639 26727 net.cpp:106] Creating Layer conv4
I0521 05:47:52.404649 26727 net.cpp:454] conv4 <- pool3
I0521 05:47:52.404664 26727 net.cpp:411] conv4 -> conv4
I0521 05:47:52.407461 26727 net.cpp:150] Setting up conv4
I0521 05:47:52.407485 26727 net.cpp:157] Top shape: 730 36 6 42 (6622560)
I0521 05:47:52.407495 26727 net.cpp:165] Memory required for data: 1110134360
I0521 05:47:52.407511 26727 layer_factory.hpp:77] Creating layer relu4
I0521 05:47:52.407526 26727 net.cpp:106] Creating Layer relu4
I0521 05:47:52.407536 26727 net.cpp:454] relu4 <- conv4
I0521 05:47:52.407548 26727 net.cpp:397] relu4 -> conv4 (in-place)
I0521 05:47:52.408018 26727 net.cpp:150] Setting up relu4
I0521 05:47:52.408035 26727 net.cpp:157] Top shape: 730 36 6 42 (6622560)
I0521 05:47:52.408046 26727 net.cpp:165] Memory required for data: 1136624600
I0521 05:47:52.408056 26727 layer_factory.hpp:77] Creating layer pool4
I0521 05:47:52.408069 26727 net.cpp:106] Creating Layer pool4
I0521 05:47:52.408079 26727 net.cpp:454] pool4 <- conv4
I0521 05:47:52.408092 26727 net.cpp:411] pool4 -> pool4
I0521 05:47:52.408160 26727 net.cpp:150] Setting up pool4
I0521 05:47:52.408174 26727 net.cpp:157] Top shape: 730 36 3 42 (3311280)
I0521 05:47:52.408185 26727 net.cpp:165] Memory required for data: 1149869720
I0521 05:47:52.408195 26727 layer_factory.hpp:77] Creating layer ip1
I0521 05:47:52.408215 26727 net.cpp:106] Creating Layer ip1
I0521 05:47:52.408226 26727 net.cpp:454] ip1 <- pool4
I0521 05:47:52.408238 26727 net.cpp:411] ip1 -> ip1
I0521 05:47:52.423683 26727 net.cpp:150] Setting up ip1
I0521 05:47:52.423712 26727 net.cpp:157] Top shape: 730 196 (143080)
I0521 05:47:52.423725 26727 net.cpp:165] Memory required for data: 1150442040
I0521 05:47:52.423748 26727 layer_factory.hpp:77] Creating layer relu5
I0521 05:47:52.423763 26727 net.cpp:106] Creating Layer relu5
I0521 05:47:52.423774 26727 net.cpp:454] relu5 <- ip1
I0521 05:47:52.423785 26727 net.cpp:397] relu5 -> ip1 (in-place)
I0521 05:47:52.424129 26727 net.cpp:150] Setting up relu5
I0521 05:47:52.424144 26727 net.cpp:157] Top shape: 730 196 (143080)
I0521 05:47:52.424154 26727 net.cpp:165] Memory required for data: 1151014360
I0521 05:47:52.424165 26727 layer_factory.hpp:77] Creating layer drop1
I0521 05:47:52.424187 26727 net.cpp:106] Creating Layer drop1
I0521 05:47:52.424197 26727 net.cpp:454] drop1 <- ip1
I0521 05:47:52.424222 26727 net.cpp:397] drop1 -> ip1 (in-place)
I0521 05:47:52.424269 26727 net.cpp:150] Setting up drop1
I0521 05:47:52.424283 26727 net.cpp:157] Top shape: 730 196 (143080)
I0521 05:47:52.424293 26727 net.cpp:165] Memory required for data: 1151586680
I0521 05:47:52.424304 26727 layer_factory.hpp:77] Creating layer ip2
I0521 05:47:52.424321 26727 net.cpp:106] Creating Layer ip2
I0521 05:47:52.424331 26727 net.cpp:454] ip2 <- ip1
I0521 05:47:52.424345 26727 net.cpp:411] ip2 -> ip2
I0521 05:47:52.424810 26727 net.cpp:150] Setting up ip2
I0521 05:47:52.424823 26727 net.cpp:157] Top shape: 730 98 (71540)
I0521 05:47:52.424834 26727 net.cpp:165] Memory required for data: 1151872840
I0521 05:47:52.424849 26727 layer_factory.hpp:77] Creating layer relu6
I0521 05:47:52.424861 26727 net.cpp:106] Creating Layer relu6
I0521 05:47:52.424871 26727 net.cpp:454] relu6 <- ip2
I0521 05:47:52.424883 26727 net.cpp:397] relu6 -> ip2 (in-place)
I0521 05:47:52.425403 26727 net.cpp:150] Setting up relu6
I0521 05:47:52.425420 26727 net.cpp:157] Top shape: 730 98 (71540)
I0521 05:47:52.425431 26727 net.cpp:165] Memory required for data: 1152159000
I0521 05:47:52.425441 26727 layer_factory.hpp:77] Creating layer drop2
I0521 05:47:52.425453 26727 net.cpp:106] Creating Layer drop2
I0521 05:47:52.425463 26727 net.cpp:454] drop2 <- ip2
I0521 05:47:52.425477 26727 net.cpp:397] drop2 -> ip2 (in-place)
I0521 05:47:52.425519 26727 net.cpp:150] Setting up drop2
I0521 05:47:52.425532 26727 net.cpp:157] Top shape: 730 98 (71540)
I0521 05:47:52.425542 26727 net.cpp:165] Memory required for data: 1152445160
I0521 05:47:52.425551 26727 layer_factory.hpp:77] Creating layer ip3
I0521 05:47:52.425565 26727 net.cpp:106] Creating Layer ip3
I0521 05:47:52.425575 26727 net.cpp:454] ip3 <- ip2
I0521 05:47:52.425587 26727 net.cpp:411] ip3 -> ip3
I0521 05:47:52.425797 26727 net.cpp:150] Setting up ip3
I0521 05:47:52.425811 26727 net.cpp:157] Top shape: 730 11 (8030)
I0521 05:47:52.425820 26727 net.cpp:165] Memory required for data: 1152477280
I0521 05:47:52.425835 26727 layer_factory.hpp:77] Creating layer drop3
I0521 05:47:52.425848 26727 net.cpp:106] Creating Layer drop3
I0521 05:47:52.425858 26727 net.cpp:454] drop3 <- ip3
I0521 05:47:52.425870 26727 net.cpp:397] drop3 -> ip3 (in-place)
I0521 05:47:52.425909 26727 net.cpp:150] Setting up drop3
I0521 05:47:52.425922 26727 net.cpp:157] Top shape: 730 11 (8030)
I0521 05:47:52.425932 26727 net.cpp:165] Memory required for data: 1152509400
I0521 05:47:52.425941 26727 layer_factory.hpp:77] Creating layer loss
I0521 05:47:52.425961 26727 net.cpp:106] Creating Layer loss
I0521 05:47:52.425971 26727 net.cpp:454] loss <- ip3
I0521 05:47:52.425982 26727 net.cpp:454] loss <- label
I0521 05:47:52.425994 26727 net.cpp:411] loss -> loss
I0521 05:47:52.426012 26727 layer_factory.hpp:77] Creating layer loss
I0521 05:47:52.426662 26727 net.cpp:150] Setting up loss
I0521 05:47:52.426682 26727 net.cpp:157] Top shape: (1)
I0521 05:47:52.426697 26727 net.cpp:160]     with loss weight 1
I0521 05:47:52.426741 26727 net.cpp:165] Memory required for data: 1152509404
I0521 05:47:52.426751 26727 net.cpp:226] loss needs backward computation.
I0521 05:47:52.426764 26727 net.cpp:226] drop3 needs backward computation.
I0521 05:47:52.426774 26727 net.cpp:226] ip3 needs backward computation.
I0521 05:47:52.426781 26727 net.cpp:226] drop2 needs backward computation.
I0521 05:47:52.426791 26727 net.cpp:226] relu6 needs backward computation.
I0521 05:47:52.426801 26727 net.cpp:226] ip2 needs backward computation.
I0521 05:47:52.426811 26727 net.cpp:226] drop1 needs backward computation.
I0521 05:47:52.426821 26727 net.cpp:226] relu5 needs backward computation.
I0521 05:47:52.426831 26727 net.cpp:226] ip1 needs backward computation.
I0521 05:47:52.426841 26727 net.cpp:226] pool4 needs backward computation.
I0521 05:47:52.426851 26727 net.cpp:226] relu4 needs backward computation.
I0521 05:47:52.426861 26727 net.cpp:226] conv4 needs backward computation.
I0521 05:47:52.426872 26727 net.cpp:226] pool3 needs backward computation.
I0521 05:47:52.426890 26727 net.cpp:226] relu3 needs backward computation.
I0521 05:47:52.426898 26727 net.cpp:226] conv3 needs backward computation.
I0521 05:47:52.426910 26727 net.cpp:226] pool2 needs backward computation.
I0521 05:47:52.426920 26727 net.cpp:226] relu2 needs backward computation.
I0521 05:47:52.426929 26727 net.cpp:226] conv2 needs backward computation.
I0521 05:47:52.426940 26727 net.cpp:226] pool1 needs backward computation.
I0521 05:47:52.426950 26727 net.cpp:226] relu1 needs backward computation.
I0521 05:47:52.426960 26727 net.cpp:226] conv1 needs backward computation.
I0521 05:47:52.426971 26727 net.cpp:228] data_hdf5 does not need backward computation.
I0521 05:47:52.426981 26727 net.cpp:270] This network produces output loss
I0521 05:47:52.427006 26727 net.cpp:283] Network initialization done.
I0521 05:47:52.428562 26727 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620.prototxt
I0521 05:47:52.428632 26727 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 05:47:52.428985 26727 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 730
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 05:47:52.429173 26727 layer_factory.hpp:77] Creating layer data_hdf5
I0521 05:47:52.429188 26727 net.cpp:106] Creating Layer data_hdf5
I0521 05:47:52.429200 26727 net.cpp:411] data_hdf5 -> data
I0521 05:47:52.429217 26727 net.cpp:411] data_hdf5 -> label
I0521 05:47:52.429244 26727 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 05:47:52.430413 26727 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 05:48:13.729810 26727 net.cpp:150] Setting up data_hdf5
I0521 05:48:13.729986 26727 net.cpp:157] Top shape: 730 1 127 50 (4635500)
I0521 05:48:13.730000 26727 net.cpp:157] Top shape: 730 (730)
I0521 05:48:13.730011 26727 net.cpp:165] Memory required for data: 18544920
I0521 05:48:13.730026 26727 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 05:48:13.730053 26727 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 05:48:13.730064 26727 net.cpp:454] label_data_hdf5_1_split <- label
I0521 05:48:13.730079 26727 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 05:48:13.730099 26727 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 05:48:13.730172 26727 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 05:48:13.730186 26727 net.cpp:157] Top shape: 730 (730)
I0521 05:48:13.730197 26727 net.cpp:157] Top shape: 730 (730)
I0521 05:48:13.730207 26727 net.cpp:165] Memory required for data: 18550760
I0521 05:48:13.730217 26727 layer_factory.hpp:77] Creating layer conv1
I0521 05:48:13.730239 26727 net.cpp:106] Creating Layer conv1
I0521 05:48:13.730250 26727 net.cpp:454] conv1 <- data
I0521 05:48:13.730265 26727 net.cpp:411] conv1 -> conv1
I0521 05:48:13.732172 26727 net.cpp:150] Setting up conv1
I0521 05:48:13.732195 26727 net.cpp:157] Top shape: 730 12 120 48 (50457600)
I0521 05:48:13.732208 26727 net.cpp:165] Memory required for data: 220381160
I0521 05:48:13.732228 26727 layer_factory.hpp:77] Creating layer relu1
I0521 05:48:13.732244 26727 net.cpp:106] Creating Layer relu1
I0521 05:48:13.732254 26727 net.cpp:454] relu1 <- conv1
I0521 05:48:13.732266 26727 net.cpp:397] relu1 -> conv1 (in-place)
I0521 05:48:13.732760 26727 net.cpp:150] Setting up relu1
I0521 05:48:13.732776 26727 net.cpp:157] Top shape: 730 12 120 48 (50457600)
I0521 05:48:13.732786 26727 net.cpp:165] Memory required for data: 422211560
I0521 05:48:13.732797 26727 layer_factory.hpp:77] Creating layer pool1
I0521 05:48:13.732813 26727 net.cpp:106] Creating Layer pool1
I0521 05:48:13.732823 26727 net.cpp:454] pool1 <- conv1
I0521 05:48:13.732836 26727 net.cpp:411] pool1 -> pool1
I0521 05:48:13.732911 26727 net.cpp:150] Setting up pool1
I0521 05:48:13.732924 26727 net.cpp:157] Top shape: 730 12 60 48 (25228800)
I0521 05:48:13.732935 26727 net.cpp:165] Memory required for data: 523126760
I0521 05:48:13.732946 26727 layer_factory.hpp:77] Creating layer conv2
I0521 05:48:13.732964 26727 net.cpp:106] Creating Layer conv2
I0521 05:48:13.732975 26727 net.cpp:454] conv2 <- pool1
I0521 05:48:13.732988 26727 net.cpp:411] conv2 -> conv2
I0521 05:48:13.734916 26727 net.cpp:150] Setting up conv2
I0521 05:48:13.734940 26727 net.cpp:157] Top shape: 730 20 54 46 (36266400)
I0521 05:48:13.734949 26727 net.cpp:165] Memory required for data: 668192360
I0521 05:48:13.734969 26727 layer_factory.hpp:77] Creating layer relu2
I0521 05:48:13.734982 26727 net.cpp:106] Creating Layer relu2
I0521 05:48:13.734992 26727 net.cpp:454] relu2 <- conv2
I0521 05:48:13.735004 26727 net.cpp:397] relu2 -> conv2 (in-place)
I0521 05:48:13.735340 26727 net.cpp:150] Setting up relu2
I0521 05:48:13.735354 26727 net.cpp:157] Top shape: 730 20 54 46 (36266400)
I0521 05:48:13.735364 26727 net.cpp:165] Memory required for data: 813257960
I0521 05:48:13.735375 26727 layer_factory.hpp:77] Creating layer pool2
I0521 05:48:13.735388 26727 net.cpp:106] Creating Layer pool2
I0521 05:48:13.735399 26727 net.cpp:454] pool2 <- conv2
I0521 05:48:13.735410 26727 net.cpp:411] pool2 -> pool2
I0521 05:48:13.735482 26727 net.cpp:150] Setting up pool2
I0521 05:48:13.735496 26727 net.cpp:157] Top shape: 730 20 27 46 (18133200)
I0521 05:48:13.735507 26727 net.cpp:165] Memory required for data: 885790760
I0521 05:48:13.735517 26727 layer_factory.hpp:77] Creating layer conv3
I0521 05:48:13.735534 26727 net.cpp:106] Creating Layer conv3
I0521 05:48:13.735545 26727 net.cpp:454] conv3 <- pool2
I0521 05:48:13.735559 26727 net.cpp:411] conv3 -> conv3
I0521 05:48:13.737543 26727 net.cpp:150] Setting up conv3
I0521 05:48:13.737567 26727 net.cpp:157] Top shape: 730 28 22 44 (19785920)
I0521 05:48:13.737579 26727 net.cpp:165] Memory required for data: 964934440
I0521 05:48:13.737612 26727 layer_factory.hpp:77] Creating layer relu3
I0521 05:48:13.737625 26727 net.cpp:106] Creating Layer relu3
I0521 05:48:13.737637 26727 net.cpp:454] relu3 <- conv3
I0521 05:48:13.737649 26727 net.cpp:397] relu3 -> conv3 (in-place)
I0521 05:48:13.738119 26727 net.cpp:150] Setting up relu3
I0521 05:48:13.738135 26727 net.cpp:157] Top shape: 730 28 22 44 (19785920)
I0521 05:48:13.738147 26727 net.cpp:165] Memory required for data: 1044078120
I0521 05:48:13.738157 26727 layer_factory.hpp:77] Creating layer pool3
I0521 05:48:13.738169 26727 net.cpp:106] Creating Layer pool3
I0521 05:48:13.738179 26727 net.cpp:454] pool3 <- conv3
I0521 05:48:13.738193 26727 net.cpp:411] pool3 -> pool3
I0521 05:48:13.738265 26727 net.cpp:150] Setting up pool3
I0521 05:48:13.738277 26727 net.cpp:157] Top shape: 730 28 11 44 (9892960)
I0521 05:48:13.738287 26727 net.cpp:165] Memory required for data: 1083649960
I0521 05:48:13.738298 26727 layer_factory.hpp:77] Creating layer conv4
I0521 05:48:13.738314 26727 net.cpp:106] Creating Layer conv4
I0521 05:48:13.738325 26727 net.cpp:454] conv4 <- pool3
I0521 05:48:13.738339 26727 net.cpp:411] conv4 -> conv4
I0521 05:48:13.740393 26727 net.cpp:150] Setting up conv4
I0521 05:48:13.740417 26727 net.cpp:157] Top shape: 730 36 6 42 (6622560)
I0521 05:48:13.740428 26727 net.cpp:165] Memory required for data: 1110140200
I0521 05:48:13.740445 26727 layer_factory.hpp:77] Creating layer relu4
I0521 05:48:13.740458 26727 net.cpp:106] Creating Layer relu4
I0521 05:48:13.740468 26727 net.cpp:454] relu4 <- conv4
I0521 05:48:13.740481 26727 net.cpp:397] relu4 -> conv4 (in-place)
I0521 05:48:13.740950 26727 net.cpp:150] Setting up relu4
I0521 05:48:13.740967 26727 net.cpp:157] Top shape: 730 36 6 42 (6622560)
I0521 05:48:13.740978 26727 net.cpp:165] Memory required for data: 1136630440
I0521 05:48:13.740988 26727 layer_factory.hpp:77] Creating layer pool4
I0521 05:48:13.741000 26727 net.cpp:106] Creating Layer pool4
I0521 05:48:13.741010 26727 net.cpp:454] pool4 <- conv4
I0521 05:48:13.741024 26727 net.cpp:411] pool4 -> pool4
I0521 05:48:13.741094 26727 net.cpp:150] Setting up pool4
I0521 05:48:13.741108 26727 net.cpp:157] Top shape: 730 36 3 42 (3311280)
I0521 05:48:13.741118 26727 net.cpp:165] Memory required for data: 1149875560
I0521 05:48:13.741127 26727 layer_factory.hpp:77] Creating layer ip1
I0521 05:48:13.741142 26727 net.cpp:106] Creating Layer ip1
I0521 05:48:13.741152 26727 net.cpp:454] ip1 <- pool4
I0521 05:48:13.741165 26727 net.cpp:411] ip1 -> ip1
I0521 05:48:13.756634 26727 net.cpp:150] Setting up ip1
I0521 05:48:13.756662 26727 net.cpp:157] Top shape: 730 196 (143080)
I0521 05:48:13.756675 26727 net.cpp:165] Memory required for data: 1150447880
I0521 05:48:13.756696 26727 layer_factory.hpp:77] Creating layer relu5
I0521 05:48:13.756711 26727 net.cpp:106] Creating Layer relu5
I0521 05:48:13.756722 26727 net.cpp:454] relu5 <- ip1
I0521 05:48:13.756736 26727 net.cpp:397] relu5 -> ip1 (in-place)
I0521 05:48:13.757081 26727 net.cpp:150] Setting up relu5
I0521 05:48:13.757096 26727 net.cpp:157] Top shape: 730 196 (143080)
I0521 05:48:13.757105 26727 net.cpp:165] Memory required for data: 1151020200
I0521 05:48:13.757115 26727 layer_factory.hpp:77] Creating layer drop1
I0521 05:48:13.757134 26727 net.cpp:106] Creating Layer drop1
I0521 05:48:13.757144 26727 net.cpp:454] drop1 <- ip1
I0521 05:48:13.757158 26727 net.cpp:397] drop1 -> ip1 (in-place)
I0521 05:48:13.757202 26727 net.cpp:150] Setting up drop1
I0521 05:48:13.757215 26727 net.cpp:157] Top shape: 730 196 (143080)
I0521 05:48:13.757225 26727 net.cpp:165] Memory required for data: 1151592520
I0521 05:48:13.757241 26727 layer_factory.hpp:77] Creating layer ip2
I0521 05:48:13.757256 26727 net.cpp:106] Creating Layer ip2
I0521 05:48:13.757266 26727 net.cpp:454] ip2 <- ip1
I0521 05:48:13.757280 26727 net.cpp:411] ip2 -> ip2
I0521 05:48:13.757764 26727 net.cpp:150] Setting up ip2
I0521 05:48:13.757777 26727 net.cpp:157] Top shape: 730 98 (71540)
I0521 05:48:13.757787 26727 net.cpp:165] Memory required for data: 1151878680
I0521 05:48:13.757814 26727 layer_factory.hpp:77] Creating layer relu6
I0521 05:48:13.757828 26727 net.cpp:106] Creating Layer relu6
I0521 05:48:13.757838 26727 net.cpp:454] relu6 <- ip2
I0521 05:48:13.757851 26727 net.cpp:397] relu6 -> ip2 (in-place)
I0521 05:48:13.758376 26727 net.cpp:150] Setting up relu6
I0521 05:48:13.758394 26727 net.cpp:157] Top shape: 730 98 (71540)
I0521 05:48:13.758402 26727 net.cpp:165] Memory required for data: 1152164840
I0521 05:48:13.758412 26727 layer_factory.hpp:77] Creating layer drop2
I0521 05:48:13.758426 26727 net.cpp:106] Creating Layer drop2
I0521 05:48:13.758436 26727 net.cpp:454] drop2 <- ip2
I0521 05:48:13.758450 26727 net.cpp:397] drop2 -> ip2 (in-place)
I0521 05:48:13.758493 26727 net.cpp:150] Setting up drop2
I0521 05:48:13.758507 26727 net.cpp:157] Top shape: 730 98 (71540)
I0521 05:48:13.758517 26727 net.cpp:165] Memory required for data: 1152451000
I0521 05:48:13.758527 26727 layer_factory.hpp:77] Creating layer ip3
I0521 05:48:13.758541 26727 net.cpp:106] Creating Layer ip3
I0521 05:48:13.758551 26727 net.cpp:454] ip3 <- ip2
I0521 05:48:13.758565 26727 net.cpp:411] ip3 -> ip3
I0521 05:48:13.758787 26727 net.cpp:150] Setting up ip3
I0521 05:48:13.758800 26727 net.cpp:157] Top shape: 730 11 (8030)
I0521 05:48:13.758810 26727 net.cpp:165] Memory required for data: 1152483120
I0521 05:48:13.758826 26727 layer_factory.hpp:77] Creating layer drop3
I0521 05:48:13.758839 26727 net.cpp:106] Creating Layer drop3
I0521 05:48:13.758849 26727 net.cpp:454] drop3 <- ip3
I0521 05:48:13.758862 26727 net.cpp:397] drop3 -> ip3 (in-place)
I0521 05:48:13.758903 26727 net.cpp:150] Setting up drop3
I0521 05:48:13.758916 26727 net.cpp:157] Top shape: 730 11 (8030)
I0521 05:48:13.758926 26727 net.cpp:165] Memory required for data: 1152515240
I0521 05:48:13.758936 26727 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 05:48:13.758949 26727 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 05:48:13.758958 26727 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 05:48:13.758971 26727 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 05:48:13.758986 26727 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 05:48:13.759059 26727 net.cpp:150] Setting up ip3_drop3_0_split
I0521 05:48:13.759073 26727 net.cpp:157] Top shape: 730 11 (8030)
I0521 05:48:13.759085 26727 net.cpp:157] Top shape: 730 11 (8030)
I0521 05:48:13.759095 26727 net.cpp:165] Memory required for data: 1152579480
I0521 05:48:13.759105 26727 layer_factory.hpp:77] Creating layer accuracy
I0521 05:48:13.759126 26727 net.cpp:106] Creating Layer accuracy
I0521 05:48:13.759136 26727 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 05:48:13.759147 26727 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 05:48:13.759160 26727 net.cpp:411] accuracy -> accuracy
I0521 05:48:13.759184 26727 net.cpp:150] Setting up accuracy
I0521 05:48:13.759197 26727 net.cpp:157] Top shape: (1)
I0521 05:48:13.759207 26727 net.cpp:165] Memory required for data: 1152579484
I0521 05:48:13.759217 26727 layer_factory.hpp:77] Creating layer loss
I0521 05:48:13.759232 26727 net.cpp:106] Creating Layer loss
I0521 05:48:13.759241 26727 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 05:48:13.759253 26727 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 05:48:13.759265 26727 net.cpp:411] loss -> loss
I0521 05:48:13.759282 26727 layer_factory.hpp:77] Creating layer loss
I0521 05:48:13.759775 26727 net.cpp:150] Setting up loss
I0521 05:48:13.759789 26727 net.cpp:157] Top shape: (1)
I0521 05:48:13.759799 26727 net.cpp:160]     with loss weight 1
I0521 05:48:13.759817 26727 net.cpp:165] Memory required for data: 1152579488
I0521 05:48:13.759827 26727 net.cpp:226] loss needs backward computation.
I0521 05:48:13.759838 26727 net.cpp:228] accuracy does not need backward computation.
I0521 05:48:13.759850 26727 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 05:48:13.759860 26727 net.cpp:226] drop3 needs backward computation.
I0521 05:48:13.759867 26727 net.cpp:226] ip3 needs backward computation.
I0521 05:48:13.759878 26727 net.cpp:226] drop2 needs backward computation.
I0521 05:48:13.759897 26727 net.cpp:226] relu6 needs backward computation.
I0521 05:48:13.759907 26727 net.cpp:226] ip2 needs backward computation.
I0521 05:48:13.759917 26727 net.cpp:226] drop1 needs backward computation.
I0521 05:48:13.759927 26727 net.cpp:226] relu5 needs backward computation.
I0521 05:48:13.759937 26727 net.cpp:226] ip1 needs backward computation.
I0521 05:48:13.759946 26727 net.cpp:226] pool4 needs backward computation.
I0521 05:48:13.759956 26727 net.cpp:226] relu4 needs backward computation.
I0521 05:48:13.759966 26727 net.cpp:226] conv4 needs backward computation.
I0521 05:48:13.759976 26727 net.cpp:226] pool3 needs backward computation.
I0521 05:48:13.759986 26727 net.cpp:226] relu3 needs backward computation.
I0521 05:48:13.759996 26727 net.cpp:226] conv3 needs backward computation.
I0521 05:48:13.760006 26727 net.cpp:226] pool2 needs backward computation.
I0521 05:48:13.760017 26727 net.cpp:226] relu2 needs backward computation.
I0521 05:48:13.760027 26727 net.cpp:226] conv2 needs backward computation.
I0521 05:48:13.760037 26727 net.cpp:226] pool1 needs backward computation.
I0521 05:48:13.760048 26727 net.cpp:226] relu1 needs backward computation.
I0521 05:48:13.760058 26727 net.cpp:226] conv1 needs backward computation.
I0521 05:48:13.760069 26727 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 05:48:13.760081 26727 net.cpp:228] data_hdf5 does not need backward computation.
I0521 05:48:13.760090 26727 net.cpp:270] This network produces output accuracy
I0521 05:48:13.760100 26727 net.cpp:270] This network produces output loss
I0521 05:48:13.760128 26727 net.cpp:283] Network initialization done.
I0521 05:48:13.760262 26727 solver.cpp:60] Solver scaffolding done.
I0521 05:48:13.761410 26727 caffe.cpp:212] Starting Optimization
I0521 05:48:13.761428 26727 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 05:48:13.761442 26727 solver.cpp:289] Learning Rate Policy: fixed
I0521 05:48:13.762661 26727 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 05:48:59.705938 26727 solver.cpp:409]     Test net output #0: accuracy = 0.126542
I0521 05:48:59.706094 26727 solver.cpp:409]     Test net output #1: loss = 2.39852 (* 1 = 2.39852 loss)
I0521 05:48:59.842803 26727 solver.cpp:237] Iteration 0, loss = 2.39893
I0521 05:48:59.842841 26727 solver.cpp:253]     Train net output #0: loss = 2.39893 (* 1 = 2.39893 loss)
I0521 05:48:59.842859 26727 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 05:49:07.697609 26727 solver.cpp:237] Iteration 20, loss = 2.38853
I0521 05:49:07.697645 26727 solver.cpp:253]     Train net output #0: loss = 2.38853 (* 1 = 2.38853 loss)
I0521 05:49:07.697659 26727 sgd_solver.cpp:106] Iteration 20, lr = 0.0025
I0521 05:49:15.547819 26727 solver.cpp:237] Iteration 40, loss = 2.37305
I0521 05:49:15.547852 26727 solver.cpp:253]     Train net output #0: loss = 2.37305 (* 1 = 2.37305 loss)
I0521 05:49:15.547869 26727 sgd_solver.cpp:106] Iteration 40, lr = 0.0025
I0521 05:49:23.394875 26727 solver.cpp:237] Iteration 60, loss = 2.35868
I0521 05:49:23.394906 26727 solver.cpp:253]     Train net output #0: loss = 2.35868 (* 1 = 2.35868 loss)
I0521 05:49:23.394927 26727 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 05:49:31.243975 26727 solver.cpp:237] Iteration 80, loss = 2.34019
I0521 05:49:31.244122 26727 solver.cpp:253]     Train net output #0: loss = 2.34019 (* 1 = 2.34019 loss)
I0521 05:49:31.244135 26727 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0521 05:49:39.093302 26727 solver.cpp:237] Iteration 100, loss = 2.345
I0521 05:49:39.093333 26727 solver.cpp:253]     Train net output #0: loss = 2.345 (* 1 = 2.345 loss)
I0521 05:49:39.093351 26727 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0521 05:49:46.938573 26727 solver.cpp:237] Iteration 120, loss = 2.33506
I0521 05:49:46.938621 26727 solver.cpp:253]     Train net output #0: loss = 2.33506 (* 1 = 2.33506 loss)
I0521 05:49:46.938633 26727 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 05:50:16.882292 26727 solver.cpp:237] Iteration 140, loss = 2.32687
I0521 05:50:16.882453 26727 solver.cpp:253]     Train net output #0: loss = 2.32687 (* 1 = 2.32687 loss)
I0521 05:50:16.882467 26727 sgd_solver.cpp:106] Iteration 140, lr = 0.0025
I0521 05:50:24.737736 26727 solver.cpp:237] Iteration 160, loss = 2.33346
I0521 05:50:24.737768 26727 solver.cpp:253]     Train net output #0: loss = 2.33346 (* 1 = 2.33346 loss)
I0521 05:50:24.737785 26727 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 05:50:32.592818 26727 solver.cpp:237] Iteration 180, loss = 2.321
I0521 05:50:32.592862 26727 solver.cpp:253]     Train net output #0: loss = 2.321 (* 1 = 2.321 loss)
I0521 05:50:32.592878 26727 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 05:50:40.445868 26727 solver.cpp:237] Iteration 200, loss = 2.31605
I0521 05:50:40.445901 26727 solver.cpp:253]     Train net output #0: loss = 2.31605 (* 1 = 2.31605 loss)
I0521 05:50:40.445919 26727 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0521 05:50:42.016289 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_205.caffemodel
I0521 05:50:42.337586 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_205.solverstate
I0521 05:50:48.368386 26727 solver.cpp:237] Iteration 220, loss = 2.31101
I0521 05:50:48.368540 26727 solver.cpp:253]     Train net output #0: loss = 2.31101 (* 1 = 2.31101 loss)
I0521 05:50:48.368552 26727 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0521 05:50:56.223032 26727 solver.cpp:237] Iteration 240, loss = 2.31098
I0521 05:50:56.223065 26727 solver.cpp:253]     Train net output #0: loss = 2.31098 (* 1 = 2.31098 loss)
I0521 05:50:56.223081 26727 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 05:51:04.071871 26727 solver.cpp:237] Iteration 260, loss = 2.28366
I0521 05:51:04.071905 26727 solver.cpp:253]     Train net output #0: loss = 2.28366 (* 1 = 2.28366 loss)
I0521 05:51:04.071929 26727 sgd_solver.cpp:106] Iteration 260, lr = 0.0025
I0521 05:51:34.026661 26727 solver.cpp:237] Iteration 280, loss = 2.2476
I0521 05:51:34.026818 26727 solver.cpp:253]     Train net output #0: loss = 2.2476 (* 1 = 2.2476 loss)
I0521 05:51:34.026831 26727 sgd_solver.cpp:106] Iteration 280, lr = 0.0025
I0521 05:51:41.878140 26727 solver.cpp:237] Iteration 300, loss = 2.24992
I0521 05:51:41.878172 26727 solver.cpp:253]     Train net output #0: loss = 2.24992 (* 1 = 2.24992 loss)
I0521 05:51:41.878190 26727 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 05:51:49.735888 26727 solver.cpp:237] Iteration 320, loss = 2.24513
I0521 05:51:49.735939 26727 solver.cpp:253]     Train net output #0: loss = 2.24513 (* 1 = 2.24513 loss)
I0521 05:51:49.735951 26727 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 05:51:57.590098 26727 solver.cpp:237] Iteration 340, loss = 2.19073
I0521 05:51:57.590132 26727 solver.cpp:253]     Train net output #0: loss = 2.19073 (* 1 = 2.19073 loss)
I0521 05:51:57.590150 26727 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0521 05:52:05.448055 26727 solver.cpp:237] Iteration 360, loss = 2.17852
I0521 05:52:05.448199 26727 solver.cpp:253]     Train net output #0: loss = 2.17852 (* 1 = 2.17852 loss)
I0521 05:52:05.448213 26727 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 05:52:13.300524 26727 solver.cpp:237] Iteration 380, loss = 2.13473
I0521 05:52:13.300561 26727 solver.cpp:253]     Train net output #0: loss = 2.13473 (* 1 = 2.13473 loss)
I0521 05:52:13.300580 26727 sgd_solver.cpp:106] Iteration 380, lr = 0.0025
I0521 05:52:21.158952 26727 solver.cpp:237] Iteration 400, loss = 2.07905
I0521 05:52:21.158985 26727 solver.cpp:253]     Train net output #0: loss = 2.07905 (* 1 = 2.07905 loss)
I0521 05:52:21.159001 26727 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 05:52:24.694602 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_410.caffemodel
I0521 05:52:25.007863 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_410.solverstate
I0521 05:52:25.033653 26727 solver.cpp:341] Iteration 410, Testing net (#0)
I0521 05:53:10.023746 26727 solver.cpp:409]     Test net output #0: accuracy = 0.450939
I0521 05:53:10.023913 26727 solver.cpp:409]     Test net output #1: loss = 1.92201 (* 1 = 1.92201 loss)
I0521 05:53:36.238032 26727 solver.cpp:237] Iteration 420, loss = 2.07728
I0521 05:53:36.238082 26727 solver.cpp:253]     Train net output #0: loss = 2.07728 (* 1 = 2.07728 loss)
I0521 05:53:36.238098 26727 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 05:53:44.091120 26727 solver.cpp:237] Iteration 440, loss = 2.09517
I0521 05:53:44.091265 26727 solver.cpp:253]     Train net output #0: loss = 2.09517 (* 1 = 2.09517 loss)
I0521 05:53:44.091279 26727 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0521 05:53:51.947121 26727 solver.cpp:237] Iteration 460, loss = 2.03761
I0521 05:53:51.947165 26727 solver.cpp:253]     Train net output #0: loss = 2.03761 (* 1 = 2.03761 loss)
I0521 05:53:51.947180 26727 sgd_solver.cpp:106] Iteration 460, lr = 0.0025
I0521 05:53:59.803078 26727 solver.cpp:237] Iteration 480, loss = 2.01127
I0521 05:53:59.803113 26727 solver.cpp:253]     Train net output #0: loss = 2.01127 (* 1 = 2.01127 loss)
I0521 05:53:59.803129 26727 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 05:54:07.662513 26727 solver.cpp:237] Iteration 500, loss = 1.99595
I0521 05:54:07.662545 26727 solver.cpp:253]     Train net output #0: loss = 1.99595 (* 1 = 1.99595 loss)
I0521 05:54:07.662562 26727 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0521 05:54:15.510846 26727 solver.cpp:237] Iteration 520, loss = 2.00701
I0521 05:54:15.510982 26727 solver.cpp:253]     Train net output #0: loss = 2.00701 (* 1 = 2.00701 loss)
I0521 05:54:15.510996 26727 sgd_solver.cpp:106] Iteration 520, lr = 0.0025
I0521 05:54:23.364784 26727 solver.cpp:237] Iteration 540, loss = 2.01984
I0521 05:54:23.364830 26727 solver.cpp:253]     Train net output #0: loss = 2.01984 (* 1 = 2.01984 loss)
I0521 05:54:23.364843 26727 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 05:54:53.470728 26727 solver.cpp:237] Iteration 560, loss = 1.93701
I0521 05:54:53.470896 26727 solver.cpp:253]     Train net output #0: loss = 1.93701 (* 1 = 1.93701 loss)
I0521 05:54:53.470911 26727 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 05:55:01.324678 26727 solver.cpp:237] Iteration 580, loss = 1.9567
I0521 05:55:01.324712 26727 solver.cpp:253]     Train net output #0: loss = 1.9567 (* 1 = 1.9567 loss)
I0521 05:55:01.324729 26727 sgd_solver.cpp:106] Iteration 580, lr = 0.0025
I0521 05:55:09.175092 26727 solver.cpp:237] Iteration 600, loss = 1.88917
I0521 05:55:09.175132 26727 solver.cpp:253]     Train net output #0: loss = 1.88917 (* 1 = 1.88917 loss)
I0521 05:55:09.175148 26727 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 05:55:14.668148 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_615.caffemodel
I0521 05:55:14.984647 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_615.solverstate
I0521 05:55:17.093349 26727 solver.cpp:237] Iteration 620, loss = 1.93202
I0521 05:55:17.093399 26727 solver.cpp:253]     Train net output #0: loss = 1.93202 (* 1 = 1.93202 loss)
I0521 05:55:17.093412 26727 sgd_solver.cpp:106] Iteration 620, lr = 0.0025
I0521 05:55:24.952848 26727 solver.cpp:237] Iteration 640, loss = 1.88206
I0521 05:55:24.953001 26727 solver.cpp:253]     Train net output #0: loss = 1.88206 (* 1 = 1.88206 loss)
I0521 05:55:24.953014 26727 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 05:55:32.810537 26727 solver.cpp:237] Iteration 660, loss = 1.8774
I0521 05:55:32.810580 26727 solver.cpp:253]     Train net output #0: loss = 1.8774 (* 1 = 1.8774 loss)
I0521 05:55:32.810597 26727 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 05:55:40.666018 26727 solver.cpp:237] Iteration 680, loss = 1.88636
I0521 05:55:40.666052 26727 solver.cpp:253]     Train net output #0: loss = 1.88636 (* 1 = 1.88636 loss)
I0521 05:55:40.666069 26727 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0521 05:56:10.680594 26727 solver.cpp:237] Iteration 700, loss = 1.89717
I0521 05:56:10.680757 26727 solver.cpp:253]     Train net output #0: loss = 1.89717 (* 1 = 1.89717 loss)
I0521 05:56:10.680771 26727 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0521 05:56:18.535938 26727 solver.cpp:237] Iteration 720, loss = 1.8715
I0521 05:56:18.535971 26727 solver.cpp:253]     Train net output #0: loss = 1.8715 (* 1 = 1.8715 loss)
I0521 05:56:18.535989 26727 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 05:56:26.388720 26727 solver.cpp:237] Iteration 740, loss = 1.8471
I0521 05:56:26.388756 26727 solver.cpp:253]     Train net output #0: loss = 1.8471 (* 1 = 1.8471 loss)
I0521 05:56:26.388773 26727 sgd_solver.cpp:106] Iteration 740, lr = 0.0025
I0521 05:56:34.242054 26727 solver.cpp:237] Iteration 760, loss = 1.84626
I0521 05:56:34.242086 26727 solver.cpp:253]     Train net output #0: loss = 1.84626 (* 1 = 1.84626 loss)
I0521 05:56:34.242103 26727 sgd_solver.cpp:106] Iteration 760, lr = 0.0025
I0521 05:56:42.098428 26727 solver.cpp:237] Iteration 780, loss = 1.9254
I0521 05:56:42.098562 26727 solver.cpp:253]     Train net output #0: loss = 1.9254 (* 1 = 1.9254 loss)
I0521 05:56:42.098575 26727 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 05:56:49.955134 26727 solver.cpp:237] Iteration 800, loss = 1.83125
I0521 05:56:49.955173 26727 solver.cpp:253]     Train net output #0: loss = 1.83125 (* 1 = 1.83125 loss)
I0521 05:56:49.955196 26727 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 05:56:57.419232 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_820.caffemodel
I0521 05:56:57.735996 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_820.solverstate
I0521 05:56:57.763468 26727 solver.cpp:341] Iteration 820, Testing net (#0)
I0521 05:58:03.604107 26727 solver.cpp:409]     Test net output #0: accuracy = 0.596218
I0521 05:58:03.604279 26727 solver.cpp:409]     Test net output #1: loss = 1.41932 (* 1 = 1.41932 loss)
I0521 05:58:03.720765 26727 solver.cpp:237] Iteration 820, loss = 1.79569
I0521 05:58:03.720793 26727 solver.cpp:253]     Train net output #0: loss = 1.79569 (* 1 = 1.79569 loss)
I0521 05:58:03.720811 26727 sgd_solver.cpp:106] Iteration 820, lr = 0.0025
I0521 05:58:33.748955 26727 solver.cpp:237] Iteration 840, loss = 1.81565
I0521 05:58:33.749119 26727 solver.cpp:253]     Train net output #0: loss = 1.81565 (* 1 = 1.81565 loss)
I0521 05:58:33.749135 26727 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 05:58:41.595130 26727 solver.cpp:237] Iteration 860, loss = 1.97214
I0521 05:58:41.595163 26727 solver.cpp:253]     Train net output #0: loss = 1.97214 (* 1 = 1.97214 loss)
I0521 05:58:41.595180 26727 sgd_solver.cpp:106] Iteration 860, lr = 0.0025
I0521 05:58:49.434629 26727 solver.cpp:237] Iteration 880, loss = 1.78429
I0521 05:58:49.434662 26727 solver.cpp:253]     Train net output #0: loss = 1.78429 (* 1 = 1.78429 loss)
I0521 05:58:49.434679 26727 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 05:58:57.268781 26727 solver.cpp:237] Iteration 900, loss = 1.82408
I0521 05:58:57.268821 26727 solver.cpp:253]     Train net output #0: loss = 1.82408 (* 1 = 1.82408 loss)
I0521 05:58:57.268836 26727 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 05:59:05.104936 26727 solver.cpp:237] Iteration 920, loss = 1.81155
I0521 05:59:05.105070 26727 solver.cpp:253]     Train net output #0: loss = 1.81155 (* 1 = 1.81155 loss)
I0521 05:59:05.105083 26727 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0521 05:59:12.946563 26727 solver.cpp:237] Iteration 940, loss = 1.86267
I0521 05:59:12.946596 26727 solver.cpp:253]     Train net output #0: loss = 1.86267 (* 1 = 1.86267 loss)
I0521 05:59:12.946614 26727 sgd_solver.cpp:106] Iteration 940, lr = 0.0025
I0521 05:59:42.959895 26727 solver.cpp:237] Iteration 960, loss = 1.73904
I0521 05:59:42.960059 26727 solver.cpp:253]     Train net output #0: loss = 1.73904 (* 1 = 1.73904 loss)
I0521 05:59:42.960073 26727 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 05:59:50.807247 26727 solver.cpp:237] Iteration 980, loss = 1.76164
I0521 05:59:50.807279 26727 solver.cpp:253]     Train net output #0: loss = 1.76164 (* 1 = 1.76164 loss)
I0521 05:59:50.807296 26727 sgd_solver.cpp:106] Iteration 980, lr = 0.0025
I0521 05:59:58.648084 26727 solver.cpp:237] Iteration 1000, loss = 1.85767
I0521 05:59:58.648118 26727 solver.cpp:253]     Train net output #0: loss = 1.85767 (* 1 = 1.85767 loss)
I0521 05:59:58.648134 26727 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0521 06:00:06.490353 26727 solver.cpp:237] Iteration 1020, loss = 1.82721
I0521 06:00:06.490392 26727 solver.cpp:253]     Train net output #0: loss = 1.82721 (* 1 = 1.82721 loss)
I0521 06:00:06.490412 26727 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 06:00:08.058687 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1025.caffemodel
I0521 06:00:08.374337 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1025.solverstate
I0521 06:00:14.397470 26727 solver.cpp:237] Iteration 1040, loss = 1.7588
I0521 06:00:14.397629 26727 solver.cpp:253]     Train net output #0: loss = 1.7588 (* 1 = 1.7588 loss)
I0521 06:00:14.397644 26727 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 06:00:22.242658 26727 solver.cpp:237] Iteration 1060, loss = 1.89486
I0521 06:00:22.242691 26727 solver.cpp:253]     Train net output #0: loss = 1.89486 (* 1 = 1.89486 loss)
I0521 06:00:22.242708 26727 sgd_solver.cpp:106] Iteration 1060, lr = 0.0025
I0521 06:00:30.084652 26727 solver.cpp:237] Iteration 1080, loss = 1.77215
I0521 06:00:30.084686 26727 solver.cpp:253]     Train net output #0: loss = 1.77215 (* 1 = 1.77215 loss)
I0521 06:00:30.084698 26727 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 06:01:00.121747 26727 solver.cpp:237] Iteration 1100, loss = 1.78053
I0521 06:01:00.121924 26727 solver.cpp:253]     Train net output #0: loss = 1.78053 (* 1 = 1.78053 loss)
I0521 06:01:00.121940 26727 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 06:01:07.967937 26727 solver.cpp:237] Iteration 1120, loss = 1.79297
I0521 06:01:07.967970 26727 solver.cpp:253]     Train net output #0: loss = 1.79297 (* 1 = 1.79297 loss)
I0521 06:01:07.967988 26727 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 06:01:15.807030 26727 solver.cpp:237] Iteration 1140, loss = 1.88585
I0521 06:01:15.807065 26727 solver.cpp:253]     Train net output #0: loss = 1.88585 (* 1 = 1.88585 loss)
I0521 06:01:15.807081 26727 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 06:01:23.649384 26727 solver.cpp:237] Iteration 1160, loss = 1.79677
I0521 06:01:23.649431 26727 solver.cpp:253]     Train net output #0: loss = 1.79677 (* 1 = 1.79677 loss)
I0521 06:01:23.649446 26727 sgd_solver.cpp:106] Iteration 1160, lr = 0.0025
I0521 06:01:31.491781 26727 solver.cpp:237] Iteration 1180, loss = 1.75848
I0521 06:01:31.491922 26727 solver.cpp:253]     Train net output #0: loss = 1.75848 (* 1 = 1.75848 loss)
I0521 06:01:31.491935 26727 sgd_solver.cpp:106] Iteration 1180, lr = 0.0025
I0521 06:01:39.331653 26727 solver.cpp:237] Iteration 1200, loss = 1.73959
I0521 06:01:39.331686 26727 solver.cpp:253]     Train net output #0: loss = 1.73959 (* 1 = 1.73959 loss)
I0521 06:01:39.331703 26727 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 06:01:47.179797 26727 solver.cpp:237] Iteration 1220, loss = 1.79604
I0521 06:01:47.179834 26727 solver.cpp:253]     Train net output #0: loss = 1.79604 (* 1 = 1.79604 loss)
I0521 06:01:47.179852 26727 sgd_solver.cpp:106] Iteration 1220, lr = 0.0025
I0521 06:01:50.708221 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1230.caffemodel
I0521 06:01:51.020790 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1230.solverstate
I0521 06:01:51.046937 26727 solver.cpp:341] Iteration 1230, Testing net (#0)
I0521 06:02:35.741367 26727 solver.cpp:409]     Test net output #0: accuracy = 0.654059
I0521 06:02:35.741534 26727 solver.cpp:409]     Test net output #1: loss = 1.26362 (* 1 = 1.26362 loss)
I0521 06:03:01.970384 26727 solver.cpp:237] Iteration 1240, loss = 1.72515
I0521 06:03:01.970437 26727 solver.cpp:253]     Train net output #0: loss = 1.72515 (* 1 = 1.72515 loss)
I0521 06:03:01.970453 26727 sgd_solver.cpp:106] Iteration 1240, lr = 0.0025
I0521 06:03:09.821094 26727 solver.cpp:237] Iteration 1260, loss = 1.79017
I0521 06:03:09.821244 26727 solver.cpp:253]     Train net output #0: loss = 1.79017 (* 1 = 1.79017 loss)
I0521 06:03:09.821256 26727 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 06:03:17.670904 26727 solver.cpp:237] Iteration 1280, loss = 1.69256
I0521 06:03:17.670938 26727 solver.cpp:253]     Train net output #0: loss = 1.69256 (* 1 = 1.69256 loss)
I0521 06:03:17.670955 26727 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 06:03:25.519419 26727 solver.cpp:237] Iteration 1300, loss = 1.71297
I0521 06:03:25.519453 26727 solver.cpp:253]     Train net output #0: loss = 1.71297 (* 1 = 1.71297 loss)
I0521 06:03:25.519469 26727 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0521 06:03:33.368696 26727 solver.cpp:237] Iteration 1320, loss = 1.66617
I0521 06:03:33.368732 26727 solver.cpp:253]     Train net output #0: loss = 1.66617 (* 1 = 1.66617 loss)
I0521 06:03:33.368753 26727 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 06:03:41.212353 26727 solver.cpp:237] Iteration 1340, loss = 1.84258
I0521 06:03:41.212507 26727 solver.cpp:253]     Train net output #0: loss = 1.84258 (* 1 = 1.84258 loss)
I0521 06:03:41.212520 26727 sgd_solver.cpp:106] Iteration 1340, lr = 0.0025
I0521 06:03:49.063308 26727 solver.cpp:237] Iteration 1360, loss = 1.72799
I0521 06:03:49.063340 26727 solver.cpp:253]     Train net output #0: loss = 1.72799 (* 1 = 1.72799 loss)
I0521 06:03:49.063359 26727 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 06:04:19.087610 26727 solver.cpp:237] Iteration 1380, loss = 1.78609
I0521 06:04:19.087770 26727 solver.cpp:253]     Train net output #0: loss = 1.78609 (* 1 = 1.78609 loss)
I0521 06:04:19.087785 26727 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 06:04:26.941941 26727 solver.cpp:237] Iteration 1400, loss = 1.71042
I0521 06:04:26.941973 26727 solver.cpp:253]     Train net output #0: loss = 1.71042 (* 1 = 1.71042 loss)
I0521 06:04:26.941990 26727 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0521 06:04:34.791163 26727 solver.cpp:237] Iteration 1420, loss = 1.67122
I0521 06:04:34.791196 26727 solver.cpp:253]     Train net output #0: loss = 1.67122 (* 1 = 1.67122 loss)
I0521 06:04:34.791213 26727 sgd_solver.cpp:106] Iteration 1420, lr = 0.0025
I0521 06:04:40.286116 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1435.caffemodel
I0521 06:04:40.600442 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1435.solverstate
I0521 06:04:42.706698 26727 solver.cpp:237] Iteration 1440, loss = 1.7179
I0521 06:04:42.706746 26727 solver.cpp:253]     Train net output #0: loss = 1.7179 (* 1 = 1.7179 loss)
I0521 06:04:42.706763 26727 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 06:04:50.545704 26727 solver.cpp:237] Iteration 1460, loss = 1.70752
I0521 06:04:50.545850 26727 solver.cpp:253]     Train net output #0: loss = 1.70752 (* 1 = 1.70752 loss)
I0521 06:04:50.545863 26727 sgd_solver.cpp:106] Iteration 1460, lr = 0.0025
I0521 06:04:58.391650 26727 solver.cpp:237] Iteration 1480, loss = 1.69193
I0521 06:04:58.391683 26727 solver.cpp:253]     Train net output #0: loss = 1.69193 (* 1 = 1.69193 loss)
I0521 06:04:58.391698 26727 sgd_solver.cpp:106] Iteration 1480, lr = 0.0025
I0521 06:05:06.240331 26727 solver.cpp:237] Iteration 1500, loss = 1.80063
I0521 06:05:06.240365 26727 solver.cpp:253]     Train net output #0: loss = 1.80063 (* 1 = 1.80063 loss)
I0521 06:05:06.240377 26727 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 06:05:36.315405 26727 solver.cpp:237] Iteration 1520, loss = 1.66931
I0521 06:05:36.315584 26727 solver.cpp:253]     Train net output #0: loss = 1.66931 (* 1 = 1.66931 loss)
I0521 06:05:36.315600 26727 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 06:05:44.162632 26727 solver.cpp:237] Iteration 1540, loss = 1.64477
I0521 06:05:44.162667 26727 solver.cpp:253]     Train net output #0: loss = 1.64477 (* 1 = 1.64477 loss)
I0521 06:05:44.162683 26727 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 06:05:52.009874 26727 solver.cpp:237] Iteration 1560, loss = 1.65024
I0521 06:05:52.009908 26727 solver.cpp:253]     Train net output #0: loss = 1.65024 (* 1 = 1.65024 loss)
I0521 06:05:52.009924 26727 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 06:05:59.853914 26727 solver.cpp:237] Iteration 1580, loss = 1.68721
I0521 06:05:59.853951 26727 solver.cpp:253]     Train net output #0: loss = 1.68721 (* 1 = 1.68721 loss)
I0521 06:05:59.853972 26727 sgd_solver.cpp:106] Iteration 1580, lr = 0.0025
I0521 06:06:07.699841 26727 solver.cpp:237] Iteration 1600, loss = 1.7017
I0521 06:06:07.699978 26727 solver.cpp:253]     Train net output #0: loss = 1.7017 (* 1 = 1.7017 loss)
I0521 06:06:07.699992 26727 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 06:06:15.544672 26727 solver.cpp:237] Iteration 1620, loss = 1.73063
I0521 06:06:15.544704 26727 solver.cpp:253]     Train net output #0: loss = 1.73063 (* 1 = 1.73063 loss)
I0521 06:06:15.544723 26727 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 06:06:23.003407 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1640.caffemodel
I0521 06:06:23.317078 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1640.solverstate
I0521 06:06:23.342943 26727 solver.cpp:341] Iteration 1640, Testing net (#0)
I0521 06:07:29.231966 26727 solver.cpp:409]     Test net output #0: accuracy = 0.66137
I0521 06:07:29.232139 26727 solver.cpp:409]     Test net output #1: loss = 1.15841 (* 1 = 1.15841 loss)
I0521 06:07:29.348549 26727 solver.cpp:237] Iteration 1640, loss = 1.66816
I0521 06:07:29.348577 26727 solver.cpp:253]     Train net output #0: loss = 1.66816 (* 1 = 1.66816 loss)
I0521 06:07:29.348594 26727 sgd_solver.cpp:106] Iteration 1640, lr = 0.0025
I0521 06:07:59.369670 26727 solver.cpp:237] Iteration 1660, loss = 1.67639
I0521 06:07:59.369842 26727 solver.cpp:253]     Train net output #0: loss = 1.67639 (* 1 = 1.67639 loss)
I0521 06:07:59.369856 26727 sgd_solver.cpp:106] Iteration 1660, lr = 0.0025
I0521 06:08:07.219754 26727 solver.cpp:237] Iteration 1680, loss = 1.69607
I0521 06:08:07.219791 26727 solver.cpp:253]     Train net output #0: loss = 1.69607 (* 1 = 1.69607 loss)
I0521 06:08:07.219812 26727 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 06:08:15.064927 26727 solver.cpp:237] Iteration 1700, loss = 1.71165
I0521 06:08:15.064960 26727 solver.cpp:253]     Train net output #0: loss = 1.71165 (* 1 = 1.71165 loss)
I0521 06:08:15.064976 26727 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 06:08:22.914001 26727 solver.cpp:237] Iteration 1720, loss = 1.6308
I0521 06:08:22.914036 26727 solver.cpp:253]     Train net output #0: loss = 1.6308 (* 1 = 1.6308 loss)
I0521 06:08:22.914052 26727 sgd_solver.cpp:106] Iteration 1720, lr = 0.0025
I0521 06:08:30.758838 26727 solver.cpp:237] Iteration 1740, loss = 1.64114
I0521 06:08:30.758994 26727 solver.cpp:253]     Train net output #0: loss = 1.64114 (* 1 = 1.64114 loss)
I0521 06:08:30.759008 26727 sgd_solver.cpp:106] Iteration 1740, lr = 0.0025
I0521 06:08:38.603569 26727 solver.cpp:237] Iteration 1760, loss = 1.69461
I0521 06:08:38.603600 26727 solver.cpp:253]     Train net output #0: loss = 1.69461 (* 1 = 1.69461 loss)
I0521 06:08:38.603618 26727 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0521 06:09:08.632951 26727 solver.cpp:237] Iteration 1780, loss = 1.72419
I0521 06:09:08.633124 26727 solver.cpp:253]     Train net output #0: loss = 1.72419 (* 1 = 1.72419 loss)
I0521 06:09:08.633139 26727 sgd_solver.cpp:106] Iteration 1780, lr = 0.0025
I0521 06:09:16.479473 26727 solver.cpp:237] Iteration 1800, loss = 1.65824
I0521 06:09:16.479518 26727 solver.cpp:253]     Train net output #0: loss = 1.65824 (* 1 = 1.65824 loss)
I0521 06:09:16.479537 26727 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 06:09:24.322875 26727 solver.cpp:237] Iteration 1820, loss = 1.72641
I0521 06:09:24.322909 26727 solver.cpp:253]     Train net output #0: loss = 1.72641 (* 1 = 1.72641 loss)
I0521 06:09:24.322926 26727 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0521 06:09:32.166123 26727 solver.cpp:237] Iteration 1840, loss = 1.69466
I0521 06:09:32.166157 26727 solver.cpp:253]     Train net output #0: loss = 1.69466 (* 1 = 1.69466 loss)
I0521 06:09:32.166170 26727 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0521 06:09:33.734416 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1845.caffemodel
I0521 06:09:34.050074 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_1845.solverstate
I0521 06:09:40.085467 26727 solver.cpp:237] Iteration 1860, loss = 1.69762
I0521 06:09:40.085636 26727 solver.cpp:253]     Train net output #0: loss = 1.69762 (* 1 = 1.69762 loss)
I0521 06:09:40.085650 26727 sgd_solver.cpp:106] Iteration 1860, lr = 0.0025
I0521 06:09:47.930119 26727 solver.cpp:237] Iteration 1880, loss = 1.68047
I0521 06:09:47.930163 26727 solver.cpp:253]     Train net output #0: loss = 1.68047 (* 1 = 1.68047 loss)
I0521 06:09:47.930181 26727 sgd_solver.cpp:106] Iteration 1880, lr = 0.0025
I0521 06:09:55.779737 26727 solver.cpp:237] Iteration 1900, loss = 1.74597
I0521 06:09:55.779770 26727 solver.cpp:253]     Train net output #0: loss = 1.74597 (* 1 = 1.74597 loss)
I0521 06:09:55.779786 26727 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0521 06:10:25.787838 26727 solver.cpp:237] Iteration 1920, loss = 1.73964
I0521 06:10:25.788012 26727 solver.cpp:253]     Train net output #0: loss = 1.73964 (* 1 = 1.73964 loss)
I0521 06:10:25.788025 26727 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0521 06:10:33.639375 26727 solver.cpp:237] Iteration 1940, loss = 1.63841
I0521 06:10:33.639415 26727 solver.cpp:253]     Train net output #0: loss = 1.63841 (* 1 = 1.63841 loss)
I0521 06:10:33.639436 26727 sgd_solver.cpp:106] Iteration 1940, lr = 0.0025
I0521 06:10:41.486904 26727 solver.cpp:237] Iteration 1960, loss = 1.62865
I0521 06:10:41.486938 26727 solver.cpp:253]     Train net output #0: loss = 1.62865 (* 1 = 1.62865 loss)
I0521 06:10:41.486954 26727 sgd_solver.cpp:106] Iteration 1960, lr = 0.0025
I0521 06:10:49.339114 26727 solver.cpp:237] Iteration 1980, loss = 1.66791
I0521 06:10:49.339148 26727 solver.cpp:253]     Train net output #0: loss = 1.66791 (* 1 = 1.66791 loss)
I0521 06:10:49.339165 26727 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 06:10:57.186813 26727 solver.cpp:237] Iteration 2000, loss = 1.70513
I0521 06:10:57.186959 26727 solver.cpp:253]     Train net output #0: loss = 1.70513 (* 1 = 1.70513 loss)
I0521 06:10:57.186975 26727 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0521 06:11:05.034719 26727 solver.cpp:237] Iteration 2020, loss = 1.61102
I0521 06:11:05.034752 26727 solver.cpp:253]     Train net output #0: loss = 1.61102 (* 1 = 1.61102 loss)
I0521 06:11:05.034770 26727 sgd_solver.cpp:106] Iteration 2020, lr = 0.0025
I0521 06:11:12.883577 26727 solver.cpp:237] Iteration 2040, loss = 1.59698
I0521 06:11:12.883610 26727 solver.cpp:253]     Train net output #0: loss = 1.59698 (* 1 = 1.59698 loss)
I0521 06:11:12.883627 26727 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0521 06:11:16.417285 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_2050.caffemodel
I0521 06:11:16.733000 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_2050.solverstate
I0521 06:11:16.761334 26727 solver.cpp:341] Iteration 2050, Testing net (#0)
I0521 06:12:01.818651 26727 solver.cpp:409]     Test net output #0: accuracy = 0.661791
I0521 06:12:01.818815 26727 solver.cpp:409]     Test net output #1: loss = 1.18336 (* 1 = 1.18336 loss)
I0521 06:12:03.111750 26727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_2054.caffemodel
I0521 06:12:03.427048 26727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620_iter_2054.solverstate
I0521 06:12:03.455952 26727 solver.cpp:326] Optimization Done.
I0521 06:12:03.455979 26727 caffe.cpp:215] Optimization Done.
Application 11236983 resources: utime ~1249s, stime ~227s, Rss ~5329352, inblocks ~3594474, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_730_2016-05-20T11.20.59.270620.solver"
	User time (seconds): 0.57
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.15
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 2668
	Involuntary context switches: 127
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
