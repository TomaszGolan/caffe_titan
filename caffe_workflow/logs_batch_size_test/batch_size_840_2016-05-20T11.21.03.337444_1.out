2806356
I0521 08:15:07.459552 15047 caffe.cpp:184] Using GPUs 0
I0521 08:15:07.881737 15047 solver.cpp:48] Initializing solver from parameters: 
test_iter: 178
test_interval: 357
base_lr: 0.0025
display: 17
max_iter: 1785
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 178
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444.prototxt"
I0521 08:15:07.883404 15047 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444.prototxt
I0521 08:15:07.894413 15047 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 08:15:07.894474 15047 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 08:15:07.894817 15047 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 840
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 08:15:07.894994 15047 layer_factory.hpp:77] Creating layer data_hdf5
I0521 08:15:07.895018 15047 net.cpp:106] Creating Layer data_hdf5
I0521 08:15:07.895032 15047 net.cpp:411] data_hdf5 -> data
I0521 08:15:07.895066 15047 net.cpp:411] data_hdf5 -> label
I0521 08:15:07.895097 15047 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 08:15:07.896252 15047 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 08:15:07.898468 15047 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 08:15:29.495138 15047 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 08:15:29.500257 15047 net.cpp:150] Setting up data_hdf5
I0521 08:15:29.500298 15047 net.cpp:157] Top shape: 840 1 127 50 (5334000)
I0521 08:15:29.500311 15047 net.cpp:157] Top shape: 840 (840)
I0521 08:15:29.500324 15047 net.cpp:165] Memory required for data: 21339360
I0521 08:15:29.500337 15047 layer_factory.hpp:77] Creating layer conv1
I0521 08:15:29.500371 15047 net.cpp:106] Creating Layer conv1
I0521 08:15:29.500382 15047 net.cpp:454] conv1 <- data
I0521 08:15:29.500403 15047 net.cpp:411] conv1 -> conv1
I0521 08:15:29.862093 15047 net.cpp:150] Setting up conv1
I0521 08:15:29.862138 15047 net.cpp:157] Top shape: 840 12 120 48 (58060800)
I0521 08:15:29.862149 15047 net.cpp:165] Memory required for data: 253582560
I0521 08:15:29.862179 15047 layer_factory.hpp:77] Creating layer relu1
I0521 08:15:29.862202 15047 net.cpp:106] Creating Layer relu1
I0521 08:15:29.862213 15047 net.cpp:454] relu1 <- conv1
I0521 08:15:29.862226 15047 net.cpp:397] relu1 -> conv1 (in-place)
I0521 08:15:29.862737 15047 net.cpp:150] Setting up relu1
I0521 08:15:29.862754 15047 net.cpp:157] Top shape: 840 12 120 48 (58060800)
I0521 08:15:29.862766 15047 net.cpp:165] Memory required for data: 485825760
I0521 08:15:29.862776 15047 layer_factory.hpp:77] Creating layer pool1
I0521 08:15:29.862792 15047 net.cpp:106] Creating Layer pool1
I0521 08:15:29.862802 15047 net.cpp:454] pool1 <- conv1
I0521 08:15:29.862815 15047 net.cpp:411] pool1 -> pool1
I0521 08:15:29.862895 15047 net.cpp:150] Setting up pool1
I0521 08:15:29.862908 15047 net.cpp:157] Top shape: 840 12 60 48 (29030400)
I0521 08:15:29.862920 15047 net.cpp:165] Memory required for data: 601947360
I0521 08:15:29.862929 15047 layer_factory.hpp:77] Creating layer conv2
I0521 08:15:29.862952 15047 net.cpp:106] Creating Layer conv2
I0521 08:15:29.862962 15047 net.cpp:454] conv2 <- pool1
I0521 08:15:29.862975 15047 net.cpp:411] conv2 -> conv2
I0521 08:15:29.865650 15047 net.cpp:150] Setting up conv2
I0521 08:15:29.865679 15047 net.cpp:157] Top shape: 840 20 54 46 (41731200)
I0521 08:15:29.865689 15047 net.cpp:165] Memory required for data: 768872160
I0521 08:15:29.865708 15047 layer_factory.hpp:77] Creating layer relu2
I0521 08:15:29.865722 15047 net.cpp:106] Creating Layer relu2
I0521 08:15:29.865733 15047 net.cpp:454] relu2 <- conv2
I0521 08:15:29.865746 15047 net.cpp:397] relu2 -> conv2 (in-place)
I0521 08:15:29.866075 15047 net.cpp:150] Setting up relu2
I0521 08:15:29.866089 15047 net.cpp:157] Top shape: 840 20 54 46 (41731200)
I0521 08:15:29.866101 15047 net.cpp:165] Memory required for data: 935796960
I0521 08:15:29.866111 15047 layer_factory.hpp:77] Creating layer pool2
I0521 08:15:29.866124 15047 net.cpp:106] Creating Layer pool2
I0521 08:15:29.866134 15047 net.cpp:454] pool2 <- conv2
I0521 08:15:29.866159 15047 net.cpp:411] pool2 -> pool2
I0521 08:15:29.866226 15047 net.cpp:150] Setting up pool2
I0521 08:15:29.866240 15047 net.cpp:157] Top shape: 840 20 27 46 (20865600)
I0521 08:15:29.866250 15047 net.cpp:165] Memory required for data: 1019259360
I0521 08:15:29.866261 15047 layer_factory.hpp:77] Creating layer conv3
I0521 08:15:29.866278 15047 net.cpp:106] Creating Layer conv3
I0521 08:15:29.866289 15047 net.cpp:454] conv3 <- pool2
I0521 08:15:29.866302 15047 net.cpp:411] conv3 -> conv3
I0521 08:15:29.868207 15047 net.cpp:150] Setting up conv3
I0521 08:15:29.868232 15047 net.cpp:157] Top shape: 840 28 22 44 (22767360)
I0521 08:15:29.868243 15047 net.cpp:165] Memory required for data: 1110328800
I0521 08:15:29.868260 15047 layer_factory.hpp:77] Creating layer relu3
I0521 08:15:29.868278 15047 net.cpp:106] Creating Layer relu3
I0521 08:15:29.868288 15047 net.cpp:454] relu3 <- conv3
I0521 08:15:29.868299 15047 net.cpp:397] relu3 -> conv3 (in-place)
I0521 08:15:29.868780 15047 net.cpp:150] Setting up relu3
I0521 08:15:29.868798 15047 net.cpp:157] Top shape: 840 28 22 44 (22767360)
I0521 08:15:29.868808 15047 net.cpp:165] Memory required for data: 1201398240
I0521 08:15:29.868818 15047 layer_factory.hpp:77] Creating layer pool3
I0521 08:15:29.868830 15047 net.cpp:106] Creating Layer pool3
I0521 08:15:29.868840 15047 net.cpp:454] pool3 <- conv3
I0521 08:15:29.868854 15047 net.cpp:411] pool3 -> pool3
I0521 08:15:29.868922 15047 net.cpp:150] Setting up pool3
I0521 08:15:29.868935 15047 net.cpp:157] Top shape: 840 28 11 44 (11383680)
I0521 08:15:29.868945 15047 net.cpp:165] Memory required for data: 1246932960
I0521 08:15:29.868953 15047 layer_factory.hpp:77] Creating layer conv4
I0521 08:15:29.868971 15047 net.cpp:106] Creating Layer conv4
I0521 08:15:29.868983 15047 net.cpp:454] conv4 <- pool3
I0521 08:15:29.868995 15047 net.cpp:411] conv4 -> conv4
I0521 08:15:29.871727 15047 net.cpp:150] Setting up conv4
I0521 08:15:29.871754 15047 net.cpp:157] Top shape: 840 36 6 42 (7620480)
I0521 08:15:29.871765 15047 net.cpp:165] Memory required for data: 1277414880
I0521 08:15:29.871781 15047 layer_factory.hpp:77] Creating layer relu4
I0521 08:15:29.871795 15047 net.cpp:106] Creating Layer relu4
I0521 08:15:29.871805 15047 net.cpp:454] relu4 <- conv4
I0521 08:15:29.871819 15047 net.cpp:397] relu4 -> conv4 (in-place)
I0521 08:15:29.872284 15047 net.cpp:150] Setting up relu4
I0521 08:15:29.872301 15047 net.cpp:157] Top shape: 840 36 6 42 (7620480)
I0521 08:15:29.872311 15047 net.cpp:165] Memory required for data: 1307896800
I0521 08:15:29.872321 15047 layer_factory.hpp:77] Creating layer pool4
I0521 08:15:29.872334 15047 net.cpp:106] Creating Layer pool4
I0521 08:15:29.872344 15047 net.cpp:454] pool4 <- conv4
I0521 08:15:29.872357 15047 net.cpp:411] pool4 -> pool4
I0521 08:15:29.872426 15047 net.cpp:150] Setting up pool4
I0521 08:15:29.872438 15047 net.cpp:157] Top shape: 840 36 3 42 (3810240)
I0521 08:15:29.872449 15047 net.cpp:165] Memory required for data: 1323137760
I0521 08:15:29.872458 15047 layer_factory.hpp:77] Creating layer ip1
I0521 08:15:29.872480 15047 net.cpp:106] Creating Layer ip1
I0521 08:15:29.872490 15047 net.cpp:454] ip1 <- pool4
I0521 08:15:29.872504 15047 net.cpp:411] ip1 -> ip1
I0521 08:15:29.887929 15047 net.cpp:150] Setting up ip1
I0521 08:15:29.887959 15047 net.cpp:157] Top shape: 840 196 (164640)
I0521 08:15:29.887970 15047 net.cpp:165] Memory required for data: 1323796320
I0521 08:15:29.887994 15047 layer_factory.hpp:77] Creating layer relu5
I0521 08:15:29.888007 15047 net.cpp:106] Creating Layer relu5
I0521 08:15:29.888017 15047 net.cpp:454] relu5 <- ip1
I0521 08:15:29.888031 15047 net.cpp:397] relu5 -> ip1 (in-place)
I0521 08:15:29.888370 15047 net.cpp:150] Setting up relu5
I0521 08:15:29.888386 15047 net.cpp:157] Top shape: 840 196 (164640)
I0521 08:15:29.888396 15047 net.cpp:165] Memory required for data: 1324454880
I0521 08:15:29.888406 15047 layer_factory.hpp:77] Creating layer drop1
I0521 08:15:29.888427 15047 net.cpp:106] Creating Layer drop1
I0521 08:15:29.888437 15047 net.cpp:454] drop1 <- ip1
I0521 08:15:29.888463 15047 net.cpp:397] drop1 -> ip1 (in-place)
I0521 08:15:29.888510 15047 net.cpp:150] Setting up drop1
I0521 08:15:29.888523 15047 net.cpp:157] Top shape: 840 196 (164640)
I0521 08:15:29.888533 15047 net.cpp:165] Memory required for data: 1325113440
I0521 08:15:29.888550 15047 layer_factory.hpp:77] Creating layer ip2
I0521 08:15:29.888569 15047 net.cpp:106] Creating Layer ip2
I0521 08:15:29.888581 15047 net.cpp:454] ip2 <- ip1
I0521 08:15:29.888593 15047 net.cpp:411] ip2 -> ip2
I0521 08:15:29.889057 15047 net.cpp:150] Setting up ip2
I0521 08:15:29.889071 15047 net.cpp:157] Top shape: 840 98 (82320)
I0521 08:15:29.889081 15047 net.cpp:165] Memory required for data: 1325442720
I0521 08:15:29.889096 15047 layer_factory.hpp:77] Creating layer relu6
I0521 08:15:29.889108 15047 net.cpp:106] Creating Layer relu6
I0521 08:15:29.889118 15047 net.cpp:454] relu6 <- ip2
I0521 08:15:29.889130 15047 net.cpp:397] relu6 -> ip2 (in-place)
I0521 08:15:29.889652 15047 net.cpp:150] Setting up relu6
I0521 08:15:29.889668 15047 net.cpp:157] Top shape: 840 98 (82320)
I0521 08:15:29.889679 15047 net.cpp:165] Memory required for data: 1325772000
I0521 08:15:29.889689 15047 layer_factory.hpp:77] Creating layer drop2
I0521 08:15:29.889703 15047 net.cpp:106] Creating Layer drop2
I0521 08:15:29.889713 15047 net.cpp:454] drop2 <- ip2
I0521 08:15:29.889724 15047 net.cpp:397] drop2 -> ip2 (in-place)
I0521 08:15:29.889766 15047 net.cpp:150] Setting up drop2
I0521 08:15:29.889780 15047 net.cpp:157] Top shape: 840 98 (82320)
I0521 08:15:29.889789 15047 net.cpp:165] Memory required for data: 1326101280
I0521 08:15:29.889799 15047 layer_factory.hpp:77] Creating layer ip3
I0521 08:15:29.889813 15047 net.cpp:106] Creating Layer ip3
I0521 08:15:29.889823 15047 net.cpp:454] ip3 <- ip2
I0521 08:15:29.889835 15047 net.cpp:411] ip3 -> ip3
I0521 08:15:29.890048 15047 net.cpp:150] Setting up ip3
I0521 08:15:29.890060 15047 net.cpp:157] Top shape: 840 11 (9240)
I0521 08:15:29.890070 15047 net.cpp:165] Memory required for data: 1326138240
I0521 08:15:29.890085 15047 layer_factory.hpp:77] Creating layer drop3
I0521 08:15:29.890099 15047 net.cpp:106] Creating Layer drop3
I0521 08:15:29.890108 15047 net.cpp:454] drop3 <- ip3
I0521 08:15:29.890120 15047 net.cpp:397] drop3 -> ip3 (in-place)
I0521 08:15:29.890159 15047 net.cpp:150] Setting up drop3
I0521 08:15:29.890172 15047 net.cpp:157] Top shape: 840 11 (9240)
I0521 08:15:29.890182 15047 net.cpp:165] Memory required for data: 1326175200
I0521 08:15:29.890192 15047 layer_factory.hpp:77] Creating layer loss
I0521 08:15:29.890211 15047 net.cpp:106] Creating Layer loss
I0521 08:15:29.890221 15047 net.cpp:454] loss <- ip3
I0521 08:15:29.890233 15047 net.cpp:454] loss <- label
I0521 08:15:29.890244 15047 net.cpp:411] loss -> loss
I0521 08:15:29.890260 15047 layer_factory.hpp:77] Creating layer loss
I0521 08:15:29.890913 15047 net.cpp:150] Setting up loss
I0521 08:15:29.890934 15047 net.cpp:157] Top shape: (1)
I0521 08:15:29.890947 15047 net.cpp:160]     with loss weight 1
I0521 08:15:29.890988 15047 net.cpp:165] Memory required for data: 1326175204
I0521 08:15:29.891000 15047 net.cpp:226] loss needs backward computation.
I0521 08:15:29.891010 15047 net.cpp:226] drop3 needs backward computation.
I0521 08:15:29.891018 15047 net.cpp:226] ip3 needs backward computation.
I0521 08:15:29.891031 15047 net.cpp:226] drop2 needs backward computation.
I0521 08:15:29.891041 15047 net.cpp:226] relu6 needs backward computation.
I0521 08:15:29.891049 15047 net.cpp:226] ip2 needs backward computation.
I0521 08:15:29.891060 15047 net.cpp:226] drop1 needs backward computation.
I0521 08:15:29.891069 15047 net.cpp:226] relu5 needs backward computation.
I0521 08:15:29.891079 15047 net.cpp:226] ip1 needs backward computation.
I0521 08:15:29.891089 15047 net.cpp:226] pool4 needs backward computation.
I0521 08:15:29.891099 15047 net.cpp:226] relu4 needs backward computation.
I0521 08:15:29.891109 15047 net.cpp:226] conv4 needs backward computation.
I0521 08:15:29.891120 15047 net.cpp:226] pool3 needs backward computation.
I0521 08:15:29.891139 15047 net.cpp:226] relu3 needs backward computation.
I0521 08:15:29.891147 15047 net.cpp:226] conv3 needs backward computation.
I0521 08:15:29.891160 15047 net.cpp:226] pool2 needs backward computation.
I0521 08:15:29.891170 15047 net.cpp:226] relu2 needs backward computation.
I0521 08:15:29.891180 15047 net.cpp:226] conv2 needs backward computation.
I0521 08:15:29.891191 15047 net.cpp:226] pool1 needs backward computation.
I0521 08:15:29.891201 15047 net.cpp:226] relu1 needs backward computation.
I0521 08:15:29.891211 15047 net.cpp:226] conv1 needs backward computation.
I0521 08:15:29.891222 15047 net.cpp:228] data_hdf5 does not need backward computation.
I0521 08:15:29.891232 15047 net.cpp:270] This network produces output loss
I0521 08:15:29.891254 15047 net.cpp:283] Network initialization done.
I0521 08:15:29.892843 15047 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444.prototxt
I0521 08:15:29.892915 15047 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 08:15:29.893270 15047 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 840
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 08:15:29.893458 15047 layer_factory.hpp:77] Creating layer data_hdf5
I0521 08:15:29.893473 15047 net.cpp:106] Creating Layer data_hdf5
I0521 08:15:29.893486 15047 net.cpp:411] data_hdf5 -> data
I0521 08:15:29.893502 15047 net.cpp:411] data_hdf5 -> label
I0521 08:15:29.893518 15047 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 08:15:29.894724 15047 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 08:15:51.233206 15047 net.cpp:150] Setting up data_hdf5
I0521 08:15:51.233371 15047 net.cpp:157] Top shape: 840 1 127 50 (5334000)
I0521 08:15:51.233386 15047 net.cpp:157] Top shape: 840 (840)
I0521 08:15:51.233398 15047 net.cpp:165] Memory required for data: 21339360
I0521 08:15:51.233412 15047 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 08:15:51.233439 15047 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 08:15:51.233450 15047 net.cpp:454] label_data_hdf5_1_split <- label
I0521 08:15:51.233465 15047 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 08:15:51.233486 15047 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 08:15:51.233559 15047 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 08:15:51.233572 15047 net.cpp:157] Top shape: 840 (840)
I0521 08:15:51.233584 15047 net.cpp:157] Top shape: 840 (840)
I0521 08:15:51.233594 15047 net.cpp:165] Memory required for data: 21346080
I0521 08:15:51.233604 15047 layer_factory.hpp:77] Creating layer conv1
I0521 08:15:51.233625 15047 net.cpp:106] Creating Layer conv1
I0521 08:15:51.233636 15047 net.cpp:454] conv1 <- data
I0521 08:15:51.233651 15047 net.cpp:411] conv1 -> conv1
I0521 08:15:51.235574 15047 net.cpp:150] Setting up conv1
I0521 08:15:51.235599 15047 net.cpp:157] Top shape: 840 12 120 48 (58060800)
I0521 08:15:51.235610 15047 net.cpp:165] Memory required for data: 253589280
I0521 08:15:51.235630 15047 layer_factory.hpp:77] Creating layer relu1
I0521 08:15:51.235644 15047 net.cpp:106] Creating Layer relu1
I0521 08:15:51.235654 15047 net.cpp:454] relu1 <- conv1
I0521 08:15:51.235667 15047 net.cpp:397] relu1 -> conv1 (in-place)
I0521 08:15:51.236163 15047 net.cpp:150] Setting up relu1
I0521 08:15:51.236179 15047 net.cpp:157] Top shape: 840 12 120 48 (58060800)
I0521 08:15:51.236189 15047 net.cpp:165] Memory required for data: 485832480
I0521 08:15:51.236201 15047 layer_factory.hpp:77] Creating layer pool1
I0521 08:15:51.236215 15047 net.cpp:106] Creating Layer pool1
I0521 08:15:51.236225 15047 net.cpp:454] pool1 <- conv1
I0521 08:15:51.236239 15047 net.cpp:411] pool1 -> pool1
I0521 08:15:51.236315 15047 net.cpp:150] Setting up pool1
I0521 08:15:51.236327 15047 net.cpp:157] Top shape: 840 12 60 48 (29030400)
I0521 08:15:51.236337 15047 net.cpp:165] Memory required for data: 601954080
I0521 08:15:51.236348 15047 layer_factory.hpp:77] Creating layer conv2
I0521 08:15:51.236366 15047 net.cpp:106] Creating Layer conv2
I0521 08:15:51.236377 15047 net.cpp:454] conv2 <- pool1
I0521 08:15:51.236390 15047 net.cpp:411] conv2 -> conv2
I0521 08:15:51.238312 15047 net.cpp:150] Setting up conv2
I0521 08:15:51.238334 15047 net.cpp:157] Top shape: 840 20 54 46 (41731200)
I0521 08:15:51.238344 15047 net.cpp:165] Memory required for data: 768878880
I0521 08:15:51.238363 15047 layer_factory.hpp:77] Creating layer relu2
I0521 08:15:51.238378 15047 net.cpp:106] Creating Layer relu2
I0521 08:15:51.238387 15047 net.cpp:454] relu2 <- conv2
I0521 08:15:51.238400 15047 net.cpp:397] relu2 -> conv2 (in-place)
I0521 08:15:51.238822 15047 net.cpp:150] Setting up relu2
I0521 08:15:51.238837 15047 net.cpp:157] Top shape: 840 20 54 46 (41731200)
I0521 08:15:51.238847 15047 net.cpp:165] Memory required for data: 935803680
I0521 08:15:51.238857 15047 layer_factory.hpp:77] Creating layer pool2
I0521 08:15:51.238872 15047 net.cpp:106] Creating Layer pool2
I0521 08:15:51.238880 15047 net.cpp:454] pool2 <- conv2
I0521 08:15:51.238893 15047 net.cpp:411] pool2 -> pool2
I0521 08:15:51.238965 15047 net.cpp:150] Setting up pool2
I0521 08:15:51.238979 15047 net.cpp:157] Top shape: 840 20 27 46 (20865600)
I0521 08:15:51.238989 15047 net.cpp:165] Memory required for data: 1019266080
I0521 08:15:51.238999 15047 layer_factory.hpp:77] Creating layer conv3
I0521 08:15:51.239019 15047 net.cpp:106] Creating Layer conv3
I0521 08:15:51.239030 15047 net.cpp:454] conv3 <- pool2
I0521 08:15:51.239043 15047 net.cpp:411] conv3 -> conv3
I0521 08:15:51.241027 15047 net.cpp:150] Setting up conv3
I0521 08:15:51.241050 15047 net.cpp:157] Top shape: 840 28 22 44 (22767360)
I0521 08:15:51.241062 15047 net.cpp:165] Memory required for data: 1110335520
I0521 08:15:51.241094 15047 layer_factory.hpp:77] Creating layer relu3
I0521 08:15:51.241108 15047 net.cpp:106] Creating Layer relu3
I0521 08:15:51.241118 15047 net.cpp:454] relu3 <- conv3
I0521 08:15:51.241132 15047 net.cpp:397] relu3 -> conv3 (in-place)
I0521 08:15:51.241600 15047 net.cpp:150] Setting up relu3
I0521 08:15:51.241616 15047 net.cpp:157] Top shape: 840 28 22 44 (22767360)
I0521 08:15:51.241626 15047 net.cpp:165] Memory required for data: 1201404960
I0521 08:15:51.241637 15047 layer_factory.hpp:77] Creating layer pool3
I0521 08:15:51.241650 15047 net.cpp:106] Creating Layer pool3
I0521 08:15:51.241660 15047 net.cpp:454] pool3 <- conv3
I0521 08:15:51.241673 15047 net.cpp:411] pool3 -> pool3
I0521 08:15:51.241745 15047 net.cpp:150] Setting up pool3
I0521 08:15:51.241760 15047 net.cpp:157] Top shape: 840 28 11 44 (11383680)
I0521 08:15:51.241768 15047 net.cpp:165] Memory required for data: 1246939680
I0521 08:15:51.241780 15047 layer_factory.hpp:77] Creating layer conv4
I0521 08:15:51.241796 15047 net.cpp:106] Creating Layer conv4
I0521 08:15:51.241806 15047 net.cpp:454] conv4 <- pool3
I0521 08:15:51.241819 15047 net.cpp:411] conv4 -> conv4
I0521 08:15:51.243873 15047 net.cpp:150] Setting up conv4
I0521 08:15:51.243896 15047 net.cpp:157] Top shape: 840 36 6 42 (7620480)
I0521 08:15:51.243908 15047 net.cpp:165] Memory required for data: 1277421600
I0521 08:15:51.243923 15047 layer_factory.hpp:77] Creating layer relu4
I0521 08:15:51.243937 15047 net.cpp:106] Creating Layer relu4
I0521 08:15:51.243947 15047 net.cpp:454] relu4 <- conv4
I0521 08:15:51.243960 15047 net.cpp:397] relu4 -> conv4 (in-place)
I0521 08:15:51.244429 15047 net.cpp:150] Setting up relu4
I0521 08:15:51.244446 15047 net.cpp:157] Top shape: 840 36 6 42 (7620480)
I0521 08:15:51.244456 15047 net.cpp:165] Memory required for data: 1307903520
I0521 08:15:51.244465 15047 layer_factory.hpp:77] Creating layer pool4
I0521 08:15:51.244478 15047 net.cpp:106] Creating Layer pool4
I0521 08:15:51.244488 15047 net.cpp:454] pool4 <- conv4
I0521 08:15:51.244501 15047 net.cpp:411] pool4 -> pool4
I0521 08:15:51.244581 15047 net.cpp:150] Setting up pool4
I0521 08:15:51.244596 15047 net.cpp:157] Top shape: 840 36 3 42 (3810240)
I0521 08:15:51.244606 15047 net.cpp:165] Memory required for data: 1323144480
I0521 08:15:51.244616 15047 layer_factory.hpp:77] Creating layer ip1
I0521 08:15:51.244631 15047 net.cpp:106] Creating Layer ip1
I0521 08:15:51.244642 15047 net.cpp:454] ip1 <- pool4
I0521 08:15:51.244654 15047 net.cpp:411] ip1 -> ip1
I0521 08:15:51.260120 15047 net.cpp:150] Setting up ip1
I0521 08:15:51.260149 15047 net.cpp:157] Top shape: 840 196 (164640)
I0521 08:15:51.260159 15047 net.cpp:165] Memory required for data: 1323803040
I0521 08:15:51.260180 15047 layer_factory.hpp:77] Creating layer relu5
I0521 08:15:51.260195 15047 net.cpp:106] Creating Layer relu5
I0521 08:15:51.260206 15047 net.cpp:454] relu5 <- ip1
I0521 08:15:51.260221 15047 net.cpp:397] relu5 -> ip1 (in-place)
I0521 08:15:51.260570 15047 net.cpp:150] Setting up relu5
I0521 08:15:51.260583 15047 net.cpp:157] Top shape: 840 196 (164640)
I0521 08:15:51.260593 15047 net.cpp:165] Memory required for data: 1324461600
I0521 08:15:51.260603 15047 layer_factory.hpp:77] Creating layer drop1
I0521 08:15:51.260622 15047 net.cpp:106] Creating Layer drop1
I0521 08:15:51.260632 15047 net.cpp:454] drop1 <- ip1
I0521 08:15:51.260645 15047 net.cpp:397] drop1 -> ip1 (in-place)
I0521 08:15:51.260689 15047 net.cpp:150] Setting up drop1
I0521 08:15:51.260701 15047 net.cpp:157] Top shape: 840 196 (164640)
I0521 08:15:51.260710 15047 net.cpp:165] Memory required for data: 1325120160
I0521 08:15:51.260722 15047 layer_factory.hpp:77] Creating layer ip2
I0521 08:15:51.260736 15047 net.cpp:106] Creating Layer ip2
I0521 08:15:51.260746 15047 net.cpp:454] ip2 <- ip1
I0521 08:15:51.260761 15047 net.cpp:411] ip2 -> ip2
I0521 08:15:51.261240 15047 net.cpp:150] Setting up ip2
I0521 08:15:51.261252 15047 net.cpp:157] Top shape: 840 98 (82320)
I0521 08:15:51.261262 15047 net.cpp:165] Memory required for data: 1325449440
I0521 08:15:51.261291 15047 layer_factory.hpp:77] Creating layer relu6
I0521 08:15:51.261304 15047 net.cpp:106] Creating Layer relu6
I0521 08:15:51.261314 15047 net.cpp:454] relu6 <- ip2
I0521 08:15:51.261327 15047 net.cpp:397] relu6 -> ip2 (in-place)
I0521 08:15:51.261858 15047 net.cpp:150] Setting up relu6
I0521 08:15:51.261879 15047 net.cpp:157] Top shape: 840 98 (82320)
I0521 08:15:51.261889 15047 net.cpp:165] Memory required for data: 1325778720
I0521 08:15:51.261899 15047 layer_factory.hpp:77] Creating layer drop2
I0521 08:15:51.261914 15047 net.cpp:106] Creating Layer drop2
I0521 08:15:51.261924 15047 net.cpp:454] drop2 <- ip2
I0521 08:15:51.261936 15047 net.cpp:397] drop2 -> ip2 (in-place)
I0521 08:15:51.261979 15047 net.cpp:150] Setting up drop2
I0521 08:15:51.261992 15047 net.cpp:157] Top shape: 840 98 (82320)
I0521 08:15:51.262003 15047 net.cpp:165] Memory required for data: 1326108000
I0521 08:15:51.262013 15047 layer_factory.hpp:77] Creating layer ip3
I0521 08:15:51.262027 15047 net.cpp:106] Creating Layer ip3
I0521 08:15:51.262037 15047 net.cpp:454] ip3 <- ip2
I0521 08:15:51.262051 15047 net.cpp:411] ip3 -> ip3
I0521 08:15:51.262274 15047 net.cpp:150] Setting up ip3
I0521 08:15:51.262286 15047 net.cpp:157] Top shape: 840 11 (9240)
I0521 08:15:51.262296 15047 net.cpp:165] Memory required for data: 1326144960
I0521 08:15:51.262311 15047 layer_factory.hpp:77] Creating layer drop3
I0521 08:15:51.262326 15047 net.cpp:106] Creating Layer drop3
I0521 08:15:51.262334 15047 net.cpp:454] drop3 <- ip3
I0521 08:15:51.262347 15047 net.cpp:397] drop3 -> ip3 (in-place)
I0521 08:15:51.262388 15047 net.cpp:150] Setting up drop3
I0521 08:15:51.262401 15047 net.cpp:157] Top shape: 840 11 (9240)
I0521 08:15:51.262411 15047 net.cpp:165] Memory required for data: 1326181920
I0521 08:15:51.262419 15047 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 08:15:51.262433 15047 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 08:15:51.262444 15047 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 08:15:51.262456 15047 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 08:15:51.262471 15047 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 08:15:51.262544 15047 net.cpp:150] Setting up ip3_drop3_0_split
I0521 08:15:51.262557 15047 net.cpp:157] Top shape: 840 11 (9240)
I0521 08:15:51.262569 15047 net.cpp:157] Top shape: 840 11 (9240)
I0521 08:15:51.262579 15047 net.cpp:165] Memory required for data: 1326255840
I0521 08:15:51.262589 15047 layer_factory.hpp:77] Creating layer accuracy
I0521 08:15:51.262612 15047 net.cpp:106] Creating Layer accuracy
I0521 08:15:51.262624 15047 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 08:15:51.262634 15047 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 08:15:51.262647 15047 net.cpp:411] accuracy -> accuracy
I0521 08:15:51.262671 15047 net.cpp:150] Setting up accuracy
I0521 08:15:51.262683 15047 net.cpp:157] Top shape: (1)
I0521 08:15:51.262693 15047 net.cpp:165] Memory required for data: 1326255844
I0521 08:15:51.262703 15047 layer_factory.hpp:77] Creating layer loss
I0521 08:15:51.262717 15047 net.cpp:106] Creating Layer loss
I0521 08:15:51.262727 15047 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 08:15:51.262738 15047 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 08:15:51.262751 15047 net.cpp:411] loss -> loss
I0521 08:15:51.262769 15047 layer_factory.hpp:77] Creating layer loss
I0521 08:15:51.263263 15047 net.cpp:150] Setting up loss
I0521 08:15:51.263278 15047 net.cpp:157] Top shape: (1)
I0521 08:15:51.263288 15047 net.cpp:160]     with loss weight 1
I0521 08:15:51.263305 15047 net.cpp:165] Memory required for data: 1326255848
I0521 08:15:51.263315 15047 net.cpp:226] loss needs backward computation.
I0521 08:15:51.263326 15047 net.cpp:228] accuracy does not need backward computation.
I0521 08:15:51.263337 15047 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 08:15:51.263348 15047 net.cpp:226] drop3 needs backward computation.
I0521 08:15:51.263360 15047 net.cpp:226] ip3 needs backward computation.
I0521 08:15:51.263370 15047 net.cpp:226] drop2 needs backward computation.
I0521 08:15:51.263388 15047 net.cpp:226] relu6 needs backward computation.
I0521 08:15:51.263398 15047 net.cpp:226] ip2 needs backward computation.
I0521 08:15:51.263408 15047 net.cpp:226] drop1 needs backward computation.
I0521 08:15:51.263417 15047 net.cpp:226] relu5 needs backward computation.
I0521 08:15:51.263427 15047 net.cpp:226] ip1 needs backward computation.
I0521 08:15:51.263437 15047 net.cpp:226] pool4 needs backward computation.
I0521 08:15:51.263447 15047 net.cpp:226] relu4 needs backward computation.
I0521 08:15:51.263458 15047 net.cpp:226] conv4 needs backward computation.
I0521 08:15:51.263468 15047 net.cpp:226] pool3 needs backward computation.
I0521 08:15:51.263478 15047 net.cpp:226] relu3 needs backward computation.
I0521 08:15:51.263489 15047 net.cpp:226] conv3 needs backward computation.
I0521 08:15:51.263499 15047 net.cpp:226] pool2 needs backward computation.
I0521 08:15:51.263509 15047 net.cpp:226] relu2 needs backward computation.
I0521 08:15:51.263519 15047 net.cpp:226] conv2 needs backward computation.
I0521 08:15:51.263530 15047 net.cpp:226] pool1 needs backward computation.
I0521 08:15:51.263540 15047 net.cpp:226] relu1 needs backward computation.
I0521 08:15:51.263550 15047 net.cpp:226] conv1 needs backward computation.
I0521 08:15:51.263561 15047 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 08:15:51.263572 15047 net.cpp:228] data_hdf5 does not need backward computation.
I0521 08:15:51.263582 15047 net.cpp:270] This network produces output accuracy
I0521 08:15:51.263592 15047 net.cpp:270] This network produces output loss
I0521 08:15:51.263620 15047 net.cpp:283] Network initialization done.
I0521 08:15:51.263754 15047 solver.cpp:60] Solver scaffolding done.
I0521 08:15:51.264894 15047 caffe.cpp:212] Starting Optimization
I0521 08:15:51.264912 15047 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 08:15:51.264925 15047 solver.cpp:289] Learning Rate Policy: fixed
I0521 08:15:51.266147 15047 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 08:16:37.179527 15047 solver.cpp:409]     Test net output #0: accuracy = 0.0829254
I0521 08:16:37.179697 15047 solver.cpp:409]     Test net output #1: loss = 2.39706 (* 1 = 2.39706 loss)
I0521 08:16:37.334692 15047 solver.cpp:237] Iteration 0, loss = 2.39547
I0521 08:16:37.334728 15047 solver.cpp:253]     Train net output #0: loss = 2.39547 (* 1 = 2.39547 loss)
I0521 08:16:37.334746 15047 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 08:16:44.977715 15047 solver.cpp:237] Iteration 17, loss = 2.38671
I0521 08:16:44.977756 15047 solver.cpp:253]     Train net output #0: loss = 2.38671 (* 1 = 2.38671 loss)
I0521 08:16:44.977772 15047 sgd_solver.cpp:106] Iteration 17, lr = 0.0025
I0521 08:16:52.614843 15047 solver.cpp:237] Iteration 34, loss = 2.37147
I0521 08:16:52.614874 15047 solver.cpp:253]     Train net output #0: loss = 2.37147 (* 1 = 2.37147 loss)
I0521 08:16:52.614889 15047 sgd_solver.cpp:106] Iteration 34, lr = 0.0025
I0521 08:17:00.257292 15047 solver.cpp:237] Iteration 51, loss = 2.3568
I0521 08:17:00.257324 15047 solver.cpp:253]     Train net output #0: loss = 2.3568 (* 1 = 2.3568 loss)
I0521 08:17:00.257340 15047 sgd_solver.cpp:106] Iteration 51, lr = 0.0025
I0521 08:17:07.899052 15047 solver.cpp:237] Iteration 68, loss = 2.35306
I0521 08:17:07.899197 15047 solver.cpp:253]     Train net output #0: loss = 2.35306 (* 1 = 2.35306 loss)
I0521 08:17:07.899211 15047 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0521 08:17:15.540799 15047 solver.cpp:237] Iteration 85, loss = 2.34978
I0521 08:17:15.540829 15047 solver.cpp:253]     Train net output #0: loss = 2.34978 (* 1 = 2.34978 loss)
I0521 08:17:15.540850 15047 sgd_solver.cpp:106] Iteration 85, lr = 0.0025
I0521 08:17:23.178333 15047 solver.cpp:237] Iteration 102, loss = 2.33431
I0521 08:17:23.178364 15047 solver.cpp:253]     Train net output #0: loss = 2.33431 (* 1 = 2.33431 loss)
I0521 08:17:23.178380 15047 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0521 08:17:52.960059 15047 solver.cpp:237] Iteration 119, loss = 2.32533
I0521 08:17:52.960219 15047 solver.cpp:253]     Train net output #0: loss = 2.32533 (* 1 = 2.32533 loss)
I0521 08:17:52.960234 15047 sgd_solver.cpp:106] Iteration 119, lr = 0.0025
I0521 08:18:00.606490 15047 solver.cpp:237] Iteration 136, loss = 2.3124
I0521 08:18:00.606529 15047 solver.cpp:253]     Train net output #0: loss = 2.3124 (* 1 = 2.3124 loss)
I0521 08:18:00.606546 15047 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0521 08:18:08.253762 15047 solver.cpp:237] Iteration 153, loss = 2.32498
I0521 08:18:08.253795 15047 solver.cpp:253]     Train net output #0: loss = 2.32498 (* 1 = 2.32498 loss)
I0521 08:18:08.253811 15047 sgd_solver.cpp:106] Iteration 153, lr = 0.0025
I0521 08:18:15.902374 15047 solver.cpp:237] Iteration 170, loss = 2.30272
I0521 08:18:15.902408 15047 solver.cpp:253]     Train net output #0: loss = 2.30272 (* 1 = 2.30272 loss)
I0521 08:18:15.902422 15047 sgd_solver.cpp:106] Iteration 170, lr = 0.0025
I0521 08:18:19.049212 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_178.caffemodel
I0521 08:18:19.407330 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_178.solverstate
I0521 08:18:23.616339 15047 solver.cpp:237] Iteration 187, loss = 2.29355
I0521 08:18:23.616503 15047 solver.cpp:253]     Train net output #0: loss = 2.29355 (* 1 = 2.29355 loss)
I0521 08:18:23.616518 15047 sgd_solver.cpp:106] Iteration 187, lr = 0.0025
I0521 08:18:31.258649 15047 solver.cpp:237] Iteration 204, loss = 2.30208
I0521 08:18:31.258682 15047 solver.cpp:253]     Train net output #0: loss = 2.30208 (* 1 = 2.30208 loss)
I0521 08:18:31.258699 15047 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0521 08:18:38.901172 15047 solver.cpp:237] Iteration 221, loss = 2.28292
I0521 08:18:38.901204 15047 solver.cpp:253]     Train net output #0: loss = 2.28292 (* 1 = 2.28292 loss)
I0521 08:18:38.901218 15047 sgd_solver.cpp:106] Iteration 221, lr = 0.0025
I0521 08:19:08.699151 15047 solver.cpp:237] Iteration 238, loss = 2.26604
I0521 08:19:08.699316 15047 solver.cpp:253]     Train net output #0: loss = 2.26604 (* 1 = 2.26604 loss)
I0521 08:19:08.699331 15047 sgd_solver.cpp:106] Iteration 238, lr = 0.0025
I0521 08:19:16.344054 15047 solver.cpp:237] Iteration 255, loss = 2.25909
I0521 08:19:16.344084 15047 solver.cpp:253]     Train net output #0: loss = 2.25909 (* 1 = 2.25909 loss)
I0521 08:19:16.344102 15047 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 08:19:23.989163 15047 solver.cpp:237] Iteration 272, loss = 2.21242
I0521 08:19:23.989197 15047 solver.cpp:253]     Train net output #0: loss = 2.21242 (* 1 = 2.21242 loss)
I0521 08:19:23.989213 15047 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 08:19:31.631264 15047 solver.cpp:237] Iteration 289, loss = 2.19013
I0521 08:19:31.631299 15047 solver.cpp:253]     Train net output #0: loss = 2.19013 (* 1 = 2.19013 loss)
I0521 08:19:31.631315 15047 sgd_solver.cpp:106] Iteration 289, lr = 0.0025
I0521 08:19:39.278374 15047 solver.cpp:237] Iteration 306, loss = 2.18048
I0521 08:19:39.278532 15047 solver.cpp:253]     Train net output #0: loss = 2.18048 (* 1 = 2.18048 loss)
I0521 08:19:39.278547 15047 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 08:19:46.916558 15047 solver.cpp:237] Iteration 323, loss = 2.14802
I0521 08:19:46.916591 15047 solver.cpp:253]     Train net output #0: loss = 2.14802 (* 1 = 2.14802 loss)
I0521 08:19:46.916606 15047 sgd_solver.cpp:106] Iteration 323, lr = 0.0025
I0521 08:19:54.557716 15047 solver.cpp:237] Iteration 340, loss = 2.15939
I0521 08:19:54.557749 15047 solver.cpp:253]     Train net output #0: loss = 2.15939 (* 1 = 2.15939 loss)
I0521 08:19:54.557766 15047 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0521 08:20:01.296720 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_356.caffemodel
I0521 08:20:01.651510 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_356.solverstate
I0521 08:20:01.812450 15047 solver.cpp:341] Iteration 357, Testing net (#0)
I0521 08:20:47.141252 15047 solver.cpp:409]     Test net output #0: accuracy = 0.470934
I0521 08:20:47.141412 15047 solver.cpp:409]     Test net output #1: loss = 1.93714 (* 1 = 1.93714 loss)
I0521 08:21:09.434715 15047 solver.cpp:237] Iteration 357, loss = 2.16346
I0521 08:21:09.434767 15047 solver.cpp:253]     Train net output #0: loss = 2.16346 (* 1 = 2.16346 loss)
I0521 08:21:09.434782 15047 sgd_solver.cpp:106] Iteration 357, lr = 0.0025
I0521 08:21:17.076992 15047 solver.cpp:237] Iteration 374, loss = 2.05053
I0521 08:21:17.077031 15047 solver.cpp:253]     Train net output #0: loss = 2.05053 (* 1 = 2.05053 loss)
I0521 08:21:17.077052 15047 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 08:21:24.714568 15047 solver.cpp:237] Iteration 391, loss = 2.11091
I0521 08:21:24.714711 15047 solver.cpp:253]     Train net output #0: loss = 2.11091 (* 1 = 2.11091 loss)
I0521 08:21:24.714725 15047 sgd_solver.cpp:106] Iteration 391, lr = 0.0025
I0521 08:21:32.350883 15047 solver.cpp:237] Iteration 408, loss = 2.0873
I0521 08:21:32.350914 15047 solver.cpp:253]     Train net output #0: loss = 2.0873 (* 1 = 2.0873 loss)
I0521 08:21:32.350931 15047 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0521 08:21:39.990013 15047 solver.cpp:237] Iteration 425, loss = 2.0464
I0521 08:21:39.990046 15047 solver.cpp:253]     Train net output #0: loss = 2.0464 (* 1 = 2.0464 loss)
I0521 08:21:39.990062 15047 sgd_solver.cpp:106] Iteration 425, lr = 0.0025
I0521 08:21:47.628654 15047 solver.cpp:237] Iteration 442, loss = 2.04628
I0521 08:21:47.628693 15047 solver.cpp:253]     Train net output #0: loss = 2.04628 (* 1 = 2.04628 loss)
I0521 08:21:47.628715 15047 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0521 08:21:55.267457 15047 solver.cpp:237] Iteration 459, loss = 1.96062
I0521 08:21:55.267601 15047 solver.cpp:253]     Train net output #0: loss = 1.96062 (* 1 = 1.96062 loss)
I0521 08:21:55.267614 15047 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0521 08:22:25.115087 15047 solver.cpp:237] Iteration 476, loss = 1.95014
I0521 08:22:25.115140 15047 solver.cpp:253]     Train net output #0: loss = 1.95014 (* 1 = 1.95014 loss)
I0521 08:22:25.115157 15047 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0521 08:22:32.752696 15047 solver.cpp:237] Iteration 493, loss = 1.97889
I0521 08:22:32.752859 15047 solver.cpp:253]     Train net output #0: loss = 1.97889 (* 1 = 1.97889 loss)
I0521 08:22:32.752874 15047 sgd_solver.cpp:106] Iteration 493, lr = 0.0025
I0521 08:22:40.386699 15047 solver.cpp:237] Iteration 510, loss = 1.92649
I0521 08:22:40.386731 15047 solver.cpp:253]     Train net output #0: loss = 1.92649 (* 1 = 1.92649 loss)
I0521 08:22:40.386749 15047 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 08:22:48.020997 15047 solver.cpp:237] Iteration 527, loss = 1.91336
I0521 08:22:48.021028 15047 solver.cpp:253]     Train net output #0: loss = 1.91336 (* 1 = 1.91336 loss)
I0521 08:22:48.021044 15047 sgd_solver.cpp:106] Iteration 527, lr = 0.0025
I0521 08:22:50.714601 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_534.caffemodel
I0521 08:22:51.068945 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_534.solverstate
I0521 08:22:55.720680 15047 solver.cpp:237] Iteration 544, loss = 1.89679
I0521 08:22:55.720724 15047 solver.cpp:253]     Train net output #0: loss = 1.89679 (* 1 = 1.89679 loss)
I0521 08:22:55.720743 15047 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 08:23:03.361196 15047 solver.cpp:237] Iteration 561, loss = 1.93937
I0521 08:23:03.361346 15047 solver.cpp:253]     Train net output #0: loss = 1.93937 (* 1 = 1.93937 loss)
I0521 08:23:03.361361 15047 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0521 08:23:10.996861 15047 solver.cpp:237] Iteration 578, loss = 1.96622
I0521 08:23:10.996892 15047 solver.cpp:253]     Train net output #0: loss = 1.96622 (* 1 = 1.96622 loss)
I0521 08:23:10.996911 15047 sgd_solver.cpp:106] Iteration 578, lr = 0.0025
I0521 08:23:40.784829 15047 solver.cpp:237] Iteration 595, loss = 1.92631
I0521 08:23:40.785001 15047 solver.cpp:253]     Train net output #0: loss = 1.92631 (* 1 = 1.92631 loss)
I0521 08:23:40.785015 15047 sgd_solver.cpp:106] Iteration 595, lr = 0.0025
I0521 08:23:48.421478 15047 solver.cpp:237] Iteration 612, loss = 1.93822
I0521 08:23:48.421521 15047 solver.cpp:253]     Train net output #0: loss = 1.93822 (* 1 = 1.93822 loss)
I0521 08:23:48.421537 15047 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 08:23:56.064363 15047 solver.cpp:237] Iteration 629, loss = 1.80379
I0521 08:23:56.064396 15047 solver.cpp:253]     Train net output #0: loss = 1.80379 (* 1 = 1.80379 loss)
I0521 08:23:56.064414 15047 sgd_solver.cpp:106] Iteration 629, lr = 0.0025
I0521 08:24:03.698056 15047 solver.cpp:237] Iteration 646, loss = 1.87939
I0521 08:24:03.698088 15047 solver.cpp:253]     Train net output #0: loss = 1.87939 (* 1 = 1.87939 loss)
I0521 08:24:03.698107 15047 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0521 08:24:11.338625 15047 solver.cpp:237] Iteration 663, loss = 1.80801
I0521 08:24:11.338770 15047 solver.cpp:253]     Train net output #0: loss = 1.80801 (* 1 = 1.80801 loss)
I0521 08:24:11.338784 15047 sgd_solver.cpp:106] Iteration 663, lr = 0.0025
I0521 08:24:18.972041 15047 solver.cpp:237] Iteration 680, loss = 1.85707
I0521 08:24:18.972075 15047 solver.cpp:253]     Train net output #0: loss = 1.85707 (* 1 = 1.85707 loss)
I0521 08:24:18.972097 15047 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0521 08:24:26.608227 15047 solver.cpp:237] Iteration 697, loss = 1.84692
I0521 08:24:26.608258 15047 solver.cpp:253]     Train net output #0: loss = 1.84692 (* 1 = 1.84692 loss)
I0521 08:24:26.608278 15047 sgd_solver.cpp:106] Iteration 697, lr = 0.0025
I0521 08:24:32.900012 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_712.caffemodel
I0521 08:24:33.256279 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_712.solverstate
I0521 08:24:33.867286 15047 solver.cpp:341] Iteration 714, Testing net (#0)
I0521 08:25:40.160830 15047 solver.cpp:409]     Test net output #0: accuracy = 0.591861
I0521 08:25:40.161000 15047 solver.cpp:409]     Test net output #1: loss = 1.46808 (* 1 = 1.46808 loss)
I0521 08:26:02.468957 15047 solver.cpp:237] Iteration 714, loss = 1.90383
I0521 08:26:02.469010 15047 solver.cpp:253]     Train net output #0: loss = 1.90383 (* 1 = 1.90383 loss)
I0521 08:26:02.469025 15047 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0521 08:26:10.107522 15047 solver.cpp:237] Iteration 731, loss = 1.86295
I0521 08:26:10.107555 15047 solver.cpp:253]     Train net output #0: loss = 1.86295 (* 1 = 1.86295 loss)
I0521 08:26:10.107568 15047 sgd_solver.cpp:106] Iteration 731, lr = 0.0025
I0521 08:26:17.745168 15047 solver.cpp:237] Iteration 748, loss = 1.84738
I0521 08:26:17.745318 15047 solver.cpp:253]     Train net output #0: loss = 1.84738 (* 1 = 1.84738 loss)
I0521 08:26:17.745332 15047 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 08:26:25.381011 15047 solver.cpp:237] Iteration 765, loss = 1.86955
I0521 08:26:25.381042 15047 solver.cpp:253]     Train net output #0: loss = 1.86955 (* 1 = 1.86955 loss)
I0521 08:26:25.381060 15047 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 08:26:33.021998 15047 solver.cpp:237] Iteration 782, loss = 1.83948
I0521 08:26:33.022030 15047 solver.cpp:253]     Train net output #0: loss = 1.83948 (* 1 = 1.83948 loss)
I0521 08:26:33.022047 15047 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0521 08:26:40.656909 15047 solver.cpp:237] Iteration 799, loss = 1.85382
I0521 08:26:40.656941 15047 solver.cpp:253]     Train net output #0: loss = 1.85382 (* 1 = 1.85382 loss)
I0521 08:26:40.656958 15047 sgd_solver.cpp:106] Iteration 799, lr = 0.0025
I0521 08:26:48.293467 15047 solver.cpp:237] Iteration 816, loss = 1.80369
I0521 08:26:48.293622 15047 solver.cpp:253]     Train net output #0: loss = 1.80369 (* 1 = 1.80369 loss)
I0521 08:26:48.293635 15047 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 08:27:18.114181 15047 solver.cpp:237] Iteration 833, loss = 1.79637
I0521 08:27:18.114231 15047 solver.cpp:253]     Train net output #0: loss = 1.79637 (* 1 = 1.79637 loss)
I0521 08:27:18.114246 15047 sgd_solver.cpp:106] Iteration 833, lr = 0.0025
I0521 08:27:25.750201 15047 solver.cpp:237] Iteration 850, loss = 1.79863
I0521 08:27:25.750349 15047 solver.cpp:253]     Train net output #0: loss = 1.79863 (* 1 = 1.79863 loss)
I0521 08:27:25.750363 15047 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0521 08:27:33.384035 15047 solver.cpp:237] Iteration 867, loss = 1.79709
I0521 08:27:33.384080 15047 solver.cpp:253]     Train net output #0: loss = 1.79709 (* 1 = 1.79709 loss)
I0521 08:27:33.384094 15047 sgd_solver.cpp:106] Iteration 867, lr = 0.0025
I0521 08:27:41.015558 15047 solver.cpp:237] Iteration 884, loss = 1.78521
I0521 08:27:41.015589 15047 solver.cpp:253]     Train net output #0: loss = 1.78521 (* 1 = 1.78521 loss)
I0521 08:27:41.015606 15047 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0521 08:27:43.259378 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_890.caffemodel
I0521 08:27:43.614874 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_890.solverstate
I0521 08:27:48.716686 15047 solver.cpp:237] Iteration 901, loss = 1.86813
I0521 08:27:48.716734 15047 solver.cpp:253]     Train net output #0: loss = 1.86813 (* 1 = 1.86813 loss)
I0521 08:27:48.716749 15047 sgd_solver.cpp:106] Iteration 901, lr = 0.0025
I0521 08:27:56.349709 15047 solver.cpp:237] Iteration 918, loss = 1.82756
I0521 08:27:56.349858 15047 solver.cpp:253]     Train net output #0: loss = 1.82756 (* 1 = 1.82756 loss)
I0521 08:27:56.349872 15047 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 08:28:03.986603 15047 solver.cpp:237] Iteration 935, loss = 1.81335
I0521 08:28:03.986649 15047 solver.cpp:253]     Train net output #0: loss = 1.81335 (* 1 = 1.81335 loss)
I0521 08:28:03.986666 15047 sgd_solver.cpp:106] Iteration 935, lr = 0.0025
I0521 08:28:33.816016 15047 solver.cpp:237] Iteration 952, loss = 1.80244
I0521 08:28:33.816186 15047 solver.cpp:253]     Train net output #0: loss = 1.80244 (* 1 = 1.80244 loss)
I0521 08:28:33.816200 15047 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0521 08:28:41.450814 15047 solver.cpp:237] Iteration 969, loss = 1.77733
I0521 08:28:41.450845 15047 solver.cpp:253]     Train net output #0: loss = 1.77733 (* 1 = 1.77733 loss)
I0521 08:28:41.450863 15047 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0521 08:28:49.085645 15047 solver.cpp:237] Iteration 986, loss = 1.77205
I0521 08:28:49.085691 15047 solver.cpp:253]     Train net output #0: loss = 1.77205 (* 1 = 1.77205 loss)
I0521 08:28:49.085708 15047 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0521 08:28:56.718114 15047 solver.cpp:237] Iteration 1003, loss = 1.8563
I0521 08:28:56.718147 15047 solver.cpp:253]     Train net output #0: loss = 1.8563 (* 1 = 1.8563 loss)
I0521 08:28:56.718164 15047 sgd_solver.cpp:106] Iteration 1003, lr = 0.0025
I0521 08:29:04.350620 15047 solver.cpp:237] Iteration 1020, loss = 1.83257
I0521 08:29:04.350754 15047 solver.cpp:253]     Train net output #0: loss = 1.83257 (* 1 = 1.83257 loss)
I0521 08:29:04.350767 15047 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 08:29:11.987854 15047 solver.cpp:237] Iteration 1037, loss = 1.78438
I0521 08:29:11.987890 15047 solver.cpp:253]     Train net output #0: loss = 1.78438 (* 1 = 1.78438 loss)
I0521 08:29:11.987906 15047 sgd_solver.cpp:106] Iteration 1037, lr = 0.0025
I0521 08:29:19.617168 15047 solver.cpp:237] Iteration 1054, loss = 1.79573
I0521 08:29:19.617200 15047 solver.cpp:253]     Train net output #0: loss = 1.79573 (* 1 = 1.79573 loss)
I0521 08:29:19.617218 15047 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0521 08:29:25.453683 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1068.caffemodel
I0521 08:29:25.806962 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1068.solverstate
I0521 08:29:26.865039 15047 solver.cpp:341] Iteration 1071, Testing net (#0)
I0521 08:30:11.883520 15047 solver.cpp:409]     Test net output #0: accuracy = 0.607069
I0521 08:30:11.883687 15047 solver.cpp:409]     Test net output #1: loss = 1.36127 (* 1 = 1.36127 loss)
I0521 08:30:34.165616 15047 solver.cpp:237] Iteration 1071, loss = 1.84341
I0521 08:30:34.165669 15047 solver.cpp:253]     Train net output #0: loss = 1.84341 (* 1 = 1.84341 loss)
I0521 08:30:34.165684 15047 sgd_solver.cpp:106] Iteration 1071, lr = 0.0025
I0521 08:30:41.804296 15047 solver.cpp:237] Iteration 1088, loss = 1.7358
I0521 08:30:41.804328 15047 solver.cpp:253]     Train net output #0: loss = 1.7358 (* 1 = 1.7358 loss)
I0521 08:30:41.804342 15047 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 08:30:49.445631 15047 solver.cpp:237] Iteration 1105, loss = 1.74267
I0521 08:30:49.445780 15047 solver.cpp:253]     Train net output #0: loss = 1.74267 (* 1 = 1.74267 loss)
I0521 08:30:49.445793 15047 sgd_solver.cpp:106] Iteration 1105, lr = 0.0025
I0521 08:30:57.085055 15047 solver.cpp:237] Iteration 1122, loss = 1.70206
I0521 08:30:57.085098 15047 solver.cpp:253]     Train net output #0: loss = 1.70206 (* 1 = 1.70206 loss)
I0521 08:30:57.085114 15047 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 08:31:04.726002 15047 solver.cpp:237] Iteration 1139, loss = 1.76899
I0521 08:31:04.726034 15047 solver.cpp:253]     Train net output #0: loss = 1.76899 (* 1 = 1.76899 loss)
I0521 08:31:04.726048 15047 sgd_solver.cpp:106] Iteration 1139, lr = 0.0025
I0521 08:31:12.363103 15047 solver.cpp:237] Iteration 1156, loss = 1.80013
I0521 08:31:12.363137 15047 solver.cpp:253]     Train net output #0: loss = 1.80013 (* 1 = 1.80013 loss)
I0521 08:31:12.363153 15047 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0521 08:31:20.002423 15047 solver.cpp:237] Iteration 1173, loss = 1.76192
I0521 08:31:20.002594 15047 solver.cpp:253]     Train net output #0: loss = 1.76192 (* 1 = 1.76192 loss)
I0521 08:31:20.002609 15047 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0521 08:31:49.811895 15047 solver.cpp:237] Iteration 1190, loss = 1.71926
I0521 08:31:49.811946 15047 solver.cpp:253]     Train net output #0: loss = 1.71926 (* 1 = 1.71926 loss)
I0521 08:31:49.811965 15047 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0521 08:31:57.454526 15047 solver.cpp:237] Iteration 1207, loss = 1.75697
I0521 08:31:57.454674 15047 solver.cpp:253]     Train net output #0: loss = 1.75697 (* 1 = 1.75697 loss)
I0521 08:31:57.454689 15047 sgd_solver.cpp:106] Iteration 1207, lr = 0.0025
I0521 08:32:05.091894 15047 solver.cpp:237] Iteration 1224, loss = 1.73689
I0521 08:32:05.091925 15047 solver.cpp:253]     Train net output #0: loss = 1.73689 (* 1 = 1.73689 loss)
I0521 08:32:05.091943 15047 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 08:32:12.733361 15047 solver.cpp:237] Iteration 1241, loss = 1.69234
I0521 08:32:12.733405 15047 solver.cpp:253]     Train net output #0: loss = 1.69234 (* 1 = 1.69234 loss)
I0521 08:32:12.733419 15047 sgd_solver.cpp:106] Iteration 1241, lr = 0.0025
I0521 08:32:14.531252 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1246.caffemodel
I0521 08:32:14.888470 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1246.solverstate
I0521 08:32:20.438418 15047 solver.cpp:237] Iteration 1258, loss = 1.67235
I0521 08:32:20.438465 15047 solver.cpp:253]     Train net output #0: loss = 1.67235 (* 1 = 1.67235 loss)
I0521 08:32:20.438482 15047 sgd_solver.cpp:106] Iteration 1258, lr = 0.0025
I0521 08:32:28.076570 15047 solver.cpp:237] Iteration 1275, loss = 1.77803
I0521 08:32:28.076714 15047 solver.cpp:253]     Train net output #0: loss = 1.77803 (* 1 = 1.77803 loss)
I0521 08:32:28.076728 15047 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 08:32:35.719241 15047 solver.cpp:237] Iteration 1292, loss = 1.69575
I0521 08:32:35.719280 15047 solver.cpp:253]     Train net output #0: loss = 1.69575 (* 1 = 1.69575 loss)
I0521 08:32:35.719300 15047 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0521 08:33:05.540813 15047 solver.cpp:237] Iteration 1309, loss = 1.72782
I0521 08:33:05.540989 15047 solver.cpp:253]     Train net output #0: loss = 1.72782 (* 1 = 1.72782 loss)
I0521 08:33:05.541004 15047 sgd_solver.cpp:106] Iteration 1309, lr = 0.0025
I0521 08:33:13.181154 15047 solver.cpp:237] Iteration 1326, loss = 1.77059
I0521 08:33:13.181186 15047 solver.cpp:253]     Train net output #0: loss = 1.77059 (* 1 = 1.77059 loss)
I0521 08:33:13.181201 15047 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0521 08:33:20.816041 15047 solver.cpp:237] Iteration 1343, loss = 1.7353
I0521 08:33:20.816076 15047 solver.cpp:253]     Train net output #0: loss = 1.7353 (* 1 = 1.7353 loss)
I0521 08:33:20.816090 15047 sgd_solver.cpp:106] Iteration 1343, lr = 0.0025
I0521 08:33:28.458739 15047 solver.cpp:237] Iteration 1360, loss = 1.73011
I0521 08:33:28.458778 15047 solver.cpp:253]     Train net output #0: loss = 1.73011 (* 1 = 1.73011 loss)
I0521 08:33:28.458796 15047 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 08:33:36.101227 15047 solver.cpp:237] Iteration 1377, loss = 1.63858
I0521 08:33:36.101385 15047 solver.cpp:253]     Train net output #0: loss = 1.63858 (* 1 = 1.63858 loss)
I0521 08:33:36.101399 15047 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0521 08:33:43.732152 15047 solver.cpp:237] Iteration 1394, loss = 1.65375
I0521 08:33:43.732183 15047 solver.cpp:253]     Train net output #0: loss = 1.65375 (* 1 = 1.65375 loss)
I0521 08:33:43.732200 15047 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0521 08:33:51.375967 15047 solver.cpp:237] Iteration 1411, loss = 1.71515
I0521 08:33:51.376008 15047 solver.cpp:253]     Train net output #0: loss = 1.71515 (* 1 = 1.71515 loss)
I0521 08:33:51.376024 15047 sgd_solver.cpp:106] Iteration 1411, lr = 0.0025
I0521 08:33:56.772265 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1424.caffemodel
I0521 08:33:57.126310 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1424.solverstate
I0521 08:33:58.635192 15047 solver.cpp:341] Iteration 1428, Testing net (#0)
I0521 08:35:04.904502 15047 solver.cpp:409]     Test net output #0: accuracy = 0.662119
I0521 08:35:04.904670 15047 solver.cpp:409]     Test net output #1: loss = 1.18123 (* 1 = 1.18123 loss)
I0521 08:35:27.199920 15047 solver.cpp:237] Iteration 1428, loss = 1.65603
I0521 08:35:27.199970 15047 solver.cpp:253]     Train net output #0: loss = 1.65603 (* 1 = 1.65603 loss)
I0521 08:35:27.199987 15047 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 08:35:34.846233 15047 solver.cpp:237] Iteration 1445, loss = 1.69887
I0521 08:35:34.846266 15047 solver.cpp:253]     Train net output #0: loss = 1.69887 (* 1 = 1.69887 loss)
I0521 08:35:34.846282 15047 sgd_solver.cpp:106] Iteration 1445, lr = 0.0025
I0521 08:35:42.490324 15047 solver.cpp:237] Iteration 1462, loss = 1.77888
I0521 08:35:42.490474 15047 solver.cpp:253]     Train net output #0: loss = 1.77888 (* 1 = 1.77888 loss)
I0521 08:35:42.490489 15047 sgd_solver.cpp:106] Iteration 1462, lr = 0.0025
I0521 08:35:50.136292 15047 solver.cpp:237] Iteration 1479, loss = 1.66795
I0521 08:35:50.136322 15047 solver.cpp:253]     Train net output #0: loss = 1.66795 (* 1 = 1.66795 loss)
I0521 08:35:50.136339 15047 sgd_solver.cpp:106] Iteration 1479, lr = 0.0025
I0521 08:35:57.774332 15047 solver.cpp:237] Iteration 1496, loss = 1.75162
I0521 08:35:57.774369 15047 solver.cpp:253]     Train net output #0: loss = 1.75162 (* 1 = 1.75162 loss)
I0521 08:35:57.774387 15047 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 08:36:05.413661 15047 solver.cpp:237] Iteration 1513, loss = 1.63027
I0521 08:36:05.413691 15047 solver.cpp:253]     Train net output #0: loss = 1.63027 (* 1 = 1.63027 loss)
I0521 08:36:05.413707 15047 sgd_solver.cpp:106] Iteration 1513, lr = 0.0025
I0521 08:36:13.060487 15047 solver.cpp:237] Iteration 1530, loss = 1.67897
I0521 08:36:13.060632 15047 solver.cpp:253]     Train net output #0: loss = 1.67897 (* 1 = 1.67897 loss)
I0521 08:36:13.060645 15047 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 08:36:42.903002 15047 solver.cpp:237] Iteration 1547, loss = 1.75565
I0521 08:36:42.903053 15047 solver.cpp:253]     Train net output #0: loss = 1.75565 (* 1 = 1.75565 loss)
I0521 08:36:42.903070 15047 sgd_solver.cpp:106] Iteration 1547, lr = 0.0025
I0521 08:36:50.547482 15047 solver.cpp:237] Iteration 1564, loss = 1.62842
I0521 08:36:50.547628 15047 solver.cpp:253]     Train net output #0: loss = 1.62842 (* 1 = 1.62842 loss)
I0521 08:36:50.547641 15047 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0521 08:36:58.190330 15047 solver.cpp:237] Iteration 1581, loss = 1.7036
I0521 08:36:58.190361 15047 solver.cpp:253]     Train net output #0: loss = 1.7036 (* 1 = 1.7036 loss)
I0521 08:36:58.190377 15047 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0521 08:37:05.828338 15047 solver.cpp:237] Iteration 1598, loss = 1.76828
I0521 08:37:05.828369 15047 solver.cpp:253]     Train net output #0: loss = 1.76828 (* 1 = 1.76828 loss)
I0521 08:37:05.828385 15047 sgd_solver.cpp:106] Iteration 1598, lr = 0.0025
I0521 08:37:07.178611 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1602.caffemodel
I0521 08:37:07.533828 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1602.solverstate
I0521 08:37:13.539628 15047 solver.cpp:237] Iteration 1615, loss = 1.71028
I0521 08:37:13.539676 15047 solver.cpp:253]     Train net output #0: loss = 1.71028 (* 1 = 1.71028 loss)
I0521 08:37:13.539691 15047 sgd_solver.cpp:106] Iteration 1615, lr = 0.0025
I0521 08:37:21.181246 15047 solver.cpp:237] Iteration 1632, loss = 1.74952
I0521 08:37:21.181406 15047 solver.cpp:253]     Train net output #0: loss = 1.74952 (* 1 = 1.74952 loss)
I0521 08:37:21.181418 15047 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 08:37:28.827596 15047 solver.cpp:237] Iteration 1649, loss = 1.68535
I0521 08:37:28.827627 15047 solver.cpp:253]     Train net output #0: loss = 1.68535 (* 1 = 1.68535 loss)
I0521 08:37:28.827643 15047 sgd_solver.cpp:106] Iteration 1649, lr = 0.0025
I0521 08:37:58.694057 15047 solver.cpp:237] Iteration 1666, loss = 1.69937
I0521 08:37:58.694237 15047 solver.cpp:253]     Train net output #0: loss = 1.69937 (* 1 = 1.69937 loss)
I0521 08:37:58.694252 15047 sgd_solver.cpp:106] Iteration 1666, lr = 0.0025
I0521 08:38:06.335389 15047 solver.cpp:237] Iteration 1683, loss = 1.67817
I0521 08:38:06.335420 15047 solver.cpp:253]     Train net output #0: loss = 1.67817 (* 1 = 1.67817 loss)
I0521 08:38:06.335436 15047 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0521 08:38:13.979729 15047 solver.cpp:237] Iteration 1700, loss = 1.73059
I0521 08:38:13.979763 15047 solver.cpp:253]     Train net output #0: loss = 1.73059 (* 1 = 1.73059 loss)
I0521 08:38:13.979776 15047 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 08:38:21.622977 15047 solver.cpp:237] Iteration 1717, loss = 1.63171
I0521 08:38:21.623013 15047 solver.cpp:253]     Train net output #0: loss = 1.63171 (* 1 = 1.63171 loss)
I0521 08:38:21.623030 15047 sgd_solver.cpp:106] Iteration 1717, lr = 0.0025
I0521 08:38:29.274361 15047 solver.cpp:237] Iteration 1734, loss = 1.64794
I0521 08:38:29.274502 15047 solver.cpp:253]     Train net output #0: loss = 1.64794 (* 1 = 1.64794 loss)
I0521 08:38:29.274514 15047 sgd_solver.cpp:106] Iteration 1734, lr = 0.0025
I0521 08:38:36.915729 15047 solver.cpp:237] Iteration 1751, loss = 1.67553
I0521 08:38:36.915760 15047 solver.cpp:253]     Train net output #0: loss = 1.67553 (* 1 = 1.67553 loss)
I0521 08:38:36.915776 15047 sgd_solver.cpp:106] Iteration 1751, lr = 0.0025
I0521 08:38:44.557698 15047 solver.cpp:237] Iteration 1768, loss = 1.76659
I0521 08:38:44.557729 15047 solver.cpp:253]     Train net output #0: loss = 1.76659 (* 1 = 1.76659 loss)
I0521 08:38:44.557744 15047 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0521 08:38:49.502410 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1780.caffemodel
I0521 08:38:49.856343 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1780.solverstate
I0521 08:38:51.819613 15047 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1785.caffemodel
I0521 08:38:52.178345 15047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444_iter_1785.solverstate
I0521 08:39:13.306826 15047 solver.cpp:321] Iteration 1785, loss = 1.63273
I0521 08:39:13.307000 15047 solver.cpp:341] Iteration 1785, Testing net (#0)
I0521 08:39:58.384888 15047 solver.cpp:409]     Test net output #0: accuracy = 0.676726
I0521 08:39:58.385056 15047 solver.cpp:409]     Test net output #1: loss = 1.12765 (* 1 = 1.12765 loss)
I0521 08:39:58.385069 15047 solver.cpp:326] Optimization Done.
I0521 08:39:58.385082 15047 caffe.cpp:215] Optimization Done.
Application 11237301 resources: utime ~1265s, stime ~228s, Rss ~5332948, inblocks ~3744348, outblocks ~194565
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_840_2016-05-20T11.21.03.337444.solver"
	User time (seconds): 0.57
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:56.68
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 2752
	Involuntary context switches: 140
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
