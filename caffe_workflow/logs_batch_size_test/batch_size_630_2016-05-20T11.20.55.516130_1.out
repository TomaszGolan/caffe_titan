2806236
I0521 04:05:11.397964  7754 caffe.cpp:184] Using GPUs 0
I0521 04:05:11.818138  7754 solver.cpp:48] Initializing solver from parameters: 
test_iter: 238
test_interval: 476
base_lr: 0.0025
display: 23
max_iter: 2380
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 238
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130.prototxt"
I0521 04:05:11.820145  7754 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130.prototxt
I0521 04:05:11.836812  7754 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 04:05:11.836872  7754 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 04:05:11.837216  7754 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 630
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:05:11.837399  7754 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:05:11.837422  7754 net.cpp:106] Creating Layer data_hdf5
I0521 04:05:11.837437  7754 net.cpp:411] data_hdf5 -> data
I0521 04:05:11.837472  7754 net.cpp:411] data_hdf5 -> label
I0521 04:05:11.837504  7754 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 04:05:11.838891  7754 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 04:05:11.841194  7754 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 04:05:33.356343  7754 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 04:05:33.361487  7754 net.cpp:150] Setting up data_hdf5
I0521 04:05:33.361531  7754 net.cpp:157] Top shape: 630 1 127 50 (4000500)
I0521 04:05:33.361546  7754 net.cpp:157] Top shape: 630 (630)
I0521 04:05:33.361557  7754 net.cpp:165] Memory required for data: 16004520
I0521 04:05:33.361569  7754 layer_factory.hpp:77] Creating layer conv1
I0521 04:05:33.361604  7754 net.cpp:106] Creating Layer conv1
I0521 04:05:33.361615  7754 net.cpp:454] conv1 <- data
I0521 04:05:33.361639  7754 net.cpp:411] conv1 -> conv1
I0521 04:05:33.725380  7754 net.cpp:150] Setting up conv1
I0521 04:05:33.725424  7754 net.cpp:157] Top shape: 630 12 120 48 (43545600)
I0521 04:05:33.725435  7754 net.cpp:165] Memory required for data: 190186920
I0521 04:05:33.725462  7754 layer_factory.hpp:77] Creating layer relu1
I0521 04:05:33.725483  7754 net.cpp:106] Creating Layer relu1
I0521 04:05:33.725494  7754 net.cpp:454] relu1 <- conv1
I0521 04:05:33.725507  7754 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:05:33.726028  7754 net.cpp:150] Setting up relu1
I0521 04:05:33.726045  7754 net.cpp:157] Top shape: 630 12 120 48 (43545600)
I0521 04:05:33.726057  7754 net.cpp:165] Memory required for data: 364369320
I0521 04:05:33.726066  7754 layer_factory.hpp:77] Creating layer pool1
I0521 04:05:33.726083  7754 net.cpp:106] Creating Layer pool1
I0521 04:05:33.726094  7754 net.cpp:454] pool1 <- conv1
I0521 04:05:33.726106  7754 net.cpp:411] pool1 -> pool1
I0521 04:05:33.726186  7754 net.cpp:150] Setting up pool1
I0521 04:05:33.726200  7754 net.cpp:157] Top shape: 630 12 60 48 (21772800)
I0521 04:05:33.726210  7754 net.cpp:165] Memory required for data: 451460520
I0521 04:05:33.726220  7754 layer_factory.hpp:77] Creating layer conv2
I0521 04:05:33.726243  7754 net.cpp:106] Creating Layer conv2
I0521 04:05:33.726253  7754 net.cpp:454] conv2 <- pool1
I0521 04:05:33.726267  7754 net.cpp:411] conv2 -> conv2
I0521 04:05:33.728958  7754 net.cpp:150] Setting up conv2
I0521 04:05:33.728986  7754 net.cpp:157] Top shape: 630 20 54 46 (31298400)
I0521 04:05:33.728997  7754 net.cpp:165] Memory required for data: 576654120
I0521 04:05:33.729017  7754 layer_factory.hpp:77] Creating layer relu2
I0521 04:05:33.729032  7754 net.cpp:106] Creating Layer relu2
I0521 04:05:33.729041  7754 net.cpp:454] relu2 <- conv2
I0521 04:05:33.729054  7754 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:05:33.729384  7754 net.cpp:150] Setting up relu2
I0521 04:05:33.729399  7754 net.cpp:157] Top shape: 630 20 54 46 (31298400)
I0521 04:05:33.729409  7754 net.cpp:165] Memory required for data: 701847720
I0521 04:05:33.729420  7754 layer_factory.hpp:77] Creating layer pool2
I0521 04:05:33.729434  7754 net.cpp:106] Creating Layer pool2
I0521 04:05:33.729444  7754 net.cpp:454] pool2 <- conv2
I0521 04:05:33.729468  7754 net.cpp:411] pool2 -> pool2
I0521 04:05:33.729537  7754 net.cpp:150] Setting up pool2
I0521 04:05:33.729550  7754 net.cpp:157] Top shape: 630 20 27 46 (15649200)
I0521 04:05:33.729560  7754 net.cpp:165] Memory required for data: 764444520
I0521 04:05:33.729570  7754 layer_factory.hpp:77] Creating layer conv3
I0521 04:05:33.729588  7754 net.cpp:106] Creating Layer conv3
I0521 04:05:33.729599  7754 net.cpp:454] conv3 <- pool2
I0521 04:05:33.729612  7754 net.cpp:411] conv3 -> conv3
I0521 04:05:33.731530  7754 net.cpp:150] Setting up conv3
I0521 04:05:33.731554  7754 net.cpp:157] Top shape: 630 28 22 44 (17075520)
I0521 04:05:33.731566  7754 net.cpp:165] Memory required for data: 832746600
I0521 04:05:33.731585  7754 layer_factory.hpp:77] Creating layer relu3
I0521 04:05:33.731600  7754 net.cpp:106] Creating Layer relu3
I0521 04:05:33.731611  7754 net.cpp:454] relu3 <- conv3
I0521 04:05:33.731622  7754 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:05:33.732091  7754 net.cpp:150] Setting up relu3
I0521 04:05:33.732110  7754 net.cpp:157] Top shape: 630 28 22 44 (17075520)
I0521 04:05:33.732120  7754 net.cpp:165] Memory required for data: 901048680
I0521 04:05:33.732130  7754 layer_factory.hpp:77] Creating layer pool3
I0521 04:05:33.732142  7754 net.cpp:106] Creating Layer pool3
I0521 04:05:33.732152  7754 net.cpp:454] pool3 <- conv3
I0521 04:05:33.732166  7754 net.cpp:411] pool3 -> pool3
I0521 04:05:33.732233  7754 net.cpp:150] Setting up pool3
I0521 04:05:33.732246  7754 net.cpp:157] Top shape: 630 28 11 44 (8537760)
I0521 04:05:33.732256  7754 net.cpp:165] Memory required for data: 935199720
I0521 04:05:33.732266  7754 layer_factory.hpp:77] Creating layer conv4
I0521 04:05:33.732283  7754 net.cpp:106] Creating Layer conv4
I0521 04:05:33.732295  7754 net.cpp:454] conv4 <- pool3
I0521 04:05:33.732308  7754 net.cpp:411] conv4 -> conv4
I0521 04:05:33.735096  7754 net.cpp:150] Setting up conv4
I0521 04:05:33.735124  7754 net.cpp:157] Top shape: 630 36 6 42 (5715360)
I0521 04:05:33.735136  7754 net.cpp:165] Memory required for data: 958061160
I0521 04:05:33.735152  7754 layer_factory.hpp:77] Creating layer relu4
I0521 04:05:33.735165  7754 net.cpp:106] Creating Layer relu4
I0521 04:05:33.735177  7754 net.cpp:454] relu4 <- conv4
I0521 04:05:33.735189  7754 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:05:33.735669  7754 net.cpp:150] Setting up relu4
I0521 04:05:33.735685  7754 net.cpp:157] Top shape: 630 36 6 42 (5715360)
I0521 04:05:33.735697  7754 net.cpp:165] Memory required for data: 980922600
I0521 04:05:33.735707  7754 layer_factory.hpp:77] Creating layer pool4
I0521 04:05:33.735719  7754 net.cpp:106] Creating Layer pool4
I0521 04:05:33.735729  7754 net.cpp:454] pool4 <- conv4
I0521 04:05:33.735743  7754 net.cpp:411] pool4 -> pool4
I0521 04:05:33.735810  7754 net.cpp:150] Setting up pool4
I0521 04:05:33.735824  7754 net.cpp:157] Top shape: 630 36 3 42 (2857680)
I0521 04:05:33.735836  7754 net.cpp:165] Memory required for data: 992353320
I0521 04:05:33.735844  7754 layer_factory.hpp:77] Creating layer ip1
I0521 04:05:33.735862  7754 net.cpp:106] Creating Layer ip1
I0521 04:05:33.735873  7754 net.cpp:454] ip1 <- pool4
I0521 04:05:33.735887  7754 net.cpp:411] ip1 -> ip1
I0521 04:05:33.751457  7754 net.cpp:150] Setting up ip1
I0521 04:05:33.751484  7754 net.cpp:157] Top shape: 630 196 (123480)
I0521 04:05:33.751502  7754 net.cpp:165] Memory required for data: 992847240
I0521 04:05:33.751526  7754 layer_factory.hpp:77] Creating layer relu5
I0521 04:05:33.751541  7754 net.cpp:106] Creating Layer relu5
I0521 04:05:33.751552  7754 net.cpp:454] relu5 <- ip1
I0521 04:05:33.751565  7754 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:05:33.751909  7754 net.cpp:150] Setting up relu5
I0521 04:05:33.751924  7754 net.cpp:157] Top shape: 630 196 (123480)
I0521 04:05:33.751935  7754 net.cpp:165] Memory required for data: 993341160
I0521 04:05:33.751945  7754 layer_factory.hpp:77] Creating layer drop1
I0521 04:05:33.751966  7754 net.cpp:106] Creating Layer drop1
I0521 04:05:33.751976  7754 net.cpp:454] drop1 <- ip1
I0521 04:05:33.752002  7754 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:05:33.752048  7754 net.cpp:150] Setting up drop1
I0521 04:05:33.752061  7754 net.cpp:157] Top shape: 630 196 (123480)
I0521 04:05:33.752069  7754 net.cpp:165] Memory required for data: 993835080
I0521 04:05:33.752079  7754 layer_factory.hpp:77] Creating layer ip2
I0521 04:05:33.752097  7754 net.cpp:106] Creating Layer ip2
I0521 04:05:33.752107  7754 net.cpp:454] ip2 <- ip1
I0521 04:05:33.752120  7754 net.cpp:411] ip2 -> ip2
I0521 04:05:33.752586  7754 net.cpp:150] Setting up ip2
I0521 04:05:33.752600  7754 net.cpp:157] Top shape: 630 98 (61740)
I0521 04:05:33.752610  7754 net.cpp:165] Memory required for data: 994082040
I0521 04:05:33.752625  7754 layer_factory.hpp:77] Creating layer relu6
I0521 04:05:33.752637  7754 net.cpp:106] Creating Layer relu6
I0521 04:05:33.752647  7754 net.cpp:454] relu6 <- ip2
I0521 04:05:33.752658  7754 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:05:33.753172  7754 net.cpp:150] Setting up relu6
I0521 04:05:33.753190  7754 net.cpp:157] Top shape: 630 98 (61740)
I0521 04:05:33.753201  7754 net.cpp:165] Memory required for data: 994329000
I0521 04:05:33.753211  7754 layer_factory.hpp:77] Creating layer drop2
I0521 04:05:33.753223  7754 net.cpp:106] Creating Layer drop2
I0521 04:05:33.753233  7754 net.cpp:454] drop2 <- ip2
I0521 04:05:33.753245  7754 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:05:33.753286  7754 net.cpp:150] Setting up drop2
I0521 04:05:33.753300  7754 net.cpp:157] Top shape: 630 98 (61740)
I0521 04:05:33.753310  7754 net.cpp:165] Memory required for data: 994575960
I0521 04:05:33.753320  7754 layer_factory.hpp:77] Creating layer ip3
I0521 04:05:33.753334  7754 net.cpp:106] Creating Layer ip3
I0521 04:05:33.753343  7754 net.cpp:454] ip3 <- ip2
I0521 04:05:33.753355  7754 net.cpp:411] ip3 -> ip3
I0521 04:05:33.753564  7754 net.cpp:150] Setting up ip3
I0521 04:05:33.753577  7754 net.cpp:157] Top shape: 630 11 (6930)
I0521 04:05:33.753587  7754 net.cpp:165] Memory required for data: 994603680
I0521 04:05:33.753602  7754 layer_factory.hpp:77] Creating layer drop3
I0521 04:05:33.753613  7754 net.cpp:106] Creating Layer drop3
I0521 04:05:33.753623  7754 net.cpp:454] drop3 <- ip3
I0521 04:05:33.753635  7754 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:05:33.753674  7754 net.cpp:150] Setting up drop3
I0521 04:05:33.753686  7754 net.cpp:157] Top shape: 630 11 (6930)
I0521 04:05:33.753696  7754 net.cpp:165] Memory required for data: 994631400
I0521 04:05:33.753705  7754 layer_factory.hpp:77] Creating layer loss
I0521 04:05:33.753726  7754 net.cpp:106] Creating Layer loss
I0521 04:05:33.753736  7754 net.cpp:454] loss <- ip3
I0521 04:05:33.753746  7754 net.cpp:454] loss <- label
I0521 04:05:33.753758  7754 net.cpp:411] loss -> loss
I0521 04:05:33.753775  7754 layer_factory.hpp:77] Creating layer loss
I0521 04:05:33.754426  7754 net.cpp:150] Setting up loss
I0521 04:05:33.754446  7754 net.cpp:157] Top shape: (1)
I0521 04:05:33.754459  7754 net.cpp:160]     with loss weight 1
I0521 04:05:33.754501  7754 net.cpp:165] Memory required for data: 994631404
I0521 04:05:33.754511  7754 net.cpp:226] loss needs backward computation.
I0521 04:05:33.754523  7754 net.cpp:226] drop3 needs backward computation.
I0521 04:05:33.754533  7754 net.cpp:226] ip3 needs backward computation.
I0521 04:05:33.754544  7754 net.cpp:226] drop2 needs backward computation.
I0521 04:05:33.754552  7754 net.cpp:226] relu6 needs backward computation.
I0521 04:05:33.754562  7754 net.cpp:226] ip2 needs backward computation.
I0521 04:05:33.754570  7754 net.cpp:226] drop1 needs backward computation.
I0521 04:05:33.754580  7754 net.cpp:226] relu5 needs backward computation.
I0521 04:05:33.754590  7754 net.cpp:226] ip1 needs backward computation.
I0521 04:05:33.754601  7754 net.cpp:226] pool4 needs backward computation.
I0521 04:05:33.754611  7754 net.cpp:226] relu4 needs backward computation.
I0521 04:05:33.754621  7754 net.cpp:226] conv4 needs backward computation.
I0521 04:05:33.754631  7754 net.cpp:226] pool3 needs backward computation.
I0521 04:05:33.754649  7754 net.cpp:226] relu3 needs backward computation.
I0521 04:05:33.754660  7754 net.cpp:226] conv3 needs backward computation.
I0521 04:05:33.754672  7754 net.cpp:226] pool2 needs backward computation.
I0521 04:05:33.754683  7754 net.cpp:226] relu2 needs backward computation.
I0521 04:05:33.754693  7754 net.cpp:226] conv2 needs backward computation.
I0521 04:05:33.754704  7754 net.cpp:226] pool1 needs backward computation.
I0521 04:05:33.754714  7754 net.cpp:226] relu1 needs backward computation.
I0521 04:05:33.754724  7754 net.cpp:226] conv1 needs backward computation.
I0521 04:05:33.754735  7754 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:05:33.754745  7754 net.cpp:270] This network produces output loss
I0521 04:05:33.754770  7754 net.cpp:283] Network initialization done.
I0521 04:05:33.756345  7754 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130.prototxt
I0521 04:05:33.756417  7754 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 04:05:33.756769  7754 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 630
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:05:33.756959  7754 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:05:33.756974  7754 net.cpp:106] Creating Layer data_hdf5
I0521 04:05:33.756988  7754 net.cpp:411] data_hdf5 -> data
I0521 04:05:33.757004  7754 net.cpp:411] data_hdf5 -> label
I0521 04:05:33.757020  7754 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 04:05:33.758218  7754 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 04:05:55.051326  7754 net.cpp:150] Setting up data_hdf5
I0521 04:05:55.051493  7754 net.cpp:157] Top shape: 630 1 127 50 (4000500)
I0521 04:05:55.051508  7754 net.cpp:157] Top shape: 630 (630)
I0521 04:05:55.051520  7754 net.cpp:165] Memory required for data: 16004520
I0521 04:05:55.051533  7754 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 04:05:55.051563  7754 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 04:05:55.051573  7754 net.cpp:454] label_data_hdf5_1_split <- label
I0521 04:05:55.051589  7754 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 04:05:55.051610  7754 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 04:05:55.051683  7754 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 04:05:55.051697  7754 net.cpp:157] Top shape: 630 (630)
I0521 04:05:55.051709  7754 net.cpp:157] Top shape: 630 (630)
I0521 04:05:55.051718  7754 net.cpp:165] Memory required for data: 16009560
I0521 04:05:55.051729  7754 layer_factory.hpp:77] Creating layer conv1
I0521 04:05:55.051751  7754 net.cpp:106] Creating Layer conv1
I0521 04:05:55.051762  7754 net.cpp:454] conv1 <- data
I0521 04:05:55.051777  7754 net.cpp:411] conv1 -> conv1
I0521 04:05:55.053737  7754 net.cpp:150] Setting up conv1
I0521 04:05:55.053762  7754 net.cpp:157] Top shape: 630 12 120 48 (43545600)
I0521 04:05:55.053774  7754 net.cpp:165] Memory required for data: 190191960
I0521 04:05:55.053794  7754 layer_factory.hpp:77] Creating layer relu1
I0521 04:05:55.053809  7754 net.cpp:106] Creating Layer relu1
I0521 04:05:55.053820  7754 net.cpp:454] relu1 <- conv1
I0521 04:05:55.053833  7754 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:05:55.054332  7754 net.cpp:150] Setting up relu1
I0521 04:05:55.054348  7754 net.cpp:157] Top shape: 630 12 120 48 (43545600)
I0521 04:05:55.054359  7754 net.cpp:165] Memory required for data: 364374360
I0521 04:05:55.054369  7754 layer_factory.hpp:77] Creating layer pool1
I0521 04:05:55.054386  7754 net.cpp:106] Creating Layer pool1
I0521 04:05:55.054396  7754 net.cpp:454] pool1 <- conv1
I0521 04:05:55.054409  7754 net.cpp:411] pool1 -> pool1
I0521 04:05:55.054484  7754 net.cpp:150] Setting up pool1
I0521 04:05:55.054498  7754 net.cpp:157] Top shape: 630 12 60 48 (21772800)
I0521 04:05:55.054508  7754 net.cpp:165] Memory required for data: 451465560
I0521 04:05:55.054518  7754 layer_factory.hpp:77] Creating layer conv2
I0521 04:05:55.054536  7754 net.cpp:106] Creating Layer conv2
I0521 04:05:55.054548  7754 net.cpp:454] conv2 <- pool1
I0521 04:05:55.054561  7754 net.cpp:411] conv2 -> conv2
I0521 04:05:55.056485  7754 net.cpp:150] Setting up conv2
I0521 04:05:55.056507  7754 net.cpp:157] Top shape: 630 20 54 46 (31298400)
I0521 04:05:55.056520  7754 net.cpp:165] Memory required for data: 576659160
I0521 04:05:55.056537  7754 layer_factory.hpp:77] Creating layer relu2
I0521 04:05:55.056551  7754 net.cpp:106] Creating Layer relu2
I0521 04:05:55.056561  7754 net.cpp:454] relu2 <- conv2
I0521 04:05:55.056573  7754 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:05:55.056905  7754 net.cpp:150] Setting up relu2
I0521 04:05:55.056920  7754 net.cpp:157] Top shape: 630 20 54 46 (31298400)
I0521 04:05:55.056929  7754 net.cpp:165] Memory required for data: 701852760
I0521 04:05:55.056939  7754 layer_factory.hpp:77] Creating layer pool2
I0521 04:05:55.056953  7754 net.cpp:106] Creating Layer pool2
I0521 04:05:55.056963  7754 net.cpp:454] pool2 <- conv2
I0521 04:05:55.056975  7754 net.cpp:411] pool2 -> pool2
I0521 04:05:55.057046  7754 net.cpp:150] Setting up pool2
I0521 04:05:55.057060  7754 net.cpp:157] Top shape: 630 20 27 46 (15649200)
I0521 04:05:55.057070  7754 net.cpp:165] Memory required for data: 764449560
I0521 04:05:55.057080  7754 layer_factory.hpp:77] Creating layer conv3
I0521 04:05:55.057097  7754 net.cpp:106] Creating Layer conv3
I0521 04:05:55.057108  7754 net.cpp:454] conv3 <- pool2
I0521 04:05:55.057122  7754 net.cpp:411] conv3 -> conv3
I0521 04:05:55.059080  7754 net.cpp:150] Setting up conv3
I0521 04:05:55.059103  7754 net.cpp:157] Top shape: 630 28 22 44 (17075520)
I0521 04:05:55.059115  7754 net.cpp:165] Memory required for data: 832751640
I0521 04:05:55.059149  7754 layer_factory.hpp:77] Creating layer relu3
I0521 04:05:55.059161  7754 net.cpp:106] Creating Layer relu3
I0521 04:05:55.059172  7754 net.cpp:454] relu3 <- conv3
I0521 04:05:55.059185  7754 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:05:55.059666  7754 net.cpp:150] Setting up relu3
I0521 04:05:55.059682  7754 net.cpp:157] Top shape: 630 28 22 44 (17075520)
I0521 04:05:55.059694  7754 net.cpp:165] Memory required for data: 901053720
I0521 04:05:55.059703  7754 layer_factory.hpp:77] Creating layer pool3
I0521 04:05:55.059716  7754 net.cpp:106] Creating Layer pool3
I0521 04:05:55.059726  7754 net.cpp:454] pool3 <- conv3
I0521 04:05:55.059739  7754 net.cpp:411] pool3 -> pool3
I0521 04:05:55.059811  7754 net.cpp:150] Setting up pool3
I0521 04:05:55.059825  7754 net.cpp:157] Top shape: 630 28 11 44 (8537760)
I0521 04:05:55.059835  7754 net.cpp:165] Memory required for data: 935204760
I0521 04:05:55.059846  7754 layer_factory.hpp:77] Creating layer conv4
I0521 04:05:55.059864  7754 net.cpp:106] Creating Layer conv4
I0521 04:05:55.059873  7754 net.cpp:454] conv4 <- pool3
I0521 04:05:55.059888  7754 net.cpp:411] conv4 -> conv4
I0521 04:05:55.061939  7754 net.cpp:150] Setting up conv4
I0521 04:05:55.061962  7754 net.cpp:157] Top shape: 630 36 6 42 (5715360)
I0521 04:05:55.061975  7754 net.cpp:165] Memory required for data: 958066200
I0521 04:05:55.061990  7754 layer_factory.hpp:77] Creating layer relu4
I0521 04:05:55.062003  7754 net.cpp:106] Creating Layer relu4
I0521 04:05:55.062013  7754 net.cpp:454] relu4 <- conv4
I0521 04:05:55.062026  7754 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:05:55.062494  7754 net.cpp:150] Setting up relu4
I0521 04:05:55.062510  7754 net.cpp:157] Top shape: 630 36 6 42 (5715360)
I0521 04:05:55.062521  7754 net.cpp:165] Memory required for data: 980927640
I0521 04:05:55.062531  7754 layer_factory.hpp:77] Creating layer pool4
I0521 04:05:55.062544  7754 net.cpp:106] Creating Layer pool4
I0521 04:05:55.062554  7754 net.cpp:454] pool4 <- conv4
I0521 04:05:55.062567  7754 net.cpp:411] pool4 -> pool4
I0521 04:05:55.062639  7754 net.cpp:150] Setting up pool4
I0521 04:05:55.062652  7754 net.cpp:157] Top shape: 630 36 3 42 (2857680)
I0521 04:05:55.062662  7754 net.cpp:165] Memory required for data: 992358360
I0521 04:05:55.062674  7754 layer_factory.hpp:77] Creating layer ip1
I0521 04:05:55.062688  7754 net.cpp:106] Creating Layer ip1
I0521 04:05:55.062698  7754 net.cpp:454] ip1 <- pool4
I0521 04:05:55.062711  7754 net.cpp:411] ip1 -> ip1
I0521 04:05:55.078222  7754 net.cpp:150] Setting up ip1
I0521 04:05:55.078249  7754 net.cpp:157] Top shape: 630 196 (123480)
I0521 04:05:55.078266  7754 net.cpp:165] Memory required for data: 992852280
I0521 04:05:55.078289  7754 layer_factory.hpp:77] Creating layer relu5
I0521 04:05:55.078304  7754 net.cpp:106] Creating Layer relu5
I0521 04:05:55.078315  7754 net.cpp:454] relu5 <- ip1
I0521 04:05:55.078332  7754 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:05:55.078678  7754 net.cpp:150] Setting up relu5
I0521 04:05:55.078692  7754 net.cpp:157] Top shape: 630 196 (123480)
I0521 04:05:55.078702  7754 net.cpp:165] Memory required for data: 993346200
I0521 04:05:55.078712  7754 layer_factory.hpp:77] Creating layer drop1
I0521 04:05:55.078732  7754 net.cpp:106] Creating Layer drop1
I0521 04:05:55.078742  7754 net.cpp:454] drop1 <- ip1
I0521 04:05:55.078754  7754 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:05:55.078799  7754 net.cpp:150] Setting up drop1
I0521 04:05:55.078811  7754 net.cpp:157] Top shape: 630 196 (123480)
I0521 04:05:55.078822  7754 net.cpp:165] Memory required for data: 993840120
I0521 04:05:55.078831  7754 layer_factory.hpp:77] Creating layer ip2
I0521 04:05:55.078846  7754 net.cpp:106] Creating Layer ip2
I0521 04:05:55.078855  7754 net.cpp:454] ip2 <- ip1
I0521 04:05:55.078869  7754 net.cpp:411] ip2 -> ip2
I0521 04:05:55.079357  7754 net.cpp:150] Setting up ip2
I0521 04:05:55.079370  7754 net.cpp:157] Top shape: 630 98 (61740)
I0521 04:05:55.079380  7754 net.cpp:165] Memory required for data: 994087080
I0521 04:05:55.079409  7754 layer_factory.hpp:77] Creating layer relu6
I0521 04:05:55.079422  7754 net.cpp:106] Creating Layer relu6
I0521 04:05:55.079432  7754 net.cpp:454] relu6 <- ip2
I0521 04:05:55.079444  7754 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:05:55.079980  7754 net.cpp:150] Setting up relu6
I0521 04:05:55.079996  7754 net.cpp:157] Top shape: 630 98 (61740)
I0521 04:05:55.080008  7754 net.cpp:165] Memory required for data: 994334040
I0521 04:05:55.080018  7754 layer_factory.hpp:77] Creating layer drop2
I0521 04:05:55.080030  7754 net.cpp:106] Creating Layer drop2
I0521 04:05:55.080041  7754 net.cpp:454] drop2 <- ip2
I0521 04:05:55.080054  7754 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:05:55.080097  7754 net.cpp:150] Setting up drop2
I0521 04:05:55.080111  7754 net.cpp:157] Top shape: 630 98 (61740)
I0521 04:05:55.080121  7754 net.cpp:165] Memory required for data: 994581000
I0521 04:05:55.080130  7754 layer_factory.hpp:77] Creating layer ip3
I0521 04:05:55.080145  7754 net.cpp:106] Creating Layer ip3
I0521 04:05:55.080155  7754 net.cpp:454] ip3 <- ip2
I0521 04:05:55.080168  7754 net.cpp:411] ip3 -> ip3
I0521 04:05:55.080391  7754 net.cpp:150] Setting up ip3
I0521 04:05:55.080404  7754 net.cpp:157] Top shape: 630 11 (6930)
I0521 04:05:55.080415  7754 net.cpp:165] Memory required for data: 994608720
I0521 04:05:55.080430  7754 layer_factory.hpp:77] Creating layer drop3
I0521 04:05:55.080443  7754 net.cpp:106] Creating Layer drop3
I0521 04:05:55.080453  7754 net.cpp:454] drop3 <- ip3
I0521 04:05:55.080466  7754 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:05:55.080507  7754 net.cpp:150] Setting up drop3
I0521 04:05:55.080520  7754 net.cpp:157] Top shape: 630 11 (6930)
I0521 04:05:55.080530  7754 net.cpp:165] Memory required for data: 994636440
I0521 04:05:55.080539  7754 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 04:05:55.080552  7754 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 04:05:55.080562  7754 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 04:05:55.080574  7754 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 04:05:55.080590  7754 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 04:05:55.080662  7754 net.cpp:150] Setting up ip3_drop3_0_split
I0521 04:05:55.080675  7754 net.cpp:157] Top shape: 630 11 (6930)
I0521 04:05:55.080687  7754 net.cpp:157] Top shape: 630 11 (6930)
I0521 04:05:55.080698  7754 net.cpp:165] Memory required for data: 994691880
I0521 04:05:55.080708  7754 layer_factory.hpp:77] Creating layer accuracy
I0521 04:05:55.080729  7754 net.cpp:106] Creating Layer accuracy
I0521 04:05:55.080739  7754 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 04:05:55.080750  7754 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 04:05:55.080765  7754 net.cpp:411] accuracy -> accuracy
I0521 04:05:55.080787  7754 net.cpp:150] Setting up accuracy
I0521 04:05:55.080799  7754 net.cpp:157] Top shape: (1)
I0521 04:05:55.080809  7754 net.cpp:165] Memory required for data: 994691884
I0521 04:05:55.080819  7754 layer_factory.hpp:77] Creating layer loss
I0521 04:05:55.080832  7754 net.cpp:106] Creating Layer loss
I0521 04:05:55.080842  7754 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 04:05:55.080852  7754 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 04:05:55.080867  7754 net.cpp:411] loss -> loss
I0521 04:05:55.080883  7754 layer_factory.hpp:77] Creating layer loss
I0521 04:05:55.081377  7754 net.cpp:150] Setting up loss
I0521 04:05:55.081390  7754 net.cpp:157] Top shape: (1)
I0521 04:05:55.081400  7754 net.cpp:160]     with loss weight 1
I0521 04:05:55.081418  7754 net.cpp:165] Memory required for data: 994691888
I0521 04:05:55.081428  7754 net.cpp:226] loss needs backward computation.
I0521 04:05:55.081439  7754 net.cpp:228] accuracy does not need backward computation.
I0521 04:05:55.081450  7754 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 04:05:55.081460  7754 net.cpp:226] drop3 needs backward computation.
I0521 04:05:55.081471  7754 net.cpp:226] ip3 needs backward computation.
I0521 04:05:55.081485  7754 net.cpp:226] drop2 needs backward computation.
I0521 04:05:55.081503  7754 net.cpp:226] relu6 needs backward computation.
I0521 04:05:55.081513  7754 net.cpp:226] ip2 needs backward computation.
I0521 04:05:55.081523  7754 net.cpp:226] drop1 needs backward computation.
I0521 04:05:55.081532  7754 net.cpp:226] relu5 needs backward computation.
I0521 04:05:55.081542  7754 net.cpp:226] ip1 needs backward computation.
I0521 04:05:55.081552  7754 net.cpp:226] pool4 needs backward computation.
I0521 04:05:55.081562  7754 net.cpp:226] relu4 needs backward computation.
I0521 04:05:55.081573  7754 net.cpp:226] conv4 needs backward computation.
I0521 04:05:55.081583  7754 net.cpp:226] pool3 needs backward computation.
I0521 04:05:55.081593  7754 net.cpp:226] relu3 needs backward computation.
I0521 04:05:55.081604  7754 net.cpp:226] conv3 needs backward computation.
I0521 04:05:55.081614  7754 net.cpp:226] pool2 needs backward computation.
I0521 04:05:55.081624  7754 net.cpp:226] relu2 needs backward computation.
I0521 04:05:55.081634  7754 net.cpp:226] conv2 needs backward computation.
I0521 04:05:55.081645  7754 net.cpp:226] pool1 needs backward computation.
I0521 04:05:55.081655  7754 net.cpp:226] relu1 needs backward computation.
I0521 04:05:55.081665  7754 net.cpp:226] conv1 needs backward computation.
I0521 04:05:55.081676  7754 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 04:05:55.081687  7754 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:05:55.081699  7754 net.cpp:270] This network produces output accuracy
I0521 04:05:55.081709  7754 net.cpp:270] This network produces output loss
I0521 04:05:55.081737  7754 net.cpp:283] Network initialization done.
I0521 04:05:55.081871  7754 solver.cpp:60] Solver scaffolding done.
I0521 04:05:55.083004  7754 caffe.cpp:212] Starting Optimization
I0521 04:05:55.083022  7754 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 04:05:55.083036  7754 solver.cpp:289] Learning Rate Policy: fixed
I0521 04:05:55.084259  7754 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 04:06:41.019338  7754 solver.cpp:409]     Test net output #0: accuracy = 0.116046
I0521 04:06:41.019500  7754 solver.cpp:409]     Test net output #1: loss = 2.39802 (* 1 = 2.39802 loss)
I0521 04:06:41.139581  7754 solver.cpp:237] Iteration 0, loss = 2.39884
I0521 04:06:41.139616  7754 solver.cpp:253]     Train net output #0: loss = 2.39884 (* 1 = 2.39884 loss)
I0521 04:06:41.139634  7754 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 04:06:48.920168  7754 solver.cpp:237] Iteration 23, loss = 2.38408
I0521 04:06:48.920204  7754 solver.cpp:253]     Train net output #0: loss = 2.38408 (* 1 = 2.38408 loss)
I0521 04:06:48.920222  7754 sgd_solver.cpp:106] Iteration 23, lr = 0.0025
I0521 04:06:56.695821  7754 solver.cpp:237] Iteration 46, loss = 2.37134
I0521 04:06:56.695866  7754 solver.cpp:253]     Train net output #0: loss = 2.37134 (* 1 = 2.37134 loss)
I0521 04:06:56.695883  7754 sgd_solver.cpp:106] Iteration 46, lr = 0.0025
I0521 04:07:04.470746  7754 solver.cpp:237] Iteration 69, loss = 2.34867
I0521 04:07:04.470778  7754 solver.cpp:253]     Train net output #0: loss = 2.34867 (* 1 = 2.34867 loss)
I0521 04:07:04.470794  7754 sgd_solver.cpp:106] Iteration 69, lr = 0.0025
I0521 04:07:12.247498  7754 solver.cpp:237] Iteration 92, loss = 2.3355
I0521 04:07:12.247642  7754 solver.cpp:253]     Train net output #0: loss = 2.3355 (* 1 = 2.3355 loss)
I0521 04:07:12.247655  7754 sgd_solver.cpp:106] Iteration 92, lr = 0.0025
I0521 04:07:20.021391  7754 solver.cpp:237] Iteration 115, loss = 2.34366
I0521 04:07:20.021423  7754 solver.cpp:253]     Train net output #0: loss = 2.34366 (* 1 = 2.34366 loss)
I0521 04:07:20.021440  7754 sgd_solver.cpp:106] Iteration 115, lr = 0.0025
I0521 04:07:27.797657  7754 solver.cpp:237] Iteration 138, loss = 2.32508
I0521 04:07:27.797701  7754 solver.cpp:253]     Train net output #0: loss = 2.32508 (* 1 = 2.32508 loss)
I0521 04:07:27.797718  7754 sgd_solver.cpp:106] Iteration 138, lr = 0.0025
I0521 04:07:57.659314  7754 solver.cpp:237] Iteration 161, loss = 2.31979
I0521 04:07:57.659476  7754 solver.cpp:253]     Train net output #0: loss = 2.31979 (* 1 = 2.31979 loss)
I0521 04:07:57.659490  7754 sgd_solver.cpp:106] Iteration 161, lr = 0.0025
I0521 04:08:05.434716  7754 solver.cpp:237] Iteration 184, loss = 2.31627
I0521 04:08:05.434749  7754 solver.cpp:253]     Train net output #0: loss = 2.31627 (* 1 = 2.31627 loss)
I0521 04:08:05.434767  7754 sgd_solver.cpp:106] Iteration 184, lr = 0.0025
I0521 04:08:13.219214  7754 solver.cpp:237] Iteration 207, loss = 2.30464
I0521 04:08:13.219256  7754 solver.cpp:253]     Train net output #0: loss = 2.30464 (* 1 = 2.30464 loss)
I0521 04:08:13.219276  7754 sgd_solver.cpp:106] Iteration 207, lr = 0.0025
I0521 04:08:20.996953  7754 solver.cpp:237] Iteration 230, loss = 2.28604
I0521 04:08:20.996986  7754 solver.cpp:253]     Train net output #0: loss = 2.28604 (* 1 = 2.28604 loss)
I0521 04:08:20.997004  7754 sgd_solver.cpp:106] Iteration 230, lr = 0.0025
I0521 04:08:23.365456  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_238.caffemodel
I0521 04:08:23.644604  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_238.solverstate
I0521 04:08:28.844316  7754 solver.cpp:237] Iteration 253, loss = 2.26684
I0521 04:08:28.844465  7754 solver.cpp:253]     Train net output #0: loss = 2.26684 (* 1 = 2.26684 loss)
I0521 04:08:28.844480  7754 sgd_solver.cpp:106] Iteration 253, lr = 0.0025
I0521 04:08:36.620363  7754 solver.cpp:237] Iteration 276, loss = 2.26807
I0521 04:08:36.620409  7754 solver.cpp:253]     Train net output #0: loss = 2.26807 (* 1 = 2.26807 loss)
I0521 04:08:36.620424  7754 sgd_solver.cpp:106] Iteration 276, lr = 0.0025
I0521 04:08:44.401329  7754 solver.cpp:237] Iteration 299, loss = 2.18781
I0521 04:08:44.401358  7754 solver.cpp:253]     Train net output #0: loss = 2.18781 (* 1 = 2.18781 loss)
I0521 04:08:44.401371  7754 sgd_solver.cpp:106] Iteration 299, lr = 0.0025
I0521 04:09:14.285540  7754 solver.cpp:237] Iteration 322, loss = 2.13454
I0521 04:09:14.285694  7754 solver.cpp:253]     Train net output #0: loss = 2.13454 (* 1 = 2.13454 loss)
I0521 04:09:14.285708  7754 sgd_solver.cpp:106] Iteration 322, lr = 0.0025
I0521 04:09:22.062881  7754 solver.cpp:237] Iteration 345, loss = 2.15152
I0521 04:09:22.062916  7754 solver.cpp:253]     Train net output #0: loss = 2.15152 (* 1 = 2.15152 loss)
I0521 04:09:22.062932  7754 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 04:09:29.846230  7754 solver.cpp:237] Iteration 368, loss = 2.10021
I0521 04:09:29.846276  7754 solver.cpp:253]     Train net output #0: loss = 2.10021 (* 1 = 2.10021 loss)
I0521 04:09:29.846290  7754 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0521 04:09:37.624069  7754 solver.cpp:237] Iteration 391, loss = 2.08226
I0521 04:09:37.624104  7754 solver.cpp:253]     Train net output #0: loss = 2.08226 (* 1 = 2.08226 loss)
I0521 04:09:37.624120  7754 sgd_solver.cpp:106] Iteration 391, lr = 0.0025
I0521 04:09:45.403905  7754 solver.cpp:237] Iteration 414, loss = 2.04188
I0521 04:09:45.404050  7754 solver.cpp:253]     Train net output #0: loss = 2.04188 (* 1 = 2.04188 loss)
I0521 04:09:45.404064  7754 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0521 04:09:53.183003  7754 solver.cpp:237] Iteration 437, loss = 1.96653
I0521 04:09:53.183040  7754 solver.cpp:253]     Train net output #0: loss = 1.96653 (* 1 = 1.96653 loss)
I0521 04:09:53.183063  7754 sgd_solver.cpp:106] Iteration 437, lr = 0.0025
I0521 04:10:00.962883  7754 solver.cpp:237] Iteration 460, loss = 2.07877
I0521 04:10:00.962918  7754 solver.cpp:253]     Train net output #0: loss = 2.07877 (* 1 = 2.07877 loss)
I0521 04:10:00.962931  7754 sgd_solver.cpp:106] Iteration 460, lr = 0.0025
I0521 04:10:06.035948  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_476.caffemodel
I0521 04:10:06.310869  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_476.solverstate
I0521 04:10:06.335783  7754 solver.cpp:341] Iteration 476, Testing net (#0)
I0521 04:10:51.357625  7754 solver.cpp:409]     Test net output #0: accuracy = 0.513759
I0521 04:10:51.357782  7754 solver.cpp:409]     Test net output #1: loss = 1.74783 (* 1 = 1.74783 loss)
I0521 04:11:15.944025  7754 solver.cpp:237] Iteration 483, loss = 1.97926
I0521 04:11:15.944077  7754 solver.cpp:253]     Train net output #0: loss = 1.97926 (* 1 = 1.97926 loss)
I0521 04:11:15.944092  7754 sgd_solver.cpp:106] Iteration 483, lr = 0.0025
I0521 04:11:23.715688  7754 solver.cpp:237] Iteration 506, loss = 1.95101
I0521 04:11:23.715833  7754 solver.cpp:253]     Train net output #0: loss = 1.95101 (* 1 = 1.95101 loss)
I0521 04:11:23.715847  7754 sgd_solver.cpp:106] Iteration 506, lr = 0.0025
I0521 04:11:31.484369  7754 solver.cpp:237] Iteration 529, loss = 1.92108
I0521 04:11:31.484412  7754 solver.cpp:253]     Train net output #0: loss = 1.92108 (* 1 = 1.92108 loss)
I0521 04:11:31.484429  7754 sgd_solver.cpp:106] Iteration 529, lr = 0.0025
I0521 04:11:39.254868  7754 solver.cpp:237] Iteration 552, loss = 1.94513
I0521 04:11:39.254900  7754 solver.cpp:253]     Train net output #0: loss = 1.94513 (* 1 = 1.94513 loss)
I0521 04:11:39.254916  7754 sgd_solver.cpp:106] Iteration 552, lr = 0.0025
I0521 04:11:47.027405  7754 solver.cpp:237] Iteration 575, loss = 1.90759
I0521 04:11:47.027438  7754 solver.cpp:253]     Train net output #0: loss = 1.90759 (* 1 = 1.90759 loss)
I0521 04:11:47.027454  7754 sgd_solver.cpp:106] Iteration 575, lr = 0.0025
I0521 04:11:54.798974  7754 solver.cpp:237] Iteration 598, loss = 1.94812
I0521 04:11:54.799103  7754 solver.cpp:253]     Train net output #0: loss = 1.94812 (* 1 = 1.94812 loss)
I0521 04:11:54.799116  7754 sgd_solver.cpp:106] Iteration 598, lr = 0.0025
I0521 04:12:02.569219  7754 solver.cpp:237] Iteration 621, loss = 1.95876
I0521 04:12:02.569264  7754 solver.cpp:253]     Train net output #0: loss = 1.95876 (* 1 = 1.95876 loss)
I0521 04:12:02.569279  7754 sgd_solver.cpp:106] Iteration 621, lr = 0.0025
I0521 04:12:32.482188  7754 solver.cpp:237] Iteration 644, loss = 1.89151
I0521 04:12:32.482360  7754 solver.cpp:253]     Train net output #0: loss = 1.89151 (* 1 = 1.89151 loss)
I0521 04:12:32.482374  7754 sgd_solver.cpp:106] Iteration 644, lr = 0.0025
I0521 04:12:40.248390  7754 solver.cpp:237] Iteration 667, loss = 1.86015
I0521 04:12:40.248423  7754 solver.cpp:253]     Train net output #0: loss = 1.86015 (* 1 = 1.86015 loss)
I0521 04:12:40.248441  7754 sgd_solver.cpp:106] Iteration 667, lr = 0.0025
I0521 04:12:48.017112  7754 solver.cpp:237] Iteration 690, loss = 1.81111
I0521 04:12:48.017156  7754 solver.cpp:253]     Train net output #0: loss = 1.81111 (* 1 = 1.81111 loss)
I0521 04:12:48.017174  7754 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 04:12:55.788211  7754 solver.cpp:237] Iteration 713, loss = 1.82941
I0521 04:12:55.788244  7754 solver.cpp:253]     Train net output #0: loss = 1.82941 (* 1 = 1.82941 loss)
I0521 04:12:55.788261  7754 sgd_solver.cpp:106] Iteration 713, lr = 0.0025
I0521 04:12:55.788645  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_714.caffemodel
I0521 04:12:56.066447  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_714.solverstate
I0521 04:13:03.622092  7754 solver.cpp:237] Iteration 736, loss = 1.90491
I0521 04:13:03.622251  7754 solver.cpp:253]     Train net output #0: loss = 1.90491 (* 1 = 1.90491 loss)
I0521 04:13:03.622264  7754 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 04:13:11.398634  7754 solver.cpp:237] Iteration 759, loss = 1.83893
I0521 04:13:11.398677  7754 solver.cpp:253]     Train net output #0: loss = 1.83893 (* 1 = 1.83893 loss)
I0521 04:13:11.398694  7754 sgd_solver.cpp:106] Iteration 759, lr = 0.0025
I0521 04:13:19.170167  7754 solver.cpp:237] Iteration 782, loss = 1.88803
I0521 04:13:19.170202  7754 solver.cpp:253]     Train net output #0: loss = 1.88803 (* 1 = 1.88803 loss)
I0521 04:13:19.170219  7754 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0521 04:13:49.079955  7754 solver.cpp:237] Iteration 805, loss = 1.79485
I0521 04:13:49.080114  7754 solver.cpp:253]     Train net output #0: loss = 1.79485 (* 1 = 1.79485 loss)
I0521 04:13:49.080128  7754 sgd_solver.cpp:106] Iteration 805, lr = 0.0025
I0521 04:13:56.849673  7754 solver.cpp:237] Iteration 828, loss = 1.88135
I0521 04:13:56.849704  7754 solver.cpp:253]     Train net output #0: loss = 1.88135 (* 1 = 1.88135 loss)
I0521 04:13:56.849722  7754 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0521 04:14:04.619431  7754 solver.cpp:237] Iteration 851, loss = 1.85976
I0521 04:14:04.619459  7754 solver.cpp:253]     Train net output #0: loss = 1.85976 (* 1 = 1.85976 loss)
I0521 04:14:04.619477  7754 sgd_solver.cpp:106] Iteration 851, lr = 0.0025
I0521 04:14:12.387250  7754 solver.cpp:237] Iteration 874, loss = 1.81228
I0521 04:14:12.387282  7754 solver.cpp:253]     Train net output #0: loss = 1.81228 (* 1 = 1.81228 loss)
I0521 04:14:12.387306  7754 sgd_solver.cpp:106] Iteration 874, lr = 0.0025
I0521 04:14:20.155510  7754 solver.cpp:237] Iteration 897, loss = 1.82078
I0521 04:14:20.155653  7754 solver.cpp:253]     Train net output #0: loss = 1.82078 (* 1 = 1.82078 loss)
I0521 04:14:20.155668  7754 sgd_solver.cpp:106] Iteration 897, lr = 0.0025
I0521 04:14:27.928820  7754 solver.cpp:237] Iteration 920, loss = 1.81536
I0521 04:14:27.928853  7754 solver.cpp:253]     Train net output #0: loss = 1.81536 (* 1 = 1.81536 loss)
I0521 04:14:27.928875  7754 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0521 04:14:35.699951  7754 solver.cpp:237] Iteration 943, loss = 1.7833
I0521 04:14:35.699983  7754 solver.cpp:253]     Train net output #0: loss = 1.7833 (* 1 = 1.7833 loss)
I0521 04:14:35.700002  7754 sgd_solver.cpp:106] Iteration 943, lr = 0.0025
I0521 04:14:38.404386  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_952.caffemodel
I0521 04:14:38.680702  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_952.solverstate
I0521 04:14:38.708305  7754 solver.cpp:341] Iteration 952, Testing net (#0)
I0521 04:15:44.522649  7754 solver.cpp:409]     Test net output #0: accuracy = 0.610751
I0521 04:15:44.522815  7754 solver.cpp:409]     Test net output #1: loss = 1.36905 (* 1 = 1.36905 loss)
I0521 04:16:11.518491  7754 solver.cpp:237] Iteration 966, loss = 1.77112
I0521 04:16:11.518543  7754 solver.cpp:253]     Train net output #0: loss = 1.77112 (* 1 = 1.77112 loss)
I0521 04:16:11.518559  7754 sgd_solver.cpp:106] Iteration 966, lr = 0.0025
I0521 04:16:19.285719  7754 solver.cpp:237] Iteration 989, loss = 1.7802
I0521 04:16:19.285864  7754 solver.cpp:253]     Train net output #0: loss = 1.7802 (* 1 = 1.7802 loss)
I0521 04:16:19.285878  7754 sgd_solver.cpp:106] Iteration 989, lr = 0.0025
I0521 04:16:27.057091  7754 solver.cpp:237] Iteration 1012, loss = 1.79936
I0521 04:16:27.057122  7754 solver.cpp:253]     Train net output #0: loss = 1.79936 (* 1 = 1.79936 loss)
I0521 04:16:27.057142  7754 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0521 04:16:34.825772  7754 solver.cpp:237] Iteration 1035, loss = 1.82185
I0521 04:16:34.825809  7754 solver.cpp:253]     Train net output #0: loss = 1.82185 (* 1 = 1.82185 loss)
I0521 04:16:34.825831  7754 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 04:16:42.597970  7754 solver.cpp:237] Iteration 1058, loss = 1.8421
I0521 04:16:42.598002  7754 solver.cpp:253]     Train net output #0: loss = 1.8421 (* 1 = 1.8421 loss)
I0521 04:16:42.598019  7754 sgd_solver.cpp:106] Iteration 1058, lr = 0.0025
I0521 04:16:50.367002  7754 solver.cpp:237] Iteration 1081, loss = 1.74597
I0521 04:16:50.367136  7754 solver.cpp:253]     Train net output #0: loss = 1.74597 (* 1 = 1.74597 loss)
I0521 04:16:50.367151  7754 sgd_solver.cpp:106] Iteration 1081, lr = 0.0025
I0521 04:16:58.136679  7754 solver.cpp:237] Iteration 1104, loss = 1.6476
I0521 04:16:58.136725  7754 solver.cpp:253]     Train net output #0: loss = 1.6476 (* 1 = 1.6476 loss)
I0521 04:16:58.136739  7754 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 04:17:28.040328  7754 solver.cpp:237] Iteration 1127, loss = 1.70291
I0521 04:17:28.040493  7754 solver.cpp:253]     Train net output #0: loss = 1.70291 (* 1 = 1.70291 loss)
I0521 04:17:28.040508  7754 sgd_solver.cpp:106] Iteration 1127, lr = 0.0025
I0521 04:17:35.805248  7754 solver.cpp:237] Iteration 1150, loss = 1.78276
I0521 04:17:35.805280  7754 solver.cpp:253]     Train net output #0: loss = 1.78276 (* 1 = 1.78276 loss)
I0521 04:17:35.805297  7754 sgd_solver.cpp:106] Iteration 1150, lr = 0.0025
I0521 04:17:43.574550  7754 solver.cpp:237] Iteration 1173, loss = 1.74308
I0521 04:17:43.574584  7754 solver.cpp:253]     Train net output #0: loss = 1.74308 (* 1 = 1.74308 loss)
I0521 04:17:43.574599  7754 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0521 04:17:48.980340  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1190.caffemodel
I0521 04:17:49.257154  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1190.solverstate
I0521 04:17:51.412062  7754 solver.cpp:237] Iteration 1196, loss = 1.85413
I0521 04:17:51.412109  7754 solver.cpp:253]     Train net output #0: loss = 1.85413 (* 1 = 1.85413 loss)
I0521 04:17:51.412127  7754 sgd_solver.cpp:106] Iteration 1196, lr = 0.0025
I0521 04:17:59.179844  7754 solver.cpp:237] Iteration 1219, loss = 1.72601
I0521 04:17:59.179998  7754 solver.cpp:253]     Train net output #0: loss = 1.72601 (* 1 = 1.72601 loss)
I0521 04:17:59.180012  7754 sgd_solver.cpp:106] Iteration 1219, lr = 0.0025
I0521 04:18:06.951935  7754 solver.cpp:237] Iteration 1242, loss = 1.75729
I0521 04:18:06.951967  7754 solver.cpp:253]     Train net output #0: loss = 1.75729 (* 1 = 1.75729 loss)
I0521 04:18:06.951983  7754 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 04:18:14.720949  7754 solver.cpp:237] Iteration 1265, loss = 1.77418
I0521 04:18:14.720988  7754 solver.cpp:253]     Train net output #0: loss = 1.77418 (* 1 = 1.77418 loss)
I0521 04:18:14.721009  7754 sgd_solver.cpp:106] Iteration 1265, lr = 0.0025
I0521 04:18:44.645448  7754 solver.cpp:237] Iteration 1288, loss = 1.75127
I0521 04:18:44.645620  7754 solver.cpp:253]     Train net output #0: loss = 1.75127 (* 1 = 1.75127 loss)
I0521 04:18:44.645634  7754 sgd_solver.cpp:106] Iteration 1288, lr = 0.0025
I0521 04:18:52.415160  7754 solver.cpp:237] Iteration 1311, loss = 1.69046
I0521 04:18:52.415192  7754 solver.cpp:253]     Train net output #0: loss = 1.69046 (* 1 = 1.69046 loss)
I0521 04:18:52.415210  7754 sgd_solver.cpp:106] Iteration 1311, lr = 0.0025
I0521 04:19:00.183192  7754 solver.cpp:237] Iteration 1334, loss = 1.72776
I0521 04:19:00.183225  7754 solver.cpp:253]     Train net output #0: loss = 1.72776 (* 1 = 1.72776 loss)
I0521 04:19:00.183243  7754 sgd_solver.cpp:106] Iteration 1334, lr = 0.0025
I0521 04:19:07.951902  7754 solver.cpp:237] Iteration 1357, loss = 1.61558
I0521 04:19:07.951941  7754 solver.cpp:253]     Train net output #0: loss = 1.61558 (* 1 = 1.61558 loss)
I0521 04:19:07.951962  7754 sgd_solver.cpp:106] Iteration 1357, lr = 0.0025
I0521 04:19:15.716840  7754 solver.cpp:237] Iteration 1380, loss = 1.76905
I0521 04:19:15.716980  7754 solver.cpp:253]     Train net output #0: loss = 1.76905 (* 1 = 1.76905 loss)
I0521 04:19:15.716994  7754 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 04:19:23.482156  7754 solver.cpp:237] Iteration 1403, loss = 1.65581
I0521 04:19:23.482188  7754 solver.cpp:253]     Train net output #0: loss = 1.65581 (* 1 = 1.65581 loss)
I0521 04:19:23.482206  7754 sgd_solver.cpp:106] Iteration 1403, lr = 0.0025
I0521 04:19:31.248916  7754 solver.cpp:237] Iteration 1426, loss = 1.71185
I0521 04:19:31.248960  7754 solver.cpp:253]     Train net output #0: loss = 1.71185 (* 1 = 1.71185 loss)
I0521 04:19:31.248977  7754 sgd_solver.cpp:106] Iteration 1426, lr = 0.0025
I0521 04:19:31.586807  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1428.caffemodel
I0521 04:19:31.861363  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1428.solverstate
I0521 04:19:31.887709  7754 solver.cpp:341] Iteration 1428, Testing net (#0)
I0521 04:20:16.620612  7754 solver.cpp:409]     Test net output #0: accuracy = 0.65691
I0521 04:20:16.620759  7754 solver.cpp:409]     Test net output #1: loss = 1.25723 (* 1 = 1.25723 loss)
I0521 04:20:45.965965  7754 solver.cpp:237] Iteration 1449, loss = 1.83721
I0521 04:20:45.966012  7754 solver.cpp:253]     Train net output #0: loss = 1.83721 (* 1 = 1.83721 loss)
I0521 04:20:45.966030  7754 sgd_solver.cpp:106] Iteration 1449, lr = 0.0025
I0521 04:20:53.734701  7754 solver.cpp:237] Iteration 1472, loss = 1.74949
I0521 04:20:53.734849  7754 solver.cpp:253]     Train net output #0: loss = 1.74949 (* 1 = 1.74949 loss)
I0521 04:20:53.734863  7754 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 04:21:01.506739  7754 solver.cpp:237] Iteration 1495, loss = 1.69748
I0521 04:21:01.506772  7754 solver.cpp:253]     Train net output #0: loss = 1.69748 (* 1 = 1.69748 loss)
I0521 04:21:01.506788  7754 sgd_solver.cpp:106] Iteration 1495, lr = 0.0025
I0521 04:21:09.277539  7754 solver.cpp:237] Iteration 1518, loss = 1.68134
I0521 04:21:09.277585  7754 solver.cpp:253]     Train net output #0: loss = 1.68134 (* 1 = 1.68134 loss)
I0521 04:21:09.277602  7754 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0521 04:21:17.051776  7754 solver.cpp:237] Iteration 1541, loss = 1.70312
I0521 04:21:17.051810  7754 solver.cpp:253]     Train net output #0: loss = 1.70312 (* 1 = 1.70312 loss)
I0521 04:21:17.051822  7754 sgd_solver.cpp:106] Iteration 1541, lr = 0.0025
I0521 04:21:24.823791  7754 solver.cpp:237] Iteration 1564, loss = 1.72494
I0521 04:21:24.823938  7754 solver.cpp:253]     Train net output #0: loss = 1.72494 (* 1 = 1.72494 loss)
I0521 04:21:24.823951  7754 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0521 04:21:54.734441  7754 solver.cpp:237] Iteration 1587, loss = 1.69134
I0521 04:21:54.734493  7754 solver.cpp:253]     Train net output #0: loss = 1.69134 (* 1 = 1.69134 loss)
I0521 04:21:54.734509  7754 sgd_solver.cpp:106] Iteration 1587, lr = 0.0025
I0521 04:22:02.512390  7754 solver.cpp:237] Iteration 1610, loss = 1.66695
I0521 04:22:02.512548  7754 solver.cpp:253]     Train net output #0: loss = 1.66695 (* 1 = 1.66695 loss)
I0521 04:22:02.512562  7754 sgd_solver.cpp:106] Iteration 1610, lr = 0.0025
I0521 04:22:10.286018  7754 solver.cpp:237] Iteration 1633, loss = 1.68226
I0521 04:22:10.286051  7754 solver.cpp:253]     Train net output #0: loss = 1.68226 (* 1 = 1.68226 loss)
I0521 04:22:10.286069  7754 sgd_solver.cpp:106] Iteration 1633, lr = 0.0025
I0521 04:22:18.059746  7754 solver.cpp:237] Iteration 1656, loss = 1.81534
I0521 04:22:18.059780  7754 solver.cpp:253]     Train net output #0: loss = 1.81534 (* 1 = 1.81534 loss)
I0521 04:22:18.059797  7754 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 04:22:21.100569  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1666.caffemodel
I0521 04:22:21.380688  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1666.solverstate
I0521 04:22:25.901284  7754 solver.cpp:237] Iteration 1679, loss = 1.64014
I0521 04:22:25.901330  7754 solver.cpp:253]     Train net output #0: loss = 1.64014 (* 1 = 1.64014 loss)
I0521 04:22:25.901346  7754 sgd_solver.cpp:106] Iteration 1679, lr = 0.0025
I0521 04:22:33.676173  7754 solver.cpp:237] Iteration 1702, loss = 1.71463
I0521 04:22:33.676317  7754 solver.cpp:253]     Train net output #0: loss = 1.71463 (* 1 = 1.71463 loss)
I0521 04:22:33.676331  7754 sgd_solver.cpp:106] Iteration 1702, lr = 0.0025
I0521 04:22:41.446768  7754 solver.cpp:237] Iteration 1725, loss = 1.68888
I0521 04:22:41.446799  7754 solver.cpp:253]     Train net output #0: loss = 1.68888 (* 1 = 1.68888 loss)
I0521 04:22:41.446816  7754 sgd_solver.cpp:106] Iteration 1725, lr = 0.0025
I0521 04:23:11.390100  7754 solver.cpp:237] Iteration 1748, loss = 1.72903
I0521 04:23:11.390266  7754 solver.cpp:253]     Train net output #0: loss = 1.72903 (* 1 = 1.72903 loss)
I0521 04:23:11.390280  7754 sgd_solver.cpp:106] Iteration 1748, lr = 0.0025
I0521 04:23:19.164471  7754 solver.cpp:237] Iteration 1771, loss = 1.68358
I0521 04:23:19.164505  7754 solver.cpp:253]     Train net output #0: loss = 1.68358 (* 1 = 1.68358 loss)
I0521 04:23:19.164523  7754 sgd_solver.cpp:106] Iteration 1771, lr = 0.0025
I0521 04:23:26.934578  7754 solver.cpp:237] Iteration 1794, loss = 1.75907
I0521 04:23:26.934612  7754 solver.cpp:253]     Train net output #0: loss = 1.75907 (* 1 = 1.75907 loss)
I0521 04:23:26.934628  7754 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0521 04:23:34.707109  7754 solver.cpp:237] Iteration 1817, loss = 1.69915
I0521 04:23:34.707141  7754 solver.cpp:253]     Train net output #0: loss = 1.69915 (* 1 = 1.69915 loss)
I0521 04:23:34.707157  7754 sgd_solver.cpp:106] Iteration 1817, lr = 0.0025
I0521 04:23:42.471767  7754 solver.cpp:237] Iteration 1840, loss = 1.71537
I0521 04:23:42.471918  7754 solver.cpp:253]     Train net output #0: loss = 1.71537 (* 1 = 1.71537 loss)
I0521 04:23:42.471932  7754 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0521 04:23:50.245733  7754 solver.cpp:237] Iteration 1863, loss = 1.65508
I0521 04:23:50.245764  7754 solver.cpp:253]     Train net output #0: loss = 1.65508 (* 1 = 1.65508 loss)
I0521 04:23:50.245784  7754 sgd_solver.cpp:106] Iteration 1863, lr = 0.0025
I0521 04:23:58.020998  7754 solver.cpp:237] Iteration 1886, loss = 1.66019
I0521 04:23:58.021031  7754 solver.cpp:253]     Train net output #0: loss = 1.66019 (* 1 = 1.66019 loss)
I0521 04:23:58.021049  7754 sgd_solver.cpp:106] Iteration 1886, lr = 0.0025
I0521 04:24:03.767009  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1904.caffemodel
I0521 04:24:04.042549  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_1904.solverstate
I0521 04:24:04.068861  7754 solver.cpp:341] Iteration 1904, Testing net (#0)
I0521 04:25:09.903841  7754 solver.cpp:409]     Test net output #0: accuracy = 0.662812
I0521 04:25:09.904011  7754 solver.cpp:409]     Test net output #1: loss = 1.2213 (* 1 = 1.2213 loss)
I0521 04:25:33.822046  7754 solver.cpp:237] Iteration 1909, loss = 1.71098
I0521 04:25:33.822096  7754 solver.cpp:253]     Train net output #0: loss = 1.71098 (* 1 = 1.71098 loss)
I0521 04:25:33.822111  7754 sgd_solver.cpp:106] Iteration 1909, lr = 0.0025
I0521 04:25:41.597760  7754 solver.cpp:237] Iteration 1932, loss = 1.64951
I0521 04:25:41.597914  7754 solver.cpp:253]     Train net output #0: loss = 1.64951 (* 1 = 1.64951 loss)
I0521 04:25:41.597929  7754 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0521 04:25:49.371312  7754 solver.cpp:237] Iteration 1955, loss = 1.72103
I0521 04:25:49.371345  7754 solver.cpp:253]     Train net output #0: loss = 1.72103 (* 1 = 1.72103 loss)
I0521 04:25:49.371362  7754 sgd_solver.cpp:106] Iteration 1955, lr = 0.0025
I0521 04:25:57.148368  7754 solver.cpp:237] Iteration 1978, loss = 1.65863
I0521 04:25:57.148401  7754 solver.cpp:253]     Train net output #0: loss = 1.65863 (* 1 = 1.65863 loss)
I0521 04:25:57.148417  7754 sgd_solver.cpp:106] Iteration 1978, lr = 0.0025
I0521 04:26:04.927371  7754 solver.cpp:237] Iteration 2001, loss = 1.6793
I0521 04:26:04.927403  7754 solver.cpp:253]     Train net output #0: loss = 1.6793 (* 1 = 1.6793 loss)
I0521 04:26:04.927419  7754 sgd_solver.cpp:106] Iteration 2001, lr = 0.0025
I0521 04:26:12.707892  7754 solver.cpp:237] Iteration 2024, loss = 1.67942
I0521 04:26:12.708034  7754 solver.cpp:253]     Train net output #0: loss = 1.67942 (* 1 = 1.67942 loss)
I0521 04:26:12.708047  7754 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0521 04:26:20.481262  7754 solver.cpp:237] Iteration 2047, loss = 1.69916
I0521 04:26:20.481294  7754 solver.cpp:253]     Train net output #0: loss = 1.69916 (* 1 = 1.69916 loss)
I0521 04:26:20.481312  7754 sgd_solver.cpp:106] Iteration 2047, lr = 0.0025
I0521 04:26:50.370524  7754 solver.cpp:237] Iteration 2070, loss = 1.65853
I0521 04:26:50.370702  7754 solver.cpp:253]     Train net output #0: loss = 1.65853 (* 1 = 1.65853 loss)
I0521 04:26:50.370717  7754 sgd_solver.cpp:106] Iteration 2070, lr = 0.0025
I0521 04:26:58.143849  7754 solver.cpp:237] Iteration 2093, loss = 1.56551
I0521 04:26:58.143882  7754 solver.cpp:253]     Train net output #0: loss = 1.56551 (* 1 = 1.56551 loss)
I0521 04:26:58.143903  7754 sgd_solver.cpp:106] Iteration 2093, lr = 0.0025
I0521 04:27:05.919183  7754 solver.cpp:237] Iteration 2116, loss = 1.63143
I0521 04:27:05.919215  7754 solver.cpp:253]     Train net output #0: loss = 1.63143 (* 1 = 1.63143 loss)
I0521 04:27:05.919231  7754 sgd_solver.cpp:106] Iteration 2116, lr = 0.0025
I0521 04:27:13.693020  7754 solver.cpp:237] Iteration 2139, loss = 1.65909
I0521 04:27:13.693053  7754 solver.cpp:253]     Train net output #0: loss = 1.65909 (* 1 = 1.65909 loss)
I0521 04:27:13.693069  7754 sgd_solver.cpp:106] Iteration 2139, lr = 0.0025
I0521 04:27:14.370189  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_2142.caffemodel
I0521 04:27:14.646792  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_2142.solverstate
I0521 04:27:21.544905  7754 solver.cpp:237] Iteration 2162, loss = 1.66206
I0521 04:27:21.545071  7754 solver.cpp:253]     Train net output #0: loss = 1.66206 (* 1 = 1.66206 loss)
I0521 04:27:21.545085  7754 sgd_solver.cpp:106] Iteration 2162, lr = 0.0025
I0521 04:27:29.325565  7754 solver.cpp:237] Iteration 2185, loss = 1.62933
I0521 04:27:29.325598  7754 solver.cpp:253]     Train net output #0: loss = 1.62933 (* 1 = 1.62933 loss)
I0521 04:27:29.325614  7754 sgd_solver.cpp:106] Iteration 2185, lr = 0.0025
I0521 04:27:37.102524  7754 solver.cpp:237] Iteration 2208, loss = 1.70159
I0521 04:27:37.102556  7754 solver.cpp:253]     Train net output #0: loss = 1.70159 (* 1 = 1.70159 loss)
I0521 04:27:37.102572  7754 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0521 04:28:07.022554  7754 solver.cpp:237] Iteration 2231, loss = 1.6791
I0521 04:28:07.022724  7754 solver.cpp:253]     Train net output #0: loss = 1.6791 (* 1 = 1.6791 loss)
I0521 04:28:07.022739  7754 sgd_solver.cpp:106] Iteration 2231, lr = 0.0025
I0521 04:28:14.800184  7754 solver.cpp:237] Iteration 2254, loss = 1.66254
I0521 04:28:14.800223  7754 solver.cpp:253]     Train net output #0: loss = 1.66254 (* 1 = 1.66254 loss)
I0521 04:28:14.800245  7754 sgd_solver.cpp:106] Iteration 2254, lr = 0.0025
I0521 04:28:22.573777  7754 solver.cpp:237] Iteration 2277, loss = 1.62173
I0521 04:28:22.573809  7754 solver.cpp:253]     Train net output #0: loss = 1.62173 (* 1 = 1.62173 loss)
I0521 04:28:22.573827  7754 sgd_solver.cpp:106] Iteration 2277, lr = 0.0025
I0521 04:28:30.356174  7754 solver.cpp:237] Iteration 2300, loss = 1.66039
I0521 04:28:30.356209  7754 solver.cpp:253]     Train net output #0: loss = 1.66039 (* 1 = 1.66039 loss)
I0521 04:28:30.356225  7754 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0521 04:28:38.130596  7754 solver.cpp:237] Iteration 2323, loss = 1.58885
I0521 04:28:38.130753  7754 solver.cpp:253]     Train net output #0: loss = 1.58885 (* 1 = 1.58885 loss)
I0521 04:28:38.130766  7754 sgd_solver.cpp:106] Iteration 2323, lr = 0.0025
I0521 04:28:45.909796  7754 solver.cpp:237] Iteration 2346, loss = 1.60765
I0521 04:28:45.909829  7754 solver.cpp:253]     Train net output #0: loss = 1.60765 (* 1 = 1.60765 loss)
I0521 04:28:45.909847  7754 sgd_solver.cpp:106] Iteration 2346, lr = 0.0025
I0521 04:28:53.689915  7754 solver.cpp:237] Iteration 2369, loss = 1.6768
I0521 04:28:53.689949  7754 solver.cpp:253]     Train net output #0: loss = 1.6768 (* 1 = 1.6768 loss)
I0521 04:28:53.689965  7754 sgd_solver.cpp:106] Iteration 2369, lr = 0.0025
I0521 04:28:57.073485  7754 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_2380.caffemodel
I0521 04:28:57.352504  7754 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130_iter_2380.solverstate
I0521 04:28:57.380661  7754 solver.cpp:341] Iteration 2380, Testing net (#0)
I0521 04:29:42.409137  7754 solver.cpp:409]     Test net output #0: accuracy = 0.682653
I0521 04:29:42.409301  7754 solver.cpp:409]     Test net output #1: loss = 1.10612 (* 1 = 1.10612 loss)
I0521 04:29:42.409314  7754 solver.cpp:326] Optimization Done.
I0521 04:29:42.409327  7754 caffe.cpp:215] Optimization Done.
Application 11236747 resources: utime ~1248s, stime ~225s, Rss ~5332488, inblocks ~3594475, outblocks ~179820
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_630_2016-05-20T11.20.55.516130.solver"
	User time (seconds): 0.54
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:36.98
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 2706
	Involuntary context switches: 73
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
