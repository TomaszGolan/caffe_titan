2806151
I0521 02:07:26.243908 18536 caffe.cpp:184] Using GPUs 0
I0521 02:07:26.683796 18536 solver.cpp:48] Initializing solver from parameters: 
test_iter: 272
test_interval: 545
base_lr: 0.0025
display: 27
max_iter: 2727
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 272
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481.prototxt"
I0521 02:07:26.685598 18536 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481.prototxt
I0521 02:07:26.706274 18536 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 02:07:26.706334 18536 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 02:07:26.706678 18536 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 550
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:07:26.706862 18536 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:07:26.706887 18536 net.cpp:106] Creating Layer data_hdf5
I0521 02:07:26.706900 18536 net.cpp:411] data_hdf5 -> data
I0521 02:07:26.706933 18536 net.cpp:411] data_hdf5 -> label
I0521 02:07:26.706965 18536 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 02:07:26.708354 18536 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 02:07:26.710549 18536 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 02:07:48.277362 18536 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 02:07:48.282521 18536 net.cpp:150] Setting up data_hdf5
I0521 02:07:48.282562 18536 net.cpp:157] Top shape: 550 1 127 50 (3492500)
I0521 02:07:48.282577 18536 net.cpp:157] Top shape: 550 (550)
I0521 02:07:48.282587 18536 net.cpp:165] Memory required for data: 13972200
I0521 02:07:48.282601 18536 layer_factory.hpp:77] Creating layer conv1
I0521 02:07:48.282635 18536 net.cpp:106] Creating Layer conv1
I0521 02:07:48.282646 18536 net.cpp:454] conv1 <- data
I0521 02:07:48.282668 18536 net.cpp:411] conv1 -> conv1
I0521 02:07:48.715986 18536 net.cpp:150] Setting up conv1
I0521 02:07:48.716030 18536 net.cpp:157] Top shape: 550 12 120 48 (38016000)
I0521 02:07:48.716043 18536 net.cpp:165] Memory required for data: 166036200
I0521 02:07:48.716071 18536 layer_factory.hpp:77] Creating layer relu1
I0521 02:07:48.716092 18536 net.cpp:106] Creating Layer relu1
I0521 02:07:48.716104 18536 net.cpp:454] relu1 <- conv1
I0521 02:07:48.716119 18536 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:07:48.716636 18536 net.cpp:150] Setting up relu1
I0521 02:07:48.716652 18536 net.cpp:157] Top shape: 550 12 120 48 (38016000)
I0521 02:07:48.716666 18536 net.cpp:165] Memory required for data: 318100200
I0521 02:07:48.716675 18536 layer_factory.hpp:77] Creating layer pool1
I0521 02:07:48.716692 18536 net.cpp:106] Creating Layer pool1
I0521 02:07:48.716702 18536 net.cpp:454] pool1 <- conv1
I0521 02:07:48.716716 18536 net.cpp:411] pool1 -> pool1
I0521 02:07:48.716795 18536 net.cpp:150] Setting up pool1
I0521 02:07:48.716809 18536 net.cpp:157] Top shape: 550 12 60 48 (19008000)
I0521 02:07:48.716819 18536 net.cpp:165] Memory required for data: 394132200
I0521 02:07:48.716830 18536 layer_factory.hpp:77] Creating layer conv2
I0521 02:07:48.716850 18536 net.cpp:106] Creating Layer conv2
I0521 02:07:48.716861 18536 net.cpp:454] conv2 <- pool1
I0521 02:07:48.716874 18536 net.cpp:411] conv2 -> conv2
I0521 02:07:48.719550 18536 net.cpp:150] Setting up conv2
I0521 02:07:48.719578 18536 net.cpp:157] Top shape: 550 20 54 46 (27324000)
I0521 02:07:48.719588 18536 net.cpp:165] Memory required for data: 503428200
I0521 02:07:48.719607 18536 layer_factory.hpp:77] Creating layer relu2
I0521 02:07:48.719621 18536 net.cpp:106] Creating Layer relu2
I0521 02:07:48.719631 18536 net.cpp:454] relu2 <- conv2
I0521 02:07:48.719645 18536 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:07:48.719985 18536 net.cpp:150] Setting up relu2
I0521 02:07:48.719998 18536 net.cpp:157] Top shape: 550 20 54 46 (27324000)
I0521 02:07:48.720010 18536 net.cpp:165] Memory required for data: 612724200
I0521 02:07:48.720018 18536 layer_factory.hpp:77] Creating layer pool2
I0521 02:07:48.720031 18536 net.cpp:106] Creating Layer pool2
I0521 02:07:48.720041 18536 net.cpp:454] pool2 <- conv2
I0521 02:07:48.720067 18536 net.cpp:411] pool2 -> pool2
I0521 02:07:48.720135 18536 net.cpp:150] Setting up pool2
I0521 02:07:48.720149 18536 net.cpp:157] Top shape: 550 20 27 46 (13662000)
I0521 02:07:48.720158 18536 net.cpp:165] Memory required for data: 667372200
I0521 02:07:48.720168 18536 layer_factory.hpp:77] Creating layer conv3
I0521 02:07:48.720186 18536 net.cpp:106] Creating Layer conv3
I0521 02:07:48.720196 18536 net.cpp:454] conv3 <- pool2
I0521 02:07:48.720209 18536 net.cpp:411] conv3 -> conv3
I0521 02:07:48.722129 18536 net.cpp:150] Setting up conv3
I0521 02:07:48.722152 18536 net.cpp:157] Top shape: 550 28 22 44 (14907200)
I0521 02:07:48.722163 18536 net.cpp:165] Memory required for data: 727001000
I0521 02:07:48.722182 18536 layer_factory.hpp:77] Creating layer relu3
I0521 02:07:48.722196 18536 net.cpp:106] Creating Layer relu3
I0521 02:07:48.722206 18536 net.cpp:454] relu3 <- conv3
I0521 02:07:48.722219 18536 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:07:48.722688 18536 net.cpp:150] Setting up relu3
I0521 02:07:48.722705 18536 net.cpp:157] Top shape: 550 28 22 44 (14907200)
I0521 02:07:48.722717 18536 net.cpp:165] Memory required for data: 786629800
I0521 02:07:48.722725 18536 layer_factory.hpp:77] Creating layer pool3
I0521 02:07:48.722738 18536 net.cpp:106] Creating Layer pool3
I0521 02:07:48.722748 18536 net.cpp:454] pool3 <- conv3
I0521 02:07:48.722760 18536 net.cpp:411] pool3 -> pool3
I0521 02:07:48.722828 18536 net.cpp:150] Setting up pool3
I0521 02:07:48.722841 18536 net.cpp:157] Top shape: 550 28 11 44 (7453600)
I0521 02:07:48.722851 18536 net.cpp:165] Memory required for data: 816444200
I0521 02:07:48.722861 18536 layer_factory.hpp:77] Creating layer conv4
I0521 02:07:48.722877 18536 net.cpp:106] Creating Layer conv4
I0521 02:07:48.722887 18536 net.cpp:454] conv4 <- pool3
I0521 02:07:48.722900 18536 net.cpp:411] conv4 -> conv4
I0521 02:07:48.725703 18536 net.cpp:150] Setting up conv4
I0521 02:07:48.725726 18536 net.cpp:157] Top shape: 550 36 6 42 (4989600)
I0521 02:07:48.725738 18536 net.cpp:165] Memory required for data: 836402600
I0521 02:07:48.725752 18536 layer_factory.hpp:77] Creating layer relu4
I0521 02:07:48.725766 18536 net.cpp:106] Creating Layer relu4
I0521 02:07:48.725777 18536 net.cpp:454] relu4 <- conv4
I0521 02:07:48.725790 18536 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:07:48.726266 18536 net.cpp:150] Setting up relu4
I0521 02:07:48.726284 18536 net.cpp:157] Top shape: 550 36 6 42 (4989600)
I0521 02:07:48.726294 18536 net.cpp:165] Memory required for data: 856361000
I0521 02:07:48.726303 18536 layer_factory.hpp:77] Creating layer pool4
I0521 02:07:48.726315 18536 net.cpp:106] Creating Layer pool4
I0521 02:07:48.726325 18536 net.cpp:454] pool4 <- conv4
I0521 02:07:48.726338 18536 net.cpp:411] pool4 -> pool4
I0521 02:07:48.726407 18536 net.cpp:150] Setting up pool4
I0521 02:07:48.726420 18536 net.cpp:157] Top shape: 550 36 3 42 (2494800)
I0521 02:07:48.726430 18536 net.cpp:165] Memory required for data: 866340200
I0521 02:07:48.726440 18536 layer_factory.hpp:77] Creating layer ip1
I0521 02:07:48.726460 18536 net.cpp:106] Creating Layer ip1
I0521 02:07:48.726470 18536 net.cpp:454] ip1 <- pool4
I0521 02:07:48.726483 18536 net.cpp:411] ip1 -> ip1
I0521 02:07:48.741940 18536 net.cpp:150] Setting up ip1
I0521 02:07:48.741964 18536 net.cpp:157] Top shape: 550 196 (107800)
I0521 02:07:48.741977 18536 net.cpp:165] Memory required for data: 866771400
I0521 02:07:48.741999 18536 layer_factory.hpp:77] Creating layer relu5
I0521 02:07:48.742013 18536 net.cpp:106] Creating Layer relu5
I0521 02:07:48.742024 18536 net.cpp:454] relu5 <- ip1
I0521 02:07:48.742039 18536 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:07:48.742383 18536 net.cpp:150] Setting up relu5
I0521 02:07:48.742398 18536 net.cpp:157] Top shape: 550 196 (107800)
I0521 02:07:48.742408 18536 net.cpp:165] Memory required for data: 867202600
I0521 02:07:48.742416 18536 layer_factory.hpp:77] Creating layer drop1
I0521 02:07:48.742439 18536 net.cpp:106] Creating Layer drop1
I0521 02:07:48.742449 18536 net.cpp:454] drop1 <- ip1
I0521 02:07:48.742473 18536 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:07:48.742519 18536 net.cpp:150] Setting up drop1
I0521 02:07:48.742532 18536 net.cpp:157] Top shape: 550 196 (107800)
I0521 02:07:48.742542 18536 net.cpp:165] Memory required for data: 867633800
I0521 02:07:48.742552 18536 layer_factory.hpp:77] Creating layer ip2
I0521 02:07:48.742570 18536 net.cpp:106] Creating Layer ip2
I0521 02:07:48.742579 18536 net.cpp:454] ip2 <- ip1
I0521 02:07:48.742594 18536 net.cpp:411] ip2 -> ip2
I0521 02:07:48.743057 18536 net.cpp:150] Setting up ip2
I0521 02:07:48.743070 18536 net.cpp:157] Top shape: 550 98 (53900)
I0521 02:07:48.743080 18536 net.cpp:165] Memory required for data: 867849400
I0521 02:07:48.743094 18536 layer_factory.hpp:77] Creating layer relu6
I0521 02:07:48.743108 18536 net.cpp:106] Creating Layer relu6
I0521 02:07:48.743118 18536 net.cpp:454] relu6 <- ip2
I0521 02:07:48.743129 18536 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:07:48.743649 18536 net.cpp:150] Setting up relu6
I0521 02:07:48.743665 18536 net.cpp:157] Top shape: 550 98 (53900)
I0521 02:07:48.743677 18536 net.cpp:165] Memory required for data: 868065000
I0521 02:07:48.743685 18536 layer_factory.hpp:77] Creating layer drop2
I0521 02:07:48.743698 18536 net.cpp:106] Creating Layer drop2
I0521 02:07:48.743708 18536 net.cpp:454] drop2 <- ip2
I0521 02:07:48.743721 18536 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:07:48.743762 18536 net.cpp:150] Setting up drop2
I0521 02:07:48.743775 18536 net.cpp:157] Top shape: 550 98 (53900)
I0521 02:07:48.743785 18536 net.cpp:165] Memory required for data: 868280600
I0521 02:07:48.743794 18536 layer_factory.hpp:77] Creating layer ip3
I0521 02:07:48.743806 18536 net.cpp:106] Creating Layer ip3
I0521 02:07:48.743824 18536 net.cpp:454] ip3 <- ip2
I0521 02:07:48.743837 18536 net.cpp:411] ip3 -> ip3
I0521 02:07:48.744047 18536 net.cpp:150] Setting up ip3
I0521 02:07:48.744060 18536 net.cpp:157] Top shape: 550 11 (6050)
I0521 02:07:48.744069 18536 net.cpp:165] Memory required for data: 868304800
I0521 02:07:48.744086 18536 layer_factory.hpp:77] Creating layer drop3
I0521 02:07:48.744098 18536 net.cpp:106] Creating Layer drop3
I0521 02:07:48.744108 18536 net.cpp:454] drop3 <- ip3
I0521 02:07:48.744120 18536 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:07:48.744159 18536 net.cpp:150] Setting up drop3
I0521 02:07:48.744171 18536 net.cpp:157] Top shape: 550 11 (6050)
I0521 02:07:48.744181 18536 net.cpp:165] Memory required for data: 868329000
I0521 02:07:48.744190 18536 layer_factory.hpp:77] Creating layer loss
I0521 02:07:48.744210 18536 net.cpp:106] Creating Layer loss
I0521 02:07:48.744220 18536 net.cpp:454] loss <- ip3
I0521 02:07:48.744230 18536 net.cpp:454] loss <- label
I0521 02:07:48.744242 18536 net.cpp:411] loss -> loss
I0521 02:07:48.744258 18536 layer_factory.hpp:77] Creating layer loss
I0521 02:07:48.744910 18536 net.cpp:150] Setting up loss
I0521 02:07:48.744931 18536 net.cpp:157] Top shape: (1)
I0521 02:07:48.744943 18536 net.cpp:160]     with loss weight 1
I0521 02:07:48.744985 18536 net.cpp:165] Memory required for data: 868329004
I0521 02:07:48.744995 18536 net.cpp:226] loss needs backward computation.
I0521 02:07:48.745007 18536 net.cpp:226] drop3 needs backward computation.
I0521 02:07:48.745018 18536 net.cpp:226] ip3 needs backward computation.
I0521 02:07:48.745028 18536 net.cpp:226] drop2 needs backward computation.
I0521 02:07:48.745038 18536 net.cpp:226] relu6 needs backward computation.
I0521 02:07:48.745046 18536 net.cpp:226] ip2 needs backward computation.
I0521 02:07:48.745056 18536 net.cpp:226] drop1 needs backward computation.
I0521 02:07:48.745066 18536 net.cpp:226] relu5 needs backward computation.
I0521 02:07:48.745076 18536 net.cpp:226] ip1 needs backward computation.
I0521 02:07:48.745087 18536 net.cpp:226] pool4 needs backward computation.
I0521 02:07:48.745100 18536 net.cpp:226] relu4 needs backward computation.
I0521 02:07:48.745110 18536 net.cpp:226] conv4 needs backward computation.
I0521 02:07:48.745120 18536 net.cpp:226] pool3 needs backward computation.
I0521 02:07:48.745137 18536 net.cpp:226] relu3 needs backward computation.
I0521 02:07:48.745147 18536 net.cpp:226] conv3 needs backward computation.
I0521 02:07:48.745158 18536 net.cpp:226] pool2 needs backward computation.
I0521 02:07:48.745168 18536 net.cpp:226] relu2 needs backward computation.
I0521 02:07:48.745177 18536 net.cpp:226] conv2 needs backward computation.
I0521 02:07:48.745188 18536 net.cpp:226] pool1 needs backward computation.
I0521 02:07:48.745198 18536 net.cpp:226] relu1 needs backward computation.
I0521 02:07:48.745208 18536 net.cpp:226] conv1 needs backward computation.
I0521 02:07:48.745219 18536 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:07:48.745229 18536 net.cpp:270] This network produces output loss
I0521 02:07:48.745254 18536 net.cpp:283] Network initialization done.
I0521 02:07:48.746843 18536 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481.prototxt
I0521 02:07:48.746914 18536 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 02:07:48.747269 18536 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 550
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:07:48.747459 18536 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:07:48.747474 18536 net.cpp:106] Creating Layer data_hdf5
I0521 02:07:48.747486 18536 net.cpp:411] data_hdf5 -> data
I0521 02:07:48.747503 18536 net.cpp:411] data_hdf5 -> label
I0521 02:07:48.747519 18536 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 02:07:48.748795 18536 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 02:08:10.161326 18536 net.cpp:150] Setting up data_hdf5
I0521 02:08:10.161494 18536 net.cpp:157] Top shape: 550 1 127 50 (3492500)
I0521 02:08:10.161509 18536 net.cpp:157] Top shape: 550 (550)
I0521 02:08:10.161521 18536 net.cpp:165] Memory required for data: 13972200
I0521 02:08:10.161537 18536 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 02:08:10.161566 18536 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 02:08:10.161576 18536 net.cpp:454] label_data_hdf5_1_split <- label
I0521 02:08:10.161590 18536 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 02:08:10.161612 18536 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 02:08:10.161684 18536 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 02:08:10.161698 18536 net.cpp:157] Top shape: 550 (550)
I0521 02:08:10.161710 18536 net.cpp:157] Top shape: 550 (550)
I0521 02:08:10.161720 18536 net.cpp:165] Memory required for data: 13976600
I0521 02:08:10.161730 18536 layer_factory.hpp:77] Creating layer conv1
I0521 02:08:10.161751 18536 net.cpp:106] Creating Layer conv1
I0521 02:08:10.161761 18536 net.cpp:454] conv1 <- data
I0521 02:08:10.161777 18536 net.cpp:411] conv1 -> conv1
I0521 02:08:10.163727 18536 net.cpp:150] Setting up conv1
I0521 02:08:10.163751 18536 net.cpp:157] Top shape: 550 12 120 48 (38016000)
I0521 02:08:10.163763 18536 net.cpp:165] Memory required for data: 166040600
I0521 02:08:10.163782 18536 layer_factory.hpp:77] Creating layer relu1
I0521 02:08:10.163797 18536 net.cpp:106] Creating Layer relu1
I0521 02:08:10.163807 18536 net.cpp:454] relu1 <- conv1
I0521 02:08:10.163828 18536 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:08:10.164330 18536 net.cpp:150] Setting up relu1
I0521 02:08:10.164346 18536 net.cpp:157] Top shape: 550 12 120 48 (38016000)
I0521 02:08:10.164356 18536 net.cpp:165] Memory required for data: 318104600
I0521 02:08:10.164366 18536 layer_factory.hpp:77] Creating layer pool1
I0521 02:08:10.164382 18536 net.cpp:106] Creating Layer pool1
I0521 02:08:10.164392 18536 net.cpp:454] pool1 <- conv1
I0521 02:08:10.164404 18536 net.cpp:411] pool1 -> pool1
I0521 02:08:10.164479 18536 net.cpp:150] Setting up pool1
I0521 02:08:10.164492 18536 net.cpp:157] Top shape: 550 12 60 48 (19008000)
I0521 02:08:10.164502 18536 net.cpp:165] Memory required for data: 394136600
I0521 02:08:10.164511 18536 layer_factory.hpp:77] Creating layer conv2
I0521 02:08:10.164528 18536 net.cpp:106] Creating Layer conv2
I0521 02:08:10.164541 18536 net.cpp:454] conv2 <- pool1
I0521 02:08:10.164556 18536 net.cpp:411] conv2 -> conv2
I0521 02:08:10.166481 18536 net.cpp:150] Setting up conv2
I0521 02:08:10.166504 18536 net.cpp:157] Top shape: 550 20 54 46 (27324000)
I0521 02:08:10.166514 18536 net.cpp:165] Memory required for data: 503432600
I0521 02:08:10.166532 18536 layer_factory.hpp:77] Creating layer relu2
I0521 02:08:10.166545 18536 net.cpp:106] Creating Layer relu2
I0521 02:08:10.166555 18536 net.cpp:454] relu2 <- conv2
I0521 02:08:10.166568 18536 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:08:10.166901 18536 net.cpp:150] Setting up relu2
I0521 02:08:10.166915 18536 net.cpp:157] Top shape: 550 20 54 46 (27324000)
I0521 02:08:10.166925 18536 net.cpp:165] Memory required for data: 612728600
I0521 02:08:10.166934 18536 layer_factory.hpp:77] Creating layer pool2
I0521 02:08:10.166947 18536 net.cpp:106] Creating Layer pool2
I0521 02:08:10.166957 18536 net.cpp:454] pool2 <- conv2
I0521 02:08:10.166970 18536 net.cpp:411] pool2 -> pool2
I0521 02:08:10.167042 18536 net.cpp:150] Setting up pool2
I0521 02:08:10.167054 18536 net.cpp:157] Top shape: 550 20 27 46 (13662000)
I0521 02:08:10.167063 18536 net.cpp:165] Memory required for data: 667376600
I0521 02:08:10.167073 18536 layer_factory.hpp:77] Creating layer conv3
I0521 02:08:10.167093 18536 net.cpp:106] Creating Layer conv3
I0521 02:08:10.167103 18536 net.cpp:454] conv3 <- pool2
I0521 02:08:10.167115 18536 net.cpp:411] conv3 -> conv3
I0521 02:08:10.169087 18536 net.cpp:150] Setting up conv3
I0521 02:08:10.169106 18536 net.cpp:157] Top shape: 550 28 22 44 (14907200)
I0521 02:08:10.169116 18536 net.cpp:165] Memory required for data: 727005400
I0521 02:08:10.169147 18536 layer_factory.hpp:77] Creating layer relu3
I0521 02:08:10.169162 18536 net.cpp:106] Creating Layer relu3
I0521 02:08:10.169172 18536 net.cpp:454] relu3 <- conv3
I0521 02:08:10.169184 18536 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:08:10.169661 18536 net.cpp:150] Setting up relu3
I0521 02:08:10.169677 18536 net.cpp:157] Top shape: 550 28 22 44 (14907200)
I0521 02:08:10.169687 18536 net.cpp:165] Memory required for data: 786634200
I0521 02:08:10.169695 18536 layer_factory.hpp:77] Creating layer pool3
I0521 02:08:10.169711 18536 net.cpp:106] Creating Layer pool3
I0521 02:08:10.169721 18536 net.cpp:454] pool3 <- conv3
I0521 02:08:10.169734 18536 net.cpp:411] pool3 -> pool3
I0521 02:08:10.169806 18536 net.cpp:150] Setting up pool3
I0521 02:08:10.169821 18536 net.cpp:157] Top shape: 550 28 11 44 (7453600)
I0521 02:08:10.169831 18536 net.cpp:165] Memory required for data: 816448600
I0521 02:08:10.169841 18536 layer_factory.hpp:77] Creating layer conv4
I0521 02:08:10.169857 18536 net.cpp:106] Creating Layer conv4
I0521 02:08:10.169867 18536 net.cpp:454] conv4 <- pool3
I0521 02:08:10.169883 18536 net.cpp:411] conv4 -> conv4
I0521 02:08:10.171943 18536 net.cpp:150] Setting up conv4
I0521 02:08:10.171965 18536 net.cpp:157] Top shape: 550 36 6 42 (4989600)
I0521 02:08:10.171977 18536 net.cpp:165] Memory required for data: 836407000
I0521 02:08:10.171991 18536 layer_factory.hpp:77] Creating layer relu4
I0521 02:08:10.172005 18536 net.cpp:106] Creating Layer relu4
I0521 02:08:10.172015 18536 net.cpp:454] relu4 <- conv4
I0521 02:08:10.172029 18536 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:08:10.172499 18536 net.cpp:150] Setting up relu4
I0521 02:08:10.172514 18536 net.cpp:157] Top shape: 550 36 6 42 (4989600)
I0521 02:08:10.172524 18536 net.cpp:165] Memory required for data: 856365400
I0521 02:08:10.172534 18536 layer_factory.hpp:77] Creating layer pool4
I0521 02:08:10.172547 18536 net.cpp:106] Creating Layer pool4
I0521 02:08:10.172557 18536 net.cpp:454] pool4 <- conv4
I0521 02:08:10.172569 18536 net.cpp:411] pool4 -> pool4
I0521 02:08:10.172641 18536 net.cpp:150] Setting up pool4
I0521 02:08:10.172654 18536 net.cpp:157] Top shape: 550 36 3 42 (2494800)
I0521 02:08:10.172663 18536 net.cpp:165] Memory required for data: 866344600
I0521 02:08:10.172673 18536 layer_factory.hpp:77] Creating layer ip1
I0521 02:08:10.172688 18536 net.cpp:106] Creating Layer ip1
I0521 02:08:10.172698 18536 net.cpp:454] ip1 <- pool4
I0521 02:08:10.172710 18536 net.cpp:411] ip1 -> ip1
I0521 02:08:10.188201 18536 net.cpp:150] Setting up ip1
I0521 02:08:10.188225 18536 net.cpp:157] Top shape: 550 196 (107800)
I0521 02:08:10.188235 18536 net.cpp:165] Memory required for data: 866775800
I0521 02:08:10.188259 18536 layer_factory.hpp:77] Creating layer relu5
I0521 02:08:10.188274 18536 net.cpp:106] Creating Layer relu5
I0521 02:08:10.188284 18536 net.cpp:454] relu5 <- ip1
I0521 02:08:10.188300 18536 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:08:10.188647 18536 net.cpp:150] Setting up relu5
I0521 02:08:10.188662 18536 net.cpp:157] Top shape: 550 196 (107800)
I0521 02:08:10.188670 18536 net.cpp:165] Memory required for data: 867207000
I0521 02:08:10.188679 18536 layer_factory.hpp:77] Creating layer drop1
I0521 02:08:10.188699 18536 net.cpp:106] Creating Layer drop1
I0521 02:08:10.188709 18536 net.cpp:454] drop1 <- ip1
I0521 02:08:10.188724 18536 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:08:10.188766 18536 net.cpp:150] Setting up drop1
I0521 02:08:10.188779 18536 net.cpp:157] Top shape: 550 196 (107800)
I0521 02:08:10.188788 18536 net.cpp:165] Memory required for data: 867638200
I0521 02:08:10.188799 18536 layer_factory.hpp:77] Creating layer ip2
I0521 02:08:10.188813 18536 net.cpp:106] Creating Layer ip2
I0521 02:08:10.188823 18536 net.cpp:454] ip2 <- ip1
I0521 02:08:10.188837 18536 net.cpp:411] ip2 -> ip2
I0521 02:08:10.189314 18536 net.cpp:150] Setting up ip2
I0521 02:08:10.189327 18536 net.cpp:157] Top shape: 550 98 (53900)
I0521 02:08:10.189337 18536 net.cpp:165] Memory required for data: 867853800
I0521 02:08:10.189365 18536 layer_factory.hpp:77] Creating layer relu6
I0521 02:08:10.189378 18536 net.cpp:106] Creating Layer relu6
I0521 02:08:10.189388 18536 net.cpp:454] relu6 <- ip2
I0521 02:08:10.189404 18536 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:08:10.189937 18536 net.cpp:150] Setting up relu6
I0521 02:08:10.189954 18536 net.cpp:157] Top shape: 550 98 (53900)
I0521 02:08:10.189965 18536 net.cpp:165] Memory required for data: 868069400
I0521 02:08:10.189973 18536 layer_factory.hpp:77] Creating layer drop2
I0521 02:08:10.189986 18536 net.cpp:106] Creating Layer drop2
I0521 02:08:10.189996 18536 net.cpp:454] drop2 <- ip2
I0521 02:08:10.190009 18536 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:08:10.190053 18536 net.cpp:150] Setting up drop2
I0521 02:08:10.190065 18536 net.cpp:157] Top shape: 550 98 (53900)
I0521 02:08:10.190075 18536 net.cpp:165] Memory required for data: 868285000
I0521 02:08:10.190086 18536 layer_factory.hpp:77] Creating layer ip3
I0521 02:08:10.190099 18536 net.cpp:106] Creating Layer ip3
I0521 02:08:10.190109 18536 net.cpp:454] ip3 <- ip2
I0521 02:08:10.190124 18536 net.cpp:411] ip3 -> ip3
I0521 02:08:10.190346 18536 net.cpp:150] Setting up ip3
I0521 02:08:10.190358 18536 net.cpp:157] Top shape: 550 11 (6050)
I0521 02:08:10.190366 18536 net.cpp:165] Memory required for data: 868309200
I0521 02:08:10.190383 18536 layer_factory.hpp:77] Creating layer drop3
I0521 02:08:10.190397 18536 net.cpp:106] Creating Layer drop3
I0521 02:08:10.190405 18536 net.cpp:454] drop3 <- ip3
I0521 02:08:10.190418 18536 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:08:10.190459 18536 net.cpp:150] Setting up drop3
I0521 02:08:10.190472 18536 net.cpp:157] Top shape: 550 11 (6050)
I0521 02:08:10.190482 18536 net.cpp:165] Memory required for data: 868333400
I0521 02:08:10.190490 18536 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 02:08:10.190503 18536 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 02:08:10.190513 18536 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 02:08:10.190526 18536 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 02:08:10.190541 18536 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 02:08:10.190613 18536 net.cpp:150] Setting up ip3_drop3_0_split
I0521 02:08:10.190626 18536 net.cpp:157] Top shape: 550 11 (6050)
I0521 02:08:10.190639 18536 net.cpp:157] Top shape: 550 11 (6050)
I0521 02:08:10.190706 18536 net.cpp:165] Memory required for data: 868381800
I0521 02:08:10.190716 18536 layer_factory.hpp:77] Creating layer accuracy
I0521 02:08:10.190738 18536 net.cpp:106] Creating Layer accuracy
I0521 02:08:10.190748 18536 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 02:08:10.190759 18536 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 02:08:10.190773 18536 net.cpp:411] accuracy -> accuracy
I0521 02:08:10.190798 18536 net.cpp:150] Setting up accuracy
I0521 02:08:10.190810 18536 net.cpp:157] Top shape: (1)
I0521 02:08:10.190819 18536 net.cpp:165] Memory required for data: 868381804
I0521 02:08:10.190829 18536 layer_factory.hpp:77] Creating layer loss
I0521 02:08:10.190842 18536 net.cpp:106] Creating Layer loss
I0521 02:08:10.190852 18536 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 02:08:10.190861 18536 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 02:08:10.190874 18536 net.cpp:411] loss -> loss
I0521 02:08:10.190893 18536 layer_factory.hpp:77] Creating layer loss
I0521 02:08:10.191386 18536 net.cpp:150] Setting up loss
I0521 02:08:10.191401 18536 net.cpp:157] Top shape: (1)
I0521 02:08:10.191409 18536 net.cpp:160]     with loss weight 1
I0521 02:08:10.191428 18536 net.cpp:165] Memory required for data: 868381808
I0521 02:08:10.191438 18536 net.cpp:226] loss needs backward computation.
I0521 02:08:10.191449 18536 net.cpp:228] accuracy does not need backward computation.
I0521 02:08:10.191460 18536 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 02:08:10.191470 18536 net.cpp:226] drop3 needs backward computation.
I0521 02:08:10.191480 18536 net.cpp:226] ip3 needs backward computation.
I0521 02:08:10.191490 18536 net.cpp:226] drop2 needs backward computation.
I0521 02:08:10.191509 18536 net.cpp:226] relu6 needs backward computation.
I0521 02:08:10.191516 18536 net.cpp:226] ip2 needs backward computation.
I0521 02:08:10.191526 18536 net.cpp:226] drop1 needs backward computation.
I0521 02:08:10.191536 18536 net.cpp:226] relu5 needs backward computation.
I0521 02:08:10.191545 18536 net.cpp:226] ip1 needs backward computation.
I0521 02:08:10.191555 18536 net.cpp:226] pool4 needs backward computation.
I0521 02:08:10.191565 18536 net.cpp:226] relu4 needs backward computation.
I0521 02:08:10.191573 18536 net.cpp:226] conv4 needs backward computation.
I0521 02:08:10.191584 18536 net.cpp:226] pool3 needs backward computation.
I0521 02:08:10.191594 18536 net.cpp:226] relu3 needs backward computation.
I0521 02:08:10.191603 18536 net.cpp:226] conv3 needs backward computation.
I0521 02:08:10.191613 18536 net.cpp:226] pool2 needs backward computation.
I0521 02:08:10.191624 18536 net.cpp:226] relu2 needs backward computation.
I0521 02:08:10.191634 18536 net.cpp:226] conv2 needs backward computation.
I0521 02:08:10.191644 18536 net.cpp:226] pool1 needs backward computation.
I0521 02:08:10.191654 18536 net.cpp:226] relu1 needs backward computation.
I0521 02:08:10.191665 18536 net.cpp:226] conv1 needs backward computation.
I0521 02:08:10.191676 18536 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 02:08:10.191689 18536 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:08:10.191697 18536 net.cpp:270] This network produces output accuracy
I0521 02:08:10.191709 18536 net.cpp:270] This network produces output loss
I0521 02:08:10.191735 18536 net.cpp:283] Network initialization done.
I0521 02:08:10.191875 18536 solver.cpp:60] Solver scaffolding done.
I0521 02:08:10.193003 18536 caffe.cpp:212] Starting Optimization
I0521 02:08:10.193022 18536 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 02:08:10.193034 18536 solver.cpp:289] Learning Rate Policy: fixed
I0521 02:08:10.194255 18536 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 02:08:56.174459 18536 solver.cpp:409]     Test net output #0: accuracy = 0.0865576
I0521 02:08:56.174618 18536 solver.cpp:409]     Test net output #1: loss = 2.39828 (* 1 = 2.39828 loss)
I0521 02:08:56.281363 18536 solver.cpp:237] Iteration 0, loss = 2.39965
I0521 02:08:56.281399 18536 solver.cpp:253]     Train net output #0: loss = 2.39965 (* 1 = 2.39965 loss)
I0521 02:08:56.281419 18536 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 02:09:04.255667 18536 solver.cpp:237] Iteration 27, loss = 2.37816
I0521 02:09:04.255704 18536 solver.cpp:253]     Train net output #0: loss = 2.37816 (* 1 = 2.37816 loss)
I0521 02:09:04.255717 18536 sgd_solver.cpp:106] Iteration 27, lr = 0.0025
I0521 02:09:12.223666 18536 solver.cpp:237] Iteration 54, loss = 2.35739
I0521 02:09:12.223700 18536 solver.cpp:253]     Train net output #0: loss = 2.35739 (* 1 = 2.35739 loss)
I0521 02:09:12.223714 18536 sgd_solver.cpp:106] Iteration 54, lr = 0.0025
I0521 02:09:20.193861 18536 solver.cpp:237] Iteration 81, loss = 2.35743
I0521 02:09:20.193902 18536 solver.cpp:253]     Train net output #0: loss = 2.35743 (* 1 = 2.35743 loss)
I0521 02:09:20.193922 18536 sgd_solver.cpp:106] Iteration 81, lr = 0.0025
I0521 02:09:28.165091 18536 solver.cpp:237] Iteration 108, loss = 2.32843
I0521 02:09:28.165236 18536 solver.cpp:253]     Train net output #0: loss = 2.32843 (* 1 = 2.32843 loss)
I0521 02:09:28.165249 18536 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0521 02:09:36.146306 18536 solver.cpp:237] Iteration 135, loss = 2.31492
I0521 02:09:36.146337 18536 solver.cpp:253]     Train net output #0: loss = 2.31492 (* 1 = 2.31492 loss)
I0521 02:09:36.146353 18536 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 02:09:44.123579 18536 solver.cpp:237] Iteration 162, loss = 2.3056
I0521 02:09:44.123622 18536 solver.cpp:253]     Train net output #0: loss = 2.3056 (* 1 = 2.3056 loss)
I0521 02:09:44.123637 18536 sgd_solver.cpp:106] Iteration 162, lr = 0.0025
I0521 02:10:14.291326 18536 solver.cpp:237] Iteration 189, loss = 2.30153
I0521 02:10:14.291487 18536 solver.cpp:253]     Train net output #0: loss = 2.30153 (* 1 = 2.30153 loss)
I0521 02:10:14.291501 18536 sgd_solver.cpp:106] Iteration 189, lr = 0.0025
I0521 02:10:22.273269 18536 solver.cpp:237] Iteration 216, loss = 2.31028
I0521 02:10:22.273303 18536 solver.cpp:253]     Train net output #0: loss = 2.31028 (* 1 = 2.31028 loss)
I0521 02:10:22.273316 18536 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 02:10:30.252879 18536 solver.cpp:237] Iteration 243, loss = 2.32315
I0521 02:10:30.252914 18536 solver.cpp:253]     Train net output #0: loss = 2.32315 (* 1 = 2.32315 loss)
I0521 02:10:30.252926 18536 sgd_solver.cpp:106] Iteration 243, lr = 0.0025
I0521 02:10:38.231492 18536 solver.cpp:237] Iteration 270, loss = 2.26957
I0521 02:10:38.231526 18536 solver.cpp:253]     Train net output #0: loss = 2.26957 (* 1 = 2.26957 loss)
I0521 02:10:38.231539 18536 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 02:10:38.527429 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_272.caffemodel
I0521 02:10:38.776788 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_272.solverstate
I0521 02:10:46.277809 18536 solver.cpp:237] Iteration 297, loss = 2.23808
I0521 02:10:46.277961 18536 solver.cpp:253]     Train net output #0: loss = 2.23808 (* 1 = 2.23808 loss)
I0521 02:10:46.277976 18536 sgd_solver.cpp:106] Iteration 297, lr = 0.0025
I0521 02:10:54.246732 18536 solver.cpp:237] Iteration 324, loss = 2.21522
I0521 02:10:54.246765 18536 solver.cpp:253]     Train net output #0: loss = 2.21522 (* 1 = 2.21522 loss)
I0521 02:10:54.246778 18536 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0521 02:11:02.218796 18536 solver.cpp:237] Iteration 351, loss = 2.24021
I0521 02:11:02.218832 18536 solver.cpp:253]     Train net output #0: loss = 2.24021 (* 1 = 2.24021 loss)
I0521 02:11:02.218847 18536 sgd_solver.cpp:106] Iteration 351, lr = 0.0025
I0521 02:11:32.385154 18536 solver.cpp:237] Iteration 378, loss = 2.14273
I0521 02:11:32.385308 18536 solver.cpp:253]     Train net output #0: loss = 2.14273 (* 1 = 2.14273 loss)
I0521 02:11:32.385323 18536 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 02:11:40.369330 18536 solver.cpp:237] Iteration 405, loss = 2.1474
I0521 02:11:40.369362 18536 solver.cpp:253]     Train net output #0: loss = 2.1474 (* 1 = 2.1474 loss)
I0521 02:11:40.369379 18536 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 02:11:48.350574 18536 solver.cpp:237] Iteration 432, loss = 2.0632
I0521 02:11:48.350606 18536 solver.cpp:253]     Train net output #0: loss = 2.0632 (* 1 = 2.0632 loss)
I0521 02:11:48.350618 18536 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 02:11:56.333662 18536 solver.cpp:237] Iteration 459, loss = 2.04982
I0521 02:11:56.333696 18536 solver.cpp:253]     Train net output #0: loss = 2.04982 (* 1 = 2.04982 loss)
I0521 02:11:56.333710 18536 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0521 02:12:04.312269 18536 solver.cpp:237] Iteration 486, loss = 2.01265
I0521 02:12:04.312412 18536 solver.cpp:253]     Train net output #0: loss = 2.01265 (* 1 = 2.01265 loss)
I0521 02:12:04.312425 18536 sgd_solver.cpp:106] Iteration 486, lr = 0.0025
I0521 02:12:12.293937 18536 solver.cpp:237] Iteration 513, loss = 1.99755
I0521 02:12:12.293970 18536 solver.cpp:253]     Train net output #0: loss = 1.99755 (* 1 = 1.99755 loss)
I0521 02:12:12.293982 18536 sgd_solver.cpp:106] Iteration 513, lr = 0.0025
I0521 02:12:20.273903 18536 solver.cpp:237] Iteration 540, loss = 1.96548
I0521 02:12:20.273936 18536 solver.cpp:253]     Train net output #0: loss = 1.96548 (* 1 = 1.96548 loss)
I0521 02:12:20.273952 18536 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 02:12:21.161402 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_544.caffemodel
I0521 02:12:21.407361 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_544.solverstate
I0521 02:12:21.521913 18536 solver.cpp:341] Iteration 545, Testing net (#0)
I0521 02:13:06.768137 18536 solver.cpp:409]     Test net output #0: accuracy = 0.549465
I0521 02:13:06.768303 18536 solver.cpp:409]     Test net output #1: loss = 1.682 (* 1 = 1.682 loss)
I0521 02:13:35.471379 18536 solver.cpp:237] Iteration 567, loss = 1.964
I0521 02:13:35.471427 18536 solver.cpp:253]     Train net output #0: loss = 1.964 (* 1 = 1.964 loss)
I0521 02:13:35.471441 18536 sgd_solver.cpp:106] Iteration 567, lr = 0.0025
I0521 02:13:43.454708 18536 solver.cpp:237] Iteration 594, loss = 1.88347
I0521 02:13:43.454854 18536 solver.cpp:253]     Train net output #0: loss = 1.88347 (* 1 = 1.88347 loss)
I0521 02:13:43.454866 18536 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 02:13:51.431823 18536 solver.cpp:237] Iteration 621, loss = 1.86841
I0521 02:13:51.431859 18536 solver.cpp:253]     Train net output #0: loss = 1.86841 (* 1 = 1.86841 loss)
I0521 02:13:51.431874 18536 sgd_solver.cpp:106] Iteration 621, lr = 0.0025
I0521 02:13:59.412802 18536 solver.cpp:237] Iteration 648, loss = 1.93176
I0521 02:13:59.412834 18536 solver.cpp:253]     Train net output #0: loss = 1.93176 (* 1 = 1.93176 loss)
I0521 02:13:59.412849 18536 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 02:14:07.382582 18536 solver.cpp:237] Iteration 675, loss = 1.90507
I0521 02:14:07.382616 18536 solver.cpp:253]     Train net output #0: loss = 1.90507 (* 1 = 1.90507 loss)
I0521 02:14:07.382632 18536 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 02:14:15.357511 18536 solver.cpp:237] Iteration 702, loss = 1.87836
I0521 02:14:15.357656 18536 solver.cpp:253]     Train net output #0: loss = 1.87836 (* 1 = 1.87836 loss)
I0521 02:14:15.357671 18536 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 02:14:45.563418 18536 solver.cpp:237] Iteration 729, loss = 1.87208
I0521 02:14:45.563581 18536 solver.cpp:253]     Train net output #0: loss = 1.87208 (* 1 = 1.87208 loss)
I0521 02:14:45.563597 18536 sgd_solver.cpp:106] Iteration 729, lr = 0.0025
I0521 02:14:53.542229 18536 solver.cpp:237] Iteration 756, loss = 1.86798
I0521 02:14:53.542261 18536 solver.cpp:253]     Train net output #0: loss = 1.86798 (* 1 = 1.86798 loss)
I0521 02:14:53.542278 18536 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 02:15:01.516412 18536 solver.cpp:237] Iteration 783, loss = 1.83748
I0521 02:15:01.516448 18536 solver.cpp:253]     Train net output #0: loss = 1.83748 (* 1 = 1.83748 loss)
I0521 02:15:01.516463 18536 sgd_solver.cpp:106] Iteration 783, lr = 0.0025
I0521 02:15:09.496856 18536 solver.cpp:237] Iteration 810, loss = 1.81427
I0521 02:15:09.496888 18536 solver.cpp:253]     Train net output #0: loss = 1.81427 (* 1 = 1.81427 loss)
I0521 02:15:09.496901 18536 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 02:15:10.974710 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_816.caffemodel
I0521 02:15:11.308670 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_816.solverstate
I0521 02:15:17.809409 18536 solver.cpp:237] Iteration 837, loss = 1.86757
I0521 02:15:17.809574 18536 solver.cpp:253]     Train net output #0: loss = 1.86757 (* 1 = 1.86757 loss)
I0521 02:15:17.809588 18536 sgd_solver.cpp:106] Iteration 837, lr = 0.0025
I0521 02:15:25.789930 18536 solver.cpp:237] Iteration 864, loss = 1.86426
I0521 02:15:25.789963 18536 solver.cpp:253]     Train net output #0: loss = 1.86426 (* 1 = 1.86426 loss)
I0521 02:15:25.789978 18536 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 02:15:33.767880 18536 solver.cpp:237] Iteration 891, loss = 1.81215
I0521 02:15:33.767911 18536 solver.cpp:253]     Train net output #0: loss = 1.81215 (* 1 = 1.81215 loss)
I0521 02:15:33.767923 18536 sgd_solver.cpp:106] Iteration 891, lr = 0.0025
I0521 02:16:03.898207 18536 solver.cpp:237] Iteration 918, loss = 1.76298
I0521 02:16:03.898372 18536 solver.cpp:253]     Train net output #0: loss = 1.76298 (* 1 = 1.76298 loss)
I0521 02:16:03.898387 18536 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 02:16:11.874984 18536 solver.cpp:237] Iteration 945, loss = 1.90284
I0521 02:16:11.875017 18536 solver.cpp:253]     Train net output #0: loss = 1.90284 (* 1 = 1.90284 loss)
I0521 02:16:11.875033 18536 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 02:16:19.848423 18536 solver.cpp:237] Iteration 972, loss = 1.85767
I0521 02:16:19.848459 18536 solver.cpp:253]     Train net output #0: loss = 1.85767 (* 1 = 1.85767 loss)
I0521 02:16:19.848481 18536 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0521 02:16:27.826597 18536 solver.cpp:237] Iteration 999, loss = 1.86055
I0521 02:16:27.826630 18536 solver.cpp:253]     Train net output #0: loss = 1.86055 (* 1 = 1.86055 loss)
I0521 02:16:27.826645 18536 sgd_solver.cpp:106] Iteration 999, lr = 0.0025
I0521 02:16:35.807492 18536 solver.cpp:237] Iteration 1026, loss = 1.72622
I0521 02:16:35.807626 18536 solver.cpp:253]     Train net output #0: loss = 1.72622 (* 1 = 1.72622 loss)
I0521 02:16:35.807639 18536 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 02:16:43.785558 18536 solver.cpp:237] Iteration 1053, loss = 1.80826
I0521 02:16:43.785596 18536 solver.cpp:253]     Train net output #0: loss = 1.80826 (* 1 = 1.80826 loss)
I0521 02:16:43.785611 18536 sgd_solver.cpp:106] Iteration 1053, lr = 0.0025
I0521 02:16:51.766785 18536 solver.cpp:237] Iteration 1080, loss = 1.74619
I0521 02:16:51.766819 18536 solver.cpp:253]     Train net output #0: loss = 1.74619 (* 1 = 1.74619 loss)
I0521 02:16:51.766834 18536 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 02:16:53.832706 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1088.caffemodel
I0521 02:16:54.080529 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1088.solverstate
I0521 02:16:54.494024 18536 solver.cpp:341] Iteration 1090, Testing net (#0)
I0521 02:18:03.420826 18536 solver.cpp:409]     Test net output #0: accuracy = 0.633623
I0521 02:18:03.420992 18536 solver.cpp:409]     Test net output #1: loss = 1.29806 (* 1 = 1.29806 loss)
I0521 02:18:30.691696 18536 solver.cpp:237] Iteration 1107, loss = 1.86475
I0521 02:18:30.691745 18536 solver.cpp:253]     Train net output #0: loss = 1.86475 (* 1 = 1.86475 loss)
I0521 02:18:30.691761 18536 sgd_solver.cpp:106] Iteration 1107, lr = 0.0025
I0521 02:18:38.660944 18536 solver.cpp:237] Iteration 1134, loss = 1.80645
I0521 02:18:38.661089 18536 solver.cpp:253]     Train net output #0: loss = 1.80645 (* 1 = 1.80645 loss)
I0521 02:18:38.661103 18536 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 02:18:46.630838 18536 solver.cpp:237] Iteration 1161, loss = 1.78971
I0521 02:18:46.630870 18536 solver.cpp:253]     Train net output #0: loss = 1.78971 (* 1 = 1.78971 loss)
I0521 02:18:46.630887 18536 sgd_solver.cpp:106] Iteration 1161, lr = 0.0025
I0521 02:18:54.599314 18536 solver.cpp:237] Iteration 1188, loss = 1.75098
I0521 02:18:54.599355 18536 solver.cpp:253]     Train net output #0: loss = 1.75098 (* 1 = 1.75098 loss)
I0521 02:18:54.599371 18536 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 02:19:02.568915 18536 solver.cpp:237] Iteration 1215, loss = 1.81827
I0521 02:19:02.568948 18536 solver.cpp:253]     Train net output #0: loss = 1.81827 (* 1 = 1.81827 loss)
I0521 02:19:02.568964 18536 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 02:19:10.535430 18536 solver.cpp:237] Iteration 1242, loss = 1.65295
I0521 02:19:10.535564 18536 solver.cpp:253]     Train net output #0: loss = 1.65295 (* 1 = 1.65295 loss)
I0521 02:19:10.535578 18536 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 02:19:18.502872 18536 solver.cpp:237] Iteration 1269, loss = 1.68453
I0521 02:19:18.502912 18536 solver.cpp:253]     Train net output #0: loss = 1.68453 (* 1 = 1.68453 loss)
I0521 02:19:18.502931 18536 sgd_solver.cpp:106] Iteration 1269, lr = 0.0025
I0521 02:19:48.598058 18536 solver.cpp:237] Iteration 1296, loss = 1.87497
I0521 02:19:48.598217 18536 solver.cpp:253]     Train net output #0: loss = 1.87497 (* 1 = 1.87497 loss)
I0521 02:19:48.598232 18536 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 02:19:56.565040 18536 solver.cpp:237] Iteration 1323, loss = 1.71125
I0521 02:19:56.565073 18536 solver.cpp:253]     Train net output #0: loss = 1.71125 (* 1 = 1.71125 loss)
I0521 02:19:56.565089 18536 sgd_solver.cpp:106] Iteration 1323, lr = 0.0025
I0521 02:20:04.529400 18536 solver.cpp:237] Iteration 1350, loss = 1.70985
I0521 02:20:04.529443 18536 solver.cpp:253]     Train net output #0: loss = 1.70985 (* 1 = 1.70985 loss)
I0521 02:20:04.529458 18536 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 02:20:07.189424 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1360.caffemodel
I0521 02:20:07.436084 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1360.solverstate
I0521 02:20:12.571393 18536 solver.cpp:237] Iteration 1377, loss = 1.69991
I0521 02:20:12.571440 18536 solver.cpp:253]     Train net output #0: loss = 1.69991 (* 1 = 1.69991 loss)
I0521 02:20:12.571454 18536 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0521 02:20:20.542407 18536 solver.cpp:237] Iteration 1404, loss = 1.80463
I0521 02:20:20.542549 18536 solver.cpp:253]     Train net output #0: loss = 1.80463 (* 1 = 1.80463 loss)
I0521 02:20:20.542563 18536 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 02:20:28.513073 18536 solver.cpp:237] Iteration 1431, loss = 1.70986
I0521 02:20:28.513116 18536 solver.cpp:253]     Train net output #0: loss = 1.70986 (* 1 = 1.70986 loss)
I0521 02:20:28.513139 18536 sgd_solver.cpp:106] Iteration 1431, lr = 0.0025
I0521 02:20:58.630713 18536 solver.cpp:237] Iteration 1458, loss = 1.67852
I0521 02:20:58.630887 18536 solver.cpp:253]     Train net output #0: loss = 1.67852 (* 1 = 1.67852 loss)
I0521 02:20:58.630903 18536 sgd_solver.cpp:106] Iteration 1458, lr = 0.0025
I0521 02:21:06.597278 18536 solver.cpp:237] Iteration 1485, loss = 1.74235
I0521 02:21:06.597311 18536 solver.cpp:253]     Train net output #0: loss = 1.74235 (* 1 = 1.74235 loss)
I0521 02:21:06.597326 18536 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 02:21:14.571352 18536 solver.cpp:237] Iteration 1512, loss = 1.70178
I0521 02:21:14.571387 18536 solver.cpp:253]     Train net output #0: loss = 1.70178 (* 1 = 1.70178 loss)
I0521 02:21:14.571400 18536 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 02:21:22.536747 18536 solver.cpp:237] Iteration 1539, loss = 1.63477
I0521 02:21:22.536788 18536 solver.cpp:253]     Train net output #0: loss = 1.63477 (* 1 = 1.63477 loss)
I0521 02:21:22.536805 18536 sgd_solver.cpp:106] Iteration 1539, lr = 0.0025
I0521 02:21:30.503263 18536 solver.cpp:237] Iteration 1566, loss = 1.76672
I0521 02:21:30.503403 18536 solver.cpp:253]     Train net output #0: loss = 1.76672 (* 1 = 1.76672 loss)
I0521 02:21:30.503417 18536 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 02:21:38.472769 18536 solver.cpp:237] Iteration 1593, loss = 1.78371
I0521 02:21:38.472801 18536 solver.cpp:253]     Train net output #0: loss = 1.78371 (* 1 = 1.78371 loss)
I0521 02:21:38.472817 18536 sgd_solver.cpp:106] Iteration 1593, lr = 0.0025
I0521 02:21:46.442611 18536 solver.cpp:237] Iteration 1620, loss = 1.75875
I0521 02:21:46.442656 18536 solver.cpp:253]     Train net output #0: loss = 1.75875 (* 1 = 1.75875 loss)
I0521 02:21:46.442672 18536 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 02:21:49.689818 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1632.caffemodel
I0521 02:21:49.935053 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1632.solverstate
I0521 02:21:50.640547 18536 solver.cpp:341] Iteration 1635, Testing net (#0)
I0521 02:22:35.561187 18536 solver.cpp:409]     Test net output #0: accuracy = 0.662233
I0521 02:22:35.561342 18536 solver.cpp:409]     Test net output #1: loss = 1.16442 (* 1 = 1.16442 loss)
I0521 02:23:01.395582 18536 solver.cpp:237] Iteration 1647, loss = 1.76309
I0521 02:23:01.395630 18536 solver.cpp:253]     Train net output #0: loss = 1.76309 (* 1 = 1.76309 loss)
I0521 02:23:01.395647 18536 sgd_solver.cpp:106] Iteration 1647, lr = 0.0025
I0521 02:23:09.362761 18536 solver.cpp:237] Iteration 1674, loss = 1.77817
I0521 02:23:09.362911 18536 solver.cpp:253]     Train net output #0: loss = 1.77817 (* 1 = 1.77817 loss)
I0521 02:23:09.362925 18536 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 02:23:17.330690 18536 solver.cpp:237] Iteration 1701, loss = 1.76955
I0521 02:23:17.330724 18536 solver.cpp:253]     Train net output #0: loss = 1.76955 (* 1 = 1.76955 loss)
I0521 02:23:17.330739 18536 sgd_solver.cpp:106] Iteration 1701, lr = 0.0025
I0521 02:23:25.297804 18536 solver.cpp:237] Iteration 1728, loss = 1.7089
I0521 02:23:25.297844 18536 solver.cpp:253]     Train net output #0: loss = 1.7089 (* 1 = 1.7089 loss)
I0521 02:23:25.297863 18536 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 02:23:33.258246 18536 solver.cpp:237] Iteration 1755, loss = 1.671
I0521 02:23:33.258280 18536 solver.cpp:253]     Train net output #0: loss = 1.671 (* 1 = 1.671 loss)
I0521 02:23:33.258293 18536 sgd_solver.cpp:106] Iteration 1755, lr = 0.0025
I0521 02:23:41.227053 18536 solver.cpp:237] Iteration 1782, loss = 1.65956
I0521 02:23:41.227195 18536 solver.cpp:253]     Train net output #0: loss = 1.65956 (* 1 = 1.65956 loss)
I0521 02:23:41.227208 18536 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 02:23:49.194484 18536 solver.cpp:237] Iteration 1809, loss = 1.66464
I0521 02:23:49.194527 18536 solver.cpp:253]     Train net output #0: loss = 1.66464 (* 1 = 1.66464 loss)
I0521 02:23:49.194545 18536 sgd_solver.cpp:106] Iteration 1809, lr = 0.0025
I0521 02:24:19.329193 18536 solver.cpp:237] Iteration 1836, loss = 1.66608
I0521 02:24:19.329365 18536 solver.cpp:253]     Train net output #0: loss = 1.66608 (* 1 = 1.66608 loss)
I0521 02:24:19.329378 18536 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0521 02:24:27.299160 18536 solver.cpp:237] Iteration 1863, loss = 1.68088
I0521 02:24:27.299192 18536 solver.cpp:253]     Train net output #0: loss = 1.68088 (* 1 = 1.68088 loss)
I0521 02:24:27.299208 18536 sgd_solver.cpp:106] Iteration 1863, lr = 0.0025
I0521 02:24:35.265053 18536 solver.cpp:237] Iteration 1890, loss = 1.59403
I0521 02:24:35.265087 18536 solver.cpp:253]     Train net output #0: loss = 1.59403 (* 1 = 1.59403 loss)
I0521 02:24:35.265101 18536 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0521 02:24:39.100965 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1904.caffemodel
I0521 02:24:39.346032 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_1904.solverstate
I0521 02:24:43.296569 18536 solver.cpp:237] Iteration 1917, loss = 1.70519
I0521 02:24:43.296615 18536 solver.cpp:253]     Train net output #0: loss = 1.70519 (* 1 = 1.70519 loss)
I0521 02:24:43.296630 18536 sgd_solver.cpp:106] Iteration 1917, lr = 0.0025
I0521 02:24:51.263926 18536 solver.cpp:237] Iteration 1944, loss = 1.62559
I0521 02:24:51.264080 18536 solver.cpp:253]     Train net output #0: loss = 1.62559 (* 1 = 1.62559 loss)
I0521 02:24:51.264093 18536 sgd_solver.cpp:106] Iteration 1944, lr = 0.0025
I0521 02:24:59.231869 18536 solver.cpp:237] Iteration 1971, loss = 1.69954
I0521 02:24:59.231901 18536 solver.cpp:253]     Train net output #0: loss = 1.69954 (* 1 = 1.69954 loss)
I0521 02:24:59.231915 18536 sgd_solver.cpp:106] Iteration 1971, lr = 0.0025
I0521 02:25:07.200196 18536 solver.cpp:237] Iteration 1998, loss = 1.78596
I0521 02:25:07.200235 18536 solver.cpp:253]     Train net output #0: loss = 1.78596 (* 1 = 1.78596 loss)
I0521 02:25:07.200253 18536 sgd_solver.cpp:106] Iteration 1998, lr = 0.0025
I0521 02:25:37.342350 18536 solver.cpp:237] Iteration 2025, loss = 1.72403
I0521 02:25:37.342519 18536 solver.cpp:253]     Train net output #0: loss = 1.72403 (* 1 = 1.72403 loss)
I0521 02:25:37.342535 18536 sgd_solver.cpp:106] Iteration 2025, lr = 0.0025
I0521 02:25:45.309653 18536 solver.cpp:237] Iteration 2052, loss = 1.66301
I0521 02:25:45.309686 18536 solver.cpp:253]     Train net output #0: loss = 1.66301 (* 1 = 1.66301 loss)
I0521 02:25:45.309698 18536 sgd_solver.cpp:106] Iteration 2052, lr = 0.0025
I0521 02:25:53.277694 18536 solver.cpp:237] Iteration 2079, loss = 1.69355
I0521 02:25:53.277727 18536 solver.cpp:253]     Train net output #0: loss = 1.69355 (* 1 = 1.69355 loss)
I0521 02:25:53.277742 18536 sgd_solver.cpp:106] Iteration 2079, lr = 0.0025
I0521 02:26:01.247803 18536 solver.cpp:237] Iteration 2106, loss = 1.66852
I0521 02:26:01.247840 18536 solver.cpp:253]     Train net output #0: loss = 1.66852 (* 1 = 1.66852 loss)
I0521 02:26:01.247853 18536 sgd_solver.cpp:106] Iteration 2106, lr = 0.0025
I0521 02:26:09.216084 18536 solver.cpp:237] Iteration 2133, loss = 1.62451
I0521 02:26:09.216222 18536 solver.cpp:253]     Train net output #0: loss = 1.62451 (* 1 = 1.62451 loss)
I0521 02:26:09.216234 18536 sgd_solver.cpp:106] Iteration 2133, lr = 0.0025
I0521 02:26:17.183125 18536 solver.cpp:237] Iteration 2160, loss = 1.66076
I0521 02:26:17.183156 18536 solver.cpp:253]     Train net output #0: loss = 1.66076 (* 1 = 1.66076 loss)
I0521 02:26:17.183172 18536 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0521 02:26:21.611053 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2176.caffemodel
I0521 02:26:21.855232 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2176.solverstate
I0521 02:26:22.856737 18536 solver.cpp:341] Iteration 2180, Testing net (#0)
I0521 02:27:28.964395 18536 solver.cpp:409]     Test net output #0: accuracy = 0.680427
I0521 02:27:28.964570 18536 solver.cpp:409]     Test net output #1: loss = 1.0879 (* 1 = 1.0879 loss)
I0521 02:27:53.287962 18536 solver.cpp:237] Iteration 2187, loss = 1.62832
I0521 02:27:53.288012 18536 solver.cpp:253]     Train net output #0: loss = 1.62832 (* 1 = 1.62832 loss)
I0521 02:27:53.288027 18536 sgd_solver.cpp:106] Iteration 2187, lr = 0.0025
I0521 02:28:01.258107 18536 solver.cpp:237] Iteration 2214, loss = 1.63927
I0521 02:28:01.258272 18536 solver.cpp:253]     Train net output #0: loss = 1.63927 (* 1 = 1.63927 loss)
I0521 02:28:01.258286 18536 sgd_solver.cpp:106] Iteration 2214, lr = 0.0025
I0521 02:28:09.225776 18536 solver.cpp:237] Iteration 2241, loss = 1.61671
I0521 02:28:09.225808 18536 solver.cpp:253]     Train net output #0: loss = 1.61671 (* 1 = 1.61671 loss)
I0521 02:28:09.225823 18536 sgd_solver.cpp:106] Iteration 2241, lr = 0.0025
I0521 02:28:17.197787 18536 solver.cpp:237] Iteration 2268, loss = 1.6299
I0521 02:28:17.197819 18536 solver.cpp:253]     Train net output #0: loss = 1.6299 (* 1 = 1.6299 loss)
I0521 02:28:17.197834 18536 sgd_solver.cpp:106] Iteration 2268, lr = 0.0025
I0521 02:28:25.166379 18536 solver.cpp:237] Iteration 2295, loss = 1.5692
I0521 02:28:25.166416 18536 solver.cpp:253]     Train net output #0: loss = 1.5692 (* 1 = 1.5692 loss)
I0521 02:28:25.166436 18536 sgd_solver.cpp:106] Iteration 2295, lr = 0.0025
I0521 02:28:33.135596 18536 solver.cpp:237] Iteration 2322, loss = 1.57464
I0521 02:28:33.135736 18536 solver.cpp:253]     Train net output #0: loss = 1.57464 (* 1 = 1.57464 loss)
I0521 02:28:33.135749 18536 sgd_solver.cpp:106] Iteration 2322, lr = 0.0025
I0521 02:28:41.105805 18536 solver.cpp:237] Iteration 2349, loss = 1.59321
I0521 02:28:41.105837 18536 solver.cpp:253]     Train net output #0: loss = 1.59321 (* 1 = 1.59321 loss)
I0521 02:28:41.105854 18536 sgd_solver.cpp:106] Iteration 2349, lr = 0.0025
I0521 02:29:11.220244 18536 solver.cpp:237] Iteration 2376, loss = 1.64968
I0521 02:29:11.220413 18536 solver.cpp:253]     Train net output #0: loss = 1.64968 (* 1 = 1.64968 loss)
I0521 02:29:11.220432 18536 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0521 02:29:19.190055 18536 solver.cpp:237] Iteration 2403, loss = 1.73008
I0521 02:29:19.190099 18536 solver.cpp:253]     Train net output #0: loss = 1.73008 (* 1 = 1.73008 loss)
I0521 02:29:19.190117 18536 sgd_solver.cpp:106] Iteration 2403, lr = 0.0025
I0521 02:29:27.161388 18536 solver.cpp:237] Iteration 2430, loss = 1.6134
I0521 02:29:27.161422 18536 solver.cpp:253]     Train net output #0: loss = 1.6134 (* 1 = 1.6134 loss)
I0521 02:29:27.161437 18536 sgd_solver.cpp:106] Iteration 2430, lr = 0.0025
I0521 02:29:32.180637 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2448.caffemodel
I0521 02:29:32.429173 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2448.solverstate
I0521 02:29:35.201457 18536 solver.cpp:237] Iteration 2457, loss = 1.64116
I0521 02:29:35.201506 18536 solver.cpp:253]     Train net output #0: loss = 1.64116 (* 1 = 1.64116 loss)
I0521 02:29:35.201520 18536 sgd_solver.cpp:106] Iteration 2457, lr = 0.0025
I0521 02:29:43.173663 18536 solver.cpp:237] Iteration 2484, loss = 1.6507
I0521 02:29:43.173841 18536 solver.cpp:253]     Train net output #0: loss = 1.6507 (* 1 = 1.6507 loss)
I0521 02:29:43.173856 18536 sgd_solver.cpp:106] Iteration 2484, lr = 0.0025
I0521 02:29:51.145185 18536 solver.cpp:237] Iteration 2511, loss = 1.64343
I0521 02:29:51.145220 18536 solver.cpp:253]     Train net output #0: loss = 1.64343 (* 1 = 1.64343 loss)
I0521 02:29:51.145234 18536 sgd_solver.cpp:106] Iteration 2511, lr = 0.0025
I0521 02:29:59.116761 18536 solver.cpp:237] Iteration 2538, loss = 1.59684
I0521 02:29:59.116796 18536 solver.cpp:253]     Train net output #0: loss = 1.59684 (* 1 = 1.59684 loss)
I0521 02:29:59.116809 18536 sgd_solver.cpp:106] Iteration 2538, lr = 0.0025
I0521 02:30:29.261886 18536 solver.cpp:237] Iteration 2565, loss = 1.66332
I0521 02:30:29.262061 18536 solver.cpp:253]     Train net output #0: loss = 1.66332 (* 1 = 1.66332 loss)
I0521 02:30:29.262075 18536 sgd_solver.cpp:106] Iteration 2565, lr = 0.0025
I0521 02:30:37.239130 18536 solver.cpp:237] Iteration 2592, loss = 1.72648
I0521 02:30:37.239162 18536 solver.cpp:253]     Train net output #0: loss = 1.72648 (* 1 = 1.72648 loss)
I0521 02:30:37.239178 18536 sgd_solver.cpp:106] Iteration 2592, lr = 0.0025
I0521 02:30:45.210940 18536 solver.cpp:237] Iteration 2619, loss = 1.64466
I0521 02:30:45.210975 18536 solver.cpp:253]     Train net output #0: loss = 1.64466 (* 1 = 1.64466 loss)
I0521 02:30:45.210989 18536 sgd_solver.cpp:106] Iteration 2619, lr = 0.0025
I0521 02:30:53.182307 18536 solver.cpp:237] Iteration 2646, loss = 1.74599
I0521 02:30:53.182339 18536 solver.cpp:253]     Train net output #0: loss = 1.74599 (* 1 = 1.74599 loss)
I0521 02:30:53.182354 18536 sgd_solver.cpp:106] Iteration 2646, lr = 0.0025
I0521 02:31:01.149384 18536 solver.cpp:237] Iteration 2673, loss = 1.63511
I0521 02:31:01.149545 18536 solver.cpp:253]     Train net output #0: loss = 1.63511 (* 1 = 1.63511 loss)
I0521 02:31:01.149559 18536 sgd_solver.cpp:106] Iteration 2673, lr = 0.0025
I0521 02:31:09.118167 18536 solver.cpp:237] Iteration 2700, loss = 1.57112
I0521 02:31:09.118199 18536 solver.cpp:253]     Train net output #0: loss = 1.57112 (* 1 = 1.57112 loss)
I0521 02:31:09.118216 18536 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0521 02:31:14.724422 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2720.caffemodel
I0521 02:31:14.971263 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2720.solverstate
I0521 02:31:16.269949 18536 solver.cpp:341] Iteration 2725, Testing net (#0)
I0521 02:32:01.532878 18536 solver.cpp:409]     Test net output #0: accuracy = 0.700749
I0521 02:32:01.533041 18536 solver.cpp:409]     Test net output #1: loss = 1.05286 (* 1 = 1.05286 loss)
I0521 02:32:01.916250 18536 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2727.caffemodel
I0521 02:32:02.163203 18536 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481_iter_2727.solverstate
I0521 02:32:23.239290 18536 solver.cpp:321] Iteration 2727, loss = 1.63026
I0521 02:32:23.239332 18536 solver.cpp:326] Optimization Done.
I0521 02:32:23.239341 18536 caffe.cpp:215] Optimization Done.
Application 11236502 resources: utime ~1267s, stime ~228s, Rss ~5329712, inblocks ~3744348, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_550_2016-05-20T11.20.52.582481.solver"
	User time (seconds): 0.54
	System time (seconds): 0.17
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:03.06
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15074
	Voluntary context switches: 2907
	Involuntary context switches: 166
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
