2805144
I0520 13:43:59.674728 27950 caffe.cpp:184] Using GPUs 0
I0520 13:44:00.100527 27950 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1666
test_interval: 3333
base_lr: 0.0025
display: 166
max_iter: 16666
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1666
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143.prototxt"
I0520 13:44:00.102177 27950 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143.prototxt
I0520 13:44:00.121925 27950 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 13:44:00.121990 27950 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 13:44:00.122370 27950 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 90
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 13:44:00.122575 27950 layer_factory.hpp:77] Creating layer data_hdf5
I0520 13:44:00.122601 27950 net.cpp:106] Creating Layer data_hdf5
I0520 13:44:00.122622 27950 net.cpp:411] data_hdf5 -> data
I0520 13:44:00.122661 27950 net.cpp:411] data_hdf5 -> label
I0520 13:44:00.122697 27950 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 13:44:00.123993 27950 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 13:44:00.148057 27950 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 13:44:21.689574 27950 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 13:44:21.694874 27950 net.cpp:150] Setting up data_hdf5
I0520 13:44:21.694914 27950 net.cpp:157] Top shape: 90 1 127 50 (571500)
I0520 13:44:21.694931 27950 net.cpp:157] Top shape: 90 (90)
I0520 13:44:21.694944 27950 net.cpp:165] Memory required for data: 2286360
I0520 13:44:21.694963 27950 layer_factory.hpp:77] Creating layer conv1
I0520 13:44:21.695009 27950 net.cpp:106] Creating Layer conv1
I0520 13:44:21.695024 27950 net.cpp:454] conv1 <- data
I0520 13:44:21.695049 27950 net.cpp:411] conv1 -> conv1
I0520 13:44:24.150408 27950 net.cpp:150] Setting up conv1
I0520 13:44:24.150462 27950 net.cpp:157] Top shape: 90 12 120 48 (6220800)
I0520 13:44:24.150486 27950 net.cpp:165] Memory required for data: 27169560
I0520 13:44:24.150516 27950 layer_factory.hpp:77] Creating layer relu1
I0520 13:44:24.150538 27950 net.cpp:106] Creating Layer relu1
I0520 13:44:24.150559 27950 net.cpp:454] relu1 <- conv1
I0520 13:44:24.150595 27950 net.cpp:397] relu1 -> conv1 (in-place)
I0520 13:44:24.151124 27950 net.cpp:150] Setting up relu1
I0520 13:44:24.151149 27950 net.cpp:157] Top shape: 90 12 120 48 (6220800)
I0520 13:44:24.151161 27950 net.cpp:165] Memory required for data: 52052760
I0520 13:44:24.151177 27950 layer_factory.hpp:77] Creating layer pool1
I0520 13:44:24.151206 27950 net.cpp:106] Creating Layer pool1
I0520 13:44:24.151218 27950 net.cpp:454] pool1 <- conv1
I0520 13:44:24.151234 27950 net.cpp:411] pool1 -> pool1
I0520 13:44:24.151329 27950 net.cpp:150] Setting up pool1
I0520 13:44:24.151347 27950 net.cpp:157] Top shape: 90 12 60 48 (3110400)
I0520 13:44:24.151368 27950 net.cpp:165] Memory required for data: 64494360
I0520 13:44:24.151382 27950 layer_factory.hpp:77] Creating layer conv2
I0520 13:44:24.151407 27950 net.cpp:106] Creating Layer conv2
I0520 13:44:24.151420 27950 net.cpp:454] conv2 <- pool1
I0520 13:44:24.151438 27950 net.cpp:411] conv2 -> conv2
I0520 13:44:24.154202 27950 net.cpp:150] Setting up conv2
I0520 13:44:24.154237 27950 net.cpp:157] Top shape: 90 20 54 46 (4471200)
I0520 13:44:24.154250 27950 net.cpp:165] Memory required for data: 82379160
I0520 13:44:24.154278 27950 layer_factory.hpp:77] Creating layer relu2
I0520 13:44:24.154295 27950 net.cpp:106] Creating Layer relu2
I0520 13:44:24.154320 27950 net.cpp:454] relu2 <- conv2
I0520 13:44:24.154336 27950 net.cpp:397] relu2 -> conv2 (in-place)
I0520 13:44:24.154697 27950 net.cpp:150] Setting up relu2
I0520 13:44:24.154717 27950 net.cpp:157] Top shape: 90 20 54 46 (4471200)
I0520 13:44:24.154731 27950 net.cpp:165] Memory required for data: 100263960
I0520 13:44:24.154745 27950 layer_factory.hpp:77] Creating layer pool2
I0520 13:44:24.154769 27950 net.cpp:106] Creating Layer pool2
I0520 13:44:24.154783 27950 net.cpp:454] pool2 <- conv2
I0520 13:44:24.154817 27950 net.cpp:411] pool2 -> pool2
I0520 13:44:24.154902 27950 net.cpp:150] Setting up pool2
I0520 13:44:24.154919 27950 net.cpp:157] Top shape: 90 20 27 46 (2235600)
I0520 13:44:24.154934 27950 net.cpp:165] Memory required for data: 109206360
I0520 13:44:24.154947 27950 layer_factory.hpp:77] Creating layer conv3
I0520 13:44:24.154974 27950 net.cpp:106] Creating Layer conv3
I0520 13:44:24.154988 27950 net.cpp:454] conv3 <- pool2
I0520 13:44:24.155004 27950 net.cpp:411] conv3 -> conv3
I0520 13:44:24.156965 27950 net.cpp:150] Setting up conv3
I0520 13:44:24.156988 27950 net.cpp:157] Top shape: 90 28 22 44 (2439360)
I0520 13:44:24.157009 27950 net.cpp:165] Memory required for data: 118963800
I0520 13:44:24.157032 27950 layer_factory.hpp:77] Creating layer relu3
I0520 13:44:24.157054 27950 net.cpp:106] Creating Layer relu3
I0520 13:44:24.157076 27950 net.cpp:454] relu3 <- conv3
I0520 13:44:24.157093 27950 net.cpp:397] relu3 -> conv3 (in-place)
I0520 13:44:24.157598 27950 net.cpp:150] Setting up relu3
I0520 13:44:24.157621 27950 net.cpp:157] Top shape: 90 28 22 44 (2439360)
I0520 13:44:24.157635 27950 net.cpp:165] Memory required for data: 128721240
I0520 13:44:24.157651 27950 layer_factory.hpp:77] Creating layer pool3
I0520 13:44:24.157675 27950 net.cpp:106] Creating Layer pool3
I0520 13:44:24.157688 27950 net.cpp:454] pool3 <- conv3
I0520 13:44:24.157704 27950 net.cpp:411] pool3 -> pool3
I0520 13:44:24.157788 27950 net.cpp:150] Setting up pool3
I0520 13:44:24.157805 27950 net.cpp:157] Top shape: 90 28 11 44 (1219680)
I0520 13:44:24.157820 27950 net.cpp:165] Memory required for data: 133599960
I0520 13:44:24.157832 27950 layer_factory.hpp:77] Creating layer conv4
I0520 13:44:24.157860 27950 net.cpp:106] Creating Layer conv4
I0520 13:44:24.157872 27950 net.cpp:454] conv4 <- pool3
I0520 13:44:24.157889 27950 net.cpp:411] conv4 -> conv4
I0520 13:44:24.160902 27950 net.cpp:150] Setting up conv4
I0520 13:44:24.160933 27950 net.cpp:157] Top shape: 90 36 6 42 (816480)
I0520 13:44:24.160948 27950 net.cpp:165] Memory required for data: 136865880
I0520 13:44:24.160966 27950 layer_factory.hpp:77] Creating layer relu4
I0520 13:44:24.160989 27950 net.cpp:106] Creating Layer relu4
I0520 13:44:24.161015 27950 net.cpp:454] relu4 <- conv4
I0520 13:44:24.161031 27950 net.cpp:397] relu4 -> conv4 (in-place)
I0520 13:44:24.161537 27950 net.cpp:150] Setting up relu4
I0520 13:44:24.161561 27950 net.cpp:157] Top shape: 90 36 6 42 (816480)
I0520 13:44:24.161573 27950 net.cpp:165] Memory required for data: 140131800
I0520 13:44:24.161590 27950 layer_factory.hpp:77] Creating layer pool4
I0520 13:44:24.161605 27950 net.cpp:106] Creating Layer pool4
I0520 13:44:24.161628 27950 net.cpp:454] pool4 <- conv4
I0520 13:44:24.161643 27950 net.cpp:411] pool4 -> pool4
I0520 13:44:24.161727 27950 net.cpp:150] Setting up pool4
I0520 13:44:24.161751 27950 net.cpp:157] Top shape: 90 36 3 42 (408240)
I0520 13:44:24.161762 27950 net.cpp:165] Memory required for data: 141764760
I0520 13:44:24.161777 27950 layer_factory.hpp:77] Creating layer ip1
I0520 13:44:24.161798 27950 net.cpp:106] Creating Layer ip1
I0520 13:44:24.161818 27950 net.cpp:454] ip1 <- pool4
I0520 13:44:24.161834 27950 net.cpp:411] ip1 -> ip1
I0520 13:44:24.177289 27950 net.cpp:150] Setting up ip1
I0520 13:44:24.177320 27950 net.cpp:157] Top shape: 90 196 (17640)
I0520 13:44:24.177342 27950 net.cpp:165] Memory required for data: 141835320
I0520 13:44:24.177368 27950 layer_factory.hpp:77] Creating layer relu5
I0520 13:44:24.177389 27950 net.cpp:106] Creating Layer relu5
I0520 13:44:24.177414 27950 net.cpp:454] relu5 <- ip1
I0520 13:44:24.177433 27950 net.cpp:397] relu5 -> ip1 (in-place)
I0520 13:44:24.177786 27950 net.cpp:150] Setting up relu5
I0520 13:44:24.177806 27950 net.cpp:157] Top shape: 90 196 (17640)
I0520 13:44:24.177819 27950 net.cpp:165] Memory required for data: 141905880
I0520 13:44:24.177834 27950 layer_factory.hpp:77] Creating layer drop1
I0520 13:44:24.177865 27950 net.cpp:106] Creating Layer drop1
I0520 13:44:24.177880 27950 net.cpp:454] drop1 <- ip1
I0520 13:44:24.177894 27950 net.cpp:397] drop1 -> ip1 (in-place)
I0520 13:44:24.177968 27950 net.cpp:150] Setting up drop1
I0520 13:44:24.177984 27950 net.cpp:157] Top shape: 90 196 (17640)
I0520 13:44:24.177996 27950 net.cpp:165] Memory required for data: 141976440
I0520 13:44:24.178011 27950 layer_factory.hpp:77] Creating layer ip2
I0520 13:44:24.178032 27950 net.cpp:106] Creating Layer ip2
I0520 13:44:24.178052 27950 net.cpp:454] ip2 <- ip1
I0520 13:44:24.178077 27950 net.cpp:411] ip2 -> ip2
I0520 13:44:24.178565 27950 net.cpp:150] Setting up ip2
I0520 13:44:24.178583 27950 net.cpp:157] Top shape: 90 98 (8820)
I0520 13:44:24.178596 27950 net.cpp:165] Memory required for data: 142011720
I0520 13:44:24.178617 27950 layer_factory.hpp:77] Creating layer relu6
I0520 13:44:24.178639 27950 net.cpp:106] Creating Layer relu6
I0520 13:44:24.178653 27950 net.cpp:454] relu6 <- ip2
I0520 13:44:24.178668 27950 net.cpp:397] relu6 -> ip2 (in-place)
I0520 13:44:24.179219 27950 net.cpp:150] Setting up relu6
I0520 13:44:24.179242 27950 net.cpp:157] Top shape: 90 98 (8820)
I0520 13:44:24.179256 27950 net.cpp:165] Memory required for data: 142047000
I0520 13:44:24.179271 27950 layer_factory.hpp:77] Creating layer drop2
I0520 13:44:24.179294 27950 net.cpp:106] Creating Layer drop2
I0520 13:44:24.179308 27950 net.cpp:454] drop2 <- ip2
I0520 13:44:24.179324 27950 net.cpp:397] drop2 -> ip2 (in-place)
I0520 13:44:24.179379 27950 net.cpp:150] Setting up drop2
I0520 13:44:24.179395 27950 net.cpp:157] Top shape: 90 98 (8820)
I0520 13:44:24.179409 27950 net.cpp:165] Memory required for data: 142082280
I0520 13:44:24.179422 27950 layer_factory.hpp:77] Creating layer ip3
I0520 13:44:24.179438 27950 net.cpp:106] Creating Layer ip3
I0520 13:44:24.179453 27950 net.cpp:454] ip3 <- ip2
I0520 13:44:24.179474 27950 net.cpp:411] ip3 -> ip3
I0520 13:44:24.179698 27950 net.cpp:150] Setting up ip3
I0520 13:44:24.179718 27950 net.cpp:157] Top shape: 90 11 (990)
I0520 13:44:24.179730 27950 net.cpp:165] Memory required for data: 142086240
I0520 13:44:24.179751 27950 layer_factory.hpp:77] Creating layer drop3
I0520 13:44:24.179774 27950 net.cpp:106] Creating Layer drop3
I0520 13:44:24.179786 27950 net.cpp:454] drop3 <- ip3
I0520 13:44:24.179801 27950 net.cpp:397] drop3 -> ip3 (in-place)
I0520 13:44:24.179847 27950 net.cpp:150] Setting up drop3
I0520 13:44:24.179870 27950 net.cpp:157] Top shape: 90 11 (990)
I0520 13:44:24.179883 27950 net.cpp:165] Memory required for data: 142090200
I0520 13:44:24.179901 27950 layer_factory.hpp:77] Creating layer loss
I0520 13:44:24.179924 27950 net.cpp:106] Creating Layer loss
I0520 13:44:24.179939 27950 net.cpp:454] loss <- ip3
I0520 13:44:24.179960 27950 net.cpp:454] loss <- label
I0520 13:44:24.179975 27950 net.cpp:411] loss -> loss
I0520 13:44:24.179994 27950 layer_factory.hpp:77] Creating layer loss
I0520 13:44:24.180667 27950 net.cpp:150] Setting up loss
I0520 13:44:24.180690 27950 net.cpp:157] Top shape: (1)
I0520 13:44:24.180706 27950 net.cpp:160]     with loss weight 1
I0520 13:44:24.180757 27950 net.cpp:165] Memory required for data: 142090204
I0520 13:44:24.180779 27950 net.cpp:226] loss needs backward computation.
I0520 13:44:24.180794 27950 net.cpp:226] drop3 needs backward computation.
I0520 13:44:24.180811 27950 net.cpp:226] ip3 needs backward computation.
I0520 13:44:24.180824 27950 net.cpp:226] drop2 needs backward computation.
I0520 13:44:24.180836 27950 net.cpp:226] relu6 needs backward computation.
I0520 13:44:24.180851 27950 net.cpp:226] ip2 needs backward computation.
I0520 13:44:24.180863 27950 net.cpp:226] drop1 needs backward computation.
I0520 13:44:24.180886 27950 net.cpp:226] relu5 needs backward computation.
I0520 13:44:24.180897 27950 net.cpp:226] ip1 needs backward computation.
I0520 13:44:24.180913 27950 net.cpp:226] pool4 needs backward computation.
I0520 13:44:24.180927 27950 net.cpp:226] relu4 needs backward computation.
I0520 13:44:24.180938 27950 net.cpp:226] conv4 needs backward computation.
I0520 13:44:24.180954 27950 net.cpp:226] pool3 needs backward computation.
I0520 13:44:24.180974 27950 net.cpp:226] relu3 needs backward computation.
I0520 13:44:24.180996 27950 net.cpp:226] conv3 needs backward computation.
I0520 13:44:24.181010 27950 net.cpp:226] pool2 needs backward computation.
I0520 13:44:24.181023 27950 net.cpp:226] relu2 needs backward computation.
I0520 13:44:24.181035 27950 net.cpp:226] conv2 needs backward computation.
I0520 13:44:24.181048 27950 net.cpp:226] pool1 needs backward computation.
I0520 13:44:24.181064 27950 net.cpp:226] relu1 needs backward computation.
I0520 13:44:24.181084 27950 net.cpp:226] conv1 needs backward computation.
I0520 13:44:24.181099 27950 net.cpp:228] data_hdf5 does not need backward computation.
I0520 13:44:24.181115 27950 net.cpp:270] This network produces output loss
I0520 13:44:24.181143 27950 net.cpp:283] Network initialization done.
I0520 13:44:24.182772 27950 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143.prototxt
I0520 13:44:24.182852 27950 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 13:44:24.183228 27950 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 90
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 13:44:24.183450 27950 layer_factory.hpp:77] Creating layer data_hdf5
I0520 13:44:24.183470 27950 net.cpp:106] Creating Layer data_hdf5
I0520 13:44:24.183485 27950 net.cpp:411] data_hdf5 -> data
I0520 13:44:24.183506 27950 net.cpp:411] data_hdf5 -> label
I0520 13:44:24.183526 27950 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 13:44:24.204542 27950 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 13:44:45.525076 27950 net.cpp:150] Setting up data_hdf5
I0520 13:44:45.525260 27950 net.cpp:157] Top shape: 90 1 127 50 (571500)
I0520 13:44:45.525280 27950 net.cpp:157] Top shape: 90 (90)
I0520 13:44:45.525293 27950 net.cpp:165] Memory required for data: 2286360
I0520 13:44:45.525308 27950 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 13:44:45.525341 27950 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 13:44:45.525354 27950 net.cpp:454] label_data_hdf5_1_split <- label
I0520 13:44:45.525372 27950 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 13:44:45.525413 27950 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 13:44:45.525496 27950 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 13:44:45.525521 27950 net.cpp:157] Top shape: 90 (90)
I0520 13:44:45.525537 27950 net.cpp:157] Top shape: 90 (90)
I0520 13:44:45.525558 27950 net.cpp:165] Memory required for data: 2287080
I0520 13:44:45.525571 27950 layer_factory.hpp:77] Creating layer conv1
I0520 13:44:45.525599 27950 net.cpp:106] Creating Layer conv1
I0520 13:44:45.525617 27950 net.cpp:454] conv1 <- data
I0520 13:44:45.525635 27950 net.cpp:411] conv1 -> conv1
I0520 13:44:45.527608 27950 net.cpp:150] Setting up conv1
I0520 13:44:45.527634 27950 net.cpp:157] Top shape: 90 12 120 48 (6220800)
I0520 13:44:45.527654 27950 net.cpp:165] Memory required for data: 27170280
I0520 13:44:45.527678 27950 layer_factory.hpp:77] Creating layer relu1
I0520 13:44:45.527699 27950 net.cpp:106] Creating Layer relu1
I0520 13:44:45.527721 27950 net.cpp:454] relu1 <- conv1
I0520 13:44:45.527737 27950 net.cpp:397] relu1 -> conv1 (in-place)
I0520 13:44:45.528249 27950 net.cpp:150] Setting up relu1
I0520 13:44:45.528271 27950 net.cpp:157] Top shape: 90 12 120 48 (6220800)
I0520 13:44:45.528285 27950 net.cpp:165] Memory required for data: 52053480
I0520 13:44:45.528298 27950 layer_factory.hpp:77] Creating layer pool1
I0520 13:44:45.528328 27950 net.cpp:106] Creating Layer pool1
I0520 13:44:45.528342 27950 net.cpp:454] pool1 <- conv1
I0520 13:44:45.528357 27950 net.cpp:411] pool1 -> pool1
I0520 13:44:45.528446 27950 net.cpp:150] Setting up pool1
I0520 13:44:45.528465 27950 net.cpp:157] Top shape: 90 12 60 48 (3110400)
I0520 13:44:45.528478 27950 net.cpp:165] Memory required for data: 64495080
I0520 13:44:45.528497 27950 layer_factory.hpp:77] Creating layer conv2
I0520 13:44:45.528517 27950 net.cpp:106] Creating Layer conv2
I0520 13:44:45.528530 27950 net.cpp:454] conv2 <- pool1
I0520 13:44:45.528548 27950 net.cpp:411] conv2 -> conv2
I0520 13:44:45.530506 27950 net.cpp:150] Setting up conv2
I0520 13:44:45.530530 27950 net.cpp:157] Top shape: 90 20 54 46 (4471200)
I0520 13:44:45.530551 27950 net.cpp:165] Memory required for data: 82379880
I0520 13:44:45.530573 27950 layer_factory.hpp:77] Creating layer relu2
I0520 13:44:45.530592 27950 net.cpp:106] Creating Layer relu2
I0520 13:44:45.530614 27950 net.cpp:454] relu2 <- conv2
I0520 13:44:45.530630 27950 net.cpp:397] relu2 -> conv2 (in-place)
I0520 13:44:45.530979 27950 net.cpp:150] Setting up relu2
I0520 13:44:45.530999 27950 net.cpp:157] Top shape: 90 20 54 46 (4471200)
I0520 13:44:45.531011 27950 net.cpp:165] Memory required for data: 100264680
I0520 13:44:45.531026 27950 layer_factory.hpp:77] Creating layer pool2
I0520 13:44:45.531044 27950 net.cpp:106] Creating Layer pool2
I0520 13:44:45.531064 27950 net.cpp:454] pool2 <- conv2
I0520 13:44:45.531080 27950 net.cpp:411] pool2 -> pool2
I0520 13:44:45.531168 27950 net.cpp:150] Setting up pool2
I0520 13:44:45.531185 27950 net.cpp:157] Top shape: 90 20 27 46 (2235600)
I0520 13:44:45.531199 27950 net.cpp:165] Memory required for data: 109207080
I0520 13:44:45.531219 27950 layer_factory.hpp:77] Creating layer conv3
I0520 13:44:45.531241 27950 net.cpp:106] Creating Layer conv3
I0520 13:44:45.531255 27950 net.cpp:454] conv3 <- pool2
I0520 13:44:45.531270 27950 net.cpp:411] conv3 -> conv3
I0520 13:44:45.533300 27950 net.cpp:150] Setting up conv3
I0520 13:44:45.533325 27950 net.cpp:157] Top shape: 90 28 22 44 (2439360)
I0520 13:44:45.533345 27950 net.cpp:165] Memory required for data: 118964520
I0520 13:44:45.533385 27950 layer_factory.hpp:77] Creating layer relu3
I0520 13:44:45.533409 27950 net.cpp:106] Creating Layer relu3
I0520 13:44:45.533423 27950 net.cpp:454] relu3 <- conv3
I0520 13:44:45.533439 27950 net.cpp:397] relu3 -> conv3 (in-place)
I0520 13:44:45.533944 27950 net.cpp:150] Setting up relu3
I0520 13:44:45.533967 27950 net.cpp:157] Top shape: 90 28 22 44 (2439360)
I0520 13:44:45.533982 27950 net.cpp:165] Memory required for data: 128721960
I0520 13:44:45.533996 27950 layer_factory.hpp:77] Creating layer pool3
I0520 13:44:45.534020 27950 net.cpp:106] Creating Layer pool3
I0520 13:44:45.534034 27950 net.cpp:454] pool3 <- conv3
I0520 13:44:45.534050 27950 net.cpp:411] pool3 -> pool3
I0520 13:44:45.534137 27950 net.cpp:150] Setting up pool3
I0520 13:44:45.534153 27950 net.cpp:157] Top shape: 90 28 11 44 (1219680)
I0520 13:44:45.534168 27950 net.cpp:165] Memory required for data: 133600680
I0520 13:44:45.534180 27950 layer_factory.hpp:77] Creating layer conv4
I0520 13:44:45.534209 27950 net.cpp:106] Creating Layer conv4
I0520 13:44:45.534221 27950 net.cpp:454] conv4 <- pool3
I0520 13:44:45.534238 27950 net.cpp:411] conv4 -> conv4
I0520 13:44:45.536348 27950 net.cpp:150] Setting up conv4
I0520 13:44:45.536372 27950 net.cpp:157] Top shape: 90 36 6 42 (816480)
I0520 13:44:45.536393 27950 net.cpp:165] Memory required for data: 136866600
I0520 13:44:45.536412 27950 layer_factory.hpp:77] Creating layer relu4
I0520 13:44:45.536432 27950 net.cpp:106] Creating Layer relu4
I0520 13:44:45.536444 27950 net.cpp:454] relu4 <- conv4
I0520 13:44:45.536469 27950 net.cpp:397] relu4 -> conv4 (in-place)
I0520 13:44:45.536964 27950 net.cpp:150] Setting up relu4
I0520 13:44:45.536988 27950 net.cpp:157] Top shape: 90 36 6 42 (816480)
I0520 13:44:45.537000 27950 net.cpp:165] Memory required for data: 140132520
I0520 13:44:45.537016 27950 layer_factory.hpp:77] Creating layer pool4
I0520 13:44:45.537032 27950 net.cpp:106] Creating Layer pool4
I0520 13:44:45.537053 27950 net.cpp:454] pool4 <- conv4
I0520 13:44:45.537070 27950 net.cpp:411] pool4 -> pool4
I0520 13:44:45.537156 27950 net.cpp:150] Setting up pool4
I0520 13:44:45.537173 27950 net.cpp:157] Top shape: 90 36 3 42 (408240)
I0520 13:44:45.537189 27950 net.cpp:165] Memory required for data: 141765480
I0520 13:44:45.537207 27950 layer_factory.hpp:77] Creating layer ip1
I0520 13:44:45.537233 27950 net.cpp:106] Creating Layer ip1
I0520 13:44:45.537256 27950 net.cpp:454] ip1 <- pool4
I0520 13:44:45.537273 27950 net.cpp:411] ip1 -> ip1
I0520 13:44:45.552731 27950 net.cpp:150] Setting up ip1
I0520 13:44:45.552762 27950 net.cpp:157] Top shape: 90 196 (17640)
I0520 13:44:45.552784 27950 net.cpp:165] Memory required for data: 141836040
I0520 13:44:45.552810 27950 layer_factory.hpp:77] Creating layer relu5
I0520 13:44:45.552832 27950 net.cpp:106] Creating Layer relu5
I0520 13:44:45.552857 27950 net.cpp:454] relu5 <- ip1
I0520 13:44:45.552875 27950 net.cpp:397] relu5 -> ip1 (in-place)
I0520 13:44:45.553247 27950 net.cpp:150] Setting up relu5
I0520 13:44:45.553267 27950 net.cpp:157] Top shape: 90 196 (17640)
I0520 13:44:45.553280 27950 net.cpp:165] Memory required for data: 141906600
I0520 13:44:45.553292 27950 layer_factory.hpp:77] Creating layer drop1
I0520 13:44:45.553323 27950 net.cpp:106] Creating Layer drop1
I0520 13:44:45.553339 27950 net.cpp:454] drop1 <- ip1
I0520 13:44:45.553355 27950 net.cpp:397] drop1 -> ip1 (in-place)
I0520 13:44:45.553416 27950 net.cpp:150] Setting up drop1
I0520 13:44:45.553432 27950 net.cpp:157] Top shape: 90 196 (17640)
I0520 13:44:45.553444 27950 net.cpp:165] Memory required for data: 141977160
I0520 13:44:45.553458 27950 layer_factory.hpp:77] Creating layer ip2
I0520 13:44:45.553478 27950 net.cpp:106] Creating Layer ip2
I0520 13:44:45.553493 27950 net.cpp:454] ip2 <- ip1
I0520 13:44:45.553516 27950 net.cpp:411] ip2 -> ip2
I0520 13:44:45.554009 27950 net.cpp:150] Setting up ip2
I0520 13:44:45.554029 27950 net.cpp:157] Top shape: 90 98 (8820)
I0520 13:44:45.554041 27950 net.cpp:165] Memory required for data: 142012440
I0520 13:44:45.554064 27950 layer_factory.hpp:77] Creating layer relu6
I0520 13:44:45.554100 27950 net.cpp:106] Creating Layer relu6
I0520 13:44:45.554113 27950 net.cpp:454] relu6 <- ip2
I0520 13:44:45.554128 27950 net.cpp:397] relu6 -> ip2 (in-place)
I0520 13:44:45.554697 27950 net.cpp:150] Setting up relu6
I0520 13:44:45.554721 27950 net.cpp:157] Top shape: 90 98 (8820)
I0520 13:44:45.554735 27950 net.cpp:165] Memory required for data: 142047720
I0520 13:44:45.554749 27950 layer_factory.hpp:77] Creating layer drop2
I0520 13:44:45.554767 27950 net.cpp:106] Creating Layer drop2
I0520 13:44:45.554788 27950 net.cpp:454] drop2 <- ip2
I0520 13:44:45.554806 27950 net.cpp:397] drop2 -> ip2 (in-place)
I0520 13:44:45.554864 27950 net.cpp:150] Setting up drop2
I0520 13:44:45.554880 27950 net.cpp:157] Top shape: 90 98 (8820)
I0520 13:44:45.554893 27950 net.cpp:165] Memory required for data: 142083000
I0520 13:44:45.554913 27950 layer_factory.hpp:77] Creating layer ip3
I0520 13:44:45.554932 27950 net.cpp:106] Creating Layer ip3
I0520 13:44:45.554944 27950 net.cpp:454] ip3 <- ip2
I0520 13:44:45.554963 27950 net.cpp:411] ip3 -> ip3
I0520 13:44:45.555207 27950 net.cpp:150] Setting up ip3
I0520 13:44:45.555225 27950 net.cpp:157] Top shape: 90 11 (990)
I0520 13:44:45.555238 27950 net.cpp:165] Memory required for data: 142086960
I0520 13:44:45.555259 27950 layer_factory.hpp:77] Creating layer drop3
I0520 13:44:45.555282 27950 net.cpp:106] Creating Layer drop3
I0520 13:44:45.555295 27950 net.cpp:454] drop3 <- ip3
I0520 13:44:45.555310 27950 net.cpp:397] drop3 -> ip3 (in-place)
I0520 13:44:45.555358 27950 net.cpp:150] Setting up drop3
I0520 13:44:45.555382 27950 net.cpp:157] Top shape: 90 11 (990)
I0520 13:44:45.555393 27950 net.cpp:165] Memory required for data: 142090920
I0520 13:44:45.555411 27950 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 13:44:45.555428 27950 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 13:44:45.555440 27950 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 13:44:45.555459 27950 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 13:44:45.555485 27950 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 13:44:45.555564 27950 net.cpp:150] Setting up ip3_drop3_0_split
I0520 13:44:45.555588 27950 net.cpp:157] Top shape: 90 11 (990)
I0520 13:44:45.555604 27950 net.cpp:157] Top shape: 90 11 (990)
I0520 13:44:45.555616 27950 net.cpp:165] Memory required for data: 142098840
I0520 13:44:45.555631 27950 layer_factory.hpp:77] Creating layer accuracy
I0520 13:44:45.555654 27950 net.cpp:106] Creating Layer accuracy
I0520 13:44:45.555666 27950 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 13:44:45.555691 27950 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 13:44:45.555707 27950 net.cpp:411] accuracy -> accuracy
I0520 13:44:45.555735 27950 net.cpp:150] Setting up accuracy
I0520 13:44:45.555750 27950 net.cpp:157] Top shape: (1)
I0520 13:44:45.555771 27950 net.cpp:165] Memory required for data: 142098844
I0520 13:44:45.555786 27950 layer_factory.hpp:77] Creating layer loss
I0520 13:44:45.555804 27950 net.cpp:106] Creating Layer loss
I0520 13:44:45.555815 27950 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 13:44:45.555831 27950 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 13:44:45.555847 27950 net.cpp:411] loss -> loss
I0520 13:44:45.555874 27950 layer_factory.hpp:77] Creating layer loss
I0520 13:44:45.556393 27950 net.cpp:150] Setting up loss
I0520 13:44:45.556413 27950 net.cpp:157] Top shape: (1)
I0520 13:44:45.556427 27950 net.cpp:160]     with loss weight 1
I0520 13:44:45.556452 27950 net.cpp:165] Memory required for data: 142098848
I0520 13:44:45.556471 27950 net.cpp:226] loss needs backward computation.
I0520 13:44:45.556486 27950 net.cpp:228] accuracy does not need backward computation.
I0520 13:44:45.556504 27950 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 13:44:45.556517 27950 net.cpp:226] drop3 needs backward computation.
I0520 13:44:45.556529 27950 net.cpp:226] ip3 needs backward computation.
I0520 13:44:45.556545 27950 net.cpp:226] drop2 needs backward computation.
I0520 13:44:45.556556 27950 net.cpp:226] relu6 needs backward computation.
I0520 13:44:45.556586 27950 net.cpp:226] ip2 needs backward computation.
I0520 13:44:45.556602 27950 net.cpp:226] drop1 needs backward computation.
I0520 13:44:45.556615 27950 net.cpp:226] relu5 needs backward computation.
I0520 13:44:45.556627 27950 net.cpp:226] ip1 needs backward computation.
I0520 13:44:45.556641 27950 net.cpp:226] pool4 needs backward computation.
I0520 13:44:45.556654 27950 net.cpp:226] relu4 needs backward computation.
I0520 13:44:45.556674 27950 net.cpp:226] conv4 needs backward computation.
I0520 13:44:45.556686 27950 net.cpp:226] pool3 needs backward computation.
I0520 13:44:45.556702 27950 net.cpp:226] relu3 needs backward computation.
I0520 13:44:45.556715 27950 net.cpp:226] conv3 needs backward computation.
I0520 13:44:45.556727 27950 net.cpp:226] pool2 needs backward computation.
I0520 13:44:45.556743 27950 net.cpp:226] relu2 needs backward computation.
I0520 13:44:45.556756 27950 net.cpp:226] conv2 needs backward computation.
I0520 13:44:45.556776 27950 net.cpp:226] pool1 needs backward computation.
I0520 13:44:45.556788 27950 net.cpp:226] relu1 needs backward computation.
I0520 13:44:45.556804 27950 net.cpp:226] conv1 needs backward computation.
I0520 13:44:45.556818 27950 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 13:44:45.556833 27950 net.cpp:228] data_hdf5 does not need backward computation.
I0520 13:44:45.556843 27950 net.cpp:270] This network produces output accuracy
I0520 13:44:45.556859 27950 net.cpp:270] This network produces output loss
I0520 13:44:45.556890 27950 net.cpp:283] Network initialization done.
I0520 13:44:45.557024 27950 solver.cpp:60] Solver scaffolding done.
I0520 13:44:45.558177 27950 caffe.cpp:212] Starting Optimization
I0520 13:44:45.558195 27950 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 13:44:45.558212 27950 solver.cpp:289] Learning Rate Policy: fixed
I0520 13:44:45.559294 27950 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 13:45:33.624013 27950 solver.cpp:409]     Test net output #0: accuracy = 0.121542
I0520 13:45:33.624179 27950 solver.cpp:409]     Test net output #1: loss = 2.39733 (* 1 = 2.39733 loss)
I0520 13:45:33.655351 27950 solver.cpp:237] Iteration 0, loss = 2.3936
I0520 13:45:33.655397 27950 solver.cpp:253]     Train net output #0: loss = 2.3936 (* 1 = 2.3936 loss)
I0520 13:45:33.655418 27950 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 13:45:42.484282 27950 solver.cpp:237] Iteration 166, loss = 2.33024
I0520 13:45:42.484344 27950 solver.cpp:253]     Train net output #0: loss = 2.33024 (* 1 = 2.33024 loss)
I0520 13:45:42.484375 27950 sgd_solver.cpp:106] Iteration 166, lr = 0.0025
I0520 13:45:51.323921 27950 solver.cpp:237] Iteration 332, loss = 2.11543
I0520 13:45:51.323958 27950 solver.cpp:253]     Train net output #0: loss = 2.11543 (* 1 = 2.11543 loss)
I0520 13:45:51.323982 27950 sgd_solver.cpp:106] Iteration 332, lr = 0.0025
I0520 13:46:00.168392 27950 solver.cpp:237] Iteration 498, loss = 2.05484
I0520 13:46:00.168428 27950 solver.cpp:253]     Train net output #0: loss = 2.05484 (* 1 = 2.05484 loss)
I0520 13:46:00.168452 27950 sgd_solver.cpp:106] Iteration 498, lr = 0.0025
I0520 13:46:09.004834 27950 solver.cpp:237] Iteration 664, loss = 1.97515
I0520 13:46:09.005005 27950 solver.cpp:253]     Train net output #0: loss = 1.97515 (* 1 = 1.97515 loss)
I0520 13:46:09.005025 27950 sgd_solver.cpp:106] Iteration 664, lr = 0.0025
I0520 13:46:17.840736 27950 solver.cpp:237] Iteration 830, loss = 1.88032
I0520 13:46:17.840773 27950 solver.cpp:253]     Train net output #0: loss = 1.88032 (* 1 = 1.88032 loss)
I0520 13:46:17.840797 27950 sgd_solver.cpp:106] Iteration 830, lr = 0.0025
I0520 13:46:26.671757 27950 solver.cpp:237] Iteration 996, loss = 1.96355
I0520 13:46:26.671793 27950 solver.cpp:253]     Train net output #0: loss = 1.96355 (* 1 = 1.96355 loss)
I0520 13:46:26.671818 27950 sgd_solver.cpp:106] Iteration 996, lr = 0.0025
I0520 13:46:57.599874 27950 solver.cpp:237] Iteration 1162, loss = 1.68323
I0520 13:46:57.600044 27950 solver.cpp:253]     Train net output #0: loss = 1.68323 (* 1 = 1.68323 loss)
I0520 13:46:57.600060 27950 sgd_solver.cpp:106] Iteration 1162, lr = 0.0025
I0520 13:47:06.427403 27950 solver.cpp:237] Iteration 1328, loss = 1.98108
I0520 13:47:06.427445 27950 solver.cpp:253]     Train net output #0: loss = 1.98108 (* 1 = 1.98108 loss)
I0520 13:47:06.427462 27950 sgd_solver.cpp:106] Iteration 1328, lr = 0.0025
I0520 13:47:15.259224 27950 solver.cpp:237] Iteration 1494, loss = 1.87642
I0520 13:47:15.259261 27950 solver.cpp:253]     Train net output #0: loss = 1.87642 (* 1 = 1.87642 loss)
I0520 13:47:15.259285 27950 sgd_solver.cpp:106] Iteration 1494, lr = 0.0025
I0520 13:47:24.099640 27950 solver.cpp:237] Iteration 1660, loss = 1.73257
I0520 13:47:24.099694 27950 solver.cpp:253]     Train net output #0: loss = 1.73257 (* 1 = 1.73257 loss)
I0520 13:47:24.099711 27950 sgd_solver.cpp:106] Iteration 1660, lr = 0.0025
I0520 13:47:24.366385 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_1666.caffemodel
I0520 13:47:24.444918 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_1666.solverstate
I0520 13:47:32.999806 27950 solver.cpp:237] Iteration 1826, loss = 1.72384
I0520 13:47:32.999972 27950 solver.cpp:253]     Train net output #0: loss = 1.72384 (* 1 = 1.72384 loss)
I0520 13:47:32.999990 27950 sgd_solver.cpp:106] Iteration 1826, lr = 0.0025
I0520 13:47:41.834336 27950 solver.cpp:237] Iteration 1992, loss = 1.67577
I0520 13:47:41.834372 27950 solver.cpp:253]     Train net output #0: loss = 1.67577 (* 1 = 1.67577 loss)
I0520 13:47:41.834396 27950 sgd_solver.cpp:106] Iteration 1992, lr = 0.0025
I0520 13:47:50.670533 27950 solver.cpp:237] Iteration 2158, loss = 1.68144
I0520 13:47:50.670588 27950 solver.cpp:253]     Train net output #0: loss = 1.68144 (* 1 = 1.68144 loss)
I0520 13:47:50.670604 27950 sgd_solver.cpp:106] Iteration 2158, lr = 0.0025
I0520 13:48:21.644214 27950 solver.cpp:237] Iteration 2324, loss = 1.64088
I0520 13:48:21.644377 27950 solver.cpp:253]     Train net output #0: loss = 1.64088 (* 1 = 1.64088 loss)
I0520 13:48:21.644395 27950 sgd_solver.cpp:106] Iteration 2324, lr = 0.0025
I0520 13:48:30.480393 27950 solver.cpp:237] Iteration 2490, loss = 1.87904
I0520 13:48:30.480432 27950 solver.cpp:253]     Train net output #0: loss = 1.87904 (* 1 = 1.87904 loss)
I0520 13:48:30.480450 27950 sgd_solver.cpp:106] Iteration 2490, lr = 0.0025
I0520 13:48:39.310297 27950 solver.cpp:237] Iteration 2656, loss = 1.68821
I0520 13:48:39.310350 27950 solver.cpp:253]     Train net output #0: loss = 1.68821 (* 1 = 1.68821 loss)
I0520 13:48:39.310367 27950 sgd_solver.cpp:106] Iteration 2656, lr = 0.0025
I0520 13:48:48.147155 27950 solver.cpp:237] Iteration 2822, loss = 1.60726
I0520 13:48:48.147192 27950 solver.cpp:253]     Train net output #0: loss = 1.60726 (* 1 = 1.60726 loss)
I0520 13:48:48.147209 27950 sgd_solver.cpp:106] Iteration 2822, lr = 0.0025
I0520 13:48:56.980305 27950 solver.cpp:237] Iteration 2988, loss = 1.64435
I0520 13:48:56.980459 27950 solver.cpp:253]     Train net output #0: loss = 1.64435 (* 1 = 1.64435 loss)
I0520 13:48:56.980475 27950 sgd_solver.cpp:106] Iteration 2988, lr = 0.0025
I0520 13:49:05.811117 27950 solver.cpp:237] Iteration 3154, loss = 1.55382
I0520 13:49:05.811175 27950 solver.cpp:253]     Train net output #0: loss = 1.55382 (* 1 = 1.55382 loss)
I0520 13:49:05.811200 27950 sgd_solver.cpp:106] Iteration 3154, lr = 0.0025
I0520 13:49:14.643524 27950 solver.cpp:237] Iteration 3320, loss = 1.50398
I0520 13:49:14.643560 27950 solver.cpp:253]     Train net output #0: loss = 1.50398 (* 1 = 1.50398 loss)
I0520 13:49:14.643584 27950 sgd_solver.cpp:106] Iteration 3320, lr = 0.0025
I0520 13:49:15.228246 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_3332.caffemodel
I0520 13:49:15.302891 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_3332.solverstate
I0520 13:49:15.345059 27950 solver.cpp:341] Iteration 3333, Testing net (#0)
I0520 13:50:02.465138 27950 solver.cpp:409]     Test net output #0: accuracy = 0.704022
I0520 13:50:02.465318 27950 solver.cpp:409]     Test net output #1: loss = 1.00867 (* 1 = 1.00867 loss)
I0520 13:50:32.791877 27950 solver.cpp:237] Iteration 3486, loss = 1.50686
I0520 13:50:32.792042 27950 solver.cpp:253]     Train net output #0: loss = 1.50686 (* 1 = 1.50686 loss)
I0520 13:50:32.792060 27950 sgd_solver.cpp:106] Iteration 3486, lr = 0.0025
I0520 13:50:41.621093 27950 solver.cpp:237] Iteration 3652, loss = 1.74753
I0520 13:50:41.621131 27950 solver.cpp:253]     Train net output #0: loss = 1.74753 (* 1 = 1.74753 loss)
I0520 13:50:41.621150 27950 sgd_solver.cpp:106] Iteration 3652, lr = 0.0025
I0520 13:50:50.452802 27950 solver.cpp:237] Iteration 3818, loss = 1.56196
I0520 13:50:50.452853 27950 solver.cpp:253]     Train net output #0: loss = 1.56196 (* 1 = 1.56196 loss)
I0520 13:50:50.452872 27950 sgd_solver.cpp:106] Iteration 3818, lr = 0.0025
I0520 13:50:59.275749 27950 solver.cpp:237] Iteration 3984, loss = 1.64589
I0520 13:50:59.275786 27950 solver.cpp:253]     Train net output #0: loss = 1.64589 (* 1 = 1.64589 loss)
I0520 13:50:59.275805 27950 sgd_solver.cpp:106] Iteration 3984, lr = 0.0025
I0520 13:51:08.111546 27950 solver.cpp:237] Iteration 4150, loss = 1.52463
I0520 13:51:08.111708 27950 solver.cpp:253]     Train net output #0: loss = 1.52463 (* 1 = 1.52463 loss)
I0520 13:51:08.111727 27950 sgd_solver.cpp:106] Iteration 4150, lr = 0.0025
I0520 13:51:16.942036 27950 solver.cpp:237] Iteration 4316, loss = 1.44894
I0520 13:51:16.942073 27950 solver.cpp:253]     Train net output #0: loss = 1.44894 (* 1 = 1.44894 loss)
I0520 13:51:16.942096 27950 sgd_solver.cpp:106] Iteration 4316, lr = 0.0025
I0520 13:51:47.971045 27950 solver.cpp:237] Iteration 4482, loss = 1.46281
I0520 13:51:47.971217 27950 solver.cpp:253]     Train net output #0: loss = 1.46281 (* 1 = 1.46281 loss)
I0520 13:51:47.971235 27950 sgd_solver.cpp:106] Iteration 4482, lr = 0.0025
I0520 13:51:56.802438 27950 solver.cpp:237] Iteration 4648, loss = 1.54302
I0520 13:51:56.802474 27950 solver.cpp:253]     Train net output #0: loss = 1.54302 (* 1 = 1.54302 loss)
I0520 13:51:56.802494 27950 sgd_solver.cpp:106] Iteration 4648, lr = 0.0025
I0520 13:52:05.637948 27950 solver.cpp:237] Iteration 4814, loss = 1.47983
I0520 13:52:05.638007 27950 solver.cpp:253]     Train net output #0: loss = 1.47983 (* 1 = 1.47983 loss)
I0520 13:52:05.638031 27950 sgd_solver.cpp:106] Iteration 4814, lr = 0.0025
I0520 13:52:14.462039 27950 solver.cpp:237] Iteration 4980, loss = 1.47497
I0520 13:52:14.462076 27950 solver.cpp:253]     Train net output #0: loss = 1.47497 (* 1 = 1.47497 loss)
I0520 13:52:14.462095 27950 sgd_solver.cpp:106] Iteration 4980, lr = 0.0025
I0520 13:52:15.370158 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_4998.caffemodel
I0520 13:52:15.450155 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_4998.solverstate
I0520 13:52:23.362290 27950 solver.cpp:237] Iteration 5146, loss = 1.46553
I0520 13:52:23.362470 27950 solver.cpp:253]     Train net output #0: loss = 1.46553 (* 1 = 1.46553 loss)
I0520 13:52:23.362489 27950 sgd_solver.cpp:106] Iteration 5146, lr = 0.0025
I0520 13:52:32.191119 27950 solver.cpp:237] Iteration 5312, loss = 1.33969
I0520 13:52:32.191157 27950 solver.cpp:253]     Train net output #0: loss = 1.33969 (* 1 = 1.33969 loss)
I0520 13:52:32.191176 27950 sgd_solver.cpp:106] Iteration 5312, lr = 0.0025
I0520 13:52:41.013764 27950 solver.cpp:237] Iteration 5478, loss = 1.37964
I0520 13:52:41.013802 27950 solver.cpp:253]     Train net output #0: loss = 1.37964 (* 1 = 1.37964 loss)
I0520 13:52:41.013819 27950 sgd_solver.cpp:106] Iteration 5478, lr = 0.0025
I0520 13:53:12.062366 27950 solver.cpp:237] Iteration 5644, loss = 1.28087
I0520 13:53:12.062536 27950 solver.cpp:253]     Train net output #0: loss = 1.28087 (* 1 = 1.28087 loss)
I0520 13:53:12.062554 27950 sgd_solver.cpp:106] Iteration 5644, lr = 0.0025
I0520 13:53:20.886766 27950 solver.cpp:237] Iteration 5810, loss = 1.38073
I0520 13:53:20.886806 27950 solver.cpp:253]     Train net output #0: loss = 1.38073 (* 1 = 1.38073 loss)
I0520 13:53:20.886823 27950 sgd_solver.cpp:106] Iteration 5810, lr = 0.0025
I0520 13:53:29.710587 27950 solver.cpp:237] Iteration 5976, loss = 1.3224
I0520 13:53:29.710623 27950 solver.cpp:253]     Train net output #0: loss = 1.3224 (* 1 = 1.3224 loss)
I0520 13:53:29.710642 27950 sgd_solver.cpp:106] Iteration 5976, lr = 0.0025
I0520 13:53:38.535419 27950 solver.cpp:237] Iteration 6142, loss = 1.21527
I0520 13:53:38.535473 27950 solver.cpp:253]     Train net output #0: loss = 1.21527 (* 1 = 1.21527 loss)
I0520 13:53:38.535501 27950 sgd_solver.cpp:106] Iteration 6142, lr = 0.0025
I0520 13:53:47.363196 27950 solver.cpp:237] Iteration 6308, loss = 1.41004
I0520 13:53:47.363339 27950 solver.cpp:253]     Train net output #0: loss = 1.41004 (* 1 = 1.41004 loss)
I0520 13:53:47.363356 27950 sgd_solver.cpp:106] Iteration 6308, lr = 0.0025
I0520 13:53:56.182360 27950 solver.cpp:237] Iteration 6474, loss = 1.48042
I0520 13:53:56.182396 27950 solver.cpp:253]     Train net output #0: loss = 1.48042 (* 1 = 1.48042 loss)
I0520 13:53:56.182415 27950 sgd_solver.cpp:106] Iteration 6474, lr = 0.0025
I0520 13:54:05.007897 27950 solver.cpp:237] Iteration 6640, loss = 1.21936
I0520 13:54:05.007956 27950 solver.cpp:253]     Train net output #0: loss = 1.21936 (* 1 = 1.21936 loss)
I0520 13:54:05.007983 27950 sgd_solver.cpp:106] Iteration 6640, lr = 0.0025
I0520 13:54:06.229183 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_6664.caffemodel
I0520 13:54:06.305595 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_6664.solverstate
I0520 13:54:06.402628 27950 solver.cpp:341] Iteration 6666, Testing net (#0)
I0520 13:55:14.316419 27950 solver.cpp:409]     Test net output #0: accuracy = 0.795401
I0520 13:55:14.316598 27950 solver.cpp:409]     Test net output #1: loss = 0.770872 (* 1 = 0.770872 loss)
I0520 13:55:43.976124 27950 solver.cpp:237] Iteration 6806, loss = 1.62059
I0520 13:55:43.976183 27950 solver.cpp:253]     Train net output #0: loss = 1.62059 (* 1 = 1.62059 loss)
I0520 13:55:43.976209 27950 sgd_solver.cpp:106] Iteration 6806, lr = 0.0025
I0520 13:55:52.817322 27950 solver.cpp:237] Iteration 6972, loss = 1.34084
I0520 13:55:52.817484 27950 solver.cpp:253]     Train net output #0: loss = 1.34084 (* 1 = 1.34084 loss)
I0520 13:55:52.817502 27950 sgd_solver.cpp:106] Iteration 6972, lr = 0.0025
I0520 13:56:01.655582 27950 solver.cpp:237] Iteration 7138, loss = 1.56672
I0520 13:56:01.655617 27950 solver.cpp:253]     Train net output #0: loss = 1.56672 (* 1 = 1.56672 loss)
I0520 13:56:01.655637 27950 sgd_solver.cpp:106] Iteration 7138, lr = 0.0025
I0520 13:56:10.485644 27950 solver.cpp:237] Iteration 7304, loss = 1.22441
I0520 13:56:10.485699 27950 solver.cpp:253]     Train net output #0: loss = 1.22441 (* 1 = 1.22441 loss)
I0520 13:56:10.485716 27950 sgd_solver.cpp:106] Iteration 7304, lr = 0.0025
I0520 13:56:19.327646 27950 solver.cpp:237] Iteration 7470, loss = 1.44723
I0520 13:56:19.327682 27950 solver.cpp:253]     Train net output #0: loss = 1.44723 (* 1 = 1.44723 loss)
I0520 13:56:19.327700 27950 sgd_solver.cpp:106] Iteration 7470, lr = 0.0025
I0520 13:56:28.162147 27950 solver.cpp:237] Iteration 7636, loss = 1.23734
I0520 13:56:28.162292 27950 solver.cpp:253]     Train net output #0: loss = 1.23734 (* 1 = 1.23734 loss)
I0520 13:56:28.162309 27950 sgd_solver.cpp:106] Iteration 7636, lr = 0.0025
I0520 13:56:59.196646 27950 solver.cpp:237] Iteration 7802, loss = 1.36641
I0520 13:56:59.196815 27950 solver.cpp:253]     Train net output #0: loss = 1.36641 (* 1 = 1.36641 loss)
I0520 13:56:59.196833 27950 sgd_solver.cpp:106] Iteration 7802, lr = 0.0025
I0520 13:57:08.032997 27950 solver.cpp:237] Iteration 7968, loss = 1.69474
I0520 13:57:08.033035 27950 solver.cpp:253]     Train net output #0: loss = 1.69474 (* 1 = 1.69474 loss)
I0520 13:57:08.033059 27950 sgd_solver.cpp:106] Iteration 7968, lr = 0.0025
I0520 13:57:16.867862 27950 solver.cpp:237] Iteration 8134, loss = 1.44183
I0520 13:57:16.867898 27950 solver.cpp:253]     Train net output #0: loss = 1.44183 (* 1 = 1.44183 loss)
I0520 13:57:16.867918 27950 sgd_solver.cpp:106] Iteration 8134, lr = 0.0025
I0520 13:57:25.695973 27950 solver.cpp:237] Iteration 8300, loss = 1.45192
I0520 13:57:25.696032 27950 solver.cpp:253]     Train net output #0: loss = 1.45192 (* 1 = 1.45192 loss)
I0520 13:57:25.696058 27950 sgd_solver.cpp:106] Iteration 8300, lr = 0.0025
I0520 13:57:27.238373 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_8330.caffemodel
I0520 13:57:27.316045 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_8330.solverstate
I0520 13:57:34.602252 27950 solver.cpp:237] Iteration 8466, loss = 1.48847
I0520 13:57:34.602421 27950 solver.cpp:253]     Train net output #0: loss = 1.48847 (* 1 = 1.48847 loss)
I0520 13:57:34.602440 27950 sgd_solver.cpp:106] Iteration 8466, lr = 0.0025
I0520 13:57:43.432484 27950 solver.cpp:237] Iteration 8632, loss = 1.37232
I0520 13:57:43.432521 27950 solver.cpp:253]     Train net output #0: loss = 1.37232 (* 1 = 1.37232 loss)
I0520 13:57:43.432539 27950 sgd_solver.cpp:106] Iteration 8632, lr = 0.0025
I0520 13:57:52.265358 27950 solver.cpp:237] Iteration 8798, loss = 1.49228
I0520 13:57:52.265415 27950 solver.cpp:253]     Train net output #0: loss = 1.49228 (* 1 = 1.49228 loss)
I0520 13:57:52.265439 27950 sgd_solver.cpp:106] Iteration 8798, lr = 0.0025
I0520 13:58:23.303628 27950 solver.cpp:237] Iteration 8964, loss = 1.28914
I0520 13:58:23.303812 27950 solver.cpp:253]     Train net output #0: loss = 1.28914 (* 1 = 1.28914 loss)
I0520 13:58:23.303829 27950 sgd_solver.cpp:106] Iteration 8964, lr = 0.0025
I0520 13:58:32.142990 27950 solver.cpp:237] Iteration 9130, loss = 1.52744
I0520 13:58:32.143028 27950 solver.cpp:253]     Train net output #0: loss = 1.52744 (* 1 = 1.52744 loss)
I0520 13:58:32.143046 27950 sgd_solver.cpp:106] Iteration 9130, lr = 0.0025
I0520 13:58:40.979645 27950 solver.cpp:237] Iteration 9296, loss = 1.45726
I0520 13:58:40.979698 27950 solver.cpp:253]     Train net output #0: loss = 1.45726 (* 1 = 1.45726 loss)
I0520 13:58:40.979715 27950 sgd_solver.cpp:106] Iteration 9296, lr = 0.0025
I0520 13:58:49.815979 27950 solver.cpp:237] Iteration 9462, loss = 1.23473
I0520 13:58:49.816018 27950 solver.cpp:253]     Train net output #0: loss = 1.23473 (* 1 = 1.23473 loss)
I0520 13:58:49.816035 27950 sgd_solver.cpp:106] Iteration 9462, lr = 0.0025
I0520 13:58:58.651435 27950 solver.cpp:237] Iteration 9628, loss = 1.16756
I0520 13:58:58.651578 27950 solver.cpp:253]     Train net output #0: loss = 1.16756 (* 1 = 1.16756 loss)
I0520 13:58:58.651594 27950 sgd_solver.cpp:106] Iteration 9628, lr = 0.0025
I0520 13:59:07.477150 27950 solver.cpp:237] Iteration 9794, loss = 1.40399
I0520 13:59:07.477211 27950 solver.cpp:253]     Train net output #0: loss = 1.40399 (* 1 = 1.40399 loss)
I0520 13:59:07.477228 27950 sgd_solver.cpp:106] Iteration 9794, lr = 0.0025
I0520 13:59:16.312813 27950 solver.cpp:237] Iteration 9960, loss = 1.34075
I0520 13:59:16.312851 27950 solver.cpp:253]     Train net output #0: loss = 1.34075 (* 1 = 1.34075 loss)
I0520 13:59:16.312870 27950 sgd_solver.cpp:106] Iteration 9960, lr = 0.0025
I0520 13:59:18.172106 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_9996.caffemodel
I0520 13:59:18.247612 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_9996.solverstate
I0520 13:59:18.395622 27950 solver.cpp:341] Iteration 9999, Testing net (#0)
I0520 14:00:05.230242 27950 solver.cpp:409]     Test net output #0: accuracy = 0.826537
I0520 14:00:05.230414 27950 solver.cpp:409]     Test net output #1: loss = 0.589863 (* 1 = 0.589863 loss)
I0520 14:00:34.162791 27950 solver.cpp:237] Iteration 10126, loss = 1.32561
I0520 14:00:34.162850 27950 solver.cpp:253]     Train net output #0: loss = 1.32561 (* 1 = 1.32561 loss)
I0520 14:00:34.162868 27950 sgd_solver.cpp:106] Iteration 10126, lr = 0.0025
I0520 14:00:42.997565 27950 solver.cpp:237] Iteration 10292, loss = 1.39865
I0520 14:00:42.997737 27950 solver.cpp:253]     Train net output #0: loss = 1.39865 (* 1 = 1.39865 loss)
I0520 14:00:42.997756 27950 sgd_solver.cpp:106] Iteration 10292, lr = 0.0025
I0520 14:00:51.827132 27950 solver.cpp:237] Iteration 10458, loss = 1.33629
I0520 14:00:51.827170 27950 solver.cpp:253]     Train net output #0: loss = 1.33629 (* 1 = 1.33629 loss)
I0520 14:00:51.827188 27950 sgd_solver.cpp:106] Iteration 10458, lr = 0.0025
I0520 14:01:00.655408 27950 solver.cpp:237] Iteration 10624, loss = 1.34565
I0520 14:01:00.655446 27950 solver.cpp:253]     Train net output #0: loss = 1.34565 (* 1 = 1.34565 loss)
I0520 14:01:00.655463 27950 sgd_solver.cpp:106] Iteration 10624, lr = 0.0025
I0520 14:01:09.478139 27950 solver.cpp:237] Iteration 10790, loss = 1.16644
I0520 14:01:09.478195 27950 solver.cpp:253]     Train net output #0: loss = 1.16644 (* 1 = 1.16644 loss)
I0520 14:01:09.478220 27950 sgd_solver.cpp:106] Iteration 10790, lr = 0.0025
I0520 14:01:18.301443 27950 solver.cpp:237] Iteration 10956, loss = 1.36491
I0520 14:01:18.301590 27950 solver.cpp:253]     Train net output #0: loss = 1.36491 (* 1 = 1.36491 loss)
I0520 14:01:18.301607 27950 sgd_solver.cpp:106] Iteration 10956, lr = 0.0025
I0520 14:01:49.327734 27950 solver.cpp:237] Iteration 11122, loss = 1.26186
I0520 14:01:49.327921 27950 solver.cpp:253]     Train net output #0: loss = 1.26186 (* 1 = 1.26186 loss)
I0520 14:01:49.327939 27950 sgd_solver.cpp:106] Iteration 11122, lr = 0.0025
I0520 14:01:58.159955 27950 solver.cpp:237] Iteration 11288, loss = 1.12793
I0520 14:01:58.160010 27950 solver.cpp:253]     Train net output #0: loss = 1.12793 (* 1 = 1.12793 loss)
I0520 14:01:58.160037 27950 sgd_solver.cpp:106] Iteration 11288, lr = 0.0025
I0520 14:02:06.988900 27950 solver.cpp:237] Iteration 11454, loss = 1.23778
I0520 14:02:06.988937 27950 solver.cpp:253]     Train net output #0: loss = 1.23778 (* 1 = 1.23778 loss)
I0520 14:02:06.988956 27950 sgd_solver.cpp:106] Iteration 11454, lr = 0.0025
I0520 14:02:15.812937 27950 solver.cpp:237] Iteration 11620, loss = 1.16106
I0520 14:02:15.812973 27950 solver.cpp:253]     Train net output #0: loss = 1.16106 (* 1 = 1.16106 loss)
I0520 14:02:15.812991 27950 sgd_solver.cpp:106] Iteration 11620, lr = 0.0025
I0520 14:02:17.990645 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_11662.caffemodel
I0520 14:02:18.065524 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_11662.solverstate
I0520 14:02:24.708670 27950 solver.cpp:237] Iteration 11786, loss = 1.29463
I0520 14:02:24.708844 27950 solver.cpp:253]     Train net output #0: loss = 1.29463 (* 1 = 1.29463 loss)
I0520 14:02:24.708861 27950 sgd_solver.cpp:106] Iteration 11786, lr = 0.0025
I0520 14:02:33.533221 27950 solver.cpp:237] Iteration 11952, loss = 1.41071
I0520 14:02:33.533260 27950 solver.cpp:253]     Train net output #0: loss = 1.41071 (* 1 = 1.41071 loss)
I0520 14:02:33.533277 27950 sgd_solver.cpp:106] Iteration 11952, lr = 0.0025
I0520 14:02:42.362835 27950 solver.cpp:237] Iteration 12118, loss = 1.32164
I0520 14:02:42.362871 27950 solver.cpp:253]     Train net output #0: loss = 1.32164 (* 1 = 1.32164 loss)
I0520 14:02:42.362890 27950 sgd_solver.cpp:106] Iteration 12118, lr = 0.0025
I0520 14:03:13.396939 27950 solver.cpp:237] Iteration 12284, loss = 1.40818
I0520 14:03:13.397115 27950 solver.cpp:253]     Train net output #0: loss = 1.40818 (* 1 = 1.40818 loss)
I0520 14:03:13.397133 27950 sgd_solver.cpp:106] Iteration 12284, lr = 0.0025
I0520 14:03:22.230305 27950 solver.cpp:237] Iteration 12450, loss = 1.31956
I0520 14:03:22.230343 27950 solver.cpp:253]     Train net output #0: loss = 1.31956 (* 1 = 1.31956 loss)
I0520 14:03:22.230360 27950 sgd_solver.cpp:106] Iteration 12450, lr = 0.0025
I0520 14:03:31.054092 27950 solver.cpp:237] Iteration 12616, loss = 1.23341
I0520 14:03:31.054128 27950 solver.cpp:253]     Train net output #0: loss = 1.23341 (* 1 = 1.23341 loss)
I0520 14:03:31.054147 27950 sgd_solver.cpp:106] Iteration 12616, lr = 0.0025
I0520 14:03:39.880822 27950 solver.cpp:237] Iteration 12782, loss = 1.26991
I0520 14:03:39.880877 27950 solver.cpp:253]     Train net output #0: loss = 1.26991 (* 1 = 1.26991 loss)
I0520 14:03:39.880904 27950 sgd_solver.cpp:106] Iteration 12782, lr = 0.0025
I0520 14:03:48.711447 27950 solver.cpp:237] Iteration 12948, loss = 1.35802
I0520 14:03:48.711596 27950 solver.cpp:253]     Train net output #0: loss = 1.35802 (* 1 = 1.35802 loss)
I0520 14:03:48.711612 27950 sgd_solver.cpp:106] Iteration 12948, lr = 0.0025
I0520 14:03:57.532987 27950 solver.cpp:237] Iteration 13114, loss = 1.2289
I0520 14:03:57.533025 27950 solver.cpp:253]     Train net output #0: loss = 1.2289 (* 1 = 1.2289 loss)
I0520 14:03:57.533043 27950 sgd_solver.cpp:106] Iteration 13114, lr = 0.0025
I0520 14:04:06.363070 27950 solver.cpp:237] Iteration 13280, loss = 1.13846
I0520 14:04:06.363128 27950 solver.cpp:253]     Train net output #0: loss = 1.13846 (* 1 = 1.13846 loss)
I0520 14:04:06.363154 27950 sgd_solver.cpp:106] Iteration 13280, lr = 0.0025
I0520 14:04:08.860051 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_13328.caffemodel
I0520 14:04:08.934927 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_13328.solverstate
I0520 14:04:09.137681 27950 solver.cpp:341] Iteration 13332, Testing net (#0)
I0520 14:05:17.065886 27950 solver.cpp:409]     Test net output #0: accuracy = 0.838222
I0520 14:05:17.066069 27950 solver.cpp:409]     Test net output #1: loss = 0.535665 (* 1 = 0.535665 loss)
I0520 14:05:45.322008 27950 solver.cpp:237] Iteration 13446, loss = 1.72724
I0520 14:05:45.322070 27950 solver.cpp:253]     Train net output #0: loss = 1.72724 (* 1 = 1.72724 loss)
I0520 14:05:45.322096 27950 sgd_solver.cpp:106] Iteration 13446, lr = 0.0025
I0520 14:05:54.155740 27950 solver.cpp:237] Iteration 13612, loss = 1.24045
I0520 14:05:54.155899 27950 solver.cpp:253]     Train net output #0: loss = 1.24045 (* 1 = 1.24045 loss)
I0520 14:05:54.155915 27950 sgd_solver.cpp:106] Iteration 13612, lr = 0.0025
I0520 14:06:02.992605 27950 solver.cpp:237] Iteration 13778, loss = 1.41904
I0520 14:06:02.992641 27950 solver.cpp:253]     Train net output #0: loss = 1.41904 (* 1 = 1.41904 loss)
I0520 14:06:02.992660 27950 sgd_solver.cpp:106] Iteration 13778, lr = 0.0025
I0520 14:06:11.836386 27950 solver.cpp:237] Iteration 13944, loss = 1.08622
I0520 14:06:11.836443 27950 solver.cpp:253]     Train net output #0: loss = 1.08622 (* 1 = 1.08622 loss)
I0520 14:06:11.836469 27950 sgd_solver.cpp:106] Iteration 13944, lr = 0.0025
I0520 14:06:20.666638 27950 solver.cpp:237] Iteration 14110, loss = 1.30475
I0520 14:06:20.666676 27950 solver.cpp:253]     Train net output #0: loss = 1.30475 (* 1 = 1.30475 loss)
I0520 14:06:20.666693 27950 sgd_solver.cpp:106] Iteration 14110, lr = 0.0025
I0520 14:06:29.501569 27950 solver.cpp:237] Iteration 14276, loss = 0.969696
I0520 14:06:29.501700 27950 solver.cpp:253]     Train net output #0: loss = 0.969696 (* 1 = 0.969696 loss)
I0520 14:06:29.501716 27950 sgd_solver.cpp:106] Iteration 14276, lr = 0.0025
I0520 14:06:38.331792 27950 solver.cpp:237] Iteration 14442, loss = 1.28673
I0520 14:06:38.331848 27950 solver.cpp:253]     Train net output #0: loss = 1.28673 (* 1 = 1.28673 loss)
I0520 14:06:38.331873 27950 sgd_solver.cpp:106] Iteration 14442, lr = 0.0025
I0520 14:07:09.382094 27950 solver.cpp:237] Iteration 14608, loss = 1.1861
I0520 14:07:09.382272 27950 solver.cpp:253]     Train net output #0: loss = 1.1861 (* 1 = 1.1861 loss)
I0520 14:07:09.382290 27950 sgd_solver.cpp:106] Iteration 14608, lr = 0.0025
I0520 14:07:18.218647 27950 solver.cpp:237] Iteration 14774, loss = 1.3369
I0520 14:07:18.218683 27950 solver.cpp:253]     Train net output #0: loss = 1.3369 (* 1 = 1.3369 loss)
I0520 14:07:18.218703 27950 sgd_solver.cpp:106] Iteration 14774, lr = 0.0025
I0520 14:07:27.053923 27950 solver.cpp:237] Iteration 14940, loss = 1.07377
I0520 14:07:27.053982 27950 solver.cpp:253]     Train net output #0: loss = 1.07377 (* 1 = 1.07377 loss)
I0520 14:07:27.054006 27950 sgd_solver.cpp:106] Iteration 14940, lr = 0.0025
I0520 14:07:29.876581 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_14994.caffemodel
I0520 14:07:29.954107 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_14994.solverstate
I0520 14:07:35.955801 27950 solver.cpp:237] Iteration 15106, loss = 1.1908
I0520 14:07:35.955854 27950 solver.cpp:253]     Train net output #0: loss = 1.1908 (* 1 = 1.1908 loss)
I0520 14:07:35.955880 27950 sgd_solver.cpp:106] Iteration 15106, lr = 0.0025
I0520 14:07:44.793581 27950 solver.cpp:237] Iteration 15272, loss = 1.51268
I0520 14:07:44.793743 27950 solver.cpp:253]     Train net output #0: loss = 1.51268 (* 1 = 1.51268 loss)
I0520 14:07:44.793761 27950 sgd_solver.cpp:106] Iteration 15272, lr = 0.0025
I0520 14:07:53.629144 27950 solver.cpp:237] Iteration 15438, loss = 1.32894
I0520 14:07:53.629204 27950 solver.cpp:253]     Train net output #0: loss = 1.32894 (* 1 = 1.32894 loss)
I0520 14:07:53.629222 27950 sgd_solver.cpp:106] Iteration 15438, lr = 0.0025
I0520 14:08:24.626116 27950 solver.cpp:237] Iteration 15604, loss = 1.35299
I0520 14:08:24.626291 27950 solver.cpp:253]     Train net output #0: loss = 1.35299 (* 1 = 1.35299 loss)
I0520 14:08:24.626307 27950 sgd_solver.cpp:106] Iteration 15604, lr = 0.0025
I0520 14:08:33.460768 27950 solver.cpp:237] Iteration 15770, loss = 1.3088
I0520 14:08:33.460805 27950 solver.cpp:253]     Train net output #0: loss = 1.3088 (* 1 = 1.3088 loss)
I0520 14:08:33.460824 27950 sgd_solver.cpp:106] Iteration 15770, lr = 0.0025
I0520 14:08:42.299741 27950 solver.cpp:237] Iteration 15936, loss = 1.13913
I0520 14:08:42.299798 27950 solver.cpp:253]     Train net output #0: loss = 1.13913 (* 1 = 1.13913 loss)
I0520 14:08:42.299823 27950 sgd_solver.cpp:106] Iteration 15936, lr = 0.0025
I0520 14:08:51.128358 27950 solver.cpp:237] Iteration 16102, loss = 1.04291
I0520 14:08:51.128396 27950 solver.cpp:253]     Train net output #0: loss = 1.04291 (* 1 = 1.04291 loss)
I0520 14:08:51.128414 27950 sgd_solver.cpp:106] Iteration 16102, lr = 0.0025
I0520 14:08:59.958340 27950 solver.cpp:237] Iteration 16268, loss = 1.38736
I0520 14:08:59.958501 27950 solver.cpp:253]     Train net output #0: loss = 1.38736 (* 1 = 1.38736 loss)
I0520 14:08:59.958518 27950 sgd_solver.cpp:106] Iteration 16268, lr = 0.0025
I0520 14:09:08.794134 27950 solver.cpp:237] Iteration 16434, loss = 1.3977
I0520 14:09:08.794189 27950 solver.cpp:253]     Train net output #0: loss = 1.3977 (* 1 = 1.3977 loss)
I0520 14:09:08.794216 27950 sgd_solver.cpp:106] Iteration 16434, lr = 0.0025
I0520 14:09:17.612941 27950 solver.cpp:237] Iteration 16600, loss = 1.18726
I0520 14:09:17.612977 27950 solver.cpp:253]     Train net output #0: loss = 1.18726 (* 1 = 1.18726 loss)
I0520 14:09:17.612994 27950 sgd_solver.cpp:106] Iteration 16600, lr = 0.0025
I0520 14:09:20.749733 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_16660.caffemodel
I0520 14:09:20.827702 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_16660.solverstate
I0520 14:09:21.083526 27950 solver.cpp:341] Iteration 16665, Testing net (#0)
I0520 14:10:08.208086 27950 solver.cpp:409]     Test net output #0: accuracy = 0.852262
I0520 14:10:08.208261 27950 solver.cpp:409]     Test net output #1: loss = 0.497577 (* 1 = 0.497577 loss)
I0520 14:10:08.224884 27950 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_16666.caffemodel
I0520 14:10:08.300952 27950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143_iter_16666.solverstate
I0520 14:10:08.328233 27950 solver.cpp:326] Optimization Done.
I0520 14:10:08.328269 27950 caffe.cpp:215] Optimization Done.
Application 11232401 resources: utime ~1335s, stime ~233s, Rss ~5329528, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_90_2016-05-20T11.20.35.954143.solver"
	User time (seconds): 0.56
	System time (seconds): 0.20
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 26:16.58
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15113
	Voluntary context switches: 3000
	Involuntary context switches: 287
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
