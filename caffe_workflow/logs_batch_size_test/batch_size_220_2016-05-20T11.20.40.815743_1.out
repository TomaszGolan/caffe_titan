2805858
I0520 18:39:30.967725  2537 caffe.cpp:184] Using GPUs 0
I0520 18:39:31.393882  2537 solver.cpp:48] Initializing solver from parameters: 
test_iter: 681
test_interval: 1363
base_lr: 0.0025
display: 68
max_iter: 6818
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 681
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743.prototxt"
I0520 18:39:31.395380  2537 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743.prototxt
I0520 18:39:31.407259  2537 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 18:39:31.407320  2537 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 18:39:31.407665  2537 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 220
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 18:39:31.407841  2537 layer_factory.hpp:77] Creating layer data_hdf5
I0520 18:39:31.407866  2537 net.cpp:106] Creating Layer data_hdf5
I0520 18:39:31.407879  2537 net.cpp:411] data_hdf5 -> data
I0520 18:39:31.407914  2537 net.cpp:411] data_hdf5 -> label
I0520 18:39:31.407946  2537 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 18:39:31.409127  2537 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 18:39:31.411401  2537 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 18:39:52.954519  2537 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 18:39:52.959657  2537 net.cpp:150] Setting up data_hdf5
I0520 18:39:52.959697  2537 net.cpp:157] Top shape: 220 1 127 50 (1397000)
I0520 18:39:52.959712  2537 net.cpp:157] Top shape: 220 (220)
I0520 18:39:52.959722  2537 net.cpp:165] Memory required for data: 5588880
I0520 18:39:52.959736  2537 layer_factory.hpp:77] Creating layer conv1
I0520 18:39:52.959770  2537 net.cpp:106] Creating Layer conv1
I0520 18:39:52.959781  2537 net.cpp:454] conv1 <- data
I0520 18:39:52.959803  2537 net.cpp:411] conv1 -> conv1
I0520 18:39:53.327165  2537 net.cpp:150] Setting up conv1
I0520 18:39:53.327213  2537 net.cpp:157] Top shape: 220 12 120 48 (15206400)
I0520 18:39:53.327224  2537 net.cpp:165] Memory required for data: 66414480
I0520 18:39:53.327251  2537 layer_factory.hpp:77] Creating layer relu1
I0520 18:39:53.327272  2537 net.cpp:106] Creating Layer relu1
I0520 18:39:53.327283  2537 net.cpp:454] relu1 <- conv1
I0520 18:39:53.327297  2537 net.cpp:397] relu1 -> conv1 (in-place)
I0520 18:39:53.327813  2537 net.cpp:150] Setting up relu1
I0520 18:39:53.327831  2537 net.cpp:157] Top shape: 220 12 120 48 (15206400)
I0520 18:39:53.327841  2537 net.cpp:165] Memory required for data: 127240080
I0520 18:39:53.327852  2537 layer_factory.hpp:77] Creating layer pool1
I0520 18:39:53.327867  2537 net.cpp:106] Creating Layer pool1
I0520 18:39:53.327877  2537 net.cpp:454] pool1 <- conv1
I0520 18:39:53.327890  2537 net.cpp:411] pool1 -> pool1
I0520 18:39:53.327970  2537 net.cpp:150] Setting up pool1
I0520 18:39:53.327983  2537 net.cpp:157] Top shape: 220 12 60 48 (7603200)
I0520 18:39:53.327993  2537 net.cpp:165] Memory required for data: 157652880
I0520 18:39:53.328003  2537 layer_factory.hpp:77] Creating layer conv2
I0520 18:39:53.328027  2537 net.cpp:106] Creating Layer conv2
I0520 18:39:53.328037  2537 net.cpp:454] conv2 <- pool1
I0520 18:39:53.328050  2537 net.cpp:411] conv2 -> conv2
I0520 18:39:53.330723  2537 net.cpp:150] Setting up conv2
I0520 18:39:53.330750  2537 net.cpp:157] Top shape: 220 20 54 46 (10929600)
I0520 18:39:53.330761  2537 net.cpp:165] Memory required for data: 201371280
I0520 18:39:53.330780  2537 layer_factory.hpp:77] Creating layer relu2
I0520 18:39:53.330795  2537 net.cpp:106] Creating Layer relu2
I0520 18:39:53.330804  2537 net.cpp:454] relu2 <- conv2
I0520 18:39:53.330817  2537 net.cpp:397] relu2 -> conv2 (in-place)
I0520 18:39:53.331146  2537 net.cpp:150] Setting up relu2
I0520 18:39:53.331161  2537 net.cpp:157] Top shape: 220 20 54 46 (10929600)
I0520 18:39:53.331171  2537 net.cpp:165] Memory required for data: 245089680
I0520 18:39:53.331182  2537 layer_factory.hpp:77] Creating layer pool2
I0520 18:39:53.331194  2537 net.cpp:106] Creating Layer pool2
I0520 18:39:53.331204  2537 net.cpp:454] pool2 <- conv2
I0520 18:39:53.331229  2537 net.cpp:411] pool2 -> pool2
I0520 18:39:53.331298  2537 net.cpp:150] Setting up pool2
I0520 18:39:53.331311  2537 net.cpp:157] Top shape: 220 20 27 46 (5464800)
I0520 18:39:53.331321  2537 net.cpp:165] Memory required for data: 266948880
I0520 18:39:53.331331  2537 layer_factory.hpp:77] Creating layer conv3
I0520 18:39:53.331348  2537 net.cpp:106] Creating Layer conv3
I0520 18:39:53.331359  2537 net.cpp:454] conv3 <- pool2
I0520 18:39:53.331372  2537 net.cpp:411] conv3 -> conv3
I0520 18:39:53.333299  2537 net.cpp:150] Setting up conv3
I0520 18:39:53.333323  2537 net.cpp:157] Top shape: 220 28 22 44 (5962880)
I0520 18:39:53.333334  2537 net.cpp:165] Memory required for data: 290800400
I0520 18:39:53.333353  2537 layer_factory.hpp:77] Creating layer relu3
I0520 18:39:53.333369  2537 net.cpp:106] Creating Layer relu3
I0520 18:39:53.333379  2537 net.cpp:454] relu3 <- conv3
I0520 18:39:53.333391  2537 net.cpp:397] relu3 -> conv3 (in-place)
I0520 18:39:53.333861  2537 net.cpp:150] Setting up relu3
I0520 18:39:53.333878  2537 net.cpp:157] Top shape: 220 28 22 44 (5962880)
I0520 18:39:53.333889  2537 net.cpp:165] Memory required for data: 314651920
I0520 18:39:53.333899  2537 layer_factory.hpp:77] Creating layer pool3
I0520 18:39:53.333911  2537 net.cpp:106] Creating Layer pool3
I0520 18:39:53.333920  2537 net.cpp:454] pool3 <- conv3
I0520 18:39:53.333933  2537 net.cpp:411] pool3 -> pool3
I0520 18:39:53.334000  2537 net.cpp:150] Setting up pool3
I0520 18:39:53.334013  2537 net.cpp:157] Top shape: 220 28 11 44 (2981440)
I0520 18:39:53.334023  2537 net.cpp:165] Memory required for data: 326577680
I0520 18:39:53.334033  2537 layer_factory.hpp:77] Creating layer conv4
I0520 18:39:53.334050  2537 net.cpp:106] Creating Layer conv4
I0520 18:39:53.334060  2537 net.cpp:454] conv4 <- pool3
I0520 18:39:53.334074  2537 net.cpp:411] conv4 -> conv4
I0520 18:39:53.336802  2537 net.cpp:150] Setting up conv4
I0520 18:39:53.336823  2537 net.cpp:157] Top shape: 220 36 6 42 (1995840)
I0520 18:39:53.336834  2537 net.cpp:165] Memory required for data: 334561040
I0520 18:39:53.336849  2537 layer_factory.hpp:77] Creating layer relu4
I0520 18:39:53.336864  2537 net.cpp:106] Creating Layer relu4
I0520 18:39:53.336874  2537 net.cpp:454] relu4 <- conv4
I0520 18:39:53.336887  2537 net.cpp:397] relu4 -> conv4 (in-place)
I0520 18:39:53.337349  2537 net.cpp:150] Setting up relu4
I0520 18:39:53.337365  2537 net.cpp:157] Top shape: 220 36 6 42 (1995840)
I0520 18:39:53.337376  2537 net.cpp:165] Memory required for data: 342544400
I0520 18:39:53.337386  2537 layer_factory.hpp:77] Creating layer pool4
I0520 18:39:53.337399  2537 net.cpp:106] Creating Layer pool4
I0520 18:39:53.337409  2537 net.cpp:454] pool4 <- conv4
I0520 18:39:53.337422  2537 net.cpp:411] pool4 -> pool4
I0520 18:39:53.337489  2537 net.cpp:150] Setting up pool4
I0520 18:39:53.337503  2537 net.cpp:157] Top shape: 220 36 3 42 (997920)
I0520 18:39:53.337513  2537 net.cpp:165] Memory required for data: 346536080
I0520 18:39:53.337523  2537 layer_factory.hpp:77] Creating layer ip1
I0520 18:39:53.337543  2537 net.cpp:106] Creating Layer ip1
I0520 18:39:53.337554  2537 net.cpp:454] ip1 <- pool4
I0520 18:39:53.337566  2537 net.cpp:411] ip1 -> ip1
I0520 18:39:53.353106  2537 net.cpp:150] Setting up ip1
I0520 18:39:53.353135  2537 net.cpp:157] Top shape: 220 196 (43120)
I0520 18:39:53.353152  2537 net.cpp:165] Memory required for data: 346708560
I0520 18:39:53.353178  2537 layer_factory.hpp:77] Creating layer relu5
I0520 18:39:53.353193  2537 net.cpp:106] Creating Layer relu5
I0520 18:39:53.353204  2537 net.cpp:454] relu5 <- ip1
I0520 18:39:53.353216  2537 net.cpp:397] relu5 -> ip1 (in-place)
I0520 18:39:53.353559  2537 net.cpp:150] Setting up relu5
I0520 18:39:53.353574  2537 net.cpp:157] Top shape: 220 196 (43120)
I0520 18:39:53.353585  2537 net.cpp:165] Memory required for data: 346881040
I0520 18:39:53.353595  2537 layer_factory.hpp:77] Creating layer drop1
I0520 18:39:53.353615  2537 net.cpp:106] Creating Layer drop1
I0520 18:39:53.353626  2537 net.cpp:454] drop1 <- ip1
I0520 18:39:53.353651  2537 net.cpp:397] drop1 -> ip1 (in-place)
I0520 18:39:53.353698  2537 net.cpp:150] Setting up drop1
I0520 18:39:53.353713  2537 net.cpp:157] Top shape: 220 196 (43120)
I0520 18:39:53.353723  2537 net.cpp:165] Memory required for data: 347053520
I0520 18:39:53.353732  2537 layer_factory.hpp:77] Creating layer ip2
I0520 18:39:53.353750  2537 net.cpp:106] Creating Layer ip2
I0520 18:39:53.353761  2537 net.cpp:454] ip2 <- ip1
I0520 18:39:53.353775  2537 net.cpp:411] ip2 -> ip2
I0520 18:39:53.354238  2537 net.cpp:150] Setting up ip2
I0520 18:39:53.354251  2537 net.cpp:157] Top shape: 220 98 (21560)
I0520 18:39:53.354261  2537 net.cpp:165] Memory required for data: 347139760
I0520 18:39:53.354276  2537 layer_factory.hpp:77] Creating layer relu6
I0520 18:39:53.354288  2537 net.cpp:106] Creating Layer relu6
I0520 18:39:53.354298  2537 net.cpp:454] relu6 <- ip2
I0520 18:39:53.354310  2537 net.cpp:397] relu6 -> ip2 (in-place)
I0520 18:39:53.354831  2537 net.cpp:150] Setting up relu6
I0520 18:39:53.354847  2537 net.cpp:157] Top shape: 220 98 (21560)
I0520 18:39:53.354857  2537 net.cpp:165] Memory required for data: 347226000
I0520 18:39:53.354867  2537 layer_factory.hpp:77] Creating layer drop2
I0520 18:39:53.354881  2537 net.cpp:106] Creating Layer drop2
I0520 18:39:53.354890  2537 net.cpp:454] drop2 <- ip2
I0520 18:39:53.354902  2537 net.cpp:397] drop2 -> ip2 (in-place)
I0520 18:39:53.354945  2537 net.cpp:150] Setting up drop2
I0520 18:39:53.354959  2537 net.cpp:157] Top shape: 220 98 (21560)
I0520 18:39:53.354969  2537 net.cpp:165] Memory required for data: 347312240
I0520 18:39:53.354979  2537 layer_factory.hpp:77] Creating layer ip3
I0520 18:39:53.354991  2537 net.cpp:106] Creating Layer ip3
I0520 18:39:53.355001  2537 net.cpp:454] ip3 <- ip2
I0520 18:39:53.355015  2537 net.cpp:411] ip3 -> ip3
I0520 18:39:53.355226  2537 net.cpp:150] Setting up ip3
I0520 18:39:53.355238  2537 net.cpp:157] Top shape: 220 11 (2420)
I0520 18:39:53.355248  2537 net.cpp:165] Memory required for data: 347321920
I0520 18:39:53.355263  2537 layer_factory.hpp:77] Creating layer drop3
I0520 18:39:53.355276  2537 net.cpp:106] Creating Layer drop3
I0520 18:39:53.355285  2537 net.cpp:454] drop3 <- ip3
I0520 18:39:53.355298  2537 net.cpp:397] drop3 -> ip3 (in-place)
I0520 18:39:53.355337  2537 net.cpp:150] Setting up drop3
I0520 18:39:53.355350  2537 net.cpp:157] Top shape: 220 11 (2420)
I0520 18:39:53.355360  2537 net.cpp:165] Memory required for data: 347331600
I0520 18:39:53.355370  2537 layer_factory.hpp:77] Creating layer loss
I0520 18:39:53.355388  2537 net.cpp:106] Creating Layer loss
I0520 18:39:53.355398  2537 net.cpp:454] loss <- ip3
I0520 18:39:53.355409  2537 net.cpp:454] loss <- label
I0520 18:39:53.355422  2537 net.cpp:411] loss -> loss
I0520 18:39:53.355438  2537 layer_factory.hpp:77] Creating layer loss
I0520 18:39:53.356081  2537 net.cpp:150] Setting up loss
I0520 18:39:53.356101  2537 net.cpp:157] Top shape: (1)
I0520 18:39:53.356114  2537 net.cpp:160]     with loss weight 1
I0520 18:39:53.356156  2537 net.cpp:165] Memory required for data: 347331604
I0520 18:39:53.356166  2537 net.cpp:226] loss needs backward computation.
I0520 18:39:53.356178  2537 net.cpp:226] drop3 needs backward computation.
I0520 18:39:53.356187  2537 net.cpp:226] ip3 needs backward computation.
I0520 18:39:53.356197  2537 net.cpp:226] drop2 needs backward computation.
I0520 18:39:53.356206  2537 net.cpp:226] relu6 needs backward computation.
I0520 18:39:53.356216  2537 net.cpp:226] ip2 needs backward computation.
I0520 18:39:53.356227  2537 net.cpp:226] drop1 needs backward computation.
I0520 18:39:53.356237  2537 net.cpp:226] relu5 needs backward computation.
I0520 18:39:53.356248  2537 net.cpp:226] ip1 needs backward computation.
I0520 18:39:53.356258  2537 net.cpp:226] pool4 needs backward computation.
I0520 18:39:53.356268  2537 net.cpp:226] relu4 needs backward computation.
I0520 18:39:53.356278  2537 net.cpp:226] conv4 needs backward computation.
I0520 18:39:53.356288  2537 net.cpp:226] pool3 needs backward computation.
I0520 18:39:53.356308  2537 net.cpp:226] relu3 needs backward computation.
I0520 18:39:53.356318  2537 net.cpp:226] conv3 needs backward computation.
I0520 18:39:53.356326  2537 net.cpp:226] pool2 needs backward computation.
I0520 18:39:53.356338  2537 net.cpp:226] relu2 needs backward computation.
I0520 18:39:53.356348  2537 net.cpp:226] conv2 needs backward computation.
I0520 18:39:53.356359  2537 net.cpp:226] pool1 needs backward computation.
I0520 18:39:53.356369  2537 net.cpp:226] relu1 needs backward computation.
I0520 18:39:53.356379  2537 net.cpp:226] conv1 needs backward computation.
I0520 18:39:53.356389  2537 net.cpp:228] data_hdf5 does not need backward computation.
I0520 18:39:53.356400  2537 net.cpp:270] This network produces output loss
I0520 18:39:53.356422  2537 net.cpp:283] Network initialization done.
I0520 18:39:53.357990  2537 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743.prototxt
I0520 18:39:53.358060  2537 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 18:39:53.358414  2537 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 220
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 18:39:53.358603  2537 layer_factory.hpp:77] Creating layer data_hdf5
I0520 18:39:53.358618  2537 net.cpp:106] Creating Layer data_hdf5
I0520 18:39:53.358629  2537 net.cpp:411] data_hdf5 -> data
I0520 18:39:53.358659  2537 net.cpp:411] data_hdf5 -> label
I0520 18:39:53.358675  2537 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 18:39:53.359827  2537 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 18:40:14.683305  2537 net.cpp:150] Setting up data_hdf5
I0520 18:40:14.683470  2537 net.cpp:157] Top shape: 220 1 127 50 (1397000)
I0520 18:40:14.683485  2537 net.cpp:157] Top shape: 220 (220)
I0520 18:40:14.683495  2537 net.cpp:165] Memory required for data: 5588880
I0520 18:40:14.683508  2537 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 18:40:14.683537  2537 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 18:40:14.683547  2537 net.cpp:454] label_data_hdf5_1_split <- label
I0520 18:40:14.683562  2537 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 18:40:14.683584  2537 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 18:40:14.683656  2537 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 18:40:14.683670  2537 net.cpp:157] Top shape: 220 (220)
I0520 18:40:14.683681  2537 net.cpp:157] Top shape: 220 (220)
I0520 18:40:14.683691  2537 net.cpp:165] Memory required for data: 5590640
I0520 18:40:14.683702  2537 layer_factory.hpp:77] Creating layer conv1
I0520 18:40:14.683724  2537 net.cpp:106] Creating Layer conv1
I0520 18:40:14.683734  2537 net.cpp:454] conv1 <- data
I0520 18:40:14.683749  2537 net.cpp:411] conv1 -> conv1
I0520 18:40:14.685668  2537 net.cpp:150] Setting up conv1
I0520 18:40:14.685693  2537 net.cpp:157] Top shape: 220 12 120 48 (15206400)
I0520 18:40:14.685704  2537 net.cpp:165] Memory required for data: 66416240
I0520 18:40:14.685725  2537 layer_factory.hpp:77] Creating layer relu1
I0520 18:40:14.685741  2537 net.cpp:106] Creating Layer relu1
I0520 18:40:14.685750  2537 net.cpp:454] relu1 <- conv1
I0520 18:40:14.685763  2537 net.cpp:397] relu1 -> conv1 (in-place)
I0520 18:40:14.686262  2537 net.cpp:150] Setting up relu1
I0520 18:40:14.686278  2537 net.cpp:157] Top shape: 220 12 120 48 (15206400)
I0520 18:40:14.686290  2537 net.cpp:165] Memory required for data: 127241840
I0520 18:40:14.686300  2537 layer_factory.hpp:77] Creating layer pool1
I0520 18:40:14.686316  2537 net.cpp:106] Creating Layer pool1
I0520 18:40:14.686326  2537 net.cpp:454] pool1 <- conv1
I0520 18:40:14.686338  2537 net.cpp:411] pool1 -> pool1
I0520 18:40:14.686414  2537 net.cpp:150] Setting up pool1
I0520 18:40:14.686426  2537 net.cpp:157] Top shape: 220 12 60 48 (7603200)
I0520 18:40:14.686436  2537 net.cpp:165] Memory required for data: 157654640
I0520 18:40:14.686446  2537 layer_factory.hpp:77] Creating layer conv2
I0520 18:40:14.686465  2537 net.cpp:106] Creating Layer conv2
I0520 18:40:14.686475  2537 net.cpp:454] conv2 <- pool1
I0520 18:40:14.686488  2537 net.cpp:411] conv2 -> conv2
I0520 18:40:14.688411  2537 net.cpp:150] Setting up conv2
I0520 18:40:14.688434  2537 net.cpp:157] Top shape: 220 20 54 46 (10929600)
I0520 18:40:14.688446  2537 net.cpp:165] Memory required for data: 201373040
I0520 18:40:14.688463  2537 layer_factory.hpp:77] Creating layer relu2
I0520 18:40:14.688477  2537 net.cpp:106] Creating Layer relu2
I0520 18:40:14.688488  2537 net.cpp:454] relu2 <- conv2
I0520 18:40:14.688499  2537 net.cpp:397] relu2 -> conv2 (in-place)
I0520 18:40:14.688832  2537 net.cpp:150] Setting up relu2
I0520 18:40:14.688846  2537 net.cpp:157] Top shape: 220 20 54 46 (10929600)
I0520 18:40:14.688856  2537 net.cpp:165] Memory required for data: 245091440
I0520 18:40:14.688866  2537 layer_factory.hpp:77] Creating layer pool2
I0520 18:40:14.688879  2537 net.cpp:106] Creating Layer pool2
I0520 18:40:14.688889  2537 net.cpp:454] pool2 <- conv2
I0520 18:40:14.688901  2537 net.cpp:411] pool2 -> pool2
I0520 18:40:14.688972  2537 net.cpp:150] Setting up pool2
I0520 18:40:14.688985  2537 net.cpp:157] Top shape: 220 20 27 46 (5464800)
I0520 18:40:14.688995  2537 net.cpp:165] Memory required for data: 266950640
I0520 18:40:14.689005  2537 layer_factory.hpp:77] Creating layer conv3
I0520 18:40:14.689023  2537 net.cpp:106] Creating Layer conv3
I0520 18:40:14.689033  2537 net.cpp:454] conv3 <- pool2
I0520 18:40:14.689046  2537 net.cpp:411] conv3 -> conv3
I0520 18:40:14.691016  2537 net.cpp:150] Setting up conv3
I0520 18:40:14.691040  2537 net.cpp:157] Top shape: 220 28 22 44 (5962880)
I0520 18:40:14.691051  2537 net.cpp:165] Memory required for data: 290802160
I0520 18:40:14.691084  2537 layer_factory.hpp:77] Creating layer relu3
I0520 18:40:14.691098  2537 net.cpp:106] Creating Layer relu3
I0520 18:40:14.691108  2537 net.cpp:454] relu3 <- conv3
I0520 18:40:14.691121  2537 net.cpp:397] relu3 -> conv3 (in-place)
I0520 18:40:14.691593  2537 net.cpp:150] Setting up relu3
I0520 18:40:14.691609  2537 net.cpp:157] Top shape: 220 28 22 44 (5962880)
I0520 18:40:14.691619  2537 net.cpp:165] Memory required for data: 314653680
I0520 18:40:14.691630  2537 layer_factory.hpp:77] Creating layer pool3
I0520 18:40:14.691643  2537 net.cpp:106] Creating Layer pool3
I0520 18:40:14.691653  2537 net.cpp:454] pool3 <- conv3
I0520 18:40:14.691665  2537 net.cpp:411] pool3 -> pool3
I0520 18:40:14.691737  2537 net.cpp:150] Setting up pool3
I0520 18:40:14.691751  2537 net.cpp:157] Top shape: 220 28 11 44 (2981440)
I0520 18:40:14.691761  2537 net.cpp:165] Memory required for data: 326579440
I0520 18:40:14.691768  2537 layer_factory.hpp:77] Creating layer conv4
I0520 18:40:14.691787  2537 net.cpp:106] Creating Layer conv4
I0520 18:40:14.691797  2537 net.cpp:454] conv4 <- pool3
I0520 18:40:14.691812  2537 net.cpp:411] conv4 -> conv4
I0520 18:40:14.693872  2537 net.cpp:150] Setting up conv4
I0520 18:40:14.693895  2537 net.cpp:157] Top shape: 220 36 6 42 (1995840)
I0520 18:40:14.693907  2537 net.cpp:165] Memory required for data: 334562800
I0520 18:40:14.693922  2537 layer_factory.hpp:77] Creating layer relu4
I0520 18:40:14.693936  2537 net.cpp:106] Creating Layer relu4
I0520 18:40:14.693946  2537 net.cpp:454] relu4 <- conv4
I0520 18:40:14.693959  2537 net.cpp:397] relu4 -> conv4 (in-place)
I0520 18:40:14.694427  2537 net.cpp:150] Setting up relu4
I0520 18:40:14.694443  2537 net.cpp:157] Top shape: 220 36 6 42 (1995840)
I0520 18:40:14.694453  2537 net.cpp:165] Memory required for data: 342546160
I0520 18:40:14.694463  2537 layer_factory.hpp:77] Creating layer pool4
I0520 18:40:14.694476  2537 net.cpp:106] Creating Layer pool4
I0520 18:40:14.694486  2537 net.cpp:454] pool4 <- conv4
I0520 18:40:14.694499  2537 net.cpp:411] pool4 -> pool4
I0520 18:40:14.694571  2537 net.cpp:150] Setting up pool4
I0520 18:40:14.694584  2537 net.cpp:157] Top shape: 220 36 3 42 (997920)
I0520 18:40:14.694594  2537 net.cpp:165] Memory required for data: 346537840
I0520 18:40:14.694602  2537 layer_factory.hpp:77] Creating layer ip1
I0520 18:40:14.694617  2537 net.cpp:106] Creating Layer ip1
I0520 18:40:14.694628  2537 net.cpp:454] ip1 <- pool4
I0520 18:40:14.694648  2537 net.cpp:411] ip1 -> ip1
I0520 18:40:14.710108  2537 net.cpp:150] Setting up ip1
I0520 18:40:14.710136  2537 net.cpp:157] Top shape: 220 196 (43120)
I0520 18:40:14.710147  2537 net.cpp:165] Memory required for data: 346710320
I0520 18:40:14.710170  2537 layer_factory.hpp:77] Creating layer relu5
I0520 18:40:14.710185  2537 net.cpp:106] Creating Layer relu5
I0520 18:40:14.710196  2537 net.cpp:454] relu5 <- ip1
I0520 18:40:14.710208  2537 net.cpp:397] relu5 -> ip1 (in-place)
I0520 18:40:14.710556  2537 net.cpp:150] Setting up relu5
I0520 18:40:14.710571  2537 net.cpp:157] Top shape: 220 196 (43120)
I0520 18:40:14.710580  2537 net.cpp:165] Memory required for data: 346882800
I0520 18:40:14.710590  2537 layer_factory.hpp:77] Creating layer drop1
I0520 18:40:14.710609  2537 net.cpp:106] Creating Layer drop1
I0520 18:40:14.710619  2537 net.cpp:454] drop1 <- ip1
I0520 18:40:14.710633  2537 net.cpp:397] drop1 -> ip1 (in-place)
I0520 18:40:14.710685  2537 net.cpp:150] Setting up drop1
I0520 18:40:14.710700  2537 net.cpp:157] Top shape: 220 196 (43120)
I0520 18:40:14.710708  2537 net.cpp:165] Memory required for data: 347055280
I0520 18:40:14.710718  2537 layer_factory.hpp:77] Creating layer ip2
I0520 18:40:14.710733  2537 net.cpp:106] Creating Layer ip2
I0520 18:40:14.710743  2537 net.cpp:454] ip2 <- ip1
I0520 18:40:14.710757  2537 net.cpp:411] ip2 -> ip2
I0520 18:40:14.711237  2537 net.cpp:150] Setting up ip2
I0520 18:40:14.711251  2537 net.cpp:157] Top shape: 220 98 (21560)
I0520 18:40:14.711262  2537 net.cpp:165] Memory required for data: 347141520
I0520 18:40:14.711290  2537 layer_factory.hpp:77] Creating layer relu6
I0520 18:40:14.711303  2537 net.cpp:106] Creating Layer relu6
I0520 18:40:14.711313  2537 net.cpp:454] relu6 <- ip2
I0520 18:40:14.711326  2537 net.cpp:397] relu6 -> ip2 (in-place)
I0520 18:40:14.711858  2537 net.cpp:150] Setting up relu6
I0520 18:40:14.711879  2537 net.cpp:157] Top shape: 220 98 (21560)
I0520 18:40:14.711889  2537 net.cpp:165] Memory required for data: 347227760
I0520 18:40:14.711899  2537 layer_factory.hpp:77] Creating layer drop2
I0520 18:40:14.711913  2537 net.cpp:106] Creating Layer drop2
I0520 18:40:14.711922  2537 net.cpp:454] drop2 <- ip2
I0520 18:40:14.711936  2537 net.cpp:397] drop2 -> ip2 (in-place)
I0520 18:40:14.711980  2537 net.cpp:150] Setting up drop2
I0520 18:40:14.711993  2537 net.cpp:157] Top shape: 220 98 (21560)
I0520 18:40:14.712003  2537 net.cpp:165] Memory required for data: 347314000
I0520 18:40:14.712013  2537 layer_factory.hpp:77] Creating layer ip3
I0520 18:40:14.712028  2537 net.cpp:106] Creating Layer ip3
I0520 18:40:14.712038  2537 net.cpp:454] ip3 <- ip2
I0520 18:40:14.712050  2537 net.cpp:411] ip3 -> ip3
I0520 18:40:14.712275  2537 net.cpp:150] Setting up ip3
I0520 18:40:14.712287  2537 net.cpp:157] Top shape: 220 11 (2420)
I0520 18:40:14.712298  2537 net.cpp:165] Memory required for data: 347323680
I0520 18:40:14.712313  2537 layer_factory.hpp:77] Creating layer drop3
I0520 18:40:14.712327  2537 net.cpp:106] Creating Layer drop3
I0520 18:40:14.712337  2537 net.cpp:454] drop3 <- ip3
I0520 18:40:14.712348  2537 net.cpp:397] drop3 -> ip3 (in-place)
I0520 18:40:14.712390  2537 net.cpp:150] Setting up drop3
I0520 18:40:14.712402  2537 net.cpp:157] Top shape: 220 11 (2420)
I0520 18:40:14.712412  2537 net.cpp:165] Memory required for data: 347333360
I0520 18:40:14.712422  2537 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 18:40:14.712435  2537 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 18:40:14.712445  2537 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 18:40:14.712457  2537 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 18:40:14.712472  2537 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 18:40:14.712546  2537 net.cpp:150] Setting up ip3_drop3_0_split
I0520 18:40:14.712559  2537 net.cpp:157] Top shape: 220 11 (2420)
I0520 18:40:14.712571  2537 net.cpp:157] Top shape: 220 11 (2420)
I0520 18:40:14.712581  2537 net.cpp:165] Memory required for data: 347352720
I0520 18:40:14.712592  2537 layer_factory.hpp:77] Creating layer accuracy
I0520 18:40:14.712613  2537 net.cpp:106] Creating Layer accuracy
I0520 18:40:14.712623  2537 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 18:40:14.712635  2537 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 18:40:14.712648  2537 net.cpp:411] accuracy -> accuracy
I0520 18:40:14.712671  2537 net.cpp:150] Setting up accuracy
I0520 18:40:14.712683  2537 net.cpp:157] Top shape: (1)
I0520 18:40:14.712693  2537 net.cpp:165] Memory required for data: 347352724
I0520 18:40:14.712703  2537 layer_factory.hpp:77] Creating layer loss
I0520 18:40:14.712716  2537 net.cpp:106] Creating Layer loss
I0520 18:40:14.712726  2537 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 18:40:14.712738  2537 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 18:40:14.712750  2537 net.cpp:411] loss -> loss
I0520 18:40:14.712769  2537 layer_factory.hpp:77] Creating layer loss
I0520 18:40:14.713251  2537 net.cpp:150] Setting up loss
I0520 18:40:14.713265  2537 net.cpp:157] Top shape: (1)
I0520 18:40:14.713275  2537 net.cpp:160]     with loss weight 1
I0520 18:40:14.713294  2537 net.cpp:165] Memory required for data: 347352728
I0520 18:40:14.713304  2537 net.cpp:226] loss needs backward computation.
I0520 18:40:14.713315  2537 net.cpp:228] accuracy does not need backward computation.
I0520 18:40:14.713325  2537 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 18:40:14.713335  2537 net.cpp:226] drop3 needs backward computation.
I0520 18:40:14.713346  2537 net.cpp:226] ip3 needs backward computation.
I0520 18:40:14.713356  2537 net.cpp:226] drop2 needs backward computation.
I0520 18:40:14.713376  2537 net.cpp:226] relu6 needs backward computation.
I0520 18:40:14.713387  2537 net.cpp:226] ip2 needs backward computation.
I0520 18:40:14.713397  2537 net.cpp:226] drop1 needs backward computation.
I0520 18:40:14.713405  2537 net.cpp:226] relu5 needs backward computation.
I0520 18:40:14.713415  2537 net.cpp:226] ip1 needs backward computation.
I0520 18:40:14.713425  2537 net.cpp:226] pool4 needs backward computation.
I0520 18:40:14.713435  2537 net.cpp:226] relu4 needs backward computation.
I0520 18:40:14.713445  2537 net.cpp:226] conv4 needs backward computation.
I0520 18:40:14.713454  2537 net.cpp:226] pool3 needs backward computation.
I0520 18:40:14.713464  2537 net.cpp:226] relu3 needs backward computation.
I0520 18:40:14.713475  2537 net.cpp:226] conv3 needs backward computation.
I0520 18:40:14.713485  2537 net.cpp:226] pool2 needs backward computation.
I0520 18:40:14.713496  2537 net.cpp:226] relu2 needs backward computation.
I0520 18:40:14.713505  2537 net.cpp:226] conv2 needs backward computation.
I0520 18:40:14.713515  2537 net.cpp:226] pool1 needs backward computation.
I0520 18:40:14.713526  2537 net.cpp:226] relu1 needs backward computation.
I0520 18:40:14.713536  2537 net.cpp:226] conv1 needs backward computation.
I0520 18:40:14.713546  2537 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 18:40:14.713558  2537 net.cpp:228] data_hdf5 does not need backward computation.
I0520 18:40:14.713568  2537 net.cpp:270] This network produces output accuracy
I0520 18:40:14.713578  2537 net.cpp:270] This network produces output loss
I0520 18:40:14.713608  2537 net.cpp:283] Network initialization done.
I0520 18:40:14.713742  2537 solver.cpp:60] Solver scaffolding done.
I0520 18:40:14.714889  2537 caffe.cpp:212] Starting Optimization
I0520 18:40:14.714906  2537 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 18:40:14.714920  2537 solver.cpp:289] Learning Rate Policy: fixed
I0520 18:40:14.716130  2537 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 18:41:01.236748  2537 solver.cpp:409]     Test net output #0: accuracy = 0.113223
I0520 18:41:01.236907  2537 solver.cpp:409]     Test net output #1: loss = 2.39732 (* 1 = 2.39732 loss)
I0520 18:41:01.289604  2537 solver.cpp:237] Iteration 0, loss = 2.39746
I0520 18:41:01.289640  2537 solver.cpp:253]     Train net output #0: loss = 2.39746 (* 1 = 2.39746 loss)
I0520 18:41:01.289659  2537 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 18:41:09.560964  2537 solver.cpp:237] Iteration 68, loss = 2.32491
I0520 18:41:09.561000  2537 solver.cpp:253]     Train net output #0: loss = 2.32491 (* 1 = 2.32491 loss)
I0520 18:41:09.561017  2537 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0520 18:41:17.832173  2537 solver.cpp:237] Iteration 136, loss = 2.3135
I0520 18:41:17.832206  2537 solver.cpp:253]     Train net output #0: loss = 2.3135 (* 1 = 2.3135 loss)
I0520 18:41:17.832223  2537 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0520 18:41:26.105793  2537 solver.cpp:237] Iteration 204, loss = 2.28092
I0520 18:41:26.105836  2537 solver.cpp:253]     Train net output #0: loss = 2.28092 (* 1 = 2.28092 loss)
I0520 18:41:26.105856  2537 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0520 18:41:34.376737  2537 solver.cpp:237] Iteration 272, loss = 2.15475
I0520 18:41:34.376886  2537 solver.cpp:253]     Train net output #0: loss = 2.15475 (* 1 = 2.15475 loss)
I0520 18:41:34.376900  2537 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0520 18:41:42.642976  2537 solver.cpp:237] Iteration 340, loss = 2.1556
I0520 18:41:42.643009  2537 solver.cpp:253]     Train net output #0: loss = 2.1556 (* 1 = 2.1556 loss)
I0520 18:41:42.643026  2537 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0520 18:41:50.914692  2537 solver.cpp:237] Iteration 408, loss = 2.00721
I0520 18:41:50.914733  2537 solver.cpp:253]     Train net output #0: loss = 2.00721 (* 1 = 2.00721 loss)
I0520 18:41:50.914747  2537 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0520 18:42:21.280721  2537 solver.cpp:237] Iteration 476, loss = 2.0291
I0520 18:42:21.280882  2537 solver.cpp:253]     Train net output #0: loss = 2.0291 (* 1 = 2.0291 loss)
I0520 18:42:21.280897  2537 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0520 18:42:29.554175  2537 solver.cpp:237] Iteration 544, loss = 1.90075
I0520 18:42:29.554209  2537 solver.cpp:253]     Train net output #0: loss = 1.90075 (* 1 = 1.90075 loss)
I0520 18:42:29.554225  2537 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0520 18:42:37.825403  2537 solver.cpp:237] Iteration 612, loss = 1.93744
I0520 18:42:37.825449  2537 solver.cpp:253]     Train net output #0: loss = 1.93744 (* 1 = 1.93744 loss)
I0520 18:42:37.825466  2537 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0520 18:42:46.095666  2537 solver.cpp:237] Iteration 680, loss = 1.9075
I0520 18:42:46.095701  2537 solver.cpp:253]     Train net output #0: loss = 1.9075 (* 1 = 1.9075 loss)
I0520 18:42:46.095715  2537 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0520 18:42:46.096099  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_681.caffemodel
I0520 18:42:46.222307  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_681.solverstate
I0520 18:42:54.438252  2537 solver.cpp:237] Iteration 748, loss = 1.81889
I0520 18:42:54.438402  2537 solver.cpp:253]     Train net output #0: loss = 1.81889 (* 1 = 1.81889 loss)
I0520 18:42:54.438416  2537 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0520 18:43:02.714576  2537 solver.cpp:237] Iteration 816, loss = 1.85355
I0520 18:43:02.714612  2537 solver.cpp:253]     Train net output #0: loss = 1.85355 (* 1 = 1.85355 loss)
I0520 18:43:02.714629  2537 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0520 18:43:10.991171  2537 solver.cpp:237] Iteration 884, loss = 1.74467
I0520 18:43:10.991205  2537 solver.cpp:253]     Train net output #0: loss = 1.74467 (* 1 = 1.74467 loss)
I0520 18:43:10.991226  2537 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0520 18:43:41.386157  2537 solver.cpp:237] Iteration 952, loss = 1.83484
I0520 18:43:41.386312  2537 solver.cpp:253]     Train net output #0: loss = 1.83484 (* 1 = 1.83484 loss)
I0520 18:43:41.386327  2537 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0520 18:43:49.659660  2537 solver.cpp:237] Iteration 1020, loss = 1.83306
I0520 18:43:49.659694  2537 solver.cpp:253]     Train net output #0: loss = 1.83306 (* 1 = 1.83306 loss)
I0520 18:43:49.659713  2537 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0520 18:43:57.936002  2537 solver.cpp:237] Iteration 1088, loss = 1.86398
I0520 18:43:57.936048  2537 solver.cpp:253]     Train net output #0: loss = 1.86398 (* 1 = 1.86398 loss)
I0520 18:43:57.936060  2537 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0520 18:44:06.208508  2537 solver.cpp:237] Iteration 1156, loss = 1.74712
I0520 18:44:06.208542  2537 solver.cpp:253]     Train net output #0: loss = 1.74712 (* 1 = 1.74712 loss)
I0520 18:44:06.208559  2537 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0520 18:44:14.481259  2537 solver.cpp:237] Iteration 1224, loss = 1.65642
I0520 18:44:14.481405  2537 solver.cpp:253]     Train net output #0: loss = 1.65642 (* 1 = 1.65642 loss)
I0520 18:44:14.481420  2537 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0520 18:44:22.752635  2537 solver.cpp:237] Iteration 1292, loss = 1.75381
I0520 18:44:22.752677  2537 solver.cpp:253]     Train net output #0: loss = 1.75381 (* 1 = 1.75381 loss)
I0520 18:44:22.752698  2537 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0520 18:44:31.020876  2537 solver.cpp:237] Iteration 1360, loss = 1.75872
I0520 18:44:31.020911  2537 solver.cpp:253]     Train net output #0: loss = 1.75872 (* 1 = 1.75872 loss)
I0520 18:44:31.020927  2537 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0520 18:44:31.143074  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_1362.caffemodel
I0520 18:44:31.265962  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_1362.solverstate
I0520 18:44:31.329324  2537 solver.cpp:341] Iteration 1363, Testing net (#0)
I0520 18:45:16.971443  2537 solver.cpp:409]     Test net output #0: accuracy = 0.649079
I0520 18:45:16.971602  2537 solver.cpp:409]     Test net output #1: loss = 1.19527 (* 1 = 1.19527 loss)
I0520 18:45:47.048693  2537 solver.cpp:237] Iteration 1428, loss = 1.76146
I0520 18:45:47.048848  2537 solver.cpp:253]     Train net output #0: loss = 1.76146 (* 1 = 1.76146 loss)
I0520 18:45:47.048864  2537 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0520 18:45:55.322203  2537 solver.cpp:237] Iteration 1496, loss = 1.65417
I0520 18:45:55.322237  2537 solver.cpp:253]     Train net output #0: loss = 1.65417 (* 1 = 1.65417 loss)
I0520 18:45:55.322253  2537 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 18:46:03.589926  2537 solver.cpp:237] Iteration 1564, loss = 1.72865
I0520 18:46:03.589967  2537 solver.cpp:253]     Train net output #0: loss = 1.72865 (* 1 = 1.72865 loss)
I0520 18:46:03.589987  2537 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0520 18:46:11.861155  2537 solver.cpp:237] Iteration 1632, loss = 1.7187
I0520 18:46:11.861189  2537 solver.cpp:253]     Train net output #0: loss = 1.7187 (* 1 = 1.7187 loss)
I0520 18:46:11.861205  2537 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0520 18:46:20.132722  2537 solver.cpp:237] Iteration 1700, loss = 1.71829
I0520 18:46:20.132861  2537 solver.cpp:253]     Train net output #0: loss = 1.71829 (* 1 = 1.71829 loss)
I0520 18:46:20.132875  2537 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0520 18:46:28.404564  2537 solver.cpp:237] Iteration 1768, loss = 1.70399
I0520 18:46:28.404606  2537 solver.cpp:253]     Train net output #0: loss = 1.70399 (* 1 = 1.70399 loss)
I0520 18:46:28.404625  2537 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0520 18:46:58.858175  2537 solver.cpp:237] Iteration 1836, loss = 1.66589
I0520 18:46:58.858340  2537 solver.cpp:253]     Train net output #0: loss = 1.66589 (* 1 = 1.66589 loss)
I0520 18:46:58.858355  2537 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0520 18:47:07.131299  2537 solver.cpp:237] Iteration 1904, loss = 1.59953
I0520 18:47:07.131332  2537 solver.cpp:253]     Train net output #0: loss = 1.59953 (* 1 = 1.59953 loss)
I0520 18:47:07.131350  2537 sgd_solver.cpp:106] Iteration 1904, lr = 0.0025
I0520 18:47:15.405578  2537 solver.cpp:237] Iteration 1972, loss = 1.71982
I0520 18:47:15.405624  2537 solver.cpp:253]     Train net output #0: loss = 1.71982 (* 1 = 1.71982 loss)
I0520 18:47:15.405639  2537 sgd_solver.cpp:106] Iteration 1972, lr = 0.0025
I0520 18:47:23.678390  2537 solver.cpp:237] Iteration 2040, loss = 1.64844
I0520 18:47:23.678424  2537 solver.cpp:253]     Train net output #0: loss = 1.64844 (* 1 = 1.64844 loss)
I0520 18:47:23.678438  2537 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0520 18:47:23.922140  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_2043.caffemodel
I0520 18:47:24.047902  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_2043.solverstate
I0520 18:47:32.020890  2537 solver.cpp:237] Iteration 2108, loss = 1.54764
I0520 18:47:32.021057  2537 solver.cpp:253]     Train net output #0: loss = 1.54764 (* 1 = 1.54764 loss)
I0520 18:47:32.021072  2537 sgd_solver.cpp:106] Iteration 2108, lr = 0.0025
I0520 18:47:40.291025  2537 solver.cpp:237] Iteration 2176, loss = 1.62362
I0520 18:47:40.291064  2537 solver.cpp:253]     Train net output #0: loss = 1.62362 (* 1 = 1.62362 loss)
I0520 18:47:40.291085  2537 sgd_solver.cpp:106] Iteration 2176, lr = 0.0025
I0520 18:47:48.562525  2537 solver.cpp:237] Iteration 2244, loss = 1.65302
I0520 18:47:48.562561  2537 solver.cpp:253]     Train net output #0: loss = 1.65302 (* 1 = 1.65302 loss)
I0520 18:47:48.562575  2537 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0520 18:48:19.008780  2537 solver.cpp:237] Iteration 2312, loss = 1.66692
I0520 18:48:19.008942  2537 solver.cpp:253]     Train net output #0: loss = 1.66692 (* 1 = 1.66692 loss)
I0520 18:48:19.008957  2537 sgd_solver.cpp:106] Iteration 2312, lr = 0.0025
I0520 18:48:27.280725  2537 solver.cpp:237] Iteration 2380, loss = 1.50144
I0520 18:48:27.280757  2537 solver.cpp:253]     Train net output #0: loss = 1.50144 (* 1 = 1.50144 loss)
I0520 18:48:27.280771  2537 sgd_solver.cpp:106] Iteration 2380, lr = 0.0025
I0520 18:48:35.549311  2537 solver.cpp:237] Iteration 2448, loss = 1.62404
I0520 18:48:35.549348  2537 solver.cpp:253]     Train net output #0: loss = 1.62404 (* 1 = 1.62404 loss)
I0520 18:48:35.549371  2537 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0520 18:48:43.815392  2537 solver.cpp:237] Iteration 2516, loss = 1.6511
I0520 18:48:43.815425  2537 solver.cpp:253]     Train net output #0: loss = 1.6511 (* 1 = 1.6511 loss)
I0520 18:48:43.815441  2537 sgd_solver.cpp:106] Iteration 2516, lr = 0.0025
I0520 18:48:52.081851  2537 solver.cpp:237] Iteration 2584, loss = 1.64174
I0520 18:48:52.081996  2537 solver.cpp:253]     Train net output #0: loss = 1.64174 (* 1 = 1.64174 loss)
I0520 18:48:52.082010  2537 sgd_solver.cpp:106] Iteration 2584, lr = 0.0025
I0520 18:49:00.357237  2537 solver.cpp:237] Iteration 2652, loss = 1.55691
I0520 18:49:00.357280  2537 solver.cpp:253]     Train net output #0: loss = 1.55691 (* 1 = 1.55691 loss)
I0520 18:49:00.357300  2537 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0520 18:49:08.634863  2537 solver.cpp:237] Iteration 2720, loss = 1.58174
I0520 18:49:08.634898  2537 solver.cpp:253]     Train net output #0: loss = 1.58174 (* 1 = 1.58174 loss)
I0520 18:49:08.634914  2537 sgd_solver.cpp:106] Iteration 2720, lr = 0.0025
I0520 18:49:09.000504  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_2724.caffemodel
I0520 18:49:09.126319  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_2724.solverstate
I0520 18:49:09.313268  2537 solver.cpp:341] Iteration 2726, Testing net (#0)
I0520 18:50:15.850394  2537 solver.cpp:409]     Test net output #0: accuracy = 0.707188
I0520 18:50:15.850566  2537 solver.cpp:409]     Test net output #1: loss = 1.03977 (* 1 = 1.03977 loss)
I0520 18:50:45.621655  2537 solver.cpp:237] Iteration 2788, loss = 1.68198
I0520 18:50:45.621706  2537 solver.cpp:253]     Train net output #0: loss = 1.68198 (* 1 = 1.68198 loss)
I0520 18:50:45.621721  2537 sgd_solver.cpp:106] Iteration 2788, lr = 0.0025
I0520 18:50:53.893451  2537 solver.cpp:237] Iteration 2856, loss = 1.56286
I0520 18:50:53.893601  2537 solver.cpp:253]     Train net output #0: loss = 1.56286 (* 1 = 1.56286 loss)
I0520 18:50:53.893615  2537 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0520 18:51:02.165534  2537 solver.cpp:237] Iteration 2924, loss = 1.59932
I0520 18:51:02.165576  2537 solver.cpp:253]     Train net output #0: loss = 1.59932 (* 1 = 1.59932 loss)
I0520 18:51:02.165593  2537 sgd_solver.cpp:106] Iteration 2924, lr = 0.0025
I0520 18:51:10.435211  2537 solver.cpp:237] Iteration 2992, loss = 1.57842
I0520 18:51:10.435245  2537 solver.cpp:253]     Train net output #0: loss = 1.57842 (* 1 = 1.57842 loss)
I0520 18:51:10.435262  2537 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0520 18:51:18.707680  2537 solver.cpp:237] Iteration 3060, loss = 1.53853
I0520 18:51:18.707715  2537 solver.cpp:253]     Train net output #0: loss = 1.53853 (* 1 = 1.53853 loss)
I0520 18:51:18.707731  2537 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0520 18:51:26.983873  2537 solver.cpp:237] Iteration 3128, loss = 1.607
I0520 18:51:26.984030  2537 solver.cpp:253]     Train net output #0: loss = 1.607 (* 1 = 1.607 loss)
I0520 18:51:26.984045  2537 sgd_solver.cpp:106] Iteration 3128, lr = 0.0025
I0520 18:51:57.389184  2537 solver.cpp:237] Iteration 3196, loss = 1.48505
I0520 18:51:57.389353  2537 solver.cpp:253]     Train net output #0: loss = 1.48505 (* 1 = 1.48505 loss)
I0520 18:51:57.389369  2537 sgd_solver.cpp:106] Iteration 3196, lr = 0.0025
I0520 18:52:05.659989  2537 solver.cpp:237] Iteration 3264, loss = 1.52972
I0520 18:52:05.660023  2537 solver.cpp:253]     Train net output #0: loss = 1.52972 (* 1 = 1.52972 loss)
I0520 18:52:05.660040  2537 sgd_solver.cpp:106] Iteration 3264, lr = 0.0025
I0520 18:52:13.936888  2537 solver.cpp:237] Iteration 3332, loss = 1.52391
I0520 18:52:13.936929  2537 solver.cpp:253]     Train net output #0: loss = 1.52391 (* 1 = 1.52391 loss)
I0520 18:52:13.936944  2537 sgd_solver.cpp:106] Iteration 3332, lr = 0.0025
I0520 18:52:22.211680  2537 solver.cpp:237] Iteration 3400, loss = 1.49004
I0520 18:52:22.211714  2537 solver.cpp:253]     Train net output #0: loss = 1.49004 (* 1 = 1.49004 loss)
I0520 18:52:22.211731  2537 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0520 18:52:22.697777  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_3405.caffemodel
I0520 18:52:22.822938  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_3405.solverstate
I0520 18:52:30.552232  2537 solver.cpp:237] Iteration 3468, loss = 1.5133
I0520 18:52:30.552392  2537 solver.cpp:253]     Train net output #0: loss = 1.5133 (* 1 = 1.5133 loss)
I0520 18:52:30.552407  2537 sgd_solver.cpp:106] Iteration 3468, lr = 0.0025
I0520 18:52:38.822418  2537 solver.cpp:237] Iteration 3536, loss = 1.48315
I0520 18:52:38.822458  2537 solver.cpp:253]     Train net output #0: loss = 1.48315 (* 1 = 1.48315 loss)
I0520 18:52:38.822477  2537 sgd_solver.cpp:106] Iteration 3536, lr = 0.0025
I0520 18:52:47.096097  2537 solver.cpp:237] Iteration 3604, loss = 1.53132
I0520 18:52:47.096132  2537 solver.cpp:253]     Train net output #0: loss = 1.53132 (* 1 = 1.53132 loss)
I0520 18:52:47.096148  2537 sgd_solver.cpp:106] Iteration 3604, lr = 0.0025
I0520 18:53:17.528738  2537 solver.cpp:237] Iteration 3672, loss = 1.61018
I0520 18:53:17.528906  2537 solver.cpp:253]     Train net output #0: loss = 1.61018 (* 1 = 1.61018 loss)
I0520 18:53:17.528921  2537 sgd_solver.cpp:106] Iteration 3672, lr = 0.0025
I0520 18:53:25.803205  2537 solver.cpp:237] Iteration 3740, loss = 1.71973
I0520 18:53:25.803239  2537 solver.cpp:253]     Train net output #0: loss = 1.71973 (* 1 = 1.71973 loss)
I0520 18:53:25.803256  2537 sgd_solver.cpp:106] Iteration 3740, lr = 0.0025
I0520 18:53:34.078059  2537 solver.cpp:237] Iteration 3808, loss = 1.52998
I0520 18:53:34.078100  2537 solver.cpp:253]     Train net output #0: loss = 1.52998 (* 1 = 1.52998 loss)
I0520 18:53:34.078120  2537 sgd_solver.cpp:106] Iteration 3808, lr = 0.0025
I0520 18:53:42.353240  2537 solver.cpp:237] Iteration 3876, loss = 1.46947
I0520 18:53:42.353272  2537 solver.cpp:253]     Train net output #0: loss = 1.46947 (* 1 = 1.46947 loss)
I0520 18:53:42.353289  2537 sgd_solver.cpp:106] Iteration 3876, lr = 0.0025
I0520 18:53:50.625509  2537 solver.cpp:237] Iteration 3944, loss = 1.37502
I0520 18:53:50.625665  2537 solver.cpp:253]     Train net output #0: loss = 1.37502 (* 1 = 1.37502 loss)
I0520 18:53:50.625679  2537 sgd_solver.cpp:106] Iteration 3944, lr = 0.0025
I0520 18:53:58.901412  2537 solver.cpp:237] Iteration 4012, loss = 1.45622
I0520 18:53:58.901453  2537 solver.cpp:253]     Train net output #0: loss = 1.45622 (* 1 = 1.45622 loss)
I0520 18:53:58.901473  2537 sgd_solver.cpp:106] Iteration 4012, lr = 0.0025
I0520 18:54:07.180589  2537 solver.cpp:237] Iteration 4080, loss = 1.44041
I0520 18:54:07.180624  2537 solver.cpp:253]     Train net output #0: loss = 1.44041 (* 1 = 1.44041 loss)
I0520 18:54:07.180640  2537 sgd_solver.cpp:106] Iteration 4080, lr = 0.0025
I0520 18:54:07.789777  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_4086.caffemodel
I0520 18:54:07.912609  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_4086.solverstate
I0520 18:54:08.218232  2537 solver.cpp:341] Iteration 4089, Testing net (#0)
I0520 18:54:53.586789  2537 solver.cpp:409]     Test net output #0: accuracy = 0.770839
I0520 18:54:53.586948  2537 solver.cpp:409]     Test net output #1: loss = 0.844711 (* 1 = 0.844711 loss)
I0520 18:55:22.979986  2537 solver.cpp:237] Iteration 4148, loss = 1.45523
I0520 18:55:22.980036  2537 solver.cpp:253]     Train net output #0: loss = 1.45523 (* 1 = 1.45523 loss)
I0520 18:55:22.980052  2537 sgd_solver.cpp:106] Iteration 4148, lr = 0.0025
I0520 18:55:31.252499  2537 solver.cpp:237] Iteration 4216, loss = 1.55901
I0520 18:55:31.252656  2537 solver.cpp:253]     Train net output #0: loss = 1.55901 (* 1 = 1.55901 loss)
I0520 18:55:31.252671  2537 sgd_solver.cpp:106] Iteration 4216, lr = 0.0025
I0520 18:55:39.527321  2537 solver.cpp:237] Iteration 4284, loss = 1.53204
I0520 18:55:39.527359  2537 solver.cpp:253]     Train net output #0: loss = 1.53204 (* 1 = 1.53204 loss)
I0520 18:55:39.527379  2537 sgd_solver.cpp:106] Iteration 4284, lr = 0.0025
I0520 18:55:47.805255  2537 solver.cpp:237] Iteration 4352, loss = 1.38837
I0520 18:55:47.805289  2537 solver.cpp:253]     Train net output #0: loss = 1.38837 (* 1 = 1.38837 loss)
I0520 18:55:47.805305  2537 sgd_solver.cpp:106] Iteration 4352, lr = 0.0025
I0520 18:55:56.079628  2537 solver.cpp:237] Iteration 4420, loss = 1.29623
I0520 18:55:56.079661  2537 solver.cpp:253]     Train net output #0: loss = 1.29623 (* 1 = 1.29623 loss)
I0520 18:55:56.079679  2537 sgd_solver.cpp:106] Iteration 4420, lr = 0.0025
I0520 18:56:04.348541  2537 solver.cpp:237] Iteration 4488, loss = 1.39863
I0520 18:56:04.348706  2537 solver.cpp:253]     Train net output #0: loss = 1.39863 (* 1 = 1.39863 loss)
I0520 18:56:04.348721  2537 sgd_solver.cpp:106] Iteration 4488, lr = 0.0025
I0520 18:56:34.793606  2537 solver.cpp:237] Iteration 4556, loss = 1.49465
I0520 18:56:34.793767  2537 solver.cpp:253]     Train net output #0: loss = 1.49465 (* 1 = 1.49465 loss)
I0520 18:56:34.793782  2537 sgd_solver.cpp:106] Iteration 4556, lr = 0.0025
I0520 18:56:43.069793  2537 solver.cpp:237] Iteration 4624, loss = 1.47682
I0520 18:56:43.069826  2537 solver.cpp:253]     Train net output #0: loss = 1.47682 (* 1 = 1.47682 loss)
I0520 18:56:43.069844  2537 sgd_solver.cpp:106] Iteration 4624, lr = 0.0025
I0520 18:56:51.342881  2537 solver.cpp:237] Iteration 4692, loss = 1.4444
I0520 18:56:51.342921  2537 solver.cpp:253]     Train net output #0: loss = 1.4444 (* 1 = 1.4444 loss)
I0520 18:56:51.342942  2537 sgd_solver.cpp:106] Iteration 4692, lr = 0.0025
I0520 18:56:59.613762  2537 solver.cpp:237] Iteration 4760, loss = 1.32131
I0520 18:56:59.613797  2537 solver.cpp:253]     Train net output #0: loss = 1.32131 (* 1 = 1.32131 loss)
I0520 18:56:59.613814  2537 sgd_solver.cpp:106] Iteration 4760, lr = 0.0025
I0520 18:57:00.344372  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_4767.caffemodel
I0520 18:57:00.467252  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_4767.solverstate
I0520 18:57:07.948395  2537 solver.cpp:237] Iteration 4828, loss = 1.50243
I0520 18:57:07.948551  2537 solver.cpp:253]     Train net output #0: loss = 1.50243 (* 1 = 1.50243 loss)
I0520 18:57:07.948565  2537 sgd_solver.cpp:106] Iteration 4828, lr = 0.0025
I0520 18:57:16.222456  2537 solver.cpp:237] Iteration 4896, loss = 1.43794
I0520 18:57:16.222496  2537 solver.cpp:253]     Train net output #0: loss = 1.43794 (* 1 = 1.43794 loss)
I0520 18:57:16.222517  2537 sgd_solver.cpp:106] Iteration 4896, lr = 0.0025
I0520 18:57:24.500051  2537 solver.cpp:237] Iteration 4964, loss = 1.50062
I0520 18:57:24.500087  2537 solver.cpp:253]     Train net output #0: loss = 1.50062 (* 1 = 1.50062 loss)
I0520 18:57:24.500104  2537 sgd_solver.cpp:106] Iteration 4964, lr = 0.0025
I0520 18:57:55.006728  2537 solver.cpp:237] Iteration 5032, loss = 1.51531
I0520 18:57:55.006901  2537 solver.cpp:253]     Train net output #0: loss = 1.51531 (* 1 = 1.51531 loss)
I0520 18:57:55.006916  2537 sgd_solver.cpp:106] Iteration 5032, lr = 0.0025
I0520 18:58:03.278054  2537 solver.cpp:237] Iteration 5100, loss = 1.3396
I0520 18:58:03.278101  2537 solver.cpp:253]     Train net output #0: loss = 1.3396 (* 1 = 1.3396 loss)
I0520 18:58:03.278117  2537 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0520 18:58:11.554479  2537 solver.cpp:237] Iteration 5168, loss = 1.45876
I0520 18:58:11.554514  2537 solver.cpp:253]     Train net output #0: loss = 1.45876 (* 1 = 1.45876 loss)
I0520 18:58:11.554530  2537 sgd_solver.cpp:106] Iteration 5168, lr = 0.0025
I0520 18:58:19.825812  2537 solver.cpp:237] Iteration 5236, loss = 1.39011
I0520 18:58:19.825848  2537 solver.cpp:253]     Train net output #0: loss = 1.39011 (* 1 = 1.39011 loss)
I0520 18:58:19.825865  2537 sgd_solver.cpp:106] Iteration 5236, lr = 0.0025
I0520 18:58:28.100872  2537 solver.cpp:237] Iteration 5304, loss = 1.44852
I0520 18:58:28.101030  2537 solver.cpp:253]     Train net output #0: loss = 1.44852 (* 1 = 1.44852 loss)
I0520 18:58:28.101044  2537 sgd_solver.cpp:106] Iteration 5304, lr = 0.0025
I0520 18:58:36.375125  2537 solver.cpp:237] Iteration 5372, loss = 1.65456
I0520 18:58:36.375160  2537 solver.cpp:253]     Train net output #0: loss = 1.65456 (* 1 = 1.65456 loss)
I0520 18:58:36.375177  2537 sgd_solver.cpp:106] Iteration 5372, lr = 0.0025
I0520 18:58:44.646687  2537 solver.cpp:237] Iteration 5440, loss = 1.4354
I0520 18:58:44.646721  2537 solver.cpp:253]     Train net output #0: loss = 1.4354 (* 1 = 1.4354 loss)
I0520 18:58:44.646738  2537 sgd_solver.cpp:106] Iteration 5440, lr = 0.0025
I0520 18:58:45.499142  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_5448.caffemodel
I0520 18:58:45.622334  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_5448.solverstate
I0520 18:58:46.049903  2537 solver.cpp:341] Iteration 5452, Testing net (#0)
I0520 18:59:52.628240  2537 solver.cpp:409]     Test net output #0: accuracy = 0.793225
I0520 18:59:52.628415  2537 solver.cpp:409]     Test net output #1: loss = 0.758669 (* 1 = 0.758669 loss)
I0520 19:00:21.639950  2537 solver.cpp:237] Iteration 5508, loss = 1.47425
I0520 19:00:21.640000  2537 solver.cpp:253]     Train net output #0: loss = 1.47425 (* 1 = 1.47425 loss)
I0520 19:00:21.640015  2537 sgd_solver.cpp:106] Iteration 5508, lr = 0.0025
I0520 19:00:29.913275  2537 solver.cpp:237] Iteration 5576, loss = 1.41484
I0520 19:00:29.913422  2537 solver.cpp:253]     Train net output #0: loss = 1.41484 (* 1 = 1.41484 loss)
I0520 19:00:29.913436  2537 sgd_solver.cpp:106] Iteration 5576, lr = 0.0025
I0520 19:00:38.187145  2537 solver.cpp:237] Iteration 5644, loss = 1.35141
I0520 19:00:38.187191  2537 solver.cpp:253]     Train net output #0: loss = 1.35141 (* 1 = 1.35141 loss)
I0520 19:00:38.187208  2537 sgd_solver.cpp:106] Iteration 5644, lr = 0.0025
I0520 19:00:46.459167  2537 solver.cpp:237] Iteration 5712, loss = 1.52263
I0520 19:00:46.459200  2537 solver.cpp:253]     Train net output #0: loss = 1.52263 (* 1 = 1.52263 loss)
I0520 19:00:46.459216  2537 sgd_solver.cpp:106] Iteration 5712, lr = 0.0025
I0520 19:00:54.734033  2537 solver.cpp:237] Iteration 5780, loss = 1.47073
I0520 19:00:54.734067  2537 solver.cpp:253]     Train net output #0: loss = 1.47073 (* 1 = 1.47073 loss)
I0520 19:00:54.734086  2537 sgd_solver.cpp:106] Iteration 5780, lr = 0.0025
I0520 19:01:03.007704  2537 solver.cpp:237] Iteration 5848, loss = 1.51673
I0520 19:01:03.007858  2537 solver.cpp:253]     Train net output #0: loss = 1.51673 (* 1 = 1.51673 loss)
I0520 19:01:03.007872  2537 sgd_solver.cpp:106] Iteration 5848, lr = 0.0025
I0520 19:01:33.494096  2537 solver.cpp:237] Iteration 5916, loss = 1.33032
I0520 19:01:33.494264  2537 solver.cpp:253]     Train net output #0: loss = 1.33032 (* 1 = 1.33032 loss)
I0520 19:01:33.494279  2537 sgd_solver.cpp:106] Iteration 5916, lr = 0.0025
I0520 19:01:41.764113  2537 solver.cpp:237] Iteration 5984, loss = 1.36758
I0520 19:01:41.764147  2537 solver.cpp:253]     Train net output #0: loss = 1.36758 (* 1 = 1.36758 loss)
I0520 19:01:41.764164  2537 sgd_solver.cpp:106] Iteration 5984, lr = 0.0025
I0520 19:01:50.033268  2537 solver.cpp:237] Iteration 6052, loss = 1.49375
I0520 19:01:50.033310  2537 solver.cpp:253]     Train net output #0: loss = 1.49375 (* 1 = 1.49375 loss)
I0520 19:01:50.033329  2537 sgd_solver.cpp:106] Iteration 6052, lr = 0.0025
I0520 19:01:58.305328  2537 solver.cpp:237] Iteration 6120, loss = 1.33038
I0520 19:01:58.305363  2537 solver.cpp:253]     Train net output #0: loss = 1.33038 (* 1 = 1.33038 loss)
I0520 19:01:58.305379  2537 sgd_solver.cpp:106] Iteration 6120, lr = 0.0025
I0520 19:01:59.278486  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_6129.caffemodel
I0520 19:01:59.404129  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_6129.solverstate
I0520 19:02:06.649034  2537 solver.cpp:237] Iteration 6188, loss = 1.39871
I0520 19:02:06.649195  2537 solver.cpp:253]     Train net output #0: loss = 1.39871 (* 1 = 1.39871 loss)
I0520 19:02:06.649209  2537 sgd_solver.cpp:106] Iteration 6188, lr = 0.0025
I0520 19:02:14.926620  2537 solver.cpp:237] Iteration 6256, loss = 1.43998
I0520 19:02:14.926668  2537 solver.cpp:253]     Train net output #0: loss = 1.43998 (* 1 = 1.43998 loss)
I0520 19:02:14.926688  2537 sgd_solver.cpp:106] Iteration 6256, lr = 0.0025
I0520 19:02:23.203779  2537 solver.cpp:237] Iteration 6324, loss = 1.35408
I0520 19:02:23.203814  2537 solver.cpp:253]     Train net output #0: loss = 1.35408 (* 1 = 1.35408 loss)
I0520 19:02:23.203830  2537 sgd_solver.cpp:106] Iteration 6324, lr = 0.0025
I0520 19:02:53.692704  2537 solver.cpp:237] Iteration 6392, loss = 1.31922
I0520 19:02:53.692875  2537 solver.cpp:253]     Train net output #0: loss = 1.31922 (* 1 = 1.31922 loss)
I0520 19:02:53.692890  2537 sgd_solver.cpp:106] Iteration 6392, lr = 0.0025
I0520 19:03:01.960743  2537 solver.cpp:237] Iteration 6460, loss = 1.23424
I0520 19:03:01.960777  2537 solver.cpp:253]     Train net output #0: loss = 1.23424 (* 1 = 1.23424 loss)
I0520 19:03:01.960794  2537 sgd_solver.cpp:106] Iteration 6460, lr = 0.0025
I0520 19:03:10.232224  2537 solver.cpp:237] Iteration 6528, loss = 1.4059
I0520 19:03:10.232255  2537 solver.cpp:253]     Train net output #0: loss = 1.4059 (* 1 = 1.4059 loss)
I0520 19:03:10.232272  2537 sgd_solver.cpp:106] Iteration 6528, lr = 0.0025
I0520 19:03:18.509078  2537 solver.cpp:237] Iteration 6596, loss = 1.32053
I0520 19:03:18.509114  2537 solver.cpp:253]     Train net output #0: loss = 1.32053 (* 1 = 1.32053 loss)
I0520 19:03:18.509130  2537 sgd_solver.cpp:106] Iteration 6596, lr = 0.0025
I0520 19:03:26.784437  2537 solver.cpp:237] Iteration 6664, loss = 1.46714
I0520 19:03:26.784580  2537 solver.cpp:253]     Train net output #0: loss = 1.46714 (* 1 = 1.46714 loss)
I0520 19:03:26.784595  2537 sgd_solver.cpp:106] Iteration 6664, lr = 0.0025
I0520 19:03:35.055403  2537 solver.cpp:237] Iteration 6732, loss = 1.41682
I0520 19:03:35.055440  2537 solver.cpp:253]     Train net output #0: loss = 1.41682 (* 1 = 1.41682 loss)
I0520 19:03:35.055460  2537 sgd_solver.cpp:106] Iteration 6732, lr = 0.0025
I0520 19:03:43.325429  2537 solver.cpp:237] Iteration 6800, loss = 1.34371
I0520 19:03:43.325464  2537 solver.cpp:253]     Train net output #0: loss = 1.34371 (* 1 = 1.34371 loss)
I0520 19:03:43.325479  2537 sgd_solver.cpp:106] Iteration 6800, lr = 0.0025
I0520 19:03:44.420562  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_6810.caffemodel
I0520 19:03:44.545615  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_6810.solverstate
I0520 19:03:45.097838  2537 solver.cpp:341] Iteration 6815, Testing net (#0)
I0520 19:04:30.776604  2537 solver.cpp:409]     Test net output #0: accuracy = 0.803577
I0520 19:04:30.776764  2537 solver.cpp:409]     Test net output #1: loss = 0.675366 (* 1 = 0.675366 loss)
I0520 19:04:31.056881  2537 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_6818.caffemodel
I0520 19:04:31.182464  2537 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743_iter_6818.solverstate
I0520 19:04:31.210806  2537 solver.cpp:326] Optimization Done.
I0520 19:04:31.210834  2537 caffe.cpp:215] Optimization Done.
Application 11234509 resources: utime ~1275s, stime ~228s, Rss ~5330092, inblocks ~3594474, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_220_2016-05-20T11.20.40.815743.solver"
	User time (seconds): 0.53
	System time (seconds): 0.18
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:06.04
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15082
	Voluntary context switches: 2789
	Involuntary context switches: 80
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
