2805005
I0520 11:53:53.166517 23075 caffe.cpp:184] Using GPUs 0
I0520 11:53:53.599331 23075 solver.cpp:48] Initializing solver from parameters: 
test_iter: 535
test_interval: 1071
base_lr: 0.0025
display: 53
max_iter: 5357
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 535
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096.prototxt"
I0520 11:53:53.601166 23075 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096.prototxt
I0520 11:53:53.604756 23075 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 11:53:53.604823 23075 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 11:53:53.605203 23075 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 280
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 11:53:53.605412 23075 layer_factory.hpp:77] Creating layer data_hdf5
I0520 11:53:53.605448 23075 net.cpp:106] Creating Layer data_hdf5
I0520 11:53:53.605466 23075 net.cpp:411] data_hdf5 -> data
I0520 11:53:53.605500 23075 net.cpp:411] data_hdf5 -> label
I0520 11:53:53.605543 23075 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 11:53:53.606887 23075 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 11:53:53.609177 23075 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 11:54:15.160588 23075 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 11:54:15.165778 23075 net.cpp:150] Setting up data_hdf5
I0520 11:54:15.165818 23075 net.cpp:157] Top shape: 280 1 127 50 (1778000)
I0520 11:54:15.165837 23075 net.cpp:157] Top shape: 280 (280)
I0520 11:54:15.165849 23075 net.cpp:165] Memory required for data: 7113120
I0520 11:54:15.165868 23075 layer_factory.hpp:77] Creating layer conv1
I0520 11:54:15.165915 23075 net.cpp:106] Creating Layer conv1
I0520 11:54:15.165928 23075 net.cpp:454] conv1 <- data
I0520 11:54:15.165953 23075 net.cpp:411] conv1 -> conv1
I0520 11:54:15.540383 23075 net.cpp:150] Setting up conv1
I0520 11:54:15.540437 23075 net.cpp:157] Top shape: 280 12 120 48 (19353600)
I0520 11:54:15.540460 23075 net.cpp:165] Memory required for data: 84527520
I0520 11:54:15.540490 23075 layer_factory.hpp:77] Creating layer relu1
I0520 11:54:15.540513 23075 net.cpp:106] Creating Layer relu1
I0520 11:54:15.540534 23075 net.cpp:454] relu1 <- conv1
I0520 11:54:15.540570 23075 net.cpp:397] relu1 -> conv1 (in-place)
I0520 11:54:15.541100 23075 net.cpp:150] Setting up relu1
I0520 11:54:15.541124 23075 net.cpp:157] Top shape: 280 12 120 48 (19353600)
I0520 11:54:15.541137 23075 net.cpp:165] Memory required for data: 161941920
I0520 11:54:15.541153 23075 layer_factory.hpp:77] Creating layer pool1
I0520 11:54:15.541180 23075 net.cpp:106] Creating Layer pool1
I0520 11:54:15.541193 23075 net.cpp:454] pool1 <- conv1
I0520 11:54:15.541209 23075 net.cpp:411] pool1 -> pool1
I0520 11:54:15.541302 23075 net.cpp:150] Setting up pool1
I0520 11:54:15.541321 23075 net.cpp:157] Top shape: 280 12 60 48 (9676800)
I0520 11:54:15.541335 23075 net.cpp:165] Memory required for data: 200649120
I0520 11:54:15.541354 23075 layer_factory.hpp:77] Creating layer conv2
I0520 11:54:15.541378 23075 net.cpp:106] Creating Layer conv2
I0520 11:54:15.541399 23075 net.cpp:454] conv2 <- pool1
I0520 11:54:15.541414 23075 net.cpp:411] conv2 -> conv2
I0520 11:54:15.544137 23075 net.cpp:150] Setting up conv2
I0520 11:54:15.544170 23075 net.cpp:157] Top shape: 280 20 54 46 (13910400)
I0520 11:54:15.544184 23075 net.cpp:165] Memory required for data: 256290720
I0520 11:54:15.544211 23075 layer_factory.hpp:77] Creating layer relu2
I0520 11:54:15.544239 23075 net.cpp:106] Creating Layer relu2
I0520 11:54:15.544253 23075 net.cpp:454] relu2 <- conv2
I0520 11:54:15.544270 23075 net.cpp:397] relu2 -> conv2 (in-place)
I0520 11:54:15.544626 23075 net.cpp:150] Setting up relu2
I0520 11:54:15.544647 23075 net.cpp:157] Top shape: 280 20 54 46 (13910400)
I0520 11:54:15.544661 23075 net.cpp:165] Memory required for data: 311932320
I0520 11:54:15.544672 23075 layer_factory.hpp:77] Creating layer pool2
I0520 11:54:15.544698 23075 net.cpp:106] Creating Layer pool2
I0520 11:54:15.544711 23075 net.cpp:454] pool2 <- conv2
I0520 11:54:15.544747 23075 net.cpp:411] pool2 -> pool2
I0520 11:54:15.544831 23075 net.cpp:150] Setting up pool2
I0520 11:54:15.544848 23075 net.cpp:157] Top shape: 280 20 27 46 (6955200)
I0520 11:54:15.544863 23075 net.cpp:165] Memory required for data: 339753120
I0520 11:54:15.544881 23075 layer_factory.hpp:77] Creating layer conv3
I0520 11:54:15.544903 23075 net.cpp:106] Creating Layer conv3
I0520 11:54:15.544916 23075 net.cpp:454] conv3 <- pool2
I0520 11:54:15.544932 23075 net.cpp:411] conv3 -> conv3
I0520 11:54:15.546898 23075 net.cpp:150] Setting up conv3
I0520 11:54:15.546923 23075 net.cpp:157] Top shape: 280 28 22 44 (7589120)
I0520 11:54:15.546943 23075 net.cpp:165] Memory required for data: 370109600
I0520 11:54:15.546965 23075 layer_factory.hpp:77] Creating layer relu3
I0520 11:54:15.546988 23075 net.cpp:106] Creating Layer relu3
I0520 11:54:15.547009 23075 net.cpp:454] relu3 <- conv3
I0520 11:54:15.547025 23075 net.cpp:397] relu3 -> conv3 (in-place)
I0520 11:54:15.547520 23075 net.cpp:150] Setting up relu3
I0520 11:54:15.547545 23075 net.cpp:157] Top shape: 280 28 22 44 (7589120)
I0520 11:54:15.547559 23075 net.cpp:165] Memory required for data: 400466080
I0520 11:54:15.547574 23075 layer_factory.hpp:77] Creating layer pool3
I0520 11:54:15.547590 23075 net.cpp:106] Creating Layer pool3
I0520 11:54:15.547611 23075 net.cpp:454] pool3 <- conv3
I0520 11:54:15.547627 23075 net.cpp:411] pool3 -> pool3
I0520 11:54:15.547709 23075 net.cpp:150] Setting up pool3
I0520 11:54:15.547732 23075 net.cpp:157] Top shape: 280 28 11 44 (3794560)
I0520 11:54:15.547745 23075 net.cpp:165] Memory required for data: 415644320
I0520 11:54:15.547757 23075 layer_factory.hpp:77] Creating layer conv4
I0520 11:54:15.547780 23075 net.cpp:106] Creating Layer conv4
I0520 11:54:15.547799 23075 net.cpp:454] conv4 <- pool3
I0520 11:54:15.547816 23075 net.cpp:411] conv4 -> conv4
I0520 11:54:15.550559 23075 net.cpp:150] Setting up conv4
I0520 11:54:15.550591 23075 net.cpp:157] Top shape: 280 36 6 42 (2540160)
I0520 11:54:15.550606 23075 net.cpp:165] Memory required for data: 425804960
I0520 11:54:15.550626 23075 layer_factory.hpp:77] Creating layer relu4
I0520 11:54:15.550647 23075 net.cpp:106] Creating Layer relu4
I0520 11:54:15.550673 23075 net.cpp:454] relu4 <- conv4
I0520 11:54:15.550688 23075 net.cpp:397] relu4 -> conv4 (in-place)
I0520 11:54:15.551179 23075 net.cpp:150] Setting up relu4
I0520 11:54:15.551203 23075 net.cpp:157] Top shape: 280 36 6 42 (2540160)
I0520 11:54:15.551232 23075 net.cpp:165] Memory required for data: 435965600
I0520 11:54:15.551247 23075 layer_factory.hpp:77] Creating layer pool4
I0520 11:54:15.551272 23075 net.cpp:106] Creating Layer pool4
I0520 11:54:15.551286 23075 net.cpp:454] pool4 <- conv4
I0520 11:54:15.551302 23075 net.cpp:411] pool4 -> pool4
I0520 11:54:15.551384 23075 net.cpp:150] Setting up pool4
I0520 11:54:15.551405 23075 net.cpp:157] Top shape: 280 36 3 42 (1270080)
I0520 11:54:15.551419 23075 net.cpp:165] Memory required for data: 441045920
I0520 11:54:15.551437 23075 layer_factory.hpp:77] Creating layer ip1
I0520 11:54:15.551460 23075 net.cpp:106] Creating Layer ip1
I0520 11:54:15.551472 23075 net.cpp:454] ip1 <- pool4
I0520 11:54:15.551487 23075 net.cpp:411] ip1 -> ip1
I0520 11:54:15.566906 23075 net.cpp:150] Setting up ip1
I0520 11:54:15.566943 23075 net.cpp:157] Top shape: 280 196 (54880)
I0520 11:54:15.566958 23075 net.cpp:165] Memory required for data: 441265440
I0520 11:54:15.566988 23075 layer_factory.hpp:77] Creating layer relu5
I0520 11:54:15.567018 23075 net.cpp:106] Creating Layer relu5
I0520 11:54:15.567033 23075 net.cpp:454] relu5 <- ip1
I0520 11:54:15.567047 23075 net.cpp:397] relu5 -> ip1 (in-place)
I0520 11:54:15.567425 23075 net.cpp:150] Setting up relu5
I0520 11:54:15.567445 23075 net.cpp:157] Top shape: 280 196 (54880)
I0520 11:54:15.567459 23075 net.cpp:165] Memory required for data: 441484960
I0520 11:54:15.567471 23075 layer_factory.hpp:77] Creating layer drop1
I0520 11:54:15.567507 23075 net.cpp:106] Creating Layer drop1
I0520 11:54:15.567520 23075 net.cpp:454] drop1 <- ip1
I0520 11:54:15.567549 23075 net.cpp:397] drop1 -> ip1 (in-place)
I0520 11:54:15.567608 23075 net.cpp:150] Setting up drop1
I0520 11:54:15.567625 23075 net.cpp:157] Top shape: 280 196 (54880)
I0520 11:54:15.567638 23075 net.cpp:165] Memory required for data: 441704480
I0520 11:54:15.567653 23075 layer_factory.hpp:77] Creating layer ip2
I0520 11:54:15.567674 23075 net.cpp:106] Creating Layer ip2
I0520 11:54:15.567687 23075 net.cpp:454] ip2 <- ip1
I0520 11:54:15.567703 23075 net.cpp:411] ip2 -> ip2
I0520 11:54:15.568195 23075 net.cpp:150] Setting up ip2
I0520 11:54:15.568215 23075 net.cpp:157] Top shape: 280 98 (27440)
I0520 11:54:15.568228 23075 net.cpp:165] Memory required for data: 441814240
I0520 11:54:15.568248 23075 layer_factory.hpp:77] Creating layer relu6
I0520 11:54:15.568270 23075 net.cpp:106] Creating Layer relu6
I0520 11:54:15.568284 23075 net.cpp:454] relu6 <- ip2
I0520 11:54:15.568300 23075 net.cpp:397] relu6 -> ip2 (in-place)
I0520 11:54:15.568845 23075 net.cpp:150] Setting up relu6
I0520 11:54:15.568868 23075 net.cpp:157] Top shape: 280 98 (27440)
I0520 11:54:15.568882 23075 net.cpp:165] Memory required for data: 441924000
I0520 11:54:15.568898 23075 layer_factory.hpp:77] Creating layer drop2
I0520 11:54:15.568922 23075 net.cpp:106] Creating Layer drop2
I0520 11:54:15.568935 23075 net.cpp:454] drop2 <- ip2
I0520 11:54:15.568950 23075 net.cpp:397] drop2 -> ip2 (in-place)
I0520 11:54:15.569000 23075 net.cpp:150] Setting up drop2
I0520 11:54:15.569022 23075 net.cpp:157] Top shape: 280 98 (27440)
I0520 11:54:15.569036 23075 net.cpp:165] Memory required for data: 442033760
I0520 11:54:15.569048 23075 layer_factory.hpp:77] Creating layer ip3
I0520 11:54:15.569064 23075 net.cpp:106] Creating Layer ip3
I0520 11:54:15.569078 23075 net.cpp:454] ip3 <- ip2
I0520 11:54:15.569100 23075 net.cpp:411] ip3 -> ip3
I0520 11:54:15.569325 23075 net.cpp:150] Setting up ip3
I0520 11:54:15.569345 23075 net.cpp:157] Top shape: 280 11 (3080)
I0520 11:54:15.569357 23075 net.cpp:165] Memory required for data: 442046080
I0520 11:54:15.569378 23075 layer_factory.hpp:77] Creating layer drop3
I0520 11:54:15.569399 23075 net.cpp:106] Creating Layer drop3
I0520 11:54:15.569412 23075 net.cpp:454] drop3 <- ip3
I0520 11:54:15.569428 23075 net.cpp:397] drop3 -> ip3 (in-place)
I0520 11:54:15.569476 23075 net.cpp:150] Setting up drop3
I0520 11:54:15.569499 23075 net.cpp:157] Top shape: 280 11 (3080)
I0520 11:54:15.569511 23075 net.cpp:165] Memory required for data: 442058400
I0520 11:54:15.569532 23075 layer_factory.hpp:77] Creating layer loss
I0520 11:54:15.569555 23075 net.cpp:106] Creating Layer loss
I0520 11:54:15.569569 23075 net.cpp:454] loss <- ip3
I0520 11:54:15.569582 23075 net.cpp:454] loss <- label
I0520 11:54:15.569605 23075 net.cpp:411] loss -> loss
I0520 11:54:15.569624 23075 layer_factory.hpp:77] Creating layer loss
I0520 11:54:15.570293 23075 net.cpp:150] Setting up loss
I0520 11:54:15.570314 23075 net.cpp:157] Top shape: (1)
I0520 11:54:15.570336 23075 net.cpp:160]     with loss weight 1
I0520 11:54:15.570394 23075 net.cpp:165] Memory required for data: 442058404
I0520 11:54:15.570408 23075 net.cpp:226] loss needs backward computation.
I0520 11:54:15.570422 23075 net.cpp:226] drop3 needs backward computation.
I0520 11:54:15.570436 23075 net.cpp:226] ip3 needs backward computation.
I0520 11:54:15.570448 23075 net.cpp:226] drop2 needs backward computation.
I0520 11:54:15.570461 23075 net.cpp:226] relu6 needs backward computation.
I0520 11:54:15.570474 23075 net.cpp:226] ip2 needs backward computation.
I0520 11:54:15.570494 23075 net.cpp:226] drop1 needs backward computation.
I0520 11:54:15.570508 23075 net.cpp:226] relu5 needs backward computation.
I0520 11:54:15.570519 23075 net.cpp:226] ip1 needs backward computation.
I0520 11:54:15.570536 23075 net.cpp:226] pool4 needs backward computation.
I0520 11:54:15.570550 23075 net.cpp:226] relu4 needs backward computation.
I0520 11:54:15.570562 23075 net.cpp:226] conv4 needs backward computation.
I0520 11:54:15.570577 23075 net.cpp:226] pool3 needs backward computation.
I0520 11:54:15.570607 23075 net.cpp:226] relu3 needs backward computation.
I0520 11:54:15.570621 23075 net.cpp:226] conv3 needs backward computation.
I0520 11:54:15.570633 23075 net.cpp:226] pool2 needs backward computation.
I0520 11:54:15.570647 23075 net.cpp:226] relu2 needs backward computation.
I0520 11:54:15.570659 23075 net.cpp:226] conv2 needs backward computation.
I0520 11:54:15.570672 23075 net.cpp:226] pool1 needs backward computation.
I0520 11:54:15.570688 23075 net.cpp:226] relu1 needs backward computation.
I0520 11:54:15.570708 23075 net.cpp:226] conv1 needs backward computation.
I0520 11:54:15.570722 23075 net.cpp:228] data_hdf5 does not need backward computation.
I0520 11:54:15.570739 23075 net.cpp:270] This network produces output loss
I0520 11:54:15.570766 23075 net.cpp:283] Network initialization done.
I0520 11:54:15.572464 23075 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096.prototxt
I0520 11:54:15.572543 23075 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 11:54:15.572924 23075 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 280
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 11:54:15.573146 23075 layer_factory.hpp:77] Creating layer data_hdf5
I0520 11:54:15.573166 23075 net.cpp:106] Creating Layer data_hdf5
I0520 11:54:15.573184 23075 net.cpp:411] data_hdf5 -> data
I0520 11:54:15.573204 23075 net.cpp:411] data_hdf5 -> label
I0520 11:54:15.573225 23075 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 11:54:15.574640 23075 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 11:54:36.884445 23075 net.cpp:150] Setting up data_hdf5
I0520 11:54:36.884616 23075 net.cpp:157] Top shape: 280 1 127 50 (1778000)
I0520 11:54:36.884634 23075 net.cpp:157] Top shape: 280 (280)
I0520 11:54:36.884647 23075 net.cpp:165] Memory required for data: 7113120
I0520 11:54:36.884662 23075 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 11:54:36.884696 23075 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 11:54:36.884728 23075 net.cpp:454] label_data_hdf5_1_split <- label
I0520 11:54:36.884747 23075 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 11:54:36.884769 23075 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 11:54:36.884855 23075 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 11:54:36.884873 23075 net.cpp:157] Top shape: 280 (280)
I0520 11:54:36.884888 23075 net.cpp:157] Top shape: 280 (280)
I0520 11:54:36.884912 23075 net.cpp:165] Memory required for data: 7115360
I0520 11:54:36.884924 23075 layer_factory.hpp:77] Creating layer conv1
I0520 11:54:36.884950 23075 net.cpp:106] Creating Layer conv1
I0520 11:54:36.884969 23075 net.cpp:454] conv1 <- data
I0520 11:54:36.884987 23075 net.cpp:411] conv1 -> conv1
I0520 11:54:36.886934 23075 net.cpp:150] Setting up conv1
I0520 11:54:36.886960 23075 net.cpp:157] Top shape: 280 12 120 48 (19353600)
I0520 11:54:36.886981 23075 net.cpp:165] Memory required for data: 84529760
I0520 11:54:36.887003 23075 layer_factory.hpp:77] Creating layer relu1
I0520 11:54:36.887025 23075 net.cpp:106] Creating Layer relu1
I0520 11:54:36.887048 23075 net.cpp:454] relu1 <- conv1
I0520 11:54:36.887063 23075 net.cpp:397] relu1 -> conv1 (in-place)
I0520 11:54:36.887588 23075 net.cpp:150] Setting up relu1
I0520 11:54:36.887611 23075 net.cpp:157] Top shape: 280 12 120 48 (19353600)
I0520 11:54:36.887624 23075 net.cpp:165] Memory required for data: 161944160
I0520 11:54:36.887640 23075 layer_factory.hpp:77] Creating layer pool1
I0520 11:54:36.887666 23075 net.cpp:106] Creating Layer pool1
I0520 11:54:36.887679 23075 net.cpp:454] pool1 <- conv1
I0520 11:54:36.887696 23075 net.cpp:411] pool1 -> pool1
I0520 11:54:36.887784 23075 net.cpp:150] Setting up pool1
I0520 11:54:36.887800 23075 net.cpp:157] Top shape: 280 12 60 48 (9676800)
I0520 11:54:36.887815 23075 net.cpp:165] Memory required for data: 200651360
I0520 11:54:36.887833 23075 layer_factory.hpp:77] Creating layer conv2
I0520 11:54:36.887855 23075 net.cpp:106] Creating Layer conv2
I0520 11:54:36.887867 23075 net.cpp:454] conv2 <- pool1
I0520 11:54:36.887884 23075 net.cpp:411] conv2 -> conv2
I0520 11:54:36.889825 23075 net.cpp:150] Setting up conv2
I0520 11:54:36.889853 23075 net.cpp:157] Top shape: 280 20 54 46 (13910400)
I0520 11:54:36.889868 23075 net.cpp:165] Memory required for data: 256292960
I0520 11:54:36.889892 23075 layer_factory.hpp:77] Creating layer relu2
I0520 11:54:36.889919 23075 net.cpp:106] Creating Layer relu2
I0520 11:54:36.889931 23075 net.cpp:454] relu2 <- conv2
I0520 11:54:36.889948 23075 net.cpp:397] relu2 -> conv2 (in-place)
I0520 11:54:36.890306 23075 net.cpp:150] Setting up relu2
I0520 11:54:36.890326 23075 net.cpp:157] Top shape: 280 20 54 46 (13910400)
I0520 11:54:36.890339 23075 net.cpp:165] Memory required for data: 311934560
I0520 11:54:36.890354 23075 layer_factory.hpp:77] Creating layer pool2
I0520 11:54:36.890377 23075 net.cpp:106] Creating Layer pool2
I0520 11:54:36.890390 23075 net.cpp:454] pool2 <- conv2
I0520 11:54:36.890406 23075 net.cpp:411] pool2 -> pool2
I0520 11:54:36.890498 23075 net.cpp:150] Setting up pool2
I0520 11:54:36.890516 23075 net.cpp:157] Top shape: 280 20 27 46 (6955200)
I0520 11:54:36.890528 23075 net.cpp:165] Memory required for data: 339755360
I0520 11:54:36.890543 23075 layer_factory.hpp:77] Creating layer conv3
I0520 11:54:36.890570 23075 net.cpp:106] Creating Layer conv3
I0520 11:54:36.890584 23075 net.cpp:454] conv3 <- pool2
I0520 11:54:36.890601 23075 net.cpp:411] conv3 -> conv3
I0520 11:54:36.892613 23075 net.cpp:150] Setting up conv3
I0520 11:54:36.892638 23075 net.cpp:157] Top shape: 280 28 22 44 (7589120)
I0520 11:54:36.892658 23075 net.cpp:165] Memory required for data: 370111840
I0520 11:54:36.892698 23075 layer_factory.hpp:77] Creating layer relu3
I0520 11:54:36.892722 23075 net.cpp:106] Creating Layer relu3
I0520 11:54:36.892736 23075 net.cpp:454] relu3 <- conv3
I0520 11:54:36.892753 23075 net.cpp:397] relu3 -> conv3 (in-place)
I0520 11:54:36.893249 23075 net.cpp:150] Setting up relu3
I0520 11:54:36.893271 23075 net.cpp:157] Top shape: 280 28 22 44 (7589120)
I0520 11:54:36.893285 23075 net.cpp:165] Memory required for data: 400468320
I0520 11:54:36.893301 23075 layer_factory.hpp:77] Creating layer pool3
I0520 11:54:36.893324 23075 net.cpp:106] Creating Layer pool3
I0520 11:54:36.893338 23075 net.cpp:454] pool3 <- conv3
I0520 11:54:36.893353 23075 net.cpp:411] pool3 -> pool3
I0520 11:54:36.893440 23075 net.cpp:150] Setting up pool3
I0520 11:54:36.893456 23075 net.cpp:157] Top shape: 280 28 11 44 (3794560)
I0520 11:54:36.893471 23075 net.cpp:165] Memory required for data: 415646560
I0520 11:54:36.893483 23075 layer_factory.hpp:77] Creating layer conv4
I0520 11:54:36.893510 23075 net.cpp:106] Creating Layer conv4
I0520 11:54:36.893523 23075 net.cpp:454] conv4 <- pool3
I0520 11:54:36.893540 23075 net.cpp:411] conv4 -> conv4
I0520 11:54:36.895632 23075 net.cpp:150] Setting up conv4
I0520 11:54:36.895655 23075 net.cpp:157] Top shape: 280 36 6 42 (2540160)
I0520 11:54:36.895669 23075 net.cpp:165] Memory required for data: 425807200
I0520 11:54:36.895691 23075 layer_factory.hpp:77] Creating layer relu4
I0520 11:54:36.895715 23075 net.cpp:106] Creating Layer relu4
I0520 11:54:36.895730 23075 net.cpp:454] relu4 <- conv4
I0520 11:54:36.895746 23075 net.cpp:397] relu4 -> conv4 (in-place)
I0520 11:54:36.896245 23075 net.cpp:150] Setting up relu4
I0520 11:54:36.896267 23075 net.cpp:157] Top shape: 280 36 6 42 (2540160)
I0520 11:54:36.896281 23075 net.cpp:165] Memory required for data: 435967840
I0520 11:54:36.896292 23075 layer_factory.hpp:77] Creating layer pool4
I0520 11:54:36.896312 23075 net.cpp:106] Creating Layer pool4
I0520 11:54:36.896333 23075 net.cpp:454] pool4 <- conv4
I0520 11:54:36.896350 23075 net.cpp:411] pool4 -> pool4
I0520 11:54:36.896435 23075 net.cpp:150] Setting up pool4
I0520 11:54:36.896457 23075 net.cpp:157] Top shape: 280 36 3 42 (1270080)
I0520 11:54:36.896471 23075 net.cpp:165] Memory required for data: 441048160
I0520 11:54:36.896484 23075 layer_factory.hpp:77] Creating layer ip1
I0520 11:54:36.896502 23075 net.cpp:106] Creating Layer ip1
I0520 11:54:36.896522 23075 net.cpp:454] ip1 <- pool4
I0520 11:54:36.896538 23075 net.cpp:411] ip1 -> ip1
I0520 11:54:36.912052 23075 net.cpp:150] Setting up ip1
I0520 11:54:36.912082 23075 net.cpp:157] Top shape: 280 196 (54880)
I0520 11:54:36.912103 23075 net.cpp:165] Memory required for data: 441267680
I0520 11:54:36.912129 23075 layer_factory.hpp:77] Creating layer relu5
I0520 11:54:36.912152 23075 net.cpp:106] Creating Layer relu5
I0520 11:54:36.912176 23075 net.cpp:454] relu5 <- ip1
I0520 11:54:36.912194 23075 net.cpp:397] relu5 -> ip1 (in-place)
I0520 11:54:36.912555 23075 net.cpp:150] Setting up relu5
I0520 11:54:36.912576 23075 net.cpp:157] Top shape: 280 196 (54880)
I0520 11:54:36.912590 23075 net.cpp:165] Memory required for data: 441487200
I0520 11:54:36.912605 23075 layer_factory.hpp:77] Creating layer drop1
I0520 11:54:36.912632 23075 net.cpp:106] Creating Layer drop1
I0520 11:54:36.912647 23075 net.cpp:454] drop1 <- ip1
I0520 11:54:36.912663 23075 net.cpp:397] drop1 -> ip1 (in-place)
I0520 11:54:36.912722 23075 net.cpp:150] Setting up drop1
I0520 11:54:36.912739 23075 net.cpp:157] Top shape: 280 196 (54880)
I0520 11:54:36.912751 23075 net.cpp:165] Memory required for data: 441706720
I0520 11:54:36.912765 23075 layer_factory.hpp:77] Creating layer ip2
I0520 11:54:36.912783 23075 net.cpp:106] Creating Layer ip2
I0520 11:54:36.912796 23075 net.cpp:454] ip2 <- ip1
I0520 11:54:36.912817 23075 net.cpp:411] ip2 -> ip2
I0520 11:54:36.913311 23075 net.cpp:150] Setting up ip2
I0520 11:54:36.913331 23075 net.cpp:157] Top shape: 280 98 (27440)
I0520 11:54:36.913343 23075 net.cpp:165] Memory required for data: 441816480
I0520 11:54:36.913383 23075 layer_factory.hpp:77] Creating layer relu6
I0520 11:54:36.913399 23075 net.cpp:106] Creating Layer relu6
I0520 11:54:36.913411 23075 net.cpp:454] relu6 <- ip2
I0520 11:54:36.913429 23075 net.cpp:397] relu6 -> ip2 (in-place)
I0520 11:54:36.913987 23075 net.cpp:150] Setting up relu6
I0520 11:54:36.914011 23075 net.cpp:157] Top shape: 280 98 (27440)
I0520 11:54:36.914024 23075 net.cpp:165] Memory required for data: 441926240
I0520 11:54:36.914036 23075 layer_factory.hpp:77] Creating layer drop2
I0520 11:54:36.914055 23075 net.cpp:106] Creating Layer drop2
I0520 11:54:36.914077 23075 net.cpp:454] drop2 <- ip2
I0520 11:54:36.914093 23075 net.cpp:397] drop2 -> ip2 (in-place)
I0520 11:54:36.914146 23075 net.cpp:150] Setting up drop2
I0520 11:54:36.914170 23075 net.cpp:157] Top shape: 280 98 (27440)
I0520 11:54:36.914182 23075 net.cpp:165] Memory required for data: 442036000
I0520 11:54:36.914201 23075 layer_factory.hpp:77] Creating layer ip3
I0520 11:54:36.914217 23075 net.cpp:106] Creating Layer ip3
I0520 11:54:36.914229 23075 net.cpp:454] ip3 <- ip2
I0520 11:54:36.914248 23075 net.cpp:411] ip3 -> ip3
I0520 11:54:36.914494 23075 net.cpp:150] Setting up ip3
I0520 11:54:36.914512 23075 net.cpp:157] Top shape: 280 11 (3080)
I0520 11:54:36.914525 23075 net.cpp:165] Memory required for data: 442048320
I0520 11:54:36.914546 23075 layer_factory.hpp:77] Creating layer drop3
I0520 11:54:36.914568 23075 net.cpp:106] Creating Layer drop3
I0520 11:54:36.914582 23075 net.cpp:454] drop3 <- ip3
I0520 11:54:36.914597 23075 net.cpp:397] drop3 -> ip3 (in-place)
I0520 11:54:36.914645 23075 net.cpp:150] Setting up drop3
I0520 11:54:36.914667 23075 net.cpp:157] Top shape: 280 11 (3080)
I0520 11:54:36.914680 23075 net.cpp:165] Memory required for data: 442060640
I0520 11:54:36.914697 23075 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 11:54:36.914713 23075 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 11:54:36.914726 23075 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 11:54:36.914743 23075 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 11:54:36.914768 23075 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 11:54:36.914855 23075 net.cpp:150] Setting up ip3_drop3_0_split
I0520 11:54:36.914871 23075 net.cpp:157] Top shape: 280 11 (3080)
I0520 11:54:36.914887 23075 net.cpp:157] Top shape: 280 11 (3080)
I0520 11:54:36.914898 23075 net.cpp:165] Memory required for data: 442085280
I0520 11:54:36.914913 23075 layer_factory.hpp:77] Creating layer accuracy
I0520 11:54:36.914943 23075 net.cpp:106] Creating Layer accuracy
I0520 11:54:36.914957 23075 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 11:54:36.914970 23075 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 11:54:36.914986 23075 net.cpp:411] accuracy -> accuracy
I0520 11:54:36.915014 23075 net.cpp:150] Setting up accuracy
I0520 11:54:36.915036 23075 net.cpp:157] Top shape: (1)
I0520 11:54:36.915048 23075 net.cpp:165] Memory required for data: 442085284
I0520 11:54:36.915060 23075 layer_factory.hpp:77] Creating layer loss
I0520 11:54:36.915076 23075 net.cpp:106] Creating Layer loss
I0520 11:54:36.915091 23075 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 11:54:36.915105 23075 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 11:54:36.915128 23075 net.cpp:411] loss -> loss
I0520 11:54:36.915153 23075 layer_factory.hpp:77] Creating layer loss
I0520 11:54:36.915678 23075 net.cpp:150] Setting up loss
I0520 11:54:36.915698 23075 net.cpp:157] Top shape: (1)
I0520 11:54:36.915710 23075 net.cpp:160]     with loss weight 1
I0520 11:54:36.915734 23075 net.cpp:165] Memory required for data: 442085288
I0520 11:54:36.915752 23075 net.cpp:226] loss needs backward computation.
I0520 11:54:36.915766 23075 net.cpp:228] accuracy does not need backward computation.
I0520 11:54:36.915781 23075 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 11:54:36.915794 23075 net.cpp:226] drop3 needs backward computation.
I0520 11:54:36.915807 23075 net.cpp:226] ip3 needs backward computation.
I0520 11:54:36.915822 23075 net.cpp:226] drop2 needs backward computation.
I0520 11:54:36.915843 23075 net.cpp:226] relu6 needs backward computation.
I0520 11:54:36.915863 23075 net.cpp:226] ip2 needs backward computation.
I0520 11:54:36.915875 23075 net.cpp:226] drop1 needs backward computation.
I0520 11:54:36.915889 23075 net.cpp:226] relu5 needs backward computation.
I0520 11:54:36.915900 23075 net.cpp:226] ip1 needs backward computation.
I0520 11:54:36.915915 23075 net.cpp:226] pool4 needs backward computation.
I0520 11:54:36.915928 23075 net.cpp:226] relu4 needs backward computation.
I0520 11:54:36.915947 23075 net.cpp:226] conv4 needs backward computation.
I0520 11:54:36.915961 23075 net.cpp:226] pool3 needs backward computation.
I0520 11:54:36.915974 23075 net.cpp:226] relu3 needs backward computation.
I0520 11:54:36.915987 23075 net.cpp:226] conv3 needs backward computation.
I0520 11:54:36.916002 23075 net.cpp:226] pool2 needs backward computation.
I0520 11:54:36.916013 23075 net.cpp:226] relu2 needs backward computation.
I0520 11:54:36.916033 23075 net.cpp:226] conv2 needs backward computation.
I0520 11:54:36.916048 23075 net.cpp:226] pool1 needs backward computation.
I0520 11:54:36.916064 23075 net.cpp:226] relu1 needs backward computation.
I0520 11:54:36.916076 23075 net.cpp:226] conv1 needs backward computation.
I0520 11:54:36.916090 23075 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 11:54:36.916106 23075 net.cpp:228] data_hdf5 does not need backward computation.
I0520 11:54:36.916118 23075 net.cpp:270] This network produces output accuracy
I0520 11:54:36.916138 23075 net.cpp:270] This network produces output loss
I0520 11:54:36.916172 23075 net.cpp:283] Network initialization done.
I0520 11:54:36.916326 23075 solver.cpp:60] Solver scaffolding done.
I0520 11:54:36.917481 23075 caffe.cpp:212] Starting Optimization
I0520 11:54:36.917500 23075 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 11:54:36.917516 23075 solver.cpp:289] Learning Rate Policy: fixed
I0520 11:54:36.918766 23075 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 11:55:23.301990 23075 solver.cpp:409]     Test net output #0: accuracy = 0.0591054
I0520 11:55:23.302155 23075 solver.cpp:409]     Test net output #1: loss = 2.39859 (* 1 = 2.39859 loss)
I0520 11:55:23.364707 23075 solver.cpp:237] Iteration 0, loss = 2.39911
I0520 11:55:23.364748 23075 solver.cpp:253]     Train net output #0: loss = 2.39911 (* 1 = 2.39911 loss)
I0520 11:55:23.364769 23075 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 11:55:31.413135 23075 solver.cpp:237] Iteration 53, loss = 2.36325
I0520 11:55:31.413172 23075 solver.cpp:253]     Train net output #0: loss = 2.36325 (* 1 = 2.36325 loss)
I0520 11:55:31.413188 23075 sgd_solver.cpp:106] Iteration 53, lr = 0.0025
I0520 11:55:39.459720 23075 solver.cpp:237] Iteration 106, loss = 2.33794
I0520 11:55:39.459753 23075 solver.cpp:253]     Train net output #0: loss = 2.33794 (* 1 = 2.33794 loss)
I0520 11:55:39.459776 23075 sgd_solver.cpp:106] Iteration 106, lr = 0.0025
I0520 11:55:47.504436 23075 solver.cpp:237] Iteration 159, loss = 2.32652
I0520 11:55:47.504483 23075 solver.cpp:253]     Train net output #0: loss = 2.32652 (* 1 = 2.32652 loss)
I0520 11:55:47.504501 23075 sgd_solver.cpp:106] Iteration 159, lr = 0.0025
I0520 11:55:55.549273 23075 solver.cpp:237] Iteration 212, loss = 2.30976
I0520 11:55:55.549422 23075 solver.cpp:253]     Train net output #0: loss = 2.30976 (* 1 = 2.30976 loss)
I0520 11:55:55.549438 23075 sgd_solver.cpp:106] Iteration 212, lr = 0.0025
I0520 11:56:03.595371 23075 solver.cpp:237] Iteration 265, loss = 2.30295
I0520 11:56:03.595404 23075 solver.cpp:253]     Train net output #0: loss = 2.30295 (* 1 = 2.30295 loss)
I0520 11:56:03.595428 23075 sgd_solver.cpp:106] Iteration 265, lr = 0.0025
I0520 11:56:11.641086 23075 solver.cpp:237] Iteration 318, loss = 2.28937
I0520 11:56:11.641120 23075 solver.cpp:253]     Train net output #0: loss = 2.28937 (* 1 = 2.28937 loss)
I0520 11:56:11.641139 23075 sgd_solver.cpp:106] Iteration 318, lr = 0.0025
I0520 11:56:41.793031 23075 solver.cpp:237] Iteration 371, loss = 2.19983
I0520 11:56:41.793197 23075 solver.cpp:253]     Train net output #0: loss = 2.19983 (* 1 = 2.19983 loss)
I0520 11:56:41.793215 23075 sgd_solver.cpp:106] Iteration 371, lr = 0.0025
I0520 11:56:49.846204 23075 solver.cpp:237] Iteration 424, loss = 2.14691
I0520 11:56:49.846240 23075 solver.cpp:253]     Train net output #0: loss = 2.14691 (* 1 = 2.14691 loss)
I0520 11:56:49.846263 23075 sgd_solver.cpp:106] Iteration 424, lr = 0.0025
I0520 11:56:57.898176 23075 solver.cpp:237] Iteration 477, loss = 2.11345
I0520 11:56:57.898213 23075 solver.cpp:253]     Train net output #0: loss = 2.11345 (* 1 = 2.11345 loss)
I0520 11:56:57.898231 23075 sgd_solver.cpp:106] Iteration 477, lr = 0.0025
I0520 11:57:05.949420 23075 solver.cpp:237] Iteration 530, loss = 1.96402
I0520 11:57:05.949475 23075 solver.cpp:253]     Train net output #0: loss = 1.96402 (* 1 = 1.96402 loss)
I0520 11:57:05.949499 23075 sgd_solver.cpp:106] Iteration 530, lr = 0.0025
I0520 11:57:06.556910 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_535.caffemodel
I0520 11:57:06.704120 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_535.solverstate
I0520 11:57:14.066886 23075 solver.cpp:237] Iteration 583, loss = 2.01352
I0520 11:57:14.067044 23075 solver.cpp:253]     Train net output #0: loss = 2.01352 (* 1 = 2.01352 loss)
I0520 11:57:14.067061 23075 sgd_solver.cpp:106] Iteration 583, lr = 0.0025
I0520 11:57:22.114202 23075 solver.cpp:237] Iteration 636, loss = 1.93315
I0520 11:57:22.114236 23075 solver.cpp:253]     Train net output #0: loss = 1.93315 (* 1 = 1.93315 loss)
I0520 11:57:22.114254 23075 sgd_solver.cpp:106] Iteration 636, lr = 0.0025
I0520 11:57:30.165091 23075 solver.cpp:237] Iteration 689, loss = 2.06047
I0520 11:57:30.165139 23075 solver.cpp:253]     Train net output #0: loss = 2.06047 (* 1 = 2.06047 loss)
I0520 11:57:30.165156 23075 sgd_solver.cpp:106] Iteration 689, lr = 0.0025
I0520 11:58:00.318899 23075 solver.cpp:237] Iteration 742, loss = 1.85329
I0520 11:58:00.319058 23075 solver.cpp:253]     Train net output #0: loss = 1.85329 (* 1 = 1.85329 loss)
I0520 11:58:00.319075 23075 sgd_solver.cpp:106] Iteration 742, lr = 0.0025
I0520 11:58:08.371495 23075 solver.cpp:237] Iteration 795, loss = 1.79195
I0520 11:58:08.371529 23075 solver.cpp:253]     Train net output #0: loss = 1.79195 (* 1 = 1.79195 loss)
I0520 11:58:08.371554 23075 sgd_solver.cpp:106] Iteration 795, lr = 0.0025
I0520 11:58:16.424998 23075 solver.cpp:237] Iteration 848, loss = 1.81043
I0520 11:58:16.425034 23075 solver.cpp:253]     Train net output #0: loss = 1.81043 (* 1 = 1.81043 loss)
I0520 11:58:16.425056 23075 sgd_solver.cpp:106] Iteration 848, lr = 0.0025
I0520 11:58:24.476763 23075 solver.cpp:237] Iteration 901, loss = 1.86794
I0520 11:58:24.476810 23075 solver.cpp:253]     Train net output #0: loss = 1.86794 (* 1 = 1.86794 loss)
I0520 11:58:24.476827 23075 sgd_solver.cpp:106] Iteration 901, lr = 0.0025
I0520 11:58:32.525398 23075 solver.cpp:237] Iteration 954, loss = 1.8342
I0520 11:58:32.525549 23075 solver.cpp:253]     Train net output #0: loss = 1.8342 (* 1 = 1.8342 loss)
I0520 11:58:32.525566 23075 sgd_solver.cpp:106] Iteration 954, lr = 0.0025
I0520 11:58:40.574548 23075 solver.cpp:237] Iteration 1007, loss = 1.80313
I0520 11:58:40.574584 23075 solver.cpp:253]     Train net output #0: loss = 1.80313 (* 1 = 1.80313 loss)
I0520 11:58:40.574600 23075 sgd_solver.cpp:106] Iteration 1007, lr = 0.0025
I0520 11:58:48.628376 23075 solver.cpp:237] Iteration 1060, loss = 1.83236
I0520 11:58:48.628428 23075 solver.cpp:253]     Train net output #0: loss = 1.83236 (* 1 = 1.83236 loss)
I0520 11:58:48.628453 23075 sgd_solver.cpp:106] Iteration 1060, lr = 0.0025
I0520 11:58:49.997360 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_1070.caffemodel
I0520 11:58:50.140785 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_1070.solverstate
I0520 11:58:50.214229 23075 solver.cpp:341] Iteration 1071, Testing net (#0)
I0520 11:59:35.764677 23075 solver.cpp:409]     Test net output #0: accuracy = 0.628865
I0520 11:59:35.764837 23075 solver.cpp:409]     Test net output #1: loss = 1.32771 (* 1 = 1.32771 loss)
I0520 12:00:04.284925 23075 solver.cpp:237] Iteration 1113, loss = 1.7623
I0520 12:00:04.284981 23075 solver.cpp:253]     Train net output #0: loss = 1.7623 (* 1 = 1.7623 loss)
I0520 12:00:04.285007 23075 sgd_solver.cpp:106] Iteration 1113, lr = 0.0025
I0520 12:00:12.332926 23075 solver.cpp:237] Iteration 1166, loss = 1.78119
I0520 12:00:12.333068 23075 solver.cpp:253]     Train net output #0: loss = 1.78119 (* 1 = 1.78119 loss)
I0520 12:00:12.333086 23075 sgd_solver.cpp:106] Iteration 1166, lr = 0.0025
I0520 12:00:20.383008 23075 solver.cpp:237] Iteration 1219, loss = 1.78308
I0520 12:00:20.383044 23075 solver.cpp:253]     Train net output #0: loss = 1.78308 (* 1 = 1.78308 loss)
I0520 12:00:20.383067 23075 sgd_solver.cpp:106] Iteration 1219, lr = 0.0025
I0520 12:00:28.436071 23075 solver.cpp:237] Iteration 1272, loss = 1.71637
I0520 12:00:28.436118 23075 solver.cpp:253]     Train net output #0: loss = 1.71637 (* 1 = 1.71637 loss)
I0520 12:00:28.436147 23075 sgd_solver.cpp:106] Iteration 1272, lr = 0.0025
I0520 12:00:36.488741 23075 solver.cpp:237] Iteration 1325, loss = 1.832
I0520 12:00:36.488776 23075 solver.cpp:253]     Train net output #0: loss = 1.832 (* 1 = 1.832 loss)
I0520 12:00:36.488800 23075 sgd_solver.cpp:106] Iteration 1325, lr = 0.0025
I0520 12:00:44.539531 23075 solver.cpp:237] Iteration 1378, loss = 1.81273
I0520 12:00:44.539670 23075 solver.cpp:253]     Train net output #0: loss = 1.81273 (* 1 = 1.81273 loss)
I0520 12:00:44.539687 23075 sgd_solver.cpp:106] Iteration 1378, lr = 0.0025
I0520 12:01:14.783885 23075 solver.cpp:237] Iteration 1431, loss = 1.64631
I0520 12:01:14.784055 23075 solver.cpp:253]     Train net output #0: loss = 1.64631 (* 1 = 1.64631 loss)
I0520 12:01:14.784073 23075 sgd_solver.cpp:106] Iteration 1431, lr = 0.0025
I0520 12:01:22.835216 23075 solver.cpp:237] Iteration 1484, loss = 1.78205
I0520 12:01:22.835252 23075 solver.cpp:253]     Train net output #0: loss = 1.78205 (* 1 = 1.78205 loss)
I0520 12:01:22.835275 23075 sgd_solver.cpp:106] Iteration 1484, lr = 0.0025
I0520 12:01:30.886888 23075 solver.cpp:237] Iteration 1537, loss = 1.7715
I0520 12:01:30.886924 23075 solver.cpp:253]     Train net output #0: loss = 1.7715 (* 1 = 1.7715 loss)
I0520 12:01:30.886941 23075 sgd_solver.cpp:106] Iteration 1537, lr = 0.0025
I0520 12:01:38.939915 23075 solver.cpp:237] Iteration 1590, loss = 1.64678
I0520 12:01:38.939966 23075 solver.cpp:253]     Train net output #0: loss = 1.64678 (* 1 = 1.64678 loss)
I0520 12:01:38.939995 23075 sgd_solver.cpp:106] Iteration 1590, lr = 0.0025
I0520 12:01:41.068063 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_1605.caffemodel
I0520 12:01:41.214642 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_1605.solverstate
I0520 12:01:47.061408 23075 solver.cpp:237] Iteration 1643, loss = 1.67392
I0520 12:01:47.061581 23075 solver.cpp:253]     Train net output #0: loss = 1.67392 (* 1 = 1.67392 loss)
I0520 12:01:47.061599 23075 sgd_solver.cpp:106] Iteration 1643, lr = 0.0025
I0520 12:01:55.110800 23075 solver.cpp:237] Iteration 1696, loss = 1.70825
I0520 12:01:55.110834 23075 solver.cpp:253]     Train net output #0: loss = 1.70825 (* 1 = 1.70825 loss)
I0520 12:01:55.110853 23075 sgd_solver.cpp:106] Iteration 1696, lr = 0.0025
I0520 12:02:03.163619 23075 solver.cpp:237] Iteration 1749, loss = 1.77904
I0520 12:02:03.163674 23075 solver.cpp:253]     Train net output #0: loss = 1.77904 (* 1 = 1.77904 loss)
I0520 12:02:03.163699 23075 sgd_solver.cpp:106] Iteration 1749, lr = 0.0025
I0520 12:02:33.335988 23075 solver.cpp:237] Iteration 1802, loss = 1.68681
I0520 12:02:33.336149 23075 solver.cpp:253]     Train net output #0: loss = 1.68681 (* 1 = 1.68681 loss)
I0520 12:02:33.336166 23075 sgd_solver.cpp:106] Iteration 1802, lr = 0.0025
I0520 12:02:41.386358 23075 solver.cpp:237] Iteration 1855, loss = 1.65019
I0520 12:02:41.386394 23075 solver.cpp:253]     Train net output #0: loss = 1.65019 (* 1 = 1.65019 loss)
I0520 12:02:41.386411 23075 sgd_solver.cpp:106] Iteration 1855, lr = 0.0025
I0520 12:02:49.435993 23075 solver.cpp:237] Iteration 1908, loss = 1.73744
I0520 12:02:49.436028 23075 solver.cpp:253]     Train net output #0: loss = 1.73744 (* 1 = 1.73744 loss)
I0520 12:02:49.436053 23075 sgd_solver.cpp:106] Iteration 1908, lr = 0.0025
I0520 12:02:57.490471 23075 solver.cpp:237] Iteration 1961, loss = 1.77814
I0520 12:02:57.490521 23075 solver.cpp:253]     Train net output #0: loss = 1.77814 (* 1 = 1.77814 loss)
I0520 12:02:57.490538 23075 sgd_solver.cpp:106] Iteration 1961, lr = 0.0025
I0520 12:03:05.541407 23075 solver.cpp:237] Iteration 2014, loss = 1.84163
I0520 12:03:05.541549 23075 solver.cpp:253]     Train net output #0: loss = 1.84163 (* 1 = 1.84163 loss)
I0520 12:03:05.541566 23075 sgd_solver.cpp:106] Iteration 2014, lr = 0.0025
I0520 12:03:13.587991 23075 solver.cpp:237] Iteration 2067, loss = 1.5857
I0520 12:03:13.588027 23075 solver.cpp:253]     Train net output #0: loss = 1.5857 (* 1 = 1.5857 loss)
I0520 12:03:13.588052 23075 sgd_solver.cpp:106] Iteration 2067, lr = 0.0025
I0520 12:03:21.640647 23075 solver.cpp:237] Iteration 2120, loss = 1.63345
I0520 12:03:21.640696 23075 solver.cpp:253]     Train net output #0: loss = 1.63345 (* 1 = 1.63345 loss)
I0520 12:03:21.640724 23075 sgd_solver.cpp:106] Iteration 2120, lr = 0.0025
I0520 12:03:24.526082 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_2140.caffemodel
I0520 12:03:24.672775 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_2140.solverstate
I0520 12:03:24.900262 23075 solver.cpp:341] Iteration 2142, Testing net (#0)
I0520 12:04:31.376895 23075 solver.cpp:409]     Test net output #0: accuracy = 0.675193
I0520 12:04:31.377068 23075 solver.cpp:409]     Test net output #1: loss = 1.0993 (* 1 = 1.0993 loss)
I0520 12:04:58.352357 23075 solver.cpp:237] Iteration 2173, loss = 1.58483
I0520 12:04:58.352412 23075 solver.cpp:253]     Train net output #0: loss = 1.58483 (* 1 = 1.58483 loss)
I0520 12:04:58.352439 23075 sgd_solver.cpp:106] Iteration 2173, lr = 0.0025
I0520 12:05:06.402406 23075 solver.cpp:237] Iteration 2226, loss = 1.72331
I0520 12:05:06.402561 23075 solver.cpp:253]     Train net output #0: loss = 1.72331 (* 1 = 1.72331 loss)
I0520 12:05:06.402578 23075 sgd_solver.cpp:106] Iteration 2226, lr = 0.0025
I0520 12:05:14.452396 23075 solver.cpp:237] Iteration 2279, loss = 1.68536
I0520 12:05:14.452430 23075 solver.cpp:253]     Train net output #0: loss = 1.68536 (* 1 = 1.68536 loss)
I0520 12:05:14.452455 23075 sgd_solver.cpp:106] Iteration 2279, lr = 0.0025
I0520 12:05:22.505786 23075 solver.cpp:237] Iteration 2332, loss = 1.69184
I0520 12:05:22.505834 23075 solver.cpp:253]     Train net output #0: loss = 1.69184 (* 1 = 1.69184 loss)
I0520 12:05:22.505852 23075 sgd_solver.cpp:106] Iteration 2332, lr = 0.0025
I0520 12:05:30.556404 23075 solver.cpp:237] Iteration 2385, loss = 1.59676
I0520 12:05:30.556439 23075 solver.cpp:253]     Train net output #0: loss = 1.59676 (* 1 = 1.59676 loss)
I0520 12:05:30.556464 23075 sgd_solver.cpp:106] Iteration 2385, lr = 0.0025
I0520 12:05:38.604059 23075 solver.cpp:237] Iteration 2438, loss = 1.65843
I0520 12:05:38.604203 23075 solver.cpp:253]     Train net output #0: loss = 1.65843 (* 1 = 1.65843 loss)
I0520 12:05:38.604220 23075 sgd_solver.cpp:106] Iteration 2438, lr = 0.0025
I0520 12:05:46.654155 23075 solver.cpp:237] Iteration 2491, loss = 1.69847
I0520 12:05:46.654189 23075 solver.cpp:253]     Train net output #0: loss = 1.69847 (* 1 = 1.69847 loss)
I0520 12:05:46.654212 23075 sgd_solver.cpp:106] Iteration 2491, lr = 0.0025
I0520 12:06:16.939373 23075 solver.cpp:237] Iteration 2544, loss = 1.64009
I0520 12:06:16.939540 23075 solver.cpp:253]     Train net output #0: loss = 1.64009 (* 1 = 1.64009 loss)
I0520 12:06:16.939558 23075 sgd_solver.cpp:106] Iteration 2544, lr = 0.0025
I0520 12:06:24.983701 23075 solver.cpp:237] Iteration 2597, loss = 1.56024
I0520 12:06:24.983736 23075 solver.cpp:253]     Train net output #0: loss = 1.56024 (* 1 = 1.56024 loss)
I0520 12:06:24.983759 23075 sgd_solver.cpp:106] Iteration 2597, lr = 0.0025
I0520 12:06:33.030448 23075 solver.cpp:237] Iteration 2650, loss = 1.53696
I0520 12:06:33.030483 23075 solver.cpp:253]     Train net output #0: loss = 1.53696 (* 1 = 1.53696 loss)
I0520 12:06:33.030505 23075 sgd_solver.cpp:106] Iteration 2650, lr = 0.0025
I0520 12:06:36.675649 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_2675.caffemodel
I0520 12:06:36.820868 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_2675.solverstate
I0520 12:06:41.149811 23075 solver.cpp:237] Iteration 2703, loss = 1.71261
I0520 12:06:41.149863 23075 solver.cpp:253]     Train net output #0: loss = 1.71261 (* 1 = 1.71261 loss)
I0520 12:06:41.149888 23075 sgd_solver.cpp:106] Iteration 2703, lr = 0.0025
I0520 12:06:49.205651 23075 solver.cpp:237] Iteration 2756, loss = 1.70487
I0520 12:06:49.205797 23075 solver.cpp:253]     Train net output #0: loss = 1.70487 (* 1 = 1.70487 loss)
I0520 12:06:49.205813 23075 sgd_solver.cpp:106] Iteration 2756, lr = 0.0025
I0520 12:06:57.259572 23075 solver.cpp:237] Iteration 2809, loss = 1.7855
I0520 12:06:57.259606 23075 solver.cpp:253]     Train net output #0: loss = 1.7855 (* 1 = 1.7855 loss)
I0520 12:06:57.259625 23075 sgd_solver.cpp:106] Iteration 2809, lr = 0.0025
I0520 12:07:27.574674 23075 solver.cpp:237] Iteration 2862, loss = 1.61066
I0520 12:07:27.574843 23075 solver.cpp:253]     Train net output #0: loss = 1.61066 (* 1 = 1.61066 loss)
I0520 12:07:27.574861 23075 sgd_solver.cpp:106] Iteration 2862, lr = 0.0025
I0520 12:07:35.629902 23075 solver.cpp:237] Iteration 2915, loss = 1.65678
I0520 12:07:35.629938 23075 solver.cpp:253]     Train net output #0: loss = 1.65678 (* 1 = 1.65678 loss)
I0520 12:07:35.629957 23075 sgd_solver.cpp:106] Iteration 2915, lr = 0.0025
I0520 12:07:43.688423 23075 solver.cpp:237] Iteration 2968, loss = 1.63019
I0520 12:07:43.688459 23075 solver.cpp:253]     Train net output #0: loss = 1.63019 (* 1 = 1.63019 loss)
I0520 12:07:43.688477 23075 sgd_solver.cpp:106] Iteration 2968, lr = 0.0025
I0520 12:07:51.742276 23075 solver.cpp:237] Iteration 3021, loss = 1.63999
I0520 12:07:51.742336 23075 solver.cpp:253]     Train net output #0: loss = 1.63999 (* 1 = 1.63999 loss)
I0520 12:07:51.742354 23075 sgd_solver.cpp:106] Iteration 3021, lr = 0.0025
I0520 12:07:59.797327 23075 solver.cpp:237] Iteration 3074, loss = 1.68089
I0520 12:07:59.797474 23075 solver.cpp:253]     Train net output #0: loss = 1.68089 (* 1 = 1.68089 loss)
I0520 12:07:59.797492 23075 sgd_solver.cpp:106] Iteration 3074, lr = 0.0025
I0520 12:08:07.855099 23075 solver.cpp:237] Iteration 3127, loss = 1.71864
I0520 12:08:07.855134 23075 solver.cpp:253]     Train net output #0: loss = 1.71864 (* 1 = 1.71864 loss)
I0520 12:08:07.855159 23075 sgd_solver.cpp:106] Iteration 3127, lr = 0.0025
I0520 12:08:15.905834 23075 solver.cpp:237] Iteration 3180, loss = 1.53224
I0520 12:08:15.905869 23075 solver.cpp:253]     Train net output #0: loss = 1.53224 (* 1 = 1.53224 loss)
I0520 12:08:15.905886 23075 sgd_solver.cpp:106] Iteration 3180, lr = 0.0025
I0520 12:08:20.311605 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_3210.caffemodel
I0520 12:08:20.455024 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_3210.solverstate
I0520 12:08:20.831637 23075 solver.cpp:341] Iteration 3213, Testing net (#0)
I0520 12:09:06.092098 23075 solver.cpp:409]     Test net output #0: accuracy = 0.730629
I0520 12:09:06.092260 23075 solver.cpp:409]     Test net output #1: loss = 0.944604 (* 1 = 0.944604 loss)
I0520 12:09:31.422461 23075 solver.cpp:237] Iteration 3233, loss = 1.58579
I0520 12:09:31.422518 23075 solver.cpp:253]     Train net output #0: loss = 1.58579 (* 1 = 1.58579 loss)
I0520 12:09:31.422536 23075 sgd_solver.cpp:106] Iteration 3233, lr = 0.0025
I0520 12:09:39.478060 23075 solver.cpp:237] Iteration 3286, loss = 1.46923
I0520 12:09:39.478224 23075 solver.cpp:253]     Train net output #0: loss = 1.46923 (* 1 = 1.46923 loss)
I0520 12:09:39.478241 23075 sgd_solver.cpp:106] Iteration 3286, lr = 0.0025
I0520 12:09:47.535073 23075 solver.cpp:237] Iteration 3339, loss = 1.49317
I0520 12:09:47.535109 23075 solver.cpp:253]     Train net output #0: loss = 1.49317 (* 1 = 1.49317 loss)
I0520 12:09:47.535126 23075 sgd_solver.cpp:106] Iteration 3339, lr = 0.0025
I0520 12:09:55.593797 23075 solver.cpp:237] Iteration 3392, loss = 1.46009
I0520 12:09:55.593835 23075 solver.cpp:253]     Train net output #0: loss = 1.46009 (* 1 = 1.46009 loss)
I0520 12:09:55.593852 23075 sgd_solver.cpp:106] Iteration 3392, lr = 0.0025
I0520 12:10:03.650985 23075 solver.cpp:237] Iteration 3445, loss = 1.67004
I0520 12:10:03.651034 23075 solver.cpp:253]     Train net output #0: loss = 1.67004 (* 1 = 1.67004 loss)
I0520 12:10:03.651059 23075 sgd_solver.cpp:106] Iteration 3445, lr = 0.0025
I0520 12:10:11.705924 23075 solver.cpp:237] Iteration 3498, loss = 1.51814
I0520 12:10:11.706087 23075 solver.cpp:253]     Train net output #0: loss = 1.51814 (* 1 = 1.51814 loss)
I0520 12:10:11.706104 23075 sgd_solver.cpp:106] Iteration 3498, lr = 0.0025
I0520 12:10:19.761962 23075 solver.cpp:237] Iteration 3551, loss = 1.48993
I0520 12:10:19.761997 23075 solver.cpp:253]     Train net output #0: loss = 1.48993 (* 1 = 1.48993 loss)
I0520 12:10:19.762017 23075 sgd_solver.cpp:106] Iteration 3551, lr = 0.0025
I0520 12:10:49.999593 23075 solver.cpp:237] Iteration 3604, loss = 1.48956
I0520 12:10:49.999763 23075 solver.cpp:253]     Train net output #0: loss = 1.48956 (* 1 = 1.48956 loss)
I0520 12:10:49.999781 23075 sgd_solver.cpp:106] Iteration 3604, lr = 0.0025
I0520 12:10:58.054373 23075 solver.cpp:237] Iteration 3657, loss = 1.54442
I0520 12:10:58.054406 23075 solver.cpp:253]     Train net output #0: loss = 1.54442 (* 1 = 1.54442 loss)
I0520 12:10:58.054425 23075 sgd_solver.cpp:106] Iteration 3657, lr = 0.0025
I0520 12:11:06.110409 23075 solver.cpp:237] Iteration 3710, loss = 1.45564
I0520 12:11:06.110443 23075 solver.cpp:253]     Train net output #0: loss = 1.45564 (* 1 = 1.45564 loss)
I0520 12:11:06.110461 23075 sgd_solver.cpp:106] Iteration 3710, lr = 0.0025
I0520 12:11:11.279605 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_3745.caffemodel
I0520 12:11:11.424135 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_3745.solverstate
I0520 12:11:14.233693 23075 solver.cpp:237] Iteration 3763, loss = 1.55097
I0520 12:11:14.233747 23075 solver.cpp:253]     Train net output #0: loss = 1.55097 (* 1 = 1.55097 loss)
I0520 12:11:14.233774 23075 sgd_solver.cpp:106] Iteration 3763, lr = 0.0025
I0520 12:11:22.291916 23075 solver.cpp:237] Iteration 3816, loss = 1.52551
I0520 12:11:22.292064 23075 solver.cpp:253]     Train net output #0: loss = 1.52551 (* 1 = 1.52551 loss)
I0520 12:11:22.292081 23075 sgd_solver.cpp:106] Iteration 3816, lr = 0.0025
I0520 12:11:30.351202 23075 solver.cpp:237] Iteration 3869, loss = 1.52554
I0520 12:11:30.351241 23075 solver.cpp:253]     Train net output #0: loss = 1.52554 (* 1 = 1.52554 loss)
I0520 12:11:30.351260 23075 sgd_solver.cpp:106] Iteration 3869, lr = 0.0025
I0520 12:11:38.410943 23075 solver.cpp:237] Iteration 3922, loss = 1.41739
I0520 12:11:38.410995 23075 solver.cpp:253]     Train net output #0: loss = 1.41739 (* 1 = 1.41739 loss)
I0520 12:11:38.411020 23075 sgd_solver.cpp:106] Iteration 3922, lr = 0.0025
I0520 12:12:08.671628 23075 solver.cpp:237] Iteration 3975, loss = 1.43591
I0520 12:12:08.671792 23075 solver.cpp:253]     Train net output #0: loss = 1.43591 (* 1 = 1.43591 loss)
I0520 12:12:08.671808 23075 sgd_solver.cpp:106] Iteration 3975, lr = 0.0025
I0520 12:12:16.731783 23075 solver.cpp:237] Iteration 4028, loss = 1.6636
I0520 12:12:16.731818 23075 solver.cpp:253]     Train net output #0: loss = 1.6636 (* 1 = 1.6636 loss)
I0520 12:12:16.731837 23075 sgd_solver.cpp:106] Iteration 4028, lr = 0.0025
I0520 12:12:24.784685 23075 solver.cpp:237] Iteration 4081, loss = 1.5276
I0520 12:12:24.784721 23075 solver.cpp:253]     Train net output #0: loss = 1.5276 (* 1 = 1.5276 loss)
I0520 12:12:24.784739 23075 sgd_solver.cpp:106] Iteration 4081, lr = 0.0025
I0520 12:12:32.833513 23075 solver.cpp:237] Iteration 4134, loss = 1.50262
I0520 12:12:32.833551 23075 solver.cpp:253]     Train net output #0: loss = 1.50262 (* 1 = 1.50262 loss)
I0520 12:12:32.833569 23075 sgd_solver.cpp:106] Iteration 4134, lr = 0.0025
I0520 12:12:40.884443 23075 solver.cpp:237] Iteration 4187, loss = 1.37189
I0520 12:12:40.884589 23075 solver.cpp:253]     Train net output #0: loss = 1.37189 (* 1 = 1.37189 loss)
I0520 12:12:40.884605 23075 sgd_solver.cpp:106] Iteration 4187, lr = 0.0025
I0520 12:12:48.932713 23075 solver.cpp:237] Iteration 4240, loss = 1.54877
I0520 12:12:48.932755 23075 solver.cpp:253]     Train net output #0: loss = 1.54877 (* 1 = 1.54877 loss)
I0520 12:12:48.932772 23075 sgd_solver.cpp:106] Iteration 4240, lr = 0.0025
I0520 12:12:54.859700 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_4280.caffemodel
I0520 12:12:55.004214 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_4280.solverstate
I0520 12:12:55.533321 23075 solver.cpp:341] Iteration 4284, Testing net (#0)
I0520 12:14:01.972062 23075 solver.cpp:409]     Test net output #0: accuracy = 0.750801
I0520 12:14:01.972240 23075 solver.cpp:409]     Test net output #1: loss = 0.939931 (* 1 = 0.939931 loss)
I0520 12:14:25.554340 23075 solver.cpp:237] Iteration 4293, loss = 1.4679
I0520 12:14:25.554395 23075 solver.cpp:253]     Train net output #0: loss = 1.4679 (* 1 = 1.4679 loss)
I0520 12:14:25.554420 23075 sgd_solver.cpp:106] Iteration 4293, lr = 0.0025
I0520 12:14:33.605446 23075 solver.cpp:237] Iteration 4346, loss = 1.4336
I0520 12:14:33.605613 23075 solver.cpp:253]     Train net output #0: loss = 1.4336 (* 1 = 1.4336 loss)
I0520 12:14:33.605631 23075 sgd_solver.cpp:106] Iteration 4346, lr = 0.0025
I0520 12:14:41.653695 23075 solver.cpp:237] Iteration 4399, loss = 1.29527
I0520 12:14:41.653731 23075 solver.cpp:253]     Train net output #0: loss = 1.29527 (* 1 = 1.29527 loss)
I0520 12:14:41.653749 23075 sgd_solver.cpp:106] Iteration 4399, lr = 0.0025
I0520 12:14:49.704414 23075 solver.cpp:237] Iteration 4452, loss = 1.46033
I0520 12:14:49.704449 23075 solver.cpp:253]     Train net output #0: loss = 1.46033 (* 1 = 1.46033 loss)
I0520 12:14:49.704468 23075 sgd_solver.cpp:106] Iteration 4452, lr = 0.0025
I0520 12:14:57.751888 23075 solver.cpp:237] Iteration 4505, loss = 1.49803
I0520 12:14:57.751936 23075 solver.cpp:253]     Train net output #0: loss = 1.49803 (* 1 = 1.49803 loss)
I0520 12:14:57.751955 23075 sgd_solver.cpp:106] Iteration 4505, lr = 0.0025
I0520 12:15:05.802458 23075 solver.cpp:237] Iteration 4558, loss = 1.48113
I0520 12:15:05.802603 23075 solver.cpp:253]     Train net output #0: loss = 1.48113 (* 1 = 1.48113 loss)
I0520 12:15:05.802619 23075 sgd_solver.cpp:106] Iteration 4558, lr = 0.0025
I0520 12:15:13.851704 23075 solver.cpp:237] Iteration 4611, loss = 1.37379
I0520 12:15:13.851742 23075 solver.cpp:253]     Train net output #0: loss = 1.37379 (* 1 = 1.37379 loss)
I0520 12:15:13.851758 23075 sgd_solver.cpp:106] Iteration 4611, lr = 0.0025
I0520 12:15:44.083159 23075 solver.cpp:237] Iteration 4664, loss = 1.40984
I0520 12:15:44.083338 23075 solver.cpp:253]     Train net output #0: loss = 1.40984 (* 1 = 1.40984 loss)
I0520 12:15:44.083356 23075 sgd_solver.cpp:106] Iteration 4664, lr = 0.0025
I0520 12:15:52.132593 23075 solver.cpp:237] Iteration 4717, loss = 1.37571
I0520 12:15:52.132633 23075 solver.cpp:253]     Train net output #0: loss = 1.37571 (* 1 = 1.37571 loss)
I0520 12:15:52.132650 23075 sgd_solver.cpp:106] Iteration 4717, lr = 0.0025
I0520 12:16:00.182713 23075 solver.cpp:237] Iteration 4770, loss = 1.39
I0520 12:16:00.182749 23075 solver.cpp:253]     Train net output #0: loss = 1.39 (* 1 = 1.39 loss)
I0520 12:16:00.182767 23075 sgd_solver.cpp:106] Iteration 4770, lr = 0.0025
I0520 12:16:06.866116 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_4815.caffemodel
I0520 12:16:07.011770 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_4815.solverstate
I0520 12:16:08.302908 23075 solver.cpp:237] Iteration 4823, loss = 1.3133
I0520 12:16:08.302959 23075 solver.cpp:253]     Train net output #0: loss = 1.3133 (* 1 = 1.3133 loss)
I0520 12:16:08.302979 23075 sgd_solver.cpp:106] Iteration 4823, lr = 0.0025
I0520 12:16:16.356377 23075 solver.cpp:237] Iteration 4876, loss = 1.40615
I0520 12:16:16.356550 23075 solver.cpp:253]     Train net output #0: loss = 1.40615 (* 1 = 1.40615 loss)
I0520 12:16:16.356570 23075 sgd_solver.cpp:106] Iteration 4876, lr = 0.0025
I0520 12:16:24.408434 23075 solver.cpp:237] Iteration 4929, loss = 1.36297
I0520 12:16:24.408471 23075 solver.cpp:253]     Train net output #0: loss = 1.36297 (* 1 = 1.36297 loss)
I0520 12:16:24.408489 23075 sgd_solver.cpp:106] Iteration 4929, lr = 0.0025
I0520 12:16:32.460289 23075 solver.cpp:237] Iteration 4982, loss = 1.49614
I0520 12:16:32.460324 23075 solver.cpp:253]     Train net output #0: loss = 1.49614 (* 1 = 1.49614 loss)
I0520 12:16:32.460341 23075 sgd_solver.cpp:106] Iteration 4982, lr = 0.0025
I0520 12:17:02.729387 23075 solver.cpp:237] Iteration 5035, loss = 1.29473
I0520 12:17:02.729564 23075 solver.cpp:253]     Train net output #0: loss = 1.29473 (* 1 = 1.29473 loss)
I0520 12:17:02.729581 23075 sgd_solver.cpp:106] Iteration 5035, lr = 0.0025
I0520 12:17:10.778348 23075 solver.cpp:237] Iteration 5088, loss = 1.27007
I0520 12:17:10.778384 23075 solver.cpp:253]     Train net output #0: loss = 1.27007 (* 1 = 1.27007 loss)
I0520 12:17:10.778409 23075 sgd_solver.cpp:106] Iteration 5088, lr = 0.0025
I0520 12:17:18.824820 23075 solver.cpp:237] Iteration 5141, loss = 1.3821
I0520 12:17:18.824854 23075 solver.cpp:253]     Train net output #0: loss = 1.3821 (* 1 = 1.3821 loss)
I0520 12:17:18.824872 23075 sgd_solver.cpp:106] Iteration 5141, lr = 0.0025
I0520 12:17:26.876075 23075 solver.cpp:237] Iteration 5194, loss = 1.35738
I0520 12:17:26.876129 23075 solver.cpp:253]     Train net output #0: loss = 1.35738 (* 1 = 1.35738 loss)
I0520 12:17:26.876148 23075 sgd_solver.cpp:106] Iteration 5194, lr = 0.0025
I0520 12:17:34.925403 23075 solver.cpp:237] Iteration 5247, loss = 1.42328
I0520 12:17:34.925554 23075 solver.cpp:253]     Train net output #0: loss = 1.42328 (* 1 = 1.42328 loss)
I0520 12:17:34.925571 23075 sgd_solver.cpp:106] Iteration 5247, lr = 0.0025
I0520 12:17:42.975008 23075 solver.cpp:237] Iteration 5300, loss = 1.57612
I0520 12:17:42.975042 23075 solver.cpp:253]     Train net output #0: loss = 1.57612 (* 1 = 1.57612 loss)
I0520 12:17:42.975060 23075 sgd_solver.cpp:106] Iteration 5300, lr = 0.0025
I0520 12:17:50.413326 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_5350.caffemodel
I0520 12:17:50.558771 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_5350.solverstate
I0520 12:17:51.089063 23075 solver.cpp:237] Iteration 5353, loss = 1.42644
I0520 12:17:51.089114 23075 solver.cpp:253]     Train net output #0: loss = 1.42644 (* 1 = 1.42644 loss)
I0520 12:17:51.089141 23075 sgd_solver.cpp:106] Iteration 5353, lr = 0.0025
I0520 12:17:51.241468 23075 solver.cpp:341] Iteration 5355, Testing net (#0)
I0520 12:18:36.849267 23075 solver.cpp:409]     Test net output #0: accuracy = 0.766574
I0520 12:18:36.849447 23075 solver.cpp:409]     Test net output #1: loss = 0.825999 (* 1 = 0.825999 loss)
I0520 12:18:37.047392 23075 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_5357.caffemodel
I0520 12:18:37.194051 23075 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096_iter_5357.solverstate
I0520 12:18:37.222651 23075 solver.cpp:326] Optimization Done.
I0520 12:18:37.222687 23075 caffe.cpp:215] Optimization Done.
Application 11231983 resources: utime ~1259s, stime ~228s, Rss ~5329400, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_280_2016-05-20T11.20.42.939096.solver"
	User time (seconds): 0.55
	System time (seconds): 0.22
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:49.95
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 1
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 3333
	Involuntary context switches: 303
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
