2806132
I0521 01:17:28.177039 27299 caffe.cpp:184] Using GPUs 0
I0521 01:17:28.602653 27299 solver.cpp:48] Initializing solver from parameters: 
test_iter: 294
test_interval: 588
base_lr: 0.0025
display: 29
max_iter: 2941
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 294
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332.prototxt"
I0521 01:17:28.604791 27299 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332.prototxt
I0521 01:17:28.619729 27299 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 01:17:28.619788 27299 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 01:17:28.620143 27299 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 510
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:17:28.620321 27299 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:17:28.620344 27299 net.cpp:106] Creating Layer data_hdf5
I0521 01:17:28.620358 27299 net.cpp:411] data_hdf5 -> data
I0521 01:17:28.620393 27299 net.cpp:411] data_hdf5 -> label
I0521 01:17:28.620424 27299 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 01:17:28.621664 27299 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 01:17:28.623872 27299 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 01:17:50.159505 27299 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 01:17:50.164634 27299 net.cpp:150] Setting up data_hdf5
I0521 01:17:50.164680 27299 net.cpp:157] Top shape: 510 1 127 50 (3238500)
I0521 01:17:50.164695 27299 net.cpp:157] Top shape: 510 (510)
I0521 01:17:50.164705 27299 net.cpp:165] Memory required for data: 12956040
I0521 01:17:50.164718 27299 layer_factory.hpp:77] Creating layer conv1
I0521 01:17:50.164752 27299 net.cpp:106] Creating Layer conv1
I0521 01:17:50.164763 27299 net.cpp:454] conv1 <- data
I0521 01:17:50.164785 27299 net.cpp:411] conv1 -> conv1
I0521 01:17:50.535470 27299 net.cpp:150] Setting up conv1
I0521 01:17:50.535518 27299 net.cpp:157] Top shape: 510 12 120 48 (35251200)
I0521 01:17:50.535529 27299 net.cpp:165] Memory required for data: 153960840
I0521 01:17:50.535557 27299 layer_factory.hpp:77] Creating layer relu1
I0521 01:17:50.535579 27299 net.cpp:106] Creating Layer relu1
I0521 01:17:50.535590 27299 net.cpp:454] relu1 <- conv1
I0521 01:17:50.535604 27299 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:17:50.536123 27299 net.cpp:150] Setting up relu1
I0521 01:17:50.536139 27299 net.cpp:157] Top shape: 510 12 120 48 (35251200)
I0521 01:17:50.536150 27299 net.cpp:165] Memory required for data: 294965640
I0521 01:17:50.536160 27299 layer_factory.hpp:77] Creating layer pool1
I0521 01:17:50.536176 27299 net.cpp:106] Creating Layer pool1
I0521 01:17:50.536186 27299 net.cpp:454] pool1 <- conv1
I0521 01:17:50.536200 27299 net.cpp:411] pool1 -> pool1
I0521 01:17:50.536278 27299 net.cpp:150] Setting up pool1
I0521 01:17:50.536293 27299 net.cpp:157] Top shape: 510 12 60 48 (17625600)
I0521 01:17:50.536303 27299 net.cpp:165] Memory required for data: 365468040
I0521 01:17:50.536312 27299 layer_factory.hpp:77] Creating layer conv2
I0521 01:17:50.536335 27299 net.cpp:106] Creating Layer conv2
I0521 01:17:50.536345 27299 net.cpp:454] conv2 <- pool1
I0521 01:17:50.536360 27299 net.cpp:411] conv2 -> conv2
I0521 01:17:50.539023 27299 net.cpp:150] Setting up conv2
I0521 01:17:50.539050 27299 net.cpp:157] Top shape: 510 20 54 46 (25336800)
I0521 01:17:50.539062 27299 net.cpp:165] Memory required for data: 466815240
I0521 01:17:50.539080 27299 layer_factory.hpp:77] Creating layer relu2
I0521 01:17:50.539095 27299 net.cpp:106] Creating Layer relu2
I0521 01:17:50.539105 27299 net.cpp:454] relu2 <- conv2
I0521 01:17:50.539118 27299 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:17:50.539448 27299 net.cpp:150] Setting up relu2
I0521 01:17:50.539461 27299 net.cpp:157] Top shape: 510 20 54 46 (25336800)
I0521 01:17:50.539471 27299 net.cpp:165] Memory required for data: 568162440
I0521 01:17:50.539481 27299 layer_factory.hpp:77] Creating layer pool2
I0521 01:17:50.539494 27299 net.cpp:106] Creating Layer pool2
I0521 01:17:50.539504 27299 net.cpp:454] pool2 <- conv2
I0521 01:17:50.539528 27299 net.cpp:411] pool2 -> pool2
I0521 01:17:50.539597 27299 net.cpp:150] Setting up pool2
I0521 01:17:50.539610 27299 net.cpp:157] Top shape: 510 20 27 46 (12668400)
I0521 01:17:50.539619 27299 net.cpp:165] Memory required for data: 618836040
I0521 01:17:50.539629 27299 layer_factory.hpp:77] Creating layer conv3
I0521 01:17:50.539646 27299 net.cpp:106] Creating Layer conv3
I0521 01:17:50.539656 27299 net.cpp:454] conv3 <- pool2
I0521 01:17:50.539669 27299 net.cpp:411] conv3 -> conv3
I0521 01:17:50.541594 27299 net.cpp:150] Setting up conv3
I0521 01:17:50.541617 27299 net.cpp:157] Top shape: 510 28 22 44 (13823040)
I0521 01:17:50.541627 27299 net.cpp:165] Memory required for data: 674128200
I0521 01:17:50.541647 27299 layer_factory.hpp:77] Creating layer relu3
I0521 01:17:50.541663 27299 net.cpp:106] Creating Layer relu3
I0521 01:17:50.541673 27299 net.cpp:454] relu3 <- conv3
I0521 01:17:50.541687 27299 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:17:50.542152 27299 net.cpp:150] Setting up relu3
I0521 01:17:50.542170 27299 net.cpp:157] Top shape: 510 28 22 44 (13823040)
I0521 01:17:50.542181 27299 net.cpp:165] Memory required for data: 729420360
I0521 01:17:50.542189 27299 layer_factory.hpp:77] Creating layer pool3
I0521 01:17:50.542202 27299 net.cpp:106] Creating Layer pool3
I0521 01:17:50.542212 27299 net.cpp:454] pool3 <- conv3
I0521 01:17:50.542225 27299 net.cpp:411] pool3 -> pool3
I0521 01:17:50.542292 27299 net.cpp:150] Setting up pool3
I0521 01:17:50.542305 27299 net.cpp:157] Top shape: 510 28 11 44 (6911520)
I0521 01:17:50.542315 27299 net.cpp:165] Memory required for data: 757066440
I0521 01:17:50.542325 27299 layer_factory.hpp:77] Creating layer conv4
I0521 01:17:50.542342 27299 net.cpp:106] Creating Layer conv4
I0521 01:17:50.542353 27299 net.cpp:454] conv4 <- pool3
I0521 01:17:50.542366 27299 net.cpp:411] conv4 -> conv4
I0521 01:17:50.545156 27299 net.cpp:150] Setting up conv4
I0521 01:17:50.545182 27299 net.cpp:157] Top shape: 510 36 6 42 (4626720)
I0521 01:17:50.545194 27299 net.cpp:165] Memory required for data: 775573320
I0521 01:17:50.545210 27299 layer_factory.hpp:77] Creating layer relu4
I0521 01:17:50.545224 27299 net.cpp:106] Creating Layer relu4
I0521 01:17:50.545234 27299 net.cpp:454] relu4 <- conv4
I0521 01:17:50.545246 27299 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:17:50.545717 27299 net.cpp:150] Setting up relu4
I0521 01:17:50.545733 27299 net.cpp:157] Top shape: 510 36 6 42 (4626720)
I0521 01:17:50.545744 27299 net.cpp:165] Memory required for data: 794080200
I0521 01:17:50.545754 27299 layer_factory.hpp:77] Creating layer pool4
I0521 01:17:50.545766 27299 net.cpp:106] Creating Layer pool4
I0521 01:17:50.545776 27299 net.cpp:454] pool4 <- conv4
I0521 01:17:50.545789 27299 net.cpp:411] pool4 -> pool4
I0521 01:17:50.545857 27299 net.cpp:150] Setting up pool4
I0521 01:17:50.545871 27299 net.cpp:157] Top shape: 510 36 3 42 (2313360)
I0521 01:17:50.545881 27299 net.cpp:165] Memory required for data: 803333640
I0521 01:17:50.545892 27299 layer_factory.hpp:77] Creating layer ip1
I0521 01:17:50.545912 27299 net.cpp:106] Creating Layer ip1
I0521 01:17:50.545922 27299 net.cpp:454] ip1 <- pool4
I0521 01:17:50.545933 27299 net.cpp:411] ip1 -> ip1
I0521 01:17:50.561324 27299 net.cpp:150] Setting up ip1
I0521 01:17:50.561353 27299 net.cpp:157] Top shape: 510 196 (99960)
I0521 01:17:50.561365 27299 net.cpp:165] Memory required for data: 803733480
I0521 01:17:50.561391 27299 layer_factory.hpp:77] Creating layer relu5
I0521 01:17:50.561406 27299 net.cpp:106] Creating Layer relu5
I0521 01:17:50.561417 27299 net.cpp:454] relu5 <- ip1
I0521 01:17:50.561429 27299 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:17:50.561771 27299 net.cpp:150] Setting up relu5
I0521 01:17:50.561785 27299 net.cpp:157] Top shape: 510 196 (99960)
I0521 01:17:50.561796 27299 net.cpp:165] Memory required for data: 804133320
I0521 01:17:50.561806 27299 layer_factory.hpp:77] Creating layer drop1
I0521 01:17:50.561828 27299 net.cpp:106] Creating Layer drop1
I0521 01:17:50.561839 27299 net.cpp:454] drop1 <- ip1
I0521 01:17:50.561863 27299 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:17:50.561910 27299 net.cpp:150] Setting up drop1
I0521 01:17:50.561923 27299 net.cpp:157] Top shape: 510 196 (99960)
I0521 01:17:50.561933 27299 net.cpp:165] Memory required for data: 804533160
I0521 01:17:50.561944 27299 layer_factory.hpp:77] Creating layer ip2
I0521 01:17:50.561962 27299 net.cpp:106] Creating Layer ip2
I0521 01:17:50.561972 27299 net.cpp:454] ip2 <- ip1
I0521 01:17:50.561985 27299 net.cpp:411] ip2 -> ip2
I0521 01:17:50.562449 27299 net.cpp:150] Setting up ip2
I0521 01:17:50.562463 27299 net.cpp:157] Top shape: 510 98 (49980)
I0521 01:17:50.562472 27299 net.cpp:165] Memory required for data: 804733080
I0521 01:17:50.562487 27299 layer_factory.hpp:77] Creating layer relu6
I0521 01:17:50.562500 27299 net.cpp:106] Creating Layer relu6
I0521 01:17:50.562508 27299 net.cpp:454] relu6 <- ip2
I0521 01:17:50.562520 27299 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:17:50.563035 27299 net.cpp:150] Setting up relu6
I0521 01:17:50.563051 27299 net.cpp:157] Top shape: 510 98 (49980)
I0521 01:17:50.563062 27299 net.cpp:165] Memory required for data: 804933000
I0521 01:17:50.563074 27299 layer_factory.hpp:77] Creating layer drop2
I0521 01:17:50.563087 27299 net.cpp:106] Creating Layer drop2
I0521 01:17:50.563097 27299 net.cpp:454] drop2 <- ip2
I0521 01:17:50.563109 27299 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:17:50.563151 27299 net.cpp:150] Setting up drop2
I0521 01:17:50.563164 27299 net.cpp:157] Top shape: 510 98 (49980)
I0521 01:17:50.563175 27299 net.cpp:165] Memory required for data: 805132920
I0521 01:17:50.563184 27299 layer_factory.hpp:77] Creating layer ip3
I0521 01:17:50.563197 27299 net.cpp:106] Creating Layer ip3
I0521 01:17:50.563207 27299 net.cpp:454] ip3 <- ip2
I0521 01:17:50.563220 27299 net.cpp:411] ip3 -> ip3
I0521 01:17:50.563431 27299 net.cpp:150] Setting up ip3
I0521 01:17:50.563444 27299 net.cpp:157] Top shape: 510 11 (5610)
I0521 01:17:50.563454 27299 net.cpp:165] Memory required for data: 805155360
I0521 01:17:50.563469 27299 layer_factory.hpp:77] Creating layer drop3
I0521 01:17:50.563482 27299 net.cpp:106] Creating Layer drop3
I0521 01:17:50.563490 27299 net.cpp:454] drop3 <- ip3
I0521 01:17:50.563503 27299 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:17:50.563540 27299 net.cpp:150] Setting up drop3
I0521 01:17:50.563554 27299 net.cpp:157] Top shape: 510 11 (5610)
I0521 01:17:50.563565 27299 net.cpp:165] Memory required for data: 805177800
I0521 01:17:50.563573 27299 layer_factory.hpp:77] Creating layer loss
I0521 01:17:50.563592 27299 net.cpp:106] Creating Layer loss
I0521 01:17:50.563602 27299 net.cpp:454] loss <- ip3
I0521 01:17:50.563613 27299 net.cpp:454] loss <- label
I0521 01:17:50.563626 27299 net.cpp:411] loss -> loss
I0521 01:17:50.563642 27299 layer_factory.hpp:77] Creating layer loss
I0521 01:17:50.564297 27299 net.cpp:150] Setting up loss
I0521 01:17:50.564317 27299 net.cpp:157] Top shape: (1)
I0521 01:17:50.564332 27299 net.cpp:160]     with loss weight 1
I0521 01:17:50.564375 27299 net.cpp:165] Memory required for data: 805177804
I0521 01:17:50.564386 27299 net.cpp:226] loss needs backward computation.
I0521 01:17:50.564398 27299 net.cpp:226] drop3 needs backward computation.
I0521 01:17:50.564406 27299 net.cpp:226] ip3 needs backward computation.
I0521 01:17:50.564419 27299 net.cpp:226] drop2 needs backward computation.
I0521 01:17:50.564427 27299 net.cpp:226] relu6 needs backward computation.
I0521 01:17:50.564437 27299 net.cpp:226] ip2 needs backward computation.
I0521 01:17:50.564445 27299 net.cpp:226] drop1 needs backward computation.
I0521 01:17:50.564455 27299 net.cpp:226] relu5 needs backward computation.
I0521 01:17:50.564465 27299 net.cpp:226] ip1 needs backward computation.
I0521 01:17:50.564476 27299 net.cpp:226] pool4 needs backward computation.
I0521 01:17:50.564486 27299 net.cpp:226] relu4 needs backward computation.
I0521 01:17:50.564494 27299 net.cpp:226] conv4 needs backward computation.
I0521 01:17:50.564505 27299 net.cpp:226] pool3 needs backward computation.
I0521 01:17:50.564523 27299 net.cpp:226] relu3 needs backward computation.
I0521 01:17:50.564533 27299 net.cpp:226] conv3 needs backward computation.
I0521 01:17:50.564544 27299 net.cpp:226] pool2 needs backward computation.
I0521 01:17:50.564554 27299 net.cpp:226] relu2 needs backward computation.
I0521 01:17:50.564564 27299 net.cpp:226] conv2 needs backward computation.
I0521 01:17:50.564575 27299 net.cpp:226] pool1 needs backward computation.
I0521 01:17:50.564585 27299 net.cpp:226] relu1 needs backward computation.
I0521 01:17:50.564595 27299 net.cpp:226] conv1 needs backward computation.
I0521 01:17:50.564606 27299 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:17:50.564616 27299 net.cpp:270] This network produces output loss
I0521 01:17:50.564640 27299 net.cpp:283] Network initialization done.
I0521 01:17:50.566457 27299 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332.prototxt
I0521 01:17:50.566529 27299 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 01:17:50.566884 27299 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 510
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:17:50.567101 27299 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:17:50.567116 27299 net.cpp:106] Creating Layer data_hdf5
I0521 01:17:50.567128 27299 net.cpp:411] data_hdf5 -> data
I0521 01:17:50.567145 27299 net.cpp:411] data_hdf5 -> label
I0521 01:17:50.567160 27299 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 01:17:50.568362 27299 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 01:18:11.922862 27299 net.cpp:150] Setting up data_hdf5
I0521 01:18:11.923028 27299 net.cpp:157] Top shape: 510 1 127 50 (3238500)
I0521 01:18:11.923043 27299 net.cpp:157] Top shape: 510 (510)
I0521 01:18:11.923053 27299 net.cpp:165] Memory required for data: 12956040
I0521 01:18:11.923068 27299 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 01:18:11.923095 27299 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 01:18:11.923106 27299 net.cpp:454] label_data_hdf5_1_split <- label
I0521 01:18:11.923121 27299 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 01:18:11.923142 27299 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 01:18:11.923215 27299 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 01:18:11.923229 27299 net.cpp:157] Top shape: 510 (510)
I0521 01:18:11.923241 27299 net.cpp:157] Top shape: 510 (510)
I0521 01:18:11.923251 27299 net.cpp:165] Memory required for data: 12960120
I0521 01:18:11.923261 27299 layer_factory.hpp:77] Creating layer conv1
I0521 01:18:11.923283 27299 net.cpp:106] Creating Layer conv1
I0521 01:18:11.923293 27299 net.cpp:454] conv1 <- data
I0521 01:18:11.923307 27299 net.cpp:411] conv1 -> conv1
I0521 01:18:11.925237 27299 net.cpp:150] Setting up conv1
I0521 01:18:11.925261 27299 net.cpp:157] Top shape: 510 12 120 48 (35251200)
I0521 01:18:11.925272 27299 net.cpp:165] Memory required for data: 153964920
I0521 01:18:11.925293 27299 layer_factory.hpp:77] Creating layer relu1
I0521 01:18:11.925308 27299 net.cpp:106] Creating Layer relu1
I0521 01:18:11.925318 27299 net.cpp:454] relu1 <- conv1
I0521 01:18:11.925330 27299 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:18:11.925827 27299 net.cpp:150] Setting up relu1
I0521 01:18:11.925843 27299 net.cpp:157] Top shape: 510 12 120 48 (35251200)
I0521 01:18:11.925853 27299 net.cpp:165] Memory required for data: 294969720
I0521 01:18:11.925863 27299 layer_factory.hpp:77] Creating layer pool1
I0521 01:18:11.925879 27299 net.cpp:106] Creating Layer pool1
I0521 01:18:11.925889 27299 net.cpp:454] pool1 <- conv1
I0521 01:18:11.925901 27299 net.cpp:411] pool1 -> pool1
I0521 01:18:11.925976 27299 net.cpp:150] Setting up pool1
I0521 01:18:11.925989 27299 net.cpp:157] Top shape: 510 12 60 48 (17625600)
I0521 01:18:11.925999 27299 net.cpp:165] Memory required for data: 365472120
I0521 01:18:11.926007 27299 layer_factory.hpp:77] Creating layer conv2
I0521 01:18:11.926025 27299 net.cpp:106] Creating Layer conv2
I0521 01:18:11.926036 27299 net.cpp:454] conv2 <- pool1
I0521 01:18:11.926051 27299 net.cpp:411] conv2 -> conv2
I0521 01:18:11.927978 27299 net.cpp:150] Setting up conv2
I0521 01:18:11.927999 27299 net.cpp:157] Top shape: 510 20 54 46 (25336800)
I0521 01:18:11.928012 27299 net.cpp:165] Memory required for data: 466819320
I0521 01:18:11.928030 27299 layer_factory.hpp:77] Creating layer relu2
I0521 01:18:11.928043 27299 net.cpp:106] Creating Layer relu2
I0521 01:18:11.928053 27299 net.cpp:454] relu2 <- conv2
I0521 01:18:11.928066 27299 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:18:11.928400 27299 net.cpp:150] Setting up relu2
I0521 01:18:11.928414 27299 net.cpp:157] Top shape: 510 20 54 46 (25336800)
I0521 01:18:11.928424 27299 net.cpp:165] Memory required for data: 568166520
I0521 01:18:11.928434 27299 layer_factory.hpp:77] Creating layer pool2
I0521 01:18:11.928447 27299 net.cpp:106] Creating Layer pool2
I0521 01:18:11.928457 27299 net.cpp:454] pool2 <- conv2
I0521 01:18:11.928469 27299 net.cpp:411] pool2 -> pool2
I0521 01:18:11.928541 27299 net.cpp:150] Setting up pool2
I0521 01:18:11.928555 27299 net.cpp:157] Top shape: 510 20 27 46 (12668400)
I0521 01:18:11.928565 27299 net.cpp:165] Memory required for data: 618840120
I0521 01:18:11.928575 27299 layer_factory.hpp:77] Creating layer conv3
I0521 01:18:11.928593 27299 net.cpp:106] Creating Layer conv3
I0521 01:18:11.928603 27299 net.cpp:454] conv3 <- pool2
I0521 01:18:11.928617 27299 net.cpp:411] conv3 -> conv3
I0521 01:18:11.930589 27299 net.cpp:150] Setting up conv3
I0521 01:18:11.930613 27299 net.cpp:157] Top shape: 510 28 22 44 (13823040)
I0521 01:18:11.930624 27299 net.cpp:165] Memory required for data: 674132280
I0521 01:18:11.930657 27299 layer_factory.hpp:77] Creating layer relu3
I0521 01:18:11.930670 27299 net.cpp:106] Creating Layer relu3
I0521 01:18:11.930680 27299 net.cpp:454] relu3 <- conv3
I0521 01:18:11.930693 27299 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:18:11.931164 27299 net.cpp:150] Setting up relu3
I0521 01:18:11.931180 27299 net.cpp:157] Top shape: 510 28 22 44 (13823040)
I0521 01:18:11.931190 27299 net.cpp:165] Memory required for data: 729424440
I0521 01:18:11.931200 27299 layer_factory.hpp:77] Creating layer pool3
I0521 01:18:11.931213 27299 net.cpp:106] Creating Layer pool3
I0521 01:18:11.931223 27299 net.cpp:454] pool3 <- conv3
I0521 01:18:11.931236 27299 net.cpp:411] pool3 -> pool3
I0521 01:18:11.931309 27299 net.cpp:150] Setting up pool3
I0521 01:18:11.931323 27299 net.cpp:157] Top shape: 510 28 11 44 (6911520)
I0521 01:18:11.931331 27299 net.cpp:165] Memory required for data: 757070520
I0521 01:18:11.931341 27299 layer_factory.hpp:77] Creating layer conv4
I0521 01:18:11.931357 27299 net.cpp:106] Creating Layer conv4
I0521 01:18:11.931367 27299 net.cpp:454] conv4 <- pool3
I0521 01:18:11.931381 27299 net.cpp:411] conv4 -> conv4
I0521 01:18:11.933444 27299 net.cpp:150] Setting up conv4
I0521 01:18:11.933467 27299 net.cpp:157] Top shape: 510 36 6 42 (4626720)
I0521 01:18:11.933477 27299 net.cpp:165] Memory required for data: 775577400
I0521 01:18:11.933492 27299 layer_factory.hpp:77] Creating layer relu4
I0521 01:18:11.933506 27299 net.cpp:106] Creating Layer relu4
I0521 01:18:11.933516 27299 net.cpp:454] relu4 <- conv4
I0521 01:18:11.933528 27299 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:18:11.934000 27299 net.cpp:150] Setting up relu4
I0521 01:18:11.934016 27299 net.cpp:157] Top shape: 510 36 6 42 (4626720)
I0521 01:18:11.934027 27299 net.cpp:165] Memory required for data: 794084280
I0521 01:18:11.934037 27299 layer_factory.hpp:77] Creating layer pool4
I0521 01:18:11.934051 27299 net.cpp:106] Creating Layer pool4
I0521 01:18:11.934061 27299 net.cpp:454] pool4 <- conv4
I0521 01:18:11.934074 27299 net.cpp:411] pool4 -> pool4
I0521 01:18:11.934146 27299 net.cpp:150] Setting up pool4
I0521 01:18:11.934160 27299 net.cpp:157] Top shape: 510 36 3 42 (2313360)
I0521 01:18:11.934170 27299 net.cpp:165] Memory required for data: 803337720
I0521 01:18:11.934180 27299 layer_factory.hpp:77] Creating layer ip1
I0521 01:18:11.934195 27299 net.cpp:106] Creating Layer ip1
I0521 01:18:11.934206 27299 net.cpp:454] ip1 <- pool4
I0521 01:18:11.934221 27299 net.cpp:411] ip1 -> ip1
I0521 01:18:11.949673 27299 net.cpp:150] Setting up ip1
I0521 01:18:11.949702 27299 net.cpp:157] Top shape: 510 196 (99960)
I0521 01:18:11.949717 27299 net.cpp:165] Memory required for data: 803737560
I0521 01:18:11.949738 27299 layer_factory.hpp:77] Creating layer relu5
I0521 01:18:11.949753 27299 net.cpp:106] Creating Layer relu5
I0521 01:18:11.949764 27299 net.cpp:454] relu5 <- ip1
I0521 01:18:11.949781 27299 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:18:11.950125 27299 net.cpp:150] Setting up relu5
I0521 01:18:11.950139 27299 net.cpp:157] Top shape: 510 196 (99960)
I0521 01:18:11.950150 27299 net.cpp:165] Memory required for data: 804137400
I0521 01:18:11.950160 27299 layer_factory.hpp:77] Creating layer drop1
I0521 01:18:11.950177 27299 net.cpp:106] Creating Layer drop1
I0521 01:18:11.950187 27299 net.cpp:454] drop1 <- ip1
I0521 01:18:11.950201 27299 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:18:11.950244 27299 net.cpp:150] Setting up drop1
I0521 01:18:11.950258 27299 net.cpp:157] Top shape: 510 196 (99960)
I0521 01:18:11.950266 27299 net.cpp:165] Memory required for data: 804537240
I0521 01:18:11.950278 27299 layer_factory.hpp:77] Creating layer ip2
I0521 01:18:11.950291 27299 net.cpp:106] Creating Layer ip2
I0521 01:18:11.950301 27299 net.cpp:454] ip2 <- ip1
I0521 01:18:11.950314 27299 net.cpp:411] ip2 -> ip2
I0521 01:18:11.950794 27299 net.cpp:150] Setting up ip2
I0521 01:18:11.950808 27299 net.cpp:157] Top shape: 510 98 (49980)
I0521 01:18:11.950817 27299 net.cpp:165] Memory required for data: 804737160
I0521 01:18:11.950845 27299 layer_factory.hpp:77] Creating layer relu6
I0521 01:18:11.950858 27299 net.cpp:106] Creating Layer relu6
I0521 01:18:11.950868 27299 net.cpp:454] relu6 <- ip2
I0521 01:18:11.950881 27299 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:18:11.951402 27299 net.cpp:150] Setting up relu6
I0521 01:18:11.951419 27299 net.cpp:157] Top shape: 510 98 (49980)
I0521 01:18:11.951426 27299 net.cpp:165] Memory required for data: 804937080
I0521 01:18:11.951436 27299 layer_factory.hpp:77] Creating layer drop2
I0521 01:18:11.951450 27299 net.cpp:106] Creating Layer drop2
I0521 01:18:11.951460 27299 net.cpp:454] drop2 <- ip2
I0521 01:18:11.951473 27299 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:18:11.951517 27299 net.cpp:150] Setting up drop2
I0521 01:18:11.951530 27299 net.cpp:157] Top shape: 510 98 (49980)
I0521 01:18:11.951539 27299 net.cpp:165] Memory required for data: 805137000
I0521 01:18:11.951550 27299 layer_factory.hpp:77] Creating layer ip3
I0521 01:18:11.951563 27299 net.cpp:106] Creating Layer ip3
I0521 01:18:11.951573 27299 net.cpp:454] ip3 <- ip2
I0521 01:18:11.951587 27299 net.cpp:411] ip3 -> ip3
I0521 01:18:11.951809 27299 net.cpp:150] Setting up ip3
I0521 01:18:11.951830 27299 net.cpp:157] Top shape: 510 11 (5610)
I0521 01:18:11.951840 27299 net.cpp:165] Memory required for data: 805159440
I0521 01:18:11.951855 27299 layer_factory.hpp:77] Creating layer drop3
I0521 01:18:11.951869 27299 net.cpp:106] Creating Layer drop3
I0521 01:18:11.951879 27299 net.cpp:454] drop3 <- ip3
I0521 01:18:11.951889 27299 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:18:11.951930 27299 net.cpp:150] Setting up drop3
I0521 01:18:11.951943 27299 net.cpp:157] Top shape: 510 11 (5610)
I0521 01:18:11.951953 27299 net.cpp:165] Memory required for data: 805181880
I0521 01:18:11.951962 27299 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 01:18:11.951977 27299 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 01:18:11.951987 27299 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 01:18:11.951999 27299 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 01:18:11.952015 27299 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 01:18:11.952087 27299 net.cpp:150] Setting up ip3_drop3_0_split
I0521 01:18:11.952100 27299 net.cpp:157] Top shape: 510 11 (5610)
I0521 01:18:11.952113 27299 net.cpp:157] Top shape: 510 11 (5610)
I0521 01:18:11.952122 27299 net.cpp:165] Memory required for data: 805226760
I0521 01:18:11.952132 27299 layer_factory.hpp:77] Creating layer accuracy
I0521 01:18:11.952153 27299 net.cpp:106] Creating Layer accuracy
I0521 01:18:11.952163 27299 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 01:18:11.952174 27299 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 01:18:11.952188 27299 net.cpp:411] accuracy -> accuracy
I0521 01:18:11.952211 27299 net.cpp:150] Setting up accuracy
I0521 01:18:11.952224 27299 net.cpp:157] Top shape: (1)
I0521 01:18:11.952234 27299 net.cpp:165] Memory required for data: 805226764
I0521 01:18:11.952244 27299 layer_factory.hpp:77] Creating layer loss
I0521 01:18:11.952257 27299 net.cpp:106] Creating Layer loss
I0521 01:18:11.952266 27299 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 01:18:11.952277 27299 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 01:18:11.952291 27299 net.cpp:411] loss -> loss
I0521 01:18:11.952308 27299 layer_factory.hpp:77] Creating layer loss
I0521 01:18:11.952800 27299 net.cpp:150] Setting up loss
I0521 01:18:11.952814 27299 net.cpp:157] Top shape: (1)
I0521 01:18:11.952823 27299 net.cpp:160]     with loss weight 1
I0521 01:18:11.952844 27299 net.cpp:165] Memory required for data: 805226768
I0521 01:18:11.952854 27299 net.cpp:226] loss needs backward computation.
I0521 01:18:11.952865 27299 net.cpp:228] accuracy does not need backward computation.
I0521 01:18:11.952877 27299 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 01:18:11.952886 27299 net.cpp:226] drop3 needs backward computation.
I0521 01:18:11.952896 27299 net.cpp:226] ip3 needs backward computation.
I0521 01:18:11.952906 27299 net.cpp:226] drop2 needs backward computation.
I0521 01:18:11.952925 27299 net.cpp:226] relu6 needs backward computation.
I0521 01:18:11.952935 27299 net.cpp:226] ip2 needs backward computation.
I0521 01:18:11.952945 27299 net.cpp:226] drop1 needs backward computation.
I0521 01:18:11.952955 27299 net.cpp:226] relu5 needs backward computation.
I0521 01:18:11.952965 27299 net.cpp:226] ip1 needs backward computation.
I0521 01:18:11.952975 27299 net.cpp:226] pool4 needs backward computation.
I0521 01:18:11.952985 27299 net.cpp:226] relu4 needs backward computation.
I0521 01:18:11.952994 27299 net.cpp:226] conv4 needs backward computation.
I0521 01:18:11.953006 27299 net.cpp:226] pool3 needs backward computation.
I0521 01:18:11.953016 27299 net.cpp:226] relu3 needs backward computation.
I0521 01:18:11.953023 27299 net.cpp:226] conv3 needs backward computation.
I0521 01:18:11.953034 27299 net.cpp:226] pool2 needs backward computation.
I0521 01:18:11.953044 27299 net.cpp:226] relu2 needs backward computation.
I0521 01:18:11.953054 27299 net.cpp:226] conv2 needs backward computation.
I0521 01:18:11.953064 27299 net.cpp:226] pool1 needs backward computation.
I0521 01:18:11.953075 27299 net.cpp:226] relu1 needs backward computation.
I0521 01:18:11.953084 27299 net.cpp:226] conv1 needs backward computation.
I0521 01:18:11.953096 27299 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 01:18:11.953107 27299 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:18:11.953117 27299 net.cpp:270] This network produces output accuracy
I0521 01:18:11.953128 27299 net.cpp:270] This network produces output loss
I0521 01:18:11.953155 27299 net.cpp:283] Network initialization done.
I0521 01:18:11.953290 27299 solver.cpp:60] Solver scaffolding done.
I0521 01:18:11.954421 27299 caffe.cpp:212] Starting Optimization
I0521 01:18:11.954439 27299 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 01:18:11.954453 27299 solver.cpp:289] Learning Rate Policy: fixed
I0521 01:18:11.955657 27299 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 01:18:57.981214 27299 solver.cpp:409]     Test net output #0: accuracy = 0.125397
I0521 01:18:57.981386 27299 solver.cpp:409]     Test net output #1: loss = 2.39688 (* 1 = 2.39688 loss)
I0521 01:18:58.081707 27299 solver.cpp:237] Iteration 0, loss = 2.39686
I0521 01:18:58.081743 27299 solver.cpp:253]     Train net output #0: loss = 2.39686 (* 1 = 2.39686 loss)
I0521 01:18:58.081763 27299 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 01:19:06.073606 27299 solver.cpp:237] Iteration 29, loss = 2.37749
I0521 01:19:06.073642 27299 solver.cpp:253]     Train net output #0: loss = 2.37749 (* 1 = 2.37749 loss)
I0521 01:19:06.073658 27299 sgd_solver.cpp:106] Iteration 29, lr = 0.0025
I0521 01:19:14.071578 27299 solver.cpp:237] Iteration 58, loss = 2.35504
I0521 01:19:14.071610 27299 solver.cpp:253]     Train net output #0: loss = 2.35504 (* 1 = 2.35504 loss)
I0521 01:19:14.071626 27299 sgd_solver.cpp:106] Iteration 58, lr = 0.0025
I0521 01:19:22.069082 27299 solver.cpp:237] Iteration 87, loss = 2.33469
I0521 01:19:22.069133 27299 solver.cpp:253]     Train net output #0: loss = 2.33469 (* 1 = 2.33469 loss)
I0521 01:19:22.069149 27299 sgd_solver.cpp:106] Iteration 87, lr = 0.0025
I0521 01:19:30.063997 27299 solver.cpp:237] Iteration 116, loss = 2.35283
I0521 01:19:30.064142 27299 solver.cpp:253]     Train net output #0: loss = 2.35283 (* 1 = 2.35283 loss)
I0521 01:19:30.064157 27299 sgd_solver.cpp:106] Iteration 116, lr = 0.0025
I0521 01:19:38.058267 27299 solver.cpp:237] Iteration 145, loss = 2.33191
I0521 01:19:38.058300 27299 solver.cpp:253]     Train net output #0: loss = 2.33191 (* 1 = 2.33191 loss)
I0521 01:19:38.058316 27299 sgd_solver.cpp:106] Iteration 145, lr = 0.0025
I0521 01:19:46.049484 27299 solver.cpp:237] Iteration 174, loss = 2.33175
I0521 01:19:46.049527 27299 solver.cpp:253]     Train net output #0: loss = 2.33175 (* 1 = 2.33175 loss)
I0521 01:19:46.049546 27299 sgd_solver.cpp:106] Iteration 174, lr = 0.0025
I0521 01:20:16.156172 27299 solver.cpp:237] Iteration 203, loss = 2.31658
I0521 01:20:16.156337 27299 solver.cpp:253]     Train net output #0: loss = 2.31658 (* 1 = 2.31658 loss)
I0521 01:20:16.156352 27299 sgd_solver.cpp:106] Iteration 203, lr = 0.0025
I0521 01:20:24.149142 27299 solver.cpp:237] Iteration 232, loss = 2.31577
I0521 01:20:24.149173 27299 solver.cpp:253]     Train net output #0: loss = 2.31577 (* 1 = 2.31577 loss)
I0521 01:20:24.149191 27299 sgd_solver.cpp:106] Iteration 232, lr = 0.0025
I0521 01:20:32.144451 27299 solver.cpp:237] Iteration 261, loss = 2.3159
I0521 01:20:32.144485 27299 solver.cpp:253]     Train net output #0: loss = 2.3159 (* 1 = 2.3159 loss)
I0521 01:20:32.144501 27299 sgd_solver.cpp:106] Iteration 261, lr = 0.0025
I0521 01:20:40.140393 27299 solver.cpp:237] Iteration 290, loss = 2.26566
I0521 01:20:40.140439 27299 solver.cpp:253]     Train net output #0: loss = 2.26566 (* 1 = 2.26566 loss)
I0521 01:20:40.140455 27299 sgd_solver.cpp:106] Iteration 290, lr = 0.0025
I0521 01:20:40.967386 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_294.caffemodel
I0521 01:20:41.200870 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_294.solverstate
I0521 01:20:48.196725 27299 solver.cpp:237] Iteration 319, loss = 2.23598
I0521 01:20:48.196880 27299 solver.cpp:253]     Train net output #0: loss = 2.23598 (* 1 = 2.23598 loss)
I0521 01:20:48.196894 27299 sgd_solver.cpp:106] Iteration 319, lr = 0.0025
I0521 01:20:56.192844 27299 solver.cpp:237] Iteration 348, loss = 2.24378
I0521 01:20:56.192878 27299 solver.cpp:253]     Train net output #0: loss = 2.24378 (* 1 = 2.24378 loss)
I0521 01:20:56.192894 27299 sgd_solver.cpp:106] Iteration 348, lr = 0.0025
I0521 01:21:04.182116 27299 solver.cpp:237] Iteration 377, loss = 2.19143
I0521 01:21:04.182165 27299 solver.cpp:253]     Train net output #0: loss = 2.19143 (* 1 = 2.19143 loss)
I0521 01:21:04.182180 27299 sgd_solver.cpp:106] Iteration 377, lr = 0.0025
I0521 01:21:34.289463 27299 solver.cpp:237] Iteration 406, loss = 2.16528
I0521 01:21:34.289619 27299 solver.cpp:253]     Train net output #0: loss = 2.16528 (* 1 = 2.16528 loss)
I0521 01:21:34.289635 27299 sgd_solver.cpp:106] Iteration 406, lr = 0.0025
I0521 01:21:42.288600 27299 solver.cpp:237] Iteration 435, loss = 2.06471
I0521 01:21:42.288633 27299 solver.cpp:253]     Train net output #0: loss = 2.06471 (* 1 = 2.06471 loss)
I0521 01:21:42.288650 27299 sgd_solver.cpp:106] Iteration 435, lr = 0.0025
I0521 01:21:50.287552 27299 solver.cpp:237] Iteration 464, loss = 2.10637
I0521 01:21:50.287597 27299 solver.cpp:253]     Train net output #0: loss = 2.10637 (* 1 = 2.10637 loss)
I0521 01:21:50.287616 27299 sgd_solver.cpp:106] Iteration 464, lr = 0.0025
I0521 01:21:58.286689 27299 solver.cpp:237] Iteration 493, loss = 2.03964
I0521 01:21:58.286722 27299 solver.cpp:253]     Train net output #0: loss = 2.03964 (* 1 = 2.03964 loss)
I0521 01:21:58.286737 27299 sgd_solver.cpp:106] Iteration 493, lr = 0.0025
I0521 01:22:06.286295 27299 solver.cpp:237] Iteration 522, loss = 1.98714
I0521 01:22:06.286440 27299 solver.cpp:253]     Train net output #0: loss = 1.98714 (* 1 = 1.98714 loss)
I0521 01:22:06.286454 27299 sgd_solver.cpp:106] Iteration 522, lr = 0.0025
I0521 01:22:14.285823 27299 solver.cpp:237] Iteration 551, loss = 2.02409
I0521 01:22:14.285866 27299 solver.cpp:253]     Train net output #0: loss = 2.02409 (* 1 = 2.02409 loss)
I0521 01:22:14.285879 27299 sgd_solver.cpp:106] Iteration 551, lr = 0.0025
I0521 01:22:22.275360 27299 solver.cpp:237] Iteration 580, loss = 1.95208
I0521 01:22:22.275394 27299 solver.cpp:253]     Train net output #0: loss = 1.95208 (* 1 = 1.95208 loss)
I0521 01:22:22.275409 27299 sgd_solver.cpp:106] Iteration 580, lr = 0.0025
I0521 01:22:24.204567 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_588.caffemodel
I0521 01:22:24.436211 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_588.solverstate
I0521 01:22:24.461649 27299 solver.cpp:341] Iteration 588, Testing net (#0)
I0521 01:23:09.562434 27299 solver.cpp:409]     Test net output #0: accuracy = 0.539449
I0521 01:23:09.562597 27299 solver.cpp:409]     Test net output #1: loss = 1.71196 (* 1 = 1.71196 loss)
I0521 01:23:37.579880 27299 solver.cpp:237] Iteration 609, loss = 1.96101
I0521 01:23:37.579936 27299 solver.cpp:253]     Train net output #0: loss = 1.96101 (* 1 = 1.96101 loss)
I0521 01:23:37.579949 27299 sgd_solver.cpp:106] Iteration 609, lr = 0.0025
I0521 01:23:45.566702 27299 solver.cpp:237] Iteration 638, loss = 1.94875
I0521 01:23:45.566848 27299 solver.cpp:253]     Train net output #0: loss = 1.94875 (* 1 = 1.94875 loss)
I0521 01:23:45.566876 27299 sgd_solver.cpp:106] Iteration 638, lr = 0.0025
I0521 01:23:53.553767 27299 solver.cpp:237] Iteration 667, loss = 1.95019
I0521 01:23:53.553800 27299 solver.cpp:253]     Train net output #0: loss = 1.95019 (* 1 = 1.95019 loss)
I0521 01:23:53.553815 27299 sgd_solver.cpp:106] Iteration 667, lr = 0.0025
I0521 01:24:01.539202 27299 solver.cpp:237] Iteration 696, loss = 1.89263
I0521 01:24:01.539250 27299 solver.cpp:253]     Train net output #0: loss = 1.89263 (* 1 = 1.89263 loss)
I0521 01:24:01.539264 27299 sgd_solver.cpp:106] Iteration 696, lr = 0.0025
I0521 01:24:09.530714 27299 solver.cpp:237] Iteration 725, loss = 1.90995
I0521 01:24:09.530748 27299 solver.cpp:253]     Train net output #0: loss = 1.90995 (* 1 = 1.90995 loss)
I0521 01:24:09.530762 27299 sgd_solver.cpp:106] Iteration 725, lr = 0.0025
I0521 01:24:17.523767 27299 solver.cpp:237] Iteration 754, loss = 1.89086
I0521 01:24:17.523906 27299 solver.cpp:253]     Train net output #0: loss = 1.89086 (* 1 = 1.89086 loss)
I0521 01:24:17.523921 27299 sgd_solver.cpp:106] Iteration 754, lr = 0.0025
I0521 01:24:25.513953 27299 solver.cpp:237] Iteration 783, loss = 1.9267
I0521 01:24:25.513995 27299 solver.cpp:253]     Train net output #0: loss = 1.9267 (* 1 = 1.9267 loss)
I0521 01:24:25.514014 27299 sgd_solver.cpp:106] Iteration 783, lr = 0.0025
I0521 01:24:55.674011 27299 solver.cpp:237] Iteration 812, loss = 1.89002
I0521 01:24:55.674188 27299 solver.cpp:253]     Train net output #0: loss = 1.89002 (* 1 = 1.89002 loss)
I0521 01:24:55.674203 27299 sgd_solver.cpp:106] Iteration 812, lr = 0.0025
I0521 01:25:03.655707 27299 solver.cpp:237] Iteration 841, loss = 1.82394
I0521 01:25:03.655740 27299 solver.cpp:253]     Train net output #0: loss = 1.82394 (* 1 = 1.82394 loss)
I0521 01:25:03.655755 27299 sgd_solver.cpp:106] Iteration 841, lr = 0.0025
I0521 01:25:11.649317 27299 solver.cpp:237] Iteration 870, loss = 1.85333
I0521 01:25:11.649365 27299 solver.cpp:253]     Train net output #0: loss = 1.85333 (* 1 = 1.85333 loss)
I0521 01:25:11.649379 27299 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 01:25:14.676254 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_882.caffemodel
I0521 01:25:14.909842 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_882.solverstate
I0521 01:25:19.701342 27299 solver.cpp:237] Iteration 899, loss = 1.84258
I0521 01:25:19.701393 27299 solver.cpp:253]     Train net output #0: loss = 1.84258 (* 1 = 1.84258 loss)
I0521 01:25:19.701408 27299 sgd_solver.cpp:106] Iteration 899, lr = 0.0025
I0521 01:25:27.686297 27299 solver.cpp:237] Iteration 928, loss = 1.86097
I0521 01:25:27.686439 27299 solver.cpp:253]     Train net output #0: loss = 1.86097 (* 1 = 1.86097 loss)
I0521 01:25:27.686453 27299 sgd_solver.cpp:106] Iteration 928, lr = 0.0025
I0521 01:25:35.671937 27299 solver.cpp:237] Iteration 957, loss = 1.72334
I0521 01:25:35.671990 27299 solver.cpp:253]     Train net output #0: loss = 1.72334 (* 1 = 1.72334 loss)
I0521 01:25:35.672003 27299 sgd_solver.cpp:106] Iteration 957, lr = 0.0025
I0521 01:26:05.817060 27299 solver.cpp:237] Iteration 986, loss = 1.89612
I0521 01:26:05.817227 27299 solver.cpp:253]     Train net output #0: loss = 1.89612 (* 1 = 1.89612 loss)
I0521 01:26:05.817241 27299 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0521 01:26:13.802425 27299 solver.cpp:237] Iteration 1015, loss = 1.78538
I0521 01:26:13.802456 27299 solver.cpp:253]     Train net output #0: loss = 1.78538 (* 1 = 1.78538 loss)
I0521 01:26:13.802472 27299 sgd_solver.cpp:106] Iteration 1015, lr = 0.0025
I0521 01:26:21.790113 27299 solver.cpp:237] Iteration 1044, loss = 1.78948
I0521 01:26:21.790148 27299 solver.cpp:253]     Train net output #0: loss = 1.78948 (* 1 = 1.78948 loss)
I0521 01:26:21.790163 27299 sgd_solver.cpp:106] Iteration 1044, lr = 0.0025
I0521 01:26:29.775756 27299 solver.cpp:237] Iteration 1073, loss = 1.89304
I0521 01:26:29.775804 27299 solver.cpp:253]     Train net output #0: loss = 1.89304 (* 1 = 1.89304 loss)
I0521 01:26:29.775825 27299 sgd_solver.cpp:106] Iteration 1073, lr = 0.0025
I0521 01:26:37.764602 27299 solver.cpp:237] Iteration 1102, loss = 1.77052
I0521 01:26:37.764739 27299 solver.cpp:253]     Train net output #0: loss = 1.77052 (* 1 = 1.77052 loss)
I0521 01:26:37.764752 27299 sgd_solver.cpp:106] Iteration 1102, lr = 0.0025
I0521 01:26:45.750182 27299 solver.cpp:237] Iteration 1131, loss = 1.81155
I0521 01:26:45.750214 27299 solver.cpp:253]     Train net output #0: loss = 1.81155 (* 1 = 1.81155 loss)
I0521 01:26:45.750231 27299 sgd_solver.cpp:106] Iteration 1131, lr = 0.0025
I0521 01:26:53.735677 27299 solver.cpp:237] Iteration 1160, loss = 1.69052
I0521 01:26:53.735715 27299 solver.cpp:253]     Train net output #0: loss = 1.69052 (* 1 = 1.69052 loss)
I0521 01:26:53.735729 27299 sgd_solver.cpp:106] Iteration 1160, lr = 0.0025
I0521 01:26:57.862468 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_1176.caffemodel
I0521 01:26:58.095752 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_1176.solverstate
I0521 01:26:58.124223 27299 solver.cpp:341] Iteration 1176, Testing net (#0)
I0521 01:28:04.028929 27299 solver.cpp:409]     Test net output #0: accuracy = 0.644044
I0521 01:28:04.029105 27299 solver.cpp:409]     Test net output #1: loss = 1.32483 (* 1 = 1.32483 loss)
I0521 01:28:29.821187 27299 solver.cpp:237] Iteration 1189, loss = 1.846
I0521 01:28:29.821238 27299 solver.cpp:253]     Train net output #0: loss = 1.846 (* 1 = 1.846 loss)
I0521 01:28:29.821255 27299 sgd_solver.cpp:106] Iteration 1189, lr = 0.0025
I0521 01:28:37.800874 27299 solver.cpp:237] Iteration 1218, loss = 1.81102
I0521 01:28:37.801019 27299 solver.cpp:253]     Train net output #0: loss = 1.81102 (* 1 = 1.81102 loss)
I0521 01:28:37.801033 27299 sgd_solver.cpp:106] Iteration 1218, lr = 0.0025
I0521 01:28:45.778844 27299 solver.cpp:237] Iteration 1247, loss = 1.75551
I0521 01:28:45.778877 27299 solver.cpp:253]     Train net output #0: loss = 1.75551 (* 1 = 1.75551 loss)
I0521 01:28:45.778892 27299 sgd_solver.cpp:106] Iteration 1247, lr = 0.0025
I0521 01:28:53.760213 27299 solver.cpp:237] Iteration 1276, loss = 1.71501
I0521 01:28:53.760246 27299 solver.cpp:253]     Train net output #0: loss = 1.71501 (* 1 = 1.71501 loss)
I0521 01:28:53.760260 27299 sgd_solver.cpp:106] Iteration 1276, lr = 0.0025
I0521 01:29:01.743520 27299 solver.cpp:237] Iteration 1305, loss = 1.77305
I0521 01:29:01.743551 27299 solver.cpp:253]     Train net output #0: loss = 1.77305 (* 1 = 1.77305 loss)
I0521 01:29:01.743562 27299 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0521 01:29:09.722411 27299 solver.cpp:237] Iteration 1334, loss = 1.72249
I0521 01:29:09.722546 27299 solver.cpp:253]     Train net output #0: loss = 1.72249 (* 1 = 1.72249 loss)
I0521 01:29:09.722559 27299 sgd_solver.cpp:106] Iteration 1334, lr = 0.0025
I0521 01:29:17.699811 27299 solver.cpp:237] Iteration 1363, loss = 1.71691
I0521 01:29:17.699847 27299 solver.cpp:253]     Train net output #0: loss = 1.71691 (* 1 = 1.71691 loss)
I0521 01:29:17.699863 27299 sgd_solver.cpp:106] Iteration 1363, lr = 0.0025
I0521 01:29:47.814620 27299 solver.cpp:237] Iteration 1392, loss = 1.73707
I0521 01:29:47.814780 27299 solver.cpp:253]     Train net output #0: loss = 1.73707 (* 1 = 1.73707 loss)
I0521 01:29:47.814796 27299 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 01:29:55.794862 27299 solver.cpp:237] Iteration 1421, loss = 1.81459
I0521 01:29:55.794895 27299 solver.cpp:253]     Train net output #0: loss = 1.81459 (* 1 = 1.81459 loss)
I0521 01:29:55.794910 27299 sgd_solver.cpp:106] Iteration 1421, lr = 0.0025
I0521 01:30:03.780498 27299 solver.cpp:237] Iteration 1450, loss = 1.7677
I0521 01:30:03.780531 27299 solver.cpp:253]     Train net output #0: loss = 1.7677 (* 1 = 1.7677 loss)
I0521 01:30:03.780546 27299 sgd_solver.cpp:106] Iteration 1450, lr = 0.0025
I0521 01:30:09.007582 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_1470.caffemodel
I0521 01:30:09.243458 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_1470.solverstate
I0521 01:30:11.831634 27299 solver.cpp:237] Iteration 1479, loss = 1.71197
I0521 01:30:11.831686 27299 solver.cpp:253]     Train net output #0: loss = 1.71197 (* 1 = 1.71197 loss)
I0521 01:30:11.831699 27299 sgd_solver.cpp:106] Iteration 1479, lr = 0.0025
I0521 01:30:19.812177 27299 solver.cpp:237] Iteration 1508, loss = 1.69816
I0521 01:30:19.812319 27299 solver.cpp:253]     Train net output #0: loss = 1.69816 (* 1 = 1.69816 loss)
I0521 01:30:19.812332 27299 sgd_solver.cpp:106] Iteration 1508, lr = 0.0025
I0521 01:30:27.793397 27299 solver.cpp:237] Iteration 1537, loss = 1.67922
I0521 01:30:27.793429 27299 solver.cpp:253]     Train net output #0: loss = 1.67922 (* 1 = 1.67922 loss)
I0521 01:30:27.793445 27299 sgd_solver.cpp:106] Iteration 1537, lr = 0.0025
I0521 01:30:35.771545 27299 solver.cpp:237] Iteration 1566, loss = 1.71051
I0521 01:30:35.771585 27299 solver.cpp:253]     Train net output #0: loss = 1.71051 (* 1 = 1.71051 loss)
I0521 01:30:35.771605 27299 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 01:31:05.975039 27299 solver.cpp:237] Iteration 1595, loss = 1.79928
I0521 01:31:05.975220 27299 solver.cpp:253]     Train net output #0: loss = 1.79928 (* 1 = 1.79928 loss)
I0521 01:31:05.975236 27299 sgd_solver.cpp:106] Iteration 1595, lr = 0.0025
I0521 01:31:13.954249 27299 solver.cpp:237] Iteration 1624, loss = 1.67989
I0521 01:31:13.954282 27299 solver.cpp:253]     Train net output #0: loss = 1.67989 (* 1 = 1.67989 loss)
I0521 01:31:13.954298 27299 sgd_solver.cpp:106] Iteration 1624, lr = 0.0025
I0521 01:31:21.934262 27299 solver.cpp:237] Iteration 1653, loss = 1.72094
I0521 01:31:21.934294 27299 solver.cpp:253]     Train net output #0: loss = 1.72094 (* 1 = 1.72094 loss)
I0521 01:31:21.934309 27299 sgd_solver.cpp:106] Iteration 1653, lr = 0.0025
I0521 01:31:29.921874 27299 solver.cpp:237] Iteration 1682, loss = 1.72131
I0521 01:31:29.921917 27299 solver.cpp:253]     Train net output #0: loss = 1.72131 (* 1 = 1.72131 loss)
I0521 01:31:29.921936 27299 sgd_solver.cpp:106] Iteration 1682, lr = 0.0025
I0521 01:31:37.901088 27299 solver.cpp:237] Iteration 1711, loss = 1.7365
I0521 01:31:37.901226 27299 solver.cpp:253]     Train net output #0: loss = 1.7365 (* 1 = 1.7365 loss)
I0521 01:31:37.901240 27299 sgd_solver.cpp:106] Iteration 1711, lr = 0.0025
I0521 01:31:45.881322 27299 solver.cpp:237] Iteration 1740, loss = 1.77682
I0521 01:31:45.881355 27299 solver.cpp:253]     Train net output #0: loss = 1.77682 (* 1 = 1.77682 loss)
I0521 01:31:45.881371 27299 sgd_solver.cpp:106] Iteration 1740, lr = 0.0025
I0521 01:31:52.208549 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_1764.caffemodel
I0521 01:31:52.438696 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_1764.solverstate
I0521 01:31:52.464620 27299 solver.cpp:341] Iteration 1764, Testing net (#0)
I0521 01:32:37.274677 27299 solver.cpp:409]     Test net output #0: accuracy = 0.665519
I0521 01:32:37.274844 27299 solver.cpp:409]     Test net output #1: loss = 1.13619 (* 1 = 1.13619 loss)
I0521 01:33:00.860420 27299 solver.cpp:237] Iteration 1769, loss = 1.63714
I0521 01:33:00.860474 27299 solver.cpp:253]     Train net output #0: loss = 1.63714 (* 1 = 1.63714 loss)
I0521 01:33:00.860489 27299 sgd_solver.cpp:106] Iteration 1769, lr = 0.0025
I0521 01:33:08.844008 27299 solver.cpp:237] Iteration 1798, loss = 1.72701
I0521 01:33:08.844166 27299 solver.cpp:253]     Train net output #0: loss = 1.72701 (* 1 = 1.72701 loss)
I0521 01:33:08.844180 27299 sgd_solver.cpp:106] Iteration 1798, lr = 0.0025
I0521 01:33:16.833709 27299 solver.cpp:237] Iteration 1827, loss = 1.63704
I0521 01:33:16.833741 27299 solver.cpp:253]     Train net output #0: loss = 1.63704 (* 1 = 1.63704 loss)
I0521 01:33:16.833757 27299 sgd_solver.cpp:106] Iteration 1827, lr = 0.0025
I0521 01:33:24.820909 27299 solver.cpp:237] Iteration 1856, loss = 1.73154
I0521 01:33:24.820941 27299 solver.cpp:253]     Train net output #0: loss = 1.73154 (* 1 = 1.73154 loss)
I0521 01:33:24.820956 27299 sgd_solver.cpp:106] Iteration 1856, lr = 0.0025
I0521 01:33:32.808080 27299 solver.cpp:237] Iteration 1885, loss = 1.62295
I0521 01:33:32.808118 27299 solver.cpp:253]     Train net output #0: loss = 1.62295 (* 1 = 1.62295 loss)
I0521 01:33:32.808140 27299 sgd_solver.cpp:106] Iteration 1885, lr = 0.0025
I0521 01:33:40.794111 27299 solver.cpp:237] Iteration 1914, loss = 1.72121
I0521 01:33:40.794246 27299 solver.cpp:253]     Train net output #0: loss = 1.72121 (* 1 = 1.72121 loss)
I0521 01:33:40.794260 27299 sgd_solver.cpp:106] Iteration 1914, lr = 0.0025
I0521 01:33:48.780813 27299 solver.cpp:237] Iteration 1943, loss = 1.66101
I0521 01:33:48.780846 27299 solver.cpp:253]     Train net output #0: loss = 1.66101 (* 1 = 1.66101 loss)
I0521 01:33:48.780861 27299 sgd_solver.cpp:106] Iteration 1943, lr = 0.0025
I0521 01:34:18.899881 27299 solver.cpp:237] Iteration 1972, loss = 1.67715
I0521 01:34:18.900060 27299 solver.cpp:253]     Train net output #0: loss = 1.67715 (* 1 = 1.67715 loss)
I0521 01:34:18.900075 27299 sgd_solver.cpp:106] Iteration 1972, lr = 0.0025
I0521 01:34:26.886355 27299 solver.cpp:237] Iteration 2001, loss = 1.63593
I0521 01:34:26.886389 27299 solver.cpp:253]     Train net output #0: loss = 1.63593 (* 1 = 1.63593 loss)
I0521 01:34:26.886404 27299 sgd_solver.cpp:106] Iteration 2001, lr = 0.0025
I0521 01:34:34.872618 27299 solver.cpp:237] Iteration 2030, loss = 1.72647
I0521 01:34:34.872651 27299 solver.cpp:253]     Train net output #0: loss = 1.72647 (* 1 = 1.72647 loss)
I0521 01:34:34.872666 27299 sgd_solver.cpp:106] Iteration 2030, lr = 0.0025
I0521 01:34:42.311522 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2058.caffemodel
I0521 01:34:42.543536 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2058.solverstate
I0521 01:34:42.926761 27299 solver.cpp:237] Iteration 2059, loss = 1.59113
I0521 01:34:42.926808 27299 solver.cpp:253]     Train net output #0: loss = 1.59113 (* 1 = 1.59113 loss)
I0521 01:34:42.926825 27299 sgd_solver.cpp:106] Iteration 2059, lr = 0.0025
I0521 01:34:50.916218 27299 solver.cpp:237] Iteration 2088, loss = 1.69265
I0521 01:34:50.916380 27299 solver.cpp:253]     Train net output #0: loss = 1.69265 (* 1 = 1.69265 loss)
I0521 01:34:50.916393 27299 sgd_solver.cpp:106] Iteration 2088, lr = 0.0025
I0521 01:34:58.902361 27299 solver.cpp:237] Iteration 2117, loss = 1.66026
I0521 01:34:58.902393 27299 solver.cpp:253]     Train net output #0: loss = 1.66026 (* 1 = 1.66026 loss)
I0521 01:34:58.902410 27299 sgd_solver.cpp:106] Iteration 2117, lr = 0.0025
I0521 01:35:06.888671 27299 solver.cpp:237] Iteration 2146, loss = 1.67254
I0521 01:35:06.888705 27299 solver.cpp:253]     Train net output #0: loss = 1.67254 (* 1 = 1.67254 loss)
I0521 01:35:06.888720 27299 sgd_solver.cpp:106] Iteration 2146, lr = 0.0025
I0521 01:35:37.064606 27299 solver.cpp:237] Iteration 2175, loss = 1.66243
I0521 01:35:37.064780 27299 solver.cpp:253]     Train net output #0: loss = 1.66243 (* 1 = 1.66243 loss)
I0521 01:35:37.064796 27299 sgd_solver.cpp:106] Iteration 2175, lr = 0.0025
I0521 01:35:45.049850 27299 solver.cpp:237] Iteration 2204, loss = 1.68747
I0521 01:35:45.049883 27299 solver.cpp:253]     Train net output #0: loss = 1.68747 (* 1 = 1.68747 loss)
I0521 01:35:45.049899 27299 sgd_solver.cpp:106] Iteration 2204, lr = 0.0025
I0521 01:35:53.040344 27299 solver.cpp:237] Iteration 2233, loss = 1.66534
I0521 01:35:53.040377 27299 solver.cpp:253]     Train net output #0: loss = 1.66534 (* 1 = 1.66534 loss)
I0521 01:35:53.040392 27299 sgd_solver.cpp:106] Iteration 2233, lr = 0.0025
I0521 01:36:01.028458 27299 solver.cpp:237] Iteration 2262, loss = 1.69456
I0521 01:36:01.028506 27299 solver.cpp:253]     Train net output #0: loss = 1.69456 (* 1 = 1.69456 loss)
I0521 01:36:01.028520 27299 sgd_solver.cpp:106] Iteration 2262, lr = 0.0025
I0521 01:36:09.020359 27299 solver.cpp:237] Iteration 2291, loss = 1.67878
I0521 01:36:09.020499 27299 solver.cpp:253]     Train net output #0: loss = 1.67878 (* 1 = 1.67878 loss)
I0521 01:36:09.020512 27299 sgd_solver.cpp:106] Iteration 2291, lr = 0.0025
I0521 01:36:17.011143 27299 solver.cpp:237] Iteration 2320, loss = 1.69067
I0521 01:36:17.011176 27299 solver.cpp:253]     Train net output #0: loss = 1.69067 (* 1 = 1.69067 loss)
I0521 01:36:17.011193 27299 sgd_solver.cpp:106] Iteration 2320, lr = 0.0025
I0521 01:36:24.998818 27299 solver.cpp:237] Iteration 2349, loss = 1.64068
I0521 01:36:24.998865 27299 solver.cpp:253]     Train net output #0: loss = 1.64068 (* 1 = 1.64068 loss)
I0521 01:36:24.998878 27299 sgd_solver.cpp:106] Iteration 2349, lr = 0.0025
I0521 01:36:25.548833 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2352.caffemodel
I0521 01:36:25.780459 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2352.solverstate
I0521 01:36:25.806674 27299 solver.cpp:341] Iteration 2352, Testing net (#0)
I0521 01:37:31.831899 27299 solver.cpp:409]     Test net output #0: accuracy = 0.682879
I0521 01:37:31.832077 27299 solver.cpp:409]     Test net output #1: loss = 1.08503 (* 1 = 1.08503 loss)
I0521 01:38:01.218335 27299 solver.cpp:237] Iteration 2378, loss = 1.6675
I0521 01:38:01.218386 27299 solver.cpp:253]     Train net output #0: loss = 1.6675 (* 1 = 1.6675 loss)
I0521 01:38:01.218403 27299 sgd_solver.cpp:106] Iteration 2378, lr = 0.0025
I0521 01:38:09.215977 27299 solver.cpp:237] Iteration 2407, loss = 1.62677
I0521 01:38:09.216142 27299 solver.cpp:253]     Train net output #0: loss = 1.62677 (* 1 = 1.62677 loss)
I0521 01:38:09.216157 27299 sgd_solver.cpp:106] Iteration 2407, lr = 0.0025
I0521 01:38:17.216923 27299 solver.cpp:237] Iteration 2436, loss = 1.73677
I0521 01:38:17.216955 27299 solver.cpp:253]     Train net output #0: loss = 1.73677 (* 1 = 1.73677 loss)
I0521 01:38:17.216971 27299 sgd_solver.cpp:106] Iteration 2436, lr = 0.0025
I0521 01:38:25.216375 27299 solver.cpp:237] Iteration 2465, loss = 1.62733
I0521 01:38:25.216408 27299 solver.cpp:253]     Train net output #0: loss = 1.62733 (* 1 = 1.62733 loss)
I0521 01:38:25.216423 27299 sgd_solver.cpp:106] Iteration 2465, lr = 0.0025
I0521 01:38:33.212266 27299 solver.cpp:237] Iteration 2494, loss = 1.70221
I0521 01:38:33.212306 27299 solver.cpp:253]     Train net output #0: loss = 1.70221 (* 1 = 1.70221 loss)
I0521 01:38:33.212323 27299 sgd_solver.cpp:106] Iteration 2494, lr = 0.0025
I0521 01:38:41.206465 27299 solver.cpp:237] Iteration 2523, loss = 1.62165
I0521 01:38:41.206603 27299 solver.cpp:253]     Train net output #0: loss = 1.62165 (* 1 = 1.62165 loss)
I0521 01:38:41.206615 27299 sgd_solver.cpp:106] Iteration 2523, lr = 0.0025
I0521 01:39:11.400471 27299 solver.cpp:237] Iteration 2552, loss = 1.53694
I0521 01:39:11.400660 27299 solver.cpp:253]     Train net output #0: loss = 1.53694 (* 1 = 1.53694 loss)
I0521 01:39:11.400676 27299 sgd_solver.cpp:106] Iteration 2552, lr = 0.0025
I0521 01:39:19.395592 27299 solver.cpp:237] Iteration 2581, loss = 1.52967
I0521 01:39:19.395645 27299 solver.cpp:253]     Train net output #0: loss = 1.52967 (* 1 = 1.52967 loss)
I0521 01:39:19.395660 27299 sgd_solver.cpp:106] Iteration 2581, lr = 0.0025
I0521 01:39:27.385785 27299 solver.cpp:237] Iteration 2610, loss = 1.66033
I0521 01:39:27.385820 27299 solver.cpp:253]     Train net output #0: loss = 1.66033 (* 1 = 1.66033 loss)
I0521 01:39:27.385833 27299 sgd_solver.cpp:106] Iteration 2610, lr = 0.0025
I0521 01:39:35.377238 27299 solver.cpp:237] Iteration 2639, loss = 1.52291
I0521 01:39:35.377271 27299 solver.cpp:253]     Train net output #0: loss = 1.52291 (* 1 = 1.52291 loss)
I0521 01:39:35.377286 27299 sgd_solver.cpp:106] Iteration 2639, lr = 0.0025
I0521 01:39:37.030441 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2646.caffemodel
I0521 01:39:37.265003 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2646.solverstate
I0521 01:39:43.438240 27299 solver.cpp:237] Iteration 2668, loss = 1.68667
I0521 01:39:43.438415 27299 solver.cpp:253]     Train net output #0: loss = 1.68667 (* 1 = 1.68667 loss)
I0521 01:39:43.438428 27299 sgd_solver.cpp:106] Iteration 2668, lr = 0.0025
I0521 01:39:51.431792 27299 solver.cpp:237] Iteration 2697, loss = 1.63062
I0521 01:39:51.431840 27299 solver.cpp:253]     Train net output #0: loss = 1.63062 (* 1 = 1.63062 loss)
I0521 01:39:51.431859 27299 sgd_solver.cpp:106] Iteration 2697, lr = 0.0025
I0521 01:39:59.428267 27299 solver.cpp:237] Iteration 2726, loss = 1.59797
I0521 01:39:59.428299 27299 solver.cpp:253]     Train net output #0: loss = 1.59797 (* 1 = 1.59797 loss)
I0521 01:39:59.428315 27299 sgd_solver.cpp:106] Iteration 2726, lr = 0.0025
I0521 01:40:29.573148 27299 solver.cpp:237] Iteration 2755, loss = 1.67699
I0521 01:40:29.573324 27299 solver.cpp:253]     Train net output #0: loss = 1.67699 (* 1 = 1.67699 loss)
I0521 01:40:29.573340 27299 sgd_solver.cpp:106] Iteration 2755, lr = 0.0025
I0521 01:40:37.570308 27299 solver.cpp:237] Iteration 2784, loss = 1.62896
I0521 01:40:37.570350 27299 solver.cpp:253]     Train net output #0: loss = 1.62896 (* 1 = 1.62896 loss)
I0521 01:40:37.570370 27299 sgd_solver.cpp:106] Iteration 2784, lr = 0.0025
I0521 01:40:45.563906 27299 solver.cpp:237] Iteration 2813, loss = 1.58165
I0521 01:40:45.563941 27299 solver.cpp:253]     Train net output #0: loss = 1.58165 (* 1 = 1.58165 loss)
I0521 01:40:45.563954 27299 sgd_solver.cpp:106] Iteration 2813, lr = 0.0025
I0521 01:40:53.554996 27299 solver.cpp:237] Iteration 2842, loss = 1.53304
I0521 01:40:53.555028 27299 solver.cpp:253]     Train net output #0: loss = 1.53304 (* 1 = 1.53304 loss)
I0521 01:40:53.555043 27299 sgd_solver.cpp:106] Iteration 2842, lr = 0.0025
I0521 01:41:01.546308 27299 solver.cpp:237] Iteration 2871, loss = 1.58619
I0521 01:41:01.546465 27299 solver.cpp:253]     Train net output #0: loss = 1.58619 (* 1 = 1.58619 loss)
I0521 01:41:01.546479 27299 sgd_solver.cpp:106] Iteration 2871, lr = 0.0025
I0521 01:41:09.541291 27299 solver.cpp:237] Iteration 2900, loss = 1.61087
I0521 01:41:09.541324 27299 solver.cpp:253]     Train net output #0: loss = 1.61087 (* 1 = 1.61087 loss)
I0521 01:41:09.541340 27299 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0521 01:41:17.545133 27299 solver.cpp:237] Iteration 2929, loss = 1.57598
I0521 01:41:17.545166 27299 solver.cpp:253]     Train net output #0: loss = 1.57598 (* 1 = 1.57598 loss)
I0521 01:41:17.545181 27299 sgd_solver.cpp:106] Iteration 2929, lr = 0.0025
I0521 01:41:20.303539 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2940.caffemodel
I0521 01:41:20.535902 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2940.solverstate
I0521 01:41:20.564306 27299 solver.cpp:341] Iteration 2940, Testing net (#0)
I0521 01:42:05.633935 27299 solver.cpp:409]     Test net output #0: accuracy = 0.708503
I0521 01:42:05.634104 27299 solver.cpp:409]     Test net output #1: loss = 1.01416 (* 1 = 1.01416 loss)
I0521 01:42:05.716650 27299 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2941.caffemodel
I0521 01:42:05.949327 27299 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332_iter_2941.solverstate
I0521 01:42:05.977532 27299 solver.cpp:326] Optimization Done.
I0521 01:42:05.977561 27299 caffe.cpp:215] Optimization Done.
Application 11236357 resources: utime ~1254s, stime ~226s, Rss ~5329176, inblocks ~3594474, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_510_2016-05-20T11.20.51.121332.solver"
	User time (seconds): 0.56
	System time (seconds): 0.21
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:44.01
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 3
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 3461
	Involuntary context switches: 221
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
