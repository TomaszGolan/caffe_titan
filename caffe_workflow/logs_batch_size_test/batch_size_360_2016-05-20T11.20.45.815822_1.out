2805989
I0520 22:02:44.923960 20989 caffe.cpp:184] Using GPUs 0
I0520 22:02:45.356595 20989 solver.cpp:48] Initializing solver from parameters: 
test_iter: 416
test_interval: 833
base_lr: 0.0025
display: 41
max_iter: 4166
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 416
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822.prototxt"
I0520 22:02:45.369961 20989 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822.prototxt
I0520 22:02:45.393232 20989 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 22:02:45.393292 20989 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 22:02:45.393647 20989 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 360
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:02:45.393828 20989 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:02:45.393851 20989 net.cpp:106] Creating Layer data_hdf5
I0520 22:02:45.393867 20989 net.cpp:411] data_hdf5 -> data
I0520 22:02:45.393898 20989 net.cpp:411] data_hdf5 -> label
I0520 22:02:45.393930 20989 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 22:02:45.407788 20989 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 22:02:45.412346 20989 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 22:03:06.934597 20989 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 22:03:06.946127 20989 net.cpp:150] Setting up data_hdf5
I0520 22:03:06.946173 20989 net.cpp:157] Top shape: 360 1 127 50 (2286000)
I0520 22:03:06.946188 20989 net.cpp:157] Top shape: 360 (360)
I0520 22:03:06.946198 20989 net.cpp:165] Memory required for data: 9145440
I0520 22:03:06.946213 20989 layer_factory.hpp:77] Creating layer conv1
I0520 22:03:06.946245 20989 net.cpp:106] Creating Layer conv1
I0520 22:03:06.946257 20989 net.cpp:454] conv1 <- data
I0520 22:03:06.946280 20989 net.cpp:411] conv1 -> conv1
I0520 22:03:07.529316 20989 net.cpp:150] Setting up conv1
I0520 22:03:07.529364 20989 net.cpp:157] Top shape: 360 12 120 48 (24883200)
I0520 22:03:07.529376 20989 net.cpp:165] Memory required for data: 108678240
I0520 22:03:07.529404 20989 layer_factory.hpp:77] Creating layer relu1
I0520 22:03:07.529425 20989 net.cpp:106] Creating Layer relu1
I0520 22:03:07.529436 20989 net.cpp:454] relu1 <- conv1
I0520 22:03:07.529450 20989 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:03:07.529968 20989 net.cpp:150] Setting up relu1
I0520 22:03:07.529984 20989 net.cpp:157] Top shape: 360 12 120 48 (24883200)
I0520 22:03:07.529995 20989 net.cpp:165] Memory required for data: 208211040
I0520 22:03:07.530005 20989 layer_factory.hpp:77] Creating layer pool1
I0520 22:03:07.530021 20989 net.cpp:106] Creating Layer pool1
I0520 22:03:07.530031 20989 net.cpp:454] pool1 <- conv1
I0520 22:03:07.530045 20989 net.cpp:411] pool1 -> pool1
I0520 22:03:07.530125 20989 net.cpp:150] Setting up pool1
I0520 22:03:07.530139 20989 net.cpp:157] Top shape: 360 12 60 48 (12441600)
I0520 22:03:07.530149 20989 net.cpp:165] Memory required for data: 257977440
I0520 22:03:07.530158 20989 layer_factory.hpp:77] Creating layer conv2
I0520 22:03:07.530181 20989 net.cpp:106] Creating Layer conv2
I0520 22:03:07.530192 20989 net.cpp:454] conv2 <- pool1
I0520 22:03:07.530205 20989 net.cpp:411] conv2 -> conv2
I0520 22:03:07.532888 20989 net.cpp:150] Setting up conv2
I0520 22:03:07.532917 20989 net.cpp:157] Top shape: 360 20 54 46 (17884800)
I0520 22:03:07.532927 20989 net.cpp:165] Memory required for data: 329516640
I0520 22:03:07.532945 20989 layer_factory.hpp:77] Creating layer relu2
I0520 22:03:07.532960 20989 net.cpp:106] Creating Layer relu2
I0520 22:03:07.532970 20989 net.cpp:454] relu2 <- conv2
I0520 22:03:07.532984 20989 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:03:07.533311 20989 net.cpp:150] Setting up relu2
I0520 22:03:07.533326 20989 net.cpp:157] Top shape: 360 20 54 46 (17884800)
I0520 22:03:07.533335 20989 net.cpp:165] Memory required for data: 401055840
I0520 22:03:07.533346 20989 layer_factory.hpp:77] Creating layer pool2
I0520 22:03:07.533359 20989 net.cpp:106] Creating Layer pool2
I0520 22:03:07.533368 20989 net.cpp:454] pool2 <- conv2
I0520 22:03:07.533393 20989 net.cpp:411] pool2 -> pool2
I0520 22:03:07.533463 20989 net.cpp:150] Setting up pool2
I0520 22:03:07.533476 20989 net.cpp:157] Top shape: 360 20 27 46 (8942400)
I0520 22:03:07.533486 20989 net.cpp:165] Memory required for data: 436825440
I0520 22:03:07.533493 20989 layer_factory.hpp:77] Creating layer conv3
I0520 22:03:07.533512 20989 net.cpp:106] Creating Layer conv3
I0520 22:03:07.533522 20989 net.cpp:454] conv3 <- pool2
I0520 22:03:07.533536 20989 net.cpp:411] conv3 -> conv3
I0520 22:03:07.535450 20989 net.cpp:150] Setting up conv3
I0520 22:03:07.535475 20989 net.cpp:157] Top shape: 360 28 22 44 (9757440)
I0520 22:03:07.535486 20989 net.cpp:165] Memory required for data: 475855200
I0520 22:03:07.535504 20989 layer_factory.hpp:77] Creating layer relu3
I0520 22:03:07.535521 20989 net.cpp:106] Creating Layer relu3
I0520 22:03:07.535531 20989 net.cpp:454] relu3 <- conv3
I0520 22:03:07.535543 20989 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:03:07.536026 20989 net.cpp:150] Setting up relu3
I0520 22:03:07.536042 20989 net.cpp:157] Top shape: 360 28 22 44 (9757440)
I0520 22:03:07.536053 20989 net.cpp:165] Memory required for data: 514884960
I0520 22:03:07.536063 20989 layer_factory.hpp:77] Creating layer pool3
I0520 22:03:07.536077 20989 net.cpp:106] Creating Layer pool3
I0520 22:03:07.536087 20989 net.cpp:454] pool3 <- conv3
I0520 22:03:07.536099 20989 net.cpp:411] pool3 -> pool3
I0520 22:03:07.536167 20989 net.cpp:150] Setting up pool3
I0520 22:03:07.536180 20989 net.cpp:157] Top shape: 360 28 11 44 (4878720)
I0520 22:03:07.536190 20989 net.cpp:165] Memory required for data: 534399840
I0520 22:03:07.536201 20989 layer_factory.hpp:77] Creating layer conv4
I0520 22:03:07.536217 20989 net.cpp:106] Creating Layer conv4
I0520 22:03:07.536228 20989 net.cpp:454] conv4 <- pool3
I0520 22:03:07.536242 20989 net.cpp:411] conv4 -> conv4
I0520 22:03:07.539011 20989 net.cpp:150] Setting up conv4
I0520 22:03:07.539039 20989 net.cpp:157] Top shape: 360 36 6 42 (3265920)
I0520 22:03:07.539049 20989 net.cpp:165] Memory required for data: 547463520
I0520 22:03:07.539065 20989 layer_factory.hpp:77] Creating layer relu4
I0520 22:03:07.539079 20989 net.cpp:106] Creating Layer relu4
I0520 22:03:07.539089 20989 net.cpp:454] relu4 <- conv4
I0520 22:03:07.539103 20989 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:03:07.539574 20989 net.cpp:150] Setting up relu4
I0520 22:03:07.539590 20989 net.cpp:157] Top shape: 360 36 6 42 (3265920)
I0520 22:03:07.539628 20989 net.cpp:165] Memory required for data: 560527200
I0520 22:03:07.539639 20989 layer_factory.hpp:77] Creating layer pool4
I0520 22:03:07.539652 20989 net.cpp:106] Creating Layer pool4
I0520 22:03:07.539662 20989 net.cpp:454] pool4 <- conv4
I0520 22:03:07.539676 20989 net.cpp:411] pool4 -> pool4
I0520 22:03:07.539754 20989 net.cpp:150] Setting up pool4
I0520 22:03:07.539768 20989 net.cpp:157] Top shape: 360 36 3 42 (1632960)
I0520 22:03:07.539780 20989 net.cpp:165] Memory required for data: 567059040
I0520 22:03:07.539791 20989 layer_factory.hpp:77] Creating layer ip1
I0520 22:03:07.539810 20989 net.cpp:106] Creating Layer ip1
I0520 22:03:07.539820 20989 net.cpp:454] ip1 <- pool4
I0520 22:03:07.539834 20989 net.cpp:411] ip1 -> ip1
I0520 22:03:07.555312 20989 net.cpp:150] Setting up ip1
I0520 22:03:07.555341 20989 net.cpp:157] Top shape: 360 196 (70560)
I0520 22:03:07.555353 20989 net.cpp:165] Memory required for data: 567341280
I0520 22:03:07.555376 20989 layer_factory.hpp:77] Creating layer relu5
I0520 22:03:07.555390 20989 net.cpp:106] Creating Layer relu5
I0520 22:03:07.555400 20989 net.cpp:454] relu5 <- ip1
I0520 22:03:07.555413 20989 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:03:07.555764 20989 net.cpp:150] Setting up relu5
I0520 22:03:07.555778 20989 net.cpp:157] Top shape: 360 196 (70560)
I0520 22:03:07.555789 20989 net.cpp:165] Memory required for data: 567623520
I0520 22:03:07.555799 20989 layer_factory.hpp:77] Creating layer drop1
I0520 22:03:07.555820 20989 net.cpp:106] Creating Layer drop1
I0520 22:03:07.555831 20989 net.cpp:454] drop1 <- ip1
I0520 22:03:07.555856 20989 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:03:07.555902 20989 net.cpp:150] Setting up drop1
I0520 22:03:07.555914 20989 net.cpp:157] Top shape: 360 196 (70560)
I0520 22:03:07.555925 20989 net.cpp:165] Memory required for data: 567905760
I0520 22:03:07.555934 20989 layer_factory.hpp:77] Creating layer ip2
I0520 22:03:07.555953 20989 net.cpp:106] Creating Layer ip2
I0520 22:03:07.555964 20989 net.cpp:454] ip2 <- ip1
I0520 22:03:07.555976 20989 net.cpp:411] ip2 -> ip2
I0520 22:03:07.556442 20989 net.cpp:150] Setting up ip2
I0520 22:03:07.556455 20989 net.cpp:157] Top shape: 360 98 (35280)
I0520 22:03:07.556465 20989 net.cpp:165] Memory required for data: 568046880
I0520 22:03:07.556480 20989 layer_factory.hpp:77] Creating layer relu6
I0520 22:03:07.556493 20989 net.cpp:106] Creating Layer relu6
I0520 22:03:07.556502 20989 net.cpp:454] relu6 <- ip2
I0520 22:03:07.556514 20989 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:03:07.557039 20989 net.cpp:150] Setting up relu6
I0520 22:03:07.557055 20989 net.cpp:157] Top shape: 360 98 (35280)
I0520 22:03:07.557065 20989 net.cpp:165] Memory required for data: 568188000
I0520 22:03:07.557075 20989 layer_factory.hpp:77] Creating layer drop2
I0520 22:03:07.557088 20989 net.cpp:106] Creating Layer drop2
I0520 22:03:07.557099 20989 net.cpp:454] drop2 <- ip2
I0520 22:03:07.557111 20989 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:03:07.557153 20989 net.cpp:150] Setting up drop2
I0520 22:03:07.557166 20989 net.cpp:157] Top shape: 360 98 (35280)
I0520 22:03:07.557178 20989 net.cpp:165] Memory required for data: 568329120
I0520 22:03:07.557188 20989 layer_factory.hpp:77] Creating layer ip3
I0520 22:03:07.557199 20989 net.cpp:106] Creating Layer ip3
I0520 22:03:07.557209 20989 net.cpp:454] ip3 <- ip2
I0520 22:03:07.557221 20989 net.cpp:411] ip3 -> ip3
I0520 22:03:07.557432 20989 net.cpp:150] Setting up ip3
I0520 22:03:07.557446 20989 net.cpp:157] Top shape: 360 11 (3960)
I0520 22:03:07.557456 20989 net.cpp:165] Memory required for data: 568344960
I0520 22:03:07.557471 20989 layer_factory.hpp:77] Creating layer drop3
I0520 22:03:07.557483 20989 net.cpp:106] Creating Layer drop3
I0520 22:03:07.557492 20989 net.cpp:454] drop3 <- ip3
I0520 22:03:07.557504 20989 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:03:07.557543 20989 net.cpp:150] Setting up drop3
I0520 22:03:07.557556 20989 net.cpp:157] Top shape: 360 11 (3960)
I0520 22:03:07.557566 20989 net.cpp:165] Memory required for data: 568360800
I0520 22:03:07.557576 20989 layer_factory.hpp:77] Creating layer loss
I0520 22:03:07.557595 20989 net.cpp:106] Creating Layer loss
I0520 22:03:07.557605 20989 net.cpp:454] loss <- ip3
I0520 22:03:07.557617 20989 net.cpp:454] loss <- label
I0520 22:03:07.557629 20989 net.cpp:411] loss -> loss
I0520 22:03:07.557646 20989 layer_factory.hpp:77] Creating layer loss
I0520 22:03:07.558290 20989 net.cpp:150] Setting up loss
I0520 22:03:07.558310 20989 net.cpp:157] Top shape: (1)
I0520 22:03:07.558325 20989 net.cpp:160]     with loss weight 1
I0520 22:03:07.558367 20989 net.cpp:165] Memory required for data: 568360804
I0520 22:03:07.558377 20989 net.cpp:226] loss needs backward computation.
I0520 22:03:07.558388 20989 net.cpp:226] drop3 needs backward computation.
I0520 22:03:07.558398 20989 net.cpp:226] ip3 needs backward computation.
I0520 22:03:07.558409 20989 net.cpp:226] drop2 needs backward computation.
I0520 22:03:07.558418 20989 net.cpp:226] relu6 needs backward computation.
I0520 22:03:07.558428 20989 net.cpp:226] ip2 needs backward computation.
I0520 22:03:07.558437 20989 net.cpp:226] drop1 needs backward computation.
I0520 22:03:07.558447 20989 net.cpp:226] relu5 needs backward computation.
I0520 22:03:07.558456 20989 net.cpp:226] ip1 needs backward computation.
I0520 22:03:07.558466 20989 net.cpp:226] pool4 needs backward computation.
I0520 22:03:07.558476 20989 net.cpp:226] relu4 needs backward computation.
I0520 22:03:07.558486 20989 net.cpp:226] conv4 needs backward computation.
I0520 22:03:07.558497 20989 net.cpp:226] pool3 needs backward computation.
I0520 22:03:07.558516 20989 net.cpp:226] relu3 needs backward computation.
I0520 22:03:07.558523 20989 net.cpp:226] conv3 needs backward computation.
I0520 22:03:07.558534 20989 net.cpp:226] pool2 needs backward computation.
I0520 22:03:07.558544 20989 net.cpp:226] relu2 needs backward computation.
I0520 22:03:07.558554 20989 net.cpp:226] conv2 needs backward computation.
I0520 22:03:07.558565 20989 net.cpp:226] pool1 needs backward computation.
I0520 22:03:07.558578 20989 net.cpp:226] relu1 needs backward computation.
I0520 22:03:07.558588 20989 net.cpp:226] conv1 needs backward computation.
I0520 22:03:07.558598 20989 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:03:07.558609 20989 net.cpp:270] This network produces output loss
I0520 22:03:07.558632 20989 net.cpp:283] Network initialization done.
I0520 22:03:07.568634 20989 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822.prototxt
I0520 22:03:07.568713 20989 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 22:03:07.569072 20989 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 360
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:03:07.569264 20989 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:03:07.569280 20989 net.cpp:106] Creating Layer data_hdf5
I0520 22:03:07.569294 20989 net.cpp:411] data_hdf5 -> data
I0520 22:03:07.569311 20989 net.cpp:411] data_hdf5 -> label
I0520 22:03:07.569327 20989 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 22:03:07.577008 20989 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 22:03:28.881290 20989 net.cpp:150] Setting up data_hdf5
I0520 22:03:28.881454 20989 net.cpp:157] Top shape: 360 1 127 50 (2286000)
I0520 22:03:28.881469 20989 net.cpp:157] Top shape: 360 (360)
I0520 22:03:28.881482 20989 net.cpp:165] Memory required for data: 9145440
I0520 22:03:28.881496 20989 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 22:03:28.881525 20989 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 22:03:28.881534 20989 net.cpp:454] label_data_hdf5_1_split <- label
I0520 22:03:28.881549 20989 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 22:03:28.881570 20989 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 22:03:28.881642 20989 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 22:03:28.881655 20989 net.cpp:157] Top shape: 360 (360)
I0520 22:03:28.881667 20989 net.cpp:157] Top shape: 360 (360)
I0520 22:03:28.881677 20989 net.cpp:165] Memory required for data: 9148320
I0520 22:03:28.881686 20989 layer_factory.hpp:77] Creating layer conv1
I0520 22:03:28.881710 20989 net.cpp:106] Creating Layer conv1
I0520 22:03:28.881721 20989 net.cpp:454] conv1 <- data
I0520 22:03:28.881734 20989 net.cpp:411] conv1 -> conv1
I0520 22:03:28.883692 20989 net.cpp:150] Setting up conv1
I0520 22:03:28.883715 20989 net.cpp:157] Top shape: 360 12 120 48 (24883200)
I0520 22:03:28.883728 20989 net.cpp:165] Memory required for data: 108681120
I0520 22:03:28.883747 20989 layer_factory.hpp:77] Creating layer relu1
I0520 22:03:28.883762 20989 net.cpp:106] Creating Layer relu1
I0520 22:03:28.883772 20989 net.cpp:454] relu1 <- conv1
I0520 22:03:28.883785 20989 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:03:28.884289 20989 net.cpp:150] Setting up relu1
I0520 22:03:28.884305 20989 net.cpp:157] Top shape: 360 12 120 48 (24883200)
I0520 22:03:28.884315 20989 net.cpp:165] Memory required for data: 208213920
I0520 22:03:28.884325 20989 layer_factory.hpp:77] Creating layer pool1
I0520 22:03:28.884341 20989 net.cpp:106] Creating Layer pool1
I0520 22:03:28.884352 20989 net.cpp:454] pool1 <- conv1
I0520 22:03:28.884364 20989 net.cpp:411] pool1 -> pool1
I0520 22:03:28.884439 20989 net.cpp:150] Setting up pool1
I0520 22:03:28.884452 20989 net.cpp:157] Top shape: 360 12 60 48 (12441600)
I0520 22:03:28.884462 20989 net.cpp:165] Memory required for data: 257980320
I0520 22:03:28.884470 20989 layer_factory.hpp:77] Creating layer conv2
I0520 22:03:28.884490 20989 net.cpp:106] Creating Layer conv2
I0520 22:03:28.884500 20989 net.cpp:454] conv2 <- pool1
I0520 22:03:28.884515 20989 net.cpp:411] conv2 -> conv2
I0520 22:03:28.886426 20989 net.cpp:150] Setting up conv2
I0520 22:03:28.886448 20989 net.cpp:157] Top shape: 360 20 54 46 (17884800)
I0520 22:03:28.886461 20989 net.cpp:165] Memory required for data: 329519520
I0520 22:03:28.886479 20989 layer_factory.hpp:77] Creating layer relu2
I0520 22:03:28.886492 20989 net.cpp:106] Creating Layer relu2
I0520 22:03:28.886502 20989 net.cpp:454] relu2 <- conv2
I0520 22:03:28.886514 20989 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:03:28.886847 20989 net.cpp:150] Setting up relu2
I0520 22:03:28.886860 20989 net.cpp:157] Top shape: 360 20 54 46 (17884800)
I0520 22:03:28.886870 20989 net.cpp:165] Memory required for data: 401058720
I0520 22:03:28.886880 20989 layer_factory.hpp:77] Creating layer pool2
I0520 22:03:28.886893 20989 net.cpp:106] Creating Layer pool2
I0520 22:03:28.886904 20989 net.cpp:454] pool2 <- conv2
I0520 22:03:28.886916 20989 net.cpp:411] pool2 -> pool2
I0520 22:03:28.886987 20989 net.cpp:150] Setting up pool2
I0520 22:03:28.887001 20989 net.cpp:157] Top shape: 360 20 27 46 (8942400)
I0520 22:03:28.887009 20989 net.cpp:165] Memory required for data: 436828320
I0520 22:03:28.887020 20989 layer_factory.hpp:77] Creating layer conv3
I0520 22:03:28.887039 20989 net.cpp:106] Creating Layer conv3
I0520 22:03:28.887049 20989 net.cpp:454] conv3 <- pool2
I0520 22:03:28.887063 20989 net.cpp:411] conv3 -> conv3
I0520 22:03:28.889076 20989 net.cpp:150] Setting up conv3
I0520 22:03:28.889099 20989 net.cpp:157] Top shape: 360 28 22 44 (9757440)
I0520 22:03:28.889112 20989 net.cpp:165] Memory required for data: 475858080
I0520 22:03:28.889144 20989 layer_factory.hpp:77] Creating layer relu3
I0520 22:03:28.889158 20989 net.cpp:106] Creating Layer relu3
I0520 22:03:28.889168 20989 net.cpp:454] relu3 <- conv3
I0520 22:03:28.889183 20989 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:03:28.889658 20989 net.cpp:150] Setting up relu3
I0520 22:03:28.889674 20989 net.cpp:157] Top shape: 360 28 22 44 (9757440)
I0520 22:03:28.889684 20989 net.cpp:165] Memory required for data: 514887840
I0520 22:03:28.889694 20989 layer_factory.hpp:77] Creating layer pool3
I0520 22:03:28.889708 20989 net.cpp:106] Creating Layer pool3
I0520 22:03:28.889719 20989 net.cpp:454] pool3 <- conv3
I0520 22:03:28.889730 20989 net.cpp:411] pool3 -> pool3
I0520 22:03:28.889802 20989 net.cpp:150] Setting up pool3
I0520 22:03:28.889816 20989 net.cpp:157] Top shape: 360 28 11 44 (4878720)
I0520 22:03:28.889825 20989 net.cpp:165] Memory required for data: 534402720
I0520 22:03:28.889834 20989 layer_factory.hpp:77] Creating layer conv4
I0520 22:03:28.889853 20989 net.cpp:106] Creating Layer conv4
I0520 22:03:28.889863 20989 net.cpp:454] conv4 <- pool3
I0520 22:03:28.889878 20989 net.cpp:411] conv4 -> conv4
I0520 22:03:28.891933 20989 net.cpp:150] Setting up conv4
I0520 22:03:28.891955 20989 net.cpp:157] Top shape: 360 36 6 42 (3265920)
I0520 22:03:28.891968 20989 net.cpp:165] Memory required for data: 547466400
I0520 22:03:28.891983 20989 layer_factory.hpp:77] Creating layer relu4
I0520 22:03:28.891996 20989 net.cpp:106] Creating Layer relu4
I0520 22:03:28.892006 20989 net.cpp:454] relu4 <- conv4
I0520 22:03:28.892019 20989 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:03:28.892493 20989 net.cpp:150] Setting up relu4
I0520 22:03:28.892508 20989 net.cpp:157] Top shape: 360 36 6 42 (3265920)
I0520 22:03:28.892518 20989 net.cpp:165] Memory required for data: 560530080
I0520 22:03:28.892529 20989 layer_factory.hpp:77] Creating layer pool4
I0520 22:03:28.892541 20989 net.cpp:106] Creating Layer pool4
I0520 22:03:28.892551 20989 net.cpp:454] pool4 <- conv4
I0520 22:03:28.892565 20989 net.cpp:411] pool4 -> pool4
I0520 22:03:28.892637 20989 net.cpp:150] Setting up pool4
I0520 22:03:28.892649 20989 net.cpp:157] Top shape: 360 36 3 42 (1632960)
I0520 22:03:28.892659 20989 net.cpp:165] Memory required for data: 567061920
I0520 22:03:28.892669 20989 layer_factory.hpp:77] Creating layer ip1
I0520 22:03:28.892684 20989 net.cpp:106] Creating Layer ip1
I0520 22:03:28.892695 20989 net.cpp:454] ip1 <- pool4
I0520 22:03:28.892709 20989 net.cpp:411] ip1 -> ip1
I0520 22:03:28.908241 20989 net.cpp:150] Setting up ip1
I0520 22:03:28.908269 20989 net.cpp:157] Top shape: 360 196 (70560)
I0520 22:03:28.908282 20989 net.cpp:165] Memory required for data: 567344160
I0520 22:03:28.908303 20989 layer_factory.hpp:77] Creating layer relu5
I0520 22:03:28.908318 20989 net.cpp:106] Creating Layer relu5
I0520 22:03:28.908329 20989 net.cpp:454] relu5 <- ip1
I0520 22:03:28.908344 20989 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:03:28.908689 20989 net.cpp:150] Setting up relu5
I0520 22:03:28.908704 20989 net.cpp:157] Top shape: 360 196 (70560)
I0520 22:03:28.908713 20989 net.cpp:165] Memory required for data: 567626400
I0520 22:03:28.908725 20989 layer_factory.hpp:77] Creating layer drop1
I0520 22:03:28.908742 20989 net.cpp:106] Creating Layer drop1
I0520 22:03:28.908752 20989 net.cpp:454] drop1 <- ip1
I0520 22:03:28.908766 20989 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:03:28.908810 20989 net.cpp:150] Setting up drop1
I0520 22:03:28.908823 20989 net.cpp:157] Top shape: 360 196 (70560)
I0520 22:03:28.908833 20989 net.cpp:165] Memory required for data: 567908640
I0520 22:03:28.908845 20989 layer_factory.hpp:77] Creating layer ip2
I0520 22:03:28.908859 20989 net.cpp:106] Creating Layer ip2
I0520 22:03:28.908869 20989 net.cpp:454] ip2 <- ip1
I0520 22:03:28.908884 20989 net.cpp:411] ip2 -> ip2
I0520 22:03:28.909365 20989 net.cpp:150] Setting up ip2
I0520 22:03:28.909379 20989 net.cpp:157] Top shape: 360 98 (35280)
I0520 22:03:28.909389 20989 net.cpp:165] Memory required for data: 568049760
I0520 22:03:28.909416 20989 layer_factory.hpp:77] Creating layer relu6
I0520 22:03:28.909430 20989 net.cpp:106] Creating Layer relu6
I0520 22:03:28.909440 20989 net.cpp:454] relu6 <- ip2
I0520 22:03:28.909451 20989 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:03:28.909989 20989 net.cpp:150] Setting up relu6
I0520 22:03:28.910006 20989 net.cpp:157] Top shape: 360 98 (35280)
I0520 22:03:28.910015 20989 net.cpp:165] Memory required for data: 568190880
I0520 22:03:28.910025 20989 layer_factory.hpp:77] Creating layer drop2
I0520 22:03:28.910038 20989 net.cpp:106] Creating Layer drop2
I0520 22:03:28.910049 20989 net.cpp:454] drop2 <- ip2
I0520 22:03:28.910063 20989 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:03:28.910106 20989 net.cpp:150] Setting up drop2
I0520 22:03:28.910120 20989 net.cpp:157] Top shape: 360 98 (35280)
I0520 22:03:28.910130 20989 net.cpp:165] Memory required for data: 568332000
I0520 22:03:28.910140 20989 layer_factory.hpp:77] Creating layer ip3
I0520 22:03:28.910153 20989 net.cpp:106] Creating Layer ip3
I0520 22:03:28.910163 20989 net.cpp:454] ip3 <- ip2
I0520 22:03:28.910177 20989 net.cpp:411] ip3 -> ip3
I0520 22:03:28.910400 20989 net.cpp:150] Setting up ip3
I0520 22:03:28.910413 20989 net.cpp:157] Top shape: 360 11 (3960)
I0520 22:03:28.910423 20989 net.cpp:165] Memory required for data: 568347840
I0520 22:03:28.910439 20989 layer_factory.hpp:77] Creating layer drop3
I0520 22:03:28.910451 20989 net.cpp:106] Creating Layer drop3
I0520 22:03:28.910461 20989 net.cpp:454] drop3 <- ip3
I0520 22:03:28.910475 20989 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:03:28.910516 20989 net.cpp:150] Setting up drop3
I0520 22:03:28.910528 20989 net.cpp:157] Top shape: 360 11 (3960)
I0520 22:03:28.910538 20989 net.cpp:165] Memory required for data: 568363680
I0520 22:03:28.910548 20989 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 22:03:28.910562 20989 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 22:03:28.910570 20989 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 22:03:28.910583 20989 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 22:03:28.910599 20989 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 22:03:28.910672 20989 net.cpp:150] Setting up ip3_drop3_0_split
I0520 22:03:28.910686 20989 net.cpp:157] Top shape: 360 11 (3960)
I0520 22:03:28.910698 20989 net.cpp:157] Top shape: 360 11 (3960)
I0520 22:03:28.910708 20989 net.cpp:165] Memory required for data: 568395360
I0520 22:03:28.910717 20989 layer_factory.hpp:77] Creating layer accuracy
I0520 22:03:28.910739 20989 net.cpp:106] Creating Layer accuracy
I0520 22:03:28.910749 20989 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 22:03:28.910760 20989 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 22:03:28.910774 20989 net.cpp:411] accuracy -> accuracy
I0520 22:03:28.910797 20989 net.cpp:150] Setting up accuracy
I0520 22:03:28.910810 20989 net.cpp:157] Top shape: (1)
I0520 22:03:28.910820 20989 net.cpp:165] Memory required for data: 568395364
I0520 22:03:28.910830 20989 layer_factory.hpp:77] Creating layer loss
I0520 22:03:28.910843 20989 net.cpp:106] Creating Layer loss
I0520 22:03:28.910853 20989 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 22:03:28.910864 20989 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 22:03:28.910877 20989 net.cpp:411] loss -> loss
I0520 22:03:28.910895 20989 layer_factory.hpp:77] Creating layer loss
I0520 22:03:28.911383 20989 net.cpp:150] Setting up loss
I0520 22:03:28.911396 20989 net.cpp:157] Top shape: (1)
I0520 22:03:28.911406 20989 net.cpp:160]     with loss weight 1
I0520 22:03:28.911424 20989 net.cpp:165] Memory required for data: 568395368
I0520 22:03:28.911434 20989 net.cpp:226] loss needs backward computation.
I0520 22:03:28.911447 20989 net.cpp:228] accuracy does not need backward computation.
I0520 22:03:28.911458 20989 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 22:03:28.911468 20989 net.cpp:226] drop3 needs backward computation.
I0520 22:03:28.911478 20989 net.cpp:226] ip3 needs backward computation.
I0520 22:03:28.911487 20989 net.cpp:226] drop2 needs backward computation.
I0520 22:03:28.911506 20989 net.cpp:226] relu6 needs backward computation.
I0520 22:03:28.911516 20989 net.cpp:226] ip2 needs backward computation.
I0520 22:03:28.911526 20989 net.cpp:226] drop1 needs backward computation.
I0520 22:03:28.911535 20989 net.cpp:226] relu5 needs backward computation.
I0520 22:03:28.911545 20989 net.cpp:226] ip1 needs backward computation.
I0520 22:03:28.911556 20989 net.cpp:226] pool4 needs backward computation.
I0520 22:03:28.911566 20989 net.cpp:226] relu4 needs backward computation.
I0520 22:03:28.911574 20989 net.cpp:226] conv4 needs backward computation.
I0520 22:03:28.911586 20989 net.cpp:226] pool3 needs backward computation.
I0520 22:03:28.911595 20989 net.cpp:226] relu3 needs backward computation.
I0520 22:03:28.911607 20989 net.cpp:226] conv3 needs backward computation.
I0520 22:03:28.911617 20989 net.cpp:226] pool2 needs backward computation.
I0520 22:03:28.911628 20989 net.cpp:226] relu2 needs backward computation.
I0520 22:03:28.911638 20989 net.cpp:226] conv2 needs backward computation.
I0520 22:03:28.911648 20989 net.cpp:226] pool1 needs backward computation.
I0520 22:03:28.911658 20989 net.cpp:226] relu1 needs backward computation.
I0520 22:03:28.911669 20989 net.cpp:226] conv1 needs backward computation.
I0520 22:03:28.911679 20989 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 22:03:28.911697 20989 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:03:28.911706 20989 net.cpp:270] This network produces output accuracy
I0520 22:03:28.911717 20989 net.cpp:270] This network produces output loss
I0520 22:03:28.911744 20989 net.cpp:283] Network initialization done.
I0520 22:03:28.911880 20989 solver.cpp:60] Solver scaffolding done.
I0520 22:03:28.913002 20989 caffe.cpp:212] Starting Optimization
I0520 22:03:28.913020 20989 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 22:03:28.913033 20989 solver.cpp:289] Learning Rate Policy: fixed
I0520 22:03:28.914261 20989 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 22:04:14.982733 20989 solver.cpp:409]     Test net output #0: accuracy = 0.066887
I0520 22:04:14.982895 20989 solver.cpp:409]     Test net output #1: loss = 2.39785 (* 1 = 2.39785 loss)
I0520 22:04:15.058426 20989 solver.cpp:237] Iteration 0, loss = 2.39795
I0520 22:04:15.058462 20989 solver.cpp:253]     Train net output #0: loss = 2.39795 (* 1 = 2.39795 loss)
I0520 22:04:15.058480 20989 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 22:04:23.010121 20989 solver.cpp:237] Iteration 41, loss = 2.37079
I0520 22:04:23.010159 20989 solver.cpp:253]     Train net output #0: loss = 2.37079 (* 1 = 2.37079 loss)
I0520 22:04:23.010174 20989 sgd_solver.cpp:106] Iteration 41, lr = 0.0025
I0520 22:04:30.963356 20989 solver.cpp:237] Iteration 82, loss = 2.33742
I0520 22:04:30.963392 20989 solver.cpp:253]     Train net output #0: loss = 2.33742 (* 1 = 2.33742 loss)
I0520 22:04:30.963409 20989 sgd_solver.cpp:106] Iteration 82, lr = 0.0025
I0520 22:04:38.913480 20989 solver.cpp:237] Iteration 123, loss = 2.31195
I0520 22:04:38.913512 20989 solver.cpp:253]     Train net output #0: loss = 2.31195 (* 1 = 2.31195 loss)
I0520 22:04:38.913529 20989 sgd_solver.cpp:106] Iteration 123, lr = 0.0025
I0520 22:04:46.864152 20989 solver.cpp:237] Iteration 164, loss = 2.32713
I0520 22:04:46.864297 20989 solver.cpp:253]     Train net output #0: loss = 2.32713 (* 1 = 2.32713 loss)
I0520 22:04:46.864311 20989 sgd_solver.cpp:106] Iteration 164, lr = 0.0025
I0520 22:04:54.815333 20989 solver.cpp:237] Iteration 205, loss = 2.33194
I0520 22:04:54.815371 20989 solver.cpp:253]     Train net output #0: loss = 2.33194 (* 1 = 2.33194 loss)
I0520 22:04:54.815388 20989 sgd_solver.cpp:106] Iteration 205, lr = 0.0025
I0520 22:05:02.767053 20989 solver.cpp:237] Iteration 246, loss = 2.31236
I0520 22:05:02.767087 20989 solver.cpp:253]     Train net output #0: loss = 2.31236 (* 1 = 2.31236 loss)
I0520 22:05:02.767103 20989 sgd_solver.cpp:106] Iteration 246, lr = 0.0025
I0520 22:05:32.818392 20989 solver.cpp:237] Iteration 287, loss = 2.31718
I0520 22:05:32.818552 20989 solver.cpp:253]     Train net output #0: loss = 2.31718 (* 1 = 2.31718 loss)
I0520 22:05:32.818567 20989 sgd_solver.cpp:106] Iteration 287, lr = 0.0025
I0520 22:05:40.773002 20989 solver.cpp:237] Iteration 328, loss = 2.23375
I0520 22:05:40.773039 20989 solver.cpp:253]     Train net output #0: loss = 2.23375 (* 1 = 2.23375 loss)
I0520 22:05:40.773061 20989 sgd_solver.cpp:106] Iteration 328, lr = 0.0025
I0520 22:05:48.732563 20989 solver.cpp:237] Iteration 369, loss = 2.1888
I0520 22:05:48.732596 20989 solver.cpp:253]     Train net output #0: loss = 2.1888 (* 1 = 2.1888 loss)
I0520 22:05:48.732612 20989 sgd_solver.cpp:106] Iteration 369, lr = 0.0025
I0520 22:05:56.687616 20989 solver.cpp:237] Iteration 410, loss = 2.15918
I0520 22:05:56.687651 20989 solver.cpp:253]     Train net output #0: loss = 2.15918 (* 1 = 2.15918 loss)
I0520 22:05:56.687664 20989 sgd_solver.cpp:106] Iteration 410, lr = 0.0025
I0520 22:05:57.659059 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_416.caffemodel
I0520 22:05:57.846536 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_416.solverstate
I0520 22:06:04.729159 20989 solver.cpp:237] Iteration 451, loss = 2.07537
I0520 22:06:04.729315 20989 solver.cpp:253]     Train net output #0: loss = 2.07537 (* 1 = 2.07537 loss)
I0520 22:06:04.729329 20989 sgd_solver.cpp:106] Iteration 451, lr = 0.0025
I0520 22:06:12.686213 20989 solver.cpp:237] Iteration 492, loss = 2.06389
I0520 22:06:12.686246 20989 solver.cpp:253]     Train net output #0: loss = 2.06389 (* 1 = 2.06389 loss)
I0520 22:06:12.686262 20989 sgd_solver.cpp:106] Iteration 492, lr = 0.0025
I0520 22:06:20.642045 20989 solver.cpp:237] Iteration 533, loss = 1.93857
I0520 22:06:20.642076 20989 solver.cpp:253]     Train net output #0: loss = 1.93857 (* 1 = 1.93857 loss)
I0520 22:06:20.642094 20989 sgd_solver.cpp:106] Iteration 533, lr = 0.0025
I0520 22:06:50.764171 20989 solver.cpp:237] Iteration 574, loss = 1.99499
I0520 22:06:50.764343 20989 solver.cpp:253]     Train net output #0: loss = 1.99499 (* 1 = 1.99499 loss)
I0520 22:06:50.764359 20989 sgd_solver.cpp:106] Iteration 574, lr = 0.0025
I0520 22:06:58.719815 20989 solver.cpp:237] Iteration 615, loss = 1.92409
I0520 22:06:58.719851 20989 solver.cpp:253]     Train net output #0: loss = 1.92409 (* 1 = 1.92409 loss)
I0520 22:06:58.719868 20989 sgd_solver.cpp:106] Iteration 615, lr = 0.0025
I0520 22:07:06.677827 20989 solver.cpp:237] Iteration 656, loss = 1.87444
I0520 22:07:06.677861 20989 solver.cpp:253]     Train net output #0: loss = 1.87444 (* 1 = 1.87444 loss)
I0520 22:07:06.677876 20989 sgd_solver.cpp:106] Iteration 656, lr = 0.0025
I0520 22:07:14.632515 20989 solver.cpp:237] Iteration 697, loss = 1.98824
I0520 22:07:14.632550 20989 solver.cpp:253]     Train net output #0: loss = 1.98824 (* 1 = 1.98824 loss)
I0520 22:07:14.632563 20989 sgd_solver.cpp:106] Iteration 697, lr = 0.0025
I0520 22:07:22.588805 20989 solver.cpp:237] Iteration 738, loss = 1.87402
I0520 22:07:22.588959 20989 solver.cpp:253]     Train net output #0: loss = 1.87402 (* 1 = 1.87402 loss)
I0520 22:07:22.588973 20989 sgd_solver.cpp:106] Iteration 738, lr = 0.0025
I0520 22:07:30.546802 20989 solver.cpp:237] Iteration 779, loss = 1.91483
I0520 22:07:30.546833 20989 solver.cpp:253]     Train net output #0: loss = 1.91483 (* 1 = 1.91483 loss)
I0520 22:07:30.546849 20989 sgd_solver.cpp:106] Iteration 779, lr = 0.0025
I0520 22:07:38.502622 20989 solver.cpp:237] Iteration 820, loss = 1.88458
I0520 22:07:38.502655 20989 solver.cpp:253]     Train net output #0: loss = 1.88458 (* 1 = 1.88458 loss)
I0520 22:07:38.502671 20989 sgd_solver.cpp:106] Iteration 820, lr = 0.0025
I0520 22:07:40.637652 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_832.caffemodel
I0520 22:07:40.825893 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_832.solverstate
I0520 22:07:40.920500 20989 solver.cpp:341] Iteration 833, Testing net (#0)
I0520 22:08:26.223002 20989 solver.cpp:409]     Test net output #0: accuracy = 0.575701
I0520 22:08:26.223165 20989 solver.cpp:409]     Test net output #1: loss = 1.46273 (* 1 = 1.46273 loss)
I0520 22:08:53.892169 20989 solver.cpp:237] Iteration 861, loss = 1.99659
I0520 22:08:53.892218 20989 solver.cpp:253]     Train net output #0: loss = 1.99659 (* 1 = 1.99659 loss)
I0520 22:08:53.892233 20989 sgd_solver.cpp:106] Iteration 861, lr = 0.0025
I0520 22:09:01.845062 20989 solver.cpp:237] Iteration 902, loss = 1.80456
I0520 22:09:01.845233 20989 solver.cpp:253]     Train net output #0: loss = 1.80456 (* 1 = 1.80456 loss)
I0520 22:09:01.845247 20989 sgd_solver.cpp:106] Iteration 902, lr = 0.0025
I0520 22:09:09.798229 20989 solver.cpp:237] Iteration 943, loss = 1.9892
I0520 22:09:09.798261 20989 solver.cpp:253]     Train net output #0: loss = 1.9892 (* 1 = 1.9892 loss)
I0520 22:09:09.798277 20989 sgd_solver.cpp:106] Iteration 943, lr = 0.0025
I0520 22:09:17.747308 20989 solver.cpp:237] Iteration 984, loss = 1.89396
I0520 22:09:17.747341 20989 solver.cpp:253]     Train net output #0: loss = 1.89396 (* 1 = 1.89396 loss)
I0520 22:09:17.747356 20989 sgd_solver.cpp:106] Iteration 984, lr = 0.0025
I0520 22:09:25.697803 20989 solver.cpp:237] Iteration 1025, loss = 1.73117
I0520 22:09:25.697846 20989 solver.cpp:253]     Train net output #0: loss = 1.73117 (* 1 = 1.73117 loss)
I0520 22:09:25.697860 20989 sgd_solver.cpp:106] Iteration 1025, lr = 0.0025
I0520 22:09:33.649330 20989 solver.cpp:237] Iteration 1066, loss = 1.84623
I0520 22:09:33.649466 20989 solver.cpp:253]     Train net output #0: loss = 1.84623 (* 1 = 1.84623 loss)
I0520 22:09:33.649480 20989 sgd_solver.cpp:106] Iteration 1066, lr = 0.0025
I0520 22:09:41.602295 20989 solver.cpp:237] Iteration 1107, loss = 1.85918
I0520 22:09:41.602327 20989 solver.cpp:253]     Train net output #0: loss = 1.85918 (* 1 = 1.85918 loss)
I0520 22:09:41.602344 20989 sgd_solver.cpp:106] Iteration 1107, lr = 0.0025
I0520 22:10:11.770623 20989 solver.cpp:237] Iteration 1148, loss = 1.73617
I0520 22:10:11.770797 20989 solver.cpp:253]     Train net output #0: loss = 1.73617 (* 1 = 1.73617 loss)
I0520 22:10:11.770810 20989 sgd_solver.cpp:106] Iteration 1148, lr = 0.0025
I0520 22:10:19.722681 20989 solver.cpp:237] Iteration 1189, loss = 1.80963
I0520 22:10:19.722724 20989 solver.cpp:253]     Train net output #0: loss = 1.80963 (* 1 = 1.80963 loss)
I0520 22:10:19.722739 20989 sgd_solver.cpp:106] Iteration 1189, lr = 0.0025
I0520 22:10:27.676003 20989 solver.cpp:237] Iteration 1230, loss = 1.73854
I0520 22:10:27.676038 20989 solver.cpp:253]     Train net output #0: loss = 1.73854 (* 1 = 1.73854 loss)
I0520 22:10:27.676051 20989 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0520 22:10:30.974915 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_1248.caffemodel
I0520 22:10:31.160796 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_1248.solverstate
I0520 22:10:35.713735 20989 solver.cpp:237] Iteration 1271, loss = 1.64283
I0520 22:10:35.713778 20989 solver.cpp:253]     Train net output #0: loss = 1.64283 (* 1 = 1.64283 loss)
I0520 22:10:35.713796 20989 sgd_solver.cpp:106] Iteration 1271, lr = 0.0025
I0520 22:10:43.670624 20989 solver.cpp:237] Iteration 1312, loss = 1.83049
I0520 22:10:43.670786 20989 solver.cpp:253]     Train net output #0: loss = 1.83049 (* 1 = 1.83049 loss)
I0520 22:10:43.670800 20989 sgd_solver.cpp:106] Iteration 1312, lr = 0.0025
I0520 22:10:51.618700 20989 solver.cpp:237] Iteration 1353, loss = 1.80389
I0520 22:10:51.618732 20989 solver.cpp:253]     Train net output #0: loss = 1.80389 (* 1 = 1.80389 loss)
I0520 22:10:51.618747 20989 sgd_solver.cpp:106] Iteration 1353, lr = 0.0025
I0520 22:11:21.772594 20989 solver.cpp:237] Iteration 1394, loss = 1.75795
I0520 22:11:21.772753 20989 solver.cpp:253]     Train net output #0: loss = 1.75795 (* 1 = 1.75795 loss)
I0520 22:11:21.772766 20989 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0520 22:11:29.721760 20989 solver.cpp:237] Iteration 1435, loss = 1.68424
I0520 22:11:29.721794 20989 solver.cpp:253]     Train net output #0: loss = 1.68424 (* 1 = 1.68424 loss)
I0520 22:11:29.721807 20989 sgd_solver.cpp:106] Iteration 1435, lr = 0.0025
I0520 22:11:37.675932 20989 solver.cpp:237] Iteration 1476, loss = 1.76701
I0520 22:11:37.675966 20989 solver.cpp:253]     Train net output #0: loss = 1.76701 (* 1 = 1.76701 loss)
I0520 22:11:37.675981 20989 sgd_solver.cpp:106] Iteration 1476, lr = 0.0025
I0520 22:11:45.625906 20989 solver.cpp:237] Iteration 1517, loss = 1.7798
I0520 22:11:45.625941 20989 solver.cpp:253]     Train net output #0: loss = 1.7798 (* 1 = 1.7798 loss)
I0520 22:11:45.625955 20989 sgd_solver.cpp:106] Iteration 1517, lr = 0.0025
I0520 22:11:53.577913 20989 solver.cpp:237] Iteration 1558, loss = 1.54013
I0520 22:11:53.578043 20989 solver.cpp:253]     Train net output #0: loss = 1.54013 (* 1 = 1.54013 loss)
I0520 22:11:53.578057 20989 sgd_solver.cpp:106] Iteration 1558, lr = 0.0025
I0520 22:12:01.527817 20989 solver.cpp:237] Iteration 1599, loss = 1.71968
I0520 22:12:01.527848 20989 solver.cpp:253]     Train net output #0: loss = 1.71968 (* 1 = 1.71968 loss)
I0520 22:12:01.527861 20989 sgd_solver.cpp:106] Iteration 1599, lr = 0.0025
I0520 22:12:09.479208 20989 solver.cpp:237] Iteration 1640, loss = 1.68545
I0520 22:12:09.479243 20989 solver.cpp:253]     Train net output #0: loss = 1.68545 (* 1 = 1.68545 loss)
I0520 22:12:09.479256 20989 sgd_solver.cpp:106] Iteration 1640, lr = 0.0025
I0520 22:12:13.940050 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_1664.caffemodel
I0520 22:12:14.123656 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_1664.solverstate
I0520 22:12:14.413964 20989 solver.cpp:341] Iteration 1666, Testing net (#0)
I0520 22:13:20.563823 20989 solver.cpp:409]     Test net output #0: accuracy = 0.632378
I0520 22:13:20.563995 20989 solver.cpp:409]     Test net output #1: loss = 1.25534 (* 1 = 1.25534 loss)
I0520 22:13:45.711869 20989 solver.cpp:237] Iteration 1681, loss = 1.76122
I0520 22:13:45.711918 20989 solver.cpp:253]     Train net output #0: loss = 1.76122 (* 1 = 1.76122 loss)
I0520 22:13:45.711932 20989 sgd_solver.cpp:106] Iteration 1681, lr = 0.0025
I0520 22:13:53.663861 20989 solver.cpp:237] Iteration 1722, loss = 1.67744
I0520 22:13:53.664003 20989 solver.cpp:253]     Train net output #0: loss = 1.67744 (* 1 = 1.67744 loss)
I0520 22:13:53.664017 20989 sgd_solver.cpp:106] Iteration 1722, lr = 0.0025
I0520 22:14:01.620929 20989 solver.cpp:237] Iteration 1763, loss = 1.68799
I0520 22:14:01.620975 20989 solver.cpp:253]     Train net output #0: loss = 1.68799 (* 1 = 1.68799 loss)
I0520 22:14:01.620990 20989 sgd_solver.cpp:106] Iteration 1763, lr = 0.0025
I0520 22:14:09.579588 20989 solver.cpp:237] Iteration 1804, loss = 1.64272
I0520 22:14:09.579622 20989 solver.cpp:253]     Train net output #0: loss = 1.64272 (* 1 = 1.64272 loss)
I0520 22:14:09.579637 20989 sgd_solver.cpp:106] Iteration 1804, lr = 0.0025
I0520 22:14:17.532349 20989 solver.cpp:237] Iteration 1845, loss = 1.63521
I0520 22:14:17.532383 20989 solver.cpp:253]     Train net output #0: loss = 1.63521 (* 1 = 1.63521 loss)
I0520 22:14:17.532397 20989 sgd_solver.cpp:106] Iteration 1845, lr = 0.0025
I0520 22:14:25.487262 20989 solver.cpp:237] Iteration 1886, loss = 1.64628
I0520 22:14:25.487412 20989 solver.cpp:253]     Train net output #0: loss = 1.64628 (* 1 = 1.64628 loss)
I0520 22:14:25.487427 20989 sgd_solver.cpp:106] Iteration 1886, lr = 0.0025
I0520 22:14:33.442967 20989 solver.cpp:237] Iteration 1927, loss = 1.66203
I0520 22:14:33.443001 20989 solver.cpp:253]     Train net output #0: loss = 1.66203 (* 1 = 1.66203 loss)
I0520 22:14:33.443017 20989 sgd_solver.cpp:106] Iteration 1927, lr = 0.0025
I0520 22:15:03.603986 20989 solver.cpp:237] Iteration 1968, loss = 1.6647
I0520 22:15:03.604151 20989 solver.cpp:253]     Train net output #0: loss = 1.6647 (* 1 = 1.6647 loss)
I0520 22:15:03.604166 20989 sgd_solver.cpp:106] Iteration 1968, lr = 0.0025
I0520 22:15:11.557227 20989 solver.cpp:237] Iteration 2009, loss = 1.64364
I0520 22:15:11.557260 20989 solver.cpp:253]     Train net output #0: loss = 1.64364 (* 1 = 1.64364 loss)
I0520 22:15:11.557276 20989 sgd_solver.cpp:106] Iteration 2009, lr = 0.0025
I0520 22:15:19.510984 20989 solver.cpp:237] Iteration 2050, loss = 1.68135
I0520 22:15:19.511021 20989 solver.cpp:253]     Train net output #0: loss = 1.68135 (* 1 = 1.68135 loss)
I0520 22:15:19.511037 20989 sgd_solver.cpp:106] Iteration 2050, lr = 0.0025
I0520 22:15:25.136250 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_2080.caffemodel
I0520 22:15:25.321804 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_2080.solverstate
I0520 22:15:27.554357 20989 solver.cpp:237] Iteration 2091, loss = 1.58047
I0520 22:15:27.554400 20989 solver.cpp:253]     Train net output #0: loss = 1.58047 (* 1 = 1.58047 loss)
I0520 22:15:27.554417 20989 sgd_solver.cpp:106] Iteration 2091, lr = 0.0025
I0520 22:15:35.509637 20989 solver.cpp:237] Iteration 2132, loss = 1.57103
I0520 22:15:35.509783 20989 solver.cpp:253]     Train net output #0: loss = 1.57103 (* 1 = 1.57103 loss)
I0520 22:15:35.509795 20989 sgd_solver.cpp:106] Iteration 2132, lr = 0.0025
I0520 22:15:43.467231 20989 solver.cpp:237] Iteration 2173, loss = 1.59081
I0520 22:15:43.467278 20989 solver.cpp:253]     Train net output #0: loss = 1.59081 (* 1 = 1.59081 loss)
I0520 22:15:43.467291 20989 sgd_solver.cpp:106] Iteration 2173, lr = 0.0025
I0520 22:15:51.421093 20989 solver.cpp:237] Iteration 2214, loss = 1.84476
I0520 22:15:51.421128 20989 solver.cpp:253]     Train net output #0: loss = 1.84476 (* 1 = 1.84476 loss)
I0520 22:15:51.421142 20989 sgd_solver.cpp:106] Iteration 2214, lr = 0.0025
I0520 22:16:21.551086 20989 solver.cpp:237] Iteration 2255, loss = 1.59975
I0520 22:16:21.551264 20989 solver.cpp:253]     Train net output #0: loss = 1.59975 (* 1 = 1.59975 loss)
I0520 22:16:21.551280 20989 sgd_solver.cpp:106] Iteration 2255, lr = 0.0025
I0520 22:16:29.510192 20989 solver.cpp:237] Iteration 2296, loss = 1.64559
I0520 22:16:29.510239 20989 solver.cpp:253]     Train net output #0: loss = 1.64559 (* 1 = 1.64559 loss)
I0520 22:16:29.510254 20989 sgd_solver.cpp:106] Iteration 2296, lr = 0.0025
I0520 22:16:37.460186 20989 solver.cpp:237] Iteration 2337, loss = 1.53501
I0520 22:16:37.460221 20989 solver.cpp:253]     Train net output #0: loss = 1.53501 (* 1 = 1.53501 loss)
I0520 22:16:37.460234 20989 sgd_solver.cpp:106] Iteration 2337, lr = 0.0025
I0520 22:16:45.413219 20989 solver.cpp:237] Iteration 2378, loss = 1.63274
I0520 22:16:45.413254 20989 solver.cpp:253]     Train net output #0: loss = 1.63274 (* 1 = 1.63274 loss)
I0520 22:16:45.413267 20989 sgd_solver.cpp:106] Iteration 2378, lr = 0.0025
I0520 22:16:53.364913 20989 solver.cpp:237] Iteration 2419, loss = 1.63905
I0520 22:16:53.365054 20989 solver.cpp:253]     Train net output #0: loss = 1.63905 (* 1 = 1.63905 loss)
I0520 22:16:53.365067 20989 sgd_solver.cpp:106] Iteration 2419, lr = 0.0025
I0520 22:17:01.319082 20989 solver.cpp:237] Iteration 2460, loss = 1.59678
I0520 22:17:01.319128 20989 solver.cpp:253]     Train net output #0: loss = 1.59678 (* 1 = 1.59678 loss)
I0520 22:17:01.319142 20989 sgd_solver.cpp:106] Iteration 2460, lr = 0.0025
I0520 22:17:08.109066 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_2496.caffemodel
I0520 22:17:08.294230 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_2496.solverstate
I0520 22:17:08.775215 20989 solver.cpp:341] Iteration 2499, Testing net (#0)
I0520 22:17:53.737311 20989 solver.cpp:409]     Test net output #0: accuracy = 0.689276
I0520 22:17:53.737471 20989 solver.cpp:409]     Test net output #1: loss = 1.07875 (* 1 = 1.07875 loss)
I0520 22:18:16.390692 20989 solver.cpp:237] Iteration 2501, loss = 1.62991
I0520 22:18:16.390741 20989 solver.cpp:253]     Train net output #0: loss = 1.62991 (* 1 = 1.62991 loss)
I0520 22:18:16.390758 20989 sgd_solver.cpp:106] Iteration 2501, lr = 0.0025
I0520 22:18:24.345072 20989 solver.cpp:237] Iteration 2542, loss = 1.65573
I0520 22:18:24.345234 20989 solver.cpp:253]     Train net output #0: loss = 1.65573 (* 1 = 1.65573 loss)
I0520 22:18:24.345247 20989 sgd_solver.cpp:106] Iteration 2542, lr = 0.0025
I0520 22:18:32.296892 20989 solver.cpp:237] Iteration 2583, loss = 1.70266
I0520 22:18:32.296926 20989 solver.cpp:253]     Train net output #0: loss = 1.70266 (* 1 = 1.70266 loss)
I0520 22:18:32.296941 20989 sgd_solver.cpp:106] Iteration 2583, lr = 0.0025
I0520 22:18:40.254343 20989 solver.cpp:237] Iteration 2624, loss = 1.57978
I0520 22:18:40.254384 20989 solver.cpp:253]     Train net output #0: loss = 1.57978 (* 1 = 1.57978 loss)
I0520 22:18:40.254403 20989 sgd_solver.cpp:106] Iteration 2624, lr = 0.0025
I0520 22:18:48.209015 20989 solver.cpp:237] Iteration 2665, loss = 1.65829
I0520 22:18:48.209049 20989 solver.cpp:253]     Train net output #0: loss = 1.65829 (* 1 = 1.65829 loss)
I0520 22:18:48.209064 20989 sgd_solver.cpp:106] Iteration 2665, lr = 0.0025
I0520 22:18:56.165117 20989 solver.cpp:237] Iteration 2706, loss = 1.62556
I0520 22:18:56.165266 20989 solver.cpp:253]     Train net output #0: loss = 1.62556 (* 1 = 1.62556 loss)
I0520 22:18:56.165279 20989 sgd_solver.cpp:106] Iteration 2706, lr = 0.0025
I0520 22:19:04.121064 20989 solver.cpp:237] Iteration 2747, loss = 1.59199
I0520 22:19:04.121098 20989 solver.cpp:253]     Train net output #0: loss = 1.59199 (* 1 = 1.59199 loss)
I0520 22:19:04.121109 20989 sgd_solver.cpp:106] Iteration 2747, lr = 0.0025
I0520 22:19:34.269505 20989 solver.cpp:237] Iteration 2788, loss = 1.56554
I0520 22:19:34.269685 20989 solver.cpp:253]     Train net output #0: loss = 1.56554 (* 1 = 1.56554 loss)
I0520 22:19:34.269701 20989 sgd_solver.cpp:106] Iteration 2788, lr = 0.0025
I0520 22:19:42.229409 20989 solver.cpp:237] Iteration 2829, loss = 1.54199
I0520 22:19:42.229442 20989 solver.cpp:253]     Train net output #0: loss = 1.54199 (* 1 = 1.54199 loss)
I0520 22:19:42.229459 20989 sgd_solver.cpp:106] Iteration 2829, lr = 0.0025
I0520 22:19:50.186728 20989 solver.cpp:237] Iteration 2870, loss = 1.57092
I0520 22:19:50.186772 20989 solver.cpp:253]     Train net output #0: loss = 1.57092 (* 1 = 1.57092 loss)
I0520 22:19:50.186787 20989 sgd_solver.cpp:106] Iteration 2870, lr = 0.0025
I0520 22:19:58.145344 20989 solver.cpp:237] Iteration 2911, loss = 1.60574
I0520 22:19:58.145376 20989 solver.cpp:253]     Train net output #0: loss = 1.60574 (* 1 = 1.60574 loss)
I0520 22:19:58.145392 20989 sgd_solver.cpp:106] Iteration 2911, lr = 0.0025
I0520 22:19:58.145818 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_2912.caffemodel
I0520 22:19:58.329617 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_2912.solverstate
I0520 22:20:06.191442 20989 solver.cpp:237] Iteration 2952, loss = 1.56328
I0520 22:20:06.191601 20989 solver.cpp:253]     Train net output #0: loss = 1.56328 (* 1 = 1.56328 loss)
I0520 22:20:06.191614 20989 sgd_solver.cpp:106] Iteration 2952, lr = 0.0025
I0520 22:20:14.151396 20989 solver.cpp:237] Iteration 2993, loss = 1.59681
I0520 22:20:14.151443 20989 solver.cpp:253]     Train net output #0: loss = 1.59681 (* 1 = 1.59681 loss)
I0520 22:20:14.151458 20989 sgd_solver.cpp:106] Iteration 2993, lr = 0.0025
I0520 22:20:22.106403 20989 solver.cpp:237] Iteration 3034, loss = 1.5649
I0520 22:20:22.106436 20989 solver.cpp:253]     Train net output #0: loss = 1.5649 (* 1 = 1.5649 loss)
I0520 22:20:22.106451 20989 sgd_solver.cpp:106] Iteration 3034, lr = 0.0025
I0520 22:20:52.228502 20989 solver.cpp:237] Iteration 3075, loss = 1.63186
I0520 22:20:52.228662 20989 solver.cpp:253]     Train net output #0: loss = 1.63186 (* 1 = 1.63186 loss)
I0520 22:20:52.228677 20989 sgd_solver.cpp:106] Iteration 3075, lr = 0.0025
I0520 22:21:00.181900 20989 solver.cpp:237] Iteration 3116, loss = 1.56826
I0520 22:21:00.181933 20989 solver.cpp:253]     Train net output #0: loss = 1.56826 (* 1 = 1.56826 loss)
I0520 22:21:00.181949 20989 sgd_solver.cpp:106] Iteration 3116, lr = 0.0025
I0520 22:21:08.141428 20989 solver.cpp:237] Iteration 3157, loss = 1.53127
I0520 22:21:08.141474 20989 solver.cpp:253]     Train net output #0: loss = 1.53127 (* 1 = 1.53127 loss)
I0520 22:21:08.141487 20989 sgd_solver.cpp:106] Iteration 3157, lr = 0.0025
I0520 22:21:16.094666 20989 solver.cpp:237] Iteration 3198, loss = 1.58777
I0520 22:21:16.094699 20989 solver.cpp:253]     Train net output #0: loss = 1.58777 (* 1 = 1.58777 loss)
I0520 22:21:16.094715 20989 sgd_solver.cpp:106] Iteration 3198, lr = 0.0025
I0520 22:21:24.047034 20989 solver.cpp:237] Iteration 3239, loss = 1.52707
I0520 22:21:24.047171 20989 solver.cpp:253]     Train net output #0: loss = 1.52707 (* 1 = 1.52707 loss)
I0520 22:21:24.047184 20989 sgd_solver.cpp:106] Iteration 3239, lr = 0.0025
I0520 22:21:32.001719 20989 solver.cpp:237] Iteration 3280, loss = 1.57379
I0520 22:21:32.001761 20989 solver.cpp:253]     Train net output #0: loss = 1.57379 (* 1 = 1.57379 loss)
I0520 22:21:32.001778 20989 sgd_solver.cpp:106] Iteration 3280, lr = 0.0025
I0520 22:21:39.955346 20989 solver.cpp:237] Iteration 3321, loss = 1.51043
I0520 22:21:39.955379 20989 solver.cpp:253]     Train net output #0: loss = 1.51043 (* 1 = 1.51043 loss)
I0520 22:21:39.955394 20989 sgd_solver.cpp:106] Iteration 3321, lr = 0.0025
I0520 22:21:41.118842 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_3328.caffemodel
I0520 22:21:41.302042 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_3328.solverstate
I0520 22:21:41.973325 20989 solver.cpp:341] Iteration 3332, Testing net (#0)
I0520 22:22:48.164832 20989 solver.cpp:409]     Test net output #0: accuracy = 0.728353
I0520 22:22:48.165006 20989 solver.cpp:409]     Test net output #1: loss = 0.895165 (* 1 = 0.895165 loss)
I0520 22:23:16.213795 20989 solver.cpp:237] Iteration 3362, loss = 1.56013
I0520 22:23:16.213845 20989 solver.cpp:253]     Train net output #0: loss = 1.56013 (* 1 = 1.56013 loss)
I0520 22:23:16.213861 20989 sgd_solver.cpp:106] Iteration 3362, lr = 0.0025
I0520 22:23:24.170713 20989 solver.cpp:237] Iteration 3403, loss = 1.5331
I0520 22:23:24.170879 20989 solver.cpp:253]     Train net output #0: loss = 1.5331 (* 1 = 1.5331 loss)
I0520 22:23:24.170892 20989 sgd_solver.cpp:106] Iteration 3403, lr = 0.0025
I0520 22:23:32.126762 20989 solver.cpp:237] Iteration 3444, loss = 1.50384
I0520 22:23:32.126796 20989 solver.cpp:253]     Train net output #0: loss = 1.50384 (* 1 = 1.50384 loss)
I0520 22:23:32.126811 20989 sgd_solver.cpp:106] Iteration 3444, lr = 0.0025
I0520 22:23:40.081430 20989 solver.cpp:237] Iteration 3485, loss = 1.54884
I0520 22:23:40.081470 20989 solver.cpp:253]     Train net output #0: loss = 1.54884 (* 1 = 1.54884 loss)
I0520 22:23:40.081490 20989 sgd_solver.cpp:106] Iteration 3485, lr = 0.0025
I0520 22:23:48.036712 20989 solver.cpp:237] Iteration 3526, loss = 1.46642
I0520 22:23:48.036746 20989 solver.cpp:253]     Train net output #0: loss = 1.46642 (* 1 = 1.46642 loss)
I0520 22:23:48.036761 20989 sgd_solver.cpp:106] Iteration 3526, lr = 0.0025
I0520 22:23:55.993351 20989 solver.cpp:237] Iteration 3567, loss = 1.53715
I0520 22:23:55.993494 20989 solver.cpp:253]     Train net output #0: loss = 1.53715 (* 1 = 1.53715 loss)
I0520 22:23:55.993507 20989 sgd_solver.cpp:106] Iteration 3567, lr = 0.0025
I0520 22:24:03.947248 20989 solver.cpp:237] Iteration 3608, loss = 1.56353
I0520 22:24:03.947293 20989 solver.cpp:253]     Train net output #0: loss = 1.56353 (* 1 = 1.56353 loss)
I0520 22:24:03.947307 20989 sgd_solver.cpp:106] Iteration 3608, lr = 0.0025
I0520 22:24:34.063920 20989 solver.cpp:237] Iteration 3649, loss = 1.81048
I0520 22:24:34.064083 20989 solver.cpp:253]     Train net output #0: loss = 1.81048 (* 1 = 1.81048 loss)
I0520 22:24:34.064098 20989 sgd_solver.cpp:106] Iteration 3649, lr = 0.0025
I0520 22:24:42.020572 20989 solver.cpp:237] Iteration 3690, loss = 1.49812
I0520 22:24:42.020606 20989 solver.cpp:253]     Train net output #0: loss = 1.49812 (* 1 = 1.49812 loss)
I0520 22:24:42.020622 20989 sgd_solver.cpp:106] Iteration 3690, lr = 0.0025
I0520 22:24:49.974198 20989 solver.cpp:237] Iteration 3731, loss = 1.52664
I0520 22:24:49.974241 20989 solver.cpp:253]     Train net output #0: loss = 1.52664 (* 1 = 1.52664 loss)
I0520 22:24:49.974256 20989 sgd_solver.cpp:106] Iteration 3731, lr = 0.0025
I0520 22:24:52.300125 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_3744.caffemodel
I0520 22:24:52.485193 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_3744.solverstate
I0520 22:24:58.011040 20989 solver.cpp:237] Iteration 3772, loss = 1.54034
I0520 22:24:58.011088 20989 solver.cpp:253]     Train net output #0: loss = 1.54034 (* 1 = 1.54034 loss)
I0520 22:24:58.011103 20989 sgd_solver.cpp:106] Iteration 3772, lr = 0.0025
I0520 22:25:05.965679 20989 solver.cpp:237] Iteration 3813, loss = 1.47202
I0520 22:25:05.965832 20989 solver.cpp:253]     Train net output #0: loss = 1.47202 (* 1 = 1.47202 loss)
I0520 22:25:05.965847 20989 sgd_solver.cpp:106] Iteration 3813, lr = 0.0025
I0520 22:25:13.918525 20989 solver.cpp:237] Iteration 3854, loss = 1.50132
I0520 22:25:13.918572 20989 solver.cpp:253]     Train net output #0: loss = 1.50132 (* 1 = 1.50132 loss)
I0520 22:25:13.918589 20989 sgd_solver.cpp:106] Iteration 3854, lr = 0.0025
I0520 22:25:44.096302 20989 solver.cpp:237] Iteration 3895, loss = 1.66278
I0520 22:25:44.096470 20989 solver.cpp:253]     Train net output #0: loss = 1.66278 (* 1 = 1.66278 loss)
I0520 22:25:44.096485 20989 sgd_solver.cpp:106] Iteration 3895, lr = 0.0025
I0520 22:25:52.050284 20989 solver.cpp:237] Iteration 3936, loss = 1.39777
I0520 22:25:52.050318 20989 solver.cpp:253]     Train net output #0: loss = 1.39777 (* 1 = 1.39777 loss)
I0520 22:25:52.050334 20989 sgd_solver.cpp:106] Iteration 3936, lr = 0.0025
I0520 22:26:00.004619 20989 solver.cpp:237] Iteration 3977, loss = 1.45138
I0520 22:26:00.004653 20989 solver.cpp:253]     Train net output #0: loss = 1.45138 (* 1 = 1.45138 loss)
I0520 22:26:00.004669 20989 sgd_solver.cpp:106] Iteration 3977, lr = 0.0025
I0520 22:26:07.958447 20989 solver.cpp:237] Iteration 4018, loss = 1.52512
I0520 22:26:07.958492 20989 solver.cpp:253]     Train net output #0: loss = 1.52512 (* 1 = 1.52512 loss)
I0520 22:26:07.958504 20989 sgd_solver.cpp:106] Iteration 4018, lr = 0.0025
I0520 22:26:15.915467 20989 solver.cpp:237] Iteration 4059, loss = 1.5088
I0520 22:26:15.915607 20989 solver.cpp:253]     Train net output #0: loss = 1.5088 (* 1 = 1.5088 loss)
I0520 22:26:15.915621 20989 sgd_solver.cpp:106] Iteration 4059, lr = 0.0025
I0520 22:26:23.871593 20989 solver.cpp:237] Iteration 4100, loss = 1.48839
I0520 22:26:23.871625 20989 solver.cpp:253]     Train net output #0: loss = 1.48839 (* 1 = 1.48839 loss)
I0520 22:26:23.871641 20989 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0520 22:26:31.825893 20989 solver.cpp:237] Iteration 4141, loss = 1.42964
I0520 22:26:31.825933 20989 solver.cpp:253]     Train net output #0: loss = 1.42964 (* 1 = 1.42964 loss)
I0520 22:26:31.825947 20989 sgd_solver.cpp:106] Iteration 4141, lr = 0.0025
I0520 22:26:35.317060 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_4160.caffemodel
I0520 22:26:35.506326 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_4160.solverstate
I0520 22:26:36.373698 20989 solver.cpp:341] Iteration 4165, Testing net (#0)
I0520 22:27:21.684396 20989 solver.cpp:409]     Test net output #0: accuracy = 0.772422
I0520 22:27:21.684573 20989 solver.cpp:409]     Test net output #1: loss = 0.827715 (* 1 = 0.827715 loss)
I0520 22:27:21.742838 20989 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_4166.caffemodel
I0520 22:27:21.929906 20989 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822_iter_4166.solverstate
I0520 22:27:21.967522 20989 solver.cpp:326] Optimization Done.
I0520 22:27:21.967550 20989 caffe.cpp:215] Optimization Done.
Application 11235614 resources: utime ~1253s, stime ~226s, Rss ~5329276, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_360_2016-05-20T11.20.45.815822.solver"
	User time (seconds): 0.54
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:43.23
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15106
	Voluntary context switches: 2730
	Involuntary context switches: 85
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
