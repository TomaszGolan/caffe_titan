2806140
I0521 01:42:28.344027  7546 caffe.cpp:184] Using GPUs 0
I0521 01:42:28.771935  7546 solver.cpp:48] Initializing solver from parameters: 
test_iter: 283
test_interval: 566
base_lr: 0.0025
display: 28
max_iter: 2830
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 283
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129.prototxt"
I0521 01:42:28.773425  7546 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129.prototxt
I0521 01:42:28.800259  7546 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 01:42:28.800318  7546 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 01:42:28.800659  7546 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 530
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:42:28.800835  7546 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:42:28.800859  7546 net.cpp:106] Creating Layer data_hdf5
I0521 01:42:28.800874  7546 net.cpp:411] data_hdf5 -> data
I0521 01:42:28.800907  7546 net.cpp:411] data_hdf5 -> label
I0521 01:42:28.800940  7546 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 01:42:28.805018  7546 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 01:42:28.807258  7546 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 01:42:50.395395  7546 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 01:42:50.400646  7546 net.cpp:150] Setting up data_hdf5
I0521 01:42:50.400686  7546 net.cpp:157] Top shape: 530 1 127 50 (3365500)
I0521 01:42:50.400701  7546 net.cpp:157] Top shape: 530 (530)
I0521 01:42:50.400713  7546 net.cpp:165] Memory required for data: 13464120
I0521 01:42:50.400727  7546 layer_factory.hpp:77] Creating layer conv1
I0521 01:42:50.400761  7546 net.cpp:106] Creating Layer conv1
I0521 01:42:50.400773  7546 net.cpp:454] conv1 <- data
I0521 01:42:50.400795  7546 net.cpp:411] conv1 -> conv1
I0521 01:42:50.930605  7546 net.cpp:150] Setting up conv1
I0521 01:42:50.930652  7546 net.cpp:157] Top shape: 530 12 120 48 (36633600)
I0521 01:42:50.930665  7546 net.cpp:165] Memory required for data: 159998520
I0521 01:42:50.930692  7546 layer_factory.hpp:77] Creating layer relu1
I0521 01:42:50.930714  7546 net.cpp:106] Creating Layer relu1
I0521 01:42:50.930726  7546 net.cpp:454] relu1 <- conv1
I0521 01:42:50.930738  7546 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:42:50.931253  7546 net.cpp:150] Setting up relu1
I0521 01:42:50.931270  7546 net.cpp:157] Top shape: 530 12 120 48 (36633600)
I0521 01:42:50.931282  7546 net.cpp:165] Memory required for data: 306532920
I0521 01:42:50.931291  7546 layer_factory.hpp:77] Creating layer pool1
I0521 01:42:50.931309  7546 net.cpp:106] Creating Layer pool1
I0521 01:42:50.931319  7546 net.cpp:454] pool1 <- conv1
I0521 01:42:50.931332  7546 net.cpp:411] pool1 -> pool1
I0521 01:42:50.931413  7546 net.cpp:150] Setting up pool1
I0521 01:42:50.931427  7546 net.cpp:157] Top shape: 530 12 60 48 (18316800)
I0521 01:42:50.931437  7546 net.cpp:165] Memory required for data: 379800120
I0521 01:42:50.931448  7546 layer_factory.hpp:77] Creating layer conv2
I0521 01:42:50.931470  7546 net.cpp:106] Creating Layer conv2
I0521 01:42:50.931481  7546 net.cpp:454] conv2 <- pool1
I0521 01:42:50.931495  7546 net.cpp:411] conv2 -> conv2
I0521 01:42:50.934188  7546 net.cpp:150] Setting up conv2
I0521 01:42:50.934211  7546 net.cpp:157] Top shape: 530 20 54 46 (26330400)
I0521 01:42:50.934228  7546 net.cpp:165] Memory required for data: 485121720
I0521 01:42:50.934247  7546 layer_factory.hpp:77] Creating layer relu2
I0521 01:42:50.934262  7546 net.cpp:106] Creating Layer relu2
I0521 01:42:50.934272  7546 net.cpp:454] relu2 <- conv2
I0521 01:42:50.934284  7546 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:42:50.934615  7546 net.cpp:150] Setting up relu2
I0521 01:42:50.934629  7546 net.cpp:157] Top shape: 530 20 54 46 (26330400)
I0521 01:42:50.934639  7546 net.cpp:165] Memory required for data: 590443320
I0521 01:42:50.934649  7546 layer_factory.hpp:77] Creating layer pool2
I0521 01:42:50.934662  7546 net.cpp:106] Creating Layer pool2
I0521 01:42:50.934672  7546 net.cpp:454] pool2 <- conv2
I0521 01:42:50.934696  7546 net.cpp:411] pool2 -> pool2
I0521 01:42:50.934765  7546 net.cpp:150] Setting up pool2
I0521 01:42:50.934779  7546 net.cpp:157] Top shape: 530 20 27 46 (13165200)
I0521 01:42:50.934789  7546 net.cpp:165] Memory required for data: 643104120
I0521 01:42:50.934798  7546 layer_factory.hpp:77] Creating layer conv3
I0521 01:42:50.934818  7546 net.cpp:106] Creating Layer conv3
I0521 01:42:50.934828  7546 net.cpp:454] conv3 <- pool2
I0521 01:42:50.934842  7546 net.cpp:411] conv3 -> conv3
I0521 01:42:50.936755  7546 net.cpp:150] Setting up conv3
I0521 01:42:50.936779  7546 net.cpp:157] Top shape: 530 28 22 44 (14365120)
I0521 01:42:50.936791  7546 net.cpp:165] Memory required for data: 700564600
I0521 01:42:50.936810  7546 layer_factory.hpp:77] Creating layer relu3
I0521 01:42:50.936825  7546 net.cpp:106] Creating Layer relu3
I0521 01:42:50.936835  7546 net.cpp:454] relu3 <- conv3
I0521 01:42:50.936847  7546 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:42:50.937314  7546 net.cpp:150] Setting up relu3
I0521 01:42:50.937331  7546 net.cpp:157] Top shape: 530 28 22 44 (14365120)
I0521 01:42:50.937341  7546 net.cpp:165] Memory required for data: 758025080
I0521 01:42:50.937351  7546 layer_factory.hpp:77] Creating layer pool3
I0521 01:42:50.937364  7546 net.cpp:106] Creating Layer pool3
I0521 01:42:50.937373  7546 net.cpp:454] pool3 <- conv3
I0521 01:42:50.937386  7546 net.cpp:411] pool3 -> pool3
I0521 01:42:50.937453  7546 net.cpp:150] Setting up pool3
I0521 01:42:50.937466  7546 net.cpp:157] Top shape: 530 28 11 44 (7182560)
I0521 01:42:50.937476  7546 net.cpp:165] Memory required for data: 786755320
I0521 01:42:50.937486  7546 layer_factory.hpp:77] Creating layer conv4
I0521 01:42:50.937500  7546 net.cpp:106] Creating Layer conv4
I0521 01:42:50.937511  7546 net.cpp:454] conv4 <- pool3
I0521 01:42:50.937525  7546 net.cpp:411] conv4 -> conv4
I0521 01:42:50.940366  7546 net.cpp:150] Setting up conv4
I0521 01:42:50.940394  7546 net.cpp:157] Top shape: 530 36 6 42 (4808160)
I0521 01:42:50.940405  7546 net.cpp:165] Memory required for data: 805987960
I0521 01:42:50.940421  7546 layer_factory.hpp:77] Creating layer relu4
I0521 01:42:50.940435  7546 net.cpp:106] Creating Layer relu4
I0521 01:42:50.940446  7546 net.cpp:454] relu4 <- conv4
I0521 01:42:50.940459  7546 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:42:50.940933  7546 net.cpp:150] Setting up relu4
I0521 01:42:50.940950  7546 net.cpp:157] Top shape: 530 36 6 42 (4808160)
I0521 01:42:50.940960  7546 net.cpp:165] Memory required for data: 825220600
I0521 01:42:50.940970  7546 layer_factory.hpp:77] Creating layer pool4
I0521 01:42:50.940984  7546 net.cpp:106] Creating Layer pool4
I0521 01:42:50.940994  7546 net.cpp:454] pool4 <- conv4
I0521 01:42:50.941006  7546 net.cpp:411] pool4 -> pool4
I0521 01:42:50.941074  7546 net.cpp:150] Setting up pool4
I0521 01:42:50.941088  7546 net.cpp:157] Top shape: 530 36 3 42 (2404080)
I0521 01:42:50.941098  7546 net.cpp:165] Memory required for data: 834836920
I0521 01:42:50.941108  7546 layer_factory.hpp:77] Creating layer ip1
I0521 01:42:50.941126  7546 net.cpp:106] Creating Layer ip1
I0521 01:42:50.941136  7546 net.cpp:454] ip1 <- pool4
I0521 01:42:50.941149  7546 net.cpp:411] ip1 -> ip1
I0521 01:42:50.956616  7546 net.cpp:150] Setting up ip1
I0521 01:42:50.956645  7546 net.cpp:157] Top shape: 530 196 (103880)
I0521 01:42:50.956657  7546 net.cpp:165] Memory required for data: 835252440
I0521 01:42:50.956679  7546 layer_factory.hpp:77] Creating layer relu5
I0521 01:42:50.956694  7546 net.cpp:106] Creating Layer relu5
I0521 01:42:50.956704  7546 net.cpp:454] relu5 <- ip1
I0521 01:42:50.956717  7546 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:42:50.957059  7546 net.cpp:150] Setting up relu5
I0521 01:42:50.957074  7546 net.cpp:157] Top shape: 530 196 (103880)
I0521 01:42:50.957084  7546 net.cpp:165] Memory required for data: 835667960
I0521 01:42:50.957095  7546 layer_factory.hpp:77] Creating layer drop1
I0521 01:42:50.957116  7546 net.cpp:106] Creating Layer drop1
I0521 01:42:50.957126  7546 net.cpp:454] drop1 <- ip1
I0521 01:42:50.957150  7546 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:42:50.957197  7546 net.cpp:150] Setting up drop1
I0521 01:42:50.957211  7546 net.cpp:157] Top shape: 530 196 (103880)
I0521 01:42:50.957221  7546 net.cpp:165] Memory required for data: 836083480
I0521 01:42:50.957231  7546 layer_factory.hpp:77] Creating layer ip2
I0521 01:42:50.957248  7546 net.cpp:106] Creating Layer ip2
I0521 01:42:50.957258  7546 net.cpp:454] ip2 <- ip1
I0521 01:42:50.957272  7546 net.cpp:411] ip2 -> ip2
I0521 01:42:50.957742  7546 net.cpp:150] Setting up ip2
I0521 01:42:50.957756  7546 net.cpp:157] Top shape: 530 98 (51940)
I0521 01:42:50.957765  7546 net.cpp:165] Memory required for data: 836291240
I0521 01:42:50.957780  7546 layer_factory.hpp:77] Creating layer relu6
I0521 01:42:50.957793  7546 net.cpp:106] Creating Layer relu6
I0521 01:42:50.957803  7546 net.cpp:454] relu6 <- ip2
I0521 01:42:50.957813  7546 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:42:50.958333  7546 net.cpp:150] Setting up relu6
I0521 01:42:50.958349  7546 net.cpp:157] Top shape: 530 98 (51940)
I0521 01:42:50.958360  7546 net.cpp:165] Memory required for data: 836499000
I0521 01:42:50.958369  7546 layer_factory.hpp:77] Creating layer drop2
I0521 01:42:50.958382  7546 net.cpp:106] Creating Layer drop2
I0521 01:42:50.958392  7546 net.cpp:454] drop2 <- ip2
I0521 01:42:50.958405  7546 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:42:50.958447  7546 net.cpp:150] Setting up drop2
I0521 01:42:50.958461  7546 net.cpp:157] Top shape: 530 98 (51940)
I0521 01:42:50.958470  7546 net.cpp:165] Memory required for data: 836706760
I0521 01:42:50.958480  7546 layer_factory.hpp:77] Creating layer ip3
I0521 01:42:50.958493  7546 net.cpp:106] Creating Layer ip3
I0521 01:42:50.958503  7546 net.cpp:454] ip3 <- ip2
I0521 01:42:50.958515  7546 net.cpp:411] ip3 -> ip3
I0521 01:42:50.958724  7546 net.cpp:150] Setting up ip3
I0521 01:42:50.958737  7546 net.cpp:157] Top shape: 530 11 (5830)
I0521 01:42:50.958746  7546 net.cpp:165] Memory required for data: 836730080
I0521 01:42:50.958761  7546 layer_factory.hpp:77] Creating layer drop3
I0521 01:42:50.958775  7546 net.cpp:106] Creating Layer drop3
I0521 01:42:50.958783  7546 net.cpp:454] drop3 <- ip3
I0521 01:42:50.958796  7546 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:42:50.958833  7546 net.cpp:150] Setting up drop3
I0521 01:42:50.958847  7546 net.cpp:157] Top shape: 530 11 (5830)
I0521 01:42:50.958856  7546 net.cpp:165] Memory required for data: 836753400
I0521 01:42:50.958865  7546 layer_factory.hpp:77] Creating layer loss
I0521 01:42:50.958884  7546 net.cpp:106] Creating Layer loss
I0521 01:42:50.958894  7546 net.cpp:454] loss <- ip3
I0521 01:42:50.958904  7546 net.cpp:454] loss <- label
I0521 01:42:50.958916  7546 net.cpp:411] loss -> loss
I0521 01:42:50.958933  7546 layer_factory.hpp:77] Creating layer loss
I0521 01:42:50.959578  7546 net.cpp:150] Setting up loss
I0521 01:42:50.959592  7546 net.cpp:157] Top shape: (1)
I0521 01:42:50.959602  7546 net.cpp:160]     with loss weight 1
I0521 01:42:50.959645  7546 net.cpp:165] Memory required for data: 836753404
I0521 01:42:50.959656  7546 net.cpp:226] loss needs backward computation.
I0521 01:42:50.959666  7546 net.cpp:226] drop3 needs backward computation.
I0521 01:42:50.959676  7546 net.cpp:226] ip3 needs backward computation.
I0521 01:42:50.959687  7546 net.cpp:226] drop2 needs backward computation.
I0521 01:42:50.959697  7546 net.cpp:226] relu6 needs backward computation.
I0521 01:42:50.959707  7546 net.cpp:226] ip2 needs backward computation.
I0521 01:42:50.959717  7546 net.cpp:226] drop1 needs backward computation.
I0521 01:42:50.959725  7546 net.cpp:226] relu5 needs backward computation.
I0521 01:42:50.959735  7546 net.cpp:226] ip1 needs backward computation.
I0521 01:42:50.959745  7546 net.cpp:226] pool4 needs backward computation.
I0521 01:42:50.959754  7546 net.cpp:226] relu4 needs backward computation.
I0521 01:42:50.959764  7546 net.cpp:226] conv4 needs backward computation.
I0521 01:42:50.959775  7546 net.cpp:226] pool3 needs backward computation.
I0521 01:42:50.959794  7546 net.cpp:226] relu3 needs backward computation.
I0521 01:42:50.959805  7546 net.cpp:226] conv3 needs backward computation.
I0521 01:42:50.959815  7546 net.cpp:226] pool2 needs backward computation.
I0521 01:42:50.959827  7546 net.cpp:226] relu2 needs backward computation.
I0521 01:42:50.959837  7546 net.cpp:226] conv2 needs backward computation.
I0521 01:42:50.959847  7546 net.cpp:226] pool1 needs backward computation.
I0521 01:42:50.959857  7546 net.cpp:226] relu1 needs backward computation.
I0521 01:42:50.959867  7546 net.cpp:226] conv1 needs backward computation.
I0521 01:42:50.959878  7546 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:42:50.959888  7546 net.cpp:270] This network produces output loss
I0521 01:42:50.959913  7546 net.cpp:283] Network initialization done.
I0521 01:42:50.961462  7546 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129.prototxt
I0521 01:42:50.961534  7546 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 01:42:50.961905  7546 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 530
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:42:50.962095  7546 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:42:50.962110  7546 net.cpp:106] Creating Layer data_hdf5
I0521 01:42:50.962123  7546 net.cpp:411] data_hdf5 -> data
I0521 01:42:50.962139  7546 net.cpp:411] data_hdf5 -> label
I0521 01:42:50.962155  7546 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 01:42:50.985630  7546 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 01:43:12.304528  7546 net.cpp:150] Setting up data_hdf5
I0521 01:43:12.304692  7546 net.cpp:157] Top shape: 530 1 127 50 (3365500)
I0521 01:43:12.304708  7546 net.cpp:157] Top shape: 530 (530)
I0521 01:43:12.304719  7546 net.cpp:165] Memory required for data: 13464120
I0521 01:43:12.304733  7546 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 01:43:12.304761  7546 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 01:43:12.304772  7546 net.cpp:454] label_data_hdf5_1_split <- label
I0521 01:43:12.304787  7546 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 01:43:12.304808  7546 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 01:43:12.304882  7546 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 01:43:12.304895  7546 net.cpp:157] Top shape: 530 (530)
I0521 01:43:12.304908  7546 net.cpp:157] Top shape: 530 (530)
I0521 01:43:12.304916  7546 net.cpp:165] Memory required for data: 13468360
I0521 01:43:12.304926  7546 layer_factory.hpp:77] Creating layer conv1
I0521 01:43:12.304949  7546 net.cpp:106] Creating Layer conv1
I0521 01:43:12.304958  7546 net.cpp:454] conv1 <- data
I0521 01:43:12.304973  7546 net.cpp:411] conv1 -> conv1
I0521 01:43:12.306926  7546 net.cpp:150] Setting up conv1
I0521 01:43:12.306951  7546 net.cpp:157] Top shape: 530 12 120 48 (36633600)
I0521 01:43:12.306962  7546 net.cpp:165] Memory required for data: 160002760
I0521 01:43:12.306983  7546 layer_factory.hpp:77] Creating layer relu1
I0521 01:43:12.306998  7546 net.cpp:106] Creating Layer relu1
I0521 01:43:12.307008  7546 net.cpp:454] relu1 <- conv1
I0521 01:43:12.307021  7546 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:43:12.307518  7546 net.cpp:150] Setting up relu1
I0521 01:43:12.307535  7546 net.cpp:157] Top shape: 530 12 120 48 (36633600)
I0521 01:43:12.307545  7546 net.cpp:165] Memory required for data: 306537160
I0521 01:43:12.307555  7546 layer_factory.hpp:77] Creating layer pool1
I0521 01:43:12.307571  7546 net.cpp:106] Creating Layer pool1
I0521 01:43:12.307581  7546 net.cpp:454] pool1 <- conv1
I0521 01:43:12.307591  7546 net.cpp:411] pool1 -> pool1
I0521 01:43:12.307667  7546 net.cpp:150] Setting up pool1
I0521 01:43:12.307680  7546 net.cpp:157] Top shape: 530 12 60 48 (18316800)
I0521 01:43:12.307690  7546 net.cpp:165] Memory required for data: 379804360
I0521 01:43:12.307698  7546 layer_factory.hpp:77] Creating layer conv2
I0521 01:43:12.307715  7546 net.cpp:106] Creating Layer conv2
I0521 01:43:12.307725  7546 net.cpp:454] conv2 <- pool1
I0521 01:43:12.307740  7546 net.cpp:411] conv2 -> conv2
I0521 01:43:12.309653  7546 net.cpp:150] Setting up conv2
I0521 01:43:12.309676  7546 net.cpp:157] Top shape: 530 20 54 46 (26330400)
I0521 01:43:12.309689  7546 net.cpp:165] Memory required for data: 485125960
I0521 01:43:12.309715  7546 layer_factory.hpp:77] Creating layer relu2
I0521 01:43:12.309728  7546 net.cpp:106] Creating Layer relu2
I0521 01:43:12.309738  7546 net.cpp:454] relu2 <- conv2
I0521 01:43:12.309751  7546 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:43:12.310086  7546 net.cpp:150] Setting up relu2
I0521 01:43:12.310099  7546 net.cpp:157] Top shape: 530 20 54 46 (26330400)
I0521 01:43:12.310109  7546 net.cpp:165] Memory required for data: 590447560
I0521 01:43:12.310119  7546 layer_factory.hpp:77] Creating layer pool2
I0521 01:43:12.310132  7546 net.cpp:106] Creating Layer pool2
I0521 01:43:12.310142  7546 net.cpp:454] pool2 <- conv2
I0521 01:43:12.310154  7546 net.cpp:411] pool2 -> pool2
I0521 01:43:12.310225  7546 net.cpp:150] Setting up pool2
I0521 01:43:12.310238  7546 net.cpp:157] Top shape: 530 20 27 46 (13165200)
I0521 01:43:12.310248  7546 net.cpp:165] Memory required for data: 643108360
I0521 01:43:12.310259  7546 layer_factory.hpp:77] Creating layer conv3
I0521 01:43:12.310278  7546 net.cpp:106] Creating Layer conv3
I0521 01:43:12.310288  7546 net.cpp:454] conv3 <- pool2
I0521 01:43:12.310303  7546 net.cpp:411] conv3 -> conv3
I0521 01:43:12.312264  7546 net.cpp:150] Setting up conv3
I0521 01:43:12.312288  7546 net.cpp:157] Top shape: 530 28 22 44 (14365120)
I0521 01:43:12.312299  7546 net.cpp:165] Memory required for data: 700568840
I0521 01:43:12.312332  7546 layer_factory.hpp:77] Creating layer relu3
I0521 01:43:12.312345  7546 net.cpp:106] Creating Layer relu3
I0521 01:43:12.312356  7546 net.cpp:454] relu3 <- conv3
I0521 01:43:12.312369  7546 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:43:12.312836  7546 net.cpp:150] Setting up relu3
I0521 01:43:12.312852  7546 net.cpp:157] Top shape: 530 28 22 44 (14365120)
I0521 01:43:12.312863  7546 net.cpp:165] Memory required for data: 758029320
I0521 01:43:12.312873  7546 layer_factory.hpp:77] Creating layer pool3
I0521 01:43:12.312886  7546 net.cpp:106] Creating Layer pool3
I0521 01:43:12.312896  7546 net.cpp:454] pool3 <- conv3
I0521 01:43:12.312909  7546 net.cpp:411] pool3 -> pool3
I0521 01:43:12.312980  7546 net.cpp:150] Setting up pool3
I0521 01:43:12.312994  7546 net.cpp:157] Top shape: 530 28 11 44 (7182560)
I0521 01:43:12.313004  7546 net.cpp:165] Memory required for data: 786759560
I0521 01:43:12.313014  7546 layer_factory.hpp:77] Creating layer conv4
I0521 01:43:12.313031  7546 net.cpp:106] Creating Layer conv4
I0521 01:43:12.313041  7546 net.cpp:454] conv4 <- pool3
I0521 01:43:12.313053  7546 net.cpp:411] conv4 -> conv4
I0521 01:43:12.315124  7546 net.cpp:150] Setting up conv4
I0521 01:43:12.315141  7546 net.cpp:157] Top shape: 530 36 6 42 (4808160)
I0521 01:43:12.315151  7546 net.cpp:165] Memory required for data: 805992200
I0521 01:43:12.315171  7546 layer_factory.hpp:77] Creating layer relu4
I0521 01:43:12.315184  7546 net.cpp:106] Creating Layer relu4
I0521 01:43:12.315194  7546 net.cpp:454] relu4 <- conv4
I0521 01:43:12.315207  7546 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:43:12.315680  7546 net.cpp:150] Setting up relu4
I0521 01:43:12.315696  7546 net.cpp:157] Top shape: 530 36 6 42 (4808160)
I0521 01:43:12.315706  7546 net.cpp:165] Memory required for data: 825224840
I0521 01:43:12.315716  7546 layer_factory.hpp:77] Creating layer pool4
I0521 01:43:12.315729  7546 net.cpp:106] Creating Layer pool4
I0521 01:43:12.315738  7546 net.cpp:454] pool4 <- conv4
I0521 01:43:12.315752  7546 net.cpp:411] pool4 -> pool4
I0521 01:43:12.315824  7546 net.cpp:150] Setting up pool4
I0521 01:43:12.315837  7546 net.cpp:157] Top shape: 530 36 3 42 (2404080)
I0521 01:43:12.315847  7546 net.cpp:165] Memory required for data: 834841160
I0521 01:43:12.315856  7546 layer_factory.hpp:77] Creating layer ip1
I0521 01:43:12.315870  7546 net.cpp:106] Creating Layer ip1
I0521 01:43:12.315881  7546 net.cpp:454] ip1 <- pool4
I0521 01:43:12.315893  7546 net.cpp:411] ip1 -> ip1
I0521 01:43:12.331337  7546 net.cpp:150] Setting up ip1
I0521 01:43:12.331367  7546 net.cpp:157] Top shape: 530 196 (103880)
I0521 01:43:12.331377  7546 net.cpp:165] Memory required for data: 835256680
I0521 01:43:12.331399  7546 layer_factory.hpp:77] Creating layer relu5
I0521 01:43:12.331414  7546 net.cpp:106] Creating Layer relu5
I0521 01:43:12.331424  7546 net.cpp:454] relu5 <- ip1
I0521 01:43:12.331439  7546 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:43:12.331784  7546 net.cpp:150] Setting up relu5
I0521 01:43:12.331797  7546 net.cpp:157] Top shape: 530 196 (103880)
I0521 01:43:12.331807  7546 net.cpp:165] Memory required for data: 835672200
I0521 01:43:12.331817  7546 layer_factory.hpp:77] Creating layer drop1
I0521 01:43:12.331837  7546 net.cpp:106] Creating Layer drop1
I0521 01:43:12.331847  7546 net.cpp:454] drop1 <- ip1
I0521 01:43:12.331861  7546 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:43:12.331904  7546 net.cpp:150] Setting up drop1
I0521 01:43:12.331917  7546 net.cpp:157] Top shape: 530 196 (103880)
I0521 01:43:12.331928  7546 net.cpp:165] Memory required for data: 836087720
I0521 01:43:12.331938  7546 layer_factory.hpp:77] Creating layer ip2
I0521 01:43:12.331951  7546 net.cpp:106] Creating Layer ip2
I0521 01:43:12.331961  7546 net.cpp:454] ip2 <- ip1
I0521 01:43:12.331975  7546 net.cpp:411] ip2 -> ip2
I0521 01:43:12.332458  7546 net.cpp:150] Setting up ip2
I0521 01:43:12.332471  7546 net.cpp:157] Top shape: 530 98 (51940)
I0521 01:43:12.332481  7546 net.cpp:165] Memory required for data: 836295480
I0521 01:43:12.332509  7546 layer_factory.hpp:77] Creating layer relu6
I0521 01:43:12.332521  7546 net.cpp:106] Creating Layer relu6
I0521 01:43:12.332531  7546 net.cpp:454] relu6 <- ip2
I0521 01:43:12.332545  7546 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:43:12.333078  7546 net.cpp:150] Setting up relu6
I0521 01:43:12.333099  7546 net.cpp:157] Top shape: 530 98 (51940)
I0521 01:43:12.333109  7546 net.cpp:165] Memory required for data: 836503240
I0521 01:43:12.333118  7546 layer_factory.hpp:77] Creating layer drop2
I0521 01:43:12.333132  7546 net.cpp:106] Creating Layer drop2
I0521 01:43:12.333142  7546 net.cpp:454] drop2 <- ip2
I0521 01:43:12.333155  7546 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:43:12.333199  7546 net.cpp:150] Setting up drop2
I0521 01:43:12.333212  7546 net.cpp:157] Top shape: 530 98 (51940)
I0521 01:43:12.333222  7546 net.cpp:165] Memory required for data: 836711000
I0521 01:43:12.333232  7546 layer_factory.hpp:77] Creating layer ip3
I0521 01:43:12.333246  7546 net.cpp:106] Creating Layer ip3
I0521 01:43:12.333256  7546 net.cpp:454] ip3 <- ip2
I0521 01:43:12.333269  7546 net.cpp:411] ip3 -> ip3
I0521 01:43:12.333493  7546 net.cpp:150] Setting up ip3
I0521 01:43:12.333506  7546 net.cpp:157] Top shape: 530 11 (5830)
I0521 01:43:12.333516  7546 net.cpp:165] Memory required for data: 836734320
I0521 01:43:12.333531  7546 layer_factory.hpp:77] Creating layer drop3
I0521 01:43:12.333544  7546 net.cpp:106] Creating Layer drop3
I0521 01:43:12.333552  7546 net.cpp:454] drop3 <- ip3
I0521 01:43:12.333565  7546 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:43:12.333606  7546 net.cpp:150] Setting up drop3
I0521 01:43:12.333618  7546 net.cpp:157] Top shape: 530 11 (5830)
I0521 01:43:12.333628  7546 net.cpp:165] Memory required for data: 836757640
I0521 01:43:12.333638  7546 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 01:43:12.333650  7546 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 01:43:12.333660  7546 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 01:43:12.333673  7546 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 01:43:12.333688  7546 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 01:43:12.333770  7546 net.cpp:150] Setting up ip3_drop3_0_split
I0521 01:43:12.333782  7546 net.cpp:157] Top shape: 530 11 (5830)
I0521 01:43:12.333796  7546 net.cpp:157] Top shape: 530 11 (5830)
I0521 01:43:12.333806  7546 net.cpp:165] Memory required for data: 836804280
I0521 01:43:12.333816  7546 layer_factory.hpp:77] Creating layer accuracy
I0521 01:43:12.333837  7546 net.cpp:106] Creating Layer accuracy
I0521 01:43:12.333847  7546 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 01:43:12.333858  7546 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 01:43:12.333871  7546 net.cpp:411] accuracy -> accuracy
I0521 01:43:12.333895  7546 net.cpp:150] Setting up accuracy
I0521 01:43:12.333909  7546 net.cpp:157] Top shape: (1)
I0521 01:43:12.333919  7546 net.cpp:165] Memory required for data: 836804284
I0521 01:43:12.333926  7546 layer_factory.hpp:77] Creating layer loss
I0521 01:43:12.333938  7546 net.cpp:106] Creating Layer loss
I0521 01:43:12.333948  7546 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 01:43:12.333959  7546 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 01:43:12.333972  7546 net.cpp:411] loss -> loss
I0521 01:43:12.333991  7546 layer_factory.hpp:77] Creating layer loss
I0521 01:43:12.334481  7546 net.cpp:150] Setting up loss
I0521 01:43:12.334494  7546 net.cpp:157] Top shape: (1)
I0521 01:43:12.334504  7546 net.cpp:160]     with loss weight 1
I0521 01:43:12.334522  7546 net.cpp:165] Memory required for data: 836804288
I0521 01:43:12.334532  7546 net.cpp:226] loss needs backward computation.
I0521 01:43:12.334544  7546 net.cpp:228] accuracy does not need backward computation.
I0521 01:43:12.334555  7546 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 01:43:12.334565  7546 net.cpp:226] drop3 needs backward computation.
I0521 01:43:12.334573  7546 net.cpp:226] ip3 needs backward computation.
I0521 01:43:12.334584  7546 net.cpp:226] drop2 needs backward computation.
I0521 01:43:12.334602  7546 net.cpp:226] relu6 needs backward computation.
I0521 01:43:12.334612  7546 net.cpp:226] ip2 needs backward computation.
I0521 01:43:12.334622  7546 net.cpp:226] drop1 needs backward computation.
I0521 01:43:12.334632  7546 net.cpp:226] relu5 needs backward computation.
I0521 01:43:12.334641  7546 net.cpp:226] ip1 needs backward computation.
I0521 01:43:12.334651  7546 net.cpp:226] pool4 needs backward computation.
I0521 01:43:12.334661  7546 net.cpp:226] relu4 needs backward computation.
I0521 01:43:12.334671  7546 net.cpp:226] conv4 needs backward computation.
I0521 01:43:12.334681  7546 net.cpp:226] pool3 needs backward computation.
I0521 01:43:12.334692  7546 net.cpp:226] relu3 needs backward computation.
I0521 01:43:12.334699  7546 net.cpp:226] conv3 needs backward computation.
I0521 01:43:12.334710  7546 net.cpp:226] pool2 needs backward computation.
I0521 01:43:12.334720  7546 net.cpp:226] relu2 needs backward computation.
I0521 01:43:12.334730  7546 net.cpp:226] conv2 needs backward computation.
I0521 01:43:12.334741  7546 net.cpp:226] pool1 needs backward computation.
I0521 01:43:12.334751  7546 net.cpp:226] relu1 needs backward computation.
I0521 01:43:12.334761  7546 net.cpp:226] conv1 needs backward computation.
I0521 01:43:12.334772  7546 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 01:43:12.334784  7546 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:43:12.334794  7546 net.cpp:270] This network produces output accuracy
I0521 01:43:12.334805  7546 net.cpp:270] This network produces output loss
I0521 01:43:12.334832  7546 net.cpp:283] Network initialization done.
I0521 01:43:12.334965  7546 solver.cpp:60] Solver scaffolding done.
I0521 01:43:12.336094  7546 caffe.cpp:212] Starting Optimization
I0521 01:43:12.336112  7546 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 01:43:12.336127  7546 solver.cpp:289] Learning Rate Policy: fixed
I0521 01:43:12.337342  7546 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 01:43:58.412216  7546 solver.cpp:409]     Test net output #0: accuracy = 0.0943529
I0521 01:43:58.412386  7546 solver.cpp:409]     Test net output #1: loss = 2.39836 (* 1 = 2.39836 loss)
I0521 01:43:58.516326  7546 solver.cpp:237] Iteration 0, loss = 2.39865
I0521 01:43:58.516362  7546 solver.cpp:253]     Train net output #0: loss = 2.39865 (* 1 = 2.39865 loss)
I0521 01:43:58.516381  7546 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 01:44:06.520018  7546 solver.cpp:237] Iteration 28, loss = 2.3805
I0521 01:44:06.520053  7546 solver.cpp:253]     Train net output #0: loss = 2.3805 (* 1 = 2.3805 loss)
I0521 01:44:06.520071  7546 sgd_solver.cpp:106] Iteration 28, lr = 0.0025
I0521 01:44:14.524624  7546 solver.cpp:237] Iteration 56, loss = 2.36532
I0521 01:44:14.524659  7546 solver.cpp:253]     Train net output #0: loss = 2.36532 (* 1 = 2.36532 loss)
I0521 01:44:14.524677  7546 sgd_solver.cpp:106] Iteration 56, lr = 0.0025
I0521 01:44:22.525346  7546 solver.cpp:237] Iteration 84, loss = 2.3497
I0521 01:44:22.525380  7546 solver.cpp:253]     Train net output #0: loss = 2.3497 (* 1 = 2.3497 loss)
I0521 01:44:22.525396  7546 sgd_solver.cpp:106] Iteration 84, lr = 0.0025
I0521 01:44:30.528733  7546 solver.cpp:237] Iteration 112, loss = 2.32996
I0521 01:44:30.528878  7546 solver.cpp:253]     Train net output #0: loss = 2.32996 (* 1 = 2.32996 loss)
I0521 01:44:30.528893  7546 sgd_solver.cpp:106] Iteration 112, lr = 0.0025
I0521 01:44:38.535313  7546 solver.cpp:237] Iteration 140, loss = 2.34956
I0521 01:44:38.535348  7546 solver.cpp:253]     Train net output #0: loss = 2.34956 (* 1 = 2.34956 loss)
I0521 01:44:38.535367  7546 sgd_solver.cpp:106] Iteration 140, lr = 0.0025
I0521 01:44:46.544248  7546 solver.cpp:237] Iteration 168, loss = 2.3321
I0521 01:44:46.544276  7546 solver.cpp:253]     Train net output #0: loss = 2.3321 (* 1 = 2.3321 loss)
I0521 01:44:46.544287  7546 sgd_solver.cpp:106] Iteration 168, lr = 0.0025
I0521 01:45:16.638212  7546 solver.cpp:237] Iteration 196, loss = 2.31038
I0521 01:45:16.638375  7546 solver.cpp:253]     Train net output #0: loss = 2.31038 (* 1 = 2.31038 loss)
I0521 01:45:16.638391  7546 sgd_solver.cpp:106] Iteration 196, lr = 0.0025
I0521 01:45:24.647574  7546 solver.cpp:237] Iteration 224, loss = 2.27815
I0521 01:45:24.647617  7546 solver.cpp:253]     Train net output #0: loss = 2.27815 (* 1 = 2.27815 loss)
I0521 01:45:24.647630  7546 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 01:45:32.656548  7546 solver.cpp:237] Iteration 252, loss = 2.31175
I0521 01:45:32.656581  7546 solver.cpp:253]     Train net output #0: loss = 2.31175 (* 1 = 2.31175 loss)
I0521 01:45:32.656599  7546 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 01:45:40.664991  7546 solver.cpp:237] Iteration 280, loss = 2.25835
I0521 01:45:40.665025  7546 solver.cpp:253]     Train net output #0: loss = 2.25835 (* 1 = 2.25835 loss)
I0521 01:45:40.665042  7546 sgd_solver.cpp:106] Iteration 280, lr = 0.0025
I0521 01:45:41.236763  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_283.caffemodel
I0521 01:45:41.479377  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_283.solverstate
I0521 01:45:48.743578  7546 solver.cpp:237] Iteration 308, loss = 2.22567
I0521 01:45:48.743719  7546 solver.cpp:253]     Train net output #0: loss = 2.22567 (* 1 = 2.22567 loss)
I0521 01:45:48.743733  7546 sgd_solver.cpp:106] Iteration 308, lr = 0.0025
I0521 01:45:56.755192  7546 solver.cpp:237] Iteration 336, loss = 2.20144
I0521 01:45:56.755228  7546 solver.cpp:253]     Train net output #0: loss = 2.20144 (* 1 = 2.20144 loss)
I0521 01:45:56.755249  7546 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 01:46:04.764322  7546 solver.cpp:237] Iteration 364, loss = 2.16658
I0521 01:46:04.764356  7546 solver.cpp:253]     Train net output #0: loss = 2.16658 (* 1 = 2.16658 loss)
I0521 01:46:04.764372  7546 sgd_solver.cpp:106] Iteration 364, lr = 0.0025
I0521 01:46:34.912569  7546 solver.cpp:237] Iteration 392, loss = 2.10281
I0521 01:46:34.912725  7546 solver.cpp:253]     Train net output #0: loss = 2.10281 (* 1 = 2.10281 loss)
I0521 01:46:34.912739  7546 sgd_solver.cpp:106] Iteration 392, lr = 0.0025
I0521 01:46:42.919980  7546 solver.cpp:237] Iteration 420, loss = 2.08347
I0521 01:46:42.920018  7546 solver.cpp:253]     Train net output #0: loss = 2.08347 (* 1 = 2.08347 loss)
I0521 01:46:42.920038  7546 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 01:46:50.924549  7546 solver.cpp:237] Iteration 448, loss = 2.01645
I0521 01:46:50.924583  7546 solver.cpp:253]     Train net output #0: loss = 2.01645 (* 1 = 2.01645 loss)
I0521 01:46:50.924599  7546 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 01:46:58.938503  7546 solver.cpp:237] Iteration 476, loss = 2.02979
I0521 01:46:58.938536  7546 solver.cpp:253]     Train net output #0: loss = 2.02979 (* 1 = 2.02979 loss)
I0521 01:46:58.938554  7546 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0521 01:47:06.946027  7546 solver.cpp:237] Iteration 504, loss = 2.03691
I0521 01:47:06.946182  7546 solver.cpp:253]     Train net output #0: loss = 2.03691 (* 1 = 2.03691 loss)
I0521 01:47:06.946195  7546 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 01:47:14.950759  7546 solver.cpp:237] Iteration 532, loss = 1.93024
I0521 01:47:14.950793  7546 solver.cpp:253]     Train net output #0: loss = 1.93024 (* 1 = 1.93024 loss)
I0521 01:47:14.950810  7546 sgd_solver.cpp:106] Iteration 532, lr = 0.0025
I0521 01:47:22.959771  7546 solver.cpp:237] Iteration 560, loss = 1.97772
I0521 01:47:22.959806  7546 solver.cpp:253]     Train net output #0: loss = 1.97772 (* 1 = 1.97772 loss)
I0521 01:47:22.959823  7546 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 01:47:24.391057  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_566.caffemodel
I0521 01:47:24.630302  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_566.solverstate
I0521 01:47:24.655419  7546 solver.cpp:341] Iteration 566, Testing net (#0)
I0521 01:48:09.773892  7546 solver.cpp:409]     Test net output #0: accuracy = 0.549623
I0521 01:48:09.774060  7546 solver.cpp:409]     Test net output #1: loss = 1.64069 (* 1 = 1.64069 loss)
I0521 01:48:38.239699  7546 solver.cpp:237] Iteration 588, loss = 1.92924
I0521 01:48:38.239749  7546 solver.cpp:253]     Train net output #0: loss = 1.92924 (* 1 = 1.92924 loss)
I0521 01:48:38.239764  7546 sgd_solver.cpp:106] Iteration 588, lr = 0.0025
I0521 01:48:46.245569  7546 solver.cpp:237] Iteration 616, loss = 1.93704
I0521 01:48:46.245730  7546 solver.cpp:253]     Train net output #0: loss = 1.93704 (* 1 = 1.93704 loss)
I0521 01:48:46.245745  7546 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0521 01:48:54.252959  7546 solver.cpp:237] Iteration 644, loss = 1.87269
I0521 01:48:54.252991  7546 solver.cpp:253]     Train net output #0: loss = 1.87269 (* 1 = 1.87269 loss)
I0521 01:48:54.253007  7546 sgd_solver.cpp:106] Iteration 644, lr = 0.0025
I0521 01:49:02.254552  7546 solver.cpp:237] Iteration 672, loss = 1.91542
I0521 01:49:02.254601  7546 solver.cpp:253]     Train net output #0: loss = 1.91542 (* 1 = 1.91542 loss)
I0521 01:49:02.254614  7546 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 01:49:10.265214  7546 solver.cpp:237] Iteration 700, loss = 1.87183
I0521 01:49:10.265259  7546 solver.cpp:253]     Train net output #0: loss = 1.87183 (* 1 = 1.87183 loss)
I0521 01:49:10.265276  7546 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0521 01:49:18.272101  7546 solver.cpp:237] Iteration 728, loss = 1.77983
I0521 01:49:18.272233  7546 solver.cpp:253]     Train net output #0: loss = 1.77983 (* 1 = 1.77983 loss)
I0521 01:49:18.272245  7546 sgd_solver.cpp:106] Iteration 728, lr = 0.0025
I0521 01:49:48.380276  7546 solver.cpp:237] Iteration 756, loss = 1.82822
I0521 01:49:48.380442  7546 solver.cpp:253]     Train net output #0: loss = 1.82822 (* 1 = 1.82822 loss)
I0521 01:49:48.380457  7546 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 01:49:56.390632  7546 solver.cpp:237] Iteration 784, loss = 1.92507
I0521 01:49:56.390666  7546 solver.cpp:253]     Train net output #0: loss = 1.92507 (* 1 = 1.92507 loss)
I0521 01:49:56.390682  7546 sgd_solver.cpp:106] Iteration 784, lr = 0.0025
I0521 01:50:04.398531  7546 solver.cpp:237] Iteration 812, loss = 1.88842
I0521 01:50:04.398576  7546 solver.cpp:253]     Train net output #0: loss = 1.88842 (* 1 = 1.88842 loss)
I0521 01:50:04.398592  7546 sgd_solver.cpp:106] Iteration 812, lr = 0.0025
I0521 01:50:12.407969  7546 solver.cpp:237] Iteration 840, loss = 1.82301
I0521 01:50:12.408004  7546 solver.cpp:253]     Train net output #0: loss = 1.82301 (* 1 = 1.82301 loss)
I0521 01:50:12.408020  7546 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 01:50:14.695775  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_849.caffemodel
I0521 01:50:14.936040  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_849.solverstate
I0521 01:50:20.481171  7546 solver.cpp:237] Iteration 868, loss = 1.90016
I0521 01:50:20.481339  7546 solver.cpp:253]     Train net output #0: loss = 1.90016 (* 1 = 1.90016 loss)
I0521 01:50:20.481353  7546 sgd_solver.cpp:106] Iteration 868, lr = 0.0025
I0521 01:50:28.496273  7546 solver.cpp:237] Iteration 896, loss = 1.74234
I0521 01:50:28.496314  7546 solver.cpp:253]     Train net output #0: loss = 1.74234 (* 1 = 1.74234 loss)
I0521 01:50:28.496335  7546 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 01:50:36.508831  7546 solver.cpp:237] Iteration 924, loss = 1.78369
I0521 01:50:36.508865  7546 solver.cpp:253]     Train net output #0: loss = 1.78369 (* 1 = 1.78369 loss)
I0521 01:50:36.508882  7546 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 01:51:06.622385  7546 solver.cpp:237] Iteration 952, loss = 1.96192
I0521 01:51:06.622547  7546 solver.cpp:253]     Train net output #0: loss = 1.96192 (* 1 = 1.96192 loss)
I0521 01:51:06.622563  7546 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0521 01:51:14.630133  7546 solver.cpp:237] Iteration 980, loss = 1.82492
I0521 01:51:14.630180  7546 solver.cpp:253]     Train net output #0: loss = 1.82492 (* 1 = 1.82492 loss)
I0521 01:51:14.630197  7546 sgd_solver.cpp:106] Iteration 980, lr = 0.0025
I0521 01:51:22.636576  7546 solver.cpp:237] Iteration 1008, loss = 1.84922
I0521 01:51:22.636610  7546 solver.cpp:253]     Train net output #0: loss = 1.84922 (* 1 = 1.84922 loss)
I0521 01:51:22.636626  7546 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 01:51:30.646082  7546 solver.cpp:237] Iteration 1036, loss = 1.83792
I0521 01:51:30.646111  7546 solver.cpp:253]     Train net output #0: loss = 1.83792 (* 1 = 1.83792 loss)
I0521 01:51:30.646124  7546 sgd_solver.cpp:106] Iteration 1036, lr = 0.0025
I0521 01:51:38.653692  7546 solver.cpp:237] Iteration 1064, loss = 1.78072
I0521 01:51:38.653836  7546 solver.cpp:253]     Train net output #0: loss = 1.78072 (* 1 = 1.78072 loss)
I0521 01:51:38.653848  7546 sgd_solver.cpp:106] Iteration 1064, lr = 0.0025
I0521 01:51:46.664878  7546 solver.cpp:237] Iteration 1092, loss = 1.80994
I0521 01:51:46.664916  7546 solver.cpp:253]     Train net output #0: loss = 1.80994 (* 1 = 1.80994 loss)
I0521 01:51:46.664938  7546 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0521 01:51:54.670886  7546 solver.cpp:237] Iteration 1120, loss = 1.77252
I0521 01:51:54.670920  7546 solver.cpp:253]     Train net output #0: loss = 1.77252 (* 1 = 1.77252 loss)
I0521 01:51:54.670936  7546 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 01:51:57.815668  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1132.caffemodel
I0521 01:51:58.055584  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1132.solverstate
I0521 01:51:58.083930  7546 solver.cpp:341] Iteration 1132, Testing net (#0)
I0521 01:53:04.045521  7546 solver.cpp:409]     Test net output #0: accuracy = 0.647497
I0521 01:53:04.045687  7546 solver.cpp:409]     Test net output #1: loss = 1.26838 (* 1 = 1.26838 loss)
I0521 01:53:30.823990  7546 solver.cpp:237] Iteration 1148, loss = 1.74017
I0521 01:53:30.824040  7546 solver.cpp:253]     Train net output #0: loss = 1.74017 (* 1 = 1.74017 loss)
I0521 01:53:30.824056  7546 sgd_solver.cpp:106] Iteration 1148, lr = 0.0025
I0521 01:53:38.824412  7546 solver.cpp:237] Iteration 1176, loss = 1.69821
I0521 01:53:38.824561  7546 solver.cpp:253]     Train net output #0: loss = 1.69821 (* 1 = 1.69821 loss)
I0521 01:53:38.824574  7546 sgd_solver.cpp:106] Iteration 1176, lr = 0.0025
I0521 01:53:46.819792  7546 solver.cpp:237] Iteration 1204, loss = 1.92856
I0521 01:53:46.819825  7546 solver.cpp:253]     Train net output #0: loss = 1.92856 (* 1 = 1.92856 loss)
I0521 01:53:46.819850  7546 sgd_solver.cpp:106] Iteration 1204, lr = 0.0025
I0521 01:53:54.815335  7546 solver.cpp:237] Iteration 1232, loss = 1.75981
I0521 01:53:54.815368  7546 solver.cpp:253]     Train net output #0: loss = 1.75981 (* 1 = 1.75981 loss)
I0521 01:53:54.815384  7546 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 01:54:02.815943  7546 solver.cpp:237] Iteration 1260, loss = 1.66539
I0521 01:54:02.815976  7546 solver.cpp:253]     Train net output #0: loss = 1.66539 (* 1 = 1.66539 loss)
I0521 01:54:02.815992  7546 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 01:54:10.816934  7546 solver.cpp:237] Iteration 1288, loss = 1.76122
I0521 01:54:10.817090  7546 solver.cpp:253]     Train net output #0: loss = 1.76122 (* 1 = 1.76122 loss)
I0521 01:54:10.817103  7546 sgd_solver.cpp:106] Iteration 1288, lr = 0.0025
I0521 01:54:18.814159  7546 solver.cpp:237] Iteration 1316, loss = 1.81469
I0521 01:54:18.814190  7546 solver.cpp:253]     Train net output #0: loss = 1.81469 (* 1 = 1.81469 loss)
I0521 01:54:18.814209  7546 sgd_solver.cpp:106] Iteration 1316, lr = 0.0025
I0521 01:54:48.974081  7546 solver.cpp:237] Iteration 1344, loss = 1.70054
I0521 01:54:48.974246  7546 solver.cpp:253]     Train net output #0: loss = 1.70054 (* 1 = 1.70054 loss)
I0521 01:54:48.974261  7546 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 01:54:56.979804  7546 solver.cpp:237] Iteration 1372, loss = 1.72484
I0521 01:54:56.979838  7546 solver.cpp:253]     Train net output #0: loss = 1.72484 (* 1 = 1.72484 loss)
I0521 01:54:56.979856  7546 sgd_solver.cpp:106] Iteration 1372, lr = 0.0025
I0521 01:55:04.981850  7546 solver.cpp:237] Iteration 1400, loss = 1.77293
I0521 01:55:04.981885  7546 solver.cpp:253]     Train net output #0: loss = 1.77293 (* 1 = 1.77293 loss)
I0521 01:55:04.981904  7546 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0521 01:55:08.983052  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1415.caffemodel
I0521 01:55:09.223538  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1415.solverstate
I0521 01:55:13.052747  7546 solver.cpp:237] Iteration 1428, loss = 1.74772
I0521 01:55:13.052793  7546 solver.cpp:253]     Train net output #0: loss = 1.74772 (* 1 = 1.74772 loss)
I0521 01:55:13.052812  7546 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 01:55:21.054352  7546 solver.cpp:237] Iteration 1456, loss = 1.73079
I0521 01:55:21.054496  7546 solver.cpp:253]     Train net output #0: loss = 1.73079 (* 1 = 1.73079 loss)
I0521 01:55:21.054509  7546 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 01:55:29.055178  7546 solver.cpp:237] Iteration 1484, loss = 1.6689
I0521 01:55:29.055215  7546 solver.cpp:253]     Train net output #0: loss = 1.6689 (* 1 = 1.6689 loss)
I0521 01:55:29.055234  7546 sgd_solver.cpp:106] Iteration 1484, lr = 0.0025
I0521 01:55:59.231431  7546 solver.cpp:237] Iteration 1512, loss = 1.71233
I0521 01:55:59.231609  7546 solver.cpp:253]     Train net output #0: loss = 1.71233 (* 1 = 1.71233 loss)
I0521 01:55:59.231624  7546 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 01:56:07.231250  7546 solver.cpp:237] Iteration 1540, loss = 1.73893
I0521 01:56:07.231283  7546 solver.cpp:253]     Train net output #0: loss = 1.73893 (* 1 = 1.73893 loss)
I0521 01:56:07.231302  7546 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 01:56:15.236346  7546 solver.cpp:237] Iteration 1568, loss = 1.6731
I0521 01:56:15.236387  7546 solver.cpp:253]     Train net output #0: loss = 1.6731 (* 1 = 1.6731 loss)
I0521 01:56:15.236403  7546 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 01:56:23.232779  7546 solver.cpp:237] Iteration 1596, loss = 1.76159
I0521 01:56:23.232812  7546 solver.cpp:253]     Train net output #0: loss = 1.76159 (* 1 = 1.76159 loss)
I0521 01:56:23.232827  7546 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0521 01:56:31.236974  7546 solver.cpp:237] Iteration 1624, loss = 1.63287
I0521 01:56:31.237126  7546 solver.cpp:253]     Train net output #0: loss = 1.63287 (* 1 = 1.63287 loss)
I0521 01:56:31.237140  7546 sgd_solver.cpp:106] Iteration 1624, lr = 0.0025
I0521 01:56:39.235962  7546 solver.cpp:237] Iteration 1652, loss = 1.66654
I0521 01:56:39.235998  7546 solver.cpp:253]     Train net output #0: loss = 1.66654 (* 1 = 1.66654 loss)
I0521 01:56:39.236017  7546 sgd_solver.cpp:106] Iteration 1652, lr = 0.0025
I0521 01:56:47.228498  7546 solver.cpp:237] Iteration 1680, loss = 1.68525
I0521 01:56:47.228533  7546 solver.cpp:253]     Train net output #0: loss = 1.68525 (* 1 = 1.68525 loss)
I0521 01:56:47.228549  7546 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 01:56:52.084224  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1698.caffemodel
I0521 01:56:52.323287  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1698.solverstate
I0521 01:56:52.349586  7546 solver.cpp:341] Iteration 1698, Testing net (#0)
I0521 01:57:37.197463  7546 solver.cpp:409]     Test net output #0: accuracy = 0.65275
I0521 01:57:37.197621  7546 solver.cpp:409]     Test net output #1: loss = 1.18844 (* 1 = 1.18844 loss)
I0521 01:58:02.279191  7546 solver.cpp:237] Iteration 1708, loss = 1.70846
I0521 01:58:02.279238  7546 solver.cpp:253]     Train net output #0: loss = 1.70846 (* 1 = 1.70846 loss)
I0521 01:58:02.279256  7546 sgd_solver.cpp:106] Iteration 1708, lr = 0.0025
I0521 01:58:10.280889  7546 solver.cpp:237] Iteration 1736, loss = 1.68042
I0521 01:58:10.281031  7546 solver.cpp:253]     Train net output #0: loss = 1.68042 (* 1 = 1.68042 loss)
I0521 01:58:10.281044  7546 sgd_solver.cpp:106] Iteration 1736, lr = 0.0025
I0521 01:58:18.277659  7546 solver.cpp:237] Iteration 1764, loss = 1.73685
I0521 01:58:18.277693  7546 solver.cpp:253]     Train net output #0: loss = 1.73685 (* 1 = 1.73685 loss)
I0521 01:58:18.277715  7546 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 01:58:26.278596  7546 solver.cpp:237] Iteration 1792, loss = 1.79447
I0521 01:58:26.278625  7546 solver.cpp:253]     Train net output #0: loss = 1.79447 (* 1 = 1.79447 loss)
I0521 01:58:26.278643  7546 sgd_solver.cpp:106] Iteration 1792, lr = 0.0025
I0521 01:58:34.278403  7546 solver.cpp:237] Iteration 1820, loss = 1.72876
I0521 01:58:34.278436  7546 solver.cpp:253]     Train net output #0: loss = 1.72876 (* 1 = 1.72876 loss)
I0521 01:58:34.278452  7546 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0521 01:58:42.276640  7546 solver.cpp:237] Iteration 1848, loss = 1.64281
I0521 01:58:42.276787  7546 solver.cpp:253]     Train net output #0: loss = 1.64281 (* 1 = 1.64281 loss)
I0521 01:58:42.276800  7546 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 01:58:50.281045  7546 solver.cpp:237] Iteration 1876, loss = 1.72076
I0521 01:58:50.281087  7546 solver.cpp:253]     Train net output #0: loss = 1.72076 (* 1 = 1.72076 loss)
I0521 01:58:50.281107  7546 sgd_solver.cpp:106] Iteration 1876, lr = 0.0025
I0521 01:59:20.391119  7546 solver.cpp:237] Iteration 1904, loss = 1.62006
I0521 01:59:20.391288  7546 solver.cpp:253]     Train net output #0: loss = 1.62006 (* 1 = 1.62006 loss)
I0521 01:59:20.391304  7546 sgd_solver.cpp:106] Iteration 1904, lr = 0.0025
I0521 01:59:28.390559  7546 solver.cpp:237] Iteration 1932, loss = 1.59746
I0521 01:59:28.390594  7546 solver.cpp:253]     Train net output #0: loss = 1.59746 (* 1 = 1.59746 loss)
I0521 01:59:28.390611  7546 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0521 01:59:36.387742  7546 solver.cpp:237] Iteration 1960, loss = 1.71477
I0521 01:59:36.387786  7546 solver.cpp:253]     Train net output #0: loss = 1.71477 (* 1 = 1.71477 loss)
I0521 01:59:36.387804  7546 sgd_solver.cpp:106] Iteration 1960, lr = 0.0025
I0521 01:59:42.102820  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1981.caffemodel
I0521 01:59:42.341074  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_1981.solverstate
I0521 01:59:44.453353  7546 solver.cpp:237] Iteration 1988, loss = 1.63475
I0521 01:59:44.453395  7546 solver.cpp:253]     Train net output #0: loss = 1.63475 (* 1 = 1.63475 loss)
I0521 01:59:44.453415  7546 sgd_solver.cpp:106] Iteration 1988, lr = 0.0025
I0521 01:59:52.453574  7546 solver.cpp:237] Iteration 2016, loss = 1.74918
I0521 01:59:52.453733  7546 solver.cpp:253]     Train net output #0: loss = 1.74918 (* 1 = 1.74918 loss)
I0521 01:59:52.453758  7546 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0521 02:00:00.450072  7546 solver.cpp:237] Iteration 2044, loss = 1.73008
I0521 02:00:00.450111  7546 solver.cpp:253]     Train net output #0: loss = 1.73008 (* 1 = 1.73008 loss)
I0521 02:00:00.450130  7546 sgd_solver.cpp:106] Iteration 2044, lr = 0.0025
I0521 02:00:08.445509  7546 solver.cpp:237] Iteration 2072, loss = 1.62281
I0521 02:00:08.445544  7546 solver.cpp:253]     Train net output #0: loss = 1.62281 (* 1 = 1.62281 loss)
I0521 02:00:08.445559  7546 sgd_solver.cpp:106] Iteration 2072, lr = 0.0025
I0521 02:00:38.598389  7546 solver.cpp:237] Iteration 2100, loss = 1.57886
I0521 02:00:38.598562  7546 solver.cpp:253]     Train net output #0: loss = 1.57886 (* 1 = 1.57886 loss)
I0521 02:00:38.598575  7546 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0521 02:00:46.597015  7546 solver.cpp:237] Iteration 2128, loss = 1.63003
I0521 02:00:46.597048  7546 solver.cpp:253]     Train net output #0: loss = 1.63003 (* 1 = 1.63003 loss)
I0521 02:00:46.597067  7546 sgd_solver.cpp:106] Iteration 2128, lr = 0.0025
I0521 02:00:54.595845  7546 solver.cpp:237] Iteration 2156, loss = 1.60106
I0521 02:00:54.595890  7546 solver.cpp:253]     Train net output #0: loss = 1.60106 (* 1 = 1.60106 loss)
I0521 02:00:54.595904  7546 sgd_solver.cpp:106] Iteration 2156, lr = 0.0025
I0521 02:01:02.594245  7546 solver.cpp:237] Iteration 2184, loss = 1.74182
I0521 02:01:02.594280  7546 solver.cpp:253]     Train net output #0: loss = 1.74182 (* 1 = 1.74182 loss)
I0521 02:01:02.594296  7546 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0521 02:01:10.591553  7546 solver.cpp:237] Iteration 2212, loss = 1.6624
I0521 02:01:10.591697  7546 solver.cpp:253]     Train net output #0: loss = 1.6624 (* 1 = 1.6624 loss)
I0521 02:01:10.591711  7546 sgd_solver.cpp:106] Iteration 2212, lr = 0.0025
I0521 02:01:18.591830  7546 solver.cpp:237] Iteration 2240, loss = 1.6237
I0521 02:01:18.591861  7546 solver.cpp:253]     Train net output #0: loss = 1.6237 (* 1 = 1.6237 loss)
I0521 02:01:18.591879  7546 sgd_solver.cpp:106] Iteration 2240, lr = 0.0025
I0521 02:01:25.160464  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_2264.caffemodel
I0521 02:01:25.403800  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_2264.solverstate
I0521 02:01:25.430132  7546 solver.cpp:341] Iteration 2264, Testing net (#0)
I0521 02:02:31.510135  7546 solver.cpp:409]     Test net output #0: accuracy = 0.674518
I0521 02:02:31.510306  7546 solver.cpp:409]     Test net output #1: loss = 1.12655 (* 1 = 1.12655 loss)
I0521 02:02:54.838289  7546 solver.cpp:237] Iteration 2268, loss = 1.61827
I0521 02:02:54.838336  7546 solver.cpp:253]     Train net output #0: loss = 1.61827 (* 1 = 1.61827 loss)
I0521 02:02:54.838353  7546 sgd_solver.cpp:106] Iteration 2268, lr = 0.0025
I0521 02:03:02.838446  7546 solver.cpp:237] Iteration 2296, loss = 1.63793
I0521 02:03:02.838608  7546 solver.cpp:253]     Train net output #0: loss = 1.63793 (* 1 = 1.63793 loss)
I0521 02:03:02.838621  7546 sgd_solver.cpp:106] Iteration 2296, lr = 0.0025
I0521 02:03:10.839468  7546 solver.cpp:237] Iteration 2324, loss = 1.56954
I0521 02:03:10.839503  7546 solver.cpp:253]     Train net output #0: loss = 1.56954 (* 1 = 1.56954 loss)
I0521 02:03:10.839519  7546 sgd_solver.cpp:106] Iteration 2324, lr = 0.0025
I0521 02:03:18.838215  7546 solver.cpp:237] Iteration 2352, loss = 1.64486
I0521 02:03:18.838248  7546 solver.cpp:253]     Train net output #0: loss = 1.64486 (* 1 = 1.64486 loss)
I0521 02:03:18.838265  7546 sgd_solver.cpp:106] Iteration 2352, lr = 0.0025
I0521 02:03:26.841835  7546 solver.cpp:237] Iteration 2380, loss = 1.55848
I0521 02:03:26.841869  7546 solver.cpp:253]     Train net output #0: loss = 1.55848 (* 1 = 1.55848 loss)
I0521 02:03:26.841888  7546 sgd_solver.cpp:106] Iteration 2380, lr = 0.0025
I0521 02:03:34.840093  7546 solver.cpp:237] Iteration 2408, loss = 1.64141
I0521 02:03:34.840236  7546 solver.cpp:253]     Train net output #0: loss = 1.64141 (* 1 = 1.64141 loss)
I0521 02:03:34.840250  7546 sgd_solver.cpp:106] Iteration 2408, lr = 0.0025
I0521 02:03:42.843197  7546 solver.cpp:237] Iteration 2436, loss = 1.60932
I0521 02:03:42.843230  7546 solver.cpp:253]     Train net output #0: loss = 1.60932 (* 1 = 1.60932 loss)
I0521 02:03:42.843247  7546 sgd_solver.cpp:106] Iteration 2436, lr = 0.0025
I0521 02:04:13.046072  7546 solver.cpp:237] Iteration 2464, loss = 1.6179
I0521 02:04:13.046239  7546 solver.cpp:253]     Train net output #0: loss = 1.6179 (* 1 = 1.6179 loss)
I0521 02:04:13.046255  7546 sgd_solver.cpp:106] Iteration 2464, lr = 0.0025
I0521 02:04:21.051931  7546 solver.cpp:237] Iteration 2492, loss = 1.59509
I0521 02:04:21.051964  7546 solver.cpp:253]     Train net output #0: loss = 1.59509 (* 1 = 1.59509 loss)
I0521 02:04:21.051981  7546 sgd_solver.cpp:106] Iteration 2492, lr = 0.0025
I0521 02:04:29.052414  7546 solver.cpp:237] Iteration 2520, loss = 1.61411
I0521 02:04:29.052448  7546 solver.cpp:253]     Train net output #0: loss = 1.61411 (* 1 = 1.61411 loss)
I0521 02:04:29.052464  7546 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0521 02:04:36.482519  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_2547.caffemodel
I0521 02:04:36.723209  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_2547.solverstate
I0521 02:04:37.122716  7546 solver.cpp:237] Iteration 2548, loss = 1.58654
I0521 02:04:37.122764  7546 solver.cpp:253]     Train net output #0: loss = 1.58654 (* 1 = 1.58654 loss)
I0521 02:04:37.122781  7546 sgd_solver.cpp:106] Iteration 2548, lr = 0.0025
I0521 02:04:45.122831  7546 solver.cpp:237] Iteration 2576, loss = 1.61913
I0521 02:04:45.123003  7546 solver.cpp:253]     Train net output #0: loss = 1.61913 (* 1 = 1.61913 loss)
I0521 02:04:45.123016  7546 sgd_solver.cpp:106] Iteration 2576, lr = 0.0025
I0521 02:04:53.126262  7546 solver.cpp:237] Iteration 2604, loss = 1.60024
I0521 02:04:53.126297  7546 solver.cpp:253]     Train net output #0: loss = 1.60024 (* 1 = 1.60024 loss)
I0521 02:04:53.126315  7546 sgd_solver.cpp:106] Iteration 2604, lr = 0.0025
I0521 02:05:01.124970  7546 solver.cpp:237] Iteration 2632, loss = 1.66539
I0521 02:05:01.125021  7546 solver.cpp:253]     Train net output #0: loss = 1.66539 (* 1 = 1.66539 loss)
I0521 02:05:01.125036  7546 sgd_solver.cpp:106] Iteration 2632, lr = 0.0025
I0521 02:05:31.343477  7546 solver.cpp:237] Iteration 2660, loss = 1.61326
I0521 02:05:31.343648  7546 solver.cpp:253]     Train net output #0: loss = 1.61326 (* 1 = 1.61326 loss)
I0521 02:05:31.343664  7546 sgd_solver.cpp:106] Iteration 2660, lr = 0.0025
I0521 02:05:39.346940  7546 solver.cpp:237] Iteration 2688, loss = 1.58846
I0521 02:05:39.346973  7546 solver.cpp:253]     Train net output #0: loss = 1.58846 (* 1 = 1.58846 loss)
I0521 02:05:39.346989  7546 sgd_solver.cpp:106] Iteration 2688, lr = 0.0025
I0521 02:05:47.344223  7546 solver.cpp:237] Iteration 2716, loss = 1.62108
I0521 02:05:47.344257  7546 solver.cpp:253]     Train net output #0: loss = 1.62108 (* 1 = 1.62108 loss)
I0521 02:05:47.344274  7546 sgd_solver.cpp:106] Iteration 2716, lr = 0.0025
I0521 02:05:55.345731  7546 solver.cpp:237] Iteration 2744, loss = 1.52108
I0521 02:05:55.345768  7546 solver.cpp:253]     Train net output #0: loss = 1.52108 (* 1 = 1.52108 loss)
I0521 02:05:55.345789  7546 sgd_solver.cpp:106] Iteration 2744, lr = 0.0025
I0521 02:06:03.347543  7546 solver.cpp:237] Iteration 2772, loss = 1.57008
I0521 02:06:03.347687  7546 solver.cpp:253]     Train net output #0: loss = 1.57008 (* 1 = 1.57008 loss)
I0521 02:06:03.347700  7546 sgd_solver.cpp:106] Iteration 2772, lr = 0.0025
I0521 02:06:11.352815  7546 solver.cpp:237] Iteration 2800, loss = 1.54333
I0521 02:06:11.352849  7546 solver.cpp:253]     Train net output #0: loss = 1.54333 (* 1 = 1.54333 loss)
I0521 02:06:11.352866  7546 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0521 02:06:19.355240  7546 solver.cpp:237] Iteration 2828, loss = 1.67326
I0521 02:06:19.355283  7546 solver.cpp:253]     Train net output #0: loss = 1.67326 (* 1 = 1.67326 loss)
I0521 02:06:19.355303  7546 sgd_solver.cpp:106] Iteration 2828, lr = 0.0025
I0521 02:06:19.641082  7546 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_2830.caffemodel
I0521 02:06:19.881342  7546 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129_iter_2830.solverstate
I0521 02:06:19.909586  7546 solver.cpp:341] Iteration 2830, Testing net (#0)
I0521 02:07:05.109177  7546 solver.cpp:409]     Test net output #0: accuracy = 0.65409
I0521 02:07:05.109339  7546 solver.cpp:409]     Test net output #1: loss = 1.24333 (* 1 = 1.24333 loss)
I0521 02:07:05.109354  7546 solver.cpp:326] Optimization Done.
I0521 02:07:05.109364  7546 caffe.cpp:215] Optimization Done.
Application 11236426 resources: utime ~1251s, stime ~227s, Rss ~5333172, inblocks ~3594475, outblocks ~179818
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_530_2016-05-20T11.20.51.849129.solver"
	User time (seconds): 0.53
	System time (seconds): 0.22
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:43.12
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15083
	Voluntary context switches: 3049
	Involuntary context switches: 194
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
