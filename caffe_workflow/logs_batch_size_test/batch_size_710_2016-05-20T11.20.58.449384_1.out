2806306
I0521 05:45:30.877960 30656 caffe.cpp:184] Using GPUs 0
I0521 05:45:31.300739 30656 solver.cpp:48] Initializing solver from parameters: 
test_iter: 211
test_interval: 422
base_lr: 0.0025
display: 21
max_iter: 2112
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 211
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384.prototxt"
I0521 05:45:31.302333 30656 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384.prototxt
I0521 05:45:31.318083 30656 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 05:45:31.318148 30656 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 05:45:31.318526 30656 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 710
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 05:45:31.318728 30656 layer_factory.hpp:77] Creating layer data_hdf5
I0521 05:45:31.318758 30656 net.cpp:106] Creating Layer data_hdf5
I0521 05:45:31.318783 30656 net.cpp:411] data_hdf5 -> data
I0521 05:45:31.318817 30656 net.cpp:411] data_hdf5 -> label
I0521 05:45:31.318859 30656 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 05:45:31.320050 30656 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 05:45:31.322266 30656 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 05:45:52.973063 30656 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 05:45:52.978245 30656 net.cpp:150] Setting up data_hdf5
I0521 05:45:52.978287 30656 net.cpp:157] Top shape: 710 1 127 50 (4508500)
I0521 05:45:52.978304 30656 net.cpp:157] Top shape: 710 (710)
I0521 05:45:52.978317 30656 net.cpp:165] Memory required for data: 18036840
I0521 05:45:52.978337 30656 layer_factory.hpp:77] Creating layer conv1
I0521 05:45:52.978384 30656 net.cpp:106] Creating Layer conv1
I0521 05:45:52.978397 30656 net.cpp:454] conv1 <- data
I0521 05:45:52.978431 30656 net.cpp:411] conv1 -> conv1
I0521 05:45:53.345405 30656 net.cpp:150] Setting up conv1
I0521 05:45:53.345460 30656 net.cpp:157] Top shape: 710 12 120 48 (49075200)
I0521 05:45:53.345482 30656 net.cpp:165] Memory required for data: 214337640
I0521 05:45:53.345513 30656 layer_factory.hpp:77] Creating layer relu1
I0521 05:45:53.345535 30656 net.cpp:106] Creating Layer relu1
I0521 05:45:53.345554 30656 net.cpp:454] relu1 <- conv1
I0521 05:45:53.345590 30656 net.cpp:397] relu1 -> conv1 (in-place)
I0521 05:45:53.346130 30656 net.cpp:150] Setting up relu1
I0521 05:45:53.346153 30656 net.cpp:157] Top shape: 710 12 120 48 (49075200)
I0521 05:45:53.346168 30656 net.cpp:165] Memory required for data: 410638440
I0521 05:45:53.346184 30656 layer_factory.hpp:77] Creating layer pool1
I0521 05:45:53.346210 30656 net.cpp:106] Creating Layer pool1
I0521 05:45:53.346225 30656 net.cpp:454] pool1 <- conv1
I0521 05:45:53.346240 30656 net.cpp:411] pool1 -> pool1
I0521 05:45:53.346334 30656 net.cpp:150] Setting up pool1
I0521 05:45:53.346352 30656 net.cpp:157] Top shape: 710 12 60 48 (24537600)
I0521 05:45:53.346374 30656 net.cpp:165] Memory required for data: 508788840
I0521 05:45:53.346387 30656 layer_factory.hpp:77] Creating layer conv2
I0521 05:45:53.346411 30656 net.cpp:106] Creating Layer conv2
I0521 05:45:53.346426 30656 net.cpp:454] conv2 <- pool1
I0521 05:45:53.346443 30656 net.cpp:411] conv2 -> conv2
I0521 05:45:53.349124 30656 net.cpp:150] Setting up conv2
I0521 05:45:53.349155 30656 net.cpp:157] Top shape: 710 20 54 46 (35272800)
I0521 05:45:53.349169 30656 net.cpp:165] Memory required for data: 649880040
I0521 05:45:53.349196 30656 layer_factory.hpp:77] Creating layer relu2
I0521 05:45:53.349225 30656 net.cpp:106] Creating Layer relu2
I0521 05:45:53.349238 30656 net.cpp:454] relu2 <- conv2
I0521 05:45:53.349253 30656 net.cpp:397] relu2 -> conv2 (in-place)
I0521 05:45:53.349608 30656 net.cpp:150] Setting up relu2
I0521 05:45:53.349629 30656 net.cpp:157] Top shape: 710 20 54 46 (35272800)
I0521 05:45:53.349642 30656 net.cpp:165] Memory required for data: 790971240
I0521 05:45:53.349654 30656 layer_factory.hpp:77] Creating layer pool2
I0521 05:45:53.349673 30656 net.cpp:106] Creating Layer pool2
I0521 05:45:53.349694 30656 net.cpp:454] pool2 <- conv2
I0521 05:45:53.349731 30656 net.cpp:411] pool2 -> pool2
I0521 05:45:53.349814 30656 net.cpp:150] Setting up pool2
I0521 05:45:53.349831 30656 net.cpp:157] Top shape: 710 20 27 46 (17636400)
I0521 05:45:53.349846 30656 net.cpp:165] Memory required for data: 861516840
I0521 05:45:53.349865 30656 layer_factory.hpp:77] Creating layer conv3
I0521 05:45:53.349886 30656 net.cpp:106] Creating Layer conv3
I0521 05:45:53.349900 30656 net.cpp:454] conv3 <- pool2
I0521 05:45:53.349915 30656 net.cpp:411] conv3 -> conv3
I0521 05:45:53.351866 30656 net.cpp:150] Setting up conv3
I0521 05:45:53.351891 30656 net.cpp:157] Top shape: 710 28 22 44 (19243840)
I0521 05:45:53.351912 30656 net.cpp:165] Memory required for data: 938492200
I0521 05:45:53.351934 30656 layer_factory.hpp:77] Creating layer relu3
I0521 05:45:53.351956 30656 net.cpp:106] Creating Layer relu3
I0521 05:45:53.351979 30656 net.cpp:454] relu3 <- conv3
I0521 05:45:53.351995 30656 net.cpp:397] relu3 -> conv3 (in-place)
I0521 05:45:53.352483 30656 net.cpp:150] Setting up relu3
I0521 05:45:53.352509 30656 net.cpp:157] Top shape: 710 28 22 44 (19243840)
I0521 05:45:53.352521 30656 net.cpp:165] Memory required for data: 1015467560
I0521 05:45:53.352537 30656 layer_factory.hpp:77] Creating layer pool3
I0521 05:45:53.352561 30656 net.cpp:106] Creating Layer pool3
I0521 05:45:53.352576 30656 net.cpp:454] pool3 <- conv3
I0521 05:45:53.352591 30656 net.cpp:411] pool3 -> pool3
I0521 05:45:53.352680 30656 net.cpp:150] Setting up pool3
I0521 05:45:53.352699 30656 net.cpp:157] Top shape: 710 28 11 44 (9621920)
I0521 05:45:53.352713 30656 net.cpp:165] Memory required for data: 1053955240
I0521 05:45:53.352725 30656 layer_factory.hpp:77] Creating layer conv4
I0521 05:45:53.352746 30656 net.cpp:106] Creating Layer conv4
I0521 05:45:53.352758 30656 net.cpp:454] conv4 <- pool3
I0521 05:45:53.352774 30656 net.cpp:411] conv4 -> conv4
I0521 05:45:53.355566 30656 net.cpp:150] Setting up conv4
I0521 05:45:53.355602 30656 net.cpp:157] Top shape: 710 36 6 42 (6441120)
I0521 05:45:53.355615 30656 net.cpp:165] Memory required for data: 1079719720
I0521 05:45:53.355640 30656 layer_factory.hpp:77] Creating layer relu4
I0521 05:45:53.355669 30656 net.cpp:106] Creating Layer relu4
I0521 05:45:53.355682 30656 net.cpp:454] relu4 <- conv4
I0521 05:45:53.355698 30656 net.cpp:397] relu4 -> conv4 (in-place)
I0521 05:45:53.356199 30656 net.cpp:150] Setting up relu4
I0521 05:45:53.356222 30656 net.cpp:157] Top shape: 710 36 6 42 (6441120)
I0521 05:45:53.356236 30656 net.cpp:165] Memory required for data: 1105484200
I0521 05:45:53.356252 30656 layer_factory.hpp:77] Creating layer pool4
I0521 05:45:53.356267 30656 net.cpp:106] Creating Layer pool4
I0521 05:45:53.356281 30656 net.cpp:454] pool4 <- conv4
I0521 05:45:53.356304 30656 net.cpp:411] pool4 -> pool4
I0521 05:45:53.356380 30656 net.cpp:150] Setting up pool4
I0521 05:45:53.356408 30656 net.cpp:157] Top shape: 710 36 3 42 (3220560)
I0521 05:45:53.356422 30656 net.cpp:165] Memory required for data: 1118366440
I0521 05:45:53.356437 30656 layer_factory.hpp:77] Creating layer ip1
I0521 05:45:53.356458 30656 net.cpp:106] Creating Layer ip1
I0521 05:45:53.356472 30656 net.cpp:454] ip1 <- pool4
I0521 05:45:53.356494 30656 net.cpp:411] ip1 -> ip1
I0521 05:45:53.371922 30656 net.cpp:150] Setting up ip1
I0521 05:45:53.371953 30656 net.cpp:157] Top shape: 710 196 (139160)
I0521 05:45:53.371974 30656 net.cpp:165] Memory required for data: 1118923080
I0521 05:45:53.372000 30656 layer_factory.hpp:77] Creating layer relu5
I0521 05:45:53.372022 30656 net.cpp:106] Creating Layer relu5
I0521 05:45:53.372048 30656 net.cpp:454] relu5 <- ip1
I0521 05:45:53.372066 30656 net.cpp:397] relu5 -> ip1 (in-place)
I0521 05:45:53.372422 30656 net.cpp:150] Setting up relu5
I0521 05:45:53.372442 30656 net.cpp:157] Top shape: 710 196 (139160)
I0521 05:45:53.372455 30656 net.cpp:165] Memory required for data: 1119479720
I0521 05:45:53.372470 30656 layer_factory.hpp:77] Creating layer drop1
I0521 05:45:53.372501 30656 net.cpp:106] Creating Layer drop1
I0521 05:45:53.372516 30656 net.cpp:454] drop1 <- ip1
I0521 05:45:53.372544 30656 net.cpp:397] drop1 -> ip1 (in-place)
I0521 05:45:53.372604 30656 net.cpp:150] Setting up drop1
I0521 05:45:53.372642 30656 net.cpp:157] Top shape: 710 196 (139160)
I0521 05:45:53.372658 30656 net.cpp:165] Memory required for data: 1120036360
I0521 05:45:53.372673 30656 layer_factory.hpp:77] Creating layer ip2
I0521 05:45:53.372694 30656 net.cpp:106] Creating Layer ip2
I0521 05:45:53.372712 30656 net.cpp:454] ip2 <- ip1
I0521 05:45:53.372728 30656 net.cpp:411] ip2 -> ip2
I0521 05:45:53.373222 30656 net.cpp:150] Setting up ip2
I0521 05:45:53.373241 30656 net.cpp:157] Top shape: 710 98 (69580)
I0521 05:45:53.373255 30656 net.cpp:165] Memory required for data: 1120314680
I0521 05:45:53.373275 30656 layer_factory.hpp:77] Creating layer relu6
I0521 05:45:53.373297 30656 net.cpp:106] Creating Layer relu6
I0521 05:45:53.373311 30656 net.cpp:454] relu6 <- ip2
I0521 05:45:53.373325 30656 net.cpp:397] relu6 -> ip2 (in-place)
I0521 05:45:53.373873 30656 net.cpp:150] Setting up relu6
I0521 05:45:53.373896 30656 net.cpp:157] Top shape: 710 98 (69580)
I0521 05:45:53.373910 30656 net.cpp:165] Memory required for data: 1120593000
I0521 05:45:53.373926 30656 layer_factory.hpp:77] Creating layer drop2
I0521 05:45:53.373950 30656 net.cpp:106] Creating Layer drop2
I0521 05:45:53.373963 30656 net.cpp:454] drop2 <- ip2
I0521 05:45:53.373980 30656 net.cpp:397] drop2 -> ip2 (in-place)
I0521 05:45:53.374034 30656 net.cpp:150] Setting up drop2
I0521 05:45:53.374050 30656 net.cpp:157] Top shape: 710 98 (69580)
I0521 05:45:53.374063 30656 net.cpp:165] Memory required for data: 1120871320
I0521 05:45:53.374075 30656 layer_factory.hpp:77] Creating layer ip3
I0521 05:45:53.374094 30656 net.cpp:106] Creating Layer ip3
I0521 05:45:53.374107 30656 net.cpp:454] ip3 <- ip2
I0521 05:45:53.374133 30656 net.cpp:411] ip3 -> ip3
I0521 05:45:53.374359 30656 net.cpp:150] Setting up ip3
I0521 05:45:53.374379 30656 net.cpp:157] Top shape: 710 11 (7810)
I0521 05:45:53.374392 30656 net.cpp:165] Memory required for data: 1120902560
I0521 05:45:53.374413 30656 layer_factory.hpp:77] Creating layer drop3
I0521 05:45:53.374434 30656 net.cpp:106] Creating Layer drop3
I0521 05:45:53.374447 30656 net.cpp:454] drop3 <- ip3
I0521 05:45:53.374462 30656 net.cpp:397] drop3 -> ip3 (in-place)
I0521 05:45:53.374516 30656 net.cpp:150] Setting up drop3
I0521 05:45:53.374531 30656 net.cpp:157] Top shape: 710 11 (7810)
I0521 05:45:53.374544 30656 net.cpp:165] Memory required for data: 1120933800
I0521 05:45:53.374558 30656 layer_factory.hpp:77] Creating layer loss
I0521 05:45:53.374583 30656 net.cpp:106] Creating Layer loss
I0521 05:45:53.374595 30656 net.cpp:454] loss <- ip3
I0521 05:45:53.374615 30656 net.cpp:454] loss <- label
I0521 05:45:53.374631 30656 net.cpp:411] loss -> loss
I0521 05:45:53.374650 30656 layer_factory.hpp:77] Creating layer loss
I0521 05:45:53.375332 30656 net.cpp:150] Setting up loss
I0521 05:45:53.375354 30656 net.cpp:157] Top shape: (1)
I0521 05:45:53.375376 30656 net.cpp:160]     with loss weight 1
I0521 05:45:53.375427 30656 net.cpp:165] Memory required for data: 1120933804
I0521 05:45:53.375450 30656 net.cpp:226] loss needs backward computation.
I0521 05:45:53.375465 30656 net.cpp:226] drop3 needs backward computation.
I0521 05:45:53.375478 30656 net.cpp:226] ip3 needs backward computation.
I0521 05:45:53.375491 30656 net.cpp:226] drop2 needs backward computation.
I0521 05:45:53.375502 30656 net.cpp:226] relu6 needs backward computation.
I0521 05:45:53.375517 30656 net.cpp:226] ip2 needs backward computation.
I0521 05:45:53.375536 30656 net.cpp:226] drop1 needs backward computation.
I0521 05:45:53.375550 30656 net.cpp:226] relu5 needs backward computation.
I0521 05:45:53.375561 30656 net.cpp:226] ip1 needs backward computation.
I0521 05:45:53.375576 30656 net.cpp:226] pool4 needs backward computation.
I0521 05:45:53.375587 30656 net.cpp:226] relu4 needs backward computation.
I0521 05:45:53.375599 30656 net.cpp:226] conv4 needs backward computation.
I0521 05:45:53.375615 30656 net.cpp:226] pool3 needs backward computation.
I0521 05:45:53.375644 30656 net.cpp:226] relu3 needs backward computation.
I0521 05:45:53.375658 30656 net.cpp:226] conv3 needs backward computation.
I0521 05:45:53.375672 30656 net.cpp:226] pool2 needs backward computation.
I0521 05:45:53.375685 30656 net.cpp:226] relu2 needs backward computation.
I0521 05:45:53.375697 30656 net.cpp:226] conv2 needs backward computation.
I0521 05:45:53.375713 30656 net.cpp:226] pool1 needs backward computation.
I0521 05:45:53.375732 30656 net.cpp:226] relu1 needs backward computation.
I0521 05:45:53.375746 30656 net.cpp:226] conv1 needs backward computation.
I0521 05:45:53.375759 30656 net.cpp:228] data_hdf5 does not need backward computation.
I0521 05:45:53.375771 30656 net.cpp:270] This network produces output loss
I0521 05:45:53.375798 30656 net.cpp:283] Network initialization done.
I0521 05:45:53.377454 30656 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384.prototxt
I0521 05:45:53.377532 30656 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 05:45:53.377910 30656 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 710
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 05:45:53.378135 30656 layer_factory.hpp:77] Creating layer data_hdf5
I0521 05:45:53.378155 30656 net.cpp:106] Creating Layer data_hdf5
I0521 05:45:53.378171 30656 net.cpp:411] data_hdf5 -> data
I0521 05:45:53.378193 30656 net.cpp:411] data_hdf5 -> label
I0521 05:45:53.378211 30656 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 05:45:53.379461 30656 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 05:46:14.683045 30656 net.cpp:150] Setting up data_hdf5
I0521 05:46:14.683214 30656 net.cpp:157] Top shape: 710 1 127 50 (4508500)
I0521 05:46:14.683233 30656 net.cpp:157] Top shape: 710 (710)
I0521 05:46:14.683244 30656 net.cpp:165] Memory required for data: 18036840
I0521 05:46:14.683259 30656 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 05:46:14.683295 30656 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 05:46:14.683308 30656 net.cpp:454] label_data_hdf5_1_split <- label
I0521 05:46:14.683343 30656 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 05:46:14.683367 30656 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 05:46:14.683454 30656 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 05:46:14.683471 30656 net.cpp:157] Top shape: 710 (710)
I0521 05:46:14.683486 30656 net.cpp:157] Top shape: 710 (710)
I0521 05:46:14.683500 30656 net.cpp:165] Memory required for data: 18042520
I0521 05:46:14.683511 30656 layer_factory.hpp:77] Creating layer conv1
I0521 05:46:14.683544 30656 net.cpp:106] Creating Layer conv1
I0521 05:46:14.683558 30656 net.cpp:454] conv1 <- data
I0521 05:46:14.683575 30656 net.cpp:411] conv1 -> conv1
I0521 05:46:14.685531 30656 net.cpp:150] Setting up conv1
I0521 05:46:14.685557 30656 net.cpp:157] Top shape: 710 12 120 48 (49075200)
I0521 05:46:14.685578 30656 net.cpp:165] Memory required for data: 214343320
I0521 05:46:14.685602 30656 layer_factory.hpp:77] Creating layer relu1
I0521 05:46:14.685623 30656 net.cpp:106] Creating Layer relu1
I0521 05:46:14.685645 30656 net.cpp:454] relu1 <- conv1
I0521 05:46:14.685662 30656 net.cpp:397] relu1 -> conv1 (in-place)
I0521 05:46:14.686174 30656 net.cpp:150] Setting up relu1
I0521 05:46:14.686198 30656 net.cpp:157] Top shape: 710 12 120 48 (49075200)
I0521 05:46:14.686213 30656 net.cpp:165] Memory required for data: 410644120
I0521 05:46:14.686224 30656 layer_factory.hpp:77] Creating layer pool1
I0521 05:46:14.686254 30656 net.cpp:106] Creating Layer pool1
I0521 05:46:14.686269 30656 net.cpp:454] pool1 <- conv1
I0521 05:46:14.686283 30656 net.cpp:411] pool1 -> pool1
I0521 05:46:14.686373 30656 net.cpp:150] Setting up pool1
I0521 05:46:14.686391 30656 net.cpp:157] Top shape: 710 12 60 48 (24537600)
I0521 05:46:14.686403 30656 net.cpp:165] Memory required for data: 508794520
I0521 05:46:14.686424 30656 layer_factory.hpp:77] Creating layer conv2
I0521 05:46:14.686445 30656 net.cpp:106] Creating Layer conv2
I0521 05:46:14.686458 30656 net.cpp:454] conv2 <- pool1
I0521 05:46:14.686476 30656 net.cpp:411] conv2 -> conv2
I0521 05:46:14.688416 30656 net.cpp:150] Setting up conv2
I0521 05:46:14.688443 30656 net.cpp:157] Top shape: 710 20 54 46 (35272800)
I0521 05:46:14.688457 30656 net.cpp:165] Memory required for data: 649885720
I0521 05:46:14.688482 30656 layer_factory.hpp:77] Creating layer relu2
I0521 05:46:14.688506 30656 net.cpp:106] Creating Layer relu2
I0521 05:46:14.688520 30656 net.cpp:454] relu2 <- conv2
I0521 05:46:14.688536 30656 net.cpp:397] relu2 -> conv2 (in-place)
I0521 05:46:14.688904 30656 net.cpp:150] Setting up relu2
I0521 05:46:14.688923 30656 net.cpp:157] Top shape: 710 20 54 46 (35272800)
I0521 05:46:14.688936 30656 net.cpp:165] Memory required for data: 790976920
I0521 05:46:14.688947 30656 layer_factory.hpp:77] Creating layer pool2
I0521 05:46:14.688966 30656 net.cpp:106] Creating Layer pool2
I0521 05:46:14.688987 30656 net.cpp:454] pool2 <- conv2
I0521 05:46:14.689003 30656 net.cpp:411] pool2 -> pool2
I0521 05:46:14.689095 30656 net.cpp:150] Setting up pool2
I0521 05:46:14.689111 30656 net.cpp:157] Top shape: 710 20 27 46 (17636400)
I0521 05:46:14.689124 30656 net.cpp:165] Memory required for data: 861522520
I0521 05:46:14.689139 30656 layer_factory.hpp:77] Creating layer conv3
I0521 05:46:14.689162 30656 net.cpp:106] Creating Layer conv3
I0521 05:46:14.689182 30656 net.cpp:454] conv3 <- pool2
I0521 05:46:14.689198 30656 net.cpp:411] conv3 -> conv3
I0521 05:46:14.691200 30656 net.cpp:150] Setting up conv3
I0521 05:46:14.691229 30656 net.cpp:157] Top shape: 710 28 22 44 (19243840)
I0521 05:46:14.691243 30656 net.cpp:165] Memory required for data: 938497880
I0521 05:46:14.691282 30656 layer_factory.hpp:77] Creating layer relu3
I0521 05:46:14.691298 30656 net.cpp:106] Creating Layer relu3
I0521 05:46:14.691311 30656 net.cpp:454] relu3 <- conv3
I0521 05:46:14.691344 30656 net.cpp:397] relu3 -> conv3 (in-place)
I0521 05:46:14.691844 30656 net.cpp:150] Setting up relu3
I0521 05:46:14.691867 30656 net.cpp:157] Top shape: 710 28 22 44 (19243840)
I0521 05:46:14.691880 30656 net.cpp:165] Memory required for data: 1015473240
I0521 05:46:14.691896 30656 layer_factory.hpp:77] Creating layer pool3
I0521 05:46:14.691911 30656 net.cpp:106] Creating Layer pool3
I0521 05:46:14.691933 30656 net.cpp:454] pool3 <- conv3
I0521 05:46:14.691949 30656 net.cpp:411] pool3 -> pool3
I0521 05:46:14.692035 30656 net.cpp:150] Setting up pool3
I0521 05:46:14.692054 30656 net.cpp:157] Top shape: 710 28 11 44 (9621920)
I0521 05:46:14.692068 30656 net.cpp:165] Memory required for data: 1053960920
I0521 05:46:14.692080 30656 layer_factory.hpp:77] Creating layer conv4
I0521 05:46:14.692100 30656 net.cpp:106] Creating Layer conv4
I0521 05:46:14.692113 30656 net.cpp:454] conv4 <- pool3
I0521 05:46:14.692138 30656 net.cpp:411] conv4 -> conv4
I0521 05:46:14.694244 30656 net.cpp:150] Setting up conv4
I0521 05:46:14.694268 30656 net.cpp:157] Top shape: 710 36 6 42 (6441120)
I0521 05:46:14.694289 30656 net.cpp:165] Memory required for data: 1079725400
I0521 05:46:14.694308 30656 layer_factory.hpp:77] Creating layer relu4
I0521 05:46:14.694329 30656 net.cpp:106] Creating Layer relu4
I0521 05:46:14.694350 30656 net.cpp:454] relu4 <- conv4
I0521 05:46:14.694367 30656 net.cpp:397] relu4 -> conv4 (in-place)
I0521 05:46:14.694860 30656 net.cpp:150] Setting up relu4
I0521 05:46:14.694883 30656 net.cpp:157] Top shape: 710 36 6 42 (6441120)
I0521 05:46:14.694897 30656 net.cpp:165] Memory required for data: 1105489880
I0521 05:46:14.694913 30656 layer_factory.hpp:77] Creating layer pool4
I0521 05:46:14.694929 30656 net.cpp:106] Creating Layer pool4
I0521 05:46:14.694952 30656 net.cpp:454] pool4 <- conv4
I0521 05:46:14.694968 30656 net.cpp:411] pool4 -> pool4
I0521 05:46:14.695055 30656 net.cpp:150] Setting up pool4
I0521 05:46:14.695071 30656 net.cpp:157] Top shape: 710 36 3 42 (3220560)
I0521 05:46:14.695086 30656 net.cpp:165] Memory required for data: 1118372120
I0521 05:46:14.695097 30656 layer_factory.hpp:77] Creating layer ip1
I0521 05:46:14.695122 30656 net.cpp:106] Creating Layer ip1
I0521 05:46:14.695135 30656 net.cpp:454] ip1 <- pool4
I0521 05:46:14.695152 30656 net.cpp:411] ip1 -> ip1
I0521 05:46:14.710635 30656 net.cpp:150] Setting up ip1
I0521 05:46:14.710667 30656 net.cpp:157] Top shape: 710 196 (139160)
I0521 05:46:14.710688 30656 net.cpp:165] Memory required for data: 1118928760
I0521 05:46:14.710714 30656 layer_factory.hpp:77] Creating layer relu5
I0521 05:46:14.710736 30656 net.cpp:106] Creating Layer relu5
I0521 05:46:14.710762 30656 net.cpp:454] relu5 <- ip1
I0521 05:46:14.710777 30656 net.cpp:397] relu5 -> ip1 (in-place)
I0521 05:46:14.711145 30656 net.cpp:150] Setting up relu5
I0521 05:46:14.711165 30656 net.cpp:157] Top shape: 710 196 (139160)
I0521 05:46:14.711179 30656 net.cpp:165] Memory required for data: 1119485400
I0521 05:46:14.711194 30656 layer_factory.hpp:77] Creating layer drop1
I0521 05:46:14.711223 30656 net.cpp:106] Creating Layer drop1
I0521 05:46:14.711238 30656 net.cpp:454] drop1 <- ip1
I0521 05:46:14.711263 30656 net.cpp:397] drop1 -> ip1 (in-place)
I0521 05:46:14.711320 30656 net.cpp:150] Setting up drop1
I0521 05:46:14.711336 30656 net.cpp:157] Top shape: 710 196 (139160)
I0521 05:46:14.711349 30656 net.cpp:165] Memory required for data: 1120042040
I0521 05:46:14.711362 30656 layer_factory.hpp:77] Creating layer ip2
I0521 05:46:14.711379 30656 net.cpp:106] Creating Layer ip2
I0521 05:46:14.711393 30656 net.cpp:454] ip2 <- ip1
I0521 05:46:14.711416 30656 net.cpp:411] ip2 -> ip2
I0521 05:46:14.711915 30656 net.cpp:150] Setting up ip2
I0521 05:46:14.711935 30656 net.cpp:157] Top shape: 710 98 (69580)
I0521 05:46:14.711947 30656 net.cpp:165] Memory required for data: 1120320360
I0521 05:46:14.711987 30656 layer_factory.hpp:77] Creating layer relu6
I0521 05:46:14.712004 30656 net.cpp:106] Creating Layer relu6
I0521 05:46:14.712018 30656 net.cpp:454] relu6 <- ip2
I0521 05:46:14.712033 30656 net.cpp:397] relu6 -> ip2 (in-place)
I0521 05:46:14.712592 30656 net.cpp:150] Setting up relu6
I0521 05:46:14.712621 30656 net.cpp:157] Top shape: 710 98 (69580)
I0521 05:46:14.712635 30656 net.cpp:165] Memory required for data: 1120598680
I0521 05:46:14.712651 30656 layer_factory.hpp:77] Creating layer drop2
I0521 05:46:14.712677 30656 net.cpp:106] Creating Layer drop2
I0521 05:46:14.712690 30656 net.cpp:454] drop2 <- ip2
I0521 05:46:14.712707 30656 net.cpp:397] drop2 -> ip2 (in-place)
I0521 05:46:14.712764 30656 net.cpp:150] Setting up drop2
I0521 05:46:14.712779 30656 net.cpp:157] Top shape: 710 98 (69580)
I0521 05:46:14.712792 30656 net.cpp:165] Memory required for data: 1120877000
I0521 05:46:14.712805 30656 layer_factory.hpp:77] Creating layer ip3
I0521 05:46:14.712824 30656 net.cpp:106] Creating Layer ip3
I0521 05:46:14.712836 30656 net.cpp:454] ip3 <- ip2
I0521 05:46:14.712860 30656 net.cpp:411] ip3 -> ip3
I0521 05:46:14.713099 30656 net.cpp:150] Setting up ip3
I0521 05:46:14.713117 30656 net.cpp:157] Top shape: 710 11 (7810)
I0521 05:46:14.713130 30656 net.cpp:165] Memory required for data: 1120908240
I0521 05:46:14.713150 30656 layer_factory.hpp:77] Creating layer drop3
I0521 05:46:14.713173 30656 net.cpp:106] Creating Layer drop3
I0521 05:46:14.713186 30656 net.cpp:454] drop3 <- ip3
I0521 05:46:14.713202 30656 net.cpp:397] drop3 -> ip3 (in-place)
I0521 05:46:14.713250 30656 net.cpp:150] Setting up drop3
I0521 05:46:14.713274 30656 net.cpp:157] Top shape: 710 11 (7810)
I0521 05:46:14.713285 30656 net.cpp:165] Memory required for data: 1120939480
I0521 05:46:14.713304 30656 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 05:46:14.713320 30656 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 05:46:14.713332 30656 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 05:46:14.713351 30656 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 05:46:14.713376 30656 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 05:46:14.713464 30656 net.cpp:150] Setting up ip3_drop3_0_split
I0521 05:46:14.713479 30656 net.cpp:157] Top shape: 710 11 (7810)
I0521 05:46:14.713495 30656 net.cpp:157] Top shape: 710 11 (7810)
I0521 05:46:14.713510 30656 net.cpp:165] Memory required for data: 1121001960
I0521 05:46:14.713521 30656 layer_factory.hpp:77] Creating layer accuracy
I0521 05:46:14.713551 30656 net.cpp:106] Creating Layer accuracy
I0521 05:46:14.713564 30656 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 05:46:14.713578 30656 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 05:46:14.713594 30656 net.cpp:411] accuracy -> accuracy
I0521 05:46:14.713629 30656 net.cpp:150] Setting up accuracy
I0521 05:46:14.713645 30656 net.cpp:157] Top shape: (1)
I0521 05:46:14.713660 30656 net.cpp:165] Memory required for data: 1121001964
I0521 05:46:14.713673 30656 layer_factory.hpp:77] Creating layer loss
I0521 05:46:14.713690 30656 net.cpp:106] Creating Layer loss
I0521 05:46:14.713703 30656 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 05:46:14.713717 30656 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 05:46:14.713742 30656 net.cpp:411] loss -> loss
I0521 05:46:14.713765 30656 layer_factory.hpp:77] Creating layer loss
I0521 05:46:14.714288 30656 net.cpp:150] Setting up loss
I0521 05:46:14.714308 30656 net.cpp:157] Top shape: (1)
I0521 05:46:14.714321 30656 net.cpp:160]     with loss weight 1
I0521 05:46:14.714347 30656 net.cpp:165] Memory required for data: 1121001968
I0521 05:46:14.714368 30656 net.cpp:226] loss needs backward computation.
I0521 05:46:14.714382 30656 net.cpp:228] accuracy does not need backward computation.
I0521 05:46:14.714399 30656 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 05:46:14.714413 30656 net.cpp:226] drop3 needs backward computation.
I0521 05:46:14.714426 30656 net.cpp:226] ip3 needs backward computation.
I0521 05:46:14.714440 30656 net.cpp:226] drop2 needs backward computation.
I0521 05:46:14.714468 30656 net.cpp:226] relu6 needs backward computation.
I0521 05:46:14.714483 30656 net.cpp:226] ip2 needs backward computation.
I0521 05:46:14.714498 30656 net.cpp:226] drop1 needs backward computation.
I0521 05:46:14.714510 30656 net.cpp:226] relu5 needs backward computation.
I0521 05:46:14.714522 30656 net.cpp:226] ip1 needs backward computation.
I0521 05:46:14.714537 30656 net.cpp:226] pool4 needs backward computation.
I0521 05:46:14.714550 30656 net.cpp:226] relu4 needs backward computation.
I0521 05:46:14.714570 30656 net.cpp:226] conv4 needs backward computation.
I0521 05:46:14.714582 30656 net.cpp:226] pool3 needs backward computation.
I0521 05:46:14.714599 30656 net.cpp:226] relu3 needs backward computation.
I0521 05:46:14.714612 30656 net.cpp:226] conv3 needs backward computation.
I0521 05:46:14.714624 30656 net.cpp:226] pool2 needs backward computation.
I0521 05:46:14.714639 30656 net.cpp:226] relu2 needs backward computation.
I0521 05:46:14.714651 30656 net.cpp:226] conv2 needs backward computation.
I0521 05:46:14.714671 30656 net.cpp:226] pool1 needs backward computation.
I0521 05:46:14.714686 30656 net.cpp:226] relu1 needs backward computation.
I0521 05:46:14.714701 30656 net.cpp:226] conv1 needs backward computation.
I0521 05:46:14.714716 30656 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 05:46:14.714730 30656 net.cpp:228] data_hdf5 does not need backward computation.
I0521 05:46:14.714741 30656 net.cpp:270] This network produces output accuracy
I0521 05:46:14.714757 30656 net.cpp:270] This network produces output loss
I0521 05:46:14.714787 30656 net.cpp:283] Network initialization done.
I0521 05:46:14.714922 30656 solver.cpp:60] Solver scaffolding done.
I0521 05:46:14.716059 30656 caffe.cpp:212] Starting Optimization
I0521 05:46:14.716075 30656 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 05:46:14.716091 30656 solver.cpp:289] Learning Rate Policy: fixed
I0521 05:46:14.717334 30656 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 05:47:00.694146 30656 solver.cpp:409]     Test net output #0: accuracy = 0.112422
I0521 05:47:00.694314 30656 solver.cpp:409]     Test net output #1: loss = 2.39756 (* 1 = 2.39756 loss)
I0521 05:47:00.827848 30656 solver.cpp:237] Iteration 0, loss = 2.39673
I0521 05:47:00.827888 30656 solver.cpp:253]     Train net output #0: loss = 2.39673 (* 1 = 2.39673 loss)
I0521 05:47:00.827911 30656 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 05:47:08.855746 30656 solver.cpp:237] Iteration 21, loss = 2.38394
I0521 05:47:08.855782 30656 solver.cpp:253]     Train net output #0: loss = 2.38394 (* 1 = 2.38394 loss)
I0521 05:47:08.855799 30656 sgd_solver.cpp:106] Iteration 21, lr = 0.0025
I0521 05:47:16.882858 30656 solver.cpp:237] Iteration 42, loss = 2.36981
I0521 05:47:16.882912 30656 solver.cpp:253]     Train net output #0: loss = 2.36981 (* 1 = 2.36981 loss)
I0521 05:47:16.882930 30656 sgd_solver.cpp:106] Iteration 42, lr = 0.0025
I0521 05:47:24.915536 30656 solver.cpp:237] Iteration 63, loss = 2.35097
I0521 05:47:24.915570 30656 solver.cpp:253]     Train net output #0: loss = 2.35097 (* 1 = 2.35097 loss)
I0521 05:47:24.915594 30656 sgd_solver.cpp:106] Iteration 63, lr = 0.0025
I0521 05:47:32.946440 30656 solver.cpp:237] Iteration 84, loss = 2.33643
I0521 05:47:32.946589 30656 solver.cpp:253]     Train net output #0: loss = 2.33643 (* 1 = 2.33643 loss)
I0521 05:47:32.946606 30656 sgd_solver.cpp:106] Iteration 84, lr = 0.0025
I0521 05:47:40.978803 30656 solver.cpp:237] Iteration 105, loss = 2.3383
I0521 05:47:40.978860 30656 solver.cpp:253]     Train net output #0: loss = 2.3383 (* 1 = 2.3383 loss)
I0521 05:47:40.978886 30656 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0521 05:47:49.008884 30656 solver.cpp:237] Iteration 126, loss = 2.33024
I0521 05:47:49.008919 30656 solver.cpp:253]     Train net output #0: loss = 2.33024 (* 1 = 2.33024 loss)
I0521 05:47:49.008936 30656 sgd_solver.cpp:106] Iteration 126, lr = 0.0025
I0521 05:48:19.140166 30656 solver.cpp:237] Iteration 147, loss = 2.33025
I0521 05:48:19.140336 30656 solver.cpp:253]     Train net output #0: loss = 2.33025 (* 1 = 2.33025 loss)
I0521 05:48:19.140352 30656 sgd_solver.cpp:106] Iteration 147, lr = 0.0025
I0521 05:48:27.178128 30656 solver.cpp:237] Iteration 168, loss = 2.30243
I0521 05:48:27.178163 30656 solver.cpp:253]     Train net output #0: loss = 2.30243 (* 1 = 2.30243 loss)
I0521 05:48:27.178185 30656 sgd_solver.cpp:106] Iteration 168, lr = 0.0025
I0521 05:48:35.213598 30656 solver.cpp:237] Iteration 189, loss = 2.32298
I0521 05:48:35.213640 30656 solver.cpp:253]     Train net output #0: loss = 2.32298 (* 1 = 2.32298 loss)
I0521 05:48:35.213657 30656 sgd_solver.cpp:106] Iteration 189, lr = 0.0025
I0521 05:48:43.245909 30656 solver.cpp:237] Iteration 210, loss = 2.32179
I0521 05:48:43.245944 30656 solver.cpp:253]     Train net output #0: loss = 2.32179 (* 1 = 2.32179 loss)
I0521 05:48:43.245968 30656 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 05:48:43.246378 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_211.caffemodel
I0521 05:48:43.556481 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_211.solverstate
I0521 05:48:51.343379 30656 solver.cpp:237] Iteration 231, loss = 2.28959
I0521 05:48:51.343541 30656 solver.cpp:253]     Train net output #0: loss = 2.28959 (* 1 = 2.28959 loss)
I0521 05:48:51.343561 30656 sgd_solver.cpp:106] Iteration 231, lr = 0.0025
I0521 05:48:59.378653 30656 solver.cpp:237] Iteration 252, loss = 2.29724
I0521 05:48:59.378695 30656 solver.cpp:253]     Train net output #0: loss = 2.29724 (* 1 = 2.29724 loss)
I0521 05:48:59.378712 30656 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 05:49:07.417351 30656 solver.cpp:237] Iteration 273, loss = 2.30173
I0521 05:49:07.417385 30656 solver.cpp:253]     Train net output #0: loss = 2.30173 (* 1 = 2.30173 loss)
I0521 05:49:07.417410 30656 sgd_solver.cpp:106] Iteration 273, lr = 0.0025
I0521 05:49:37.561611 30656 solver.cpp:237] Iteration 294, loss = 2.28515
I0521 05:49:37.561774 30656 solver.cpp:253]     Train net output #0: loss = 2.28515 (* 1 = 2.28515 loss)
I0521 05:49:37.561791 30656 sgd_solver.cpp:106] Iteration 294, lr = 0.0025
I0521 05:49:45.593174 30656 solver.cpp:237] Iteration 315, loss = 2.25061
I0521 05:49:45.593225 30656 solver.cpp:253]     Train net output #0: loss = 2.25061 (* 1 = 2.25061 loss)
I0521 05:49:45.593242 30656 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0521 05:49:53.626735 30656 solver.cpp:237] Iteration 336, loss = 2.24019
I0521 05:49:53.626770 30656 solver.cpp:253]     Train net output #0: loss = 2.24019 (* 1 = 2.24019 loss)
I0521 05:49:53.626793 30656 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 05:50:01.663777 30656 solver.cpp:237] Iteration 357, loss = 2.17822
I0521 05:50:01.663811 30656 solver.cpp:253]     Train net output #0: loss = 2.17822 (* 1 = 2.17822 loss)
I0521 05:50:01.663836 30656 sgd_solver.cpp:106] Iteration 357, lr = 0.0025
I0521 05:50:09.698305 30656 solver.cpp:237] Iteration 378, loss = 2.15361
I0521 05:50:09.698475 30656 solver.cpp:253]     Train net output #0: loss = 2.15361 (* 1 = 2.15361 loss)
I0521 05:50:09.698495 30656 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 05:50:17.728122 30656 solver.cpp:237] Iteration 399, loss = 2.11265
I0521 05:50:17.728157 30656 solver.cpp:253]     Train net output #0: loss = 2.11265 (* 1 = 2.11265 loss)
I0521 05:50:17.728181 30656 sgd_solver.cpp:106] Iteration 399, lr = 0.0025
I0521 05:50:25.762950 30656 solver.cpp:237] Iteration 420, loss = 2.13116
I0521 05:50:25.762985 30656 solver.cpp:253]     Train net output #0: loss = 2.13116 (* 1 = 2.13116 loss)
I0521 05:50:25.763010 30656 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 05:50:26.145246 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_422.caffemodel
I0521 05:50:26.453748 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_422.solverstate
I0521 05:50:26.479423 30656 solver.cpp:341] Iteration 422, Testing net (#0)
I0521 05:51:11.543557 30656 solver.cpp:409]     Test net output #0: accuracy = 0.494133
I0521 05:51:11.543726 30656 solver.cpp:409]     Test net output #1: loss = 1.90396 (* 1 = 1.90396 loss)
I0521 05:51:41.017846 30656 solver.cpp:237] Iteration 441, loss = 2.09454
I0521 05:51:41.017906 30656 solver.cpp:253]     Train net output #0: loss = 2.09454 (* 1 = 2.09454 loss)
I0521 05:51:41.017925 30656 sgd_solver.cpp:106] Iteration 441, lr = 0.0025
I0521 05:51:49.045120 30656 solver.cpp:237] Iteration 462, loss = 2.05444
I0521 05:51:49.045272 30656 solver.cpp:253]     Train net output #0: loss = 2.05444 (* 1 = 2.05444 loss)
I0521 05:51:49.045291 30656 sgd_solver.cpp:106] Iteration 462, lr = 0.0025
I0521 05:51:57.071636 30656 solver.cpp:237] Iteration 483, loss = 2.06807
I0521 05:51:57.071671 30656 solver.cpp:253]     Train net output #0: loss = 2.06807 (* 1 = 2.06807 loss)
I0521 05:51:57.071694 30656 sgd_solver.cpp:106] Iteration 483, lr = 0.0025
I0521 05:52:05.102124 30656 solver.cpp:237] Iteration 504, loss = 1.97328
I0521 05:52:05.102159 30656 solver.cpp:253]     Train net output #0: loss = 1.97328 (* 1 = 1.97328 loss)
I0521 05:52:05.102181 30656 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 05:52:13.127382 30656 solver.cpp:237] Iteration 525, loss = 1.94235
I0521 05:52:13.127415 30656 solver.cpp:253]     Train net output #0: loss = 1.94235 (* 1 = 1.94235 loss)
I0521 05:52:13.127439 30656 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 05:52:21.156909 30656 solver.cpp:237] Iteration 546, loss = 1.97679
I0521 05:52:21.157028 30656 solver.cpp:253]     Train net output #0: loss = 1.97679 (* 1 = 1.97679 loss)
I0521 05:52:21.157047 30656 sgd_solver.cpp:106] Iteration 546, lr = 0.0025
I0521 05:52:51.335306 30656 solver.cpp:237] Iteration 567, loss = 1.97182
I0521 05:52:51.335476 30656 solver.cpp:253]     Train net output #0: loss = 1.97182 (* 1 = 1.97182 loss)
I0521 05:52:51.335494 30656 sgd_solver.cpp:106] Iteration 567, lr = 0.0025
I0521 05:52:59.362697 30656 solver.cpp:237] Iteration 588, loss = 1.94423
I0521 05:52:59.362731 30656 solver.cpp:253]     Train net output #0: loss = 1.94423 (* 1 = 1.94423 loss)
I0521 05:52:59.362751 30656 sgd_solver.cpp:106] Iteration 588, lr = 0.0025
I0521 05:53:07.392251 30656 solver.cpp:237] Iteration 609, loss = 1.95752
I0521 05:53:07.392304 30656 solver.cpp:253]     Train net output #0: loss = 1.95752 (* 1 = 1.95752 loss)
I0521 05:53:07.392333 30656 sgd_solver.cpp:106] Iteration 609, lr = 0.0025
I0521 05:53:15.421751 30656 solver.cpp:237] Iteration 630, loss = 1.92777
I0521 05:53:15.421787 30656 solver.cpp:253]     Train net output #0: loss = 1.92777 (* 1 = 1.92777 loss)
I0521 05:53:15.421805 30656 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 05:53:16.185364 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_633.caffemodel
I0521 05:53:16.492650 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_633.solverstate
I0521 05:53:23.509850 30656 solver.cpp:237] Iteration 651, loss = 1.89034
I0521 05:53:23.510027 30656 solver.cpp:253]     Train net output #0: loss = 1.89034 (* 1 = 1.89034 loss)
I0521 05:53:23.510045 30656 sgd_solver.cpp:106] Iteration 651, lr = 0.0025
I0521 05:53:31.531659 30656 solver.cpp:237] Iteration 672, loss = 1.84669
I0521 05:53:31.531716 30656 solver.cpp:253]     Train net output #0: loss = 1.84669 (* 1 = 1.84669 loss)
I0521 05:53:31.531743 30656 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 05:53:39.558071 30656 solver.cpp:237] Iteration 693, loss = 1.90885
I0521 05:53:39.558106 30656 solver.cpp:253]     Train net output #0: loss = 1.90885 (* 1 = 1.90885 loss)
I0521 05:53:39.558125 30656 sgd_solver.cpp:106] Iteration 693, lr = 0.0025
I0521 05:54:09.689262 30656 solver.cpp:237] Iteration 714, loss = 1.84159
I0521 05:54:09.689435 30656 solver.cpp:253]     Train net output #0: loss = 1.84159 (* 1 = 1.84159 loss)
I0521 05:54:09.689452 30656 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0521 05:54:17.711310 30656 solver.cpp:237] Iteration 735, loss = 1.87691
I0521 05:54:17.711345 30656 solver.cpp:253]     Train net output #0: loss = 1.87691 (* 1 = 1.87691 loss)
I0521 05:54:17.711364 30656 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0521 05:54:25.736335 30656 solver.cpp:237] Iteration 756, loss = 1.84608
I0521 05:54:25.736388 30656 solver.cpp:253]     Train net output #0: loss = 1.84608 (* 1 = 1.84608 loss)
I0521 05:54:25.736404 30656 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 05:54:33.761533 30656 solver.cpp:237] Iteration 777, loss = 1.91548
I0521 05:54:33.761567 30656 solver.cpp:253]     Train net output #0: loss = 1.91548 (* 1 = 1.91548 loss)
I0521 05:54:33.761591 30656 sgd_solver.cpp:106] Iteration 777, lr = 0.0025
I0521 05:54:41.790987 30656 solver.cpp:237] Iteration 798, loss = 1.84157
I0521 05:54:41.791128 30656 solver.cpp:253]     Train net output #0: loss = 1.84157 (* 1 = 1.84157 loss)
I0521 05:54:41.791144 30656 sgd_solver.cpp:106] Iteration 798, lr = 0.0025
I0521 05:54:49.815703 30656 solver.cpp:237] Iteration 819, loss = 1.8142
I0521 05:54:49.815757 30656 solver.cpp:253]     Train net output #0: loss = 1.8142 (* 1 = 1.8142 loss)
I0521 05:54:49.815784 30656 sgd_solver.cpp:106] Iteration 819, lr = 0.0025
I0521 05:54:57.839344 30656 solver.cpp:237] Iteration 840, loss = 1.88576
I0521 05:54:57.839376 30656 solver.cpp:253]     Train net output #0: loss = 1.88576 (* 1 = 1.88576 loss)
I0521 05:54:57.839401 30656 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 05:54:58.985812 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_844.caffemodel
I0521 05:54:59.295959 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_844.solverstate
I0521 05:54:59.323583 30656 solver.cpp:341] Iteration 844, Testing net (#0)
I0521 05:56:05.271085 30656 solver.cpp:409]     Test net output #0: accuracy = 0.570469
I0521 05:56:05.271261 30656 solver.cpp:409]     Test net output #1: loss = 1.48932 (* 1 = 1.48932 loss)
I0521 05:56:33.983716 30656 solver.cpp:237] Iteration 861, loss = 1.91035
I0521 05:56:33.983777 30656 solver.cpp:253]     Train net output #0: loss = 1.91035 (* 1 = 1.91035 loss)
I0521 05:56:33.983796 30656 sgd_solver.cpp:106] Iteration 861, lr = 0.0025
I0521 05:56:42.007002 30656 solver.cpp:237] Iteration 882, loss = 1.84112
I0521 05:56:42.007154 30656 solver.cpp:253]     Train net output #0: loss = 1.84112 (* 1 = 1.84112 loss)
I0521 05:56:42.007171 30656 sgd_solver.cpp:106] Iteration 882, lr = 0.0025
I0521 05:56:50.035012 30656 solver.cpp:237] Iteration 903, loss = 1.78726
I0521 05:56:50.035066 30656 solver.cpp:253]     Train net output #0: loss = 1.78726 (* 1 = 1.78726 loss)
I0521 05:56:50.035091 30656 sgd_solver.cpp:106] Iteration 903, lr = 0.0025
I0521 05:56:58.059648 30656 solver.cpp:237] Iteration 924, loss = 1.92822
I0521 05:56:58.059684 30656 solver.cpp:253]     Train net output #0: loss = 1.92822 (* 1 = 1.92822 loss)
I0521 05:56:58.059706 30656 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 05:57:06.088505 30656 solver.cpp:237] Iteration 945, loss = 1.84536
I0521 05:57:06.088538 30656 solver.cpp:253]     Train net output #0: loss = 1.84536 (* 1 = 1.84536 loss)
I0521 05:57:06.088562 30656 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 05:57:14.114110 30656 solver.cpp:237] Iteration 966, loss = 1.84943
I0521 05:57:14.114269 30656 solver.cpp:253]     Train net output #0: loss = 1.84943 (* 1 = 1.84943 loss)
I0521 05:57:14.114289 30656 sgd_solver.cpp:106] Iteration 966, lr = 0.0025
I0521 05:57:44.313052 30656 solver.cpp:237] Iteration 987, loss = 1.79646
I0521 05:57:44.313221 30656 solver.cpp:253]     Train net output #0: loss = 1.79646 (* 1 = 1.79646 loss)
I0521 05:57:44.313240 30656 sgd_solver.cpp:106] Iteration 987, lr = 0.0025
I0521 05:57:52.339046 30656 solver.cpp:237] Iteration 1008, loss = 1.72802
I0521 05:57:52.339081 30656 solver.cpp:253]     Train net output #0: loss = 1.72802 (* 1 = 1.72802 loss)
I0521 05:57:52.339104 30656 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 05:58:00.365059 30656 solver.cpp:237] Iteration 1029, loss = 1.74098
I0521 05:58:00.365094 30656 solver.cpp:253]     Train net output #0: loss = 1.74098 (* 1 = 1.74098 loss)
I0521 05:58:00.365118 30656 sgd_solver.cpp:106] Iteration 1029, lr = 0.0025
I0521 05:58:08.393821 30656 solver.cpp:237] Iteration 1050, loss = 1.8049
I0521 05:58:08.393874 30656 solver.cpp:253]     Train net output #0: loss = 1.8049 (* 1 = 1.8049 loss)
I0521 05:58:08.393903 30656 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 05:58:09.923105 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1055.caffemodel
I0521 05:58:10.233310 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1055.solverstate
I0521 05:58:16.491317 30656 solver.cpp:237] Iteration 1071, loss = 1.81011
I0521 05:58:16.491484 30656 solver.cpp:253]     Train net output #0: loss = 1.81011 (* 1 = 1.81011 loss)
I0521 05:58:16.491503 30656 sgd_solver.cpp:106] Iteration 1071, lr = 0.0025
I0521 05:58:24.512070 30656 solver.cpp:237] Iteration 1092, loss = 1.83358
I0521 05:58:24.512105 30656 solver.cpp:253]     Train net output #0: loss = 1.83358 (* 1 = 1.83358 loss)
I0521 05:58:24.512130 30656 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0521 05:58:32.534399 30656 solver.cpp:237] Iteration 1113, loss = 1.84158
I0521 05:58:32.534453 30656 solver.cpp:253]     Train net output #0: loss = 1.84158 (* 1 = 1.84158 loss)
I0521 05:58:32.534471 30656 sgd_solver.cpp:106] Iteration 1113, lr = 0.0025
I0521 05:59:02.744843 30656 solver.cpp:237] Iteration 1134, loss = 1.86452
I0521 05:59:02.745014 30656 solver.cpp:253]     Train net output #0: loss = 1.86452 (* 1 = 1.86452 loss)
I0521 05:59:02.745031 30656 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 05:59:10.770689 30656 solver.cpp:237] Iteration 1155, loss = 1.80719
I0521 05:59:10.770725 30656 solver.cpp:253]     Train net output #0: loss = 1.80719 (* 1 = 1.80719 loss)
I0521 05:59:10.770748 30656 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 05:59:18.807638 30656 solver.cpp:237] Iteration 1176, loss = 1.81068
I0521 05:59:18.807698 30656 solver.cpp:253]     Train net output #0: loss = 1.81068 (* 1 = 1.81068 loss)
I0521 05:59:18.807723 30656 sgd_solver.cpp:106] Iteration 1176, lr = 0.0025
I0521 05:59:26.832515 30656 solver.cpp:237] Iteration 1197, loss = 1.81626
I0521 05:59:26.832551 30656 solver.cpp:253]     Train net output #0: loss = 1.81626 (* 1 = 1.81626 loss)
I0521 05:59:26.832573 30656 sgd_solver.cpp:106] Iteration 1197, lr = 0.0025
I0521 05:59:34.861403 30656 solver.cpp:237] Iteration 1218, loss = 1.74535
I0521 05:59:34.861548 30656 solver.cpp:253]     Train net output #0: loss = 1.74535 (* 1 = 1.74535 loss)
I0521 05:59:34.861565 30656 sgd_solver.cpp:106] Iteration 1218, lr = 0.0025
I0521 05:59:42.889956 30656 solver.cpp:237] Iteration 1239, loss = 1.87549
I0521 05:59:42.889989 30656 solver.cpp:253]     Train net output #0: loss = 1.87549 (* 1 = 1.87549 loss)
I0521 05:59:42.890013 30656 sgd_solver.cpp:106] Iteration 1239, lr = 0.0025
I0521 05:59:50.916910 30656 solver.cpp:237] Iteration 1260, loss = 1.75731
I0521 05:59:50.916965 30656 solver.cpp:253]     Train net output #0: loss = 1.75731 (* 1 = 1.75731 loss)
I0521 05:59:50.916991 30656 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 05:59:52.830051 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1266.caffemodel
I0521 05:59:53.153543 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1266.solverstate
I0521 05:59:53.180001 30656 solver.cpp:341] Iteration 1266, Testing net (#0)
I0521 06:00:37.927338 30656 solver.cpp:409]     Test net output #0: accuracy = 0.624458
I0521 06:00:37.927510 30656 solver.cpp:409]     Test net output #1: loss = 1.33684 (* 1 = 1.33684 loss)
I0521 06:01:05.947886 30656 solver.cpp:237] Iteration 1281, loss = 1.7556
I0521 06:01:05.947947 30656 solver.cpp:253]     Train net output #0: loss = 1.7556 (* 1 = 1.7556 loss)
I0521 06:01:05.947973 30656 sgd_solver.cpp:106] Iteration 1281, lr = 0.0025
I0521 06:01:13.975389 30656 solver.cpp:237] Iteration 1302, loss = 1.76122
I0521 06:01:13.975541 30656 solver.cpp:253]     Train net output #0: loss = 1.76122 (* 1 = 1.76122 loss)
I0521 06:01:13.975558 30656 sgd_solver.cpp:106] Iteration 1302, lr = 0.0025
I0521 06:01:22.004643 30656 solver.cpp:237] Iteration 1323, loss = 1.81539
I0521 06:01:22.004676 30656 solver.cpp:253]     Train net output #0: loss = 1.81539 (* 1 = 1.81539 loss)
I0521 06:01:22.004700 30656 sgd_solver.cpp:106] Iteration 1323, lr = 0.0025
I0521 06:01:30.033859 30656 solver.cpp:237] Iteration 1344, loss = 1.73032
I0521 06:01:30.033902 30656 solver.cpp:253]     Train net output #0: loss = 1.73032 (* 1 = 1.73032 loss)
I0521 06:01:30.033920 30656 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 06:01:38.055513 30656 solver.cpp:237] Iteration 1365, loss = 1.74191
I0521 06:01:38.055547 30656 solver.cpp:253]     Train net output #0: loss = 1.74191 (* 1 = 1.74191 loss)
I0521 06:01:38.055572 30656 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0521 06:01:46.081651 30656 solver.cpp:237] Iteration 1386, loss = 1.72765
I0521 06:01:46.081804 30656 solver.cpp:253]     Train net output #0: loss = 1.72765 (* 1 = 1.72765 loss)
I0521 06:01:46.081821 30656 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 06:01:54.108455 30656 solver.cpp:237] Iteration 1407, loss = 1.75623
I0521 06:01:54.108500 30656 solver.cpp:253]     Train net output #0: loss = 1.75623 (* 1 = 1.75623 loss)
I0521 06:01:54.108516 30656 sgd_solver.cpp:106] Iteration 1407, lr = 0.0025
I0521 06:02:24.297116 30656 solver.cpp:237] Iteration 1428, loss = 1.77949
I0521 06:02:24.297289 30656 solver.cpp:253]     Train net output #0: loss = 1.77949 (* 1 = 1.77949 loss)
I0521 06:02:24.297307 30656 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 06:02:32.324407 30656 solver.cpp:237] Iteration 1449, loss = 1.67629
I0521 06:02:32.324441 30656 solver.cpp:253]     Train net output #0: loss = 1.67629 (* 1 = 1.67629 loss)
I0521 06:02:32.324460 30656 sgd_solver.cpp:106] Iteration 1449, lr = 0.0025
I0521 06:02:40.351145 30656 solver.cpp:237] Iteration 1470, loss = 1.75691
I0521 06:02:40.351191 30656 solver.cpp:253]     Train net output #0: loss = 1.75691 (* 1 = 1.75691 loss)
I0521 06:02:40.351207 30656 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 06:02:42.645623 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1477.caffemodel
I0521 06:02:42.952216 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1477.solverstate
I0521 06:02:48.443717 30656 solver.cpp:237] Iteration 1491, loss = 1.69127
I0521 06:02:48.443770 30656 solver.cpp:253]     Train net output #0: loss = 1.69127 (* 1 = 1.69127 loss)
I0521 06:02:48.443796 30656 sgd_solver.cpp:106] Iteration 1491, lr = 0.0025
I0521 06:02:56.470273 30656 solver.cpp:237] Iteration 1512, loss = 1.78261
I0521 06:02:56.470422 30656 solver.cpp:253]     Train net output #0: loss = 1.78261 (* 1 = 1.78261 loss)
I0521 06:02:56.470438 30656 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 06:03:04.494530 30656 solver.cpp:237] Iteration 1533, loss = 1.7497
I0521 06:03:04.494582 30656 solver.cpp:253]     Train net output #0: loss = 1.7497 (* 1 = 1.7497 loss)
I0521 06:03:04.494608 30656 sgd_solver.cpp:106] Iteration 1533, lr = 0.0025
I0521 06:03:34.714334 30656 solver.cpp:237] Iteration 1554, loss = 1.70794
I0521 06:03:34.714514 30656 solver.cpp:253]     Train net output #0: loss = 1.70794 (* 1 = 1.70794 loss)
I0521 06:03:34.714530 30656 sgd_solver.cpp:106] Iteration 1554, lr = 0.0025
I0521 06:03:42.744129 30656 solver.cpp:237] Iteration 1575, loss = 1.65422
I0521 06:03:42.744163 30656 solver.cpp:253]     Train net output #0: loss = 1.65422 (* 1 = 1.65422 loss)
I0521 06:03:42.744185 30656 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0521 06:03:50.766923 30656 solver.cpp:237] Iteration 1596, loss = 1.65819
I0521 06:03:50.766958 30656 solver.cpp:253]     Train net output #0: loss = 1.65819 (* 1 = 1.65819 loss)
I0521 06:03:50.766981 30656 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0521 06:03:58.791673 30656 solver.cpp:237] Iteration 1617, loss = 1.68597
I0521 06:03:58.791715 30656 solver.cpp:253]     Train net output #0: loss = 1.68597 (* 1 = 1.68597 loss)
I0521 06:03:58.791733 30656 sgd_solver.cpp:106] Iteration 1617, lr = 0.0025
I0521 06:04:06.818379 30656 solver.cpp:237] Iteration 1638, loss = 1.73092
I0521 06:04:06.818518 30656 solver.cpp:253]     Train net output #0: loss = 1.73092 (* 1 = 1.73092 loss)
I0521 06:04:06.818534 30656 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 06:04:14.844715 30656 solver.cpp:237] Iteration 1659, loss = 1.73872
I0521 06:04:14.844749 30656 solver.cpp:253]     Train net output #0: loss = 1.73872 (* 1 = 1.73872 loss)
I0521 06:04:14.844772 30656 sgd_solver.cpp:106] Iteration 1659, lr = 0.0025
I0521 06:04:22.865833 30656 solver.cpp:237] Iteration 1680, loss = 1.57378
I0521 06:04:22.865871 30656 solver.cpp:253]     Train net output #0: loss = 1.57378 (* 1 = 1.57378 loss)
I0521 06:04:22.865887 30656 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 06:04:25.543845 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1688.caffemodel
I0521 06:04:25.851824 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1688.solverstate
I0521 06:04:25.878423 30656 solver.cpp:341] Iteration 1688, Testing net (#0)
I0521 06:05:31.847216 30656 solver.cpp:409]     Test net output #0: accuracy = 0.659215
I0521 06:05:31.847409 30656 solver.cpp:409]     Test net output #1: loss = 1.1715 (* 1 = 1.1715 loss)
I0521 06:05:59.083700 30656 solver.cpp:237] Iteration 1701, loss = 1.68418
I0521 06:05:59.083760 30656 solver.cpp:253]     Train net output #0: loss = 1.68418 (* 1 = 1.68418 loss)
I0521 06:05:59.083777 30656 sgd_solver.cpp:106] Iteration 1701, lr = 0.0025
I0521 06:06:07.110929 30656 solver.cpp:237] Iteration 1722, loss = 1.63559
I0521 06:06:07.111085 30656 solver.cpp:253]     Train net output #0: loss = 1.63559 (* 1 = 1.63559 loss)
I0521 06:06:07.111102 30656 sgd_solver.cpp:106] Iteration 1722, lr = 0.0025
I0521 06:06:15.132184 30656 solver.cpp:237] Iteration 1743, loss = 1.70854
I0521 06:06:15.132220 30656 solver.cpp:253]     Train net output #0: loss = 1.70854 (* 1 = 1.70854 loss)
I0521 06:06:15.132243 30656 sgd_solver.cpp:106] Iteration 1743, lr = 0.0025
I0521 06:06:23.156956 30656 solver.cpp:237] Iteration 1764, loss = 1.70607
I0521 06:06:23.156996 30656 solver.cpp:253]     Train net output #0: loss = 1.70607 (* 1 = 1.70607 loss)
I0521 06:06:23.157013 30656 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 06:06:31.183212 30656 solver.cpp:237] Iteration 1785, loss = 1.67103
I0521 06:06:31.183264 30656 solver.cpp:253]     Train net output #0: loss = 1.67103 (* 1 = 1.67103 loss)
I0521 06:06:31.183280 30656 sgd_solver.cpp:106] Iteration 1785, lr = 0.0025
I0521 06:06:39.208341 30656 solver.cpp:237] Iteration 1806, loss = 1.67809
I0521 06:06:39.208490 30656 solver.cpp:253]     Train net output #0: loss = 1.67809 (* 1 = 1.67809 loss)
I0521 06:06:39.208508 30656 sgd_solver.cpp:106] Iteration 1806, lr = 0.0025
I0521 06:06:47.227483 30656 solver.cpp:237] Iteration 1827, loss = 1.676
I0521 06:06:47.227516 30656 solver.cpp:253]     Train net output #0: loss = 1.676 (* 1 = 1.676 loss)
I0521 06:06:47.227540 30656 sgd_solver.cpp:106] Iteration 1827, lr = 0.0025
I0521 06:07:17.464747 30656 solver.cpp:237] Iteration 1848, loss = 1.61652
I0521 06:07:17.464923 30656 solver.cpp:253]     Train net output #0: loss = 1.61652 (* 1 = 1.61652 loss)
I0521 06:07:17.464941 30656 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 06:07:25.492051 30656 solver.cpp:237] Iteration 1869, loss = 1.59515
I0521 06:07:25.492086 30656 solver.cpp:253]     Train net output #0: loss = 1.59515 (* 1 = 1.59515 loss)
I0521 06:07:25.492105 30656 sgd_solver.cpp:106] Iteration 1869, lr = 0.0025
I0521 06:07:33.517115 30656 solver.cpp:237] Iteration 1890, loss = 1.69366
I0521 06:07:33.517149 30656 solver.cpp:253]     Train net output #0: loss = 1.69366 (* 1 = 1.69366 loss)
I0521 06:07:33.517168 30656 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0521 06:07:36.576087 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1899.caffemodel
I0521 06:07:36.884965 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_1899.solverstate
I0521 06:07:41.616205 30656 solver.cpp:237] Iteration 1911, loss = 1.87271
I0521 06:07:41.616264 30656 solver.cpp:253]     Train net output #0: loss = 1.87271 (* 1 = 1.87271 loss)
I0521 06:07:41.616282 30656 sgd_solver.cpp:106] Iteration 1911, lr = 0.0025
I0521 06:07:49.643728 30656 solver.cpp:237] Iteration 1932, loss = 1.63997
I0521 06:07:49.643892 30656 solver.cpp:253]     Train net output #0: loss = 1.63997 (* 1 = 1.63997 loss)
I0521 06:07:49.643908 30656 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0521 06:07:57.669930 30656 solver.cpp:237] Iteration 1953, loss = 1.61672
I0521 06:07:57.669965 30656 solver.cpp:253]     Train net output #0: loss = 1.61672 (* 1 = 1.61672 loss)
I0521 06:07:57.669989 30656 sgd_solver.cpp:106] Iteration 1953, lr = 0.0025
I0521 06:08:27.917611 30656 solver.cpp:237] Iteration 1974, loss = 1.8205
I0521 06:08:27.917793 30656 solver.cpp:253]     Train net output #0: loss = 1.8205 (* 1 = 1.8205 loss)
I0521 06:08:27.917809 30656 sgd_solver.cpp:106] Iteration 1974, lr = 0.0025
I0521 06:08:35.946326 30656 solver.cpp:237] Iteration 1995, loss = 1.77745
I0521 06:08:35.946385 30656 solver.cpp:253]     Train net output #0: loss = 1.77745 (* 1 = 1.77745 loss)
I0521 06:08:35.946403 30656 sgd_solver.cpp:106] Iteration 1995, lr = 0.0025
I0521 06:08:43.970731 30656 solver.cpp:237] Iteration 2016, loss = 1.66799
I0521 06:08:43.970767 30656 solver.cpp:253]     Train net output #0: loss = 1.66799 (* 1 = 1.66799 loss)
I0521 06:08:43.970784 30656 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0521 06:08:51.996278 30656 solver.cpp:237] Iteration 2037, loss = 1.71693
I0521 06:08:51.996310 30656 solver.cpp:253]     Train net output #0: loss = 1.71693 (* 1 = 1.71693 loss)
I0521 06:08:51.996335 30656 sgd_solver.cpp:106] Iteration 2037, lr = 0.0025
I0521 06:09:00.023633 30656 solver.cpp:237] Iteration 2058, loss = 1.70048
I0521 06:09:00.023802 30656 solver.cpp:253]     Train net output #0: loss = 1.70048 (* 1 = 1.70048 loss)
I0521 06:09:00.023819 30656 sgd_solver.cpp:106] Iteration 2058, lr = 0.0025
I0521 06:09:08.048465 30656 solver.cpp:237] Iteration 2079, loss = 1.67055
I0521 06:09:08.048499 30656 solver.cpp:253]     Train net output #0: loss = 1.67055 (* 1 = 1.67055 loss)
I0521 06:09:08.048523 30656 sgd_solver.cpp:106] Iteration 2079, lr = 0.0025
I0521 06:09:16.074674 30656 solver.cpp:237] Iteration 2100, loss = 1.66108
I0521 06:09:16.074709 30656 solver.cpp:253]     Train net output #0: loss = 1.66108 (* 1 = 1.66108 loss)
I0521 06:09:16.074733 30656 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0521 06:09:19.513120 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_2110.caffemodel
I0521 06:09:19.821852 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_2110.solverstate
I0521 06:09:19.850297 30656 solver.cpp:341] Iteration 2110, Testing net (#0)
I0521 06:10:04.942018 30656 solver.cpp:409]     Test net output #0: accuracy = 0.677538
I0521 06:10:04.942190 30656 solver.cpp:409]     Test net output #1: loss = 1.1356 (* 1 = 1.1356 loss)
I0521 06:10:05.438107 30656 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_2112.caffemodel
I0521 06:10:05.745038 30656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384_iter_2112.solverstate
I0521 06:10:05.773095 30656 solver.cpp:326] Optimization Done.
I0521 06:10:05.773133 30656 caffe.cpp:215] Optimization Done.
Application 11236939 resources: utime ~1251s, stime ~226s, Rss ~5329832, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_710_2016-05-20T11.20.58.449384.solver"
	User time (seconds): 0.53
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:40.87
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15098
	Voluntary context switches: 2757
	Involuntary context switches: 77
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
