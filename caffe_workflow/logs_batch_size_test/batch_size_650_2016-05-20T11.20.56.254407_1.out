2806255
I0521 04:30:04.183672 20020 caffe.cpp:184] Using GPUs 0
I0521 04:30:04.612844 20020 solver.cpp:48] Initializing solver from parameters: 
test_iter: 230
test_interval: 461
base_lr: 0.0025
display: 23
max_iter: 2307
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 230
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407.prototxt"
I0521 04:30:04.614743 20020 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407.prototxt
I0521 04:30:04.635313 20020 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 04:30:04.635372 20020 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 04:30:04.635716 20020 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 650
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:30:04.635895 20020 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:30:04.635918 20020 net.cpp:106] Creating Layer data_hdf5
I0521 04:30:04.635933 20020 net.cpp:411] data_hdf5 -> data
I0521 04:30:04.635967 20020 net.cpp:411] data_hdf5 -> label
I0521 04:30:04.635998 20020 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 04:30:04.637259 20020 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 04:30:04.639456 20020 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 04:30:26.216017 20020 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 04:30:26.221134 20020 net.cpp:150] Setting up data_hdf5
I0521 04:30:26.221179 20020 net.cpp:157] Top shape: 650 1 127 50 (4127500)
I0521 04:30:26.221194 20020 net.cpp:157] Top shape: 650 (650)
I0521 04:30:26.221204 20020 net.cpp:165] Memory required for data: 16512600
I0521 04:30:26.221217 20020 layer_factory.hpp:77] Creating layer conv1
I0521 04:30:26.221251 20020 net.cpp:106] Creating Layer conv1
I0521 04:30:26.221263 20020 net.cpp:454] conv1 <- data
I0521 04:30:26.221285 20020 net.cpp:411] conv1 -> conv1
I0521 04:30:26.588284 20020 net.cpp:150] Setting up conv1
I0521 04:30:26.588330 20020 net.cpp:157] Top shape: 650 12 120 48 (44928000)
I0521 04:30:26.588341 20020 net.cpp:165] Memory required for data: 196224600
I0521 04:30:26.588369 20020 layer_factory.hpp:77] Creating layer relu1
I0521 04:30:26.588392 20020 net.cpp:106] Creating Layer relu1
I0521 04:30:26.588402 20020 net.cpp:454] relu1 <- conv1
I0521 04:30:26.588415 20020 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:30:26.588930 20020 net.cpp:150] Setting up relu1
I0521 04:30:26.588948 20020 net.cpp:157] Top shape: 650 12 120 48 (44928000)
I0521 04:30:26.588958 20020 net.cpp:165] Memory required for data: 375936600
I0521 04:30:26.588966 20020 layer_factory.hpp:77] Creating layer pool1
I0521 04:30:26.588984 20020 net.cpp:106] Creating Layer pool1
I0521 04:30:26.588994 20020 net.cpp:454] pool1 <- conv1
I0521 04:30:26.589006 20020 net.cpp:411] pool1 -> pool1
I0521 04:30:26.589087 20020 net.cpp:150] Setting up pool1
I0521 04:30:26.589102 20020 net.cpp:157] Top shape: 650 12 60 48 (22464000)
I0521 04:30:26.589112 20020 net.cpp:165] Memory required for data: 465792600
I0521 04:30:26.589119 20020 layer_factory.hpp:77] Creating layer conv2
I0521 04:30:26.589143 20020 net.cpp:106] Creating Layer conv2
I0521 04:30:26.589154 20020 net.cpp:454] conv2 <- pool1
I0521 04:30:26.589166 20020 net.cpp:411] conv2 -> conv2
I0521 04:30:26.591856 20020 net.cpp:150] Setting up conv2
I0521 04:30:26.591877 20020 net.cpp:157] Top shape: 650 20 54 46 (32292000)
I0521 04:30:26.591888 20020 net.cpp:165] Memory required for data: 594960600
I0521 04:30:26.591907 20020 layer_factory.hpp:77] Creating layer relu2
I0521 04:30:26.591922 20020 net.cpp:106] Creating Layer relu2
I0521 04:30:26.591933 20020 net.cpp:454] relu2 <- conv2
I0521 04:30:26.591945 20020 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:30:26.592275 20020 net.cpp:150] Setting up relu2
I0521 04:30:26.592290 20020 net.cpp:157] Top shape: 650 20 54 46 (32292000)
I0521 04:30:26.592300 20020 net.cpp:165] Memory required for data: 724128600
I0521 04:30:26.592310 20020 layer_factory.hpp:77] Creating layer pool2
I0521 04:30:26.592322 20020 net.cpp:106] Creating Layer pool2
I0521 04:30:26.592332 20020 net.cpp:454] pool2 <- conv2
I0521 04:30:26.592357 20020 net.cpp:411] pool2 -> pool2
I0521 04:30:26.592427 20020 net.cpp:150] Setting up pool2
I0521 04:30:26.592440 20020 net.cpp:157] Top shape: 650 20 27 46 (16146000)
I0521 04:30:26.592450 20020 net.cpp:165] Memory required for data: 788712600
I0521 04:30:26.592459 20020 layer_factory.hpp:77] Creating layer conv3
I0521 04:30:26.592478 20020 net.cpp:106] Creating Layer conv3
I0521 04:30:26.592489 20020 net.cpp:454] conv3 <- pool2
I0521 04:30:26.592501 20020 net.cpp:411] conv3 -> conv3
I0521 04:30:26.594434 20020 net.cpp:150] Setting up conv3
I0521 04:30:26.594457 20020 net.cpp:157] Top shape: 650 28 22 44 (17617600)
I0521 04:30:26.594470 20020 net.cpp:165] Memory required for data: 859183000
I0521 04:30:26.594488 20020 layer_factory.hpp:77] Creating layer relu3
I0521 04:30:26.594504 20020 net.cpp:106] Creating Layer relu3
I0521 04:30:26.594514 20020 net.cpp:454] relu3 <- conv3
I0521 04:30:26.594527 20020 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:30:26.594995 20020 net.cpp:150] Setting up relu3
I0521 04:30:26.595012 20020 net.cpp:157] Top shape: 650 28 22 44 (17617600)
I0521 04:30:26.595022 20020 net.cpp:165] Memory required for data: 929653400
I0521 04:30:26.595032 20020 layer_factory.hpp:77] Creating layer pool3
I0521 04:30:26.595046 20020 net.cpp:106] Creating Layer pool3
I0521 04:30:26.595055 20020 net.cpp:454] pool3 <- conv3
I0521 04:30:26.595068 20020 net.cpp:411] pool3 -> pool3
I0521 04:30:26.595136 20020 net.cpp:150] Setting up pool3
I0521 04:30:26.595149 20020 net.cpp:157] Top shape: 650 28 11 44 (8808800)
I0521 04:30:26.595160 20020 net.cpp:165] Memory required for data: 964888600
I0521 04:30:26.595168 20020 layer_factory.hpp:77] Creating layer conv4
I0521 04:30:26.595186 20020 net.cpp:106] Creating Layer conv4
I0521 04:30:26.595197 20020 net.cpp:454] conv4 <- pool3
I0521 04:30:26.595211 20020 net.cpp:411] conv4 -> conv4
I0521 04:30:26.598003 20020 net.cpp:150] Setting up conv4
I0521 04:30:26.598027 20020 net.cpp:157] Top shape: 650 36 6 42 (5896800)
I0521 04:30:26.598042 20020 net.cpp:165] Memory required for data: 988475800
I0521 04:30:26.598058 20020 layer_factory.hpp:77] Creating layer relu4
I0521 04:30:26.598073 20020 net.cpp:106] Creating Layer relu4
I0521 04:30:26.598083 20020 net.cpp:454] relu4 <- conv4
I0521 04:30:26.598095 20020 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:30:26.598568 20020 net.cpp:150] Setting up relu4
I0521 04:30:26.598584 20020 net.cpp:157] Top shape: 650 36 6 42 (5896800)
I0521 04:30:26.598594 20020 net.cpp:165] Memory required for data: 1012063000
I0521 04:30:26.598605 20020 layer_factory.hpp:77] Creating layer pool4
I0521 04:30:26.598618 20020 net.cpp:106] Creating Layer pool4
I0521 04:30:26.598628 20020 net.cpp:454] pool4 <- conv4
I0521 04:30:26.598640 20020 net.cpp:411] pool4 -> pool4
I0521 04:30:26.598709 20020 net.cpp:150] Setting up pool4
I0521 04:30:26.598722 20020 net.cpp:157] Top shape: 650 36 3 42 (2948400)
I0521 04:30:26.598733 20020 net.cpp:165] Memory required for data: 1023856600
I0521 04:30:26.598743 20020 layer_factory.hpp:77] Creating layer ip1
I0521 04:30:26.598763 20020 net.cpp:106] Creating Layer ip1
I0521 04:30:26.598773 20020 net.cpp:454] ip1 <- pool4
I0521 04:30:26.598786 20020 net.cpp:411] ip1 -> ip1
I0521 04:30:26.614202 20020 net.cpp:150] Setting up ip1
I0521 04:30:26.614230 20020 net.cpp:157] Top shape: 650 196 (127400)
I0521 04:30:26.614243 20020 net.cpp:165] Memory required for data: 1024366200
I0521 04:30:26.614265 20020 layer_factory.hpp:77] Creating layer relu5
I0521 04:30:26.614280 20020 net.cpp:106] Creating Layer relu5
I0521 04:30:26.614290 20020 net.cpp:454] relu5 <- ip1
I0521 04:30:26.614305 20020 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:30:26.614646 20020 net.cpp:150] Setting up relu5
I0521 04:30:26.614660 20020 net.cpp:157] Top shape: 650 196 (127400)
I0521 04:30:26.614670 20020 net.cpp:165] Memory required for data: 1024875800
I0521 04:30:26.614681 20020 layer_factory.hpp:77] Creating layer drop1
I0521 04:30:26.614703 20020 net.cpp:106] Creating Layer drop1
I0521 04:30:26.614713 20020 net.cpp:454] drop1 <- ip1
I0521 04:30:26.614738 20020 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:30:26.614814 20020 net.cpp:150] Setting up drop1
I0521 04:30:26.614827 20020 net.cpp:157] Top shape: 650 196 (127400)
I0521 04:30:26.614840 20020 net.cpp:165] Memory required for data: 1025385400
I0521 04:30:26.614850 20020 layer_factory.hpp:77] Creating layer ip2
I0521 04:30:26.614869 20020 net.cpp:106] Creating Layer ip2
I0521 04:30:26.614878 20020 net.cpp:454] ip2 <- ip1
I0521 04:30:26.614892 20020 net.cpp:411] ip2 -> ip2
I0521 04:30:26.615355 20020 net.cpp:150] Setting up ip2
I0521 04:30:26.615368 20020 net.cpp:157] Top shape: 650 98 (63700)
I0521 04:30:26.615378 20020 net.cpp:165] Memory required for data: 1025640200
I0521 04:30:26.615393 20020 layer_factory.hpp:77] Creating layer relu6
I0521 04:30:26.615406 20020 net.cpp:106] Creating Layer relu6
I0521 04:30:26.615417 20020 net.cpp:454] relu6 <- ip2
I0521 04:30:26.615428 20020 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:30:26.615949 20020 net.cpp:150] Setting up relu6
I0521 04:30:26.615965 20020 net.cpp:157] Top shape: 650 98 (63700)
I0521 04:30:26.615977 20020 net.cpp:165] Memory required for data: 1025895000
I0521 04:30:26.615986 20020 layer_factory.hpp:77] Creating layer drop2
I0521 04:30:26.615999 20020 net.cpp:106] Creating Layer drop2
I0521 04:30:26.616009 20020 net.cpp:454] drop2 <- ip2
I0521 04:30:26.616021 20020 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:30:26.616065 20020 net.cpp:150] Setting up drop2
I0521 04:30:26.616078 20020 net.cpp:157] Top shape: 650 98 (63700)
I0521 04:30:26.616088 20020 net.cpp:165] Memory required for data: 1026149800
I0521 04:30:26.616098 20020 layer_factory.hpp:77] Creating layer ip3
I0521 04:30:26.616111 20020 net.cpp:106] Creating Layer ip3
I0521 04:30:26.616122 20020 net.cpp:454] ip3 <- ip2
I0521 04:30:26.616133 20020 net.cpp:411] ip3 -> ip3
I0521 04:30:26.616341 20020 net.cpp:150] Setting up ip3
I0521 04:30:26.616354 20020 net.cpp:157] Top shape: 650 11 (7150)
I0521 04:30:26.616364 20020 net.cpp:165] Memory required for data: 1026178400
I0521 04:30:26.616379 20020 layer_factory.hpp:77] Creating layer drop3
I0521 04:30:26.616391 20020 net.cpp:106] Creating Layer drop3
I0521 04:30:26.616401 20020 net.cpp:454] drop3 <- ip3
I0521 04:30:26.616413 20020 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:30:26.616451 20020 net.cpp:150] Setting up drop3
I0521 04:30:26.616464 20020 net.cpp:157] Top shape: 650 11 (7150)
I0521 04:30:26.616474 20020 net.cpp:165] Memory required for data: 1026207000
I0521 04:30:26.616484 20020 layer_factory.hpp:77] Creating layer loss
I0521 04:30:26.616503 20020 net.cpp:106] Creating Layer loss
I0521 04:30:26.616513 20020 net.cpp:454] loss <- ip3
I0521 04:30:26.616524 20020 net.cpp:454] loss <- label
I0521 04:30:26.616538 20020 net.cpp:411] loss -> loss
I0521 04:30:26.616554 20020 layer_factory.hpp:77] Creating layer loss
I0521 04:30:26.617200 20020 net.cpp:150] Setting up loss
I0521 04:30:26.617221 20020 net.cpp:157] Top shape: (1)
I0521 04:30:26.617234 20020 net.cpp:160]     with loss weight 1
I0521 04:30:26.617276 20020 net.cpp:165] Memory required for data: 1026207004
I0521 04:30:26.617287 20020 net.cpp:226] loss needs backward computation.
I0521 04:30:26.617298 20020 net.cpp:226] drop3 needs backward computation.
I0521 04:30:26.617308 20020 net.cpp:226] ip3 needs backward computation.
I0521 04:30:26.617318 20020 net.cpp:226] drop2 needs backward computation.
I0521 04:30:26.617328 20020 net.cpp:226] relu6 needs backward computation.
I0521 04:30:26.617338 20020 net.cpp:226] ip2 needs backward computation.
I0521 04:30:26.617348 20020 net.cpp:226] drop1 needs backward computation.
I0521 04:30:26.617358 20020 net.cpp:226] relu5 needs backward computation.
I0521 04:30:26.617368 20020 net.cpp:226] ip1 needs backward computation.
I0521 04:30:26.617378 20020 net.cpp:226] pool4 needs backward computation.
I0521 04:30:26.617389 20020 net.cpp:226] relu4 needs backward computation.
I0521 04:30:26.617399 20020 net.cpp:226] conv4 needs backward computation.
I0521 04:30:26.617409 20020 net.cpp:226] pool3 needs backward computation.
I0521 04:30:26.617429 20020 net.cpp:226] relu3 needs backward computation.
I0521 04:30:26.617439 20020 net.cpp:226] conv3 needs backward computation.
I0521 04:30:26.617447 20020 net.cpp:226] pool2 needs backward computation.
I0521 04:30:26.617458 20020 net.cpp:226] relu2 needs backward computation.
I0521 04:30:26.617468 20020 net.cpp:226] conv2 needs backward computation.
I0521 04:30:26.617480 20020 net.cpp:226] pool1 needs backward computation.
I0521 04:30:26.617489 20020 net.cpp:226] relu1 needs backward computation.
I0521 04:30:26.617499 20020 net.cpp:226] conv1 needs backward computation.
I0521 04:30:26.617511 20020 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:30:26.617521 20020 net.cpp:270] This network produces output loss
I0521 04:30:26.617544 20020 net.cpp:283] Network initialization done.
I0521 04:30:26.619134 20020 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407.prototxt
I0521 04:30:26.619204 20020 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 04:30:26.619560 20020 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 650
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:30:26.619753 20020 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:30:26.619768 20020 net.cpp:106] Creating Layer data_hdf5
I0521 04:30:26.619781 20020 net.cpp:411] data_hdf5 -> data
I0521 04:30:26.619797 20020 net.cpp:411] data_hdf5 -> label
I0521 04:30:26.619813 20020 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 04:30:26.637751 20020 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 04:30:47.954077 20020 net.cpp:150] Setting up data_hdf5
I0521 04:30:47.954241 20020 net.cpp:157] Top shape: 650 1 127 50 (4127500)
I0521 04:30:47.954255 20020 net.cpp:157] Top shape: 650 (650)
I0521 04:30:47.954267 20020 net.cpp:165] Memory required for data: 16512600
I0521 04:30:47.954282 20020 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 04:30:47.954309 20020 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 04:30:47.954319 20020 net.cpp:454] label_data_hdf5_1_split <- label
I0521 04:30:47.954334 20020 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 04:30:47.954355 20020 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 04:30:47.954428 20020 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 04:30:47.954442 20020 net.cpp:157] Top shape: 650 (650)
I0521 04:30:47.954453 20020 net.cpp:157] Top shape: 650 (650)
I0521 04:30:47.954463 20020 net.cpp:165] Memory required for data: 16517800
I0521 04:30:47.954473 20020 layer_factory.hpp:77] Creating layer conv1
I0521 04:30:47.954495 20020 net.cpp:106] Creating Layer conv1
I0521 04:30:47.954505 20020 net.cpp:454] conv1 <- data
I0521 04:30:47.954519 20020 net.cpp:411] conv1 -> conv1
I0521 04:30:47.956461 20020 net.cpp:150] Setting up conv1
I0521 04:30:47.956485 20020 net.cpp:157] Top shape: 650 12 120 48 (44928000)
I0521 04:30:47.956496 20020 net.cpp:165] Memory required for data: 196229800
I0521 04:30:47.956517 20020 layer_factory.hpp:77] Creating layer relu1
I0521 04:30:47.956532 20020 net.cpp:106] Creating Layer relu1
I0521 04:30:47.956542 20020 net.cpp:454] relu1 <- conv1
I0521 04:30:47.956555 20020 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:30:47.957052 20020 net.cpp:150] Setting up relu1
I0521 04:30:47.957069 20020 net.cpp:157] Top shape: 650 12 120 48 (44928000)
I0521 04:30:47.957079 20020 net.cpp:165] Memory required for data: 375941800
I0521 04:30:47.957089 20020 layer_factory.hpp:77] Creating layer pool1
I0521 04:30:47.957105 20020 net.cpp:106] Creating Layer pool1
I0521 04:30:47.957115 20020 net.cpp:454] pool1 <- conv1
I0521 04:30:47.957129 20020 net.cpp:411] pool1 -> pool1
I0521 04:30:47.957203 20020 net.cpp:150] Setting up pool1
I0521 04:30:47.957217 20020 net.cpp:157] Top shape: 650 12 60 48 (22464000)
I0521 04:30:47.957227 20020 net.cpp:165] Memory required for data: 465797800
I0521 04:30:47.957237 20020 layer_factory.hpp:77] Creating layer conv2
I0521 04:30:47.957254 20020 net.cpp:106] Creating Layer conv2
I0521 04:30:47.957265 20020 net.cpp:454] conv2 <- pool1
I0521 04:30:47.957278 20020 net.cpp:411] conv2 -> conv2
I0521 04:30:47.959197 20020 net.cpp:150] Setting up conv2
I0521 04:30:47.959218 20020 net.cpp:157] Top shape: 650 20 54 46 (32292000)
I0521 04:30:47.959233 20020 net.cpp:165] Memory required for data: 594965800
I0521 04:30:47.959250 20020 layer_factory.hpp:77] Creating layer relu2
I0521 04:30:47.959265 20020 net.cpp:106] Creating Layer relu2
I0521 04:30:47.959275 20020 net.cpp:454] relu2 <- conv2
I0521 04:30:47.959287 20020 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:30:47.959624 20020 net.cpp:150] Setting up relu2
I0521 04:30:47.959636 20020 net.cpp:157] Top shape: 650 20 54 46 (32292000)
I0521 04:30:47.959647 20020 net.cpp:165] Memory required for data: 724133800
I0521 04:30:47.959657 20020 layer_factory.hpp:77] Creating layer pool2
I0521 04:30:47.959672 20020 net.cpp:106] Creating Layer pool2
I0521 04:30:47.959682 20020 net.cpp:454] pool2 <- conv2
I0521 04:30:47.959694 20020 net.cpp:411] pool2 -> pool2
I0521 04:30:47.959765 20020 net.cpp:150] Setting up pool2
I0521 04:30:47.959779 20020 net.cpp:157] Top shape: 650 20 27 46 (16146000)
I0521 04:30:47.959789 20020 net.cpp:165] Memory required for data: 788717800
I0521 04:30:47.959800 20020 layer_factory.hpp:77] Creating layer conv3
I0521 04:30:47.959817 20020 net.cpp:106] Creating Layer conv3
I0521 04:30:47.959828 20020 net.cpp:454] conv3 <- pool2
I0521 04:30:47.959842 20020 net.cpp:411] conv3 -> conv3
I0521 04:30:47.961824 20020 net.cpp:150] Setting up conv3
I0521 04:30:47.961846 20020 net.cpp:157] Top shape: 650 28 22 44 (17617600)
I0521 04:30:47.961859 20020 net.cpp:165] Memory required for data: 859188200
I0521 04:30:47.961891 20020 layer_factory.hpp:77] Creating layer relu3
I0521 04:30:47.961905 20020 net.cpp:106] Creating Layer relu3
I0521 04:30:47.961915 20020 net.cpp:454] relu3 <- conv3
I0521 04:30:47.961927 20020 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:30:47.962399 20020 net.cpp:150] Setting up relu3
I0521 04:30:47.962414 20020 net.cpp:157] Top shape: 650 28 22 44 (17617600)
I0521 04:30:47.962425 20020 net.cpp:165] Memory required for data: 929658600
I0521 04:30:47.962435 20020 layer_factory.hpp:77] Creating layer pool3
I0521 04:30:47.962448 20020 net.cpp:106] Creating Layer pool3
I0521 04:30:47.962458 20020 net.cpp:454] pool3 <- conv3
I0521 04:30:47.962471 20020 net.cpp:411] pool3 -> pool3
I0521 04:30:47.962543 20020 net.cpp:150] Setting up pool3
I0521 04:30:47.962555 20020 net.cpp:157] Top shape: 650 28 11 44 (8808800)
I0521 04:30:47.962565 20020 net.cpp:165] Memory required for data: 964893800
I0521 04:30:47.962574 20020 layer_factory.hpp:77] Creating layer conv4
I0521 04:30:47.962591 20020 net.cpp:106] Creating Layer conv4
I0521 04:30:47.962602 20020 net.cpp:454] conv4 <- pool3
I0521 04:30:47.962616 20020 net.cpp:411] conv4 -> conv4
I0521 04:30:47.964669 20020 net.cpp:150] Setting up conv4
I0521 04:30:47.964692 20020 net.cpp:157] Top shape: 650 36 6 42 (5896800)
I0521 04:30:47.964704 20020 net.cpp:165] Memory required for data: 988481000
I0521 04:30:47.964720 20020 layer_factory.hpp:77] Creating layer relu4
I0521 04:30:47.964733 20020 net.cpp:106] Creating Layer relu4
I0521 04:30:47.964743 20020 net.cpp:454] relu4 <- conv4
I0521 04:30:47.964756 20020 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:30:47.965227 20020 net.cpp:150] Setting up relu4
I0521 04:30:47.965243 20020 net.cpp:157] Top shape: 650 36 6 42 (5896800)
I0521 04:30:47.965253 20020 net.cpp:165] Memory required for data: 1012068200
I0521 04:30:47.965263 20020 layer_factory.hpp:77] Creating layer pool4
I0521 04:30:47.965276 20020 net.cpp:106] Creating Layer pool4
I0521 04:30:47.965286 20020 net.cpp:454] pool4 <- conv4
I0521 04:30:47.965299 20020 net.cpp:411] pool4 -> pool4
I0521 04:30:47.965370 20020 net.cpp:150] Setting up pool4
I0521 04:30:47.965384 20020 net.cpp:157] Top shape: 650 36 3 42 (2948400)
I0521 04:30:47.965394 20020 net.cpp:165] Memory required for data: 1023861800
I0521 04:30:47.965400 20020 layer_factory.hpp:77] Creating layer ip1
I0521 04:30:47.965416 20020 net.cpp:106] Creating Layer ip1
I0521 04:30:47.965426 20020 net.cpp:454] ip1 <- pool4
I0521 04:30:47.965440 20020 net.cpp:411] ip1 -> ip1
I0521 04:30:47.980921 20020 net.cpp:150] Setting up ip1
I0521 04:30:47.980949 20020 net.cpp:157] Top shape: 650 196 (127400)
I0521 04:30:47.980964 20020 net.cpp:165] Memory required for data: 1024371400
I0521 04:30:47.980986 20020 layer_factory.hpp:77] Creating layer relu5
I0521 04:30:47.981003 20020 net.cpp:106] Creating Layer relu5
I0521 04:30:47.981012 20020 net.cpp:454] relu5 <- ip1
I0521 04:30:47.981026 20020 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:30:47.981374 20020 net.cpp:150] Setting up relu5
I0521 04:30:47.981387 20020 net.cpp:157] Top shape: 650 196 (127400)
I0521 04:30:47.981397 20020 net.cpp:165] Memory required for data: 1024881000
I0521 04:30:47.981407 20020 layer_factory.hpp:77] Creating layer drop1
I0521 04:30:47.981426 20020 net.cpp:106] Creating Layer drop1
I0521 04:30:47.981436 20020 net.cpp:454] drop1 <- ip1
I0521 04:30:47.981449 20020 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:30:47.981494 20020 net.cpp:150] Setting up drop1
I0521 04:30:47.981508 20020 net.cpp:157] Top shape: 650 196 (127400)
I0521 04:30:47.981518 20020 net.cpp:165] Memory required for data: 1025390600
I0521 04:30:47.981528 20020 layer_factory.hpp:77] Creating layer ip2
I0521 04:30:47.981542 20020 net.cpp:106] Creating Layer ip2
I0521 04:30:47.981551 20020 net.cpp:454] ip2 <- ip1
I0521 04:30:47.981565 20020 net.cpp:411] ip2 -> ip2
I0521 04:30:47.982053 20020 net.cpp:150] Setting up ip2
I0521 04:30:47.982066 20020 net.cpp:157] Top shape: 650 98 (63700)
I0521 04:30:47.982076 20020 net.cpp:165] Memory required for data: 1025645400
I0521 04:30:47.982105 20020 layer_factory.hpp:77] Creating layer relu6
I0521 04:30:47.982118 20020 net.cpp:106] Creating Layer relu6
I0521 04:30:47.982127 20020 net.cpp:454] relu6 <- ip2
I0521 04:30:47.982141 20020 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:30:47.982673 20020 net.cpp:150] Setting up relu6
I0521 04:30:47.982695 20020 net.cpp:157] Top shape: 650 98 (63700)
I0521 04:30:47.982704 20020 net.cpp:165] Memory required for data: 1025900200
I0521 04:30:47.982715 20020 layer_factory.hpp:77] Creating layer drop2
I0521 04:30:47.982729 20020 net.cpp:106] Creating Layer drop2
I0521 04:30:47.982739 20020 net.cpp:454] drop2 <- ip2
I0521 04:30:47.982753 20020 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:30:47.982796 20020 net.cpp:150] Setting up drop2
I0521 04:30:47.982810 20020 net.cpp:157] Top shape: 650 98 (63700)
I0521 04:30:47.982820 20020 net.cpp:165] Memory required for data: 1026155000
I0521 04:30:47.982830 20020 layer_factory.hpp:77] Creating layer ip3
I0521 04:30:47.982843 20020 net.cpp:106] Creating Layer ip3
I0521 04:30:47.982853 20020 net.cpp:454] ip3 <- ip2
I0521 04:30:47.982867 20020 net.cpp:411] ip3 -> ip3
I0521 04:30:47.983090 20020 net.cpp:150] Setting up ip3
I0521 04:30:47.983103 20020 net.cpp:157] Top shape: 650 11 (7150)
I0521 04:30:47.983114 20020 net.cpp:165] Memory required for data: 1026183600
I0521 04:30:47.983129 20020 layer_factory.hpp:77] Creating layer drop3
I0521 04:30:47.983141 20020 net.cpp:106] Creating Layer drop3
I0521 04:30:47.983151 20020 net.cpp:454] drop3 <- ip3
I0521 04:30:47.983165 20020 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:30:47.983206 20020 net.cpp:150] Setting up drop3
I0521 04:30:47.983218 20020 net.cpp:157] Top shape: 650 11 (7150)
I0521 04:30:47.983227 20020 net.cpp:165] Memory required for data: 1026212200
I0521 04:30:47.983244 20020 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 04:30:47.983258 20020 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 04:30:47.983268 20020 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 04:30:47.983280 20020 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 04:30:47.983295 20020 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 04:30:47.983369 20020 net.cpp:150] Setting up ip3_drop3_0_split
I0521 04:30:47.983382 20020 net.cpp:157] Top shape: 650 11 (7150)
I0521 04:30:47.983394 20020 net.cpp:157] Top shape: 650 11 (7150)
I0521 04:30:47.983404 20020 net.cpp:165] Memory required for data: 1026269400
I0521 04:30:47.983414 20020 layer_factory.hpp:77] Creating layer accuracy
I0521 04:30:47.983436 20020 net.cpp:106] Creating Layer accuracy
I0521 04:30:47.983446 20020 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 04:30:47.983458 20020 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 04:30:47.983471 20020 net.cpp:411] accuracy -> accuracy
I0521 04:30:47.983494 20020 net.cpp:150] Setting up accuracy
I0521 04:30:47.983507 20020 net.cpp:157] Top shape: (1)
I0521 04:30:47.983517 20020 net.cpp:165] Memory required for data: 1026269404
I0521 04:30:47.983527 20020 layer_factory.hpp:77] Creating layer loss
I0521 04:30:47.983541 20020 net.cpp:106] Creating Layer loss
I0521 04:30:47.983551 20020 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 04:30:47.983562 20020 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 04:30:47.983575 20020 net.cpp:411] loss -> loss
I0521 04:30:47.983592 20020 layer_factory.hpp:77] Creating layer loss
I0521 04:30:47.984084 20020 net.cpp:150] Setting up loss
I0521 04:30:47.984098 20020 net.cpp:157] Top shape: (1)
I0521 04:30:47.984107 20020 net.cpp:160]     with loss weight 1
I0521 04:30:47.984125 20020 net.cpp:165] Memory required for data: 1026269408
I0521 04:30:47.984136 20020 net.cpp:226] loss needs backward computation.
I0521 04:30:47.984148 20020 net.cpp:228] accuracy does not need backward computation.
I0521 04:30:47.984158 20020 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 04:30:47.984169 20020 net.cpp:226] drop3 needs backward computation.
I0521 04:30:47.984179 20020 net.cpp:226] ip3 needs backward computation.
I0521 04:30:47.984189 20020 net.cpp:226] drop2 needs backward computation.
I0521 04:30:47.984208 20020 net.cpp:226] relu6 needs backward computation.
I0521 04:30:47.984217 20020 net.cpp:226] ip2 needs backward computation.
I0521 04:30:47.984228 20020 net.cpp:226] drop1 needs backward computation.
I0521 04:30:47.984237 20020 net.cpp:226] relu5 needs backward computation.
I0521 04:30:47.984247 20020 net.cpp:226] ip1 needs backward computation.
I0521 04:30:47.984256 20020 net.cpp:226] pool4 needs backward computation.
I0521 04:30:47.984267 20020 net.cpp:226] relu4 needs backward computation.
I0521 04:30:47.984277 20020 net.cpp:226] conv4 needs backward computation.
I0521 04:30:47.984287 20020 net.cpp:226] pool3 needs backward computation.
I0521 04:30:47.984297 20020 net.cpp:226] relu3 needs backward computation.
I0521 04:30:47.984308 20020 net.cpp:226] conv3 needs backward computation.
I0521 04:30:47.984318 20020 net.cpp:226] pool2 needs backward computation.
I0521 04:30:47.984329 20020 net.cpp:226] relu2 needs backward computation.
I0521 04:30:47.984339 20020 net.cpp:226] conv2 needs backward computation.
I0521 04:30:47.984349 20020 net.cpp:226] pool1 needs backward computation.
I0521 04:30:47.984360 20020 net.cpp:226] relu1 needs backward computation.
I0521 04:30:47.984369 20020 net.cpp:226] conv1 needs backward computation.
I0521 04:30:47.984381 20020 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 04:30:47.984392 20020 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:30:47.984402 20020 net.cpp:270] This network produces output accuracy
I0521 04:30:47.984412 20020 net.cpp:270] This network produces output loss
I0521 04:30:47.984439 20020 net.cpp:283] Network initialization done.
I0521 04:30:47.984572 20020 solver.cpp:60] Solver scaffolding done.
I0521 04:30:47.985699 20020 caffe.cpp:212] Starting Optimization
I0521 04:30:47.985718 20020 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 04:30:47.985743 20020 solver.cpp:289] Learning Rate Policy: fixed
I0521 04:30:47.986976 20020 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 04:31:33.899190 20020 solver.cpp:409]     Test net output #0: accuracy = 0.123853
I0521 04:31:33.899348 20020 solver.cpp:409]     Test net output #1: loss = 2.39574 (* 1 = 2.39574 loss)
I0521 04:31:34.022581 20020 solver.cpp:237] Iteration 0, loss = 2.39792
I0521 04:31:34.022619 20020 solver.cpp:253]     Train net output #0: loss = 2.39792 (* 1 = 2.39792 loss)
I0521 04:31:34.022637 20020 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 04:31:42.047580 20020 solver.cpp:237] Iteration 23, loss = 2.3813
I0521 04:31:42.047615 20020 solver.cpp:253]     Train net output #0: loss = 2.3813 (* 1 = 2.3813 loss)
I0521 04:31:42.047628 20020 sgd_solver.cpp:106] Iteration 23, lr = 0.0025
I0521 04:31:50.079561 20020 solver.cpp:237] Iteration 46, loss = 2.35942
I0521 04:31:50.079602 20020 solver.cpp:253]     Train net output #0: loss = 2.35942 (* 1 = 2.35942 loss)
I0521 04:31:50.079617 20020 sgd_solver.cpp:106] Iteration 46, lr = 0.0025
I0521 04:31:58.106933 20020 solver.cpp:237] Iteration 69, loss = 2.34268
I0521 04:31:58.106966 20020 solver.cpp:253]     Train net output #0: loss = 2.34268 (* 1 = 2.34268 loss)
I0521 04:31:58.106979 20020 sgd_solver.cpp:106] Iteration 69, lr = 0.0025
I0521 04:32:06.129683 20020 solver.cpp:237] Iteration 92, loss = 2.34594
I0521 04:32:06.129832 20020 solver.cpp:253]     Train net output #0: loss = 2.34594 (* 1 = 2.34594 loss)
I0521 04:32:06.129844 20020 sgd_solver.cpp:106] Iteration 92, lr = 0.0025
I0521 04:32:14.156198 20020 solver.cpp:237] Iteration 115, loss = 2.32573
I0521 04:32:14.156239 20020 solver.cpp:253]     Train net output #0: loss = 2.32573 (* 1 = 2.32573 loss)
I0521 04:32:14.156257 20020 sgd_solver.cpp:106] Iteration 115, lr = 0.0025
I0521 04:32:22.183605 20020 solver.cpp:237] Iteration 138, loss = 2.33295
I0521 04:32:22.183636 20020 solver.cpp:253]     Train net output #0: loss = 2.33295 (* 1 = 2.33295 loss)
I0521 04:32:22.183651 20020 sgd_solver.cpp:106] Iteration 138, lr = 0.0025
I0521 04:32:52.344388 20020 solver.cpp:237] Iteration 161, loss = 2.33737
I0521 04:32:52.344561 20020 solver.cpp:253]     Train net output #0: loss = 2.33737 (* 1 = 2.33737 loss)
I0521 04:32:52.344578 20020 sgd_solver.cpp:106] Iteration 161, lr = 0.0025
I0521 04:33:00.370234 20020 solver.cpp:237] Iteration 184, loss = 2.30302
I0521 04:33:00.370266 20020 solver.cpp:253]     Train net output #0: loss = 2.30302 (* 1 = 2.30302 loss)
I0521 04:33:00.370282 20020 sgd_solver.cpp:106] Iteration 184, lr = 0.0025
I0521 04:33:08.396176 20020 solver.cpp:237] Iteration 207, loss = 2.303
I0521 04:33:08.396209 20020 solver.cpp:253]     Train net output #0: loss = 2.303 (* 1 = 2.303 loss)
I0521 04:33:08.396222 20020 sgd_solver.cpp:106] Iteration 207, lr = 0.0025
I0521 04:33:16.075763 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_230.caffemodel
I0521 04:33:16.363497 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_230.solverstate
I0521 04:33:16.496237 20020 solver.cpp:237] Iteration 230, loss = 2.29489
I0521 04:33:16.496280 20020 solver.cpp:253]     Train net output #0: loss = 2.29489 (* 1 = 2.29489 loss)
I0521 04:33:16.496300 20020 sgd_solver.cpp:106] Iteration 230, lr = 0.0025
I0521 04:33:24.523613 20020 solver.cpp:237] Iteration 253, loss = 2.28648
I0521 04:33:24.523751 20020 solver.cpp:253]     Train net output #0: loss = 2.28648 (* 1 = 2.28648 loss)
I0521 04:33:24.523764 20020 sgd_solver.cpp:106] Iteration 253, lr = 0.0025
I0521 04:33:32.554322 20020 solver.cpp:237] Iteration 276, loss = 2.25418
I0521 04:33:32.554352 20020 solver.cpp:253]     Train net output #0: loss = 2.25418 (* 1 = 2.25418 loss)
I0521 04:33:32.554364 20020 sgd_solver.cpp:106] Iteration 276, lr = 0.0025
I0521 04:33:40.582444 20020 solver.cpp:237] Iteration 299, loss = 2.24489
I0521 04:33:40.582476 20020 solver.cpp:253]     Train net output #0: loss = 2.24489 (* 1 = 2.24489 loss)
I0521 04:33:40.582491 20020 sgd_solver.cpp:106] Iteration 299, lr = 0.0025
I0521 04:34:10.775360 20020 solver.cpp:237] Iteration 322, loss = 2.21584
I0521 04:34:10.775516 20020 solver.cpp:253]     Train net output #0: loss = 2.21584 (* 1 = 2.21584 loss)
I0521 04:34:10.775530 20020 sgd_solver.cpp:106] Iteration 322, lr = 0.0025
I0521 04:34:18.804337 20020 solver.cpp:237] Iteration 345, loss = 2.19719
I0521 04:34:18.804383 20020 solver.cpp:253]     Train net output #0: loss = 2.19719 (* 1 = 2.19719 loss)
I0521 04:34:18.804397 20020 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 04:34:26.830273 20020 solver.cpp:237] Iteration 368, loss = 2.15497
I0521 04:34:26.830307 20020 solver.cpp:253]     Train net output #0: loss = 2.15497 (* 1 = 2.15497 loss)
I0521 04:34:26.830322 20020 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0521 04:34:34.860797 20020 solver.cpp:237] Iteration 391, loss = 2.1589
I0521 04:34:34.860831 20020 solver.cpp:253]     Train net output #0: loss = 2.1589 (* 1 = 2.1589 loss)
I0521 04:34:34.860846 20020 sgd_solver.cpp:106] Iteration 391, lr = 0.0025
I0521 04:34:42.884907 20020 solver.cpp:237] Iteration 414, loss = 2.03511
I0521 04:34:42.885068 20020 solver.cpp:253]     Train net output #0: loss = 2.03511 (* 1 = 2.03511 loss)
I0521 04:34:42.885082 20020 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0521 04:34:50.910681 20020 solver.cpp:237] Iteration 437, loss = 2.03698
I0521 04:34:50.910713 20020 solver.cpp:253]     Train net output #0: loss = 2.03698 (* 1 = 2.03698 loss)
I0521 04:34:50.910728 20020 sgd_solver.cpp:106] Iteration 437, lr = 0.0025
I0521 04:34:58.592833 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_460.caffemodel
I0521 04:34:58.878193 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_460.solverstate
I0521 04:34:59.009083 20020 solver.cpp:237] Iteration 460, loss = 2.05775
I0521 04:34:59.009131 20020 solver.cpp:253]     Train net output #0: loss = 2.05775 (* 1 = 2.05775 loss)
I0521 04:34:59.009147 20020 sgd_solver.cpp:106] Iteration 460, lr = 0.0025
I0521 04:34:59.009654 20020 solver.cpp:341] Iteration 461, Testing net (#0)
I0521 04:35:44.255218 20020 solver.cpp:409]     Test net output #0: accuracy = 0.50412
I0521 04:35:44.255376 20020 solver.cpp:409]     Test net output #1: loss = 1.80398 (* 1 = 1.80398 loss)
I0521 04:36:14.247186 20020 solver.cpp:237] Iteration 483, loss = 1.99421
I0521 04:36:14.247234 20020 solver.cpp:253]     Train net output #0: loss = 1.99421 (* 1 = 1.99421 loss)
I0521 04:36:14.247249 20020 sgd_solver.cpp:106] Iteration 483, lr = 0.0025
I0521 04:36:22.277904 20020 solver.cpp:237] Iteration 506, loss = 1.9729
I0521 04:36:22.278049 20020 solver.cpp:253]     Train net output #0: loss = 1.9729 (* 1 = 1.9729 loss)
I0521 04:36:22.278062 20020 sgd_solver.cpp:106] Iteration 506, lr = 0.0025
I0521 04:36:30.300534 20020 solver.cpp:237] Iteration 529, loss = 2.00676
I0521 04:36:30.300570 20020 solver.cpp:253]     Train net output #0: loss = 2.00676 (* 1 = 2.00676 loss)
I0521 04:36:30.300588 20020 sgd_solver.cpp:106] Iteration 529, lr = 0.0025
I0521 04:36:38.323767 20020 solver.cpp:237] Iteration 552, loss = 1.95296
I0521 04:36:38.323794 20020 solver.cpp:253]     Train net output #0: loss = 1.95296 (* 1 = 1.95296 loss)
I0521 04:36:38.323807 20020 sgd_solver.cpp:106] Iteration 552, lr = 0.0025
I0521 04:36:46.352092 20020 solver.cpp:237] Iteration 575, loss = 1.97245
I0521 04:36:46.352125 20020 solver.cpp:253]     Train net output #0: loss = 1.97245 (* 1 = 1.97245 loss)
I0521 04:36:46.352140 20020 sgd_solver.cpp:106] Iteration 575, lr = 0.0025
I0521 04:36:54.374464 20020 solver.cpp:237] Iteration 598, loss = 1.87839
I0521 04:36:54.374608 20020 solver.cpp:253]     Train net output #0: loss = 1.87839 (* 1 = 1.87839 loss)
I0521 04:36:54.374622 20020 sgd_solver.cpp:106] Iteration 598, lr = 0.0025
I0521 04:37:24.612716 20020 solver.cpp:237] Iteration 621, loss = 1.88595
I0521 04:37:24.612879 20020 solver.cpp:253]     Train net output #0: loss = 1.88595 (* 1 = 1.88595 loss)
I0521 04:37:24.612895 20020 sgd_solver.cpp:106] Iteration 621, lr = 0.0025
I0521 04:37:32.638986 20020 solver.cpp:237] Iteration 644, loss = 1.96591
I0521 04:37:32.639019 20020 solver.cpp:253]     Train net output #0: loss = 1.96591 (* 1 = 1.96591 loss)
I0521 04:37:32.639034 20020 sgd_solver.cpp:106] Iteration 644, lr = 0.0025
I0521 04:37:40.666733 20020 solver.cpp:237] Iteration 667, loss = 1.95756
I0521 04:37:40.666772 20020 solver.cpp:253]     Train net output #0: loss = 1.95756 (* 1 = 1.95756 loss)
I0521 04:37:40.666792 20020 sgd_solver.cpp:106] Iteration 667, lr = 0.0025
I0521 04:37:48.340334 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_690.caffemodel
I0521 04:37:48.625290 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_690.solverstate
I0521 04:37:48.756861 20020 solver.cpp:237] Iteration 690, loss = 1.90844
I0521 04:37:48.756909 20020 solver.cpp:253]     Train net output #0: loss = 1.90844 (* 1 = 1.90844 loss)
I0521 04:37:48.756923 20020 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 04:37:56.786478 20020 solver.cpp:237] Iteration 713, loss = 1.8641
I0521 04:37:56.786630 20020 solver.cpp:253]     Train net output #0: loss = 1.8641 (* 1 = 1.8641 loss)
I0521 04:37:56.786643 20020 sgd_solver.cpp:106] Iteration 713, lr = 0.0025
I0521 04:38:04.807238 20020 solver.cpp:237] Iteration 736, loss = 1.86432
I0521 04:38:04.807286 20020 solver.cpp:253]     Train net output #0: loss = 1.86432 (* 1 = 1.86432 loss)
I0521 04:38:04.807301 20020 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 04:38:12.836554 20020 solver.cpp:237] Iteration 759, loss = 1.87814
I0521 04:38:12.836586 20020 solver.cpp:253]     Train net output #0: loss = 1.87814 (* 1 = 1.87814 loss)
I0521 04:38:12.836601 20020 sgd_solver.cpp:106] Iteration 759, lr = 0.0025
I0521 04:38:43.045702 20020 solver.cpp:237] Iteration 782, loss = 1.84275
I0521 04:38:43.045868 20020 solver.cpp:253]     Train net output #0: loss = 1.84275 (* 1 = 1.84275 loss)
I0521 04:38:43.045883 20020 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0521 04:38:51.073688 20020 solver.cpp:237] Iteration 805, loss = 1.81961
I0521 04:38:51.073719 20020 solver.cpp:253]     Train net output #0: loss = 1.81961 (* 1 = 1.81961 loss)
I0521 04:38:51.073741 20020 sgd_solver.cpp:106] Iteration 805, lr = 0.0025
I0521 04:38:59.103286 20020 solver.cpp:237] Iteration 828, loss = 1.78524
I0521 04:38:59.103324 20020 solver.cpp:253]     Train net output #0: loss = 1.78524 (* 1 = 1.78524 loss)
I0521 04:38:59.103344 20020 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0521 04:39:07.124426 20020 solver.cpp:237] Iteration 851, loss = 1.83651
I0521 04:39:07.124459 20020 solver.cpp:253]     Train net output #0: loss = 1.83651 (* 1 = 1.83651 loss)
I0521 04:39:07.124475 20020 sgd_solver.cpp:106] Iteration 851, lr = 0.0025
I0521 04:39:15.147570 20020 solver.cpp:237] Iteration 874, loss = 1.86372
I0521 04:39:15.147707 20020 solver.cpp:253]     Train net output #0: loss = 1.86372 (* 1 = 1.86372 loss)
I0521 04:39:15.147721 20020 sgd_solver.cpp:106] Iteration 874, lr = 0.0025
I0521 04:39:23.176087 20020 solver.cpp:237] Iteration 897, loss = 1.81429
I0521 04:39:23.176126 20020 solver.cpp:253]     Train net output #0: loss = 1.81429 (* 1 = 1.81429 loss)
I0521 04:39:23.176146 20020 sgd_solver.cpp:106] Iteration 897, lr = 0.0025
I0521 04:39:30.852529 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_920.caffemodel
I0521 04:39:31.146174 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_920.solverstate
I0521 04:39:31.277494 20020 solver.cpp:237] Iteration 920, loss = 1.80365
I0521 04:39:31.277544 20020 solver.cpp:253]     Train net output #0: loss = 1.80365 (* 1 = 1.80365 loss)
I0521 04:39:31.277557 20020 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0521 04:39:31.627167 20020 solver.cpp:341] Iteration 922, Testing net (#0)
I0521 04:40:37.770295 20020 solver.cpp:409]     Test net output #0: accuracy = 0.616997
I0521 04:40:37.770463 20020 solver.cpp:409]     Test net output #1: loss = 1.36625 (* 1 = 1.36625 loss)
I0521 04:41:07.385540 20020 solver.cpp:237] Iteration 943, loss = 1.79282
I0521 04:41:07.385589 20020 solver.cpp:253]     Train net output #0: loss = 1.79282 (* 1 = 1.79282 loss)
I0521 04:41:07.385604 20020 sgd_solver.cpp:106] Iteration 943, lr = 0.0025
I0521 04:41:15.404846 20020 solver.cpp:237] Iteration 966, loss = 1.82194
I0521 04:41:15.404989 20020 solver.cpp:253]     Train net output #0: loss = 1.82194 (* 1 = 1.82194 loss)
I0521 04:41:15.405001 20020 sgd_solver.cpp:106] Iteration 966, lr = 0.0025
I0521 04:41:23.420831 20020 solver.cpp:237] Iteration 989, loss = 1.80768
I0521 04:41:23.420866 20020 solver.cpp:253]     Train net output #0: loss = 1.80768 (* 1 = 1.80768 loss)
I0521 04:41:23.420882 20020 sgd_solver.cpp:106] Iteration 989, lr = 0.0025
I0521 04:41:31.437237 20020 solver.cpp:237] Iteration 1012, loss = 1.73121
I0521 04:41:31.437268 20020 solver.cpp:253]     Train net output #0: loss = 1.73121 (* 1 = 1.73121 loss)
I0521 04:41:31.437283 20020 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0521 04:41:39.459602 20020 solver.cpp:237] Iteration 1035, loss = 1.8185
I0521 04:41:39.459635 20020 solver.cpp:253]     Train net output #0: loss = 1.8185 (* 1 = 1.8185 loss)
I0521 04:41:39.459650 20020 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 04:41:47.476233 20020 solver.cpp:237] Iteration 1058, loss = 1.75364
I0521 04:41:47.476364 20020 solver.cpp:253]     Train net output #0: loss = 1.75364 (* 1 = 1.75364 loss)
I0521 04:41:47.476377 20020 sgd_solver.cpp:106] Iteration 1058, lr = 0.0025
I0521 04:42:17.645419 20020 solver.cpp:237] Iteration 1081, loss = 1.8618
I0521 04:42:17.645581 20020 solver.cpp:253]     Train net output #0: loss = 1.8618 (* 1 = 1.8618 loss)
I0521 04:42:17.645596 20020 sgd_solver.cpp:106] Iteration 1081, lr = 0.0025
I0521 04:42:25.663209 20020 solver.cpp:237] Iteration 1104, loss = 1.76346
I0521 04:42:25.663242 20020 solver.cpp:253]     Train net output #0: loss = 1.76346 (* 1 = 1.76346 loss)
I0521 04:42:25.663259 20020 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 04:42:33.682148 20020 solver.cpp:237] Iteration 1127, loss = 1.78296
I0521 04:42:33.682175 20020 solver.cpp:253]     Train net output #0: loss = 1.78296 (* 1 = 1.78296 loss)
I0521 04:42:33.682190 20020 sgd_solver.cpp:106] Iteration 1127, lr = 0.0025
I0521 04:42:41.351976 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1150.caffemodel
I0521 04:42:41.635706 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1150.solverstate
I0521 04:42:41.768478 20020 solver.cpp:237] Iteration 1150, loss = 1.82012
I0521 04:42:41.768524 20020 solver.cpp:253]     Train net output #0: loss = 1.82012 (* 1 = 1.82012 loss)
I0521 04:42:41.768540 20020 sgd_solver.cpp:106] Iteration 1150, lr = 0.0025
I0521 04:42:49.786130 20020 solver.cpp:237] Iteration 1173, loss = 1.69891
I0521 04:42:49.786273 20020 solver.cpp:253]     Train net output #0: loss = 1.69891 (* 1 = 1.69891 loss)
I0521 04:42:49.786285 20020 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0521 04:42:57.803201 20020 solver.cpp:237] Iteration 1196, loss = 1.79289
I0521 04:42:57.803233 20020 solver.cpp:253]     Train net output #0: loss = 1.79289 (* 1 = 1.79289 loss)
I0521 04:42:57.803248 20020 sgd_solver.cpp:106] Iteration 1196, lr = 0.0025
I0521 04:43:05.821228 20020 solver.cpp:237] Iteration 1219, loss = 1.81706
I0521 04:43:05.821272 20020 solver.cpp:253]     Train net output #0: loss = 1.81706 (* 1 = 1.81706 loss)
I0521 04:43:05.821286 20020 sgd_solver.cpp:106] Iteration 1219, lr = 0.0025
I0521 04:43:36.013017 20020 solver.cpp:237] Iteration 1242, loss = 1.74999
I0521 04:43:36.013191 20020 solver.cpp:253]     Train net output #0: loss = 1.74999 (* 1 = 1.74999 loss)
I0521 04:43:36.013206 20020 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 04:43:44.031186 20020 solver.cpp:237] Iteration 1265, loss = 1.75259
I0521 04:43:44.031219 20020 solver.cpp:253]     Train net output #0: loss = 1.75259 (* 1 = 1.75259 loss)
I0521 04:43:44.031234 20020 sgd_solver.cpp:106] Iteration 1265, lr = 0.0025
I0521 04:43:52.049417 20020 solver.cpp:237] Iteration 1288, loss = 1.82629
I0521 04:43:52.049448 20020 solver.cpp:253]     Train net output #0: loss = 1.82629 (* 1 = 1.82629 loss)
I0521 04:43:52.049463 20020 sgd_solver.cpp:106] Iteration 1288, lr = 0.0025
I0521 04:44:00.069715 20020 solver.cpp:237] Iteration 1311, loss = 1.74938
I0521 04:44:00.069756 20020 solver.cpp:253]     Train net output #0: loss = 1.74938 (* 1 = 1.74938 loss)
I0521 04:44:00.069775 20020 sgd_solver.cpp:106] Iteration 1311, lr = 0.0025
I0521 04:44:08.084731 20020 solver.cpp:237] Iteration 1334, loss = 1.74509
I0521 04:44:08.084869 20020 solver.cpp:253]     Train net output #0: loss = 1.74509 (* 1 = 1.74509 loss)
I0521 04:44:08.084883 20020 sgd_solver.cpp:106] Iteration 1334, lr = 0.0025
I0521 04:44:16.097054 20020 solver.cpp:237] Iteration 1357, loss = 1.65615
I0521 04:44:16.097085 20020 solver.cpp:253]     Train net output #0: loss = 1.65615 (* 1 = 1.65615 loss)
I0521 04:44:16.097100 20020 sgd_solver.cpp:106] Iteration 1357, lr = 0.0025
I0521 04:44:23.763201 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1380.caffemodel
I0521 04:44:24.045217 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1380.solverstate
I0521 04:44:24.175410 20020 solver.cpp:237] Iteration 1380, loss = 1.67003
I0521 04:44:24.175451 20020 solver.cpp:253]     Train net output #0: loss = 1.67003 (* 1 = 1.67003 loss)
I0521 04:44:24.175470 20020 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 04:44:24.872984 20020 solver.cpp:341] Iteration 1383, Testing net (#0)
I0521 04:45:09.789269 20020 solver.cpp:409]     Test net output #0: accuracy = 0.654321
I0521 04:45:09.789425 20020 solver.cpp:409]     Test net output #1: loss = 1.21705 (* 1 = 1.21705 loss)
I0521 04:45:39.056731 20020 solver.cpp:237] Iteration 1403, loss = 1.76825
I0521 04:45:39.056779 20020 solver.cpp:253]     Train net output #0: loss = 1.76825 (* 1 = 1.76825 loss)
I0521 04:45:39.056794 20020 sgd_solver.cpp:106] Iteration 1403, lr = 0.0025
I0521 04:45:47.070715 20020 solver.cpp:237] Iteration 1426, loss = 1.6921
I0521 04:45:47.070864 20020 solver.cpp:253]     Train net output #0: loss = 1.6921 (* 1 = 1.6921 loss)
I0521 04:45:47.070878 20020 sgd_solver.cpp:106] Iteration 1426, lr = 0.0025
I0521 04:45:55.085192 20020 solver.cpp:237] Iteration 1449, loss = 1.69861
I0521 04:45:55.085224 20020 solver.cpp:253]     Train net output #0: loss = 1.69861 (* 1 = 1.69861 loss)
I0521 04:45:55.085239 20020 sgd_solver.cpp:106] Iteration 1449, lr = 0.0025
I0521 04:46:03.100306 20020 solver.cpp:237] Iteration 1472, loss = 1.67615
I0521 04:46:03.100339 20020 solver.cpp:253]     Train net output #0: loss = 1.67615 (* 1 = 1.67615 loss)
I0521 04:46:03.100356 20020 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 04:46:11.119401 20020 solver.cpp:237] Iteration 1495, loss = 1.73791
I0521 04:46:11.119433 20020 solver.cpp:253]     Train net output #0: loss = 1.73791 (* 1 = 1.73791 loss)
I0521 04:46:11.119448 20020 sgd_solver.cpp:106] Iteration 1495, lr = 0.0025
I0521 04:46:19.131561 20020 solver.cpp:237] Iteration 1518, loss = 1.77608
I0521 04:46:19.131693 20020 solver.cpp:253]     Train net output #0: loss = 1.77608 (* 1 = 1.77608 loss)
I0521 04:46:19.131706 20020 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0521 04:46:49.393458 20020 solver.cpp:237] Iteration 1541, loss = 1.78383
I0521 04:46:49.393630 20020 solver.cpp:253]     Train net output #0: loss = 1.78383 (* 1 = 1.78383 loss)
I0521 04:46:49.393646 20020 sgd_solver.cpp:106] Iteration 1541, lr = 0.0025
I0521 04:46:57.410902 20020 solver.cpp:237] Iteration 1564, loss = 1.6251
I0521 04:46:57.410934 20020 solver.cpp:253]     Train net output #0: loss = 1.6251 (* 1 = 1.6251 loss)
I0521 04:46:57.410949 20020 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0521 04:47:05.427675 20020 solver.cpp:237] Iteration 1587, loss = 1.7031
I0521 04:47:05.427708 20020 solver.cpp:253]     Train net output #0: loss = 1.7031 (* 1 = 1.7031 loss)
I0521 04:47:05.427723 20020 sgd_solver.cpp:106] Iteration 1587, lr = 0.0025
I0521 04:47:13.097900 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1610.caffemodel
I0521 04:47:13.380323 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1610.solverstate
I0521 04:47:13.510511 20020 solver.cpp:237] Iteration 1610, loss = 1.84279
I0521 04:47:13.510557 20020 solver.cpp:253]     Train net output #0: loss = 1.84279 (* 1 = 1.84279 loss)
I0521 04:47:13.510573 20020 sgd_solver.cpp:106] Iteration 1610, lr = 0.0025
I0521 04:47:21.525302 20020 solver.cpp:237] Iteration 1633, loss = 1.77735
I0521 04:47:21.525444 20020 solver.cpp:253]     Train net output #0: loss = 1.77735 (* 1 = 1.77735 loss)
I0521 04:47:21.525457 20020 sgd_solver.cpp:106] Iteration 1633, lr = 0.0025
I0521 04:47:29.546176 20020 solver.cpp:237] Iteration 1656, loss = 1.65161
I0521 04:47:29.546210 20020 solver.cpp:253]     Train net output #0: loss = 1.65161 (* 1 = 1.65161 loss)
I0521 04:47:29.546224 20020 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 04:47:37.563372 20020 solver.cpp:237] Iteration 1679, loss = 1.7222
I0521 04:47:37.563416 20020 solver.cpp:253]     Train net output #0: loss = 1.7222 (* 1 = 1.7222 loss)
I0521 04:47:37.563432 20020 sgd_solver.cpp:106] Iteration 1679, lr = 0.0025
I0521 04:48:07.718720 20020 solver.cpp:237] Iteration 1702, loss = 1.88299
I0521 04:48:07.718883 20020 solver.cpp:253]     Train net output #0: loss = 1.88299 (* 1 = 1.88299 loss)
I0521 04:48:07.718899 20020 sgd_solver.cpp:106] Iteration 1702, lr = 0.0025
I0521 04:48:15.739465 20020 solver.cpp:237] Iteration 1725, loss = 1.66019
I0521 04:48:15.739497 20020 solver.cpp:253]     Train net output #0: loss = 1.66019 (* 1 = 1.66019 loss)
I0521 04:48:15.739512 20020 sgd_solver.cpp:106] Iteration 1725, lr = 0.0025
I0521 04:48:23.753015 20020 solver.cpp:237] Iteration 1748, loss = 1.71416
I0521 04:48:23.753048 20020 solver.cpp:253]     Train net output #0: loss = 1.71416 (* 1 = 1.71416 loss)
I0521 04:48:23.753062 20020 sgd_solver.cpp:106] Iteration 1748, lr = 0.0025
I0521 04:48:31.764966 20020 solver.cpp:237] Iteration 1771, loss = 1.63275
I0521 04:48:31.764997 20020 solver.cpp:253]     Train net output #0: loss = 1.63275 (* 1 = 1.63275 loss)
I0521 04:48:31.765009 20020 sgd_solver.cpp:106] Iteration 1771, lr = 0.0025
I0521 04:48:39.782856 20020 solver.cpp:237] Iteration 1794, loss = 1.55319
I0521 04:48:39.782991 20020 solver.cpp:253]     Train net output #0: loss = 1.55319 (* 1 = 1.55319 loss)
I0521 04:48:39.783005 20020 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0521 04:48:47.801522 20020 solver.cpp:237] Iteration 1817, loss = 1.71725
I0521 04:48:47.801553 20020 solver.cpp:253]     Train net output #0: loss = 1.71725 (* 1 = 1.71725 loss)
I0521 04:48:47.801568 20020 sgd_solver.cpp:106] Iteration 1817, lr = 0.0025
I0521 04:48:55.472337 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1840.caffemodel
I0521 04:48:55.754868 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_1840.solverstate
I0521 04:48:55.884897 20020 solver.cpp:237] Iteration 1840, loss = 1.66478
I0521 04:48:55.884943 20020 solver.cpp:253]     Train net output #0: loss = 1.66478 (* 1 = 1.66478 loss)
I0521 04:48:55.884959 20020 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0521 04:48:56.931605 20020 solver.cpp:341] Iteration 1844, Testing net (#0)
I0521 04:50:02.977144 20020 solver.cpp:409]     Test net output #0: accuracy = 0.672957
I0521 04:50:02.977314 20020 solver.cpp:409]     Test net output #1: loss = 1.13259 (* 1 = 1.13259 loss)
I0521 04:50:31.863173 20020 solver.cpp:237] Iteration 1863, loss = 1.74222
I0521 04:50:31.863222 20020 solver.cpp:253]     Train net output #0: loss = 1.74222 (* 1 = 1.74222 loss)
I0521 04:50:31.863237 20020 sgd_solver.cpp:106] Iteration 1863, lr = 0.0025
I0521 04:50:39.884228 20020 solver.cpp:237] Iteration 1886, loss = 1.56607
I0521 04:50:39.884383 20020 solver.cpp:253]     Train net output #0: loss = 1.56607 (* 1 = 1.56607 loss)
I0521 04:50:39.884397 20020 sgd_solver.cpp:106] Iteration 1886, lr = 0.0025
I0521 04:50:47.906728 20020 solver.cpp:237] Iteration 1909, loss = 1.70188
I0521 04:50:47.906761 20020 solver.cpp:253]     Train net output #0: loss = 1.70188 (* 1 = 1.70188 loss)
I0521 04:50:47.906775 20020 sgd_solver.cpp:106] Iteration 1909, lr = 0.0025
I0521 04:50:55.931236 20020 solver.cpp:237] Iteration 1932, loss = 1.52892
I0521 04:50:55.931267 20020 solver.cpp:253]     Train net output #0: loss = 1.52892 (* 1 = 1.52892 loss)
I0521 04:50:55.931283 20020 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0521 04:51:03.952435 20020 solver.cpp:237] Iteration 1955, loss = 1.62301
I0521 04:51:03.952481 20020 solver.cpp:253]     Train net output #0: loss = 1.62301 (* 1 = 1.62301 loss)
I0521 04:51:03.952494 20020 sgd_solver.cpp:106] Iteration 1955, lr = 0.0025
I0521 04:51:11.969322 20020 solver.cpp:237] Iteration 1978, loss = 1.60656
I0521 04:51:11.969463 20020 solver.cpp:253]     Train net output #0: loss = 1.60656 (* 1 = 1.60656 loss)
I0521 04:51:11.969476 20020 sgd_solver.cpp:106] Iteration 1978, lr = 0.0025
I0521 04:51:42.161859 20020 solver.cpp:237] Iteration 2001, loss = 1.6484
I0521 04:51:42.162029 20020 solver.cpp:253]     Train net output #0: loss = 1.6484 (* 1 = 1.6484 loss)
I0521 04:51:42.162042 20020 sgd_solver.cpp:106] Iteration 2001, lr = 0.0025
I0521 04:51:50.182967 20020 solver.cpp:237] Iteration 2024, loss = 1.64801
I0521 04:51:50.183007 20020 solver.cpp:253]     Train net output #0: loss = 1.64801 (* 1 = 1.64801 loss)
I0521 04:51:50.183028 20020 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0521 04:51:58.207463 20020 solver.cpp:237] Iteration 2047, loss = 1.60291
I0521 04:51:58.207496 20020 solver.cpp:253]     Train net output #0: loss = 1.60291 (* 1 = 1.60291 loss)
I0521 04:51:58.207510 20020 sgd_solver.cpp:106] Iteration 2047, lr = 0.0025
I0521 04:52:05.879887 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_2070.caffemodel
I0521 04:52:06.165563 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_2070.solverstate
I0521 04:52:06.297979 20020 solver.cpp:237] Iteration 2070, loss = 1.67943
I0521 04:52:06.298027 20020 solver.cpp:253]     Train net output #0: loss = 1.67943 (* 1 = 1.67943 loss)
I0521 04:52:06.298043 20020 sgd_solver.cpp:106] Iteration 2070, lr = 0.0025
I0521 04:52:14.315531 20020 solver.cpp:237] Iteration 2093, loss = 1.6574
I0521 04:52:14.315686 20020 solver.cpp:253]     Train net output #0: loss = 1.6574 (* 1 = 1.6574 loss)
I0521 04:52:14.315701 20020 sgd_solver.cpp:106] Iteration 2093, lr = 0.0025
I0521 04:52:22.341739 20020 solver.cpp:237] Iteration 2116, loss = 1.72217
I0521 04:52:22.341770 20020 solver.cpp:253]     Train net output #0: loss = 1.72217 (* 1 = 1.72217 loss)
I0521 04:52:22.341785 20020 sgd_solver.cpp:106] Iteration 2116, lr = 0.0025
I0521 04:52:30.362745 20020 solver.cpp:237] Iteration 2139, loss = 1.61276
I0521 04:52:30.362776 20020 solver.cpp:253]     Train net output #0: loss = 1.61276 (* 1 = 1.61276 loss)
I0521 04:52:30.362792 20020 sgd_solver.cpp:106] Iteration 2139, lr = 0.0025
I0521 04:53:00.577240 20020 solver.cpp:237] Iteration 2162, loss = 1.70629
I0521 04:53:00.577424 20020 solver.cpp:253]     Train net output #0: loss = 1.70629 (* 1 = 1.70629 loss)
I0521 04:53:00.577437 20020 sgd_solver.cpp:106] Iteration 2162, lr = 0.0025
I0521 04:53:08.601382 20020 solver.cpp:237] Iteration 2185, loss = 1.71986
I0521 04:53:08.601423 20020 solver.cpp:253]     Train net output #0: loss = 1.71986 (* 1 = 1.71986 loss)
I0521 04:53:08.601444 20020 sgd_solver.cpp:106] Iteration 2185, lr = 0.0025
I0521 04:53:16.620606 20020 solver.cpp:237] Iteration 2208, loss = 1.66763
I0521 04:53:16.620640 20020 solver.cpp:253]     Train net output #0: loss = 1.66763 (* 1 = 1.66763 loss)
I0521 04:53:16.620654 20020 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0521 04:53:24.637804 20020 solver.cpp:237] Iteration 2231, loss = 1.64174
I0521 04:53:24.637837 20020 solver.cpp:253]     Train net output #0: loss = 1.64174 (* 1 = 1.64174 loss)
I0521 04:53:24.637852 20020 sgd_solver.cpp:106] Iteration 2231, lr = 0.0025
I0521 04:53:32.661532 20020 solver.cpp:237] Iteration 2254, loss = 1.63571
I0521 04:53:32.661685 20020 solver.cpp:253]     Train net output #0: loss = 1.63571 (* 1 = 1.63571 loss)
I0521 04:53:32.661700 20020 sgd_solver.cpp:106] Iteration 2254, lr = 0.0025
I0521 04:53:40.679256 20020 solver.cpp:237] Iteration 2277, loss = 1.61963
I0521 04:53:40.679287 20020 solver.cpp:253]     Train net output #0: loss = 1.61963 (* 1 = 1.61963 loss)
I0521 04:53:40.679304 20020 sgd_solver.cpp:106] Iteration 2277, lr = 0.0025
I0521 04:53:48.354131 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_2300.caffemodel
I0521 04:53:48.639909 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_2300.solverstate
I0521 04:53:48.772181 20020 solver.cpp:237] Iteration 2300, loss = 1.637
I0521 04:53:48.772228 20020 solver.cpp:253]     Train net output #0: loss = 1.637 (* 1 = 1.637 loss)
I0521 04:53:48.772243 20020 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0521 04:53:50.167596 20020 solver.cpp:341] Iteration 2305, Testing net (#0)
I0521 04:54:35.447932 20020 solver.cpp:409]     Test net output #0: accuracy = 0.687632
I0521 04:54:35.448096 20020 solver.cpp:409]     Test net output #1: loss = 1.10573 (* 1 = 1.10573 loss)
I0521 04:54:35.901124 20020 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_2307.caffemodel
I0521 04:54:36.186364 20020 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407_iter_2307.solverstate
I0521 04:54:36.214370 20020 solver.cpp:326] Optimization Done.
I0521 04:54:36.214396 20020 caffe.cpp:215] Optimization Done.
Application 11236789 resources: utime ~1247s, stime ~227s, Rss ~5329760, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_650_2016-05-20T11.20.56.254407.solver"
	User time (seconds): 0.56
	System time (seconds): 0.17
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:38.05
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 2
	Minor (reclaiming a frame) page faults: 15106
	Voluntary context switches: 3480
	Involuntary context switches: 576
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
