2805917
I0520 20:21:17.395494 23410 caffe.cpp:184] Using GPUs 0
I0520 20:21:17.821220 23410 solver.cpp:48] Initializing solver from parameters: 
test_iter: 555
test_interval: 1111
base_lr: 0.0025
display: 55
max_iter: 5555
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 555
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797.prototxt"
I0520 20:21:17.822965 23410 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797.prototxt
I0520 20:21:17.840761 23410 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 20:21:17.840829 23410 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 20:21:17.841205 23410 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 270
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 20:21:17.841409 23410 layer_factory.hpp:77] Creating layer data_hdf5
I0520 20:21:17.841445 23410 net.cpp:106] Creating Layer data_hdf5
I0520 20:21:17.841464 23410 net.cpp:411] data_hdf5 -> data
I0520 20:21:17.841496 23410 net.cpp:411] data_hdf5 -> label
I0520 20:21:17.841539 23410 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 20:21:17.842754 23410 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 20:21:17.844946 23410 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 20:21:39.382979 23410 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 20:21:39.388149 23410 net.cpp:150] Setting up data_hdf5
I0520 20:21:39.388190 23410 net.cpp:157] Top shape: 270 1 127 50 (1714500)
I0520 20:21:39.388207 23410 net.cpp:157] Top shape: 270 (270)
I0520 20:21:39.388219 23410 net.cpp:165] Memory required for data: 6859080
I0520 20:21:39.388239 23410 layer_factory.hpp:77] Creating layer conv1
I0520 20:21:39.388285 23410 net.cpp:106] Creating Layer conv1
I0520 20:21:39.388300 23410 net.cpp:454] conv1 <- data
I0520 20:21:39.388325 23410 net.cpp:411] conv1 -> conv1
I0520 20:21:39.755322 23410 net.cpp:150] Setting up conv1
I0520 20:21:39.755374 23410 net.cpp:157] Top shape: 270 12 120 48 (18662400)
I0520 20:21:39.755399 23410 net.cpp:165] Memory required for data: 81508680
I0520 20:21:39.755429 23410 layer_factory.hpp:77] Creating layer relu1
I0520 20:21:39.755452 23410 net.cpp:106] Creating Layer relu1
I0520 20:21:39.755473 23410 net.cpp:454] relu1 <- conv1
I0520 20:21:39.755508 23410 net.cpp:397] relu1 -> conv1 (in-place)
I0520 20:21:39.756042 23410 net.cpp:150] Setting up relu1
I0520 20:21:39.756064 23410 net.cpp:157] Top shape: 270 12 120 48 (18662400)
I0520 20:21:39.756078 23410 net.cpp:165] Memory required for data: 156158280
I0520 20:21:39.756094 23410 layer_factory.hpp:77] Creating layer pool1
I0520 20:21:39.756121 23410 net.cpp:106] Creating Layer pool1
I0520 20:21:39.756135 23410 net.cpp:454] pool1 <- conv1
I0520 20:21:39.756151 23410 net.cpp:411] pool1 -> pool1
I0520 20:21:39.756244 23410 net.cpp:150] Setting up pool1
I0520 20:21:39.756263 23410 net.cpp:157] Top shape: 270 12 60 48 (9331200)
I0520 20:21:39.756284 23410 net.cpp:165] Memory required for data: 193483080
I0520 20:21:39.756299 23410 layer_factory.hpp:77] Creating layer conv2
I0520 20:21:39.756324 23410 net.cpp:106] Creating Layer conv2
I0520 20:21:39.756336 23410 net.cpp:454] conv2 <- pool1
I0520 20:21:39.756352 23410 net.cpp:411] conv2 -> conv2
I0520 20:21:39.759047 23410 net.cpp:150] Setting up conv2
I0520 20:21:39.759078 23410 net.cpp:157] Top shape: 270 20 54 46 (13413600)
I0520 20:21:39.759094 23410 net.cpp:165] Memory required for data: 247137480
I0520 20:21:39.759121 23410 layer_factory.hpp:77] Creating layer relu2
I0520 20:21:39.759150 23410 net.cpp:106] Creating Layer relu2
I0520 20:21:39.759163 23410 net.cpp:454] relu2 <- conv2
I0520 20:21:39.759179 23410 net.cpp:397] relu2 -> conv2 (in-place)
I0520 20:21:39.759538 23410 net.cpp:150] Setting up relu2
I0520 20:21:39.759558 23410 net.cpp:157] Top shape: 270 20 54 46 (13413600)
I0520 20:21:39.759572 23410 net.cpp:165] Memory required for data: 300791880
I0520 20:21:39.759583 23410 layer_factory.hpp:77] Creating layer pool2
I0520 20:21:39.759609 23410 net.cpp:106] Creating Layer pool2
I0520 20:21:39.759623 23410 net.cpp:454] pool2 <- conv2
I0520 20:21:39.759657 23410 net.cpp:411] pool2 -> pool2
I0520 20:21:39.759740 23410 net.cpp:150] Setting up pool2
I0520 20:21:39.759758 23410 net.cpp:157] Top shape: 270 20 27 46 (6706800)
I0520 20:21:39.759769 23410 net.cpp:165] Memory required for data: 327619080
I0520 20:21:39.759791 23410 layer_factory.hpp:77] Creating layer conv3
I0520 20:21:39.759812 23410 net.cpp:106] Creating Layer conv3
I0520 20:21:39.759825 23410 net.cpp:454] conv3 <- pool2
I0520 20:21:39.759842 23410 net.cpp:411] conv3 -> conv3
I0520 20:21:39.761813 23410 net.cpp:150] Setting up conv3
I0520 20:21:39.761838 23410 net.cpp:157] Top shape: 270 28 22 44 (7318080)
I0520 20:21:39.761858 23410 net.cpp:165] Memory required for data: 356891400
I0520 20:21:39.761880 23410 layer_factory.hpp:77] Creating layer relu3
I0520 20:21:39.761903 23410 net.cpp:106] Creating Layer relu3
I0520 20:21:39.761924 23410 net.cpp:454] relu3 <- conv3
I0520 20:21:39.761940 23410 net.cpp:397] relu3 -> conv3 (in-place)
I0520 20:21:39.762437 23410 net.cpp:150] Setting up relu3
I0520 20:21:39.762461 23410 net.cpp:157] Top shape: 270 28 22 44 (7318080)
I0520 20:21:39.762475 23410 net.cpp:165] Memory required for data: 386163720
I0520 20:21:39.762490 23410 layer_factory.hpp:77] Creating layer pool3
I0520 20:21:39.762506 23410 net.cpp:106] Creating Layer pool3
I0520 20:21:39.762528 23410 net.cpp:454] pool3 <- conv3
I0520 20:21:39.762543 23410 net.cpp:411] pool3 -> pool3
I0520 20:21:39.762629 23410 net.cpp:150] Setting up pool3
I0520 20:21:39.762652 23410 net.cpp:157] Top shape: 270 28 11 44 (3659040)
I0520 20:21:39.762665 23410 net.cpp:165] Memory required for data: 400799880
I0520 20:21:39.762679 23410 layer_factory.hpp:77] Creating layer conv4
I0520 20:21:39.762706 23410 net.cpp:106] Creating Layer conv4
I0520 20:21:39.762720 23410 net.cpp:454] conv4 <- pool3
I0520 20:21:39.762737 23410 net.cpp:411] conv4 -> conv4
I0520 20:21:39.765472 23410 net.cpp:150] Setting up conv4
I0520 20:21:39.765502 23410 net.cpp:157] Top shape: 270 36 6 42 (2449440)
I0520 20:21:39.765522 23410 net.cpp:165] Memory required for data: 410597640
I0520 20:21:39.765542 23410 layer_factory.hpp:77] Creating layer relu4
I0520 20:21:39.765563 23410 net.cpp:106] Creating Layer relu4
I0520 20:21:39.765586 23410 net.cpp:454] relu4 <- conv4
I0520 20:21:39.765604 23410 net.cpp:397] relu4 -> conv4 (in-place)
I0520 20:21:39.766090 23410 net.cpp:150] Setting up relu4
I0520 20:21:39.766121 23410 net.cpp:157] Top shape: 270 36 6 42 (2449440)
I0520 20:21:39.766135 23410 net.cpp:165] Memory required for data: 420395400
I0520 20:21:39.766150 23410 layer_factory.hpp:77] Creating layer pool4
I0520 20:21:39.766166 23410 net.cpp:106] Creating Layer pool4
I0520 20:21:39.766188 23410 net.cpp:454] pool4 <- conv4
I0520 20:21:39.766204 23410 net.cpp:411] pool4 -> pool4
I0520 20:21:39.766288 23410 net.cpp:150] Setting up pool4
I0520 20:21:39.766304 23410 net.cpp:157] Top shape: 270 36 3 42 (1224720)
I0520 20:21:39.766320 23410 net.cpp:165] Memory required for data: 425294280
I0520 20:21:39.766332 23410 layer_factory.hpp:77] Creating layer ip1
I0520 20:21:39.766355 23410 net.cpp:106] Creating Layer ip1
I0520 20:21:39.766374 23410 net.cpp:454] ip1 <- pool4
I0520 20:21:39.766392 23410 net.cpp:411] ip1 -> ip1
I0520 20:21:39.781798 23410 net.cpp:150] Setting up ip1
I0520 20:21:39.781831 23410 net.cpp:157] Top shape: 270 196 (52920)
I0520 20:21:39.781852 23410 net.cpp:165] Memory required for data: 425505960
I0520 20:21:39.781879 23410 layer_factory.hpp:77] Creating layer relu5
I0520 20:21:39.781900 23410 net.cpp:106] Creating Layer relu5
I0520 20:21:39.781926 23410 net.cpp:454] relu5 <- ip1
I0520 20:21:39.781944 23410 net.cpp:397] relu5 -> ip1 (in-place)
I0520 20:21:39.782306 23410 net.cpp:150] Setting up relu5
I0520 20:21:39.782326 23410 net.cpp:157] Top shape: 270 196 (52920)
I0520 20:21:39.782340 23410 net.cpp:165] Memory required for data: 425717640
I0520 20:21:39.782352 23410 layer_factory.hpp:77] Creating layer drop1
I0520 20:21:39.782387 23410 net.cpp:106] Creating Layer drop1
I0520 20:21:39.782400 23410 net.cpp:454] drop1 <- ip1
I0520 20:21:39.782438 23410 net.cpp:397] drop1 -> ip1 (in-place)
I0520 20:21:39.782498 23410 net.cpp:150] Setting up drop1
I0520 20:21:39.782516 23410 net.cpp:157] Top shape: 270 196 (52920)
I0520 20:21:39.782536 23410 net.cpp:165] Memory required for data: 425929320
I0520 20:21:39.782548 23410 layer_factory.hpp:77] Creating layer ip2
I0520 20:21:39.782572 23410 net.cpp:106] Creating Layer ip2
I0520 20:21:39.782584 23410 net.cpp:454] ip2 <- ip1
I0520 20:21:39.782613 23410 net.cpp:411] ip2 -> ip2
I0520 20:21:39.783092 23410 net.cpp:150] Setting up ip2
I0520 20:21:39.783112 23410 net.cpp:157] Top shape: 270 98 (26460)
I0520 20:21:39.783124 23410 net.cpp:165] Memory required for data: 426035160
I0520 20:21:39.783144 23410 layer_factory.hpp:77] Creating layer relu6
I0520 20:21:39.783165 23410 net.cpp:106] Creating Layer relu6
I0520 20:21:39.783179 23410 net.cpp:454] relu6 <- ip2
I0520 20:21:39.783195 23410 net.cpp:397] relu6 -> ip2 (in-place)
I0520 20:21:39.783745 23410 net.cpp:150] Setting up relu6
I0520 20:21:39.783767 23410 net.cpp:157] Top shape: 270 98 (26460)
I0520 20:21:39.783781 23410 net.cpp:165] Memory required for data: 426141000
I0520 20:21:39.783797 23410 layer_factory.hpp:77] Creating layer drop2
I0520 20:21:39.783820 23410 net.cpp:106] Creating Layer drop2
I0520 20:21:39.783833 23410 net.cpp:454] drop2 <- ip2
I0520 20:21:39.783849 23410 net.cpp:397] drop2 -> ip2 (in-place)
I0520 20:21:39.783905 23410 net.cpp:150] Setting up drop2
I0520 20:21:39.783921 23410 net.cpp:157] Top shape: 270 98 (26460)
I0520 20:21:39.783933 23410 net.cpp:165] Memory required for data: 426246840
I0520 20:21:39.783946 23410 layer_factory.hpp:77] Creating layer ip3
I0520 20:21:39.783962 23410 net.cpp:106] Creating Layer ip3
I0520 20:21:39.783977 23410 net.cpp:454] ip3 <- ip2
I0520 20:21:39.783998 23410 net.cpp:411] ip3 -> ip3
I0520 20:21:39.784221 23410 net.cpp:150] Setting up ip3
I0520 20:21:39.784240 23410 net.cpp:157] Top shape: 270 11 (2970)
I0520 20:21:39.784252 23410 net.cpp:165] Memory required for data: 426258720
I0520 20:21:39.784272 23410 layer_factory.hpp:77] Creating layer drop3
I0520 20:21:39.784296 23410 net.cpp:106] Creating Layer drop3
I0520 20:21:39.784308 23410 net.cpp:454] drop3 <- ip3
I0520 20:21:39.784323 23410 net.cpp:397] drop3 -> ip3 (in-place)
I0520 20:21:39.784375 23410 net.cpp:150] Setting up drop3
I0520 20:21:39.784391 23410 net.cpp:157] Top shape: 270 11 (2970)
I0520 20:21:39.784404 23410 net.cpp:165] Memory required for data: 426270600
I0520 20:21:39.784423 23410 layer_factory.hpp:77] Creating layer loss
I0520 20:21:39.784445 23410 net.cpp:106] Creating Layer loss
I0520 20:21:39.784459 23410 net.cpp:454] loss <- ip3
I0520 20:21:39.784479 23410 net.cpp:454] loss <- label
I0520 20:21:39.784495 23410 net.cpp:411] loss -> loss
I0520 20:21:39.784515 23410 layer_factory.hpp:77] Creating layer loss
I0520 20:21:39.785181 23410 net.cpp:150] Setting up loss
I0520 20:21:39.785202 23410 net.cpp:157] Top shape: (1)
I0520 20:21:39.785219 23410 net.cpp:160]     with loss weight 1
I0520 20:21:39.785279 23410 net.cpp:165] Memory required for data: 426270604
I0520 20:21:39.785293 23410 net.cpp:226] loss needs backward computation.
I0520 20:21:39.785308 23410 net.cpp:226] drop3 needs backward computation.
I0520 20:21:39.785325 23410 net.cpp:226] ip3 needs backward computation.
I0520 20:21:39.785337 23410 net.cpp:226] drop2 needs backward computation.
I0520 20:21:39.785349 23410 net.cpp:226] relu6 needs backward computation.
I0520 20:21:39.785364 23410 net.cpp:226] ip2 needs backward computation.
I0520 20:21:39.785382 23410 net.cpp:226] drop1 needs backward computation.
I0520 20:21:39.785397 23410 net.cpp:226] relu5 needs backward computation.
I0520 20:21:39.785409 23410 net.cpp:226] ip1 needs backward computation.
I0520 20:21:39.785425 23410 net.cpp:226] pool4 needs backward computation.
I0520 20:21:39.785439 23410 net.cpp:226] relu4 needs backward computation.
I0520 20:21:39.785451 23410 net.cpp:226] conv4 needs backward computation.
I0520 20:21:39.785466 23410 net.cpp:226] pool3 needs backward computation.
I0520 20:21:39.785497 23410 net.cpp:226] relu3 needs backward computation.
I0520 20:21:39.785511 23410 net.cpp:226] conv3 needs backward computation.
I0520 20:21:39.785524 23410 net.cpp:226] pool2 needs backward computation.
I0520 20:21:39.785537 23410 net.cpp:226] relu2 needs backward computation.
I0520 20:21:39.785549 23410 net.cpp:226] conv2 needs backward computation.
I0520 20:21:39.785564 23410 net.cpp:226] pool1 needs backward computation.
I0520 20:21:39.785578 23410 net.cpp:226] relu1 needs backward computation.
I0520 20:21:39.785596 23410 net.cpp:226] conv1 needs backward computation.
I0520 20:21:39.785610 23410 net.cpp:228] data_hdf5 does not need backward computation.
I0520 20:21:39.785626 23410 net.cpp:270] This network produces output loss
I0520 20:21:39.785652 23410 net.cpp:283] Network initialization done.
I0520 20:21:39.787263 23410 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797.prototxt
I0520 20:21:39.787343 23410 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 20:21:39.787721 23410 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 270
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 20:21:39.787943 23410 layer_factory.hpp:77] Creating layer data_hdf5
I0520 20:21:39.787962 23410 net.cpp:106] Creating Layer data_hdf5
I0520 20:21:39.787978 23410 net.cpp:411] data_hdf5 -> data
I0520 20:21:39.788000 23410 net.cpp:411] data_hdf5 -> label
I0520 20:21:39.788017 23410 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 20:21:39.789286 23410 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 20:22:01.115200 23410 net.cpp:150] Setting up data_hdf5
I0520 20:22:01.115370 23410 net.cpp:157] Top shape: 270 1 127 50 (1714500)
I0520 20:22:01.115388 23410 net.cpp:157] Top shape: 270 (270)
I0520 20:22:01.115401 23410 net.cpp:165] Memory required for data: 6859080
I0520 20:22:01.115416 23410 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 20:22:01.115449 23410 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 20:22:01.115481 23410 net.cpp:454] label_data_hdf5_1_split <- label
I0520 20:22:01.115499 23410 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 20:22:01.115521 23410 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 20:22:01.115607 23410 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 20:22:01.115624 23410 net.cpp:157] Top shape: 270 (270)
I0520 20:22:01.115640 23410 net.cpp:157] Top shape: 270 (270)
I0520 20:22:01.115663 23410 net.cpp:165] Memory required for data: 6861240
I0520 20:22:01.115674 23410 layer_factory.hpp:77] Creating layer conv1
I0520 20:22:01.115701 23410 net.cpp:106] Creating Layer conv1
I0520 20:22:01.115720 23410 net.cpp:454] conv1 <- data
I0520 20:22:01.115738 23410 net.cpp:411] conv1 -> conv1
I0520 20:22:01.117699 23410 net.cpp:150] Setting up conv1
I0520 20:22:01.117725 23410 net.cpp:157] Top shape: 270 12 120 48 (18662400)
I0520 20:22:01.117744 23410 net.cpp:165] Memory required for data: 81510840
I0520 20:22:01.117769 23410 layer_factory.hpp:77] Creating layer relu1
I0520 20:22:01.117789 23410 net.cpp:106] Creating Layer relu1
I0520 20:22:01.117812 23410 net.cpp:454] relu1 <- conv1
I0520 20:22:01.117828 23410 net.cpp:397] relu1 -> conv1 (in-place)
I0520 20:22:01.118357 23410 net.cpp:150] Setting up relu1
I0520 20:22:01.118381 23410 net.cpp:157] Top shape: 270 12 120 48 (18662400)
I0520 20:22:01.118394 23410 net.cpp:165] Memory required for data: 156160440
I0520 20:22:01.118410 23410 layer_factory.hpp:77] Creating layer pool1
I0520 20:22:01.118438 23410 net.cpp:106] Creating Layer pool1
I0520 20:22:01.118450 23410 net.cpp:454] pool1 <- conv1
I0520 20:22:01.118468 23410 net.cpp:411] pool1 -> pool1
I0520 20:22:01.118556 23410 net.cpp:150] Setting up pool1
I0520 20:22:01.118573 23410 net.cpp:157] Top shape: 270 12 60 48 (9331200)
I0520 20:22:01.118588 23410 net.cpp:165] Memory required for data: 193485240
I0520 20:22:01.118608 23410 layer_factory.hpp:77] Creating layer conv2
I0520 20:22:01.118628 23410 net.cpp:106] Creating Layer conv2
I0520 20:22:01.118649 23410 net.cpp:454] conv2 <- pool1
I0520 20:22:01.118665 23410 net.cpp:411] conv2 -> conv2
I0520 20:22:01.120614 23410 net.cpp:150] Setting up conv2
I0520 20:22:01.120638 23410 net.cpp:157] Top shape: 270 20 54 46 (13413600)
I0520 20:22:01.120659 23410 net.cpp:165] Memory required for data: 247139640
I0520 20:22:01.120681 23410 layer_factory.hpp:77] Creating layer relu2
I0520 20:22:01.120700 23410 net.cpp:106] Creating Layer relu2
I0520 20:22:01.120723 23410 net.cpp:454] relu2 <- conv2
I0520 20:22:01.120738 23410 net.cpp:397] relu2 -> conv2 (in-place)
I0520 20:22:01.121088 23410 net.cpp:150] Setting up relu2
I0520 20:22:01.121109 23410 net.cpp:157] Top shape: 270 20 54 46 (13413600)
I0520 20:22:01.121121 23410 net.cpp:165] Memory required for data: 300794040
I0520 20:22:01.121134 23410 layer_factory.hpp:77] Creating layer pool2
I0520 20:22:01.121158 23410 net.cpp:106] Creating Layer pool2
I0520 20:22:01.121171 23410 net.cpp:454] pool2 <- conv2
I0520 20:22:01.121187 23410 net.cpp:411] pool2 -> pool2
I0520 20:22:01.121274 23410 net.cpp:150] Setting up pool2
I0520 20:22:01.121296 23410 net.cpp:157] Top shape: 270 20 27 46 (6706800)
I0520 20:22:01.121309 23410 net.cpp:165] Memory required for data: 327621240
I0520 20:22:01.121323 23410 layer_factory.hpp:77] Creating layer conv3
I0520 20:22:01.121352 23410 net.cpp:106] Creating Layer conv3
I0520 20:22:01.121366 23410 net.cpp:454] conv3 <- pool2
I0520 20:22:01.121382 23410 net.cpp:411] conv3 -> conv3
I0520 20:22:01.123389 23410 net.cpp:150] Setting up conv3
I0520 20:22:01.123415 23410 net.cpp:157] Top shape: 270 28 22 44 (7318080)
I0520 20:22:01.123435 23410 net.cpp:165] Memory required for data: 356893560
I0520 20:22:01.123474 23410 layer_factory.hpp:77] Creating layer relu3
I0520 20:22:01.123499 23410 net.cpp:106] Creating Layer relu3
I0520 20:22:01.123513 23410 net.cpp:454] relu3 <- conv3
I0520 20:22:01.123530 23410 net.cpp:397] relu3 -> conv3 (in-place)
I0520 20:22:01.124027 23410 net.cpp:150] Setting up relu3
I0520 20:22:01.124050 23410 net.cpp:157] Top shape: 270 28 22 44 (7318080)
I0520 20:22:01.124063 23410 net.cpp:165] Memory required for data: 386165880
I0520 20:22:01.124079 23410 layer_factory.hpp:77] Creating layer pool3
I0520 20:22:01.124104 23410 net.cpp:106] Creating Layer pool3
I0520 20:22:01.124117 23410 net.cpp:454] pool3 <- conv3
I0520 20:22:01.124133 23410 net.cpp:411] pool3 -> pool3
I0520 20:22:01.124218 23410 net.cpp:150] Setting up pool3
I0520 20:22:01.124235 23410 net.cpp:157] Top shape: 270 28 11 44 (3659040)
I0520 20:22:01.124250 23410 net.cpp:165] Memory required for data: 400802040
I0520 20:22:01.124263 23410 layer_factory.hpp:77] Creating layer conv4
I0520 20:22:01.124290 23410 net.cpp:106] Creating Layer conv4
I0520 20:22:01.124303 23410 net.cpp:454] conv4 <- pool3
I0520 20:22:01.124320 23410 net.cpp:411] conv4 -> conv4
I0520 20:22:01.126417 23410 net.cpp:150] Setting up conv4
I0520 20:22:01.126442 23410 net.cpp:157] Top shape: 270 36 6 42 (2449440)
I0520 20:22:01.126456 23410 net.cpp:165] Memory required for data: 410599800
I0520 20:22:01.126478 23410 layer_factory.hpp:77] Creating layer relu4
I0520 20:22:01.126503 23410 net.cpp:106] Creating Layer relu4
I0520 20:22:01.126516 23410 net.cpp:454] relu4 <- conv4
I0520 20:22:01.126533 23410 net.cpp:397] relu4 -> conv4 (in-place)
I0520 20:22:01.127029 23410 net.cpp:150] Setting up relu4
I0520 20:22:01.127053 23410 net.cpp:157] Top shape: 270 36 6 42 (2449440)
I0520 20:22:01.127065 23410 net.cpp:165] Memory required for data: 420397560
I0520 20:22:01.127084 23410 layer_factory.hpp:77] Creating layer pool4
I0520 20:22:01.127107 23410 net.cpp:106] Creating Layer pool4
I0520 20:22:01.127120 23410 net.cpp:454] pool4 <- conv4
I0520 20:22:01.127137 23410 net.cpp:411] pool4 -> pool4
I0520 20:22:01.127223 23410 net.cpp:150] Setting up pool4
I0520 20:22:01.127240 23410 net.cpp:157] Top shape: 270 36 3 42 (1224720)
I0520 20:22:01.127255 23410 net.cpp:165] Memory required for data: 425296440
I0520 20:22:01.127266 23410 layer_factory.hpp:77] Creating layer ip1
I0520 20:22:01.127291 23410 net.cpp:106] Creating Layer ip1
I0520 20:22:01.127305 23410 net.cpp:454] ip1 <- pool4
I0520 20:22:01.127321 23410 net.cpp:411] ip1 -> ip1
I0520 20:22:01.142802 23410 net.cpp:150] Setting up ip1
I0520 20:22:01.142835 23410 net.cpp:157] Top shape: 270 196 (52920)
I0520 20:22:01.142856 23410 net.cpp:165] Memory required for data: 425508120
I0520 20:22:01.142882 23410 layer_factory.hpp:77] Creating layer relu5
I0520 20:22:01.142904 23410 net.cpp:106] Creating Layer relu5
I0520 20:22:01.142928 23410 net.cpp:454] relu5 <- ip1
I0520 20:22:01.142945 23410 net.cpp:397] relu5 -> ip1 (in-place)
I0520 20:22:01.143308 23410 net.cpp:150] Setting up relu5
I0520 20:22:01.143329 23410 net.cpp:157] Top shape: 270 196 (52920)
I0520 20:22:01.143342 23410 net.cpp:165] Memory required for data: 425719800
I0520 20:22:01.143357 23410 layer_factory.hpp:77] Creating layer drop1
I0520 20:22:01.143386 23410 net.cpp:106] Creating Layer drop1
I0520 20:22:01.143400 23410 net.cpp:454] drop1 <- ip1
I0520 20:22:01.143416 23410 net.cpp:397] drop1 -> ip1 (in-place)
I0520 20:22:01.143473 23410 net.cpp:150] Setting up drop1
I0520 20:22:01.143491 23410 net.cpp:157] Top shape: 270 196 (52920)
I0520 20:22:01.143502 23410 net.cpp:165] Memory required for data: 425931480
I0520 20:22:01.143515 23410 layer_factory.hpp:77] Creating layer ip2
I0520 20:22:01.143534 23410 net.cpp:106] Creating Layer ip2
I0520 20:22:01.143546 23410 net.cpp:454] ip2 <- ip1
I0520 20:22:01.143570 23410 net.cpp:411] ip2 -> ip2
I0520 20:22:01.144067 23410 net.cpp:150] Setting up ip2
I0520 20:22:01.144085 23410 net.cpp:157] Top shape: 270 98 (26460)
I0520 20:22:01.144098 23410 net.cpp:165] Memory required for data: 426037320
I0520 20:22:01.144141 23410 layer_factory.hpp:77] Creating layer relu6
I0520 20:22:01.144158 23410 net.cpp:106] Creating Layer relu6
I0520 20:22:01.144181 23410 net.cpp:454] relu6 <- ip2
I0520 20:22:01.144196 23410 net.cpp:397] relu6 -> ip2 (in-place)
I0520 20:22:01.144759 23410 net.cpp:150] Setting up relu6
I0520 20:22:01.144783 23410 net.cpp:157] Top shape: 270 98 (26460)
I0520 20:22:01.144795 23410 net.cpp:165] Memory required for data: 426143160
I0520 20:22:01.144809 23410 layer_factory.hpp:77] Creating layer drop2
I0520 20:22:01.144829 23410 net.cpp:106] Creating Layer drop2
I0520 20:22:01.144850 23410 net.cpp:454] drop2 <- ip2
I0520 20:22:01.144866 23410 net.cpp:397] drop2 -> ip2 (in-place)
I0520 20:22:01.144918 23410 net.cpp:150] Setting up drop2
I0520 20:22:01.144942 23410 net.cpp:157] Top shape: 270 98 (26460)
I0520 20:22:01.144954 23410 net.cpp:165] Memory required for data: 426249000
I0520 20:22:01.144968 23410 layer_factory.hpp:77] Creating layer ip3
I0520 20:22:01.144985 23410 net.cpp:106] Creating Layer ip3
I0520 20:22:01.145000 23410 net.cpp:454] ip3 <- ip2
I0520 20:22:01.145023 23410 net.cpp:411] ip3 -> ip3
I0520 20:22:01.145262 23410 net.cpp:150] Setting up ip3
I0520 20:22:01.145282 23410 net.cpp:157] Top shape: 270 11 (2970)
I0520 20:22:01.145294 23410 net.cpp:165] Memory required for data: 426260880
I0520 20:22:01.145315 23410 layer_factory.hpp:77] Creating layer drop3
I0520 20:22:01.145337 23410 net.cpp:106] Creating Layer drop3
I0520 20:22:01.145350 23410 net.cpp:454] drop3 <- ip3
I0520 20:22:01.145366 23410 net.cpp:397] drop3 -> ip3 (in-place)
I0520 20:22:01.145414 23410 net.cpp:150] Setting up drop3
I0520 20:22:01.145437 23410 net.cpp:157] Top shape: 270 11 (2970)
I0520 20:22:01.145449 23410 net.cpp:165] Memory required for data: 426272760
I0520 20:22:01.145463 23410 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 20:22:01.145479 23410 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 20:22:01.145494 23410 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 20:22:01.145515 23410 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 20:22:01.145534 23410 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 20:22:01.145627 23410 net.cpp:150] Setting up ip3_drop3_0_split
I0520 20:22:01.145644 23410 net.cpp:157] Top shape: 270 11 (2970)
I0520 20:22:01.145661 23410 net.cpp:157] Top shape: 270 11 (2970)
I0520 20:22:01.145673 23410 net.cpp:165] Memory required for data: 426296520
I0520 20:22:01.145689 23410 layer_factory.hpp:77] Creating layer accuracy
I0520 20:22:01.145717 23410 net.cpp:106] Creating Layer accuracy
I0520 20:22:01.145730 23410 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 20:22:01.145746 23410 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 20:22:01.145762 23410 net.cpp:411] accuracy -> accuracy
I0520 20:22:01.145797 23410 net.cpp:150] Setting up accuracy
I0520 20:22:01.145812 23410 net.cpp:157] Top shape: (1)
I0520 20:22:01.145824 23410 net.cpp:165] Memory required for data: 426296524
I0520 20:22:01.145836 23410 layer_factory.hpp:77] Creating layer loss
I0520 20:22:01.145853 23410 net.cpp:106] Creating Layer loss
I0520 20:22:01.145867 23410 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 20:22:01.145880 23410 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 20:22:01.145903 23410 net.cpp:411] loss -> loss
I0520 20:22:01.145928 23410 layer_factory.hpp:77] Creating layer loss
I0520 20:22:01.146450 23410 net.cpp:150] Setting up loss
I0520 20:22:01.146469 23410 net.cpp:157] Top shape: (1)
I0520 20:22:01.146481 23410 net.cpp:160]     with loss weight 1
I0520 20:22:01.146507 23410 net.cpp:165] Memory required for data: 426296528
I0520 20:22:01.146528 23410 net.cpp:226] loss needs backward computation.
I0520 20:22:01.146543 23410 net.cpp:228] accuracy does not need backward computation.
I0520 20:22:01.146556 23410 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 20:22:01.146570 23410 net.cpp:226] drop3 needs backward computation.
I0520 20:22:01.146582 23410 net.cpp:226] ip3 needs backward computation.
I0520 20:22:01.146598 23410 net.cpp:226] drop2 needs backward computation.
I0520 20:22:01.146625 23410 net.cpp:226] relu6 needs backward computation.
I0520 20:22:01.146638 23410 net.cpp:226] ip2 needs backward computation.
I0520 20:22:01.146656 23410 net.cpp:226] drop1 needs backward computation.
I0520 20:22:01.146668 23410 net.cpp:226] relu5 needs backward computation.
I0520 20:22:01.146680 23410 net.cpp:226] ip1 needs backward computation.
I0520 20:22:01.146695 23410 net.cpp:226] pool4 needs backward computation.
I0520 20:22:01.146708 23410 net.cpp:226] relu4 needs backward computation.
I0520 20:22:01.146728 23410 net.cpp:226] conv4 needs backward computation.
I0520 20:22:01.146742 23410 net.cpp:226] pool3 needs backward computation.
I0520 20:22:01.146759 23410 net.cpp:226] relu3 needs backward computation.
I0520 20:22:01.146772 23410 net.cpp:226] conv3 needs backward computation.
I0520 20:22:01.146785 23410 net.cpp:226] pool2 needs backward computation.
I0520 20:22:01.146797 23410 net.cpp:226] relu2 needs backward computation.
I0520 20:22:01.146812 23410 net.cpp:226] conv2 needs backward computation.
I0520 20:22:01.146831 23410 net.cpp:226] pool1 needs backward computation.
I0520 20:22:01.146845 23410 net.cpp:226] relu1 needs backward computation.
I0520 20:22:01.146858 23410 net.cpp:226] conv1 needs backward computation.
I0520 20:22:01.146872 23410 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 20:22:01.146886 23410 net.cpp:228] data_hdf5 does not need backward computation.
I0520 20:22:01.146898 23410 net.cpp:270] This network produces output accuracy
I0520 20:22:01.146914 23410 net.cpp:270] This network produces output loss
I0520 20:22:01.146944 23410 net.cpp:283] Network initialization done.
I0520 20:22:01.147078 23410 solver.cpp:60] Solver scaffolding done.
I0520 20:22:01.148208 23410 caffe.cpp:212] Starting Optimization
I0520 20:22:01.148226 23410 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 20:22:01.148241 23410 solver.cpp:289] Learning Rate Policy: fixed
I0520 20:22:01.149471 23410 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 20:22:47.547969 23410 solver.cpp:409]     Test net output #0: accuracy = 0.145526
I0520 20:22:47.548147 23410 solver.cpp:409]     Test net output #1: loss = 2.39709 (* 1 = 2.39709 loss)
I0520 20:22:47.609168 23410 solver.cpp:237] Iteration 0, loss = 2.39417
I0520 20:22:47.609206 23410 solver.cpp:253]     Train net output #0: loss = 2.39417 (* 1 = 2.39417 loss)
I0520 20:22:47.609228 23410 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 20:22:55.691372 23410 solver.cpp:237] Iteration 55, loss = 2.351
I0520 20:22:55.691431 23410 solver.cpp:253]     Train net output #0: loss = 2.351 (* 1 = 2.351 loss)
I0520 20:22:55.691457 23410 sgd_solver.cpp:106] Iteration 55, lr = 0.0025
I0520 20:23:03.769592 23410 solver.cpp:237] Iteration 110, loss = 2.33152
I0520 20:23:03.769628 23410 solver.cpp:253]     Train net output #0: loss = 2.33152 (* 1 = 2.33152 loss)
I0520 20:23:03.769650 23410 sgd_solver.cpp:106] Iteration 110, lr = 0.0025
I0520 20:23:11.848371 23410 solver.cpp:237] Iteration 165, loss = 2.32289
I0520 20:23:11.848407 23410 solver.cpp:253]     Train net output #0: loss = 2.32289 (* 1 = 2.32289 loss)
I0520 20:23:11.848430 23410 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0520 20:23:19.934618 23410 solver.cpp:237] Iteration 220, loss = 2.30955
I0520 20:23:19.934787 23410 solver.cpp:253]     Train net output #0: loss = 2.30955 (* 1 = 2.30955 loss)
I0520 20:23:19.934804 23410 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0520 20:23:28.014489 23410 solver.cpp:237] Iteration 275, loss = 2.22826
I0520 20:23:28.014524 23410 solver.cpp:253]     Train net output #0: loss = 2.22826 (* 1 = 2.22826 loss)
I0520 20:23:28.014549 23410 sgd_solver.cpp:106] Iteration 275, lr = 0.0025
I0520 20:23:36.092991 23410 solver.cpp:237] Iteration 330, loss = 2.0855
I0520 20:23:36.093025 23410 solver.cpp:253]     Train net output #0: loss = 2.0855 (* 1 = 2.0855 loss)
I0520 20:23:36.093044 23410 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0520 20:24:06.299788 23410 solver.cpp:237] Iteration 385, loss = 2.09827
I0520 20:24:06.299957 23410 solver.cpp:253]     Train net output #0: loss = 2.09827 (* 1 = 2.09827 loss)
I0520 20:24:06.299973 23410 sgd_solver.cpp:106] Iteration 385, lr = 0.0025
I0520 20:24:14.383708 23410 solver.cpp:237] Iteration 440, loss = 1.92821
I0520 20:24:14.383744 23410 solver.cpp:253]     Train net output #0: loss = 1.92821 (* 1 = 1.92821 loss)
I0520 20:24:14.383766 23410 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0520 20:24:22.470418 23410 solver.cpp:237] Iteration 495, loss = 2.06905
I0520 20:24:22.470454 23410 solver.cpp:253]     Train net output #0: loss = 2.06905 (* 1 = 2.06905 loss)
I0520 20:24:22.470479 23410 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0520 20:24:30.558791 23410 solver.cpp:237] Iteration 550, loss = 1.88381
I0520 20:24:30.558850 23410 solver.cpp:253]     Train net output #0: loss = 1.88381 (* 1 = 1.88381 loss)
I0520 20:24:30.558876 23410 sgd_solver.cpp:106] Iteration 550, lr = 0.0025
I0520 20:24:31.147992 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_555.caffemodel
I0520 20:24:31.292207 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_555.solverstate
I0520 20:24:38.713898 23410 solver.cpp:237] Iteration 605, loss = 1.9373
I0520 20:24:38.714061 23410 solver.cpp:253]     Train net output #0: loss = 1.9373 (* 1 = 1.9373 loss)
I0520 20:24:38.714081 23410 sgd_solver.cpp:106] Iteration 605, lr = 0.0025
I0520 20:24:46.794584 23410 solver.cpp:237] Iteration 660, loss = 1.85025
I0520 20:24:46.794620 23410 solver.cpp:253]     Train net output #0: loss = 1.85025 (* 1 = 1.85025 loss)
I0520 20:24:46.794638 23410 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0520 20:24:54.875102 23410 solver.cpp:237] Iteration 715, loss = 1.86568
I0520 20:24:54.875156 23410 solver.cpp:253]     Train net output #0: loss = 1.86568 (* 1 = 1.86568 loss)
I0520 20:24:54.875174 23410 sgd_solver.cpp:106] Iteration 715, lr = 0.0025
I0520 20:25:25.049795 23410 solver.cpp:237] Iteration 770, loss = 1.92279
I0520 20:25:25.049959 23410 solver.cpp:253]     Train net output #0: loss = 1.92279 (* 1 = 1.92279 loss)
I0520 20:25:25.049976 23410 sgd_solver.cpp:106] Iteration 770, lr = 0.0025
I0520 20:25:33.136744 23410 solver.cpp:237] Iteration 825, loss = 1.79456
I0520 20:25:33.136777 23410 solver.cpp:253]     Train net output #0: loss = 1.79456 (* 1 = 1.79456 loss)
I0520 20:25:33.136801 23410 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0520 20:25:41.225952 23410 solver.cpp:237] Iteration 880, loss = 1.78493
I0520 20:25:41.225989 23410 solver.cpp:253]     Train net output #0: loss = 1.78493 (* 1 = 1.78493 loss)
I0520 20:25:41.226007 23410 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0520 20:25:49.315233 23410 solver.cpp:237] Iteration 935, loss = 1.80432
I0520 20:25:49.315284 23410 solver.cpp:253]     Train net output #0: loss = 1.80432 (* 1 = 1.80432 loss)
I0520 20:25:49.315299 23410 sgd_solver.cpp:106] Iteration 935, lr = 0.0025
I0520 20:25:57.399562 23410 solver.cpp:237] Iteration 990, loss = 1.91538
I0520 20:25:57.399714 23410 solver.cpp:253]     Train net output #0: loss = 1.91538 (* 1 = 1.91538 loss)
I0520 20:25:57.399731 23410 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0520 20:26:05.490757 23410 solver.cpp:237] Iteration 1045, loss = 1.77961
I0520 20:26:05.490793 23410 solver.cpp:253]     Train net output #0: loss = 1.77961 (* 1 = 1.77961 loss)
I0520 20:26:05.490811 23410 sgd_solver.cpp:106] Iteration 1045, lr = 0.0025
I0520 20:26:13.580493 23410 solver.cpp:237] Iteration 1100, loss = 1.80588
I0520 20:26:13.580546 23410 solver.cpp:253]     Train net output #0: loss = 1.80588 (* 1 = 1.80588 loss)
I0520 20:26:13.580564 23410 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0520 20:26:14.904081 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_1110.caffemodel
I0520 20:26:15.045034 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_1110.solverstate
I0520 20:26:15.116636 23410 solver.cpp:341] Iteration 1111, Testing net (#0)
I0520 20:27:00.671978 23410 solver.cpp:409]     Test net output #0: accuracy = 0.617144
I0520 20:27:00.672147 23410 solver.cpp:409]     Test net output #1: loss = 1.29885 (* 1 = 1.29885 loss)
I0520 20:27:29.338809 23410 solver.cpp:237] Iteration 1155, loss = 1.75965
I0520 20:27:29.338868 23410 solver.cpp:253]     Train net output #0: loss = 1.75965 (* 1 = 1.75965 loss)
I0520 20:27:29.338893 23410 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0520 20:27:37.414759 23410 solver.cpp:237] Iteration 1210, loss = 1.68228
I0520 20:27:37.414903 23410 solver.cpp:253]     Train net output #0: loss = 1.68228 (* 1 = 1.68228 loss)
I0520 20:27:37.414919 23410 sgd_solver.cpp:106] Iteration 1210, lr = 0.0025
I0520 20:27:45.493410 23410 solver.cpp:237] Iteration 1265, loss = 1.82881
I0520 20:27:45.493446 23410 solver.cpp:253]     Train net output #0: loss = 1.82881 (* 1 = 1.82881 loss)
I0520 20:27:45.493464 23410 sgd_solver.cpp:106] Iteration 1265, lr = 0.0025
I0520 20:27:53.577476 23410 solver.cpp:237] Iteration 1320, loss = 1.74619
I0520 20:27:53.577533 23410 solver.cpp:253]     Train net output #0: loss = 1.74619 (* 1 = 1.74619 loss)
I0520 20:27:53.577558 23410 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0520 20:28:01.657996 23410 solver.cpp:237] Iteration 1375, loss = 1.79283
I0520 20:28:01.658032 23410 solver.cpp:253]     Train net output #0: loss = 1.79283 (* 1 = 1.79283 loss)
I0520 20:28:01.658056 23410 sgd_solver.cpp:106] Iteration 1375, lr = 0.0025
I0520 20:28:09.738314 23410 solver.cpp:237] Iteration 1430, loss = 1.67979
I0520 20:28:09.738456 23410 solver.cpp:253]     Train net output #0: loss = 1.67979 (* 1 = 1.67979 loss)
I0520 20:28:09.738472 23410 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0520 20:28:40.001572 23410 solver.cpp:237] Iteration 1485, loss = 1.70197
I0520 20:28:40.001732 23410 solver.cpp:253]     Train net output #0: loss = 1.70197 (* 1 = 1.70197 loss)
I0520 20:28:40.001749 23410 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0520 20:28:48.083971 23410 solver.cpp:237] Iteration 1540, loss = 1.69395
I0520 20:28:48.084007 23410 solver.cpp:253]     Train net output #0: loss = 1.69395 (* 1 = 1.69395 loss)
I0520 20:28:48.084027 23410 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0520 20:28:56.169972 23410 solver.cpp:237] Iteration 1595, loss = 1.73628
I0520 20:28:56.170007 23410 solver.cpp:253]     Train net output #0: loss = 1.73628 (* 1 = 1.73628 loss)
I0520 20:28:56.170025 23410 sgd_solver.cpp:106] Iteration 1595, lr = 0.0025
I0520 20:29:04.256736 23410 solver.cpp:237] Iteration 1650, loss = 1.70053
I0520 20:29:04.256789 23410 solver.cpp:253]     Train net output #0: loss = 1.70053 (* 1 = 1.70053 loss)
I0520 20:29:04.256808 23410 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0520 20:29:06.314162 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_1665.caffemodel
I0520 20:29:06.457237 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_1665.solverstate
I0520 20:29:12.409464 23410 solver.cpp:237] Iteration 1705, loss = 1.78768
I0520 20:29:12.409642 23410 solver.cpp:253]     Train net output #0: loss = 1.78768 (* 1 = 1.78768 loss)
I0520 20:29:12.409659 23410 sgd_solver.cpp:106] Iteration 1705, lr = 0.0025
I0520 20:29:20.496670 23410 solver.cpp:237] Iteration 1760, loss = 1.57729
I0520 20:29:20.496706 23410 solver.cpp:253]     Train net output #0: loss = 1.57729 (* 1 = 1.57729 loss)
I0520 20:29:20.496729 23410 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0520 20:29:28.581925 23410 solver.cpp:237] Iteration 1815, loss = 1.58652
I0520 20:29:28.581982 23410 solver.cpp:253]     Train net output #0: loss = 1.58652 (* 1 = 1.58652 loss)
I0520 20:29:28.582000 23410 sgd_solver.cpp:106] Iteration 1815, lr = 0.0025
I0520 20:29:58.865699 23410 solver.cpp:237] Iteration 1870, loss = 1.65881
I0520 20:29:58.865869 23410 solver.cpp:253]     Train net output #0: loss = 1.65881 (* 1 = 1.65881 loss)
I0520 20:29:58.865886 23410 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0520 20:30:06.954058 23410 solver.cpp:237] Iteration 1925, loss = 1.78789
I0520 20:30:06.954093 23410 solver.cpp:253]     Train net output #0: loss = 1.78789 (* 1 = 1.78789 loss)
I0520 20:30:06.954115 23410 sgd_solver.cpp:106] Iteration 1925, lr = 0.0025
I0520 20:30:15.045198 23410 solver.cpp:237] Iteration 1980, loss = 1.69931
I0520 20:30:15.045233 23410 solver.cpp:253]     Train net output #0: loss = 1.69931 (* 1 = 1.69931 loss)
I0520 20:30:15.045253 23410 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0520 20:30:23.132138 23410 solver.cpp:237] Iteration 2035, loss = 1.68271
I0520 20:30:23.132194 23410 solver.cpp:253]     Train net output #0: loss = 1.68271 (* 1 = 1.68271 loss)
I0520 20:30:23.132210 23410 sgd_solver.cpp:106] Iteration 2035, lr = 0.0025
I0520 20:30:31.221547 23410 solver.cpp:237] Iteration 2090, loss = 1.62194
I0520 20:30:31.221693 23410 solver.cpp:253]     Train net output #0: loss = 1.62194 (* 1 = 1.62194 loss)
I0520 20:30:31.221709 23410 sgd_solver.cpp:106] Iteration 2090, lr = 0.0025
I0520 20:30:39.307770 23410 solver.cpp:237] Iteration 2145, loss = 1.68587
I0520 20:30:39.307804 23410 solver.cpp:253]     Train net output #0: loss = 1.68587 (* 1 = 1.68587 loss)
I0520 20:30:39.307827 23410 sgd_solver.cpp:106] Iteration 2145, lr = 0.0025
I0520 20:30:47.392863 23410 solver.cpp:237] Iteration 2200, loss = 1.63843
I0520 20:30:47.392917 23410 solver.cpp:253]     Train net output #0: loss = 1.63843 (* 1 = 1.63843 loss)
I0520 20:30:47.392937 23410 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0520 20:30:50.187089 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_2220.caffemodel
I0520 20:30:50.330297 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_2220.solverstate
I0520 20:30:50.550933 23410 solver.cpp:341] Iteration 2222, Testing net (#0)
I0520 20:31:57.011042 23410 solver.cpp:409]     Test net output #0: accuracy = 0.682603
I0520 20:31:57.011212 23410 solver.cpp:409]     Test net output #1: loss = 1.09418 (* 1 = 1.09418 loss)
I0520 20:32:24.030902 23410 solver.cpp:237] Iteration 2255, loss = 1.67815
I0520 20:32:24.030959 23410 solver.cpp:253]     Train net output #0: loss = 1.67815 (* 1 = 1.67815 loss)
I0520 20:32:24.030982 23410 sgd_solver.cpp:106] Iteration 2255, lr = 0.0025
I0520 20:32:32.106132 23410 solver.cpp:237] Iteration 2310, loss = 1.67982
I0520 20:32:32.106288 23410 solver.cpp:253]     Train net output #0: loss = 1.67982 (* 1 = 1.67982 loss)
I0520 20:32:32.106305 23410 sgd_solver.cpp:106] Iteration 2310, lr = 0.0025
I0520 20:32:40.185227 23410 solver.cpp:237] Iteration 2365, loss = 1.49104
I0520 20:32:40.185263 23410 solver.cpp:253]     Train net output #0: loss = 1.49104 (* 1 = 1.49104 loss)
I0520 20:32:40.185282 23410 sgd_solver.cpp:106] Iteration 2365, lr = 0.0025
I0520 20:32:48.264101 23410 solver.cpp:237] Iteration 2420, loss = 1.56545
I0520 20:32:48.264155 23410 solver.cpp:253]     Train net output #0: loss = 1.56545 (* 1 = 1.56545 loss)
I0520 20:32:48.264173 23410 sgd_solver.cpp:106] Iteration 2420, lr = 0.0025
I0520 20:32:56.341825 23410 solver.cpp:237] Iteration 2475, loss = 1.71901
I0520 20:32:56.341861 23410 solver.cpp:253]     Train net output #0: loss = 1.71901 (* 1 = 1.71901 loss)
I0520 20:32:56.341881 23410 sgd_solver.cpp:106] Iteration 2475, lr = 0.0025
I0520 20:33:04.420871 23410 solver.cpp:237] Iteration 2530, loss = 1.53456
I0520 20:33:04.421017 23410 solver.cpp:253]     Train net output #0: loss = 1.53456 (* 1 = 1.53456 loss)
I0520 20:33:04.421035 23410 sgd_solver.cpp:106] Iteration 2530, lr = 0.0025
I0520 20:33:12.498144 23410 solver.cpp:237] Iteration 2585, loss = 1.61469
I0520 20:33:12.498179 23410 solver.cpp:253]     Train net output #0: loss = 1.61469 (* 1 = 1.61469 loss)
I0520 20:33:12.498203 23410 sgd_solver.cpp:106] Iteration 2585, lr = 0.0025
I0520 20:33:42.707202 23410 solver.cpp:237] Iteration 2640, loss = 1.6376
I0520 20:33:42.707375 23410 solver.cpp:253]     Train net output #0: loss = 1.6376 (* 1 = 1.6376 loss)
I0520 20:33:42.707391 23410 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0520 20:33:50.785610 23410 solver.cpp:237] Iteration 2695, loss = 1.63582
I0520 20:33:50.785646 23410 solver.cpp:253]     Train net output #0: loss = 1.63582 (* 1 = 1.63582 loss)
I0520 20:33:50.785670 23410 sgd_solver.cpp:106] Iteration 2695, lr = 0.0025
I0520 20:33:58.863142 23410 solver.cpp:237] Iteration 2750, loss = 1.53228
I0520 20:33:58.863176 23410 solver.cpp:253]     Train net output #0: loss = 1.53228 (* 1 = 1.53228 loss)
I0520 20:33:58.863200 23410 sgd_solver.cpp:106] Iteration 2750, lr = 0.0025
I0520 20:34:02.389722 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_2775.caffemodel
I0520 20:34:02.532057 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_2775.solverstate
I0520 20:34:07.008430 23410 solver.cpp:237] Iteration 2805, loss = 1.65642
I0520 20:34:07.008487 23410 solver.cpp:253]     Train net output #0: loss = 1.65642 (* 1 = 1.65642 loss)
I0520 20:34:07.008507 23410 sgd_solver.cpp:106] Iteration 2805, lr = 0.0025
I0520 20:34:15.086083 23410 solver.cpp:237] Iteration 2860, loss = 1.59778
I0520 20:34:15.086246 23410 solver.cpp:253]     Train net output #0: loss = 1.59778 (* 1 = 1.59778 loss)
I0520 20:34:15.086263 23410 sgd_solver.cpp:106] Iteration 2860, lr = 0.0025
I0520 20:34:23.161988 23410 solver.cpp:237] Iteration 2915, loss = 1.69713
I0520 20:34:23.162024 23410 solver.cpp:253]     Train net output #0: loss = 1.69713 (* 1 = 1.69713 loss)
I0520 20:34:23.162048 23410 sgd_solver.cpp:106] Iteration 2915, lr = 0.0025
I0520 20:34:53.412561 23410 solver.cpp:237] Iteration 2970, loss = 1.61076
I0520 20:34:53.412737 23410 solver.cpp:253]     Train net output #0: loss = 1.61076 (* 1 = 1.61076 loss)
I0520 20:34:53.412755 23410 sgd_solver.cpp:106] Iteration 2970, lr = 0.0025
I0520 20:35:01.487500 23410 solver.cpp:237] Iteration 3025, loss = 1.65128
I0520 20:35:01.487536 23410 solver.cpp:253]     Train net output #0: loss = 1.65128 (* 1 = 1.65128 loss)
I0520 20:35:01.487555 23410 sgd_solver.cpp:106] Iteration 3025, lr = 0.0025
I0520 20:35:09.571399 23410 solver.cpp:237] Iteration 3080, loss = 1.5274
I0520 20:35:09.571435 23410 solver.cpp:253]     Train net output #0: loss = 1.5274 (* 1 = 1.5274 loss)
I0520 20:35:09.571460 23410 sgd_solver.cpp:106] Iteration 3080, lr = 0.0025
I0520 20:35:17.651665 23410 solver.cpp:237] Iteration 3135, loss = 1.56383
I0520 20:35:17.651700 23410 solver.cpp:253]     Train net output #0: loss = 1.56383 (* 1 = 1.56383 loss)
I0520 20:35:17.651718 23410 sgd_solver.cpp:106] Iteration 3135, lr = 0.0025
I0520 20:35:25.737186 23410 solver.cpp:237] Iteration 3190, loss = 1.64472
I0520 20:35:25.737344 23410 solver.cpp:253]     Train net output #0: loss = 1.64472 (* 1 = 1.64472 loss)
I0520 20:35:25.737362 23410 sgd_solver.cpp:106] Iteration 3190, lr = 0.0025
I0520 20:35:33.812882 23410 solver.cpp:237] Iteration 3245, loss = 1.57896
I0520 20:35:33.812918 23410 solver.cpp:253]     Train net output #0: loss = 1.57896 (* 1 = 1.57896 loss)
I0520 20:35:33.812937 23410 sgd_solver.cpp:106] Iteration 3245, lr = 0.0025
I0520 20:35:41.885395 23410 solver.cpp:237] Iteration 3300, loss = 1.61455
I0520 20:35:41.885429 23410 solver.cpp:253]     Train net output #0: loss = 1.61455 (* 1 = 1.61455 loss)
I0520 20:35:41.885447 23410 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0520 20:35:46.143471 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_3330.caffemodel
I0520 20:35:46.284065 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_3330.solverstate
I0520 20:35:46.649472 23410 solver.cpp:341] Iteration 3333, Testing net (#0)
I0520 20:36:31.888427 23410 solver.cpp:409]     Test net output #0: accuracy = 0.743183
I0520 20:36:31.888607 23410 solver.cpp:409]     Test net output #1: loss = 0.911643 (* 1 = 0.911643 loss)
I0520 20:36:57.332351 23410 solver.cpp:237] Iteration 3355, loss = 1.58892
I0520 20:36:57.332412 23410 solver.cpp:253]     Train net output #0: loss = 1.58892 (* 1 = 1.58892 loss)
I0520 20:36:57.332430 23410 sgd_solver.cpp:106] Iteration 3355, lr = 0.0025
I0520 20:37:05.399561 23410 solver.cpp:237] Iteration 3410, loss = 1.6364
I0520 20:37:05.399720 23410 solver.cpp:253]     Train net output #0: loss = 1.6364 (* 1 = 1.6364 loss)
I0520 20:37:05.399739 23410 sgd_solver.cpp:106] Iteration 3410, lr = 0.0025
I0520 20:37:13.468251 23410 solver.cpp:237] Iteration 3465, loss = 1.64322
I0520 20:37:13.468287 23410 solver.cpp:253]     Train net output #0: loss = 1.64322 (* 1 = 1.64322 loss)
I0520 20:37:13.468307 23410 sgd_solver.cpp:106] Iteration 3465, lr = 0.0025
I0520 20:37:21.534991 23410 solver.cpp:237] Iteration 3520, loss = 1.44446
I0520 20:37:21.535027 23410 solver.cpp:253]     Train net output #0: loss = 1.44446 (* 1 = 1.44446 loss)
I0520 20:37:21.535044 23410 sgd_solver.cpp:106] Iteration 3520, lr = 0.0025
I0520 20:37:29.610821 23410 solver.cpp:237] Iteration 3575, loss = 1.41851
I0520 20:37:29.610867 23410 solver.cpp:253]     Train net output #0: loss = 1.41851 (* 1 = 1.41851 loss)
I0520 20:37:29.610893 23410 sgd_solver.cpp:106] Iteration 3575, lr = 0.0025
I0520 20:37:37.688416 23410 solver.cpp:237] Iteration 3630, loss = 1.44079
I0520 20:37:37.688583 23410 solver.cpp:253]     Train net output #0: loss = 1.44079 (* 1 = 1.44079 loss)
I0520 20:37:37.688601 23410 sgd_solver.cpp:106] Iteration 3630, lr = 0.0025
I0520 20:37:45.764966 23410 solver.cpp:237] Iteration 3685, loss = 1.5056
I0520 20:37:45.765000 23410 solver.cpp:253]     Train net output #0: loss = 1.5056 (* 1 = 1.5056 loss)
I0520 20:37:45.765018 23410 sgd_solver.cpp:106] Iteration 3685, lr = 0.0025
I0520 20:38:16.025840 23410 solver.cpp:237] Iteration 3740, loss = 1.44946
I0520 20:38:16.026012 23410 solver.cpp:253]     Train net output #0: loss = 1.44946 (* 1 = 1.44946 loss)
I0520 20:38:16.026029 23410 sgd_solver.cpp:106] Iteration 3740, lr = 0.0025
I0520 20:38:24.104607 23410 solver.cpp:237] Iteration 3795, loss = 1.56857
I0520 20:38:24.104643 23410 solver.cpp:253]     Train net output #0: loss = 1.56857 (* 1 = 1.56857 loss)
I0520 20:38:24.104662 23410 sgd_solver.cpp:106] Iteration 3795, lr = 0.0025
I0520 20:38:32.180567 23410 solver.cpp:237] Iteration 3850, loss = 1.45523
I0520 20:38:32.180601 23410 solver.cpp:253]     Train net output #0: loss = 1.45523 (* 1 = 1.45523 loss)
I0520 20:38:32.180619 23410 sgd_solver.cpp:106] Iteration 3850, lr = 0.0025
I0520 20:38:37.175886 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_3885.caffemodel
I0520 20:38:37.315853 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_3885.solverstate
I0520 20:38:40.322808 23410 solver.cpp:237] Iteration 3905, loss = 1.53316
I0520 20:38:40.322865 23410 solver.cpp:253]     Train net output #0: loss = 1.53316 (* 1 = 1.53316 loss)
I0520 20:38:40.322881 23410 sgd_solver.cpp:106] Iteration 3905, lr = 0.0025
I0520 20:38:48.401576 23410 solver.cpp:237] Iteration 3960, loss = 1.45839
I0520 20:38:48.401727 23410 solver.cpp:253]     Train net output #0: loss = 1.45839 (* 1 = 1.45839 loss)
I0520 20:38:48.401744 23410 sgd_solver.cpp:106] Iteration 3960, lr = 0.0025
I0520 20:38:56.479066 23410 solver.cpp:237] Iteration 4015, loss = 1.3707
I0520 20:38:56.479102 23410 solver.cpp:253]     Train net output #0: loss = 1.3707 (* 1 = 1.3707 loss)
I0520 20:38:56.479120 23410 sgd_solver.cpp:106] Iteration 4015, lr = 0.0025
I0520 20:39:04.555145 23410 solver.cpp:237] Iteration 4070, loss = 1.43174
I0520 20:39:04.555199 23410 solver.cpp:253]     Train net output #0: loss = 1.43174 (* 1 = 1.43174 loss)
I0520 20:39:04.555229 23410 sgd_solver.cpp:106] Iteration 4070, lr = 0.0025
I0520 20:39:34.824190 23410 solver.cpp:237] Iteration 4125, loss = 1.47098
I0520 20:39:34.824368 23410 solver.cpp:253]     Train net output #0: loss = 1.47098 (* 1 = 1.47098 loss)
I0520 20:39:34.824384 23410 sgd_solver.cpp:106] Iteration 4125, lr = 0.0025
I0520 20:39:42.904057 23410 solver.cpp:237] Iteration 4180, loss = 1.36562
I0520 20:39:42.904091 23410 solver.cpp:253]     Train net output #0: loss = 1.36562 (* 1 = 1.36562 loss)
I0520 20:39:42.904110 23410 sgd_solver.cpp:106] Iteration 4180, lr = 0.0025
I0520 20:39:50.984483 23410 solver.cpp:237] Iteration 4235, loss = 1.35164
I0520 20:39:50.984519 23410 solver.cpp:253]     Train net output #0: loss = 1.35164 (* 1 = 1.35164 loss)
I0520 20:39:50.984537 23410 sgd_solver.cpp:106] Iteration 4235, lr = 0.0025
I0520 20:39:59.062747 23410 solver.cpp:237] Iteration 4290, loss = 1.48515
I0520 20:39:59.062800 23410 solver.cpp:253]     Train net output #0: loss = 1.48515 (* 1 = 1.48515 loss)
I0520 20:39:59.062827 23410 sgd_solver.cpp:106] Iteration 4290, lr = 0.0025
I0520 20:40:07.136767 23410 solver.cpp:237] Iteration 4345, loss = 1.40485
I0520 20:40:07.136915 23410 solver.cpp:253]     Train net output #0: loss = 1.40485 (* 1 = 1.40485 loss)
I0520 20:40:07.136931 23410 sgd_solver.cpp:106] Iteration 4345, lr = 0.0025
I0520 20:40:15.213927 23410 solver.cpp:237] Iteration 4400, loss = 1.36248
I0520 20:40:15.213963 23410 solver.cpp:253]     Train net output #0: loss = 1.36248 (* 1 = 1.36248 loss)
I0520 20:40:15.213980 23410 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0520 20:40:20.942251 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_4440.caffemodel
I0520 20:40:21.082209 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_4440.solverstate
I0520 20:40:21.595100 23410 solver.cpp:341] Iteration 4444, Testing net (#0)
I0520 20:41:28.072983 23410 solver.cpp:409]     Test net output #0: accuracy = 0.765966
I0520 20:41:28.073163 23410 solver.cpp:409]     Test net output #1: loss = 0.852506 (* 1 = 0.852506 loss)
I0520 20:41:51.868484 23410 solver.cpp:237] Iteration 4455, loss = 1.44795
I0520 20:41:51.868543 23410 solver.cpp:253]     Train net output #0: loss = 1.44795 (* 1 = 1.44795 loss)
I0520 20:41:51.868563 23410 sgd_solver.cpp:106] Iteration 4455, lr = 0.0025
I0520 20:41:59.956269 23410 solver.cpp:237] Iteration 4510, loss = 1.65636
I0520 20:41:59.956445 23410 solver.cpp:253]     Train net output #0: loss = 1.65636 (* 1 = 1.65636 loss)
I0520 20:41:59.956465 23410 sgd_solver.cpp:106] Iteration 4510, lr = 0.0025
I0520 20:42:08.046563 23410 solver.cpp:237] Iteration 4565, loss = 1.42456
I0520 20:42:08.046598 23410 solver.cpp:253]     Train net output #0: loss = 1.42456 (* 1 = 1.42456 loss)
I0520 20:42:08.046622 23410 sgd_solver.cpp:106] Iteration 4565, lr = 0.0025
I0520 20:42:16.134310 23410 solver.cpp:237] Iteration 4620, loss = 1.59798
I0520 20:42:16.134346 23410 solver.cpp:253]     Train net output #0: loss = 1.59798 (* 1 = 1.59798 loss)
I0520 20:42:16.134369 23410 sgd_solver.cpp:106] Iteration 4620, lr = 0.0025
I0520 20:42:24.221766 23410 solver.cpp:237] Iteration 4675, loss = 1.51699
I0520 20:42:24.221819 23410 solver.cpp:253]     Train net output #0: loss = 1.51699 (* 1 = 1.51699 loss)
I0520 20:42:24.221848 23410 sgd_solver.cpp:106] Iteration 4675, lr = 0.0025
I0520 20:42:32.307766 23410 solver.cpp:237] Iteration 4730, loss = 1.30283
I0520 20:42:32.307917 23410 solver.cpp:253]     Train net output #0: loss = 1.30283 (* 1 = 1.30283 loss)
I0520 20:42:32.307934 23410 sgd_solver.cpp:106] Iteration 4730, lr = 0.0025
I0520 20:42:40.395119 23410 solver.cpp:237] Iteration 4785, loss = 1.5015
I0520 20:42:40.395154 23410 solver.cpp:253]     Train net output #0: loss = 1.5015 (* 1 = 1.5015 loss)
I0520 20:42:40.395174 23410 sgd_solver.cpp:106] Iteration 4785, lr = 0.0025
I0520 20:43:10.623042 23410 solver.cpp:237] Iteration 4840, loss = 1.44845
I0520 20:43:10.623227 23410 solver.cpp:253]     Train net output #0: loss = 1.44845 (* 1 = 1.44845 loss)
I0520 20:43:10.623245 23410 sgd_solver.cpp:106] Iteration 4840, lr = 0.0025
I0520 20:43:18.701395 23410 solver.cpp:237] Iteration 4895, loss = 1.42797
I0520 20:43:18.701448 23410 solver.cpp:253]     Train net output #0: loss = 1.42797 (* 1 = 1.42797 loss)
I0520 20:43:18.701473 23410 sgd_solver.cpp:106] Iteration 4895, lr = 0.0025
I0520 20:43:26.778483 23410 solver.cpp:237] Iteration 4950, loss = 1.3358
I0520 20:43:26.778518 23410 solver.cpp:253]     Train net output #0: loss = 1.3358 (* 1 = 1.3358 loss)
I0520 20:43:26.778537 23410 sgd_solver.cpp:106] Iteration 4950, lr = 0.0025
I0520 20:43:33.244957 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_4995.caffemodel
I0520 20:43:33.388075 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_4995.solverstate
I0520 20:43:34.932054 23410 solver.cpp:237] Iteration 5005, loss = 1.51186
I0520 20:43:34.932109 23410 solver.cpp:253]     Train net output #0: loss = 1.51186 (* 1 = 1.51186 loss)
I0520 20:43:34.932134 23410 sgd_solver.cpp:106] Iteration 5005, lr = 0.0025
I0520 20:43:43.012079 23410 solver.cpp:237] Iteration 5060, loss = 1.34113
I0520 20:43:43.012261 23410 solver.cpp:253]     Train net output #0: loss = 1.34113 (* 1 = 1.34113 loss)
I0520 20:43:43.012279 23410 sgd_solver.cpp:106] Iteration 5060, lr = 0.0025
I0520 20:43:51.093160 23410 solver.cpp:237] Iteration 5115, loss = 1.56976
I0520 20:43:51.093199 23410 solver.cpp:253]     Train net output #0: loss = 1.56976 (* 1 = 1.56976 loss)
I0520 20:43:51.093215 23410 sgd_solver.cpp:106] Iteration 5115, lr = 0.0025
I0520 20:43:59.176126 23410 solver.cpp:237] Iteration 5170, loss = 1.39787
I0520 20:43:59.176161 23410 solver.cpp:253]     Train net output #0: loss = 1.39787 (* 1 = 1.39787 loss)
I0520 20:43:59.176184 23410 sgd_solver.cpp:106] Iteration 5170, lr = 0.0025
I0520 20:44:29.401772 23410 solver.cpp:237] Iteration 5225, loss = 1.35779
I0520 20:44:29.401949 23410 solver.cpp:253]     Train net output #0: loss = 1.35779 (* 1 = 1.35779 loss)
I0520 20:44:29.401967 23410 sgd_solver.cpp:106] Iteration 5225, lr = 0.0025
I0520 20:44:37.478865 23410 solver.cpp:237] Iteration 5280, loss = 1.45486
I0520 20:44:37.478902 23410 solver.cpp:253]     Train net output #0: loss = 1.45486 (* 1 = 1.45486 loss)
I0520 20:44:37.478920 23410 sgd_solver.cpp:106] Iteration 5280, lr = 0.0025
I0520 20:44:45.560047 23410 solver.cpp:237] Iteration 5335, loss = 1.41067
I0520 20:44:45.560082 23410 solver.cpp:253]     Train net output #0: loss = 1.41067 (* 1 = 1.41067 loss)
I0520 20:44:45.560106 23410 sgd_solver.cpp:106] Iteration 5335, lr = 0.0025
I0520 20:44:53.640743 23410 solver.cpp:237] Iteration 5390, loss = 1.44333
I0520 20:44:53.640801 23410 solver.cpp:253]     Train net output #0: loss = 1.44333 (* 1 = 1.44333 loss)
I0520 20:44:53.640827 23410 sgd_solver.cpp:106] Iteration 5390, lr = 0.0025
I0520 20:45:01.722995 23410 solver.cpp:237] Iteration 5445, loss = 1.39232
I0520 20:45:01.723157 23410 solver.cpp:253]     Train net output #0: loss = 1.39232 (* 1 = 1.39232 loss)
I0520 20:45:01.723176 23410 sgd_solver.cpp:106] Iteration 5445, lr = 0.0025
I0520 20:45:09.803526 23410 solver.cpp:237] Iteration 5500, loss = 1.38968
I0520 20:45:09.803561 23410 solver.cpp:253]     Train net output #0: loss = 1.38968 (* 1 = 1.38968 loss)
I0520 20:45:09.803580 23410 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0520 20:45:17.003645 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_5550.caffemodel
I0520 20:45:17.147120 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_5550.solverstate
I0520 20:45:17.808326 23410 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_5555.caffemodel
I0520 20:45:17.951483 23410 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797_iter_5555.solverstate
I0520 20:45:38.927192 23410 solver.cpp:321] Iteration 5555, loss = 1.32788
I0520 20:45:38.927367 23410 solver.cpp:341] Iteration 5555, Testing net (#0)
I0520 20:46:24.413579 23410 solver.cpp:409]     Test net output #0: accuracy = 0.797224
I0520 20:46:24.413753 23410 solver.cpp:409]     Test net output #1: loss = 0.716758 (* 1 = 0.716758 loss)
I0520 20:46:24.413770 23410 solver.cpp:326] Optimization Done.
I0520 20:46:24.413784 23410 caffe.cpp:215] Optimization Done.
Application 11235171 resources: utime ~1278s, stime ~231s, Rss ~5329160, inblocks ~3744348, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_270_2016-05-20T11.20.42.587797.solver"
	User time (seconds): 0.60
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:12.88
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15072
	Voluntary context switches: 2987
	Involuntary context switches: 202
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
