2806024
I0520 23:11:32.177151 24002 caffe.cpp:184] Using GPUs 0
I0520 23:11:32.624083 24002 solver.cpp:48] Initializing solver from parameters: 
test_iter: 365
test_interval: 731
base_lr: 0.0025
display: 36
max_iter: 3658
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 365
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379.prototxt"
I0520 23:11:32.625953 24002 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379.prototxt
I0520 23:11:32.649600 24002 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 23:11:32.649659 24002 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 23:11:32.650003 24002 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 410
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:11:32.650180 24002 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:11:32.650205 24002 net.cpp:106] Creating Layer data_hdf5
I0520 23:11:32.650219 24002 net.cpp:411] data_hdf5 -> data
I0520 23:11:32.650254 24002 net.cpp:411] data_hdf5 -> label
I0520 23:11:32.650285 24002 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 23:11:32.651564 24002 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 23:11:32.653758 24002 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 23:11:54.195036 24002 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 23:11:54.200188 24002 net.cpp:150] Setting up data_hdf5
I0520 23:11:54.200229 24002 net.cpp:157] Top shape: 410 1 127 50 (2603500)
I0520 23:11:54.200244 24002 net.cpp:157] Top shape: 410 (410)
I0520 23:11:54.200256 24002 net.cpp:165] Memory required for data: 10415640
I0520 23:11:54.200269 24002 layer_factory.hpp:77] Creating layer conv1
I0520 23:11:54.200304 24002 net.cpp:106] Creating Layer conv1
I0520 23:11:54.200315 24002 net.cpp:454] conv1 <- data
I0520 23:11:54.200337 24002 net.cpp:411] conv1 -> conv1
I0520 23:11:54.562043 24002 net.cpp:150] Setting up conv1
I0520 23:11:54.562088 24002 net.cpp:157] Top shape: 410 12 120 48 (28339200)
I0520 23:11:54.562099 24002 net.cpp:165] Memory required for data: 123772440
I0520 23:11:54.562131 24002 layer_factory.hpp:77] Creating layer relu1
I0520 23:11:54.562152 24002 net.cpp:106] Creating Layer relu1
I0520 23:11:54.562163 24002 net.cpp:454] relu1 <- conv1
I0520 23:11:54.562177 24002 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:11:54.562696 24002 net.cpp:150] Setting up relu1
I0520 23:11:54.562713 24002 net.cpp:157] Top shape: 410 12 120 48 (28339200)
I0520 23:11:54.562724 24002 net.cpp:165] Memory required for data: 237129240
I0520 23:11:54.562734 24002 layer_factory.hpp:77] Creating layer pool1
I0520 23:11:54.562750 24002 net.cpp:106] Creating Layer pool1
I0520 23:11:54.562760 24002 net.cpp:454] pool1 <- conv1
I0520 23:11:54.562773 24002 net.cpp:411] pool1 -> pool1
I0520 23:11:54.562855 24002 net.cpp:150] Setting up pool1
I0520 23:11:54.562868 24002 net.cpp:157] Top shape: 410 12 60 48 (14169600)
I0520 23:11:54.562878 24002 net.cpp:165] Memory required for data: 293807640
I0520 23:11:54.562888 24002 layer_factory.hpp:77] Creating layer conv2
I0520 23:11:54.562909 24002 net.cpp:106] Creating Layer conv2
I0520 23:11:54.562919 24002 net.cpp:454] conv2 <- pool1
I0520 23:11:54.562932 24002 net.cpp:411] conv2 -> conv2
I0520 23:11:54.565609 24002 net.cpp:150] Setting up conv2
I0520 23:11:54.565632 24002 net.cpp:157] Top shape: 410 20 54 46 (20368800)
I0520 23:11:54.565644 24002 net.cpp:165] Memory required for data: 375282840
I0520 23:11:54.565662 24002 layer_factory.hpp:77] Creating layer relu2
I0520 23:11:54.565676 24002 net.cpp:106] Creating Layer relu2
I0520 23:11:54.565686 24002 net.cpp:454] relu2 <- conv2
I0520 23:11:54.565699 24002 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:11:54.566030 24002 net.cpp:150] Setting up relu2
I0520 23:11:54.566045 24002 net.cpp:157] Top shape: 410 20 54 46 (20368800)
I0520 23:11:54.566054 24002 net.cpp:165] Memory required for data: 456758040
I0520 23:11:54.566064 24002 layer_factory.hpp:77] Creating layer pool2
I0520 23:11:54.566077 24002 net.cpp:106] Creating Layer pool2
I0520 23:11:54.566087 24002 net.cpp:454] pool2 <- conv2
I0520 23:11:54.566112 24002 net.cpp:411] pool2 -> pool2
I0520 23:11:54.566181 24002 net.cpp:150] Setting up pool2
I0520 23:11:54.566195 24002 net.cpp:157] Top shape: 410 20 27 46 (10184400)
I0520 23:11:54.566205 24002 net.cpp:165] Memory required for data: 497495640
I0520 23:11:54.566212 24002 layer_factory.hpp:77] Creating layer conv3
I0520 23:11:54.566231 24002 net.cpp:106] Creating Layer conv3
I0520 23:11:54.566241 24002 net.cpp:454] conv3 <- pool2
I0520 23:11:54.566256 24002 net.cpp:411] conv3 -> conv3
I0520 23:11:54.568217 24002 net.cpp:150] Setting up conv3
I0520 23:11:54.568239 24002 net.cpp:157] Top shape: 410 28 22 44 (11112640)
I0520 23:11:54.568253 24002 net.cpp:165] Memory required for data: 541946200
I0520 23:11:54.568271 24002 layer_factory.hpp:77] Creating layer relu3
I0520 23:11:54.568287 24002 net.cpp:106] Creating Layer relu3
I0520 23:11:54.568296 24002 net.cpp:454] relu3 <- conv3
I0520 23:11:54.568310 24002 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:11:54.568784 24002 net.cpp:150] Setting up relu3
I0520 23:11:54.568801 24002 net.cpp:157] Top shape: 410 28 22 44 (11112640)
I0520 23:11:54.568811 24002 net.cpp:165] Memory required for data: 586396760
I0520 23:11:54.568822 24002 layer_factory.hpp:77] Creating layer pool3
I0520 23:11:54.568835 24002 net.cpp:106] Creating Layer pool3
I0520 23:11:54.568845 24002 net.cpp:454] pool3 <- conv3
I0520 23:11:54.568857 24002 net.cpp:411] pool3 -> pool3
I0520 23:11:54.568925 24002 net.cpp:150] Setting up pool3
I0520 23:11:54.568938 24002 net.cpp:157] Top shape: 410 28 11 44 (5556320)
I0520 23:11:54.568948 24002 net.cpp:165] Memory required for data: 608622040
I0520 23:11:54.568958 24002 layer_factory.hpp:77] Creating layer conv4
I0520 23:11:54.568975 24002 net.cpp:106] Creating Layer conv4
I0520 23:11:54.568986 24002 net.cpp:454] conv4 <- pool3
I0520 23:11:54.569000 24002 net.cpp:411] conv4 -> conv4
I0520 23:11:54.571764 24002 net.cpp:150] Setting up conv4
I0520 23:11:54.571792 24002 net.cpp:157] Top shape: 410 36 6 42 (3719520)
I0520 23:11:54.571804 24002 net.cpp:165] Memory required for data: 623500120
I0520 23:11:54.571820 24002 layer_factory.hpp:77] Creating layer relu4
I0520 23:11:54.571833 24002 net.cpp:106] Creating Layer relu4
I0520 23:11:54.571843 24002 net.cpp:454] relu4 <- conv4
I0520 23:11:54.571857 24002 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:11:54.572329 24002 net.cpp:150] Setting up relu4
I0520 23:11:54.572345 24002 net.cpp:157] Top shape: 410 36 6 42 (3719520)
I0520 23:11:54.572355 24002 net.cpp:165] Memory required for data: 638378200
I0520 23:11:54.572365 24002 layer_factory.hpp:77] Creating layer pool4
I0520 23:11:54.572378 24002 net.cpp:106] Creating Layer pool4
I0520 23:11:54.572388 24002 net.cpp:454] pool4 <- conv4
I0520 23:11:54.572402 24002 net.cpp:411] pool4 -> pool4
I0520 23:11:54.572469 24002 net.cpp:150] Setting up pool4
I0520 23:11:54.572484 24002 net.cpp:157] Top shape: 410 36 3 42 (1859760)
I0520 23:11:54.572494 24002 net.cpp:165] Memory required for data: 645817240
I0520 23:11:54.572504 24002 layer_factory.hpp:77] Creating layer ip1
I0520 23:11:54.572525 24002 net.cpp:106] Creating Layer ip1
I0520 23:11:54.572535 24002 net.cpp:454] ip1 <- pool4
I0520 23:11:54.572548 24002 net.cpp:411] ip1 -> ip1
I0520 23:11:54.588013 24002 net.cpp:150] Setting up ip1
I0520 23:11:54.588040 24002 net.cpp:157] Top shape: 410 196 (80360)
I0520 23:11:54.588054 24002 net.cpp:165] Memory required for data: 646138680
I0520 23:11:54.588076 24002 layer_factory.hpp:77] Creating layer relu5
I0520 23:11:54.588091 24002 net.cpp:106] Creating Layer relu5
I0520 23:11:54.588101 24002 net.cpp:454] relu5 <- ip1
I0520 23:11:54.588115 24002 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:11:54.588456 24002 net.cpp:150] Setting up relu5
I0520 23:11:54.588470 24002 net.cpp:157] Top shape: 410 196 (80360)
I0520 23:11:54.588480 24002 net.cpp:165] Memory required for data: 646460120
I0520 23:11:54.588491 24002 layer_factory.hpp:77] Creating layer drop1
I0520 23:11:54.588512 24002 net.cpp:106] Creating Layer drop1
I0520 23:11:54.588522 24002 net.cpp:454] drop1 <- ip1
I0520 23:11:54.588547 24002 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:11:54.588594 24002 net.cpp:150] Setting up drop1
I0520 23:11:54.588608 24002 net.cpp:157] Top shape: 410 196 (80360)
I0520 23:11:54.588618 24002 net.cpp:165] Memory required for data: 646781560
I0520 23:11:54.588627 24002 layer_factory.hpp:77] Creating layer ip2
I0520 23:11:54.588645 24002 net.cpp:106] Creating Layer ip2
I0520 23:11:54.588655 24002 net.cpp:454] ip2 <- ip1
I0520 23:11:54.588668 24002 net.cpp:411] ip2 -> ip2
I0520 23:11:54.589131 24002 net.cpp:150] Setting up ip2
I0520 23:11:54.589145 24002 net.cpp:157] Top shape: 410 98 (40180)
I0520 23:11:54.589155 24002 net.cpp:165] Memory required for data: 646942280
I0520 23:11:54.589170 24002 layer_factory.hpp:77] Creating layer relu6
I0520 23:11:54.589182 24002 net.cpp:106] Creating Layer relu6
I0520 23:11:54.589192 24002 net.cpp:454] relu6 <- ip2
I0520 23:11:54.589205 24002 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:11:54.589718 24002 net.cpp:150] Setting up relu6
I0520 23:11:54.589735 24002 net.cpp:157] Top shape: 410 98 (40180)
I0520 23:11:54.589745 24002 net.cpp:165] Memory required for data: 647103000
I0520 23:11:54.589756 24002 layer_factory.hpp:77] Creating layer drop2
I0520 23:11:54.589768 24002 net.cpp:106] Creating Layer drop2
I0520 23:11:54.589778 24002 net.cpp:454] drop2 <- ip2
I0520 23:11:54.589790 24002 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:11:54.589833 24002 net.cpp:150] Setting up drop2
I0520 23:11:54.589845 24002 net.cpp:157] Top shape: 410 98 (40180)
I0520 23:11:54.589855 24002 net.cpp:165] Memory required for data: 647263720
I0520 23:11:54.589865 24002 layer_factory.hpp:77] Creating layer ip3
I0520 23:11:54.589879 24002 net.cpp:106] Creating Layer ip3
I0520 23:11:54.589890 24002 net.cpp:454] ip3 <- ip2
I0520 23:11:54.589900 24002 net.cpp:411] ip3 -> ip3
I0520 23:11:54.590111 24002 net.cpp:150] Setting up ip3
I0520 23:11:54.590124 24002 net.cpp:157] Top shape: 410 11 (4510)
I0520 23:11:54.590134 24002 net.cpp:165] Memory required for data: 647281760
I0520 23:11:54.590149 24002 layer_factory.hpp:77] Creating layer drop3
I0520 23:11:54.590162 24002 net.cpp:106] Creating Layer drop3
I0520 23:11:54.590172 24002 net.cpp:454] drop3 <- ip3
I0520 23:11:54.590183 24002 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:11:54.590224 24002 net.cpp:150] Setting up drop3
I0520 23:11:54.590235 24002 net.cpp:157] Top shape: 410 11 (4510)
I0520 23:11:54.590246 24002 net.cpp:165] Memory required for data: 647299800
I0520 23:11:54.590256 24002 layer_factory.hpp:77] Creating layer loss
I0520 23:11:54.590276 24002 net.cpp:106] Creating Layer loss
I0520 23:11:54.590286 24002 net.cpp:454] loss <- ip3
I0520 23:11:54.590296 24002 net.cpp:454] loss <- label
I0520 23:11:54.590309 24002 net.cpp:411] loss -> loss
I0520 23:11:54.590327 24002 layer_factory.hpp:77] Creating layer loss
I0520 23:11:54.590971 24002 net.cpp:150] Setting up loss
I0520 23:11:54.590992 24002 net.cpp:157] Top shape: (1)
I0520 23:11:54.591006 24002 net.cpp:160]     with loss weight 1
I0520 23:11:54.591049 24002 net.cpp:165] Memory required for data: 647299804
I0520 23:11:54.591060 24002 net.cpp:226] loss needs backward computation.
I0520 23:11:54.591071 24002 net.cpp:226] drop3 needs backward computation.
I0520 23:11:54.591080 24002 net.cpp:226] ip3 needs backward computation.
I0520 23:11:54.591089 24002 net.cpp:226] drop2 needs backward computation.
I0520 23:11:54.591099 24002 net.cpp:226] relu6 needs backward computation.
I0520 23:11:54.591109 24002 net.cpp:226] ip2 needs backward computation.
I0520 23:11:54.591119 24002 net.cpp:226] drop1 needs backward computation.
I0520 23:11:54.591128 24002 net.cpp:226] relu5 needs backward computation.
I0520 23:11:54.591138 24002 net.cpp:226] ip1 needs backward computation.
I0520 23:11:54.591148 24002 net.cpp:226] pool4 needs backward computation.
I0520 23:11:54.591158 24002 net.cpp:226] relu4 needs backward computation.
I0520 23:11:54.591168 24002 net.cpp:226] conv4 needs backward computation.
I0520 23:11:54.591179 24002 net.cpp:226] pool3 needs backward computation.
I0520 23:11:54.591197 24002 net.cpp:226] relu3 needs backward computation.
I0520 23:11:54.591207 24002 net.cpp:226] conv3 needs backward computation.
I0520 23:11:54.591218 24002 net.cpp:226] pool2 needs backward computation.
I0520 23:11:54.591228 24002 net.cpp:226] relu2 needs backward computation.
I0520 23:11:54.591238 24002 net.cpp:226] conv2 needs backward computation.
I0520 23:11:54.591251 24002 net.cpp:226] pool1 needs backward computation.
I0520 23:11:54.591261 24002 net.cpp:226] relu1 needs backward computation.
I0520 23:11:54.591271 24002 net.cpp:226] conv1 needs backward computation.
I0520 23:11:54.591282 24002 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:11:54.591292 24002 net.cpp:270] This network produces output loss
I0520 23:11:54.591316 24002 net.cpp:283] Network initialization done.
I0520 23:11:54.593231 24002 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379.prototxt
I0520 23:11:54.593302 24002 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 23:11:54.593657 24002 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 410
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:11:54.593847 24002 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:11:54.593863 24002 net.cpp:106] Creating Layer data_hdf5
I0520 23:11:54.593874 24002 net.cpp:411] data_hdf5 -> data
I0520 23:11:54.593891 24002 net.cpp:411] data_hdf5 -> label
I0520 23:11:54.593907 24002 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 23:11:54.595121 24002 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 23:12:15.916797 24002 net.cpp:150] Setting up data_hdf5
I0520 23:12:15.916962 24002 net.cpp:157] Top shape: 410 1 127 50 (2603500)
I0520 23:12:15.916977 24002 net.cpp:157] Top shape: 410 (410)
I0520 23:12:15.916990 24002 net.cpp:165] Memory required for data: 10415640
I0520 23:12:15.917002 24002 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 23:12:15.917032 24002 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 23:12:15.917043 24002 net.cpp:454] label_data_hdf5_1_split <- label
I0520 23:12:15.917058 24002 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 23:12:15.917079 24002 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 23:12:15.917153 24002 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 23:12:15.917167 24002 net.cpp:157] Top shape: 410 (410)
I0520 23:12:15.917178 24002 net.cpp:157] Top shape: 410 (410)
I0520 23:12:15.917188 24002 net.cpp:165] Memory required for data: 10418920
I0520 23:12:15.917199 24002 layer_factory.hpp:77] Creating layer conv1
I0520 23:12:15.917220 24002 net.cpp:106] Creating Layer conv1
I0520 23:12:15.917300 24002 net.cpp:454] conv1 <- data
I0520 23:12:15.917316 24002 net.cpp:411] conv1 -> conv1
I0520 23:12:15.919240 24002 net.cpp:150] Setting up conv1
I0520 23:12:15.919265 24002 net.cpp:157] Top shape: 410 12 120 48 (28339200)
I0520 23:12:15.919276 24002 net.cpp:165] Memory required for data: 123775720
I0520 23:12:15.919297 24002 layer_factory.hpp:77] Creating layer relu1
I0520 23:12:15.919312 24002 net.cpp:106] Creating Layer relu1
I0520 23:12:15.919322 24002 net.cpp:454] relu1 <- conv1
I0520 23:12:15.919337 24002 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:12:15.919843 24002 net.cpp:150] Setting up relu1
I0520 23:12:15.919860 24002 net.cpp:157] Top shape: 410 12 120 48 (28339200)
I0520 23:12:15.919870 24002 net.cpp:165] Memory required for data: 237132520
I0520 23:12:15.919880 24002 layer_factory.hpp:77] Creating layer pool1
I0520 23:12:15.919898 24002 net.cpp:106] Creating Layer pool1
I0520 23:12:15.919908 24002 net.cpp:454] pool1 <- conv1
I0520 23:12:15.919920 24002 net.cpp:411] pool1 -> pool1
I0520 23:12:15.919996 24002 net.cpp:150] Setting up pool1
I0520 23:12:15.920009 24002 net.cpp:157] Top shape: 410 12 60 48 (14169600)
I0520 23:12:15.920018 24002 net.cpp:165] Memory required for data: 293810920
I0520 23:12:15.920030 24002 layer_factory.hpp:77] Creating layer conv2
I0520 23:12:15.920048 24002 net.cpp:106] Creating Layer conv2
I0520 23:12:15.920059 24002 net.cpp:454] conv2 <- pool1
I0520 23:12:15.920073 24002 net.cpp:411] conv2 -> conv2
I0520 23:12:15.921983 24002 net.cpp:150] Setting up conv2
I0520 23:12:15.922005 24002 net.cpp:157] Top shape: 410 20 54 46 (20368800)
I0520 23:12:15.922018 24002 net.cpp:165] Memory required for data: 375286120
I0520 23:12:15.922035 24002 layer_factory.hpp:77] Creating layer relu2
I0520 23:12:15.922049 24002 net.cpp:106] Creating Layer relu2
I0520 23:12:15.922060 24002 net.cpp:454] relu2 <- conv2
I0520 23:12:15.922071 24002 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:12:15.922405 24002 net.cpp:150] Setting up relu2
I0520 23:12:15.922418 24002 net.cpp:157] Top shape: 410 20 54 46 (20368800)
I0520 23:12:15.922428 24002 net.cpp:165] Memory required for data: 456761320
I0520 23:12:15.922438 24002 layer_factory.hpp:77] Creating layer pool2
I0520 23:12:15.922451 24002 net.cpp:106] Creating Layer pool2
I0520 23:12:15.922461 24002 net.cpp:454] pool2 <- conv2
I0520 23:12:15.922473 24002 net.cpp:411] pool2 -> pool2
I0520 23:12:15.922544 24002 net.cpp:150] Setting up pool2
I0520 23:12:15.922557 24002 net.cpp:157] Top shape: 410 20 27 46 (10184400)
I0520 23:12:15.922566 24002 net.cpp:165] Memory required for data: 497498920
I0520 23:12:15.922576 24002 layer_factory.hpp:77] Creating layer conv3
I0520 23:12:15.922595 24002 net.cpp:106] Creating Layer conv3
I0520 23:12:15.922606 24002 net.cpp:454] conv3 <- pool2
I0520 23:12:15.922621 24002 net.cpp:411] conv3 -> conv3
I0520 23:12:15.924592 24002 net.cpp:150] Setting up conv3
I0520 23:12:15.924615 24002 net.cpp:157] Top shape: 410 28 22 44 (11112640)
I0520 23:12:15.924628 24002 net.cpp:165] Memory required for data: 541949480
I0520 23:12:15.924661 24002 layer_factory.hpp:77] Creating layer relu3
I0520 23:12:15.924675 24002 net.cpp:106] Creating Layer relu3
I0520 23:12:15.924685 24002 net.cpp:454] relu3 <- conv3
I0520 23:12:15.924698 24002 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:12:15.925170 24002 net.cpp:150] Setting up relu3
I0520 23:12:15.925187 24002 net.cpp:157] Top shape: 410 28 22 44 (11112640)
I0520 23:12:15.925197 24002 net.cpp:165] Memory required for data: 586400040
I0520 23:12:15.925207 24002 layer_factory.hpp:77] Creating layer pool3
I0520 23:12:15.925220 24002 net.cpp:106] Creating Layer pool3
I0520 23:12:15.925232 24002 net.cpp:454] pool3 <- conv3
I0520 23:12:15.925246 24002 net.cpp:411] pool3 -> pool3
I0520 23:12:15.925318 24002 net.cpp:150] Setting up pool3
I0520 23:12:15.925330 24002 net.cpp:157] Top shape: 410 28 11 44 (5556320)
I0520 23:12:15.925340 24002 net.cpp:165] Memory required for data: 608625320
I0520 23:12:15.925350 24002 layer_factory.hpp:77] Creating layer conv4
I0520 23:12:15.925365 24002 net.cpp:106] Creating Layer conv4
I0520 23:12:15.925376 24002 net.cpp:454] conv4 <- pool3
I0520 23:12:15.925390 24002 net.cpp:411] conv4 -> conv4
I0520 23:12:15.927443 24002 net.cpp:150] Setting up conv4
I0520 23:12:15.927464 24002 net.cpp:157] Top shape: 410 36 6 42 (3719520)
I0520 23:12:15.927479 24002 net.cpp:165] Memory required for data: 623503400
I0520 23:12:15.927494 24002 layer_factory.hpp:77] Creating layer relu4
I0520 23:12:15.927507 24002 net.cpp:106] Creating Layer relu4
I0520 23:12:15.927517 24002 net.cpp:454] relu4 <- conv4
I0520 23:12:15.927530 24002 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:12:15.928001 24002 net.cpp:150] Setting up relu4
I0520 23:12:15.928017 24002 net.cpp:157] Top shape: 410 36 6 42 (3719520)
I0520 23:12:15.928027 24002 net.cpp:165] Memory required for data: 638381480
I0520 23:12:15.928037 24002 layer_factory.hpp:77] Creating layer pool4
I0520 23:12:15.928050 24002 net.cpp:106] Creating Layer pool4
I0520 23:12:15.928061 24002 net.cpp:454] pool4 <- conv4
I0520 23:12:15.928073 24002 net.cpp:411] pool4 -> pool4
I0520 23:12:15.928179 24002 net.cpp:150] Setting up pool4
I0520 23:12:15.928194 24002 net.cpp:157] Top shape: 410 36 3 42 (1859760)
I0520 23:12:15.928203 24002 net.cpp:165] Memory required for data: 645820520
I0520 23:12:15.928213 24002 layer_factory.hpp:77] Creating layer ip1
I0520 23:12:15.928228 24002 net.cpp:106] Creating Layer ip1
I0520 23:12:15.928238 24002 net.cpp:454] ip1 <- pool4
I0520 23:12:15.928251 24002 net.cpp:411] ip1 -> ip1
I0520 23:12:15.943706 24002 net.cpp:150] Setting up ip1
I0520 23:12:15.943733 24002 net.cpp:157] Top shape: 410 196 (80360)
I0520 23:12:15.943745 24002 net.cpp:165] Memory required for data: 646141960
I0520 23:12:15.943768 24002 layer_factory.hpp:77] Creating layer relu5
I0520 23:12:15.943783 24002 net.cpp:106] Creating Layer relu5
I0520 23:12:15.943794 24002 net.cpp:454] relu5 <- ip1
I0520 23:12:15.943807 24002 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:12:15.944152 24002 net.cpp:150] Setting up relu5
I0520 23:12:15.944166 24002 net.cpp:157] Top shape: 410 196 (80360)
I0520 23:12:15.944176 24002 net.cpp:165] Memory required for data: 646463400
I0520 23:12:15.944186 24002 layer_factory.hpp:77] Creating layer drop1
I0520 23:12:15.944205 24002 net.cpp:106] Creating Layer drop1
I0520 23:12:15.944214 24002 net.cpp:454] drop1 <- ip1
I0520 23:12:15.944228 24002 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:12:15.944272 24002 net.cpp:150] Setting up drop1
I0520 23:12:15.944285 24002 net.cpp:157] Top shape: 410 196 (80360)
I0520 23:12:15.944294 24002 net.cpp:165] Memory required for data: 646784840
I0520 23:12:15.944305 24002 layer_factory.hpp:77] Creating layer ip2
I0520 23:12:15.944319 24002 net.cpp:106] Creating Layer ip2
I0520 23:12:15.944329 24002 net.cpp:454] ip2 <- ip1
I0520 23:12:15.944342 24002 net.cpp:411] ip2 -> ip2
I0520 23:12:15.944823 24002 net.cpp:150] Setting up ip2
I0520 23:12:15.944836 24002 net.cpp:157] Top shape: 410 98 (40180)
I0520 23:12:15.944845 24002 net.cpp:165] Memory required for data: 646945560
I0520 23:12:15.944874 24002 layer_factory.hpp:77] Creating layer relu6
I0520 23:12:15.944886 24002 net.cpp:106] Creating Layer relu6
I0520 23:12:15.944896 24002 net.cpp:454] relu6 <- ip2
I0520 23:12:15.944910 24002 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:12:15.945441 24002 net.cpp:150] Setting up relu6
I0520 23:12:15.945456 24002 net.cpp:157] Top shape: 410 98 (40180)
I0520 23:12:15.945466 24002 net.cpp:165] Memory required for data: 647106280
I0520 23:12:15.945477 24002 layer_factory.hpp:77] Creating layer drop2
I0520 23:12:15.945490 24002 net.cpp:106] Creating Layer drop2
I0520 23:12:15.945500 24002 net.cpp:454] drop2 <- ip2
I0520 23:12:15.945513 24002 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:12:15.945557 24002 net.cpp:150] Setting up drop2
I0520 23:12:15.945570 24002 net.cpp:157] Top shape: 410 98 (40180)
I0520 23:12:15.945580 24002 net.cpp:165] Memory required for data: 647267000
I0520 23:12:15.945590 24002 layer_factory.hpp:77] Creating layer ip3
I0520 23:12:15.945605 24002 net.cpp:106] Creating Layer ip3
I0520 23:12:15.945614 24002 net.cpp:454] ip3 <- ip2
I0520 23:12:15.945628 24002 net.cpp:411] ip3 -> ip3
I0520 23:12:15.945852 24002 net.cpp:150] Setting up ip3
I0520 23:12:15.945865 24002 net.cpp:157] Top shape: 410 11 (4510)
I0520 23:12:15.945875 24002 net.cpp:165] Memory required for data: 647285040
I0520 23:12:15.945890 24002 layer_factory.hpp:77] Creating layer drop3
I0520 23:12:15.945904 24002 net.cpp:106] Creating Layer drop3
I0520 23:12:15.945914 24002 net.cpp:454] drop3 <- ip3
I0520 23:12:15.945926 24002 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:12:15.945967 24002 net.cpp:150] Setting up drop3
I0520 23:12:15.945981 24002 net.cpp:157] Top shape: 410 11 (4510)
I0520 23:12:15.945991 24002 net.cpp:165] Memory required for data: 647303080
I0520 23:12:15.946001 24002 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 23:12:15.946013 24002 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 23:12:15.946022 24002 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 23:12:15.946035 24002 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 23:12:15.946050 24002 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 23:12:15.946125 24002 net.cpp:150] Setting up ip3_drop3_0_split
I0520 23:12:15.946138 24002 net.cpp:157] Top shape: 410 11 (4510)
I0520 23:12:15.946151 24002 net.cpp:157] Top shape: 410 11 (4510)
I0520 23:12:15.946161 24002 net.cpp:165] Memory required for data: 647339160
I0520 23:12:15.946171 24002 layer_factory.hpp:77] Creating layer accuracy
I0520 23:12:15.946193 24002 net.cpp:106] Creating Layer accuracy
I0520 23:12:15.946203 24002 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 23:12:15.946215 24002 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 23:12:15.946228 24002 net.cpp:411] accuracy -> accuracy
I0520 23:12:15.946252 24002 net.cpp:150] Setting up accuracy
I0520 23:12:15.946265 24002 net.cpp:157] Top shape: (1)
I0520 23:12:15.946275 24002 net.cpp:165] Memory required for data: 647339164
I0520 23:12:15.946285 24002 layer_factory.hpp:77] Creating layer loss
I0520 23:12:15.946300 24002 net.cpp:106] Creating Layer loss
I0520 23:12:15.946310 24002 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 23:12:15.946321 24002 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 23:12:15.946334 24002 net.cpp:411] loss -> loss
I0520 23:12:15.946352 24002 layer_factory.hpp:77] Creating layer loss
I0520 23:12:15.946841 24002 net.cpp:150] Setting up loss
I0520 23:12:15.946854 24002 net.cpp:157] Top shape: (1)
I0520 23:12:15.946864 24002 net.cpp:160]     with loss weight 1
I0520 23:12:15.946882 24002 net.cpp:165] Memory required for data: 647339168
I0520 23:12:15.946893 24002 net.cpp:226] loss needs backward computation.
I0520 23:12:15.946904 24002 net.cpp:228] accuracy does not need backward computation.
I0520 23:12:15.946915 24002 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 23:12:15.946925 24002 net.cpp:226] drop3 needs backward computation.
I0520 23:12:15.946936 24002 net.cpp:226] ip3 needs backward computation.
I0520 23:12:15.946948 24002 net.cpp:226] drop2 needs backward computation.
I0520 23:12:15.946965 24002 net.cpp:226] relu6 needs backward computation.
I0520 23:12:15.946975 24002 net.cpp:226] ip2 needs backward computation.
I0520 23:12:15.946985 24002 net.cpp:226] drop1 needs backward computation.
I0520 23:12:15.946995 24002 net.cpp:226] relu5 needs backward computation.
I0520 23:12:15.947005 24002 net.cpp:226] ip1 needs backward computation.
I0520 23:12:15.947016 24002 net.cpp:226] pool4 needs backward computation.
I0520 23:12:15.947026 24002 net.cpp:226] relu4 needs backward computation.
I0520 23:12:15.947034 24002 net.cpp:226] conv4 needs backward computation.
I0520 23:12:15.947044 24002 net.cpp:226] pool3 needs backward computation.
I0520 23:12:15.947055 24002 net.cpp:226] relu3 needs backward computation.
I0520 23:12:15.947065 24002 net.cpp:226] conv3 needs backward computation.
I0520 23:12:15.947077 24002 net.cpp:226] pool2 needs backward computation.
I0520 23:12:15.947087 24002 net.cpp:226] relu2 needs backward computation.
I0520 23:12:15.947096 24002 net.cpp:226] conv2 needs backward computation.
I0520 23:12:15.947106 24002 net.cpp:226] pool1 needs backward computation.
I0520 23:12:15.947118 24002 net.cpp:226] relu1 needs backward computation.
I0520 23:12:15.947127 24002 net.cpp:226] conv1 needs backward computation.
I0520 23:12:15.947139 24002 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 23:12:15.947151 24002 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:12:15.947161 24002 net.cpp:270] This network produces output accuracy
I0520 23:12:15.947171 24002 net.cpp:270] This network produces output loss
I0520 23:12:15.947201 24002 net.cpp:283] Network initialization done.
I0520 23:12:15.947334 24002 solver.cpp:60] Solver scaffolding done.
I0520 23:12:15.948468 24002 caffe.cpp:212] Starting Optimization
I0520 23:12:15.948487 24002 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 23:12:15.948500 24002 solver.cpp:289] Learning Rate Policy: fixed
I0520 23:12:15.949795 24002 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 23:13:02.095826 24002 solver.cpp:409]     Test net output #0: accuracy = 0.0690947
I0520 23:13:02.095988 24002 solver.cpp:409]     Test net output #1: loss = 2.39818 (* 1 = 2.39818 loss)
I0520 23:13:02.179980 24002 solver.cpp:237] Iteration 0, loss = 2.39879
I0520 23:13:02.180016 24002 solver.cpp:253]     Train net output #0: loss = 2.39879 (* 1 = 2.39879 loss)
I0520 23:13:02.180035 24002 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 23:13:10.192543 24002 solver.cpp:237] Iteration 36, loss = 2.36957
I0520 23:13:10.192579 24002 solver.cpp:253]     Train net output #0: loss = 2.36957 (* 1 = 2.36957 loss)
I0520 23:13:10.192595 24002 sgd_solver.cpp:106] Iteration 36, lr = 0.0025
I0520 23:13:18.199179 24002 solver.cpp:237] Iteration 72, loss = 2.35051
I0520 23:13:18.199224 24002 solver.cpp:253]     Train net output #0: loss = 2.35051 (* 1 = 2.35051 loss)
I0520 23:13:18.199240 24002 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0520 23:13:26.201411 24002 solver.cpp:237] Iteration 108, loss = 2.33193
I0520 23:13:26.201443 24002 solver.cpp:253]     Train net output #0: loss = 2.33193 (* 1 = 2.33193 loss)
I0520 23:13:26.201460 24002 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0520 23:13:34.202038 24002 solver.cpp:237] Iteration 144, loss = 2.32355
I0520 23:13:34.202179 24002 solver.cpp:253]     Train net output #0: loss = 2.32355 (* 1 = 2.32355 loss)
I0520 23:13:34.202193 24002 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0520 23:13:42.201519 24002 solver.cpp:237] Iteration 180, loss = 2.31905
I0520 23:13:42.201555 24002 solver.cpp:253]     Train net output #0: loss = 2.31905 (* 1 = 2.31905 loss)
I0520 23:13:42.201578 24002 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0520 23:13:50.206089 24002 solver.cpp:237] Iteration 216, loss = 2.30766
I0520 23:13:50.206122 24002 solver.cpp:253]     Train net output #0: loss = 2.30766 (* 1 = 2.30766 loss)
I0520 23:13:50.206140 24002 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0520 23:14:20.312602 24002 solver.cpp:237] Iteration 252, loss = 2.29917
I0520 23:14:20.312764 24002 solver.cpp:253]     Train net output #0: loss = 2.29917 (* 1 = 2.29917 loss)
I0520 23:14:20.312780 24002 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0520 23:14:28.323778 24002 solver.cpp:237] Iteration 288, loss = 2.28181
I0520 23:14:28.323812 24002 solver.cpp:253]     Train net output #0: loss = 2.28181 (* 1 = 2.28181 loss)
I0520 23:14:28.323829 24002 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0520 23:14:36.327112 24002 solver.cpp:237] Iteration 324, loss = 2.22732
I0520 23:14:36.327153 24002 solver.cpp:253]     Train net output #0: loss = 2.22732 (* 1 = 2.22732 loss)
I0520 23:14:36.327174 24002 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0520 23:14:44.340329 24002 solver.cpp:237] Iteration 360, loss = 2.17514
I0520 23:14:44.340363 24002 solver.cpp:253]     Train net output #0: loss = 2.17514 (* 1 = 2.17514 loss)
I0520 23:14:44.340378 24002 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0520 23:14:45.231107 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_365.caffemodel
I0520 23:14:45.494913 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_365.solverstate
I0520 23:14:52.494087 24002 solver.cpp:237] Iteration 396, loss = 2.11072
I0520 23:14:52.494240 24002 solver.cpp:253]     Train net output #0: loss = 2.11072 (* 1 = 2.11072 loss)
I0520 23:14:52.494253 24002 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0520 23:15:00.506834 24002 solver.cpp:237] Iteration 432, loss = 2.09328
I0520 23:15:00.506875 24002 solver.cpp:253]     Train net output #0: loss = 2.09328 (* 1 = 2.09328 loss)
I0520 23:15:00.506894 24002 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0520 23:15:08.514684 24002 solver.cpp:237] Iteration 468, loss = 2.06539
I0520 23:15:08.514719 24002 solver.cpp:253]     Train net output #0: loss = 2.06539 (* 1 = 2.06539 loss)
I0520 23:15:08.514734 24002 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0520 23:15:38.659097 24002 solver.cpp:237] Iteration 504, loss = 2.02211
I0520 23:15:38.659252 24002 solver.cpp:253]     Train net output #0: loss = 2.02211 (* 1 = 2.02211 loss)
I0520 23:15:38.659268 24002 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0520 23:15:46.665146 24002 solver.cpp:237] Iteration 540, loss = 2.01816
I0520 23:15:46.665180 24002 solver.cpp:253]     Train net output #0: loss = 2.01816 (* 1 = 2.01816 loss)
I0520 23:15:46.665197 24002 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0520 23:15:54.671713 24002 solver.cpp:237] Iteration 576, loss = 1.99249
I0520 23:15:54.671753 24002 solver.cpp:253]     Train net output #0: loss = 1.99249 (* 1 = 1.99249 loss)
I0520 23:15:54.671772 24002 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0520 23:16:02.680744 24002 solver.cpp:237] Iteration 612, loss = 1.94351
I0520 23:16:02.680778 24002 solver.cpp:253]     Train net output #0: loss = 1.94351 (* 1 = 1.94351 loss)
I0520 23:16:02.680795 24002 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0520 23:16:10.690398 24002 solver.cpp:237] Iteration 648, loss = 1.89952
I0520 23:16:10.690543 24002 solver.cpp:253]     Train net output #0: loss = 1.89952 (* 1 = 1.89952 loss)
I0520 23:16:10.690557 24002 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0520 23:16:18.697288 24002 solver.cpp:237] Iteration 684, loss = 1.89824
I0520 23:16:18.697330 24002 solver.cpp:253]     Train net output #0: loss = 1.89824 (* 1 = 1.89824 loss)
I0520 23:16:18.697350 24002 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0520 23:16:26.708806 24002 solver.cpp:237] Iteration 720, loss = 1.90245
I0520 23:16:26.708838 24002 solver.cpp:253]     Train net output #0: loss = 1.90245 (* 1 = 1.90245 loss)
I0520 23:16:26.708855 24002 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0520 23:16:28.709959 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_730.caffemodel
I0520 23:16:28.904826 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_730.solverstate
I0520 23:16:28.996870 24002 solver.cpp:341] Iteration 731, Testing net (#0)
I0520 23:17:14.367750 24002 solver.cpp:409]     Test net output #0: accuracy = 0.573672
I0520 23:17:14.367909 24002 solver.cpp:409]     Test net output #1: loss = 1.50906 (* 1 = 1.50906 loss)
I0520 23:17:42.120968 24002 solver.cpp:237] Iteration 756, loss = 1.81704
I0520 23:17:42.121019 24002 solver.cpp:253]     Train net output #0: loss = 1.81704 (* 1 = 1.81704 loss)
I0520 23:17:42.121036 24002 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0520 23:17:50.133141 24002 solver.cpp:237] Iteration 792, loss = 1.82342
I0520 23:17:50.133285 24002 solver.cpp:253]     Train net output #0: loss = 1.82342 (* 1 = 1.82342 loss)
I0520 23:17:50.133299 24002 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0520 23:17:58.144716 24002 solver.cpp:237] Iteration 828, loss = 1.97685
I0520 23:17:58.144762 24002 solver.cpp:253]     Train net output #0: loss = 1.97685 (* 1 = 1.97685 loss)
I0520 23:17:58.144837 24002 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0520 23:18:06.157649 24002 solver.cpp:237] Iteration 864, loss = 1.81986
I0520 23:18:06.157681 24002 solver.cpp:253]     Train net output #0: loss = 1.81986 (* 1 = 1.81986 loss)
I0520 23:18:06.157699 24002 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0520 23:18:14.167240 24002 solver.cpp:237] Iteration 900, loss = 1.86035
I0520 23:18:14.167275 24002 solver.cpp:253]     Train net output #0: loss = 1.86035 (* 1 = 1.86035 loss)
I0520 23:18:14.167292 24002 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0520 23:18:22.176182 24002 solver.cpp:237] Iteration 936, loss = 1.80021
I0520 23:18:22.176329 24002 solver.cpp:253]     Train net output #0: loss = 1.80021 (* 1 = 1.80021 loss)
I0520 23:18:22.176344 24002 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0520 23:18:30.192081 24002 solver.cpp:237] Iteration 972, loss = 1.93505
I0520 23:18:30.192114 24002 solver.cpp:253]     Train net output #0: loss = 1.93505 (* 1 = 1.93505 loss)
I0520 23:18:30.192132 24002 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0520 23:19:00.382566 24002 solver.cpp:237] Iteration 1008, loss = 1.87154
I0520 23:19:00.382738 24002 solver.cpp:253]     Train net output #0: loss = 1.87154 (* 1 = 1.87154 loss)
I0520 23:19:00.382753 24002 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0520 23:19:08.397274 24002 solver.cpp:237] Iteration 1044, loss = 1.91417
I0520 23:19:08.397313 24002 solver.cpp:253]     Train net output #0: loss = 1.91417 (* 1 = 1.91417 loss)
I0520 23:19:08.397336 24002 sgd_solver.cpp:106] Iteration 1044, lr = 0.0025
I0520 23:19:16.405488 24002 solver.cpp:237] Iteration 1080, loss = 1.86722
I0520 23:19:16.405524 24002 solver.cpp:253]     Train net output #0: loss = 1.86722 (* 1 = 1.86722 loss)
I0520 23:19:16.405536 24002 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0520 23:19:19.521659 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_1095.caffemodel
I0520 23:19:19.717695 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_1095.solverstate
I0520 23:19:24.483151 24002 solver.cpp:237] Iteration 1116, loss = 1.78859
I0520 23:19:24.483199 24002 solver.cpp:253]     Train net output #0: loss = 1.78859 (* 1 = 1.78859 loss)
I0520 23:19:24.483217 24002 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0520 23:19:32.496933 24002 solver.cpp:237] Iteration 1152, loss = 1.76854
I0520 23:19:32.497088 24002 solver.cpp:253]     Train net output #0: loss = 1.76854 (* 1 = 1.76854 loss)
I0520 23:19:32.497102 24002 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0520 23:19:40.507730 24002 solver.cpp:237] Iteration 1188, loss = 1.74871
I0520 23:19:40.507763 24002 solver.cpp:253]     Train net output #0: loss = 1.74871 (* 1 = 1.74871 loss)
I0520 23:19:40.507781 24002 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0520 23:20:10.712923 24002 solver.cpp:237] Iteration 1224, loss = 1.81575
I0520 23:20:10.713084 24002 solver.cpp:253]     Train net output #0: loss = 1.81575 (* 1 = 1.81575 loss)
I0520 23:20:10.713099 24002 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0520 23:20:18.726069 24002 solver.cpp:237] Iteration 1260, loss = 1.79195
I0520 23:20:18.726104 24002 solver.cpp:253]     Train net output #0: loss = 1.79195 (* 1 = 1.79195 loss)
I0520 23:20:18.726120 24002 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0520 23:20:26.735895 24002 solver.cpp:237] Iteration 1296, loss = 1.78003
I0520 23:20:26.735937 24002 solver.cpp:253]     Train net output #0: loss = 1.78003 (* 1 = 1.78003 loss)
I0520 23:20:26.735958 24002 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0520 23:20:34.746335 24002 solver.cpp:237] Iteration 1332, loss = 1.63939
I0520 23:20:34.746369 24002 solver.cpp:253]     Train net output #0: loss = 1.63939 (* 1 = 1.63939 loss)
I0520 23:20:34.746386 24002 sgd_solver.cpp:106] Iteration 1332, lr = 0.0025
I0520 23:20:42.754088 24002 solver.cpp:237] Iteration 1368, loss = 1.78209
I0520 23:20:42.754225 24002 solver.cpp:253]     Train net output #0: loss = 1.78209 (* 1 = 1.78209 loss)
I0520 23:20:42.754238 24002 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0520 23:20:50.763227 24002 solver.cpp:237] Iteration 1404, loss = 1.74989
I0520 23:20:50.763273 24002 solver.cpp:253]     Train net output #0: loss = 1.74989 (* 1 = 1.74989 loss)
I0520 23:20:50.763288 24002 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0520 23:20:58.775544 24002 solver.cpp:237] Iteration 1440, loss = 1.76621
I0520 23:20:58.775578 24002 solver.cpp:253]     Train net output #0: loss = 1.76621 (* 1 = 1.76621 loss)
I0520 23:20:58.775593 24002 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0520 23:21:03.004009 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_1460.caffemodel
I0520 23:21:03.199848 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_1460.solverstate
I0520 23:21:03.517649 24002 solver.cpp:341] Iteration 1462, Testing net (#0)
I0520 23:22:09.780719 24002 solver.cpp:409]     Test net output #0: accuracy = 0.651881
I0520 23:22:09.780892 24002 solver.cpp:409]     Test net output #1: loss = 1.23336 (* 1 = 1.23336 loss)
I0520 23:22:35.109432 24002 solver.cpp:237] Iteration 1476, loss = 1.65482
I0520 23:22:35.109483 24002 solver.cpp:253]     Train net output #0: loss = 1.65482 (* 1 = 1.65482 loss)
I0520 23:22:35.109498 24002 sgd_solver.cpp:106] Iteration 1476, lr = 0.0025
I0520 23:22:43.125865 24002 solver.cpp:237] Iteration 1512, loss = 1.66992
I0520 23:22:43.126008 24002 solver.cpp:253]     Train net output #0: loss = 1.66992 (* 1 = 1.66992 loss)
I0520 23:22:43.126020 24002 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0520 23:22:51.146602 24002 solver.cpp:237] Iteration 1548, loss = 1.80619
I0520 23:22:51.146636 24002 solver.cpp:253]     Train net output #0: loss = 1.80619 (* 1 = 1.80619 loss)
I0520 23:22:51.146654 24002 sgd_solver.cpp:106] Iteration 1548, lr = 0.0025
I0520 23:22:59.161155 24002 solver.cpp:237] Iteration 1584, loss = 1.7107
I0520 23:22:59.161188 24002 solver.cpp:253]     Train net output #0: loss = 1.7107 (* 1 = 1.7107 loss)
I0520 23:22:59.161209 24002 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0520 23:23:07.177201 24002 solver.cpp:237] Iteration 1620, loss = 1.76618
I0520 23:23:07.177234 24002 solver.cpp:253]     Train net output #0: loss = 1.76618 (* 1 = 1.76618 loss)
I0520 23:23:07.177253 24002 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0520 23:23:15.194793 24002 solver.cpp:237] Iteration 1656, loss = 1.73301
I0520 23:23:15.194928 24002 solver.cpp:253]     Train net output #0: loss = 1.73301 (* 1 = 1.73301 loss)
I0520 23:23:15.194942 24002 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0520 23:23:23.211282 24002 solver.cpp:237] Iteration 1692, loss = 1.65709
I0520 23:23:23.211318 24002 solver.cpp:253]     Train net output #0: loss = 1.65709 (* 1 = 1.65709 loss)
I0520 23:23:23.211335 24002 sgd_solver.cpp:106] Iteration 1692, lr = 0.0025
I0520 23:23:53.332804 24002 solver.cpp:237] Iteration 1728, loss = 1.86964
I0520 23:23:53.332972 24002 solver.cpp:253]     Train net output #0: loss = 1.86964 (* 1 = 1.86964 loss)
I0520 23:23:53.332988 24002 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0520 23:24:01.349936 24002 solver.cpp:237] Iteration 1764, loss = 1.7503
I0520 23:24:01.349968 24002 solver.cpp:253]     Train net output #0: loss = 1.7503 (* 1 = 1.7503 loss)
I0520 23:24:01.349987 24002 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0520 23:24:09.369499 24002 solver.cpp:237] Iteration 1800, loss = 1.6575
I0520 23:24:09.369536 24002 solver.cpp:253]     Train net output #0: loss = 1.6575 (* 1 = 1.6575 loss)
I0520 23:24:09.369560 24002 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0520 23:24:14.714352 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_1825.caffemodel
I0520 23:24:14.910540 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_1825.solverstate
I0520 23:24:17.454960 24002 solver.cpp:237] Iteration 1836, loss = 1.62826
I0520 23:24:17.455008 24002 solver.cpp:253]     Train net output #0: loss = 1.62826 (* 1 = 1.62826 loss)
I0520 23:24:17.455025 24002 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0520 23:24:25.470645 24002 solver.cpp:237] Iteration 1872, loss = 1.73426
I0520 23:24:25.470785 24002 solver.cpp:253]     Train net output #0: loss = 1.73426 (* 1 = 1.73426 loss)
I0520 23:24:25.470798 24002 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0520 23:24:33.485841 24002 solver.cpp:237] Iteration 1908, loss = 1.6325
I0520 23:24:33.485882 24002 solver.cpp:253]     Train net output #0: loss = 1.6325 (* 1 = 1.6325 loss)
I0520 23:24:33.485899 24002 sgd_solver.cpp:106] Iteration 1908, lr = 0.0025
I0520 23:24:41.499841 24002 solver.cpp:237] Iteration 1944, loss = 1.63917
I0520 23:24:41.499876 24002 solver.cpp:253]     Train net output #0: loss = 1.63917 (* 1 = 1.63917 loss)
I0520 23:24:41.499892 24002 sgd_solver.cpp:106] Iteration 1944, lr = 0.0025
I0520 23:25:11.680240 24002 solver.cpp:237] Iteration 1980, loss = 1.60883
I0520 23:25:11.680414 24002 solver.cpp:253]     Train net output #0: loss = 1.60883 (* 1 = 1.60883 loss)
I0520 23:25:11.680430 24002 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0520 23:25:19.693485 24002 solver.cpp:237] Iteration 2016, loss = 1.65032
I0520 23:25:19.693513 24002 solver.cpp:253]     Train net output #0: loss = 1.65032 (* 1 = 1.65032 loss)
I0520 23:25:19.693526 24002 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0520 23:25:27.713023 24002 solver.cpp:237] Iteration 2052, loss = 1.6574
I0520 23:25:27.713063 24002 solver.cpp:253]     Train net output #0: loss = 1.6574 (* 1 = 1.6574 loss)
I0520 23:25:27.713080 24002 sgd_solver.cpp:106] Iteration 2052, lr = 0.0025
I0520 23:25:35.728209 24002 solver.cpp:237] Iteration 2088, loss = 1.59357
I0520 23:25:35.728242 24002 solver.cpp:253]     Train net output #0: loss = 1.59357 (* 1 = 1.59357 loss)
I0520 23:25:35.728257 24002 sgd_solver.cpp:106] Iteration 2088, lr = 0.0025
I0520 23:25:43.747380 24002 solver.cpp:237] Iteration 2124, loss = 1.68175
I0520 23:25:43.747519 24002 solver.cpp:253]     Train net output #0: loss = 1.68175 (* 1 = 1.68175 loss)
I0520 23:25:43.747532 24002 sgd_solver.cpp:106] Iteration 2124, lr = 0.0025
I0520 23:25:51.761415 24002 solver.cpp:237] Iteration 2160, loss = 1.61824
I0520 23:25:51.761452 24002 solver.cpp:253]     Train net output #0: loss = 1.61824 (* 1 = 1.61824 loss)
I0520 23:25:51.761464 24002 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0520 23:25:58.216796 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_2190.caffemodel
I0520 23:25:58.410948 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_2190.solverstate
I0520 23:25:58.948259 24002 solver.cpp:341] Iteration 2193, Testing net (#0)
I0520 23:26:43.992743 24002 solver.cpp:409]     Test net output #0: accuracy = 0.684317
I0520 23:26:43.992902 24002 solver.cpp:409]     Test net output #1: loss = 1.08892 (* 1 = 1.08892 loss)
I0520 23:27:06.890398 24002 solver.cpp:237] Iteration 2196, loss = 1.6916
I0520 23:27:06.890449 24002 solver.cpp:253]     Train net output #0: loss = 1.6916 (* 1 = 1.6916 loss)
I0520 23:27:06.890465 24002 sgd_solver.cpp:106] Iteration 2196, lr = 0.0025
I0520 23:27:14.901103 24002 solver.cpp:237] Iteration 2232, loss = 1.65173
I0520 23:27:14.901252 24002 solver.cpp:253]     Train net output #0: loss = 1.65173 (* 1 = 1.65173 loss)
I0520 23:27:14.901265 24002 sgd_solver.cpp:106] Iteration 2232, lr = 0.0025
I0520 23:27:22.910311 24002 solver.cpp:237] Iteration 2268, loss = 1.63881
I0520 23:27:22.910346 24002 solver.cpp:253]     Train net output #0: loss = 1.63881 (* 1 = 1.63881 loss)
I0520 23:27:22.910362 24002 sgd_solver.cpp:106] Iteration 2268, lr = 0.0025
I0520 23:27:30.917208 24002 solver.cpp:237] Iteration 2304, loss = 1.56939
I0520 23:27:30.917249 24002 solver.cpp:253]     Train net output #0: loss = 1.56939 (* 1 = 1.56939 loss)
I0520 23:27:30.917268 24002 sgd_solver.cpp:106] Iteration 2304, lr = 0.0025
I0520 23:27:38.926950 24002 solver.cpp:237] Iteration 2340, loss = 1.66441
I0520 23:27:38.926983 24002 solver.cpp:253]     Train net output #0: loss = 1.66441 (* 1 = 1.66441 loss)
I0520 23:27:38.927000 24002 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0520 23:27:46.937532 24002 solver.cpp:237] Iteration 2376, loss = 1.62575
I0520 23:27:46.937682 24002 solver.cpp:253]     Train net output #0: loss = 1.62575 (* 1 = 1.62575 loss)
I0520 23:27:46.937696 24002 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0520 23:27:54.948220 24002 solver.cpp:237] Iteration 2412, loss = 1.64554
I0520 23:27:54.948266 24002 solver.cpp:253]     Train net output #0: loss = 1.64554 (* 1 = 1.64554 loss)
I0520 23:27:54.948281 24002 sgd_solver.cpp:106] Iteration 2412, lr = 0.0025
I0520 23:28:25.171413 24002 solver.cpp:237] Iteration 2448, loss = 1.66512
I0520 23:28:25.171583 24002 solver.cpp:253]     Train net output #0: loss = 1.66512 (* 1 = 1.66512 loss)
I0520 23:28:25.171598 24002 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0520 23:28:33.182862 24002 solver.cpp:237] Iteration 2484, loss = 1.65813
I0520 23:28:33.182895 24002 solver.cpp:253]     Train net output #0: loss = 1.65813 (* 1 = 1.65813 loss)
I0520 23:28:33.182912 24002 sgd_solver.cpp:106] Iteration 2484, lr = 0.0025
I0520 23:28:41.197404 24002 solver.cpp:237] Iteration 2520, loss = 1.57091
I0520 23:28:41.197439 24002 solver.cpp:253]     Train net output #0: loss = 1.57091 (* 1 = 1.57091 loss)
I0520 23:28:41.197455 24002 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0520 23:28:48.761530 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_2555.caffemodel
I0520 23:28:48.956456 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_2555.solverstate
I0520 23:28:49.271481 24002 solver.cpp:237] Iteration 2556, loss = 1.65964
I0520 23:28:49.271525 24002 solver.cpp:253]     Train net output #0: loss = 1.65964 (* 1 = 1.65964 loss)
I0520 23:28:49.271541 24002 sgd_solver.cpp:106] Iteration 2556, lr = 0.0025
I0520 23:28:57.283488 24002 solver.cpp:237] Iteration 2592, loss = 1.57805
I0520 23:28:57.283632 24002 solver.cpp:253]     Train net output #0: loss = 1.57805 (* 1 = 1.57805 loss)
I0520 23:28:57.283646 24002 sgd_solver.cpp:106] Iteration 2592, lr = 0.0025
I0520 23:29:05.290119 24002 solver.cpp:237] Iteration 2628, loss = 1.61935
I0520 23:29:05.290153 24002 solver.cpp:253]     Train net output #0: loss = 1.61935 (* 1 = 1.61935 loss)
I0520 23:29:05.290169 24002 sgd_solver.cpp:106] Iteration 2628, lr = 0.0025
I0520 23:29:13.299782 24002 solver.cpp:237] Iteration 2664, loss = 1.54106
I0520 23:29:13.299818 24002 solver.cpp:253]     Train net output #0: loss = 1.54106 (* 1 = 1.54106 loss)
I0520 23:29:13.299840 24002 sgd_solver.cpp:106] Iteration 2664, lr = 0.0025
I0520 23:29:43.448421 24002 solver.cpp:237] Iteration 2700, loss = 1.55036
I0520 23:29:43.448591 24002 solver.cpp:253]     Train net output #0: loss = 1.55036 (* 1 = 1.55036 loss)
I0520 23:29:43.448606 24002 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0520 23:29:51.457371 24002 solver.cpp:237] Iteration 2736, loss = 1.63609
I0520 23:29:51.457404 24002 solver.cpp:253]     Train net output #0: loss = 1.63609 (* 1 = 1.63609 loss)
I0520 23:29:51.457422 24002 sgd_solver.cpp:106] Iteration 2736, lr = 0.0025
I0520 23:29:59.466977 24002 solver.cpp:237] Iteration 2772, loss = 1.63882
I0520 23:29:59.467016 24002 solver.cpp:253]     Train net output #0: loss = 1.63882 (* 1 = 1.63882 loss)
I0520 23:29:59.467037 24002 sgd_solver.cpp:106] Iteration 2772, lr = 0.0025
I0520 23:30:07.472908 24002 solver.cpp:237] Iteration 2808, loss = 1.62549
I0520 23:30:07.472942 24002 solver.cpp:253]     Train net output #0: loss = 1.62549 (* 1 = 1.62549 loss)
I0520 23:30:07.472959 24002 sgd_solver.cpp:106] Iteration 2808, lr = 0.0025
I0520 23:30:15.484814 24002 solver.cpp:237] Iteration 2844, loss = 1.45784
I0520 23:30:15.484954 24002 solver.cpp:253]     Train net output #0: loss = 1.45784 (* 1 = 1.45784 loss)
I0520 23:30:15.484968 24002 sgd_solver.cpp:106] Iteration 2844, lr = 0.0025
I0520 23:30:23.498168 24002 solver.cpp:237] Iteration 2880, loss = 1.61086
I0520 23:30:23.498211 24002 solver.cpp:253]     Train net output #0: loss = 1.61086 (* 1 = 1.61086 loss)
I0520 23:30:23.498229 24002 sgd_solver.cpp:106] Iteration 2880, lr = 0.0025
I0520 23:30:31.507136 24002 solver.cpp:237] Iteration 2916, loss = 1.60778
I0520 23:30:31.507171 24002 solver.cpp:253]     Train net output #0: loss = 1.60778 (* 1 = 1.60778 loss)
I0520 23:30:31.507187 24002 sgd_solver.cpp:106] Iteration 2916, lr = 0.0025
I0520 23:30:32.175436 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_2920.caffemodel
I0520 23:30:32.368765 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_2920.solverstate
I0520 23:30:33.129070 24002 solver.cpp:341] Iteration 2924, Testing net (#0)
I0520 23:31:39.319432 24002 solver.cpp:409]     Test net output #0: accuracy = 0.714454
I0520 23:31:39.319605 24002 solver.cpp:409]     Test net output #1: loss = 0.967075 (* 1 = 0.967075 loss)
I0520 23:32:07.742645 24002 solver.cpp:237] Iteration 2952, loss = 1.69747
I0520 23:32:07.742694 24002 solver.cpp:253]     Train net output #0: loss = 1.69747 (* 1 = 1.69747 loss)
I0520 23:32:07.742712 24002 sgd_solver.cpp:106] Iteration 2952, lr = 0.0025
I0520 23:32:15.755069 24002 solver.cpp:237] Iteration 2988, loss = 1.48164
I0520 23:32:15.755221 24002 solver.cpp:253]     Train net output #0: loss = 1.48164 (* 1 = 1.48164 loss)
I0520 23:32:15.755234 24002 sgd_solver.cpp:106] Iteration 2988, lr = 0.0025
I0520 23:32:23.763484 24002 solver.cpp:237] Iteration 3024, loss = 1.47899
I0520 23:32:23.763519 24002 solver.cpp:253]     Train net output #0: loss = 1.47899 (* 1 = 1.47899 loss)
I0520 23:32:23.763535 24002 sgd_solver.cpp:106] Iteration 3024, lr = 0.0025
I0520 23:32:31.778197 24002 solver.cpp:237] Iteration 3060, loss = 1.55233
I0520 23:32:31.778241 24002 solver.cpp:253]     Train net output #0: loss = 1.55233 (* 1 = 1.55233 loss)
I0520 23:32:31.778254 24002 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0520 23:32:39.791357 24002 solver.cpp:237] Iteration 3096, loss = 1.55351
I0520 23:32:39.791390 24002 solver.cpp:253]     Train net output #0: loss = 1.55351 (* 1 = 1.55351 loss)
I0520 23:32:39.791404 24002 sgd_solver.cpp:106] Iteration 3096, lr = 0.0025
I0520 23:32:47.798835 24002 solver.cpp:237] Iteration 3132, loss = 1.51679
I0520 23:32:47.798975 24002 solver.cpp:253]     Train net output #0: loss = 1.51679 (* 1 = 1.51679 loss)
I0520 23:32:47.798990 24002 sgd_solver.cpp:106] Iteration 3132, lr = 0.0025
I0520 23:32:55.808257 24002 solver.cpp:237] Iteration 3168, loss = 1.52245
I0520 23:32:55.808300 24002 solver.cpp:253]     Train net output #0: loss = 1.52245 (* 1 = 1.52245 loss)
I0520 23:32:55.808320 24002 sgd_solver.cpp:106] Iteration 3168, lr = 0.0025
I0520 23:33:25.927464 24002 solver.cpp:237] Iteration 3204, loss = 1.78457
I0520 23:33:25.927628 24002 solver.cpp:253]     Train net output #0: loss = 1.78457 (* 1 = 1.78457 loss)
I0520 23:33:25.927644 24002 sgd_solver.cpp:106] Iteration 3204, lr = 0.0025
I0520 23:33:33.939846 24002 solver.cpp:237] Iteration 3240, loss = 1.62537
I0520 23:33:33.939879 24002 solver.cpp:253]     Train net output #0: loss = 1.62537 (* 1 = 1.62537 loss)
I0520 23:33:33.939896 24002 sgd_solver.cpp:106] Iteration 3240, lr = 0.0025
I0520 23:33:41.950495 24002 solver.cpp:237] Iteration 3276, loss = 1.52735
I0520 23:33:41.950528 24002 solver.cpp:253]     Train net output #0: loss = 1.52735 (* 1 = 1.52735 loss)
I0520 23:33:41.950543 24002 sgd_solver.cpp:106] Iteration 3276, lr = 0.0025
I0520 23:33:43.732187 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_3285.caffemodel
I0520 23:33:43.929076 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_3285.solverstate
I0520 23:33:50.033241 24002 solver.cpp:237] Iteration 3312, loss = 1.52575
I0520 23:33:50.033289 24002 solver.cpp:253]     Train net output #0: loss = 1.52575 (* 1 = 1.52575 loss)
I0520 23:33:50.033305 24002 sgd_solver.cpp:106] Iteration 3312, lr = 0.0025
I0520 23:33:58.041999 24002 solver.cpp:237] Iteration 3348, loss = 1.61057
I0520 23:33:58.042155 24002 solver.cpp:253]     Train net output #0: loss = 1.61057 (* 1 = 1.61057 loss)
I0520 23:33:58.042168 24002 sgd_solver.cpp:106] Iteration 3348, lr = 0.0025
I0520 23:34:06.053081 24002 solver.cpp:237] Iteration 3384, loss = 1.59757
I0520 23:34:06.053113 24002 solver.cpp:253]     Train net output #0: loss = 1.59757 (* 1 = 1.59757 loss)
I0520 23:34:06.053130 24002 sgd_solver.cpp:106] Iteration 3384, lr = 0.0025
I0520 23:34:36.199592 24002 solver.cpp:237] Iteration 3420, loss = 1.5988
I0520 23:34:36.199767 24002 solver.cpp:253]     Train net output #0: loss = 1.5988 (* 1 = 1.5988 loss)
I0520 23:34:36.199784 24002 sgd_solver.cpp:106] Iteration 3420, lr = 0.0025
I0520 23:34:44.212785 24002 solver.cpp:237] Iteration 3456, loss = 1.51514
I0520 23:34:44.212817 24002 solver.cpp:253]     Train net output #0: loss = 1.51514 (* 1 = 1.51514 loss)
I0520 23:34:44.212836 24002 sgd_solver.cpp:106] Iteration 3456, lr = 0.0025
I0520 23:34:52.228284 24002 solver.cpp:237] Iteration 3492, loss = 1.44604
I0520 23:34:52.228318 24002 solver.cpp:253]     Train net output #0: loss = 1.44604 (* 1 = 1.44604 loss)
I0520 23:34:52.228335 24002 sgd_solver.cpp:106] Iteration 3492, lr = 0.0025
I0520 23:35:00.239830 24002 solver.cpp:237] Iteration 3528, loss = 1.58333
I0520 23:35:00.239876 24002 solver.cpp:253]     Train net output #0: loss = 1.58333 (* 1 = 1.58333 loss)
I0520 23:35:00.239892 24002 sgd_solver.cpp:106] Iteration 3528, lr = 0.0025
I0520 23:35:08.247519 24002 solver.cpp:237] Iteration 3564, loss = 1.42938
I0520 23:35:08.247663 24002 solver.cpp:253]     Train net output #0: loss = 1.42938 (* 1 = 1.42938 loss)
I0520 23:35:08.247676 24002 sgd_solver.cpp:106] Iteration 3564, lr = 0.0025
I0520 23:35:16.257177 24002 solver.cpp:237] Iteration 3600, loss = 1.52272
I0520 23:35:16.257210 24002 solver.cpp:253]     Train net output #0: loss = 1.52272 (* 1 = 1.52272 loss)
I0520 23:35:16.257228 24002 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0520 23:35:24.269464 24002 solver.cpp:237] Iteration 3636, loss = 1.59686
I0520 23:35:24.269505 24002 solver.cpp:253]     Train net output #0: loss = 1.59686 (* 1 = 1.59686 loss)
I0520 23:35:24.269525 24002 sgd_solver.cpp:106] Iteration 3636, lr = 0.0025
I0520 23:35:27.163610 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_3650.caffemodel
I0520 23:35:29.146072 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_3650.solverstate
I0520 23:35:30.136135 24002 solver.cpp:341] Iteration 3655, Testing net (#0)
I0520 23:36:16.133366 24002 solver.cpp:409]     Test net output #0: accuracy = 0.750244
I0520 23:36:16.133528 24002 solver.cpp:409]     Test net output #1: loss = 0.909555 (* 1 = 0.909555 loss)
I0520 23:36:16.644628 24002 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_3658.caffemodel
I0520 23:36:16.899643 24002 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379_iter_3658.solverstate
I0520 23:36:17.000051 24002 solver.cpp:326] Optimization Done.
I0520 23:36:17.000078 24002 caffe.cpp:215] Optimization Done.
Application 11235795 resources: utime ~1258s, stime ~227s, Rss ~5329588, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_410_2016-05-20T11.20.47.617379.solver"
	User time (seconds): 0.53
	System time (seconds): 0.24
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:50.83
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15086
	Voluntary context switches: 2948
	Involuntary context switches: 235
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
