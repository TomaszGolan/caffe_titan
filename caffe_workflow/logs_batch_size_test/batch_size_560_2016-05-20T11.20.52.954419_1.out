2806156
I0521 02:14:26.433812 20569 caffe.cpp:184] Using GPUs 0
I0521 02:14:26.866791 20569 solver.cpp:48] Initializing solver from parameters: 
test_iter: 267
test_interval: 535
base_lr: 0.0025
display: 26
max_iter: 2678
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 267
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419.prototxt"
I0521 02:14:26.868340 20569 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419.prototxt
I0521 02:14:26.882586 20569 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 02:14:26.882647 20569 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 02:14:26.882989 20569 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 560
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:14:26.883167 20569 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:14:26.883190 20569 net.cpp:106] Creating Layer data_hdf5
I0521 02:14:26.883204 20569 net.cpp:411] data_hdf5 -> data
I0521 02:14:26.883239 20569 net.cpp:411] data_hdf5 -> label
I0521 02:14:26.883270 20569 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 02:14:26.884580 20569 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 02:14:26.886884 20569 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 02:14:48.430234 20569 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 02:14:48.435339 20569 net.cpp:150] Setting up data_hdf5
I0521 02:14:48.435379 20569 net.cpp:157] Top shape: 560 1 127 50 (3556000)
I0521 02:14:48.435395 20569 net.cpp:157] Top shape: 560 (560)
I0521 02:14:48.435406 20569 net.cpp:165] Memory required for data: 14226240
I0521 02:14:48.435420 20569 layer_factory.hpp:77] Creating layer conv1
I0521 02:14:48.435453 20569 net.cpp:106] Creating Layer conv1
I0521 02:14:48.435464 20569 net.cpp:454] conv1 <- data
I0521 02:14:48.435487 20569 net.cpp:411] conv1 -> conv1
I0521 02:14:48.815965 20569 net.cpp:150] Setting up conv1
I0521 02:14:48.816012 20569 net.cpp:157] Top shape: 560 12 120 48 (38707200)
I0521 02:14:48.816023 20569 net.cpp:165] Memory required for data: 169055040
I0521 02:14:48.816051 20569 layer_factory.hpp:77] Creating layer relu1
I0521 02:14:48.816072 20569 net.cpp:106] Creating Layer relu1
I0521 02:14:48.816082 20569 net.cpp:454] relu1 <- conv1
I0521 02:14:48.816097 20569 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:14:48.816627 20569 net.cpp:150] Setting up relu1
I0521 02:14:48.816643 20569 net.cpp:157] Top shape: 560 12 120 48 (38707200)
I0521 02:14:48.816654 20569 net.cpp:165] Memory required for data: 323883840
I0521 02:14:48.816664 20569 layer_factory.hpp:77] Creating layer pool1
I0521 02:14:48.816682 20569 net.cpp:106] Creating Layer pool1
I0521 02:14:48.816692 20569 net.cpp:454] pool1 <- conv1
I0521 02:14:48.816705 20569 net.cpp:411] pool1 -> pool1
I0521 02:14:48.816786 20569 net.cpp:150] Setting up pool1
I0521 02:14:48.816799 20569 net.cpp:157] Top shape: 560 12 60 48 (19353600)
I0521 02:14:48.816809 20569 net.cpp:165] Memory required for data: 401298240
I0521 02:14:48.816818 20569 layer_factory.hpp:77] Creating layer conv2
I0521 02:14:48.816839 20569 net.cpp:106] Creating Layer conv2
I0521 02:14:48.816850 20569 net.cpp:454] conv2 <- pool1
I0521 02:14:48.816862 20569 net.cpp:411] conv2 -> conv2
I0521 02:14:48.819526 20569 net.cpp:150] Setting up conv2
I0521 02:14:48.819553 20569 net.cpp:157] Top shape: 560 20 54 46 (27820800)
I0521 02:14:48.819564 20569 net.cpp:165] Memory required for data: 512581440
I0521 02:14:48.819582 20569 layer_factory.hpp:77] Creating layer relu2
I0521 02:14:48.819597 20569 net.cpp:106] Creating Layer relu2
I0521 02:14:48.819607 20569 net.cpp:454] relu2 <- conv2
I0521 02:14:48.819619 20569 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:14:48.819950 20569 net.cpp:150] Setting up relu2
I0521 02:14:48.819964 20569 net.cpp:157] Top shape: 560 20 54 46 (27820800)
I0521 02:14:48.819974 20569 net.cpp:165] Memory required for data: 623864640
I0521 02:14:48.819984 20569 layer_factory.hpp:77] Creating layer pool2
I0521 02:14:48.819998 20569 net.cpp:106] Creating Layer pool2
I0521 02:14:48.820008 20569 net.cpp:454] pool2 <- conv2
I0521 02:14:48.820031 20569 net.cpp:411] pool2 -> pool2
I0521 02:14:48.820101 20569 net.cpp:150] Setting up pool2
I0521 02:14:48.820113 20569 net.cpp:157] Top shape: 560 20 27 46 (13910400)
I0521 02:14:48.820123 20569 net.cpp:165] Memory required for data: 679506240
I0521 02:14:48.820132 20569 layer_factory.hpp:77] Creating layer conv3
I0521 02:14:48.820149 20569 net.cpp:106] Creating Layer conv3
I0521 02:14:48.820160 20569 net.cpp:454] conv3 <- pool2
I0521 02:14:48.820174 20569 net.cpp:411] conv3 -> conv3
I0521 02:14:48.822103 20569 net.cpp:150] Setting up conv3
I0521 02:14:48.822127 20569 net.cpp:157] Top shape: 560 28 22 44 (15178240)
I0521 02:14:48.822137 20569 net.cpp:165] Memory required for data: 740219200
I0521 02:14:48.822156 20569 layer_factory.hpp:77] Creating layer relu3
I0521 02:14:48.822172 20569 net.cpp:106] Creating Layer relu3
I0521 02:14:48.822182 20569 net.cpp:454] relu3 <- conv3
I0521 02:14:48.822196 20569 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:14:48.822666 20569 net.cpp:150] Setting up relu3
I0521 02:14:48.822685 20569 net.cpp:157] Top shape: 560 28 22 44 (15178240)
I0521 02:14:48.822695 20569 net.cpp:165] Memory required for data: 800932160
I0521 02:14:48.822705 20569 layer_factory.hpp:77] Creating layer pool3
I0521 02:14:48.822717 20569 net.cpp:106] Creating Layer pool3
I0521 02:14:48.822727 20569 net.cpp:454] pool3 <- conv3
I0521 02:14:48.822739 20569 net.cpp:411] pool3 -> pool3
I0521 02:14:48.822808 20569 net.cpp:150] Setting up pool3
I0521 02:14:48.822820 20569 net.cpp:157] Top shape: 560 28 11 44 (7589120)
I0521 02:14:48.822830 20569 net.cpp:165] Memory required for data: 831288640
I0521 02:14:48.822840 20569 layer_factory.hpp:77] Creating layer conv4
I0521 02:14:48.822857 20569 net.cpp:106] Creating Layer conv4
I0521 02:14:48.822867 20569 net.cpp:454] conv4 <- pool3
I0521 02:14:48.822880 20569 net.cpp:411] conv4 -> conv4
I0521 02:14:48.825647 20569 net.cpp:150] Setting up conv4
I0521 02:14:48.825675 20569 net.cpp:157] Top shape: 560 36 6 42 (5080320)
I0521 02:14:48.825686 20569 net.cpp:165] Memory required for data: 851609920
I0521 02:14:48.825702 20569 layer_factory.hpp:77] Creating layer relu4
I0521 02:14:48.825716 20569 net.cpp:106] Creating Layer relu4
I0521 02:14:48.825726 20569 net.cpp:454] relu4 <- conv4
I0521 02:14:48.825739 20569 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:14:48.826210 20569 net.cpp:150] Setting up relu4
I0521 02:14:48.826225 20569 net.cpp:157] Top shape: 560 36 6 42 (5080320)
I0521 02:14:48.826236 20569 net.cpp:165] Memory required for data: 871931200
I0521 02:14:48.826246 20569 layer_factory.hpp:77] Creating layer pool4
I0521 02:14:48.826259 20569 net.cpp:106] Creating Layer pool4
I0521 02:14:48.826269 20569 net.cpp:454] pool4 <- conv4
I0521 02:14:48.826282 20569 net.cpp:411] pool4 -> pool4
I0521 02:14:48.826350 20569 net.cpp:150] Setting up pool4
I0521 02:14:48.826364 20569 net.cpp:157] Top shape: 560 36 3 42 (2540160)
I0521 02:14:48.826375 20569 net.cpp:165] Memory required for data: 882091840
I0521 02:14:48.826385 20569 layer_factory.hpp:77] Creating layer ip1
I0521 02:14:48.826405 20569 net.cpp:106] Creating Layer ip1
I0521 02:14:48.826414 20569 net.cpp:454] ip1 <- pool4
I0521 02:14:48.826428 20569 net.cpp:411] ip1 -> ip1
I0521 02:14:48.841835 20569 net.cpp:150] Setting up ip1
I0521 02:14:48.841864 20569 net.cpp:157] Top shape: 560 196 (109760)
I0521 02:14:48.841876 20569 net.cpp:165] Memory required for data: 882530880
I0521 02:14:48.841898 20569 layer_factory.hpp:77] Creating layer relu5
I0521 02:14:48.841912 20569 net.cpp:106] Creating Layer relu5
I0521 02:14:48.841922 20569 net.cpp:454] relu5 <- ip1
I0521 02:14:48.841936 20569 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:14:48.842279 20569 net.cpp:150] Setting up relu5
I0521 02:14:48.842294 20569 net.cpp:157] Top shape: 560 196 (109760)
I0521 02:14:48.842304 20569 net.cpp:165] Memory required for data: 882969920
I0521 02:14:48.842315 20569 layer_factory.hpp:77] Creating layer drop1
I0521 02:14:48.842339 20569 net.cpp:106] Creating Layer drop1
I0521 02:14:48.842349 20569 net.cpp:454] drop1 <- ip1
I0521 02:14:48.842373 20569 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:14:48.842419 20569 net.cpp:150] Setting up drop1
I0521 02:14:48.842432 20569 net.cpp:157] Top shape: 560 196 (109760)
I0521 02:14:48.842443 20569 net.cpp:165] Memory required for data: 883408960
I0521 02:14:48.842453 20569 layer_factory.hpp:77] Creating layer ip2
I0521 02:14:48.842471 20569 net.cpp:106] Creating Layer ip2
I0521 02:14:48.842481 20569 net.cpp:454] ip2 <- ip1
I0521 02:14:48.842494 20569 net.cpp:411] ip2 -> ip2
I0521 02:14:48.842958 20569 net.cpp:150] Setting up ip2
I0521 02:14:48.842972 20569 net.cpp:157] Top shape: 560 98 (54880)
I0521 02:14:48.842981 20569 net.cpp:165] Memory required for data: 883628480
I0521 02:14:48.842996 20569 layer_factory.hpp:77] Creating layer relu6
I0521 02:14:48.843009 20569 net.cpp:106] Creating Layer relu6
I0521 02:14:48.843019 20569 net.cpp:454] relu6 <- ip2
I0521 02:14:48.843031 20569 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:14:48.843545 20569 net.cpp:150] Setting up relu6
I0521 02:14:48.843561 20569 net.cpp:157] Top shape: 560 98 (54880)
I0521 02:14:48.843572 20569 net.cpp:165] Memory required for data: 883848000
I0521 02:14:48.843583 20569 layer_factory.hpp:77] Creating layer drop2
I0521 02:14:48.843597 20569 net.cpp:106] Creating Layer drop2
I0521 02:14:48.843607 20569 net.cpp:454] drop2 <- ip2
I0521 02:14:48.843619 20569 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:14:48.843662 20569 net.cpp:150] Setting up drop2
I0521 02:14:48.843675 20569 net.cpp:157] Top shape: 560 98 (54880)
I0521 02:14:48.843685 20569 net.cpp:165] Memory required for data: 884067520
I0521 02:14:48.843695 20569 layer_factory.hpp:77] Creating layer ip3
I0521 02:14:48.843709 20569 net.cpp:106] Creating Layer ip3
I0521 02:14:48.843719 20569 net.cpp:454] ip3 <- ip2
I0521 02:14:48.843730 20569 net.cpp:411] ip3 -> ip3
I0521 02:14:48.843940 20569 net.cpp:150] Setting up ip3
I0521 02:14:48.843952 20569 net.cpp:157] Top shape: 560 11 (6160)
I0521 02:14:48.843963 20569 net.cpp:165] Memory required for data: 884092160
I0521 02:14:48.843978 20569 layer_factory.hpp:77] Creating layer drop3
I0521 02:14:48.843991 20569 net.cpp:106] Creating Layer drop3
I0521 02:14:48.844000 20569 net.cpp:454] drop3 <- ip3
I0521 02:14:48.844012 20569 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:14:48.844051 20569 net.cpp:150] Setting up drop3
I0521 02:14:48.844064 20569 net.cpp:157] Top shape: 560 11 (6160)
I0521 02:14:48.844074 20569 net.cpp:165] Memory required for data: 884116800
I0521 02:14:48.844084 20569 layer_factory.hpp:77] Creating layer loss
I0521 02:14:48.844102 20569 net.cpp:106] Creating Layer loss
I0521 02:14:48.844112 20569 net.cpp:454] loss <- ip3
I0521 02:14:48.844125 20569 net.cpp:454] loss <- label
I0521 02:14:48.844137 20569 net.cpp:411] loss -> loss
I0521 02:14:48.844154 20569 layer_factory.hpp:77] Creating layer loss
I0521 02:14:48.844808 20569 net.cpp:150] Setting up loss
I0521 02:14:48.844830 20569 net.cpp:157] Top shape: (1)
I0521 02:14:48.844842 20569 net.cpp:160]     with loss weight 1
I0521 02:14:48.844885 20569 net.cpp:165] Memory required for data: 884116804
I0521 02:14:48.844897 20569 net.cpp:226] loss needs backward computation.
I0521 02:14:48.844907 20569 net.cpp:226] drop3 needs backward computation.
I0521 02:14:48.844916 20569 net.cpp:226] ip3 needs backward computation.
I0521 02:14:48.844928 20569 net.cpp:226] drop2 needs backward computation.
I0521 02:14:48.844938 20569 net.cpp:226] relu6 needs backward computation.
I0521 02:14:48.844946 20569 net.cpp:226] ip2 needs backward computation.
I0521 02:14:48.844957 20569 net.cpp:226] drop1 needs backward computation.
I0521 02:14:48.844967 20569 net.cpp:226] relu5 needs backward computation.
I0521 02:14:48.844976 20569 net.cpp:226] ip1 needs backward computation.
I0521 02:14:48.844986 20569 net.cpp:226] pool4 needs backward computation.
I0521 02:14:48.844996 20569 net.cpp:226] relu4 needs backward computation.
I0521 02:14:48.845006 20569 net.cpp:226] conv4 needs backward computation.
I0521 02:14:48.845016 20569 net.cpp:226] pool3 needs backward computation.
I0521 02:14:48.845036 20569 net.cpp:226] relu3 needs backward computation.
I0521 02:14:48.845046 20569 net.cpp:226] conv3 needs backward computation.
I0521 02:14:48.845057 20569 net.cpp:226] pool2 needs backward computation.
I0521 02:14:48.845067 20569 net.cpp:226] relu2 needs backward computation.
I0521 02:14:48.845078 20569 net.cpp:226] conv2 needs backward computation.
I0521 02:14:48.845089 20569 net.cpp:226] pool1 needs backward computation.
I0521 02:14:48.845099 20569 net.cpp:226] relu1 needs backward computation.
I0521 02:14:48.845108 20569 net.cpp:226] conv1 needs backward computation.
I0521 02:14:48.845119 20569 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:14:48.845129 20569 net.cpp:270] This network produces output loss
I0521 02:14:48.845154 20569 net.cpp:283] Network initialization done.
I0521 02:14:48.846796 20569 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419.prototxt
I0521 02:14:48.846868 20569 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 02:14:48.847221 20569 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 560
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:14:48.847409 20569 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:14:48.847424 20569 net.cpp:106] Creating Layer data_hdf5
I0521 02:14:48.847436 20569 net.cpp:411] data_hdf5 -> data
I0521 02:14:48.847453 20569 net.cpp:411] data_hdf5 -> label
I0521 02:14:48.847470 20569 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 02:14:48.848762 20569 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 02:15:10.247764 20569 net.cpp:150] Setting up data_hdf5
I0521 02:15:10.247927 20569 net.cpp:157] Top shape: 560 1 127 50 (3556000)
I0521 02:15:10.247942 20569 net.cpp:157] Top shape: 560 (560)
I0521 02:15:10.247954 20569 net.cpp:165] Memory required for data: 14226240
I0521 02:15:10.247968 20569 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 02:15:10.247995 20569 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 02:15:10.248006 20569 net.cpp:454] label_data_hdf5_1_split <- label
I0521 02:15:10.248023 20569 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 02:15:10.248044 20569 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 02:15:10.248116 20569 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 02:15:10.248131 20569 net.cpp:157] Top shape: 560 (560)
I0521 02:15:10.248142 20569 net.cpp:157] Top shape: 560 (560)
I0521 02:15:10.248152 20569 net.cpp:165] Memory required for data: 14230720
I0521 02:15:10.248159 20569 layer_factory.hpp:77] Creating layer conv1
I0521 02:15:10.248183 20569 net.cpp:106] Creating Layer conv1
I0521 02:15:10.248193 20569 net.cpp:454] conv1 <- data
I0521 02:15:10.248208 20569 net.cpp:411] conv1 -> conv1
I0521 02:15:10.250147 20569 net.cpp:150] Setting up conv1
I0521 02:15:10.250171 20569 net.cpp:157] Top shape: 560 12 120 48 (38707200)
I0521 02:15:10.250181 20569 net.cpp:165] Memory required for data: 169059520
I0521 02:15:10.250203 20569 layer_factory.hpp:77] Creating layer relu1
I0521 02:15:10.250218 20569 net.cpp:106] Creating Layer relu1
I0521 02:15:10.250228 20569 net.cpp:454] relu1 <- conv1
I0521 02:15:10.250241 20569 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:15:10.250738 20569 net.cpp:150] Setting up relu1
I0521 02:15:10.250754 20569 net.cpp:157] Top shape: 560 12 120 48 (38707200)
I0521 02:15:10.250764 20569 net.cpp:165] Memory required for data: 323888320
I0521 02:15:10.250774 20569 layer_factory.hpp:77] Creating layer pool1
I0521 02:15:10.250792 20569 net.cpp:106] Creating Layer pool1
I0521 02:15:10.250800 20569 net.cpp:454] pool1 <- conv1
I0521 02:15:10.250813 20569 net.cpp:411] pool1 -> pool1
I0521 02:15:10.250888 20569 net.cpp:150] Setting up pool1
I0521 02:15:10.250901 20569 net.cpp:157] Top shape: 560 12 60 48 (19353600)
I0521 02:15:10.250911 20569 net.cpp:165] Memory required for data: 401302720
I0521 02:15:10.250921 20569 layer_factory.hpp:77] Creating layer conv2
I0521 02:15:10.250939 20569 net.cpp:106] Creating Layer conv2
I0521 02:15:10.250949 20569 net.cpp:454] conv2 <- pool1
I0521 02:15:10.250963 20569 net.cpp:411] conv2 -> conv2
I0521 02:15:10.252885 20569 net.cpp:150] Setting up conv2
I0521 02:15:10.252907 20569 net.cpp:157] Top shape: 560 20 54 46 (27820800)
I0521 02:15:10.252920 20569 net.cpp:165] Memory required for data: 512585920
I0521 02:15:10.252938 20569 layer_factory.hpp:77] Creating layer relu2
I0521 02:15:10.252951 20569 net.cpp:106] Creating Layer relu2
I0521 02:15:10.252961 20569 net.cpp:454] relu2 <- conv2
I0521 02:15:10.252974 20569 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:15:10.253307 20569 net.cpp:150] Setting up relu2
I0521 02:15:10.253321 20569 net.cpp:157] Top shape: 560 20 54 46 (27820800)
I0521 02:15:10.253331 20569 net.cpp:165] Memory required for data: 623869120
I0521 02:15:10.253341 20569 layer_factory.hpp:77] Creating layer pool2
I0521 02:15:10.253355 20569 net.cpp:106] Creating Layer pool2
I0521 02:15:10.253365 20569 net.cpp:454] pool2 <- conv2
I0521 02:15:10.253377 20569 net.cpp:411] pool2 -> pool2
I0521 02:15:10.253448 20569 net.cpp:150] Setting up pool2
I0521 02:15:10.253460 20569 net.cpp:157] Top shape: 560 20 27 46 (13910400)
I0521 02:15:10.253470 20569 net.cpp:165] Memory required for data: 679510720
I0521 02:15:10.253479 20569 layer_factory.hpp:77] Creating layer conv3
I0521 02:15:10.253497 20569 net.cpp:106] Creating Layer conv3
I0521 02:15:10.253509 20569 net.cpp:454] conv3 <- pool2
I0521 02:15:10.253521 20569 net.cpp:411] conv3 -> conv3
I0521 02:15:10.255484 20569 net.cpp:150] Setting up conv3
I0521 02:15:10.255507 20569 net.cpp:157] Top shape: 560 28 22 44 (15178240)
I0521 02:15:10.255518 20569 net.cpp:165] Memory required for data: 740223680
I0521 02:15:10.255551 20569 layer_factory.hpp:77] Creating layer relu3
I0521 02:15:10.255564 20569 net.cpp:106] Creating Layer relu3
I0521 02:15:10.255574 20569 net.cpp:454] relu3 <- conv3
I0521 02:15:10.255587 20569 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:15:10.256062 20569 net.cpp:150] Setting up relu3
I0521 02:15:10.256078 20569 net.cpp:157] Top shape: 560 28 22 44 (15178240)
I0521 02:15:10.256088 20569 net.cpp:165] Memory required for data: 800936640
I0521 02:15:10.256098 20569 layer_factory.hpp:77] Creating layer pool3
I0521 02:15:10.256110 20569 net.cpp:106] Creating Layer pool3
I0521 02:15:10.256120 20569 net.cpp:454] pool3 <- conv3
I0521 02:15:10.256134 20569 net.cpp:411] pool3 -> pool3
I0521 02:15:10.256204 20569 net.cpp:150] Setting up pool3
I0521 02:15:10.256217 20569 net.cpp:157] Top shape: 560 28 11 44 (7589120)
I0521 02:15:10.256227 20569 net.cpp:165] Memory required for data: 831293120
I0521 02:15:10.256235 20569 layer_factory.hpp:77] Creating layer conv4
I0521 02:15:10.256253 20569 net.cpp:106] Creating Layer conv4
I0521 02:15:10.256263 20569 net.cpp:454] conv4 <- pool3
I0521 02:15:10.256278 20569 net.cpp:411] conv4 -> conv4
I0521 02:15:10.258339 20569 net.cpp:150] Setting up conv4
I0521 02:15:10.258358 20569 net.cpp:157] Top shape: 560 36 6 42 (5080320)
I0521 02:15:10.258368 20569 net.cpp:165] Memory required for data: 851614400
I0521 02:15:10.258384 20569 layer_factory.hpp:77] Creating layer relu4
I0521 02:15:10.258399 20569 net.cpp:106] Creating Layer relu4
I0521 02:15:10.258409 20569 net.cpp:454] relu4 <- conv4
I0521 02:15:10.258421 20569 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:15:10.258889 20569 net.cpp:150] Setting up relu4
I0521 02:15:10.258905 20569 net.cpp:157] Top shape: 560 36 6 42 (5080320)
I0521 02:15:10.258916 20569 net.cpp:165] Memory required for data: 871935680
I0521 02:15:10.258926 20569 layer_factory.hpp:77] Creating layer pool4
I0521 02:15:10.258939 20569 net.cpp:106] Creating Layer pool4
I0521 02:15:10.258949 20569 net.cpp:454] pool4 <- conv4
I0521 02:15:10.258962 20569 net.cpp:411] pool4 -> pool4
I0521 02:15:10.259033 20569 net.cpp:150] Setting up pool4
I0521 02:15:10.259047 20569 net.cpp:157] Top shape: 560 36 3 42 (2540160)
I0521 02:15:10.259057 20569 net.cpp:165] Memory required for data: 882096320
I0521 02:15:10.259065 20569 layer_factory.hpp:77] Creating layer ip1
I0521 02:15:10.259080 20569 net.cpp:106] Creating Layer ip1
I0521 02:15:10.259090 20569 net.cpp:454] ip1 <- pool4
I0521 02:15:10.259104 20569 net.cpp:411] ip1 -> ip1
I0521 02:15:10.274616 20569 net.cpp:150] Setting up ip1
I0521 02:15:10.274643 20569 net.cpp:157] Top shape: 560 196 (109760)
I0521 02:15:10.274654 20569 net.cpp:165] Memory required for data: 882535360
I0521 02:15:10.274677 20569 layer_factory.hpp:77] Creating layer relu5
I0521 02:15:10.274691 20569 net.cpp:106] Creating Layer relu5
I0521 02:15:10.274701 20569 net.cpp:454] relu5 <- ip1
I0521 02:15:10.274715 20569 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:15:10.275063 20569 net.cpp:150] Setting up relu5
I0521 02:15:10.275076 20569 net.cpp:157] Top shape: 560 196 (109760)
I0521 02:15:10.275086 20569 net.cpp:165] Memory required for data: 882974400
I0521 02:15:10.275096 20569 layer_factory.hpp:77] Creating layer drop1
I0521 02:15:10.275117 20569 net.cpp:106] Creating Layer drop1
I0521 02:15:10.275126 20569 net.cpp:454] drop1 <- ip1
I0521 02:15:10.275141 20569 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:15:10.275184 20569 net.cpp:150] Setting up drop1
I0521 02:15:10.275197 20569 net.cpp:157] Top shape: 560 196 (109760)
I0521 02:15:10.275207 20569 net.cpp:165] Memory required for data: 883413440
I0521 02:15:10.275218 20569 layer_factory.hpp:77] Creating layer ip2
I0521 02:15:10.275231 20569 net.cpp:106] Creating Layer ip2
I0521 02:15:10.275241 20569 net.cpp:454] ip2 <- ip1
I0521 02:15:10.275256 20569 net.cpp:411] ip2 -> ip2
I0521 02:15:10.275735 20569 net.cpp:150] Setting up ip2
I0521 02:15:10.275749 20569 net.cpp:157] Top shape: 560 98 (54880)
I0521 02:15:10.275758 20569 net.cpp:165] Memory required for data: 883632960
I0521 02:15:10.275787 20569 layer_factory.hpp:77] Creating layer relu6
I0521 02:15:10.275800 20569 net.cpp:106] Creating Layer relu6
I0521 02:15:10.275810 20569 net.cpp:454] relu6 <- ip2
I0521 02:15:10.275823 20569 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:15:10.276360 20569 net.cpp:150] Setting up relu6
I0521 02:15:10.276376 20569 net.cpp:157] Top shape: 560 98 (54880)
I0521 02:15:10.276386 20569 net.cpp:165] Memory required for data: 883852480
I0521 02:15:10.276396 20569 layer_factory.hpp:77] Creating layer drop2
I0521 02:15:10.276412 20569 net.cpp:106] Creating Layer drop2
I0521 02:15:10.276422 20569 net.cpp:454] drop2 <- ip2
I0521 02:15:10.276435 20569 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:15:10.276479 20569 net.cpp:150] Setting up drop2
I0521 02:15:10.276492 20569 net.cpp:157] Top shape: 560 98 (54880)
I0521 02:15:10.276502 20569 net.cpp:165] Memory required for data: 884072000
I0521 02:15:10.276512 20569 layer_factory.hpp:77] Creating layer ip3
I0521 02:15:10.276526 20569 net.cpp:106] Creating Layer ip3
I0521 02:15:10.276536 20569 net.cpp:454] ip3 <- ip2
I0521 02:15:10.276551 20569 net.cpp:411] ip3 -> ip3
I0521 02:15:10.276772 20569 net.cpp:150] Setting up ip3
I0521 02:15:10.276784 20569 net.cpp:157] Top shape: 560 11 (6160)
I0521 02:15:10.276794 20569 net.cpp:165] Memory required for data: 884096640
I0521 02:15:10.276810 20569 layer_factory.hpp:77] Creating layer drop3
I0521 02:15:10.276823 20569 net.cpp:106] Creating Layer drop3
I0521 02:15:10.276834 20569 net.cpp:454] drop3 <- ip3
I0521 02:15:10.276845 20569 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:15:10.276887 20569 net.cpp:150] Setting up drop3
I0521 02:15:10.276901 20569 net.cpp:157] Top shape: 560 11 (6160)
I0521 02:15:10.276909 20569 net.cpp:165] Memory required for data: 884121280
I0521 02:15:10.276919 20569 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 02:15:10.276932 20569 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 02:15:10.276942 20569 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 02:15:10.276955 20569 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 02:15:10.276970 20569 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 02:15:10.277043 20569 net.cpp:150] Setting up ip3_drop3_0_split
I0521 02:15:10.277056 20569 net.cpp:157] Top shape: 560 11 (6160)
I0521 02:15:10.277068 20569 net.cpp:157] Top shape: 560 11 (6160)
I0521 02:15:10.277079 20569 net.cpp:165] Memory required for data: 884170560
I0521 02:15:10.277089 20569 layer_factory.hpp:77] Creating layer accuracy
I0521 02:15:10.277110 20569 net.cpp:106] Creating Layer accuracy
I0521 02:15:10.277120 20569 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 02:15:10.277132 20569 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 02:15:10.277145 20569 net.cpp:411] accuracy -> accuracy
I0521 02:15:10.277168 20569 net.cpp:150] Setting up accuracy
I0521 02:15:10.277180 20569 net.cpp:157] Top shape: (1)
I0521 02:15:10.277190 20569 net.cpp:165] Memory required for data: 884170564
I0521 02:15:10.277200 20569 layer_factory.hpp:77] Creating layer loss
I0521 02:15:10.277215 20569 net.cpp:106] Creating Layer loss
I0521 02:15:10.277225 20569 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 02:15:10.277235 20569 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 02:15:10.277248 20569 net.cpp:411] loss -> loss
I0521 02:15:10.277266 20569 layer_factory.hpp:77] Creating layer loss
I0521 02:15:10.277756 20569 net.cpp:150] Setting up loss
I0521 02:15:10.277770 20569 net.cpp:157] Top shape: (1)
I0521 02:15:10.277781 20569 net.cpp:160]     with loss weight 1
I0521 02:15:10.277798 20569 net.cpp:165] Memory required for data: 884170568
I0521 02:15:10.277809 20569 net.cpp:226] loss needs backward computation.
I0521 02:15:10.277820 20569 net.cpp:228] accuracy does not need backward computation.
I0521 02:15:10.277832 20569 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 02:15:10.277842 20569 net.cpp:226] drop3 needs backward computation.
I0521 02:15:10.277849 20569 net.cpp:226] ip3 needs backward computation.
I0521 02:15:10.277860 20569 net.cpp:226] drop2 needs backward computation.
I0521 02:15:10.277879 20569 net.cpp:226] relu6 needs backward computation.
I0521 02:15:10.277890 20569 net.cpp:226] ip2 needs backward computation.
I0521 02:15:10.277900 20569 net.cpp:226] drop1 needs backward computation.
I0521 02:15:10.277909 20569 net.cpp:226] relu5 needs backward computation.
I0521 02:15:10.277920 20569 net.cpp:226] ip1 needs backward computation.
I0521 02:15:10.277930 20569 net.cpp:226] pool4 needs backward computation.
I0521 02:15:10.277938 20569 net.cpp:226] relu4 needs backward computation.
I0521 02:15:10.277950 20569 net.cpp:226] conv4 needs backward computation.
I0521 02:15:10.277959 20569 net.cpp:226] pool3 needs backward computation.
I0521 02:15:10.277969 20569 net.cpp:226] relu3 needs backward computation.
I0521 02:15:10.277977 20569 net.cpp:226] conv3 needs backward computation.
I0521 02:15:10.277988 20569 net.cpp:226] pool2 needs backward computation.
I0521 02:15:10.277998 20569 net.cpp:226] relu2 needs backward computation.
I0521 02:15:10.278010 20569 net.cpp:226] conv2 needs backward computation.
I0521 02:15:10.278020 20569 net.cpp:226] pool1 needs backward computation.
I0521 02:15:10.278030 20569 net.cpp:226] relu1 needs backward computation.
I0521 02:15:10.278040 20569 net.cpp:226] conv1 needs backward computation.
I0521 02:15:10.278051 20569 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 02:15:10.278062 20569 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:15:10.278072 20569 net.cpp:270] This network produces output accuracy
I0521 02:15:10.278084 20569 net.cpp:270] This network produces output loss
I0521 02:15:10.278111 20569 net.cpp:283] Network initialization done.
I0521 02:15:10.278245 20569 solver.cpp:60] Solver scaffolding done.
I0521 02:15:10.279373 20569 caffe.cpp:212] Starting Optimization
I0521 02:15:10.279392 20569 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 02:15:10.279405 20569 solver.cpp:289] Learning Rate Policy: fixed
I0521 02:15:10.280629 20569 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 02:15:56.252857 20569 solver.cpp:409]     Test net output #0: accuracy = 0.0921147
I0521 02:15:56.253018 20569 solver.cpp:409]     Test net output #1: loss = 2.39896 (* 1 = 2.39896 loss)
I0521 02:15:56.361414 20569 solver.cpp:237] Iteration 0, loss = 2.39957
I0521 02:15:56.361450 20569 solver.cpp:253]     Train net output #0: loss = 2.39957 (* 1 = 2.39957 loss)
I0521 02:15:56.361469 20569 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 02:16:04.171743 20569 solver.cpp:237] Iteration 26, loss = 2.3827
I0521 02:16:04.171782 20569 solver.cpp:253]     Train net output #0: loss = 2.3827 (* 1 = 2.3827 loss)
I0521 02:16:04.171802 20569 sgd_solver.cpp:106] Iteration 26, lr = 0.0025
I0521 02:16:11.983379 20569 solver.cpp:237] Iteration 52, loss = 2.36798
I0521 02:16:11.983410 20569 solver.cpp:253]     Train net output #0: loss = 2.36798 (* 1 = 2.36798 loss)
I0521 02:16:11.983424 20569 sgd_solver.cpp:106] Iteration 52, lr = 0.0025
I0521 02:16:19.789774 20569 solver.cpp:237] Iteration 78, loss = 2.34171
I0521 02:16:19.789806 20569 solver.cpp:253]     Train net output #0: loss = 2.34171 (* 1 = 2.34171 loss)
I0521 02:16:19.789820 20569 sgd_solver.cpp:106] Iteration 78, lr = 0.0025
I0521 02:16:27.607189 20569 solver.cpp:237] Iteration 104, loss = 2.35195
I0521 02:16:27.607342 20569 solver.cpp:253]     Train net output #0: loss = 2.35195 (* 1 = 2.35195 loss)
I0521 02:16:27.607355 20569 sgd_solver.cpp:106] Iteration 104, lr = 0.0025
I0521 02:16:35.415071 20569 solver.cpp:237] Iteration 130, loss = 2.3155
I0521 02:16:35.415103 20569 solver.cpp:253]     Train net output #0: loss = 2.3155 (* 1 = 2.3155 loss)
I0521 02:16:35.415119 20569 sgd_solver.cpp:106] Iteration 130, lr = 0.0025
I0521 02:16:43.228796 20569 solver.cpp:237] Iteration 156, loss = 2.31351
I0521 02:16:43.228828 20569 solver.cpp:253]     Train net output #0: loss = 2.31351 (* 1 = 2.31351 loss)
I0521 02:16:43.228845 20569 sgd_solver.cpp:106] Iteration 156, lr = 0.0025
I0521 02:17:13.330219 20569 solver.cpp:237] Iteration 182, loss = 2.31526
I0521 02:17:13.330379 20569 solver.cpp:253]     Train net output #0: loss = 2.31526 (* 1 = 2.31526 loss)
I0521 02:17:13.330394 20569 sgd_solver.cpp:106] Iteration 182, lr = 0.0025
I0521 02:17:21.140281 20569 solver.cpp:237] Iteration 208, loss = 2.30597
I0521 02:17:21.140321 20569 solver.cpp:253]     Train net output #0: loss = 2.30597 (* 1 = 2.30597 loss)
I0521 02:17:21.140334 20569 sgd_solver.cpp:106] Iteration 208, lr = 0.0025
I0521 02:17:28.952447 20569 solver.cpp:237] Iteration 234, loss = 2.26596
I0521 02:17:28.952481 20569 solver.cpp:253]     Train net output #0: loss = 2.26596 (* 1 = 2.26596 loss)
I0521 02:17:28.952498 20569 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0521 02:17:36.762975 20569 solver.cpp:237] Iteration 260, loss = 2.24977
I0521 02:17:36.763017 20569 solver.cpp:253]     Train net output #0: loss = 2.24977 (* 1 = 2.24977 loss)
I0521 02:17:36.763037 20569 sgd_solver.cpp:106] Iteration 260, lr = 0.0025
I0521 02:17:38.566150 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_267.caffemodel
I0521 02:17:41.182940 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_267.solverstate
I0521 02:17:48.691186 20569 solver.cpp:237] Iteration 286, loss = 2.16743
I0521 02:17:48.691339 20569 solver.cpp:253]     Train net output #0: loss = 2.16743 (* 1 = 2.16743 loss)
I0521 02:17:48.691352 20569 sgd_solver.cpp:106] Iteration 286, lr = 0.0025
I0521 02:17:56.500547 20569 solver.cpp:237] Iteration 312, loss = 2.16753
I0521 02:17:56.500579 20569 solver.cpp:253]     Train net output #0: loss = 2.16753 (* 1 = 2.16753 loss)
I0521 02:17:56.500597 20569 sgd_solver.cpp:106] Iteration 312, lr = 0.0025
I0521 02:18:04.315819 20569 solver.cpp:237] Iteration 338, loss = 2.15922
I0521 02:18:04.315863 20569 solver.cpp:253]     Train net output #0: loss = 2.15922 (* 1 = 2.15922 loss)
I0521 02:18:04.315879 20569 sgd_solver.cpp:106] Iteration 338, lr = 0.0025
I0521 02:18:34.234700 20569 solver.cpp:237] Iteration 364, loss = 2.168
I0521 02:18:34.234855 20569 solver.cpp:253]     Train net output #0: loss = 2.168 (* 1 = 2.168 loss)
I0521 02:18:34.234869 20569 sgd_solver.cpp:106] Iteration 364, lr = 0.0025
I0521 02:18:42.047672 20569 solver.cpp:237] Iteration 390, loss = 2.0904
I0521 02:18:42.047703 20569 solver.cpp:253]     Train net output #0: loss = 2.0904 (* 1 = 2.0904 loss)
I0521 02:18:42.047719 20569 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 02:18:49.861037 20569 solver.cpp:237] Iteration 416, loss = 2.04992
I0521 02:18:49.861070 20569 solver.cpp:253]     Train net output #0: loss = 2.04992 (* 1 = 2.04992 loss)
I0521 02:18:49.861086 20569 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 02:18:57.674918 20569 solver.cpp:237] Iteration 442, loss = 2.01736
I0521 02:18:57.674957 20569 solver.cpp:253]     Train net output #0: loss = 2.01736 (* 1 = 2.01736 loss)
I0521 02:18:57.674973 20569 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0521 02:19:05.486451 20569 solver.cpp:237] Iteration 468, loss = 1.97108
I0521 02:19:05.486600 20569 solver.cpp:253]     Train net output #0: loss = 1.97108 (* 1 = 1.97108 loss)
I0521 02:19:05.486613 20569 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0521 02:19:13.301008 20569 solver.cpp:237] Iteration 494, loss = 1.97726
I0521 02:19:13.301039 20569 solver.cpp:253]     Train net output #0: loss = 1.97726 (* 1 = 1.97726 loss)
I0521 02:19:13.301057 20569 sgd_solver.cpp:106] Iteration 494, lr = 0.0025
I0521 02:19:21.111202 20569 solver.cpp:237] Iteration 520, loss = 1.95898
I0521 02:19:21.111241 20569 solver.cpp:253]     Train net output #0: loss = 1.95898 (* 1 = 1.95898 loss)
I0521 02:19:21.111261 20569 sgd_solver.cpp:106] Iteration 520, lr = 0.0025
I0521 02:19:25.018945 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_534.caffemodel
I0521 02:19:25.268409 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_534.solverstate
I0521 02:19:25.384431 20569 solver.cpp:341] Iteration 535, Testing net (#0)
I0521 02:20:10.547920 20569 solver.cpp:409]     Test net output #0: accuracy = 0.557003
I0521 02:20:10.548077 20569 solver.cpp:409]     Test net output #1: loss = 1.67896 (* 1 = 1.67896 loss)
I0521 02:20:36.059502 20569 solver.cpp:237] Iteration 546, loss = 2.02469
I0521 02:20:36.059553 20569 solver.cpp:253]     Train net output #0: loss = 2.02469 (* 1 = 2.02469 loss)
I0521 02:20:36.059568 20569 sgd_solver.cpp:106] Iteration 546, lr = 0.0025
I0521 02:20:43.861670 20569 solver.cpp:237] Iteration 572, loss = 1.90809
I0521 02:20:43.861822 20569 solver.cpp:253]     Train net output #0: loss = 1.90809 (* 1 = 1.90809 loss)
I0521 02:20:43.861836 20569 sgd_solver.cpp:106] Iteration 572, lr = 0.0025
I0521 02:20:51.663825 20569 solver.cpp:237] Iteration 598, loss = 1.88344
I0521 02:20:51.663857 20569 solver.cpp:253]     Train net output #0: loss = 1.88344 (* 1 = 1.88344 loss)
I0521 02:20:51.663874 20569 sgd_solver.cpp:106] Iteration 598, lr = 0.0025
I0521 02:20:59.472429 20569 solver.cpp:237] Iteration 624, loss = 1.91404
I0521 02:20:59.472471 20569 solver.cpp:253]     Train net output #0: loss = 1.91404 (* 1 = 1.91404 loss)
I0521 02:20:59.472486 20569 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 02:21:07.279238 20569 solver.cpp:237] Iteration 650, loss = 1.89218
I0521 02:21:07.279271 20569 solver.cpp:253]     Train net output #0: loss = 1.89218 (* 1 = 1.89218 loss)
I0521 02:21:07.279285 20569 sgd_solver.cpp:106] Iteration 650, lr = 0.0025
I0521 02:21:15.086982 20569 solver.cpp:237] Iteration 676, loss = 1.86837
I0521 02:21:15.087126 20569 solver.cpp:253]     Train net output #0: loss = 1.86837 (* 1 = 1.86837 loss)
I0521 02:21:15.087139 20569 sgd_solver.cpp:106] Iteration 676, lr = 0.0025
I0521 02:21:22.890883 20569 solver.cpp:237] Iteration 702, loss = 1.86946
I0521 02:21:22.890928 20569 solver.cpp:253]     Train net output #0: loss = 1.86946 (* 1 = 1.86946 loss)
I0521 02:21:22.890945 20569 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 02:21:52.851788 20569 solver.cpp:237] Iteration 728, loss = 1.86572
I0521 02:21:52.851960 20569 solver.cpp:253]     Train net output #0: loss = 1.86572 (* 1 = 1.86572 loss)
I0521 02:21:52.851974 20569 sgd_solver.cpp:106] Iteration 728, lr = 0.0025
I0521 02:22:00.660487 20569 solver.cpp:237] Iteration 754, loss = 1.88552
I0521 02:22:00.660519 20569 solver.cpp:253]     Train net output #0: loss = 1.88552 (* 1 = 1.88552 loss)
I0521 02:22:00.660536 20569 sgd_solver.cpp:106] Iteration 754, lr = 0.0025
I0521 02:22:08.470201 20569 solver.cpp:237] Iteration 780, loss = 1.85765
I0521 02:22:08.470233 20569 solver.cpp:253]     Train net output #0: loss = 1.85765 (* 1 = 1.85765 loss)
I0521 02:22:08.470249 20569 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 02:22:14.478330 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_801.caffemodel
I0521 02:22:14.730795 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_801.solverstate
I0521 02:22:16.349534 20569 solver.cpp:237] Iteration 806, loss = 1.84041
I0521 02:22:16.349582 20569 solver.cpp:253]     Train net output #0: loss = 1.84041 (* 1 = 1.84041 loss)
I0521 02:22:16.349597 20569 sgd_solver.cpp:106] Iteration 806, lr = 0.0025
I0521 02:22:24.156667 20569 solver.cpp:237] Iteration 832, loss = 1.88369
I0521 02:22:24.156810 20569 solver.cpp:253]     Train net output #0: loss = 1.88369 (* 1 = 1.88369 loss)
I0521 02:22:24.156822 20569 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 02:22:31.962045 20569 solver.cpp:237] Iteration 858, loss = 1.84791
I0521 02:22:31.962077 20569 solver.cpp:253]     Train net output #0: loss = 1.84791 (* 1 = 1.84791 loss)
I0521 02:22:31.962095 20569 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0521 02:22:39.767738 20569 solver.cpp:237] Iteration 884, loss = 1.87707
I0521 02:22:39.767781 20569 solver.cpp:253]     Train net output #0: loss = 1.87707 (* 1 = 1.87707 loss)
I0521 02:22:39.767798 20569 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0521 02:23:09.757746 20569 solver.cpp:237] Iteration 910, loss = 1.7742
I0521 02:23:09.757905 20569 solver.cpp:253]     Train net output #0: loss = 1.7742 (* 1 = 1.7742 loss)
I0521 02:23:09.757920 20569 sgd_solver.cpp:106] Iteration 910, lr = 0.0025
I0521 02:23:17.564622 20569 solver.cpp:237] Iteration 936, loss = 1.82589
I0521 02:23:17.564656 20569 solver.cpp:253]     Train net output #0: loss = 1.82589 (* 1 = 1.82589 loss)
I0521 02:23:17.564669 20569 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 02:23:25.372277 20569 solver.cpp:237] Iteration 962, loss = 1.81158
I0521 02:23:25.372313 20569 solver.cpp:253]     Train net output #0: loss = 1.81158 (* 1 = 1.81158 loss)
I0521 02:23:25.372326 20569 sgd_solver.cpp:106] Iteration 962, lr = 0.0025
I0521 02:23:33.181471 20569 solver.cpp:237] Iteration 988, loss = 1.91541
I0521 02:23:33.181509 20569 solver.cpp:253]     Train net output #0: loss = 1.91541 (* 1 = 1.91541 loss)
I0521 02:23:33.181529 20569 sgd_solver.cpp:106] Iteration 988, lr = 0.0025
I0521 02:23:40.992632 20569 solver.cpp:237] Iteration 1014, loss = 1.76666
I0521 02:23:40.992769 20569 solver.cpp:253]     Train net output #0: loss = 1.76666 (* 1 = 1.76666 loss)
I0521 02:23:40.992781 20569 sgd_solver.cpp:106] Iteration 1014, lr = 0.0025
I0521 02:23:48.800607 20569 solver.cpp:237] Iteration 1040, loss = 1.75558
I0521 02:23:48.800639 20569 solver.cpp:253]     Train net output #0: loss = 1.75558 (* 1 = 1.75558 loss)
I0521 02:23:48.800657 20569 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 02:23:56.605248 20569 solver.cpp:237] Iteration 1066, loss = 1.71769
I0521 02:23:56.605291 20569 solver.cpp:253]     Train net output #0: loss = 1.71769 (* 1 = 1.71769 loss)
I0521 02:23:56.605305 20569 sgd_solver.cpp:106] Iteration 1066, lr = 0.0025
I0521 02:23:56.906055 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1068.caffemodel
I0521 02:23:57.156505 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1068.solverstate
I0521 02:23:57.575497 20569 solver.cpp:341] Iteration 1070, Testing net (#0)
I0521 02:25:03.671325 20569 solver.cpp:409]     Test net output #0: accuracy = 0.628718
I0521 02:25:03.671494 20569 solver.cpp:409]     Test net output #1: loss = 1.29702 (* 1 = 1.29702 loss)
I0521 02:25:32.525723 20569 solver.cpp:237] Iteration 1092, loss = 1.77925
I0521 02:25:32.525771 20569 solver.cpp:253]     Train net output #0: loss = 1.77925 (* 1 = 1.77925 loss)
I0521 02:25:32.525789 20569 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0521 02:25:40.326452 20569 solver.cpp:237] Iteration 1118, loss = 1.74872
I0521 02:25:40.326611 20569 solver.cpp:253]     Train net output #0: loss = 1.74872 (* 1 = 1.74872 loss)
I0521 02:25:40.326623 20569 sgd_solver.cpp:106] Iteration 1118, lr = 0.0025
I0521 02:25:48.130285 20569 solver.cpp:237] Iteration 1144, loss = 1.78733
I0521 02:25:48.130316 20569 solver.cpp:253]     Train net output #0: loss = 1.78733 (* 1 = 1.78733 loss)
I0521 02:25:48.130333 20569 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0521 02:25:55.935848 20569 solver.cpp:237] Iteration 1170, loss = 1.75712
I0521 02:25:55.935879 20569 solver.cpp:253]     Train net output #0: loss = 1.75712 (* 1 = 1.75712 loss)
I0521 02:25:55.935895 20569 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 02:26:03.740411 20569 solver.cpp:237] Iteration 1196, loss = 1.77326
I0521 02:26:03.740448 20569 solver.cpp:253]     Train net output #0: loss = 1.77326 (* 1 = 1.77326 loss)
I0521 02:26:03.740468 20569 sgd_solver.cpp:106] Iteration 1196, lr = 0.0025
I0521 02:26:11.544600 20569 solver.cpp:237] Iteration 1222, loss = 1.71518
I0521 02:26:11.544735 20569 solver.cpp:253]     Train net output #0: loss = 1.71518 (* 1 = 1.71518 loss)
I0521 02:26:11.544749 20569 sgd_solver.cpp:106] Iteration 1222, lr = 0.0025
I0521 02:26:19.348074 20569 solver.cpp:237] Iteration 1248, loss = 1.78604
I0521 02:26:19.348105 20569 solver.cpp:253]     Train net output #0: loss = 1.78604 (* 1 = 1.78604 loss)
I0521 02:26:19.348124 20569 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 02:26:49.314316 20569 solver.cpp:237] Iteration 1274, loss = 1.76592
I0521 02:26:49.314478 20569 solver.cpp:253]     Train net output #0: loss = 1.76592 (* 1 = 1.76592 loss)
I0521 02:26:49.314492 20569 sgd_solver.cpp:106] Iteration 1274, lr = 0.0025
I0521 02:26:57.117569 20569 solver.cpp:237] Iteration 1300, loss = 1.79823
I0521 02:26:57.117601 20569 solver.cpp:253]     Train net output #0: loss = 1.79823 (* 1 = 1.79823 loss)
I0521 02:26:57.117619 20569 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0521 02:27:04.926218 20569 solver.cpp:237] Iteration 1326, loss = 1.66833
I0521 02:27:04.926249 20569 solver.cpp:253]     Train net output #0: loss = 1.66833 (* 1 = 1.66833 loss)
I0521 02:27:04.926265 20569 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0521 02:27:07.325624 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1335.caffemodel
I0521 02:27:07.576158 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1335.solverstate
I0521 02:27:12.797076 20569 solver.cpp:237] Iteration 1352, loss = 1.79499
I0521 02:27:12.797123 20569 solver.cpp:253]     Train net output #0: loss = 1.79499 (* 1 = 1.79499 loss)
I0521 02:27:12.797140 20569 sgd_solver.cpp:106] Iteration 1352, lr = 0.0025
I0521 02:27:20.602244 20569 solver.cpp:237] Iteration 1378, loss = 1.70442
I0521 02:27:20.602394 20569 solver.cpp:253]     Train net output #0: loss = 1.70442 (* 1 = 1.70442 loss)
I0521 02:27:20.602407 20569 sgd_solver.cpp:106] Iteration 1378, lr = 0.0025
I0521 02:27:28.404351 20569 solver.cpp:237] Iteration 1404, loss = 1.70561
I0521 02:27:28.404382 20569 solver.cpp:253]     Train net output #0: loss = 1.70561 (* 1 = 1.70561 loss)
I0521 02:27:28.404397 20569 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 02:27:58.420048 20569 solver.cpp:237] Iteration 1430, loss = 1.72192
I0521 02:27:58.420212 20569 solver.cpp:253]     Train net output #0: loss = 1.72192 (* 1 = 1.72192 loss)
I0521 02:27:58.420225 20569 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0521 02:28:06.224910 20569 solver.cpp:237] Iteration 1456, loss = 1.72793
I0521 02:28:06.224944 20569 solver.cpp:253]     Train net output #0: loss = 1.72793 (* 1 = 1.72793 loss)
I0521 02:28:06.224961 20569 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 02:28:14.026077 20569 solver.cpp:237] Iteration 1482, loss = 1.73141
I0521 02:28:14.026111 20569 solver.cpp:253]     Train net output #0: loss = 1.73141 (* 1 = 1.73141 loss)
I0521 02:28:14.026129 20569 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0521 02:28:21.828248 20569 solver.cpp:237] Iteration 1508, loss = 1.68789
I0521 02:28:21.828280 20569 solver.cpp:253]     Train net output #0: loss = 1.68789 (* 1 = 1.68789 loss)
I0521 02:28:21.828304 20569 sgd_solver.cpp:106] Iteration 1508, lr = 0.0025
I0521 02:28:29.626472 20569 solver.cpp:237] Iteration 1534, loss = 1.74991
I0521 02:28:29.626624 20569 solver.cpp:253]     Train net output #0: loss = 1.74991 (* 1 = 1.74991 loss)
I0521 02:28:29.626638 20569 sgd_solver.cpp:106] Iteration 1534, lr = 0.0025
I0521 02:28:37.428740 20569 solver.cpp:237] Iteration 1560, loss = 1.72598
I0521 02:28:37.428772 20569 solver.cpp:253]     Train net output #0: loss = 1.72598 (* 1 = 1.72598 loss)
I0521 02:28:37.428789 20569 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 02:28:45.230355 20569 solver.cpp:237] Iteration 1586, loss = 1.74702
I0521 02:28:45.230386 20569 solver.cpp:253]     Train net output #0: loss = 1.74702 (* 1 = 1.74702 loss)
I0521 02:28:45.230404 20569 sgd_solver.cpp:106] Iteration 1586, lr = 0.0025
I0521 02:28:49.735544 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1602.caffemodel
I0521 02:28:49.983909 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1602.solverstate
I0521 02:28:50.699267 20569 solver.cpp:341] Iteration 1605, Testing net (#0)
I0521 02:29:35.599452 20569 solver.cpp:409]     Test net output #0: accuracy = 0.659985
I0521 02:29:35.599617 20569 solver.cpp:409]     Test net output #1: loss = 1.19623 (* 1 = 1.19623 loss)
I0521 02:29:59.978664 20569 solver.cpp:237] Iteration 1612, loss = 1.75091
I0521 02:29:59.978715 20569 solver.cpp:253]     Train net output #0: loss = 1.75091 (* 1 = 1.75091 loss)
I0521 02:29:59.978729 20569 sgd_solver.cpp:106] Iteration 1612, lr = 0.0025
I0521 02:30:07.783969 20569 solver.cpp:237] Iteration 1638, loss = 1.63082
I0521 02:30:07.784129 20569 solver.cpp:253]     Train net output #0: loss = 1.63082 (* 1 = 1.63082 loss)
I0521 02:30:07.784143 20569 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 02:30:15.590746 20569 solver.cpp:237] Iteration 1664, loss = 1.72431
I0521 02:30:15.590778 20569 solver.cpp:253]     Train net output #0: loss = 1.72431 (* 1 = 1.72431 loss)
I0521 02:30:15.590795 20569 sgd_solver.cpp:106] Iteration 1664, lr = 0.0025
I0521 02:30:23.395941 20569 solver.cpp:237] Iteration 1690, loss = 1.61759
I0521 02:30:23.395973 20569 solver.cpp:253]     Train net output #0: loss = 1.61759 (* 1 = 1.61759 loss)
I0521 02:30:23.395987 20569 sgd_solver.cpp:106] Iteration 1690, lr = 0.0025
I0521 02:30:31.204258 20569 solver.cpp:237] Iteration 1716, loss = 1.62255
I0521 02:30:31.204290 20569 solver.cpp:253]     Train net output #0: loss = 1.62255 (* 1 = 1.62255 loss)
I0521 02:30:31.204313 20569 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0521 02:30:39.008985 20569 solver.cpp:237] Iteration 1742, loss = 1.69348
I0521 02:30:39.009135 20569 solver.cpp:253]     Train net output #0: loss = 1.69348 (* 1 = 1.69348 loss)
I0521 02:30:39.009150 20569 sgd_solver.cpp:106] Iteration 1742, lr = 0.0025
I0521 02:30:46.817188 20569 solver.cpp:237] Iteration 1768, loss = 1.68818
I0521 02:30:46.817220 20569 solver.cpp:253]     Train net output #0: loss = 1.68818 (* 1 = 1.68818 loss)
I0521 02:30:46.817239 20569 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0521 02:31:16.808188 20569 solver.cpp:237] Iteration 1794, loss = 1.64965
I0521 02:31:16.808372 20569 solver.cpp:253]     Train net output #0: loss = 1.64965 (* 1 = 1.64965 loss)
I0521 02:31:16.808385 20569 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0521 02:31:24.611788 20569 solver.cpp:237] Iteration 1820, loss = 1.689
I0521 02:31:24.611820 20569 solver.cpp:253]     Train net output #0: loss = 1.689 (* 1 = 1.689 loss)
I0521 02:31:24.611840 20569 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0521 02:31:32.420996 20569 solver.cpp:237] Iteration 1846, loss = 1.6942
I0521 02:31:32.421030 20569 solver.cpp:253]     Train net output #0: loss = 1.6942 (* 1 = 1.6942 loss)
I0521 02:31:32.421044 20569 sgd_solver.cpp:106] Iteration 1846, lr = 0.0025
I0521 02:31:39.027524 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1869.caffemodel
I0521 02:31:39.275475 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_1869.solverstate
I0521 02:31:40.292093 20569 solver.cpp:237] Iteration 1872, loss = 1.66059
I0521 02:31:40.292137 20569 solver.cpp:253]     Train net output #0: loss = 1.66059 (* 1 = 1.66059 loss)
I0521 02:31:40.292150 20569 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0521 02:31:48.096962 20569 solver.cpp:237] Iteration 1898, loss = 1.71426
I0521 02:31:48.097107 20569 solver.cpp:253]     Train net output #0: loss = 1.71426 (* 1 = 1.71426 loss)
I0521 02:31:48.097121 20569 sgd_solver.cpp:106] Iteration 1898, lr = 0.0025
I0521 02:31:55.906436 20569 solver.cpp:237] Iteration 1924, loss = 1.70473
I0521 02:31:55.906468 20569 solver.cpp:253]     Train net output #0: loss = 1.70473 (* 1 = 1.70473 loss)
I0521 02:31:55.906486 20569 sgd_solver.cpp:106] Iteration 1924, lr = 0.0025
I0521 02:32:03.713553 20569 solver.cpp:237] Iteration 1950, loss = 1.71865
I0521 02:32:03.713587 20569 solver.cpp:253]     Train net output #0: loss = 1.71865 (* 1 = 1.71865 loss)
I0521 02:32:03.713601 20569 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0521 02:32:33.675412 20569 solver.cpp:237] Iteration 1976, loss = 1.73785
I0521 02:32:33.675575 20569 solver.cpp:253]     Train net output #0: loss = 1.73785 (* 1 = 1.73785 loss)
I0521 02:32:33.675590 20569 sgd_solver.cpp:106] Iteration 1976, lr = 0.0025
I0521 02:32:41.482923 20569 solver.cpp:237] Iteration 2002, loss = 1.62731
I0521 02:32:41.482964 20569 solver.cpp:253]     Train net output #0: loss = 1.62731 (* 1 = 1.62731 loss)
I0521 02:32:41.482981 20569 sgd_solver.cpp:106] Iteration 2002, lr = 0.0025
I0521 02:32:49.286625 20569 solver.cpp:237] Iteration 2028, loss = 1.67192
I0521 02:32:49.286659 20569 solver.cpp:253]     Train net output #0: loss = 1.67192 (* 1 = 1.67192 loss)
I0521 02:32:49.286675 20569 sgd_solver.cpp:106] Iteration 2028, lr = 0.0025
I0521 02:32:57.097105 20569 solver.cpp:237] Iteration 2054, loss = 1.67064
I0521 02:32:57.097137 20569 solver.cpp:253]     Train net output #0: loss = 1.67064 (* 1 = 1.67064 loss)
I0521 02:32:57.097154 20569 sgd_solver.cpp:106] Iteration 2054, lr = 0.0025
I0521 02:33:04.906839 20569 solver.cpp:237] Iteration 2080, loss = 1.6603
I0521 02:33:04.907002 20569 solver.cpp:253]     Train net output #0: loss = 1.6603 (* 1 = 1.6603 loss)
I0521 02:33:04.907016 20569 sgd_solver.cpp:106] Iteration 2080, lr = 0.0025
I0521 02:33:12.715683 20569 solver.cpp:237] Iteration 2106, loss = 1.68741
I0521 02:33:12.715714 20569 solver.cpp:253]     Train net output #0: loss = 1.68741 (* 1 = 1.68741 loss)
I0521 02:33:12.715733 20569 sgd_solver.cpp:106] Iteration 2106, lr = 0.0025
I0521 02:33:20.524804 20569 solver.cpp:237] Iteration 2132, loss = 1.71589
I0521 02:33:20.524837 20569 solver.cpp:253]     Train net output #0: loss = 1.71589 (* 1 = 1.71589 loss)
I0521 02:33:20.524854 20569 sgd_solver.cpp:106] Iteration 2132, lr = 0.0025
I0521 02:33:21.424787 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2136.caffemodel
I0521 02:33:21.673344 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2136.solverstate
I0521 02:33:22.689179 20569 solver.cpp:341] Iteration 2140, Testing net (#0)
I0521 02:34:28.675323 20569 solver.cpp:409]     Test net output #0: accuracy = 0.672185
I0521 02:34:28.675498 20569 solver.cpp:409]     Test net output #1: loss = 1.12955 (* 1 = 1.12955 loss)
I0521 02:34:56.364737 20569 solver.cpp:237] Iteration 2158, loss = 1.60383
I0521 02:34:56.364787 20569 solver.cpp:253]     Train net output #0: loss = 1.60383 (* 1 = 1.60383 loss)
I0521 02:34:56.364804 20569 sgd_solver.cpp:106] Iteration 2158, lr = 0.0025
I0521 02:35:04.170703 20569 solver.cpp:237] Iteration 2184, loss = 1.68927
I0521 02:35:04.170866 20569 solver.cpp:253]     Train net output #0: loss = 1.68927 (* 1 = 1.68927 loss)
I0521 02:35:04.170881 20569 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0521 02:35:11.975183 20569 solver.cpp:237] Iteration 2210, loss = 1.67745
I0521 02:35:11.975222 20569 solver.cpp:253]     Train net output #0: loss = 1.67745 (* 1 = 1.67745 loss)
I0521 02:35:11.975242 20569 sgd_solver.cpp:106] Iteration 2210, lr = 0.0025
I0521 02:35:19.781366 20569 solver.cpp:237] Iteration 2236, loss = 1.58087
I0521 02:35:19.781399 20569 solver.cpp:253]     Train net output #0: loss = 1.58087 (* 1 = 1.58087 loss)
I0521 02:35:19.781416 20569 sgd_solver.cpp:106] Iteration 2236, lr = 0.0025
I0521 02:35:27.583886 20569 solver.cpp:237] Iteration 2262, loss = 1.60185
I0521 02:35:27.583920 20569 solver.cpp:253]     Train net output #0: loss = 1.60185 (* 1 = 1.60185 loss)
I0521 02:35:27.583935 20569 sgd_solver.cpp:106] Iteration 2262, lr = 0.0025
I0521 02:35:35.387398 20569 solver.cpp:237] Iteration 2288, loss = 1.62687
I0521 02:35:35.387554 20569 solver.cpp:253]     Train net output #0: loss = 1.62687 (* 1 = 1.62687 loss)
I0521 02:35:35.387568 20569 sgd_solver.cpp:106] Iteration 2288, lr = 0.0025
I0521 02:35:43.194682 20569 solver.cpp:237] Iteration 2314, loss = 1.57264
I0521 02:35:43.194713 20569 solver.cpp:253]     Train net output #0: loss = 1.57264 (* 1 = 1.57264 loss)
I0521 02:35:43.194727 20569 sgd_solver.cpp:106] Iteration 2314, lr = 0.0025
I0521 02:36:13.164840 20569 solver.cpp:237] Iteration 2340, loss = 1.68158
I0521 02:36:13.165009 20569 solver.cpp:253]     Train net output #0: loss = 1.68158 (* 1 = 1.68158 loss)
I0521 02:36:13.165024 20569 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0521 02:36:20.969799 20569 solver.cpp:237] Iteration 2366, loss = 1.66068
I0521 02:36:20.969832 20569 solver.cpp:253]     Train net output #0: loss = 1.66068 (* 1 = 1.66068 loss)
I0521 02:36:20.969849 20569 sgd_solver.cpp:106] Iteration 2366, lr = 0.0025
I0521 02:36:28.771001 20569 solver.cpp:237] Iteration 2392, loss = 1.65577
I0521 02:36:28.771040 20569 solver.cpp:253]     Train net output #0: loss = 1.65577 (* 1 = 1.65577 loss)
I0521 02:36:28.771057 20569 sgd_solver.cpp:106] Iteration 2392, lr = 0.0025
I0521 02:36:31.772544 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2403.caffemodel
I0521 02:36:32.022331 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2403.solverstate
I0521 02:36:36.642213 20569 solver.cpp:237] Iteration 2418, loss = 1.61053
I0521 02:36:36.642261 20569 solver.cpp:253]     Train net output #0: loss = 1.61053 (* 1 = 1.61053 loss)
I0521 02:36:36.642277 20569 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0521 02:36:44.448384 20569 solver.cpp:237] Iteration 2444, loss = 1.63691
I0521 02:36:44.448539 20569 solver.cpp:253]     Train net output #0: loss = 1.63691 (* 1 = 1.63691 loss)
I0521 02:36:44.448554 20569 sgd_solver.cpp:106] Iteration 2444, lr = 0.0025
I0521 02:36:52.250094 20569 solver.cpp:237] Iteration 2470, loss = 1.58885
I0521 02:36:52.250143 20569 solver.cpp:253]     Train net output #0: loss = 1.58885 (* 1 = 1.58885 loss)
I0521 02:36:52.250156 20569 sgd_solver.cpp:106] Iteration 2470, lr = 0.0025
I0521 02:37:00.055738 20569 solver.cpp:237] Iteration 2496, loss = 1.57263
I0521 02:37:00.055770 20569 solver.cpp:253]     Train net output #0: loss = 1.57263 (* 1 = 1.57263 loss)
I0521 02:37:00.055784 20569 sgd_solver.cpp:106] Iteration 2496, lr = 0.0025
I0521 02:37:29.987226 20569 solver.cpp:237] Iteration 2522, loss = 1.60573
I0521 02:37:29.987398 20569 solver.cpp:253]     Train net output #0: loss = 1.60573 (* 1 = 1.60573 loss)
I0521 02:37:29.987412 20569 sgd_solver.cpp:106] Iteration 2522, lr = 0.0025
I0521 02:37:37.789916 20569 solver.cpp:237] Iteration 2548, loss = 1.67212
I0521 02:37:37.789958 20569 solver.cpp:253]     Train net output #0: loss = 1.67212 (* 1 = 1.67212 loss)
I0521 02:37:37.789975 20569 sgd_solver.cpp:106] Iteration 2548, lr = 0.0025
I0521 02:37:45.592262 20569 solver.cpp:237] Iteration 2574, loss = 1.54984
I0521 02:37:45.592293 20569 solver.cpp:253]     Train net output #0: loss = 1.54984 (* 1 = 1.54984 loss)
I0521 02:37:45.592317 20569 sgd_solver.cpp:106] Iteration 2574, lr = 0.0025
I0521 02:37:53.394095 20569 solver.cpp:237] Iteration 2600, loss = 1.71175
I0521 02:37:53.394127 20569 solver.cpp:253]     Train net output #0: loss = 1.71175 (* 1 = 1.71175 loss)
I0521 02:37:53.394145 20569 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0521 02:38:01.198551 20569 solver.cpp:237] Iteration 2626, loss = 1.67732
I0521 02:38:01.198696 20569 solver.cpp:253]     Train net output #0: loss = 1.67732 (* 1 = 1.67732 loss)
I0521 02:38:01.198709 20569 sgd_solver.cpp:106] Iteration 2626, lr = 0.0025
I0521 02:38:09.002118 20569 solver.cpp:237] Iteration 2652, loss = 1.65244
I0521 02:38:09.002158 20569 solver.cpp:253]     Train net output #0: loss = 1.65244 (* 1 = 1.65244 loss)
I0521 02:38:09.002173 20569 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0521 02:38:14.103363 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2670.caffemodel
I0521 02:38:14.355443 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2670.solverstate
I0521 02:38:15.674232 20569 solver.cpp:341] Iteration 2675, Testing net (#0)
I0521 02:39:00.858356 20569 solver.cpp:409]     Test net output #0: accuracy = 0.685112
I0521 02:39:00.858520 20569 solver.cpp:409]     Test net output #1: loss = 1.08353 (* 1 = 1.08353 loss)
I0521 02:39:01.549037 20569 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2678.caffemodel
I0521 02:39:01.799247 20569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419_iter_2678.solverstate
I0521 02:39:22.784354 20569 solver.cpp:321] Iteration 2678, loss = 1.57726
I0521 02:39:22.784396 20569 solver.cpp:326] Optimization Done.
I0521 02:39:22.784410 20569 caffe.cpp:215] Optimization Done.
Application 11236531 resources: utime ~1266s, stime ~228s, Rss ~5329368, inblocks ~3744348, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_560_2016-05-20T11.20.52.954419.solver"
	User time (seconds): 0.57
	System time (seconds): 0.11
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:02.11
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15078
	Voluntary context switches: 2723
	Involuntary context switches: 78
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
