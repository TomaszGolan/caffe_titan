2805215
I0520 14:43:17.386076 23372 caffe.cpp:184] Using GPUs 0
I0520 14:43:17.809366 23372 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1071
test_interval: 2142
base_lr: 0.0025
display: 107
max_iter: 10714
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1071
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920.prototxt"
I0520 14:43:17.811215 23372 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920.prototxt
I0520 14:43:17.820040 23372 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 14:43:17.820106 23372 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 14:43:17.820487 23372 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 140
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:43:17.820693 23372 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:43:17.820729 23372 net.cpp:106] Creating Layer data_hdf5
I0520 14:43:17.820747 23372 net.cpp:411] data_hdf5 -> data
I0520 14:43:17.820780 23372 net.cpp:411] data_hdf5 -> label
I0520 14:43:17.820823 23372 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 14:43:17.822055 23372 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 14:43:17.846181 23372 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 14:43:39.370579 23372 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 14:43:39.375792 23372 net.cpp:150] Setting up data_hdf5
I0520 14:43:39.375831 23372 net.cpp:157] Top shape: 140 1 127 50 (889000)
I0520 14:43:39.375849 23372 net.cpp:157] Top shape: 140 (140)
I0520 14:43:39.375862 23372 net.cpp:165] Memory required for data: 3556560
I0520 14:43:39.375881 23372 layer_factory.hpp:77] Creating layer conv1
I0520 14:43:39.375927 23372 net.cpp:106] Creating Layer conv1
I0520 14:43:39.375941 23372 net.cpp:454] conv1 <- data
I0520 14:43:39.375977 23372 net.cpp:411] conv1 -> conv1
I0520 14:43:40.556411 23372 net.cpp:150] Setting up conv1
I0520 14:43:40.556460 23372 net.cpp:157] Top shape: 140 12 120 48 (9676800)
I0520 14:43:40.556475 23372 net.cpp:165] Memory required for data: 42263760
I0520 14:43:40.556504 23372 layer_factory.hpp:77] Creating layer relu1
I0520 14:43:40.556526 23372 net.cpp:106] Creating Layer relu1
I0520 14:43:40.556547 23372 net.cpp:454] relu1 <- conv1
I0520 14:43:40.556583 23372 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:43:40.557117 23372 net.cpp:150] Setting up relu1
I0520 14:43:40.557142 23372 net.cpp:157] Top shape: 140 12 120 48 (9676800)
I0520 14:43:40.557154 23372 net.cpp:165] Memory required for data: 80970960
I0520 14:43:40.557170 23372 layer_factory.hpp:77] Creating layer pool1
I0520 14:43:40.557199 23372 net.cpp:106] Creating Layer pool1
I0520 14:43:40.557211 23372 net.cpp:454] pool1 <- conv1
I0520 14:43:40.557227 23372 net.cpp:411] pool1 -> pool1
I0520 14:43:40.557322 23372 net.cpp:150] Setting up pool1
I0520 14:43:40.557340 23372 net.cpp:157] Top shape: 140 12 60 48 (4838400)
I0520 14:43:40.557355 23372 net.cpp:165] Memory required for data: 100324560
I0520 14:43:40.557376 23372 layer_factory.hpp:77] Creating layer conv2
I0520 14:43:40.557400 23372 net.cpp:106] Creating Layer conv2
I0520 14:43:40.557413 23372 net.cpp:454] conv2 <- pool1
I0520 14:43:40.557430 23372 net.cpp:411] conv2 -> conv2
I0520 14:43:40.560134 23372 net.cpp:150] Setting up conv2
I0520 14:43:40.560165 23372 net.cpp:157] Top shape: 140 20 54 46 (6955200)
I0520 14:43:40.560180 23372 net.cpp:165] Memory required for data: 128145360
I0520 14:43:40.560209 23372 layer_factory.hpp:77] Creating layer relu2
I0520 14:43:40.560236 23372 net.cpp:106] Creating Layer relu2
I0520 14:43:40.560258 23372 net.cpp:454] relu2 <- conv2
I0520 14:43:40.560287 23372 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:43:40.560643 23372 net.cpp:150] Setting up relu2
I0520 14:43:40.560664 23372 net.cpp:157] Top shape: 140 20 54 46 (6955200)
I0520 14:43:40.560678 23372 net.cpp:165] Memory required for data: 155966160
I0520 14:43:40.560693 23372 layer_factory.hpp:77] Creating layer pool2
I0520 14:43:40.560716 23372 net.cpp:106] Creating Layer pool2
I0520 14:43:40.560729 23372 net.cpp:454] pool2 <- conv2
I0520 14:43:40.560762 23372 net.cpp:411] pool2 -> pool2
I0520 14:43:40.560847 23372 net.cpp:150] Setting up pool2
I0520 14:43:40.560868 23372 net.cpp:157] Top shape: 140 20 27 46 (3477600)
I0520 14:43:40.560880 23372 net.cpp:165] Memory required for data: 169876560
I0520 14:43:40.560896 23372 layer_factory.hpp:77] Creating layer conv3
I0520 14:43:40.560922 23372 net.cpp:106] Creating Layer conv3
I0520 14:43:40.560937 23372 net.cpp:454] conv3 <- pool2
I0520 14:43:40.560953 23372 net.cpp:411] conv3 -> conv3
I0520 14:43:40.562942 23372 net.cpp:150] Setting up conv3
I0520 14:43:40.562968 23372 net.cpp:157] Top shape: 140 28 22 44 (3794560)
I0520 14:43:40.562988 23372 net.cpp:165] Memory required for data: 185054800
I0520 14:43:40.563009 23372 layer_factory.hpp:77] Creating layer relu3
I0520 14:43:40.563031 23372 net.cpp:106] Creating Layer relu3
I0520 14:43:40.563053 23372 net.cpp:454] relu3 <- conv3
I0520 14:43:40.563069 23372 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:43:40.563560 23372 net.cpp:150] Setting up relu3
I0520 14:43:40.563583 23372 net.cpp:157] Top shape: 140 28 22 44 (3794560)
I0520 14:43:40.563596 23372 net.cpp:165] Memory required for data: 200233040
I0520 14:43:40.563612 23372 layer_factory.hpp:77] Creating layer pool3
I0520 14:43:40.563637 23372 net.cpp:106] Creating Layer pool3
I0520 14:43:40.563649 23372 net.cpp:454] pool3 <- conv3
I0520 14:43:40.563665 23372 net.cpp:411] pool3 -> pool3
I0520 14:43:40.563747 23372 net.cpp:150] Setting up pool3
I0520 14:43:40.563771 23372 net.cpp:157] Top shape: 140 28 11 44 (1897280)
I0520 14:43:40.563782 23372 net.cpp:165] Memory required for data: 207822160
I0520 14:43:40.563797 23372 layer_factory.hpp:77] Creating layer conv4
I0520 14:43:40.563823 23372 net.cpp:106] Creating Layer conv4
I0520 14:43:40.563837 23372 net.cpp:454] conv4 <- pool3
I0520 14:43:40.563853 23372 net.cpp:411] conv4 -> conv4
I0520 14:43:40.566845 23372 net.cpp:150] Setting up conv4
I0520 14:43:40.566881 23372 net.cpp:157] Top shape: 140 36 6 42 (1270080)
I0520 14:43:40.566895 23372 net.cpp:165] Memory required for data: 212902480
I0520 14:43:40.566920 23372 layer_factory.hpp:77] Creating layer relu4
I0520 14:43:40.566946 23372 net.cpp:106] Creating Layer relu4
I0520 14:43:40.566961 23372 net.cpp:454] relu4 <- conv4
I0520 14:43:40.566977 23372 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:43:40.567471 23372 net.cpp:150] Setting up relu4
I0520 14:43:40.567494 23372 net.cpp:157] Top shape: 140 36 6 42 (1270080)
I0520 14:43:40.567507 23372 net.cpp:165] Memory required for data: 217982800
I0520 14:43:40.567523 23372 layer_factory.hpp:77] Creating layer pool4
I0520 14:43:40.567546 23372 net.cpp:106] Creating Layer pool4
I0520 14:43:40.567559 23372 net.cpp:454] pool4 <- conv4
I0520 14:43:40.567575 23372 net.cpp:411] pool4 -> pool4
I0520 14:43:40.567657 23372 net.cpp:150] Setting up pool4
I0520 14:43:40.567680 23372 net.cpp:157] Top shape: 140 36 3 42 (635040)
I0520 14:43:40.567693 23372 net.cpp:165] Memory required for data: 220522960
I0520 14:43:40.567708 23372 layer_factory.hpp:77] Creating layer ip1
I0520 14:43:40.567735 23372 net.cpp:106] Creating Layer ip1
I0520 14:43:40.567749 23372 net.cpp:454] ip1 <- pool4
I0520 14:43:40.567764 23372 net.cpp:411] ip1 -> ip1
I0520 14:43:40.583245 23372 net.cpp:150] Setting up ip1
I0520 14:43:40.583276 23372 net.cpp:157] Top shape: 140 196 (27440)
I0520 14:43:40.583297 23372 net.cpp:165] Memory required for data: 220632720
I0520 14:43:40.583322 23372 layer_factory.hpp:77] Creating layer relu5
I0520 14:43:40.583343 23372 net.cpp:106] Creating Layer relu5
I0520 14:43:40.583369 23372 net.cpp:454] relu5 <- ip1
I0520 14:43:40.583385 23372 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:43:40.583824 23372 net.cpp:150] Setting up relu5
I0520 14:43:40.583844 23372 net.cpp:157] Top shape: 140 196 (27440)
I0520 14:43:40.583858 23372 net.cpp:165] Memory required for data: 220742480
I0520 14:43:40.583873 23372 layer_factory.hpp:77] Creating layer drop1
I0520 14:43:40.583897 23372 net.cpp:106] Creating Layer drop1
I0520 14:43:40.583910 23372 net.cpp:454] drop1 <- ip1
I0520 14:43:40.583945 23372 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:43:40.583994 23372 net.cpp:150] Setting up drop1
I0520 14:43:40.584010 23372 net.cpp:157] Top shape: 140 196 (27440)
I0520 14:43:40.584023 23372 net.cpp:165] Memory required for data: 220852240
I0520 14:43:40.584035 23372 layer_factory.hpp:77] Creating layer ip2
I0520 14:43:40.584058 23372 net.cpp:106] Creating Layer ip2
I0520 14:43:40.584069 23372 net.cpp:454] ip2 <- ip1
I0520 14:43:40.584086 23372 net.cpp:411] ip2 -> ip2
I0520 14:43:40.584589 23372 net.cpp:150] Setting up ip2
I0520 14:43:40.584609 23372 net.cpp:157] Top shape: 140 98 (13720)
I0520 14:43:40.584621 23372 net.cpp:165] Memory required for data: 220907120
I0520 14:43:40.584642 23372 layer_factory.hpp:77] Creating layer relu6
I0520 14:43:40.584664 23372 net.cpp:106] Creating Layer relu6
I0520 14:43:40.584678 23372 net.cpp:454] relu6 <- ip2
I0520 14:43:40.584693 23372 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:43:40.585237 23372 net.cpp:150] Setting up relu6
I0520 14:43:40.585259 23372 net.cpp:157] Top shape: 140 98 (13720)
I0520 14:43:40.585273 23372 net.cpp:165] Memory required for data: 220962000
I0520 14:43:40.585289 23372 layer_factory.hpp:77] Creating layer drop2
I0520 14:43:40.585311 23372 net.cpp:106] Creating Layer drop2
I0520 14:43:40.585325 23372 net.cpp:454] drop2 <- ip2
I0520 14:43:40.585340 23372 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:43:40.585397 23372 net.cpp:150] Setting up drop2
I0520 14:43:40.585412 23372 net.cpp:157] Top shape: 140 98 (13720)
I0520 14:43:40.585425 23372 net.cpp:165] Memory required for data: 221016880
I0520 14:43:40.585440 23372 layer_factory.hpp:77] Creating layer ip3
I0520 14:43:40.585456 23372 net.cpp:106] Creating Layer ip3
I0520 14:43:40.585471 23372 net.cpp:454] ip3 <- ip2
I0520 14:43:40.585494 23372 net.cpp:411] ip3 -> ip3
I0520 14:43:40.585718 23372 net.cpp:150] Setting up ip3
I0520 14:43:40.585737 23372 net.cpp:157] Top shape: 140 11 (1540)
I0520 14:43:40.585750 23372 net.cpp:165] Memory required for data: 221023040
I0520 14:43:40.585770 23372 layer_factory.hpp:77] Creating layer drop3
I0520 14:43:40.585793 23372 net.cpp:106] Creating Layer drop3
I0520 14:43:40.585805 23372 net.cpp:454] drop3 <- ip3
I0520 14:43:40.585820 23372 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:43:40.585867 23372 net.cpp:150] Setting up drop3
I0520 14:43:40.585889 23372 net.cpp:157] Top shape: 140 11 (1540)
I0520 14:43:40.585901 23372 net.cpp:165] Memory required for data: 221029200
I0520 14:43:40.585921 23372 layer_factory.hpp:77] Creating layer loss
I0520 14:43:40.585942 23372 net.cpp:106] Creating Layer loss
I0520 14:43:40.585957 23372 net.cpp:454] loss <- ip3
I0520 14:43:40.585970 23372 net.cpp:454] loss <- label
I0520 14:43:40.585986 23372 net.cpp:411] loss -> loss
I0520 14:43:40.586004 23372 layer_factory.hpp:77] Creating layer loss
I0520 14:43:40.586683 23372 net.cpp:150] Setting up loss
I0520 14:43:40.586704 23372 net.cpp:157] Top shape: (1)
I0520 14:43:40.586720 23372 net.cpp:160]     with loss weight 1
I0520 14:43:40.586771 23372 net.cpp:165] Memory required for data: 221029204
I0520 14:43:40.586784 23372 net.cpp:226] loss needs backward computation.
I0520 14:43:40.586798 23372 net.cpp:226] drop3 needs backward computation.
I0520 14:43:40.586812 23372 net.cpp:226] ip3 needs backward computation.
I0520 14:43:40.586827 23372 net.cpp:226] drop2 needs backward computation.
I0520 14:43:40.586840 23372 net.cpp:226] relu6 needs backward computation.
I0520 14:43:40.586853 23372 net.cpp:226] ip2 needs backward computation.
I0520 14:43:40.586864 23372 net.cpp:226] drop1 needs backward computation.
I0520 14:43:40.586876 23372 net.cpp:226] relu5 needs backward computation.
I0520 14:43:40.586889 23372 net.cpp:226] ip1 needs backward computation.
I0520 14:43:40.586901 23372 net.cpp:226] pool4 needs backward computation.
I0520 14:43:40.586935 23372 net.cpp:226] relu4 needs backward computation.
I0520 14:43:40.586947 23372 net.cpp:226] conv4 needs backward computation.
I0520 14:43:40.586961 23372 net.cpp:226] pool3 needs backward computation.
I0520 14:43:40.586972 23372 net.cpp:226] relu3 needs backward computation.
I0520 14:43:40.587005 23372 net.cpp:226] conv3 needs backward computation.
I0520 14:43:40.587026 23372 net.cpp:226] pool2 needs backward computation.
I0520 14:43:40.587040 23372 net.cpp:226] relu2 needs backward computation.
I0520 14:43:40.587054 23372 net.cpp:226] conv2 needs backward computation.
I0520 14:43:40.587069 23372 net.cpp:226] pool1 needs backward computation.
I0520 14:43:40.587081 23372 net.cpp:226] relu1 needs backward computation.
I0520 14:43:40.587095 23372 net.cpp:226] conv1 needs backward computation.
I0520 14:43:40.587108 23372 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:43:40.587121 23372 net.cpp:270] This network produces output loss
I0520 14:43:40.587149 23372 net.cpp:283] Network initialization done.
I0520 14:43:40.588790 23372 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920.prototxt
I0520 14:43:40.588870 23372 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 14:43:40.589251 23372 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 140
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:43:40.589469 23372 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:43:40.589489 23372 net.cpp:106] Creating Layer data_hdf5
I0520 14:43:40.589507 23372 net.cpp:411] data_hdf5 -> data
I0520 14:43:40.589527 23372 net.cpp:411] data_hdf5 -> label
I0520 14:43:40.589548 23372 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 14:43:40.612771 23372 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 14:44:01.931756 23372 net.cpp:150] Setting up data_hdf5
I0520 14:44:01.931927 23372 net.cpp:157] Top shape: 140 1 127 50 (889000)
I0520 14:44:01.931946 23372 net.cpp:157] Top shape: 140 (140)
I0520 14:44:01.931957 23372 net.cpp:165] Memory required for data: 3556560
I0520 14:44:01.931972 23372 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 14:44:01.932006 23372 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 14:44:01.932020 23372 net.cpp:454] label_data_hdf5_1_split <- label
I0520 14:44:01.932056 23372 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 14:44:01.932080 23372 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 14:44:01.932166 23372 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 14:44:01.932184 23372 net.cpp:157] Top shape: 140 (140)
I0520 14:44:01.932199 23372 net.cpp:157] Top shape: 140 (140)
I0520 14:44:01.932211 23372 net.cpp:165] Memory required for data: 3557680
I0520 14:44:01.932224 23372 layer_factory.hpp:77] Creating layer conv1
I0520 14:44:01.932265 23372 net.cpp:106] Creating Layer conv1
I0520 14:44:01.932279 23372 net.cpp:454] conv1 <- data
I0520 14:44:01.932308 23372 net.cpp:411] conv1 -> conv1
I0520 14:44:01.934273 23372 net.cpp:150] Setting up conv1
I0520 14:44:01.934298 23372 net.cpp:157] Top shape: 140 12 120 48 (9676800)
I0520 14:44:01.934319 23372 net.cpp:165] Memory required for data: 42264880
I0520 14:44:01.934342 23372 layer_factory.hpp:77] Creating layer relu1
I0520 14:44:01.934362 23372 net.cpp:106] Creating Layer relu1
I0520 14:44:01.934384 23372 net.cpp:454] relu1 <- conv1
I0520 14:44:01.934401 23372 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:44:01.934913 23372 net.cpp:150] Setting up relu1
I0520 14:44:01.934937 23372 net.cpp:157] Top shape: 140 12 120 48 (9676800)
I0520 14:44:01.934949 23372 net.cpp:165] Memory required for data: 80972080
I0520 14:44:01.934962 23372 layer_factory.hpp:77] Creating layer pool1
I0520 14:44:01.934991 23372 net.cpp:106] Creating Layer pool1
I0520 14:44:01.935005 23372 net.cpp:454] pool1 <- conv1
I0520 14:44:01.935022 23372 net.cpp:411] pool1 -> pool1
I0520 14:44:01.935111 23372 net.cpp:150] Setting up pool1
I0520 14:44:01.935128 23372 net.cpp:157] Top shape: 140 12 60 48 (4838400)
I0520 14:44:01.935143 23372 net.cpp:165] Memory required for data: 100325680
I0520 14:44:01.935163 23372 layer_factory.hpp:77] Creating layer conv2
I0520 14:44:01.935184 23372 net.cpp:106] Creating Layer conv2
I0520 14:44:01.935195 23372 net.cpp:454] conv2 <- pool1
I0520 14:44:01.935212 23372 net.cpp:411] conv2 -> conv2
I0520 14:44:01.937177 23372 net.cpp:150] Setting up conv2
I0520 14:44:01.937201 23372 net.cpp:157] Top shape: 140 20 54 46 (6955200)
I0520 14:44:01.937222 23372 net.cpp:165] Memory required for data: 128146480
I0520 14:44:01.937244 23372 layer_factory.hpp:77] Creating layer relu2
I0520 14:44:01.937264 23372 net.cpp:106] Creating Layer relu2
I0520 14:44:01.937286 23372 net.cpp:454] relu2 <- conv2
I0520 14:44:01.937302 23372 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:44:01.937654 23372 net.cpp:150] Setting up relu2
I0520 14:44:01.937674 23372 net.cpp:157] Top shape: 140 20 54 46 (6955200)
I0520 14:44:01.937686 23372 net.cpp:165] Memory required for data: 155967280
I0520 14:44:01.937702 23372 layer_factory.hpp:77] Creating layer pool2
I0520 14:44:01.937724 23372 net.cpp:106] Creating Layer pool2
I0520 14:44:01.937737 23372 net.cpp:454] pool2 <- conv2
I0520 14:44:01.937753 23372 net.cpp:411] pool2 -> pool2
I0520 14:44:01.937846 23372 net.cpp:150] Setting up pool2
I0520 14:44:01.937862 23372 net.cpp:157] Top shape: 140 20 27 46 (3477600)
I0520 14:44:01.937875 23372 net.cpp:165] Memory required for data: 169877680
I0520 14:44:01.937887 23372 layer_factory.hpp:77] Creating layer conv3
I0520 14:44:01.937912 23372 net.cpp:106] Creating Layer conv3
I0520 14:44:01.937925 23372 net.cpp:454] conv3 <- pool2
I0520 14:44:01.937948 23372 net.cpp:411] conv3 -> conv3
I0520 14:44:01.939965 23372 net.cpp:150] Setting up conv3
I0520 14:44:01.939990 23372 net.cpp:157] Top shape: 140 28 22 44 (3794560)
I0520 14:44:01.940011 23372 net.cpp:165] Memory required for data: 185055920
I0520 14:44:01.940049 23372 layer_factory.hpp:77] Creating layer relu3
I0520 14:44:01.940074 23372 net.cpp:106] Creating Layer relu3
I0520 14:44:01.940088 23372 net.cpp:454] relu3 <- conv3
I0520 14:44:01.940104 23372 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:44:01.940606 23372 net.cpp:150] Setting up relu3
I0520 14:44:01.940630 23372 net.cpp:157] Top shape: 140 28 22 44 (3794560)
I0520 14:44:01.940644 23372 net.cpp:165] Memory required for data: 200234160
I0520 14:44:01.940659 23372 layer_factory.hpp:77] Creating layer pool3
I0520 14:44:01.940683 23372 net.cpp:106] Creating Layer pool3
I0520 14:44:01.940696 23372 net.cpp:454] pool3 <- conv3
I0520 14:44:01.940713 23372 net.cpp:411] pool3 -> pool3
I0520 14:44:01.940799 23372 net.cpp:150] Setting up pool3
I0520 14:44:01.940816 23372 net.cpp:157] Top shape: 140 28 11 44 (1897280)
I0520 14:44:01.940831 23372 net.cpp:165] Memory required for data: 207823280
I0520 14:44:01.940843 23372 layer_factory.hpp:77] Creating layer conv4
I0520 14:44:01.940871 23372 net.cpp:106] Creating Layer conv4
I0520 14:44:01.940882 23372 net.cpp:454] conv4 <- pool3
I0520 14:44:01.940899 23372 net.cpp:411] conv4 -> conv4
I0520 14:44:01.943011 23372 net.cpp:150] Setting up conv4
I0520 14:44:01.943039 23372 net.cpp:157] Top shape: 140 36 6 42 (1270080)
I0520 14:44:01.943053 23372 net.cpp:165] Memory required for data: 212903600
I0520 14:44:01.943070 23372 layer_factory.hpp:77] Creating layer relu4
I0520 14:44:01.943099 23372 net.cpp:106] Creating Layer relu4
I0520 14:44:01.943115 23372 net.cpp:454] relu4 <- conv4
I0520 14:44:01.943130 23372 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:44:01.943634 23372 net.cpp:150] Setting up relu4
I0520 14:44:01.943657 23372 net.cpp:157] Top shape: 140 36 6 42 (1270080)
I0520 14:44:01.943670 23372 net.cpp:165] Memory required for data: 217983920
I0520 14:44:01.943686 23372 layer_factory.hpp:77] Creating layer pool4
I0520 14:44:01.943711 23372 net.cpp:106] Creating Layer pool4
I0520 14:44:01.943724 23372 net.cpp:454] pool4 <- conv4
I0520 14:44:01.943740 23372 net.cpp:411] pool4 -> pool4
I0520 14:44:01.943825 23372 net.cpp:150] Setting up pool4
I0520 14:44:01.943841 23372 net.cpp:157] Top shape: 140 36 3 42 (635040)
I0520 14:44:01.943856 23372 net.cpp:165] Memory required for data: 220524080
I0520 14:44:01.943869 23372 layer_factory.hpp:77] Creating layer ip1
I0520 14:44:01.943893 23372 net.cpp:106] Creating Layer ip1
I0520 14:44:01.943907 23372 net.cpp:454] ip1 <- pool4
I0520 14:44:01.943929 23372 net.cpp:411] ip1 -> ip1
I0520 14:44:01.959403 23372 net.cpp:150] Setting up ip1
I0520 14:44:01.959435 23372 net.cpp:157] Top shape: 140 196 (27440)
I0520 14:44:01.959456 23372 net.cpp:165] Memory required for data: 220633840
I0520 14:44:01.959483 23372 layer_factory.hpp:77] Creating layer relu5
I0520 14:44:01.959506 23372 net.cpp:106] Creating Layer relu5
I0520 14:44:01.959530 23372 net.cpp:454] relu5 <- ip1
I0520 14:44:01.959547 23372 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:44:01.959911 23372 net.cpp:150] Setting up relu5
I0520 14:44:01.959931 23372 net.cpp:157] Top shape: 140 196 (27440)
I0520 14:44:01.959944 23372 net.cpp:165] Memory required for data: 220743600
I0520 14:44:01.959959 23372 layer_factory.hpp:77] Creating layer drop1
I0520 14:44:01.959987 23372 net.cpp:106] Creating Layer drop1
I0520 14:44:01.960001 23372 net.cpp:454] drop1 <- ip1
I0520 14:44:01.960017 23372 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:44:01.960077 23372 net.cpp:150] Setting up drop1
I0520 14:44:01.960093 23372 net.cpp:157] Top shape: 140 196 (27440)
I0520 14:44:01.960105 23372 net.cpp:165] Memory required for data: 220853360
I0520 14:44:01.960117 23372 layer_factory.hpp:77] Creating layer ip2
I0520 14:44:01.960137 23372 net.cpp:106] Creating Layer ip2
I0520 14:44:01.960150 23372 net.cpp:454] ip2 <- ip1
I0520 14:44:01.960172 23372 net.cpp:411] ip2 -> ip2
I0520 14:44:01.960674 23372 net.cpp:150] Setting up ip2
I0520 14:44:01.960693 23372 net.cpp:157] Top shape: 140 98 (13720)
I0520 14:44:01.960705 23372 net.cpp:165] Memory required for data: 220908240
I0520 14:44:01.960747 23372 layer_factory.hpp:77] Creating layer relu6
I0520 14:44:01.960763 23372 net.cpp:106] Creating Layer relu6
I0520 14:44:01.960777 23372 net.cpp:454] relu6 <- ip2
I0520 14:44:01.960791 23372 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:44:01.961359 23372 net.cpp:150] Setting up relu6
I0520 14:44:01.961381 23372 net.cpp:157] Top shape: 140 98 (13720)
I0520 14:44:01.961395 23372 net.cpp:165] Memory required for data: 220963120
I0520 14:44:01.961407 23372 layer_factory.hpp:77] Creating layer drop2
I0520 14:44:01.961434 23372 net.cpp:106] Creating Layer drop2
I0520 14:44:01.961448 23372 net.cpp:454] drop2 <- ip2
I0520 14:44:01.961464 23372 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:44:01.961522 23372 net.cpp:150] Setting up drop2
I0520 14:44:01.961539 23372 net.cpp:157] Top shape: 140 98 (13720)
I0520 14:44:01.961551 23372 net.cpp:165] Memory required for data: 221018000
I0520 14:44:01.961565 23372 layer_factory.hpp:77] Creating layer ip3
I0520 14:44:01.961581 23372 net.cpp:106] Creating Layer ip3
I0520 14:44:01.961597 23372 net.cpp:454] ip3 <- ip2
I0520 14:44:01.961619 23372 net.cpp:411] ip3 -> ip3
I0520 14:44:01.961861 23372 net.cpp:150] Setting up ip3
I0520 14:44:01.961880 23372 net.cpp:157] Top shape: 140 11 (1540)
I0520 14:44:01.961894 23372 net.cpp:165] Memory required for data: 221024160
I0520 14:44:01.961915 23372 layer_factory.hpp:77] Creating layer drop3
I0520 14:44:01.961937 23372 net.cpp:106] Creating Layer drop3
I0520 14:44:01.961951 23372 net.cpp:454] drop3 <- ip3
I0520 14:44:01.961966 23372 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:44:01.962021 23372 net.cpp:150] Setting up drop3
I0520 14:44:01.962038 23372 net.cpp:157] Top shape: 140 11 (1540)
I0520 14:44:01.962049 23372 net.cpp:165] Memory required for data: 221030320
I0520 14:44:01.962067 23372 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 14:44:01.962083 23372 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 14:44:01.962096 23372 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 14:44:01.962121 23372 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 14:44:01.962139 23372 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 14:44:01.962227 23372 net.cpp:150] Setting up ip3_drop3_0_split
I0520 14:44:01.962244 23372 net.cpp:157] Top shape: 140 11 (1540)
I0520 14:44:01.962260 23372 net.cpp:157] Top shape: 140 11 (1540)
I0520 14:44:01.962273 23372 net.cpp:165] Memory required for data: 221042640
I0520 14:44:01.962286 23372 layer_factory.hpp:77] Creating layer accuracy
I0520 14:44:01.962316 23372 net.cpp:106] Creating Layer accuracy
I0520 14:44:01.962328 23372 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 14:44:01.962343 23372 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 14:44:01.962362 23372 net.cpp:411] accuracy -> accuracy
I0520 14:44:01.962393 23372 net.cpp:150] Setting up accuracy
I0520 14:44:01.962409 23372 net.cpp:157] Top shape: (1)
I0520 14:44:01.962429 23372 net.cpp:165] Memory required for data: 221042644
I0520 14:44:01.962440 23372 layer_factory.hpp:77] Creating layer loss
I0520 14:44:01.962456 23372 net.cpp:106] Creating Layer loss
I0520 14:44:01.962471 23372 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 14:44:01.962484 23372 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 14:44:01.962508 23372 net.cpp:411] loss -> loss
I0520 14:44:01.962532 23372 layer_factory.hpp:77] Creating layer loss
I0520 14:44:01.963045 23372 net.cpp:150] Setting up loss
I0520 14:44:01.963066 23372 net.cpp:157] Top shape: (1)
I0520 14:44:01.963078 23372 net.cpp:160]     with loss weight 1
I0520 14:44:01.963105 23372 net.cpp:165] Memory required for data: 221042648
I0520 14:44:01.963126 23372 net.cpp:226] loss needs backward computation.
I0520 14:44:01.963141 23372 net.cpp:228] accuracy does not need backward computation.
I0520 14:44:01.963158 23372 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 14:44:01.963171 23372 net.cpp:226] drop3 needs backward computation.
I0520 14:44:01.963183 23372 net.cpp:226] ip3 needs backward computation.
I0520 14:44:01.963198 23372 net.cpp:226] drop2 needs backward computation.
I0520 14:44:01.963227 23372 net.cpp:226] relu6 needs backward computation.
I0520 14:44:01.963238 23372 net.cpp:226] ip2 needs backward computation.
I0520 14:44:01.963254 23372 net.cpp:226] drop1 needs backward computation.
I0520 14:44:01.963266 23372 net.cpp:226] relu5 needs backward computation.
I0520 14:44:01.963279 23372 net.cpp:226] ip1 needs backward computation.
I0520 14:44:01.963294 23372 net.cpp:226] pool4 needs backward computation.
I0520 14:44:01.963313 23372 net.cpp:226] relu4 needs backward computation.
I0520 14:44:01.963326 23372 net.cpp:226] conv4 needs backward computation.
I0520 14:44:01.963340 23372 net.cpp:226] pool3 needs backward computation.
I0520 14:44:01.963356 23372 net.cpp:226] relu3 needs backward computation.
I0520 14:44:01.963368 23372 net.cpp:226] conv3 needs backward computation.
I0520 14:44:01.963381 23372 net.cpp:226] pool2 needs backward computation.
I0520 14:44:01.963397 23372 net.cpp:226] relu2 needs backward computation.
I0520 14:44:01.963408 23372 net.cpp:226] conv2 needs backward computation.
I0520 14:44:01.963423 23372 net.cpp:226] pool1 needs backward computation.
I0520 14:44:01.963441 23372 net.cpp:226] relu1 needs backward computation.
I0520 14:44:01.963457 23372 net.cpp:226] conv1 needs backward computation.
I0520 14:44:01.963471 23372 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 14:44:01.963485 23372 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:44:01.963496 23372 net.cpp:270] This network produces output accuracy
I0520 14:44:01.963511 23372 net.cpp:270] This network produces output loss
I0520 14:44:01.963541 23372 net.cpp:283] Network initialization done.
I0520 14:44:01.963678 23372 solver.cpp:60] Solver scaffolding done.
I0520 14:44:01.964838 23372 caffe.cpp:212] Starting Optimization
I0520 14:44:01.964856 23372 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 14:44:01.964869 23372 solver.cpp:289] Learning Rate Policy: fixed
I0520 14:44:01.965941 23372 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 14:44:49.077699 23372 solver.cpp:409]     Test net output #0: accuracy = 0.102841
I0520 14:44:49.077869 23372 solver.cpp:409]     Test net output #1: loss = 2.39696 (* 1 = 2.39696 loss)
I0520 14:44:49.117588 23372 solver.cpp:237] Iteration 0, loss = 2.39601
I0520 14:44:49.117627 23372 solver.cpp:253]     Train net output #0: loss = 2.39601 (* 1 = 2.39601 loss)
I0520 14:44:49.117648 23372 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 14:44:57.573719 23372 solver.cpp:237] Iteration 107, loss = 2.31052
I0520 14:44:57.573757 23372 solver.cpp:253]     Train net output #0: loss = 2.31052 (* 1 = 2.31052 loss)
I0520 14:44:57.573781 23372 sgd_solver.cpp:106] Iteration 107, lr = 0.0025
I0520 14:45:06.026443 23372 solver.cpp:237] Iteration 214, loss = 2.30279
I0520 14:45:06.026479 23372 solver.cpp:253]     Train net output #0: loss = 2.30279 (* 1 = 2.30279 loss)
I0520 14:45:06.026497 23372 sgd_solver.cpp:106] Iteration 214, lr = 0.0025
I0520 14:45:14.472501 23372 solver.cpp:237] Iteration 321, loss = 2.312
I0520 14:45:14.472558 23372 solver.cpp:253]     Train net output #0: loss = 2.312 (* 1 = 2.312 loss)
I0520 14:45:14.472582 23372 sgd_solver.cpp:106] Iteration 321, lr = 0.0025
I0520 14:45:22.926592 23372 solver.cpp:237] Iteration 428, loss = 2.1916
I0520 14:45:22.926753 23372 solver.cpp:253]     Train net output #0: loss = 2.1916 (* 1 = 2.1916 loss)
I0520 14:45:22.926770 23372 sgd_solver.cpp:106] Iteration 428, lr = 0.0025
I0520 14:45:31.377185 23372 solver.cpp:237] Iteration 535, loss = 2.00564
I0520 14:45:31.377223 23372 solver.cpp:253]     Train net output #0: loss = 2.00564 (* 1 = 2.00564 loss)
I0520 14:45:31.377241 23372 sgd_solver.cpp:106] Iteration 535, lr = 0.0025
I0520 14:45:39.820966 23372 solver.cpp:237] Iteration 642, loss = 2.06258
I0520 14:45:39.821017 23372 solver.cpp:253]     Train net output #0: loss = 2.06258 (* 1 = 2.06258 loss)
I0520 14:45:39.821033 23372 sgd_solver.cpp:106] Iteration 642, lr = 0.0025
I0520 14:46:10.353881 23372 solver.cpp:237] Iteration 749, loss = 1.94211
I0520 14:46:10.354049 23372 solver.cpp:253]     Train net output #0: loss = 1.94211 (* 1 = 1.94211 loss)
I0520 14:46:10.354066 23372 sgd_solver.cpp:106] Iteration 749, lr = 0.0025
I0520 14:46:18.802815 23372 solver.cpp:237] Iteration 856, loss = 1.84745
I0520 14:46:18.802851 23372 solver.cpp:253]     Train net output #0: loss = 1.84745 (* 1 = 1.84745 loss)
I0520 14:46:18.802875 23372 sgd_solver.cpp:106] Iteration 856, lr = 0.0025
I0520 14:46:27.259647 23372 solver.cpp:237] Iteration 963, loss = 1.86825
I0520 14:46:27.259702 23372 solver.cpp:253]     Train net output #0: loss = 1.86825 (* 1 = 1.86825 loss)
I0520 14:46:27.259719 23372 sgd_solver.cpp:106] Iteration 963, lr = 0.0025
I0520 14:46:35.707425 23372 solver.cpp:237] Iteration 1070, loss = 1.91283
I0520 14:46:35.707461 23372 solver.cpp:253]     Train net output #0: loss = 1.91283 (* 1 = 1.91283 loss)
I0520 14:46:35.707479 23372 sgd_solver.cpp:106] Iteration 1070, lr = 0.0025
I0520 14:46:35.707888 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_1071.caffemodel
I0520 14:46:35.804605 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_1071.solverstate
I0520 14:46:44.227219 23372 solver.cpp:237] Iteration 1177, loss = 2.11327
I0520 14:46:44.227383 23372 solver.cpp:253]     Train net output #0: loss = 2.11327 (* 1 = 2.11327 loss)
I0520 14:46:44.227401 23372 sgd_solver.cpp:106] Iteration 1177, lr = 0.0025
I0520 14:46:52.682778 23372 solver.cpp:237] Iteration 1284, loss = 1.75416
I0520 14:46:52.682832 23372 solver.cpp:253]     Train net output #0: loss = 1.75416 (* 1 = 1.75416 loss)
I0520 14:46:52.682857 23372 sgd_solver.cpp:106] Iteration 1284, lr = 0.0025
I0520 14:47:01.140429 23372 solver.cpp:237] Iteration 1391, loss = 1.90819
I0520 14:47:01.140465 23372 solver.cpp:253]     Train net output #0: loss = 1.90819 (* 1 = 1.90819 loss)
I0520 14:47:01.140487 23372 sgd_solver.cpp:106] Iteration 1391, lr = 0.0025
I0520 14:47:31.709745 23372 solver.cpp:237] Iteration 1498, loss = 1.73663
I0520 14:47:31.709908 23372 solver.cpp:253]     Train net output #0: loss = 1.73663 (* 1 = 1.73663 loss)
I0520 14:47:31.709926 23372 sgd_solver.cpp:106] Iteration 1498, lr = 0.0025
I0520 14:47:40.166339 23372 solver.cpp:237] Iteration 1605, loss = 1.84887
I0520 14:47:40.166394 23372 solver.cpp:253]     Train net output #0: loss = 1.84887 (* 1 = 1.84887 loss)
I0520 14:47:40.166412 23372 sgd_solver.cpp:106] Iteration 1605, lr = 0.0025
I0520 14:47:48.621600 23372 solver.cpp:237] Iteration 1712, loss = 1.57547
I0520 14:47:48.621637 23372 solver.cpp:253]     Train net output #0: loss = 1.57547 (* 1 = 1.57547 loss)
I0520 14:47:48.621662 23372 sgd_solver.cpp:106] Iteration 1712, lr = 0.0025
I0520 14:47:57.074332 23372 solver.cpp:237] Iteration 1819, loss = 1.699
I0520 14:47:57.074368 23372 solver.cpp:253]     Train net output #0: loss = 1.699 (* 1 = 1.699 loss)
I0520 14:47:57.074388 23372 sgd_solver.cpp:106] Iteration 1819, lr = 0.0025
I0520 14:48:05.529364 23372 solver.cpp:237] Iteration 1926, loss = 1.73571
I0520 14:48:05.529538 23372 solver.cpp:253]     Train net output #0: loss = 1.73571 (* 1 = 1.73571 loss)
I0520 14:48:05.529558 23372 sgd_solver.cpp:106] Iteration 1926, lr = 0.0025
I0520 14:48:13.982975 23372 solver.cpp:237] Iteration 2033, loss = 1.73476
I0520 14:48:13.983011 23372 solver.cpp:253]     Train net output #0: loss = 1.73476 (* 1 = 1.73476 loss)
I0520 14:48:13.983036 23372 sgd_solver.cpp:106] Iteration 2033, lr = 0.0025
I0520 14:48:22.429792 23372 solver.cpp:237] Iteration 2140, loss = 1.80713
I0520 14:48:22.429829 23372 solver.cpp:253]     Train net output #0: loss = 1.80713 (* 1 = 1.80713 loss)
I0520 14:48:22.429847 23372 sgd_solver.cpp:106] Iteration 2140, lr = 0.0025
I0520 14:48:22.508882 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_2142.caffemodel
I0520 14:48:22.602499 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_2142.solverstate
I0520 14:48:22.628986 23372 solver.cpp:341] Iteration 2142, Testing net (#0)
I0520 14:49:08.810650 23372 solver.cpp:409]     Test net output #0: accuracy = 0.650402
I0520 14:49:08.810817 23372 solver.cpp:409]     Test net output #1: loss = 1.23541 (* 1 = 1.23541 loss)
I0520 14:49:39.263849 23372 solver.cpp:237] Iteration 2247, loss = 1.68499
I0520 14:49:39.264014 23372 solver.cpp:253]     Train net output #0: loss = 1.68499 (* 1 = 1.68499 loss)
I0520 14:49:39.264031 23372 sgd_solver.cpp:106] Iteration 2247, lr = 0.0025
I0520 14:49:47.722872 23372 solver.cpp:237] Iteration 2354, loss = 1.6829
I0520 14:49:47.722924 23372 solver.cpp:253]     Train net output #0: loss = 1.6829 (* 1 = 1.6829 loss)
I0520 14:49:47.722952 23372 sgd_solver.cpp:106] Iteration 2354, lr = 0.0025
I0520 14:49:56.179565 23372 solver.cpp:237] Iteration 2461, loss = 1.5698
I0520 14:49:56.179602 23372 solver.cpp:253]     Train net output #0: loss = 1.5698 (* 1 = 1.5698 loss)
I0520 14:49:56.179622 23372 sgd_solver.cpp:106] Iteration 2461, lr = 0.0025
I0520 14:50:04.635339 23372 solver.cpp:237] Iteration 2568, loss = 1.56583
I0520 14:50:04.635375 23372 solver.cpp:253]     Train net output #0: loss = 1.56583 (* 1 = 1.56583 loss)
I0520 14:50:04.635397 23372 sgd_solver.cpp:106] Iteration 2568, lr = 0.0025
I0520 14:50:13.086732 23372 solver.cpp:237] Iteration 2675, loss = 1.81064
I0520 14:50:13.086902 23372 solver.cpp:253]     Train net output #0: loss = 1.81064 (* 1 = 1.81064 loss)
I0520 14:50:13.086922 23372 sgd_solver.cpp:106] Iteration 2675, lr = 0.0025
I0520 14:50:21.537710 23372 solver.cpp:237] Iteration 2782, loss = 1.65176
I0520 14:50:21.537747 23372 solver.cpp:253]     Train net output #0: loss = 1.65176 (* 1 = 1.65176 loss)
I0520 14:50:21.537771 23372 sgd_solver.cpp:106] Iteration 2782, lr = 0.0025
I0520 14:50:52.215586 23372 solver.cpp:237] Iteration 2889, loss = 1.72394
I0520 14:50:52.215756 23372 solver.cpp:253]     Train net output #0: loss = 1.72394 (* 1 = 1.72394 loss)
I0520 14:50:52.215773 23372 sgd_solver.cpp:106] Iteration 2889, lr = 0.0025
I0520 14:51:00.670367 23372 solver.cpp:237] Iteration 2996, loss = 1.56109
I0520 14:51:00.670425 23372 solver.cpp:253]     Train net output #0: loss = 1.56109 (* 1 = 1.56109 loss)
I0520 14:51:00.670452 23372 sgd_solver.cpp:106] Iteration 2996, lr = 0.0025
I0520 14:51:09.124663 23372 solver.cpp:237] Iteration 3103, loss = 1.53069
I0520 14:51:09.124701 23372 solver.cpp:253]     Train net output #0: loss = 1.53069 (* 1 = 1.53069 loss)
I0520 14:51:09.124718 23372 sgd_solver.cpp:106] Iteration 3103, lr = 0.0025
I0520 14:51:17.579911 23372 solver.cpp:237] Iteration 3210, loss = 1.65931
I0520 14:51:17.579946 23372 solver.cpp:253]     Train net output #0: loss = 1.65931 (* 1 = 1.65931 loss)
I0520 14:51:17.579965 23372 sgd_solver.cpp:106] Iteration 3210, lr = 0.0025
I0520 14:51:17.738461 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_3213.caffemodel
I0520 14:51:17.835970 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_3213.solverstate
I0520 14:51:26.102164 23372 solver.cpp:237] Iteration 3317, loss = 1.66207
I0520 14:51:26.102345 23372 solver.cpp:253]     Train net output #0: loss = 1.66207 (* 1 = 1.66207 loss)
I0520 14:51:26.102362 23372 sgd_solver.cpp:106] Iteration 3317, lr = 0.0025
I0520 14:51:34.550178 23372 solver.cpp:237] Iteration 3424, loss = 1.49051
I0520 14:51:34.550214 23372 solver.cpp:253]     Train net output #0: loss = 1.49051 (* 1 = 1.49051 loss)
I0520 14:51:34.550231 23372 sgd_solver.cpp:106] Iteration 3424, lr = 0.0025
I0520 14:51:43.001741 23372 solver.cpp:237] Iteration 3531, loss = 1.59628
I0520 14:51:43.001780 23372 solver.cpp:253]     Train net output #0: loss = 1.59628 (* 1 = 1.59628 loss)
I0520 14:51:43.001796 23372 sgd_solver.cpp:106] Iteration 3531, lr = 0.0025
I0520 14:52:13.656815 23372 solver.cpp:237] Iteration 3638, loss = 1.53906
I0520 14:52:13.656986 23372 solver.cpp:253]     Train net output #0: loss = 1.53906 (* 1 = 1.53906 loss)
I0520 14:52:13.657003 23372 sgd_solver.cpp:106] Iteration 3638, lr = 0.0025
I0520 14:52:22.111474 23372 solver.cpp:237] Iteration 3745, loss = 1.64841
I0520 14:52:22.111510 23372 solver.cpp:253]     Train net output #0: loss = 1.64841 (* 1 = 1.64841 loss)
I0520 14:52:22.111534 23372 sgd_solver.cpp:106] Iteration 3745, lr = 0.0025
I0520 14:52:30.564807 23372 solver.cpp:237] Iteration 3852, loss = 1.46436
I0520 14:52:30.564846 23372 solver.cpp:253]     Train net output #0: loss = 1.46436 (* 1 = 1.46436 loss)
I0520 14:52:30.564862 23372 sgd_solver.cpp:106] Iteration 3852, lr = 0.0025
I0520 14:52:39.019734 23372 solver.cpp:237] Iteration 3959, loss = 1.56511
I0520 14:52:39.019789 23372 solver.cpp:253]     Train net output #0: loss = 1.56511 (* 1 = 1.56511 loss)
I0520 14:52:39.019806 23372 sgd_solver.cpp:106] Iteration 3959, lr = 0.0025
I0520 14:52:47.467049 23372 solver.cpp:237] Iteration 4066, loss = 1.71764
I0520 14:52:47.467195 23372 solver.cpp:253]     Train net output #0: loss = 1.71764 (* 1 = 1.71764 loss)
I0520 14:52:47.467211 23372 sgd_solver.cpp:106] Iteration 4066, lr = 0.0025
I0520 14:52:55.918854 23372 solver.cpp:237] Iteration 4173, loss = 1.48581
I0520 14:52:55.918890 23372 solver.cpp:253]     Train net output #0: loss = 1.48581 (* 1 = 1.48581 loss)
I0520 14:52:55.918910 23372 sgd_solver.cpp:106] Iteration 4173, lr = 0.0025
I0520 14:53:04.372947 23372 solver.cpp:237] Iteration 4280, loss = 1.5754
I0520 14:53:04.373003 23372 solver.cpp:253]     Train net output #0: loss = 1.5754 (* 1 = 1.5754 loss)
I0520 14:53:04.373020 23372 sgd_solver.cpp:106] Iteration 4280, lr = 0.0025
I0520 14:53:04.610035 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_4284.caffemodel
I0520 14:53:04.705478 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_4284.solverstate
I0520 14:53:04.734283 23372 solver.cpp:341] Iteration 4284, Testing net (#0)
I0520 14:54:11.750450 23372 solver.cpp:409]     Test net output #0: accuracy = 0.749753
I0520 14:54:11.750635 23372 solver.cpp:409]     Test net output #1: loss = 0.898353 (* 1 = 0.898353 loss)
I0520 14:54:42.099164 23372 solver.cpp:237] Iteration 4387, loss = 1.44332
I0520 14:54:42.099339 23372 solver.cpp:253]     Train net output #0: loss = 1.44332 (* 1 = 1.44332 loss)
I0520 14:54:42.099357 23372 sgd_solver.cpp:106] Iteration 4387, lr = 0.0025
I0520 14:54:50.555369 23372 solver.cpp:237] Iteration 4494, loss = 1.5615
I0520 14:54:50.555411 23372 solver.cpp:253]     Train net output #0: loss = 1.5615 (* 1 = 1.5615 loss)
I0520 14:54:50.555429 23372 sgd_solver.cpp:106] Iteration 4494, lr = 0.0025
I0520 14:54:59.009181 23372 solver.cpp:237] Iteration 4601, loss = 1.46642
I0520 14:54:59.009215 23372 solver.cpp:253]     Train net output #0: loss = 1.46642 (* 1 = 1.46642 loss)
I0520 14:54:59.009238 23372 sgd_solver.cpp:106] Iteration 4601, lr = 0.0025
I0520 14:55:07.459637 23372 solver.cpp:237] Iteration 4708, loss = 1.45021
I0520 14:55:07.459672 23372 solver.cpp:253]     Train net output #0: loss = 1.45021 (* 1 = 1.45021 loss)
I0520 14:55:07.459691 23372 sgd_solver.cpp:106] Iteration 4708, lr = 0.0025
I0520 14:55:15.911104 23372 solver.cpp:237] Iteration 4815, loss = 1.38996
I0520 14:55:15.911264 23372 solver.cpp:253]     Train net output #0: loss = 1.38996 (* 1 = 1.38996 loss)
I0520 14:55:15.911283 23372 sgd_solver.cpp:106] Iteration 4815, lr = 0.0025
I0520 14:55:24.362184 23372 solver.cpp:237] Iteration 4922, loss = 1.46136
I0520 14:55:24.362218 23372 solver.cpp:253]     Train net output #0: loss = 1.46136 (* 1 = 1.46136 loss)
I0520 14:55:24.362242 23372 sgd_solver.cpp:106] Iteration 4922, lr = 0.0025
I0520 14:55:54.979171 23372 solver.cpp:237] Iteration 5029, loss = 1.40983
I0520 14:55:54.979344 23372 solver.cpp:253]     Train net output #0: loss = 1.40983 (* 1 = 1.40983 loss)
I0520 14:55:54.979362 23372 sgd_solver.cpp:106] Iteration 5029, lr = 0.0025
I0520 14:56:03.437010 23372 solver.cpp:237] Iteration 5136, loss = 1.46009
I0520 14:56:03.437058 23372 solver.cpp:253]     Train net output #0: loss = 1.46009 (* 1 = 1.46009 loss)
I0520 14:56:03.437086 23372 sgd_solver.cpp:106] Iteration 5136, lr = 0.0025
I0520 14:56:11.890306 23372 solver.cpp:237] Iteration 5243, loss = 1.46384
I0520 14:56:11.890343 23372 solver.cpp:253]     Train net output #0: loss = 1.46384 (* 1 = 1.46384 loss)
I0520 14:56:11.890367 23372 sgd_solver.cpp:106] Iteration 5243, lr = 0.0025
I0520 14:56:20.340497 23372 solver.cpp:237] Iteration 5350, loss = 1.31246
I0520 14:56:20.340531 23372 solver.cpp:253]     Train net output #0: loss = 1.31246 (* 1 = 1.31246 loss)
I0520 14:56:20.340555 23372 sgd_solver.cpp:106] Iteration 5350, lr = 0.0025
I0520 14:56:20.657095 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_5355.caffemodel
I0520 14:56:20.752094 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_5355.solverstate
I0520 14:56:28.865411 23372 solver.cpp:237] Iteration 5457, loss = 1.51167
I0520 14:56:28.865579 23372 solver.cpp:253]     Train net output #0: loss = 1.51167 (* 1 = 1.51167 loss)
I0520 14:56:28.865604 23372 sgd_solver.cpp:106] Iteration 5457, lr = 0.0025
I0520 14:56:37.316129 23372 solver.cpp:237] Iteration 5564, loss = 1.40181
I0520 14:56:37.316165 23372 solver.cpp:253]     Train net output #0: loss = 1.40181 (* 1 = 1.40181 loss)
I0520 14:56:37.316184 23372 sgd_solver.cpp:106] Iteration 5564, lr = 0.0025
I0520 14:56:45.766000 23372 solver.cpp:237] Iteration 5671, loss = 1.36723
I0520 14:56:45.766036 23372 solver.cpp:253]     Train net output #0: loss = 1.36723 (* 1 = 1.36723 loss)
I0520 14:56:45.766054 23372 sgd_solver.cpp:106] Iteration 5671, lr = 0.0025
I0520 14:57:16.406803 23372 solver.cpp:237] Iteration 5778, loss = 1.37581
I0520 14:57:16.406980 23372 solver.cpp:253]     Train net output #0: loss = 1.37581 (* 1 = 1.37581 loss)
I0520 14:57:16.406997 23372 sgd_solver.cpp:106] Iteration 5778, lr = 0.0025
I0520 14:57:24.860280 23372 solver.cpp:237] Iteration 5885, loss = 1.53106
I0520 14:57:24.860314 23372 solver.cpp:253]     Train net output #0: loss = 1.53106 (* 1 = 1.53106 loss)
I0520 14:57:24.860337 23372 sgd_solver.cpp:106] Iteration 5885, lr = 0.0025
I0520 14:57:33.316795 23372 solver.cpp:237] Iteration 5992, loss = 1.45417
I0520 14:57:33.316831 23372 solver.cpp:253]     Train net output #0: loss = 1.45417 (* 1 = 1.45417 loss)
I0520 14:57:33.316854 23372 sgd_solver.cpp:106] Iteration 5992, lr = 0.0025
I0520 14:57:41.772016 23372 solver.cpp:237] Iteration 6099, loss = 1.38152
I0520 14:57:41.772073 23372 solver.cpp:253]     Train net output #0: loss = 1.38152 (* 1 = 1.38152 loss)
I0520 14:57:41.772090 23372 sgd_solver.cpp:106] Iteration 6099, lr = 0.0025
I0520 14:57:50.220219 23372 solver.cpp:237] Iteration 6206, loss = 1.48735
I0520 14:57:50.220372 23372 solver.cpp:253]     Train net output #0: loss = 1.48735 (* 1 = 1.48735 loss)
I0520 14:57:50.220388 23372 sgd_solver.cpp:106] Iteration 6206, lr = 0.0025
I0520 14:57:58.672397 23372 solver.cpp:237] Iteration 6313, loss = 1.44562
I0520 14:57:58.672432 23372 solver.cpp:253]     Train net output #0: loss = 1.44562 (* 1 = 1.44562 loss)
I0520 14:57:58.672451 23372 sgd_solver.cpp:106] Iteration 6313, lr = 0.0025
I0520 14:58:07.123191 23372 solver.cpp:237] Iteration 6420, loss = 1.47364
I0520 14:58:07.123250 23372 solver.cpp:253]     Train net output #0: loss = 1.47364 (* 1 = 1.47364 loss)
I0520 14:58:07.123275 23372 sgd_solver.cpp:106] Iteration 6420, lr = 0.0025
I0520 14:58:07.518118 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_6426.caffemodel
I0520 14:58:07.612117 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_6426.solverstate
I0520 14:58:07.638684 23372 solver.cpp:341] Iteration 6426, Testing net (#0)
I0520 14:58:53.542320 23372 solver.cpp:409]     Test net output #0: accuracy = 0.803403
I0520 14:58:53.542490 23372 solver.cpp:409]     Test net output #1: loss = 0.68443 (* 1 = 0.68443 loss)
I0520 14:59:23.710758 23372 solver.cpp:237] Iteration 6527, loss = 1.42286
I0520 14:59:23.710929 23372 solver.cpp:253]     Train net output #0: loss = 1.42286 (* 1 = 1.42286 loss)
I0520 14:59:23.710947 23372 sgd_solver.cpp:106] Iteration 6527, lr = 0.0025
I0520 14:59:32.158365 23372 solver.cpp:237] Iteration 6634, loss = 1.49215
I0520 14:59:32.158402 23372 solver.cpp:253]     Train net output #0: loss = 1.49215 (* 1 = 1.49215 loss)
I0520 14:59:32.158421 23372 sgd_solver.cpp:106] Iteration 6634, lr = 0.0025
I0520 14:59:40.601910 23372 solver.cpp:237] Iteration 6741, loss = 1.38974
I0520 14:59:40.601948 23372 solver.cpp:253]     Train net output #0: loss = 1.38974 (* 1 = 1.38974 loss)
I0520 14:59:40.601966 23372 sgd_solver.cpp:106] Iteration 6741, lr = 0.0025
I0520 14:59:49.057080 23372 solver.cpp:237] Iteration 6848, loss = 1.54433
I0520 14:59:49.057138 23372 solver.cpp:253]     Train net output #0: loss = 1.54433 (* 1 = 1.54433 loss)
I0520 14:59:49.057164 23372 sgd_solver.cpp:106] Iteration 6848, lr = 0.0025
I0520 14:59:57.508803 23372 solver.cpp:237] Iteration 6955, loss = 1.40663
I0520 14:59:57.508960 23372 solver.cpp:253]     Train net output #0: loss = 1.40663 (* 1 = 1.40663 loss)
I0520 14:59:57.508977 23372 sgd_solver.cpp:106] Iteration 6955, lr = 0.0025
I0520 15:00:05.957273 23372 solver.cpp:237] Iteration 7062, loss = 1.41368
I0520 15:00:05.957309 23372 solver.cpp:253]     Train net output #0: loss = 1.41368 (* 1 = 1.41368 loss)
I0520 15:00:05.957326 23372 sgd_solver.cpp:106] Iteration 7062, lr = 0.0025
I0520 15:00:36.644945 23372 solver.cpp:237] Iteration 7169, loss = 1.45017
I0520 15:00:36.645123 23372 solver.cpp:253]     Train net output #0: loss = 1.45017 (* 1 = 1.45017 loss)
I0520 15:00:36.645141 23372 sgd_solver.cpp:106] Iteration 7169, lr = 0.0025
I0520 15:00:45.097654 23372 solver.cpp:237] Iteration 7276, loss = 1.29809
I0520 15:00:45.097697 23372 solver.cpp:253]     Train net output #0: loss = 1.29809 (* 1 = 1.29809 loss)
I0520 15:00:45.097712 23372 sgd_solver.cpp:106] Iteration 7276, lr = 0.0025
I0520 15:00:53.542491 23372 solver.cpp:237] Iteration 7383, loss = 1.49561
I0520 15:00:53.542527 23372 solver.cpp:253]     Train net output #0: loss = 1.49561 (* 1 = 1.49561 loss)
I0520 15:00:53.542551 23372 sgd_solver.cpp:106] Iteration 7383, lr = 0.0025
I0520 15:01:02.007333 23372 solver.cpp:237] Iteration 7490, loss = 1.20816
I0520 15:01:02.007388 23372 solver.cpp:253]     Train net output #0: loss = 1.20816 (* 1 = 1.20816 loss)
I0520 15:01:02.007405 23372 sgd_solver.cpp:106] Iteration 7490, lr = 0.0025
I0520 15:01:02.482365 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_7497.caffemodel
I0520 15:01:02.575018 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_7497.solverstate
I0520 15:01:10.523823 23372 solver.cpp:237] Iteration 7597, loss = 1.20012
I0520 15:01:10.523993 23372 solver.cpp:253]     Train net output #0: loss = 1.20012 (* 1 = 1.20012 loss)
I0520 15:01:10.524010 23372 sgd_solver.cpp:106] Iteration 7597, lr = 0.0025
I0520 15:01:18.978145 23372 solver.cpp:237] Iteration 7704, loss = 1.27553
I0520 15:01:18.978183 23372 solver.cpp:253]     Train net output #0: loss = 1.27553 (* 1 = 1.27553 loss)
I0520 15:01:18.978201 23372 sgd_solver.cpp:106] Iteration 7704, lr = 0.0025
I0520 15:01:27.435318 23372 solver.cpp:237] Iteration 7811, loss = 1.51448
I0520 15:01:27.435374 23372 solver.cpp:253]     Train net output #0: loss = 1.51448 (* 1 = 1.51448 loss)
I0520 15:01:27.435390 23372 sgd_solver.cpp:106] Iteration 7811, lr = 0.0025
I0520 15:01:58.077265 23372 solver.cpp:237] Iteration 7918, loss = 1.29929
I0520 15:01:58.077437 23372 solver.cpp:253]     Train net output #0: loss = 1.29929 (* 1 = 1.29929 loss)
I0520 15:01:58.077455 23372 sgd_solver.cpp:106] Iteration 7918, lr = 0.0025
I0520 15:02:06.531141 23372 solver.cpp:237] Iteration 8025, loss = 1.25348
I0520 15:02:06.531175 23372 solver.cpp:253]     Train net output #0: loss = 1.25348 (* 1 = 1.25348 loss)
I0520 15:02:06.531194 23372 sgd_solver.cpp:106] Iteration 8025, lr = 0.0025
I0520 15:02:14.981272 23372 solver.cpp:237] Iteration 8132, loss = 1.4617
I0520 15:02:14.981324 23372 solver.cpp:253]     Train net output #0: loss = 1.4617 (* 1 = 1.4617 loss)
I0520 15:02:14.981341 23372 sgd_solver.cpp:106] Iteration 8132, lr = 0.0025
I0520 15:02:23.442718 23372 solver.cpp:237] Iteration 8239, loss = 1.28327
I0520 15:02:23.442754 23372 solver.cpp:253]     Train net output #0: loss = 1.28327 (* 1 = 1.28327 loss)
I0520 15:02:23.442778 23372 sgd_solver.cpp:106] Iteration 8239, lr = 0.0025
I0520 15:02:31.895635 23372 solver.cpp:237] Iteration 8346, loss = 1.31169
I0520 15:02:31.895782 23372 solver.cpp:253]     Train net output #0: loss = 1.31169 (* 1 = 1.31169 loss)
I0520 15:02:31.895798 23372 sgd_solver.cpp:106] Iteration 8346, lr = 0.0025
I0520 15:02:40.347642 23372 solver.cpp:237] Iteration 8453, loss = 1.54181
I0520 15:02:40.347700 23372 solver.cpp:253]     Train net output #0: loss = 1.54181 (* 1 = 1.54181 loss)
I0520 15:02:40.347726 23372 sgd_solver.cpp:106] Iteration 8453, lr = 0.0025
I0520 15:02:48.798502 23372 solver.cpp:237] Iteration 8560, loss = 1.33689
I0520 15:02:48.798538 23372 solver.cpp:253]     Train net output #0: loss = 1.33689 (* 1 = 1.33689 loss)
I0520 15:02:48.798563 23372 sgd_solver.cpp:106] Iteration 8560, lr = 0.0025
I0520 15:02:49.351893 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_8568.caffemodel
I0520 15:02:49.444627 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_8568.solverstate
I0520 15:02:49.471145 23372 solver.cpp:341] Iteration 8568, Testing net (#0)
I0520 15:03:56.453613 23372 solver.cpp:409]     Test net output #0: accuracy = 0.82241
I0520 15:03:56.453806 23372 solver.cpp:409]     Test net output #1: loss = 0.650037 (* 1 = 0.650037 loss)
I0520 15:04:26.470938 23372 solver.cpp:237] Iteration 8667, loss = 1.47272
I0520 15:04:26.471118 23372 solver.cpp:253]     Train net output #0: loss = 1.47272 (* 1 = 1.47272 loss)
I0520 15:04:26.471143 23372 sgd_solver.cpp:106] Iteration 8667, lr = 0.0025
I0520 15:04:34.922245 23372 solver.cpp:237] Iteration 8774, loss = 1.51282
I0520 15:04:34.922282 23372 solver.cpp:253]     Train net output #0: loss = 1.51282 (* 1 = 1.51282 loss)
I0520 15:04:34.922300 23372 sgd_solver.cpp:106] Iteration 8774, lr = 0.0025
I0520 15:04:43.374343 23372 solver.cpp:237] Iteration 8881, loss = 1.35739
I0520 15:04:43.374400 23372 solver.cpp:253]     Train net output #0: loss = 1.35739 (* 1 = 1.35739 loss)
I0520 15:04:43.374428 23372 sgd_solver.cpp:106] Iteration 8881, lr = 0.0025
I0520 15:04:51.831038 23372 solver.cpp:237] Iteration 8988, loss = 1.36347
I0520 15:04:51.831074 23372 solver.cpp:253]     Train net output #0: loss = 1.36347 (* 1 = 1.36347 loss)
I0520 15:04:51.831099 23372 sgd_solver.cpp:106] Iteration 8988, lr = 0.0025
I0520 15:05:00.282549 23372 solver.cpp:237] Iteration 9095, loss = 1.2981
I0520 15:05:00.282699 23372 solver.cpp:253]     Train net output #0: loss = 1.2981 (* 1 = 1.2981 loss)
I0520 15:05:00.282716 23372 sgd_solver.cpp:106] Iteration 9095, lr = 0.0025
I0520 15:05:08.740427 23372 solver.cpp:237] Iteration 9202, loss = 1.19357
I0520 15:05:08.740480 23372 solver.cpp:253]     Train net output #0: loss = 1.19357 (* 1 = 1.19357 loss)
I0520 15:05:08.740497 23372 sgd_solver.cpp:106] Iteration 9202, lr = 0.0025
I0520 15:05:39.426262 23372 solver.cpp:237] Iteration 9309, loss = 1.33298
I0520 15:05:39.426446 23372 solver.cpp:253]     Train net output #0: loss = 1.33298 (* 1 = 1.33298 loss)
I0520 15:05:39.426463 23372 sgd_solver.cpp:106] Iteration 9309, lr = 0.0025
I0520 15:05:47.879593 23372 solver.cpp:237] Iteration 9416, loss = 1.25259
I0520 15:05:47.879629 23372 solver.cpp:253]     Train net output #0: loss = 1.25259 (* 1 = 1.25259 loss)
I0520 15:05:47.879653 23372 sgd_solver.cpp:106] Iteration 9416, lr = 0.0025
I0520 15:05:56.330422 23372 solver.cpp:237] Iteration 9523, loss = 1.21179
I0520 15:05:56.330458 23372 solver.cpp:253]     Train net output #0: loss = 1.21179 (* 1 = 1.21179 loss)
I0520 15:05:56.330477 23372 sgd_solver.cpp:106] Iteration 9523, lr = 0.0025
I0520 15:06:04.781065 23372 solver.cpp:237] Iteration 9630, loss = 1.60992
I0520 15:06:04.781121 23372 solver.cpp:253]     Train net output #0: loss = 1.60992 (* 1 = 1.60992 loss)
I0520 15:06:04.781138 23372 sgd_solver.cpp:106] Iteration 9630, lr = 0.0025
I0520 15:06:05.412266 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_9639.caffemodel
I0520 15:06:05.508781 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_9639.solverstate
I0520 15:06:13.299329 23372 solver.cpp:237] Iteration 9737, loss = 1.21562
I0520 15:06:13.299504 23372 solver.cpp:253]     Train net output #0: loss = 1.21562 (* 1 = 1.21562 loss)
I0520 15:06:13.299522 23372 sgd_solver.cpp:106] Iteration 9737, lr = 0.0025
I0520 15:06:21.746562 23372 solver.cpp:237] Iteration 9844, loss = 1.3371
I0520 15:06:21.746598 23372 solver.cpp:253]     Train net output #0: loss = 1.3371 (* 1 = 1.3371 loss)
I0520 15:06:21.746621 23372 sgd_solver.cpp:106] Iteration 9844, lr = 0.0025
I0520 15:06:30.201813 23372 solver.cpp:237] Iteration 9951, loss = 1.40506
I0520 15:06:30.201874 23372 solver.cpp:253]     Train net output #0: loss = 1.40506 (* 1 = 1.40506 loss)
I0520 15:06:30.201897 23372 sgd_solver.cpp:106] Iteration 9951, lr = 0.0025
I0520 15:07:00.879034 23372 solver.cpp:237] Iteration 10058, loss = 1.19774
I0520 15:07:00.879216 23372 solver.cpp:253]     Train net output #0: loss = 1.19774 (* 1 = 1.19774 loss)
I0520 15:07:00.879233 23372 sgd_solver.cpp:106] Iteration 10058, lr = 0.0025
I0520 15:07:09.329777 23372 solver.cpp:237] Iteration 10165, loss = 1.68995
I0520 15:07:09.329813 23372 solver.cpp:253]     Train net output #0: loss = 1.68995 (* 1 = 1.68995 loss)
I0520 15:07:09.329836 23372 sgd_solver.cpp:106] Iteration 10165, lr = 0.0025
I0520 15:07:17.781574 23372 solver.cpp:237] Iteration 10272, loss = 1.46637
I0520 15:07:17.781631 23372 solver.cpp:253]     Train net output #0: loss = 1.46637 (* 1 = 1.46637 loss)
I0520 15:07:17.781657 23372 sgd_solver.cpp:106] Iteration 10272, lr = 0.0025
I0520 15:07:26.235126 23372 solver.cpp:237] Iteration 10379, loss = 1.33863
I0520 15:07:26.235162 23372 solver.cpp:253]     Train net output #0: loss = 1.33863 (* 1 = 1.33863 loss)
I0520 15:07:26.235179 23372 sgd_solver.cpp:106] Iteration 10379, lr = 0.0025
I0520 15:07:34.685961 23372 solver.cpp:237] Iteration 10486, loss = 1.20591
I0520 15:07:34.686115 23372 solver.cpp:253]     Train net output #0: loss = 1.20591 (* 1 = 1.20591 loss)
I0520 15:07:34.686131 23372 sgd_solver.cpp:106] Iteration 10486, lr = 0.0025
I0520 15:07:43.135412 23372 solver.cpp:237] Iteration 10593, loss = 1.27941
I0520 15:07:43.135470 23372 solver.cpp:253]     Train net output #0: loss = 1.27941 (* 1 = 1.27941 loss)
I0520 15:07:43.135498 23372 sgd_solver.cpp:106] Iteration 10593, lr = 0.0025
I0520 15:07:51.589807 23372 solver.cpp:237] Iteration 10700, loss = 1.44726
I0520 15:07:51.589843 23372 solver.cpp:253]     Train net output #0: loss = 1.44726 (* 1 = 1.44726 loss)
I0520 15:07:51.589867 23372 sgd_solver.cpp:106] Iteration 10700, lr = 0.0025
I0520 15:07:52.303079 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_10710.caffemodel
I0520 15:07:52.398499 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_10710.solverstate
I0520 15:07:52.426329 23372 solver.cpp:341] Iteration 10710, Testing net (#0)
I0520 15:08:38.641091 23372 solver.cpp:409]     Test net output #0: accuracy = 0.829913
I0520 15:08:38.641266 23372 solver.cpp:409]     Test net output #1: loss = 0.600628 (* 1 = 0.600628 loss)
I0520 15:08:38.902869 23372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_10714.caffemodel
I0520 15:08:38.997920 23372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920_iter_10714.solverstate
I0520 15:08:39.025204 23372 solver.cpp:326] Optimization Done.
I0520 15:08:39.025241 23372 caffe.cpp:215] Optimization Done.
Application 11232704 resources: utime ~1292s, stime ~231s, Rss ~5329872, inblocks ~3594474, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_140_2016-05-20T11.20.37.752920.solver"
	User time (seconds): 0.54
	System time (seconds): 0.18
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:27.92
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15111
	Voluntary context switches: 2819
	Involuntary context switches: 127
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
