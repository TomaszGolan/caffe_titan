2806343
I0521 07:27:40.033071 16945 caffe.cpp:184] Using GPUs 0
I0521 07:27:40.457520 16945 solver.cpp:48] Initializing solver from parameters: 
test_iter: 185
test_interval: 370
base_lr: 0.0025
display: 18
max_iter: 1851
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 185
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442.prototxt"
I0521 07:27:40.459147 16945 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442.prototxt
I0521 07:27:40.476250 16945 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 07:27:40.476310 16945 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 07:27:40.476652 16945 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 810
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:27:40.476830 16945 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:27:40.476855 16945 net.cpp:106] Creating Layer data_hdf5
I0521 07:27:40.476869 16945 net.cpp:411] data_hdf5 -> data
I0521 07:27:40.476904 16945 net.cpp:411] data_hdf5 -> label
I0521 07:27:40.476936 16945 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 07:27:40.478266 16945 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 07:27:40.480533 16945 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 07:28:02.094640 16945 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 07:28:02.099748 16945 net.cpp:150] Setting up data_hdf5
I0521 07:28:02.099791 16945 net.cpp:157] Top shape: 810 1 127 50 (5143500)
I0521 07:28:02.099805 16945 net.cpp:157] Top shape: 810 (810)
I0521 07:28:02.099815 16945 net.cpp:165] Memory required for data: 20577240
I0521 07:28:02.099829 16945 layer_factory.hpp:77] Creating layer conv1
I0521 07:28:02.099864 16945 net.cpp:106] Creating Layer conv1
I0521 07:28:02.099875 16945 net.cpp:454] conv1 <- data
I0521 07:28:02.099898 16945 net.cpp:411] conv1 -> conv1
I0521 07:28:02.464006 16945 net.cpp:150] Setting up conv1
I0521 07:28:02.464053 16945 net.cpp:157] Top shape: 810 12 120 48 (55987200)
I0521 07:28:02.464064 16945 net.cpp:165] Memory required for data: 244526040
I0521 07:28:02.464092 16945 layer_factory.hpp:77] Creating layer relu1
I0521 07:28:02.464113 16945 net.cpp:106] Creating Layer relu1
I0521 07:28:02.464125 16945 net.cpp:454] relu1 <- conv1
I0521 07:28:02.464138 16945 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:28:02.464658 16945 net.cpp:150] Setting up relu1
I0521 07:28:02.464675 16945 net.cpp:157] Top shape: 810 12 120 48 (55987200)
I0521 07:28:02.464685 16945 net.cpp:165] Memory required for data: 468474840
I0521 07:28:02.464696 16945 layer_factory.hpp:77] Creating layer pool1
I0521 07:28:02.464712 16945 net.cpp:106] Creating Layer pool1
I0521 07:28:02.464722 16945 net.cpp:454] pool1 <- conv1
I0521 07:28:02.464736 16945 net.cpp:411] pool1 -> pool1
I0521 07:28:02.464815 16945 net.cpp:150] Setting up pool1
I0521 07:28:02.464829 16945 net.cpp:157] Top shape: 810 12 60 48 (27993600)
I0521 07:28:02.464839 16945 net.cpp:165] Memory required for data: 580449240
I0521 07:28:02.464849 16945 layer_factory.hpp:77] Creating layer conv2
I0521 07:28:02.464872 16945 net.cpp:106] Creating Layer conv2
I0521 07:28:02.464882 16945 net.cpp:454] conv2 <- pool1
I0521 07:28:02.464895 16945 net.cpp:411] conv2 -> conv2
I0521 07:28:02.467581 16945 net.cpp:150] Setting up conv2
I0521 07:28:02.467603 16945 net.cpp:157] Top shape: 810 20 54 46 (40240800)
I0521 07:28:02.467614 16945 net.cpp:165] Memory required for data: 741412440
I0521 07:28:02.467634 16945 layer_factory.hpp:77] Creating layer relu2
I0521 07:28:02.467648 16945 net.cpp:106] Creating Layer relu2
I0521 07:28:02.467658 16945 net.cpp:454] relu2 <- conv2
I0521 07:28:02.467671 16945 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:28:02.468003 16945 net.cpp:150] Setting up relu2
I0521 07:28:02.468017 16945 net.cpp:157] Top shape: 810 20 54 46 (40240800)
I0521 07:28:02.468027 16945 net.cpp:165] Memory required for data: 902375640
I0521 07:28:02.468039 16945 layer_factory.hpp:77] Creating layer pool2
I0521 07:28:02.468050 16945 net.cpp:106] Creating Layer pool2
I0521 07:28:02.468060 16945 net.cpp:454] pool2 <- conv2
I0521 07:28:02.468086 16945 net.cpp:411] pool2 -> pool2
I0521 07:28:02.468154 16945 net.cpp:150] Setting up pool2
I0521 07:28:02.468168 16945 net.cpp:157] Top shape: 810 20 27 46 (20120400)
I0521 07:28:02.468178 16945 net.cpp:165] Memory required for data: 982857240
I0521 07:28:02.468186 16945 layer_factory.hpp:77] Creating layer conv3
I0521 07:28:02.468205 16945 net.cpp:106] Creating Layer conv3
I0521 07:28:02.468215 16945 net.cpp:454] conv3 <- pool2
I0521 07:28:02.468228 16945 net.cpp:411] conv3 -> conv3
I0521 07:28:02.470157 16945 net.cpp:150] Setting up conv3
I0521 07:28:02.470180 16945 net.cpp:157] Top shape: 810 28 22 44 (21954240)
I0521 07:28:02.470192 16945 net.cpp:165] Memory required for data: 1070674200
I0521 07:28:02.470211 16945 layer_factory.hpp:77] Creating layer relu3
I0521 07:28:02.470227 16945 net.cpp:106] Creating Layer relu3
I0521 07:28:02.470237 16945 net.cpp:454] relu3 <- conv3
I0521 07:28:02.470249 16945 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:28:02.470720 16945 net.cpp:150] Setting up relu3
I0521 07:28:02.470737 16945 net.cpp:157] Top shape: 810 28 22 44 (21954240)
I0521 07:28:02.470747 16945 net.cpp:165] Memory required for data: 1158491160
I0521 07:28:02.470757 16945 layer_factory.hpp:77] Creating layer pool3
I0521 07:28:02.470770 16945 net.cpp:106] Creating Layer pool3
I0521 07:28:02.470779 16945 net.cpp:454] pool3 <- conv3
I0521 07:28:02.470793 16945 net.cpp:411] pool3 -> pool3
I0521 07:28:02.470860 16945 net.cpp:150] Setting up pool3
I0521 07:28:02.470873 16945 net.cpp:157] Top shape: 810 28 11 44 (10977120)
I0521 07:28:02.470883 16945 net.cpp:165] Memory required for data: 1202399640
I0521 07:28:02.470892 16945 layer_factory.hpp:77] Creating layer conv4
I0521 07:28:02.470909 16945 net.cpp:106] Creating Layer conv4
I0521 07:28:02.470919 16945 net.cpp:454] conv4 <- pool3
I0521 07:28:02.470933 16945 net.cpp:411] conv4 -> conv4
I0521 07:28:02.473642 16945 net.cpp:150] Setting up conv4
I0521 07:28:02.473669 16945 net.cpp:157] Top shape: 810 36 6 42 (7348320)
I0521 07:28:02.473680 16945 net.cpp:165] Memory required for data: 1231792920
I0521 07:28:02.473695 16945 layer_factory.hpp:77] Creating layer relu4
I0521 07:28:02.473711 16945 net.cpp:106] Creating Layer relu4
I0521 07:28:02.473721 16945 net.cpp:454] relu4 <- conv4
I0521 07:28:02.473733 16945 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:28:02.474198 16945 net.cpp:150] Setting up relu4
I0521 07:28:02.474215 16945 net.cpp:157] Top shape: 810 36 6 42 (7348320)
I0521 07:28:02.474225 16945 net.cpp:165] Memory required for data: 1261186200
I0521 07:28:02.474236 16945 layer_factory.hpp:77] Creating layer pool4
I0521 07:28:02.474248 16945 net.cpp:106] Creating Layer pool4
I0521 07:28:02.474258 16945 net.cpp:454] pool4 <- conv4
I0521 07:28:02.474272 16945 net.cpp:411] pool4 -> pool4
I0521 07:28:02.474340 16945 net.cpp:150] Setting up pool4
I0521 07:28:02.474354 16945 net.cpp:157] Top shape: 810 36 3 42 (3674160)
I0521 07:28:02.474364 16945 net.cpp:165] Memory required for data: 1275882840
I0521 07:28:02.474375 16945 layer_factory.hpp:77] Creating layer ip1
I0521 07:28:02.474395 16945 net.cpp:106] Creating Layer ip1
I0521 07:28:02.474406 16945 net.cpp:454] ip1 <- pool4
I0521 07:28:02.474418 16945 net.cpp:411] ip1 -> ip1
I0521 07:28:02.489773 16945 net.cpp:150] Setting up ip1
I0521 07:28:02.489802 16945 net.cpp:157] Top shape: 810 196 (158760)
I0521 07:28:02.489814 16945 net.cpp:165] Memory required for data: 1276517880
I0521 07:28:02.489836 16945 layer_factory.hpp:77] Creating layer relu5
I0521 07:28:02.489851 16945 net.cpp:106] Creating Layer relu5
I0521 07:28:02.489862 16945 net.cpp:454] relu5 <- ip1
I0521 07:28:02.489876 16945 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:28:02.490218 16945 net.cpp:150] Setting up relu5
I0521 07:28:02.490233 16945 net.cpp:157] Top shape: 810 196 (158760)
I0521 07:28:02.490242 16945 net.cpp:165] Memory required for data: 1277152920
I0521 07:28:02.490252 16945 layer_factory.hpp:77] Creating layer drop1
I0521 07:28:02.490275 16945 net.cpp:106] Creating Layer drop1
I0521 07:28:02.490285 16945 net.cpp:454] drop1 <- ip1
I0521 07:28:02.490311 16945 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:28:02.490358 16945 net.cpp:150] Setting up drop1
I0521 07:28:02.490371 16945 net.cpp:157] Top shape: 810 196 (158760)
I0521 07:28:02.490383 16945 net.cpp:165] Memory required for data: 1277787960
I0521 07:28:02.490393 16945 layer_factory.hpp:77] Creating layer ip2
I0521 07:28:02.490412 16945 net.cpp:106] Creating Layer ip2
I0521 07:28:02.490422 16945 net.cpp:454] ip2 <- ip1
I0521 07:28:02.490435 16945 net.cpp:411] ip2 -> ip2
I0521 07:28:02.490900 16945 net.cpp:150] Setting up ip2
I0521 07:28:02.490914 16945 net.cpp:157] Top shape: 810 98 (79380)
I0521 07:28:02.490924 16945 net.cpp:165] Memory required for data: 1278105480
I0521 07:28:02.490939 16945 layer_factory.hpp:77] Creating layer relu6
I0521 07:28:02.490952 16945 net.cpp:106] Creating Layer relu6
I0521 07:28:02.490962 16945 net.cpp:454] relu6 <- ip2
I0521 07:28:02.490973 16945 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:28:02.491488 16945 net.cpp:150] Setting up relu6
I0521 07:28:02.491504 16945 net.cpp:157] Top shape: 810 98 (79380)
I0521 07:28:02.491514 16945 net.cpp:165] Memory required for data: 1278423000
I0521 07:28:02.491526 16945 layer_factory.hpp:77] Creating layer drop2
I0521 07:28:02.491539 16945 net.cpp:106] Creating Layer drop2
I0521 07:28:02.491549 16945 net.cpp:454] drop2 <- ip2
I0521 07:28:02.491562 16945 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:28:02.491605 16945 net.cpp:150] Setting up drop2
I0521 07:28:02.491618 16945 net.cpp:157] Top shape: 810 98 (79380)
I0521 07:28:02.491628 16945 net.cpp:165] Memory required for data: 1278740520
I0521 07:28:02.491638 16945 layer_factory.hpp:77] Creating layer ip3
I0521 07:28:02.491652 16945 net.cpp:106] Creating Layer ip3
I0521 07:28:02.491662 16945 net.cpp:454] ip3 <- ip2
I0521 07:28:02.491674 16945 net.cpp:411] ip3 -> ip3
I0521 07:28:02.491886 16945 net.cpp:150] Setting up ip3
I0521 07:28:02.491899 16945 net.cpp:157] Top shape: 810 11 (8910)
I0521 07:28:02.491909 16945 net.cpp:165] Memory required for data: 1278776160
I0521 07:28:02.491925 16945 layer_factory.hpp:77] Creating layer drop3
I0521 07:28:02.491936 16945 net.cpp:106] Creating Layer drop3
I0521 07:28:02.491946 16945 net.cpp:454] drop3 <- ip3
I0521 07:28:02.491958 16945 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:28:02.491998 16945 net.cpp:150] Setting up drop3
I0521 07:28:02.492010 16945 net.cpp:157] Top shape: 810 11 (8910)
I0521 07:28:02.492020 16945 net.cpp:165] Memory required for data: 1278811800
I0521 07:28:02.492028 16945 layer_factory.hpp:77] Creating layer loss
I0521 07:28:02.492048 16945 net.cpp:106] Creating Layer loss
I0521 07:28:02.492059 16945 net.cpp:454] loss <- ip3
I0521 07:28:02.492070 16945 net.cpp:454] loss <- label
I0521 07:28:02.492082 16945 net.cpp:411] loss -> loss
I0521 07:28:02.492099 16945 layer_factory.hpp:77] Creating layer loss
I0521 07:28:02.492748 16945 net.cpp:150] Setting up loss
I0521 07:28:02.492764 16945 net.cpp:157] Top shape: (1)
I0521 07:28:02.492779 16945 net.cpp:160]     with loss weight 1
I0521 07:28:02.492820 16945 net.cpp:165] Memory required for data: 1278811804
I0521 07:28:02.492831 16945 net.cpp:226] loss needs backward computation.
I0521 07:28:02.492842 16945 net.cpp:226] drop3 needs backward computation.
I0521 07:28:02.492851 16945 net.cpp:226] ip3 needs backward computation.
I0521 07:28:02.492862 16945 net.cpp:226] drop2 needs backward computation.
I0521 07:28:02.492872 16945 net.cpp:226] relu6 needs backward computation.
I0521 07:28:02.492882 16945 net.cpp:226] ip2 needs backward computation.
I0521 07:28:02.492892 16945 net.cpp:226] drop1 needs backward computation.
I0521 07:28:02.492902 16945 net.cpp:226] relu5 needs backward computation.
I0521 07:28:02.492911 16945 net.cpp:226] ip1 needs backward computation.
I0521 07:28:02.492921 16945 net.cpp:226] pool4 needs backward computation.
I0521 07:28:02.492931 16945 net.cpp:226] relu4 needs backward computation.
I0521 07:28:02.492941 16945 net.cpp:226] conv4 needs backward computation.
I0521 07:28:02.492952 16945 net.cpp:226] pool3 needs backward computation.
I0521 07:28:02.492970 16945 net.cpp:226] relu3 needs backward computation.
I0521 07:28:02.492981 16945 net.cpp:226] conv3 needs backward computation.
I0521 07:28:02.492990 16945 net.cpp:226] pool2 needs backward computation.
I0521 07:28:02.493001 16945 net.cpp:226] relu2 needs backward computation.
I0521 07:28:02.493010 16945 net.cpp:226] conv2 needs backward computation.
I0521 07:28:02.493019 16945 net.cpp:226] pool1 needs backward computation.
I0521 07:28:02.493031 16945 net.cpp:226] relu1 needs backward computation.
I0521 07:28:02.493041 16945 net.cpp:226] conv1 needs backward computation.
I0521 07:28:02.493052 16945 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:28:02.493062 16945 net.cpp:270] This network produces output loss
I0521 07:28:02.493084 16945 net.cpp:283] Network initialization done.
I0521 07:28:02.494686 16945 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442.prototxt
I0521 07:28:02.494757 16945 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 07:28:02.495112 16945 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 810
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:28:02.495301 16945 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:28:02.495317 16945 net.cpp:106] Creating Layer data_hdf5
I0521 07:28:02.495329 16945 net.cpp:411] data_hdf5 -> data
I0521 07:28:02.495343 16945 net.cpp:411] data_hdf5 -> label
I0521 07:28:02.495359 16945 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 07:28:02.496520 16945 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 07:28:23.862962 16945 net.cpp:150] Setting up data_hdf5
I0521 07:28:23.863128 16945 net.cpp:157] Top shape: 810 1 127 50 (5143500)
I0521 07:28:23.863142 16945 net.cpp:157] Top shape: 810 (810)
I0521 07:28:23.863152 16945 net.cpp:165] Memory required for data: 20577240
I0521 07:28:23.863167 16945 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 07:28:23.863195 16945 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 07:28:23.863206 16945 net.cpp:454] label_data_hdf5_1_split <- label
I0521 07:28:23.863221 16945 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 07:28:23.863242 16945 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 07:28:23.863317 16945 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 07:28:23.863330 16945 net.cpp:157] Top shape: 810 (810)
I0521 07:28:23.863343 16945 net.cpp:157] Top shape: 810 (810)
I0521 07:28:23.863351 16945 net.cpp:165] Memory required for data: 20583720
I0521 07:28:23.863361 16945 layer_factory.hpp:77] Creating layer conv1
I0521 07:28:23.863384 16945 net.cpp:106] Creating Layer conv1
I0521 07:28:23.863394 16945 net.cpp:454] conv1 <- data
I0521 07:28:23.863409 16945 net.cpp:411] conv1 -> conv1
I0521 07:28:23.865335 16945 net.cpp:150] Setting up conv1
I0521 07:28:23.865360 16945 net.cpp:157] Top shape: 810 12 120 48 (55987200)
I0521 07:28:23.865370 16945 net.cpp:165] Memory required for data: 244532520
I0521 07:28:23.865391 16945 layer_factory.hpp:77] Creating layer relu1
I0521 07:28:23.865406 16945 net.cpp:106] Creating Layer relu1
I0521 07:28:23.865417 16945 net.cpp:454] relu1 <- conv1
I0521 07:28:23.865428 16945 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:28:23.865927 16945 net.cpp:150] Setting up relu1
I0521 07:28:23.865942 16945 net.cpp:157] Top shape: 810 12 120 48 (55987200)
I0521 07:28:23.865952 16945 net.cpp:165] Memory required for data: 468481320
I0521 07:28:23.865963 16945 layer_factory.hpp:77] Creating layer pool1
I0521 07:28:23.865979 16945 net.cpp:106] Creating Layer pool1
I0521 07:28:23.865988 16945 net.cpp:454] pool1 <- conv1
I0521 07:28:23.866001 16945 net.cpp:411] pool1 -> pool1
I0521 07:28:23.866076 16945 net.cpp:150] Setting up pool1
I0521 07:28:23.866091 16945 net.cpp:157] Top shape: 810 12 60 48 (27993600)
I0521 07:28:23.866099 16945 net.cpp:165] Memory required for data: 580455720
I0521 07:28:23.866109 16945 layer_factory.hpp:77] Creating layer conv2
I0521 07:28:23.866127 16945 net.cpp:106] Creating Layer conv2
I0521 07:28:23.866138 16945 net.cpp:454] conv2 <- pool1
I0521 07:28:23.866152 16945 net.cpp:411] conv2 -> conv2
I0521 07:28:23.868062 16945 net.cpp:150] Setting up conv2
I0521 07:28:23.868083 16945 net.cpp:157] Top shape: 810 20 54 46 (40240800)
I0521 07:28:23.868096 16945 net.cpp:165] Memory required for data: 741418920
I0521 07:28:23.868114 16945 layer_factory.hpp:77] Creating layer relu2
I0521 07:28:23.868127 16945 net.cpp:106] Creating Layer relu2
I0521 07:28:23.868137 16945 net.cpp:454] relu2 <- conv2
I0521 07:28:23.868150 16945 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:28:23.868484 16945 net.cpp:150] Setting up relu2
I0521 07:28:23.868499 16945 net.cpp:157] Top shape: 810 20 54 46 (40240800)
I0521 07:28:23.868510 16945 net.cpp:165] Memory required for data: 902382120
I0521 07:28:23.868520 16945 layer_factory.hpp:77] Creating layer pool2
I0521 07:28:23.868533 16945 net.cpp:106] Creating Layer pool2
I0521 07:28:23.868543 16945 net.cpp:454] pool2 <- conv2
I0521 07:28:23.868556 16945 net.cpp:411] pool2 -> pool2
I0521 07:28:23.868626 16945 net.cpp:150] Setting up pool2
I0521 07:28:23.868639 16945 net.cpp:157] Top shape: 810 20 27 46 (20120400)
I0521 07:28:23.868649 16945 net.cpp:165] Memory required for data: 982863720
I0521 07:28:23.868659 16945 layer_factory.hpp:77] Creating layer conv3
I0521 07:28:23.868676 16945 net.cpp:106] Creating Layer conv3
I0521 07:28:23.868687 16945 net.cpp:454] conv3 <- pool2
I0521 07:28:23.868701 16945 net.cpp:411] conv3 -> conv3
I0521 07:28:23.870679 16945 net.cpp:150] Setting up conv3
I0521 07:28:23.870703 16945 net.cpp:157] Top shape: 810 28 22 44 (21954240)
I0521 07:28:23.870714 16945 net.cpp:165] Memory required for data: 1070680680
I0521 07:28:23.870748 16945 layer_factory.hpp:77] Creating layer relu3
I0521 07:28:23.870761 16945 net.cpp:106] Creating Layer relu3
I0521 07:28:23.870771 16945 net.cpp:454] relu3 <- conv3
I0521 07:28:23.870784 16945 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:28:23.871259 16945 net.cpp:150] Setting up relu3
I0521 07:28:23.871275 16945 net.cpp:157] Top shape: 810 28 22 44 (21954240)
I0521 07:28:23.871285 16945 net.cpp:165] Memory required for data: 1158497640
I0521 07:28:23.871295 16945 layer_factory.hpp:77] Creating layer pool3
I0521 07:28:23.871309 16945 net.cpp:106] Creating Layer pool3
I0521 07:28:23.871318 16945 net.cpp:454] pool3 <- conv3
I0521 07:28:23.871331 16945 net.cpp:411] pool3 -> pool3
I0521 07:28:23.871403 16945 net.cpp:150] Setting up pool3
I0521 07:28:23.871417 16945 net.cpp:157] Top shape: 810 28 11 44 (10977120)
I0521 07:28:23.871426 16945 net.cpp:165] Memory required for data: 1202406120
I0521 07:28:23.871436 16945 layer_factory.hpp:77] Creating layer conv4
I0521 07:28:23.871454 16945 net.cpp:106] Creating Layer conv4
I0521 07:28:23.871464 16945 net.cpp:454] conv4 <- pool3
I0521 07:28:23.871477 16945 net.cpp:411] conv4 -> conv4
I0521 07:28:23.873538 16945 net.cpp:150] Setting up conv4
I0521 07:28:23.873559 16945 net.cpp:157] Top shape: 810 36 6 42 (7348320)
I0521 07:28:23.873571 16945 net.cpp:165] Memory required for data: 1231799400
I0521 07:28:23.873586 16945 layer_factory.hpp:77] Creating layer relu4
I0521 07:28:23.873600 16945 net.cpp:106] Creating Layer relu4
I0521 07:28:23.873610 16945 net.cpp:454] relu4 <- conv4
I0521 07:28:23.873622 16945 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:28:23.874092 16945 net.cpp:150] Setting up relu4
I0521 07:28:23.874109 16945 net.cpp:157] Top shape: 810 36 6 42 (7348320)
I0521 07:28:23.874119 16945 net.cpp:165] Memory required for data: 1261192680
I0521 07:28:23.874130 16945 layer_factory.hpp:77] Creating layer pool4
I0521 07:28:23.874142 16945 net.cpp:106] Creating Layer pool4
I0521 07:28:23.874152 16945 net.cpp:454] pool4 <- conv4
I0521 07:28:23.874166 16945 net.cpp:411] pool4 -> pool4
I0521 07:28:23.874238 16945 net.cpp:150] Setting up pool4
I0521 07:28:23.874250 16945 net.cpp:157] Top shape: 810 36 3 42 (3674160)
I0521 07:28:23.874259 16945 net.cpp:165] Memory required for data: 1275889320
I0521 07:28:23.874270 16945 layer_factory.hpp:77] Creating layer ip1
I0521 07:28:23.874285 16945 net.cpp:106] Creating Layer ip1
I0521 07:28:23.874295 16945 net.cpp:454] ip1 <- pool4
I0521 07:28:23.874308 16945 net.cpp:411] ip1 -> ip1
I0521 07:28:23.889780 16945 net.cpp:150] Setting up ip1
I0521 07:28:23.889807 16945 net.cpp:157] Top shape: 810 196 (158760)
I0521 07:28:23.889823 16945 net.cpp:165] Memory required for data: 1276524360
I0521 07:28:23.889845 16945 layer_factory.hpp:77] Creating layer relu5
I0521 07:28:23.889861 16945 net.cpp:106] Creating Layer relu5
I0521 07:28:23.889871 16945 net.cpp:454] relu5 <- ip1
I0521 07:28:23.889889 16945 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:28:23.890233 16945 net.cpp:150] Setting up relu5
I0521 07:28:23.890246 16945 net.cpp:157] Top shape: 810 196 (158760)
I0521 07:28:23.890256 16945 net.cpp:165] Memory required for data: 1277159400
I0521 07:28:23.890267 16945 layer_factory.hpp:77] Creating layer drop1
I0521 07:28:23.890285 16945 net.cpp:106] Creating Layer drop1
I0521 07:28:23.890295 16945 net.cpp:454] drop1 <- ip1
I0521 07:28:23.890308 16945 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:28:23.890352 16945 net.cpp:150] Setting up drop1
I0521 07:28:23.890365 16945 net.cpp:157] Top shape: 810 196 (158760)
I0521 07:28:23.890377 16945 net.cpp:165] Memory required for data: 1277794440
I0521 07:28:23.890385 16945 layer_factory.hpp:77] Creating layer ip2
I0521 07:28:23.890400 16945 net.cpp:106] Creating Layer ip2
I0521 07:28:23.890410 16945 net.cpp:454] ip2 <- ip1
I0521 07:28:23.890424 16945 net.cpp:411] ip2 -> ip2
I0521 07:28:23.890902 16945 net.cpp:150] Setting up ip2
I0521 07:28:23.890915 16945 net.cpp:157] Top shape: 810 98 (79380)
I0521 07:28:23.890925 16945 net.cpp:165] Memory required for data: 1278111960
I0521 07:28:23.890954 16945 layer_factory.hpp:77] Creating layer relu6
I0521 07:28:23.890966 16945 net.cpp:106] Creating Layer relu6
I0521 07:28:23.890976 16945 net.cpp:454] relu6 <- ip2
I0521 07:28:23.890988 16945 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:28:23.891516 16945 net.cpp:150] Setting up relu6
I0521 07:28:23.891532 16945 net.cpp:157] Top shape: 810 98 (79380)
I0521 07:28:23.891542 16945 net.cpp:165] Memory required for data: 1278429480
I0521 07:28:23.891552 16945 layer_factory.hpp:77] Creating layer drop2
I0521 07:28:23.891566 16945 net.cpp:106] Creating Layer drop2
I0521 07:28:23.891576 16945 net.cpp:454] drop2 <- ip2
I0521 07:28:23.891588 16945 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:28:23.891631 16945 net.cpp:150] Setting up drop2
I0521 07:28:23.891644 16945 net.cpp:157] Top shape: 810 98 (79380)
I0521 07:28:23.891654 16945 net.cpp:165] Memory required for data: 1278747000
I0521 07:28:23.891664 16945 layer_factory.hpp:77] Creating layer ip3
I0521 07:28:23.891677 16945 net.cpp:106] Creating Layer ip3
I0521 07:28:23.891687 16945 net.cpp:454] ip3 <- ip2
I0521 07:28:23.891701 16945 net.cpp:411] ip3 -> ip3
I0521 07:28:23.891923 16945 net.cpp:150] Setting up ip3
I0521 07:28:23.891937 16945 net.cpp:157] Top shape: 810 11 (8910)
I0521 07:28:23.891947 16945 net.cpp:165] Memory required for data: 1278782640
I0521 07:28:23.891963 16945 layer_factory.hpp:77] Creating layer drop3
I0521 07:28:23.891975 16945 net.cpp:106] Creating Layer drop3
I0521 07:28:23.891984 16945 net.cpp:454] drop3 <- ip3
I0521 07:28:23.891998 16945 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:28:23.892040 16945 net.cpp:150] Setting up drop3
I0521 07:28:23.892051 16945 net.cpp:157] Top shape: 810 11 (8910)
I0521 07:28:23.892061 16945 net.cpp:165] Memory required for data: 1278818280
I0521 07:28:23.892071 16945 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 07:28:23.892084 16945 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 07:28:23.892094 16945 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 07:28:23.892107 16945 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 07:28:23.892122 16945 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 07:28:23.892194 16945 net.cpp:150] Setting up ip3_drop3_0_split
I0521 07:28:23.892206 16945 net.cpp:157] Top shape: 810 11 (8910)
I0521 07:28:23.892220 16945 net.cpp:157] Top shape: 810 11 (8910)
I0521 07:28:23.892228 16945 net.cpp:165] Memory required for data: 1278889560
I0521 07:28:23.892238 16945 layer_factory.hpp:77] Creating layer accuracy
I0521 07:28:23.892259 16945 net.cpp:106] Creating Layer accuracy
I0521 07:28:23.892269 16945 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 07:28:23.892280 16945 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 07:28:23.892294 16945 net.cpp:411] accuracy -> accuracy
I0521 07:28:23.892318 16945 net.cpp:150] Setting up accuracy
I0521 07:28:23.892330 16945 net.cpp:157] Top shape: (1)
I0521 07:28:23.892339 16945 net.cpp:165] Memory required for data: 1278889564
I0521 07:28:23.892349 16945 layer_factory.hpp:77] Creating layer loss
I0521 07:28:23.892364 16945 net.cpp:106] Creating Layer loss
I0521 07:28:23.892374 16945 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 07:28:23.892382 16945 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 07:28:23.892395 16945 net.cpp:411] loss -> loss
I0521 07:28:23.892413 16945 layer_factory.hpp:77] Creating layer loss
I0521 07:28:23.892907 16945 net.cpp:150] Setting up loss
I0521 07:28:23.892920 16945 net.cpp:157] Top shape: (1)
I0521 07:28:23.892930 16945 net.cpp:160]     with loss weight 1
I0521 07:28:23.892949 16945 net.cpp:165] Memory required for data: 1278889568
I0521 07:28:23.892959 16945 net.cpp:226] loss needs backward computation.
I0521 07:28:23.892971 16945 net.cpp:228] accuracy does not need backward computation.
I0521 07:28:23.892982 16945 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 07:28:23.892992 16945 net.cpp:226] drop3 needs backward computation.
I0521 07:28:23.893002 16945 net.cpp:226] ip3 needs backward computation.
I0521 07:28:23.893013 16945 net.cpp:226] drop2 needs backward computation.
I0521 07:28:23.893031 16945 net.cpp:226] relu6 needs backward computation.
I0521 07:28:23.893041 16945 net.cpp:226] ip2 needs backward computation.
I0521 07:28:23.893051 16945 net.cpp:226] drop1 needs backward computation.
I0521 07:28:23.893060 16945 net.cpp:226] relu5 needs backward computation.
I0521 07:28:23.893070 16945 net.cpp:226] ip1 needs backward computation.
I0521 07:28:23.893080 16945 net.cpp:226] pool4 needs backward computation.
I0521 07:28:23.893090 16945 net.cpp:226] relu4 needs backward computation.
I0521 07:28:23.893100 16945 net.cpp:226] conv4 needs backward computation.
I0521 07:28:23.893110 16945 net.cpp:226] pool3 needs backward computation.
I0521 07:28:23.893120 16945 net.cpp:226] relu3 needs backward computation.
I0521 07:28:23.893131 16945 net.cpp:226] conv3 needs backward computation.
I0521 07:28:23.893141 16945 net.cpp:226] pool2 needs backward computation.
I0521 07:28:23.893151 16945 net.cpp:226] relu2 needs backward computation.
I0521 07:28:23.893162 16945 net.cpp:226] conv2 needs backward computation.
I0521 07:28:23.893172 16945 net.cpp:226] pool1 needs backward computation.
I0521 07:28:23.893182 16945 net.cpp:226] relu1 needs backward computation.
I0521 07:28:23.893193 16945 net.cpp:226] conv1 needs backward computation.
I0521 07:28:23.893203 16945 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 07:28:23.893215 16945 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:28:23.893224 16945 net.cpp:270] This network produces output accuracy
I0521 07:28:23.893235 16945 net.cpp:270] This network produces output loss
I0521 07:28:23.893263 16945 net.cpp:283] Network initialization done.
I0521 07:28:23.893405 16945 solver.cpp:60] Solver scaffolding done.
I0521 07:28:23.894534 16945 caffe.cpp:212] Starting Optimization
I0521 07:28:23.894553 16945 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 07:28:23.894567 16945 solver.cpp:289] Learning Rate Policy: fixed
I0521 07:28:23.895792 16945 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 07:29:09.858752 16945 solver.cpp:409]     Test net output #0: accuracy = 0.0354088
I0521 07:29:09.858911 16945 solver.cpp:409]     Test net output #1: loss = 2.40048 (* 1 = 2.40048 loss)
I0521 07:29:10.008718 16945 solver.cpp:237] Iteration 0, loss = 2.40135
I0521 07:29:10.008754 16945 solver.cpp:253]     Train net output #0: loss = 2.40135 (* 1 = 2.40135 loss)
I0521 07:29:10.008772 16945 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 07:29:17.820015 16945 solver.cpp:237] Iteration 18, loss = 2.39099
I0521 07:29:17.820063 16945 solver.cpp:253]     Train net output #0: loss = 2.39099 (* 1 = 2.39099 loss)
I0521 07:29:17.820078 16945 sgd_solver.cpp:106] Iteration 18, lr = 0.0025
I0521 07:29:25.622406 16945 solver.cpp:237] Iteration 36, loss = 2.37775
I0521 07:29:25.622438 16945 solver.cpp:253]     Train net output #0: loss = 2.37775 (* 1 = 2.37775 loss)
I0521 07:29:25.622453 16945 sgd_solver.cpp:106] Iteration 36, lr = 0.0025
I0521 07:29:33.431051 16945 solver.cpp:237] Iteration 54, loss = 2.3667
I0521 07:29:33.431082 16945 solver.cpp:253]     Train net output #0: loss = 2.3667 (* 1 = 2.3667 loss)
I0521 07:29:33.431100 16945 sgd_solver.cpp:106] Iteration 54, lr = 0.0025
I0521 07:29:41.239295 16945 solver.cpp:237] Iteration 72, loss = 2.35158
I0521 07:29:41.239454 16945 solver.cpp:253]     Train net output #0: loss = 2.35158 (* 1 = 2.35158 loss)
I0521 07:29:41.239470 16945 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0521 07:29:49.040035 16945 solver.cpp:237] Iteration 90, loss = 2.34592
I0521 07:29:49.040067 16945 solver.cpp:253]     Train net output #0: loss = 2.34592 (* 1 = 2.34592 loss)
I0521 07:29:49.040084 16945 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 07:29:56.841212 16945 solver.cpp:237] Iteration 108, loss = 2.33529
I0521 07:29:56.841243 16945 solver.cpp:253]     Train net output #0: loss = 2.33529 (* 1 = 2.33529 loss)
I0521 07:29:56.841260 16945 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0521 07:30:26.815971 16945 solver.cpp:237] Iteration 126, loss = 2.32955
I0521 07:30:26.816133 16945 solver.cpp:253]     Train net output #0: loss = 2.32955 (* 1 = 2.32955 loss)
I0521 07:30:26.816148 16945 sgd_solver.cpp:106] Iteration 126, lr = 0.0025
I0521 07:30:34.628691 16945 solver.cpp:237] Iteration 144, loss = 2.32149
I0521 07:30:34.628731 16945 solver.cpp:253]     Train net output #0: loss = 2.32149 (* 1 = 2.32149 loss)
I0521 07:30:34.628751 16945 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 07:30:42.439553 16945 solver.cpp:237] Iteration 162, loss = 2.32231
I0521 07:30:42.439585 16945 solver.cpp:253]     Train net output #0: loss = 2.32231 (* 1 = 2.32231 loss)
I0521 07:30:42.439602 16945 sgd_solver.cpp:106] Iteration 162, lr = 0.0025
I0521 07:30:50.244968 16945 solver.cpp:237] Iteration 180, loss = 2.30852
I0521 07:30:50.245002 16945 solver.cpp:253]     Train net output #0: loss = 2.30852 (* 1 = 2.30852 loss)
I0521 07:30:50.245018 16945 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 07:30:51.979192 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_185.caffemodel
I0521 07:30:52.326184 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_185.solverstate
I0521 07:30:58.121762 16945 solver.cpp:237] Iteration 198, loss = 2.30779
I0521 07:30:58.121915 16945 solver.cpp:253]     Train net output #0: loss = 2.30779 (* 1 = 2.30779 loss)
I0521 07:30:58.121929 16945 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 07:31:05.927126 16945 solver.cpp:237] Iteration 216, loss = 2.30387
I0521 07:31:05.927158 16945 solver.cpp:253]     Train net output #0: loss = 2.30387 (* 1 = 2.30387 loss)
I0521 07:31:05.927177 16945 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 07:31:13.735127 16945 solver.cpp:237] Iteration 234, loss = 2.30855
I0521 07:31:13.735160 16945 solver.cpp:253]     Train net output #0: loss = 2.30855 (* 1 = 2.30855 loss)
I0521 07:31:13.735177 16945 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0521 07:31:43.739038 16945 solver.cpp:237] Iteration 252, loss = 2.28739
I0521 07:31:43.739190 16945 solver.cpp:253]     Train net output #0: loss = 2.28739 (* 1 = 2.28739 loss)
I0521 07:31:43.739204 16945 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 07:31:51.548166 16945 solver.cpp:237] Iteration 270, loss = 2.27512
I0521 07:31:51.548197 16945 solver.cpp:253]     Train net output #0: loss = 2.27512 (* 1 = 2.27512 loss)
I0521 07:31:51.548213 16945 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 07:31:59.356386 16945 solver.cpp:237] Iteration 288, loss = 2.26799
I0521 07:31:59.356420 16945 solver.cpp:253]     Train net output #0: loss = 2.26799 (* 1 = 2.26799 loss)
I0521 07:31:59.356437 16945 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 07:32:07.168313 16945 solver.cpp:237] Iteration 306, loss = 2.21718
I0521 07:32:07.168345 16945 solver.cpp:253]     Train net output #0: loss = 2.21718 (* 1 = 2.21718 loss)
I0521 07:32:07.168364 16945 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 07:32:14.977994 16945 solver.cpp:237] Iteration 324, loss = 2.25729
I0521 07:32:14.978152 16945 solver.cpp:253]     Train net output #0: loss = 2.25729 (* 1 = 2.25729 loss)
I0521 07:32:14.978165 16945 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0521 07:32:22.792889 16945 solver.cpp:237] Iteration 342, loss = 2.18793
I0521 07:32:22.792922 16945 solver.cpp:253]     Train net output #0: loss = 2.18793 (* 1 = 2.18793 loss)
I0521 07:32:22.792940 16945 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0521 07:32:30.603579 16945 solver.cpp:237] Iteration 360, loss = 2.1701
I0521 07:32:30.603610 16945 solver.cpp:253]     Train net output #0: loss = 2.1701 (* 1 = 2.1701 loss)
I0521 07:32:30.603628 16945 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 07:32:34.510277 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_370.caffemodel
I0521 07:32:34.852457 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_370.solverstate
I0521 07:32:34.878296 16945 solver.cpp:341] Iteration 370, Testing net (#0)
I0521 07:33:19.947921 16945 solver.cpp:409]     Test net output #0: accuracy = 0.436463
I0521 07:33:19.948081 16945 solver.cpp:409]     Test net output #1: loss = 2.00191 (* 1 = 2.00191 loss)
I0521 07:33:45.694499 16945 solver.cpp:237] Iteration 378, loss = 2.11219
I0521 07:33:45.694548 16945 solver.cpp:253]     Train net output #0: loss = 2.11219 (* 1 = 2.11219 loss)
I0521 07:33:45.694564 16945 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 07:33:53.503304 16945 solver.cpp:237] Iteration 396, loss = 2.14865
I0521 07:33:53.503456 16945 solver.cpp:253]     Train net output #0: loss = 2.14865 (* 1 = 2.14865 loss)
I0521 07:33:53.503470 16945 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 07:34:01.316630 16945 solver.cpp:237] Iteration 414, loss = 2.11714
I0521 07:34:01.316663 16945 solver.cpp:253]     Train net output #0: loss = 2.11714 (* 1 = 2.11714 loss)
I0521 07:34:01.316680 16945 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0521 07:34:09.127166 16945 solver.cpp:237] Iteration 432, loss = 2.08863
I0521 07:34:09.127197 16945 solver.cpp:253]     Train net output #0: loss = 2.08863 (* 1 = 2.08863 loss)
I0521 07:34:09.127214 16945 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 07:34:16.934854 16945 solver.cpp:237] Iteration 450, loss = 2.05569
I0521 07:34:16.934891 16945 solver.cpp:253]     Train net output #0: loss = 2.05569 (* 1 = 2.05569 loss)
I0521 07:34:16.934912 16945 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 07:34:24.739856 16945 solver.cpp:237] Iteration 468, loss = 2.07209
I0521 07:34:24.739985 16945 solver.cpp:253]     Train net output #0: loss = 2.07209 (* 1 = 2.07209 loss)
I0521 07:34:24.739998 16945 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0521 07:34:32.550803 16945 solver.cpp:237] Iteration 486, loss = 2.01532
I0521 07:34:32.550834 16945 solver.cpp:253]     Train net output #0: loss = 2.01532 (* 1 = 2.01532 loss)
I0521 07:34:32.550853 16945 sgd_solver.cpp:106] Iteration 486, lr = 0.0025
I0521 07:35:02.517235 16945 solver.cpp:237] Iteration 504, loss = 2.00049
I0521 07:35:02.517412 16945 solver.cpp:253]     Train net output #0: loss = 2.00049 (* 1 = 2.00049 loss)
I0521 07:35:02.517429 16945 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 07:35:10.325800 16945 solver.cpp:237] Iteration 522, loss = 1.96404
I0521 07:35:10.325845 16945 solver.cpp:253]     Train net output #0: loss = 1.96404 (* 1 = 1.96404 loss)
I0521 07:35:10.325860 16945 sgd_solver.cpp:106] Iteration 522, lr = 0.0025
I0521 07:35:18.135004 16945 solver.cpp:237] Iteration 540, loss = 1.9742
I0521 07:35:18.135036 16945 solver.cpp:253]     Train net output #0: loss = 1.9742 (* 1 = 1.9742 loss)
I0521 07:35:18.135053 16945 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 07:35:24.207020 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_555.caffemodel
I0521 07:35:24.552350 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_555.solverstate
I0521 07:35:26.009007 16945 solver.cpp:237] Iteration 558, loss = 1.90615
I0521 07:35:26.009058 16945 solver.cpp:253]     Train net output #0: loss = 1.90615 (* 1 = 1.90615 loss)
I0521 07:35:26.009071 16945 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0521 07:35:33.815179 16945 solver.cpp:237] Iteration 576, loss = 1.9926
I0521 07:35:33.815330 16945 solver.cpp:253]     Train net output #0: loss = 1.9926 (* 1 = 1.9926 loss)
I0521 07:35:33.815343 16945 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 07:35:41.626713 16945 solver.cpp:237] Iteration 594, loss = 1.90101
I0521 07:35:41.626746 16945 solver.cpp:253]     Train net output #0: loss = 1.90101 (* 1 = 1.90101 loss)
I0521 07:35:41.626763 16945 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 07:35:49.433781 16945 solver.cpp:237] Iteration 612, loss = 1.91436
I0521 07:35:49.433815 16945 solver.cpp:253]     Train net output #0: loss = 1.91436 (* 1 = 1.91436 loss)
I0521 07:35:49.433830 16945 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 07:36:19.431661 16945 solver.cpp:237] Iteration 630, loss = 1.87471
I0521 07:36:19.431823 16945 solver.cpp:253]     Train net output #0: loss = 1.87471 (* 1 = 1.87471 loss)
I0521 07:36:19.431838 16945 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 07:36:27.245481 16945 solver.cpp:237] Iteration 648, loss = 1.89098
I0521 07:36:27.245517 16945 solver.cpp:253]     Train net output #0: loss = 1.89098 (* 1 = 1.89098 loss)
I0521 07:36:27.245538 16945 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 07:36:35.054168 16945 solver.cpp:237] Iteration 666, loss = 1.95806
I0521 07:36:35.054201 16945 solver.cpp:253]     Train net output #0: loss = 1.95806 (* 1 = 1.95806 loss)
I0521 07:36:35.054219 16945 sgd_solver.cpp:106] Iteration 666, lr = 0.0025
I0521 07:36:42.861515 16945 solver.cpp:237] Iteration 684, loss = 1.87374
I0521 07:36:42.861548 16945 solver.cpp:253]     Train net output #0: loss = 1.87374 (* 1 = 1.87374 loss)
I0521 07:36:42.861565 16945 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0521 07:36:50.667403 16945 solver.cpp:237] Iteration 702, loss = 1.86774
I0521 07:36:50.667551 16945 solver.cpp:253]     Train net output #0: loss = 1.86774 (* 1 = 1.86774 loss)
I0521 07:36:50.667567 16945 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 07:36:58.467736 16945 solver.cpp:237] Iteration 720, loss = 1.82101
I0521 07:36:58.467768 16945 solver.cpp:253]     Train net output #0: loss = 1.82101 (* 1 = 1.82101 loss)
I0521 07:36:58.467787 16945 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 07:37:06.273790 16945 solver.cpp:237] Iteration 738, loss = 1.8517
I0521 07:37:06.273823 16945 solver.cpp:253]     Train net output #0: loss = 1.8517 (* 1 = 1.8517 loss)
I0521 07:37:06.273840 16945 sgd_solver.cpp:106] Iteration 738, lr = 0.0025
I0521 07:37:06.708173 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_740.caffemodel
I0521 07:37:07.051982 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_740.solverstate
I0521 07:37:07.078999 16945 solver.cpp:341] Iteration 740, Testing net (#0)
I0521 07:38:13.031546 16945 solver.cpp:409]     Test net output #0: accuracy = 0.605565
I0521 07:38:13.031713 16945 solver.cpp:409]     Test net output #1: loss = 1.40771 (* 1 = 1.40771 loss)
I0521 07:38:42.250658 16945 solver.cpp:237] Iteration 756, loss = 1.90278
I0521 07:38:42.250706 16945 solver.cpp:253]     Train net output #0: loss = 1.90278 (* 1 = 1.90278 loss)
I0521 07:38:42.250725 16945 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 07:38:50.056928 16945 solver.cpp:237] Iteration 774, loss = 1.84011
I0521 07:38:50.057097 16945 solver.cpp:253]     Train net output #0: loss = 1.84011 (* 1 = 1.84011 loss)
I0521 07:38:50.057112 16945 sgd_solver.cpp:106] Iteration 774, lr = 0.0025
I0521 07:38:57.860632 16945 solver.cpp:237] Iteration 792, loss = 1.84137
I0521 07:38:57.860666 16945 solver.cpp:253]     Train net output #0: loss = 1.84137 (* 1 = 1.84137 loss)
I0521 07:38:57.860683 16945 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 07:39:05.661094 16945 solver.cpp:237] Iteration 810, loss = 1.85644
I0521 07:39:05.661128 16945 solver.cpp:253]     Train net output #0: loss = 1.85644 (* 1 = 1.85644 loss)
I0521 07:39:05.661144 16945 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 07:39:13.462119 16945 solver.cpp:237] Iteration 828, loss = 1.80461
I0521 07:39:13.462152 16945 solver.cpp:253]     Train net output #0: loss = 1.80461 (* 1 = 1.80461 loss)
I0521 07:39:13.462169 16945 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0521 07:39:21.265202 16945 solver.cpp:237] Iteration 846, loss = 1.74755
I0521 07:39:21.265357 16945 solver.cpp:253]     Train net output #0: loss = 1.74755 (* 1 = 1.74755 loss)
I0521 07:39:21.265370 16945 sgd_solver.cpp:106] Iteration 846, lr = 0.0025
I0521 07:39:51.295485 16945 solver.cpp:237] Iteration 864, loss = 1.80191
I0521 07:39:51.295650 16945 solver.cpp:253]     Train net output #0: loss = 1.80191 (* 1 = 1.80191 loss)
I0521 07:39:51.295665 16945 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 07:39:59.100571 16945 solver.cpp:237] Iteration 882, loss = 1.81031
I0521 07:39:59.100605 16945 solver.cpp:253]     Train net output #0: loss = 1.81031 (* 1 = 1.81031 loss)
I0521 07:39:59.100620 16945 sgd_solver.cpp:106] Iteration 882, lr = 0.0025
I0521 07:40:06.904196 16945 solver.cpp:237] Iteration 900, loss = 1.7832
I0521 07:40:06.904240 16945 solver.cpp:253]     Train net output #0: loss = 1.7832 (* 1 = 1.7832 loss)
I0521 07:40:06.904255 16945 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 07:40:14.703613 16945 solver.cpp:237] Iteration 918, loss = 1.82917
I0521 07:40:14.703646 16945 solver.cpp:253]     Train net output #0: loss = 1.82917 (* 1 = 1.82917 loss)
I0521 07:40:14.703661 16945 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 07:40:17.302676 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_925.caffemodel
I0521 07:40:17.689129 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_925.solverstate
I0521 07:40:22.613517 16945 solver.cpp:237] Iteration 936, loss = 1.80372
I0521 07:40:22.613682 16945 solver.cpp:253]     Train net output #0: loss = 1.80372 (* 1 = 1.80372 loss)
I0521 07:40:22.613697 16945 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 07:40:30.417433 16945 solver.cpp:237] Iteration 954, loss = 1.82287
I0521 07:40:30.417474 16945 solver.cpp:253]     Train net output #0: loss = 1.82287 (* 1 = 1.82287 loss)
I0521 07:40:30.417495 16945 sgd_solver.cpp:106] Iteration 954, lr = 0.0025
I0521 07:40:38.216680 16945 solver.cpp:237] Iteration 972, loss = 1.77306
I0521 07:40:38.216713 16945 solver.cpp:253]     Train net output #0: loss = 1.77306 (* 1 = 1.77306 loss)
I0521 07:40:38.216729 16945 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0521 07:41:08.255004 16945 solver.cpp:237] Iteration 990, loss = 1.76166
I0521 07:41:08.255168 16945 solver.cpp:253]     Train net output #0: loss = 1.76166 (* 1 = 1.76166 loss)
I0521 07:41:08.255184 16945 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 07:41:16.055516 16945 solver.cpp:237] Iteration 1008, loss = 1.7234
I0521 07:41:16.055548 16945 solver.cpp:253]     Train net output #0: loss = 1.7234 (* 1 = 1.7234 loss)
I0521 07:41:16.055565 16945 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 07:41:23.857695 16945 solver.cpp:237] Iteration 1026, loss = 1.77219
I0521 07:41:23.857741 16945 solver.cpp:253]     Train net output #0: loss = 1.77219 (* 1 = 1.77219 loss)
I0521 07:41:23.857758 16945 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 07:41:31.658567 16945 solver.cpp:237] Iteration 1044, loss = 1.78772
I0521 07:41:31.658601 16945 solver.cpp:253]     Train net output #0: loss = 1.78772 (* 1 = 1.78772 loss)
I0521 07:41:31.658617 16945 sgd_solver.cpp:106] Iteration 1044, lr = 0.0025
I0521 07:41:39.463937 16945 solver.cpp:237] Iteration 1062, loss = 1.77282
I0521 07:41:39.464076 16945 solver.cpp:253]     Train net output #0: loss = 1.77282 (* 1 = 1.77282 loss)
I0521 07:41:39.464089 16945 sgd_solver.cpp:106] Iteration 1062, lr = 0.0025
I0521 07:41:47.270879 16945 solver.cpp:237] Iteration 1080, loss = 1.77531
I0521 07:41:47.270923 16945 solver.cpp:253]     Train net output #0: loss = 1.77531 (* 1 = 1.77531 loss)
I0521 07:41:47.270941 16945 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 07:41:55.070878 16945 solver.cpp:237] Iteration 1098, loss = 1.76195
I0521 07:41:55.070911 16945 solver.cpp:253]     Train net output #0: loss = 1.76195 (* 1 = 1.76195 loss)
I0521 07:41:55.070927 16945 sgd_solver.cpp:106] Iteration 1098, lr = 0.0025
I0521 07:41:59.837585 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1110.caffemodel
I0521 07:42:00.179468 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1110.solverstate
I0521 07:42:00.205809 16945 solver.cpp:341] Iteration 1110, Testing net (#0)
I0521 07:42:44.939941 16945 solver.cpp:409]     Test net output #0: accuracy = 0.608869
I0521 07:42:44.940101 16945 solver.cpp:409]     Test net output #1: loss = 1.34116 (* 1 = 1.34116 loss)
I0521 07:43:09.846673 16945 solver.cpp:237] Iteration 1116, loss = 1.73352
I0521 07:43:09.846724 16945 solver.cpp:253]     Train net output #0: loss = 1.73352 (* 1 = 1.73352 loss)
I0521 07:43:09.846740 16945 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0521 07:43:17.644570 16945 solver.cpp:237] Iteration 1134, loss = 1.82867
I0521 07:43:17.644726 16945 solver.cpp:253]     Train net output #0: loss = 1.82867 (* 1 = 1.82867 loss)
I0521 07:43:17.644740 16945 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 07:43:25.441987 16945 solver.cpp:237] Iteration 1152, loss = 1.70284
I0521 07:43:25.442023 16945 solver.cpp:253]     Train net output #0: loss = 1.70284 (* 1 = 1.70284 loss)
I0521 07:43:25.442042 16945 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 07:43:33.242990 16945 solver.cpp:237] Iteration 1170, loss = 1.75463
I0521 07:43:33.243022 16945 solver.cpp:253]     Train net output #0: loss = 1.75463 (* 1 = 1.75463 loss)
I0521 07:43:33.243039 16945 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 07:43:41.044564 16945 solver.cpp:237] Iteration 1188, loss = 1.70686
I0521 07:43:41.044596 16945 solver.cpp:253]     Train net output #0: loss = 1.70686 (* 1 = 1.70686 loss)
I0521 07:43:41.044613 16945 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 07:43:48.842646 16945 solver.cpp:237] Iteration 1206, loss = 1.72254
I0521 07:43:48.842789 16945 solver.cpp:253]     Train net output #0: loss = 1.72254 (* 1 = 1.72254 loss)
I0521 07:43:48.842803 16945 sgd_solver.cpp:106] Iteration 1206, lr = 0.0025
I0521 07:43:56.641425 16945 solver.cpp:237] Iteration 1224, loss = 1.71974
I0521 07:43:56.641458 16945 solver.cpp:253]     Train net output #0: loss = 1.71974 (* 1 = 1.71974 loss)
I0521 07:43:56.641477 16945 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 07:44:26.619204 16945 solver.cpp:237] Iteration 1242, loss = 1.75092
I0521 07:44:26.619360 16945 solver.cpp:253]     Train net output #0: loss = 1.75092 (* 1 = 1.75092 loss)
I0521 07:44:26.619374 16945 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 07:44:34.419863 16945 solver.cpp:237] Iteration 1260, loss = 1.70599
I0521 07:44:34.419895 16945 solver.cpp:253]     Train net output #0: loss = 1.70599 (* 1 = 1.70599 loss)
I0521 07:44:34.419914 16945 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 07:44:42.215644 16945 solver.cpp:237] Iteration 1278, loss = 1.6608
I0521 07:44:42.215683 16945 solver.cpp:253]     Train net output #0: loss = 1.6608 (* 1 = 1.6608 loss)
I0521 07:44:42.215704 16945 sgd_solver.cpp:106] Iteration 1278, lr = 0.0025
I0521 07:44:49.142540 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1295.caffemodel
I0521 07:44:49.490425 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1295.solverstate
I0521 07:44:50.079485 16945 solver.cpp:237] Iteration 1296, loss = 1.70115
I0521 07:44:50.079530 16945 solver.cpp:253]     Train net output #0: loss = 1.70115 (* 1 = 1.70115 loss)
I0521 07:44:50.079546 16945 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 07:44:57.879083 16945 solver.cpp:237] Iteration 1314, loss = 1.70487
I0521 07:44:57.879228 16945 solver.cpp:253]     Train net output #0: loss = 1.70487 (* 1 = 1.70487 loss)
I0521 07:44:57.879241 16945 sgd_solver.cpp:106] Iteration 1314, lr = 0.0025
I0521 07:45:05.677172 16945 solver.cpp:237] Iteration 1332, loss = 1.69657
I0521 07:45:05.677211 16945 solver.cpp:253]     Train net output #0: loss = 1.69657 (* 1 = 1.69657 loss)
I0521 07:45:05.677228 16945 sgd_solver.cpp:106] Iteration 1332, lr = 0.0025
I0521 07:45:13.478871 16945 solver.cpp:237] Iteration 1350, loss = 1.73306
I0521 07:45:13.478904 16945 solver.cpp:253]     Train net output #0: loss = 1.73306 (* 1 = 1.73306 loss)
I0521 07:45:13.478922 16945 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 07:45:43.481897 16945 solver.cpp:237] Iteration 1368, loss = 1.72635
I0521 07:45:43.482065 16945 solver.cpp:253]     Train net output #0: loss = 1.72635 (* 1 = 1.72635 loss)
I0521 07:45:43.482079 16945 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 07:45:51.283082 16945 solver.cpp:237] Iteration 1386, loss = 1.70746
I0521 07:45:51.283114 16945 solver.cpp:253]     Train net output #0: loss = 1.70746 (* 1 = 1.70746 loss)
I0521 07:45:51.283133 16945 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 07:45:59.083387 16945 solver.cpp:237] Iteration 1404, loss = 1.66908
I0521 07:45:59.083417 16945 solver.cpp:253]     Train net output #0: loss = 1.66908 (* 1 = 1.66908 loss)
I0521 07:45:59.083434 16945 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 07:46:06.883918 16945 solver.cpp:237] Iteration 1422, loss = 1.69152
I0521 07:46:06.883949 16945 solver.cpp:253]     Train net output #0: loss = 1.69152 (* 1 = 1.69152 loss)
I0521 07:46:06.883966 16945 sgd_solver.cpp:106] Iteration 1422, lr = 0.0025
I0521 07:46:14.682451 16945 solver.cpp:237] Iteration 1440, loss = 1.64377
I0521 07:46:14.682593 16945 solver.cpp:253]     Train net output #0: loss = 1.64377 (* 1 = 1.64377 loss)
I0521 07:46:14.682606 16945 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 07:46:22.480736 16945 solver.cpp:237] Iteration 1458, loss = 1.71229
I0521 07:46:22.480767 16945 solver.cpp:253]     Train net output #0: loss = 1.71229 (* 1 = 1.71229 loss)
I0521 07:46:22.480783 16945 sgd_solver.cpp:106] Iteration 1458, lr = 0.0025
I0521 07:46:30.280310 16945 solver.cpp:237] Iteration 1476, loss = 1.63601
I0521 07:46:30.280344 16945 solver.cpp:253]     Train net output #0: loss = 1.63601 (* 1 = 1.63601 loss)
I0521 07:46:30.280360 16945 sgd_solver.cpp:106] Iteration 1476, lr = 0.0025
I0521 07:46:31.579390 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1480.caffemodel
I0521 07:46:31.921792 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1480.solverstate
I0521 07:46:31.947934 16945 solver.cpp:341] Iteration 1480, Testing net (#0)
I0521 07:47:37.929528 16945 solver.cpp:409]     Test net output #0: accuracy = 0.670711
I0521 07:47:37.929700 16945 solver.cpp:409]     Test net output #1: loss = 1.13687 (* 1 = 1.13687 loss)
I0521 07:48:06.324813 16945 solver.cpp:237] Iteration 1494, loss = 1.74034
I0521 07:48:06.324862 16945 solver.cpp:253]     Train net output #0: loss = 1.74034 (* 1 = 1.74034 loss)
I0521 07:48:06.324879 16945 sgd_solver.cpp:106] Iteration 1494, lr = 0.0025
I0521 07:48:14.126128 16945 solver.cpp:237] Iteration 1512, loss = 1.66422
I0521 07:48:14.126279 16945 solver.cpp:253]     Train net output #0: loss = 1.66422 (* 1 = 1.66422 loss)
I0521 07:48:14.126292 16945 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 07:48:21.929077 16945 solver.cpp:237] Iteration 1530, loss = 1.65959
I0521 07:48:21.929110 16945 solver.cpp:253]     Train net output #0: loss = 1.65959 (* 1 = 1.65959 loss)
I0521 07:48:21.929126 16945 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 07:48:29.732616 16945 solver.cpp:237] Iteration 1548, loss = 1.63536
I0521 07:48:29.732647 16945 solver.cpp:253]     Train net output #0: loss = 1.63536 (* 1 = 1.63536 loss)
I0521 07:48:29.732669 16945 sgd_solver.cpp:106] Iteration 1548, lr = 0.0025
I0521 07:48:37.536830 16945 solver.cpp:237] Iteration 1566, loss = 1.72009
I0521 07:48:37.536864 16945 solver.cpp:253]     Train net output #0: loss = 1.72009 (* 1 = 1.72009 loss)
I0521 07:48:37.536880 16945 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 07:48:45.339824 16945 solver.cpp:237] Iteration 1584, loss = 1.69129
I0521 07:48:45.339963 16945 solver.cpp:253]     Train net output #0: loss = 1.69129 (* 1 = 1.69129 loss)
I0521 07:48:45.339977 16945 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 07:48:53.139708 16945 solver.cpp:237] Iteration 1602, loss = 1.63152
I0521 07:48:53.139753 16945 solver.cpp:253]     Train net output #0: loss = 1.63152 (* 1 = 1.63152 loss)
I0521 07:48:53.139767 16945 sgd_solver.cpp:106] Iteration 1602, lr = 0.0025
I0521 07:49:23.167909 16945 solver.cpp:237] Iteration 1620, loss = 1.72535
I0521 07:49:23.168077 16945 solver.cpp:253]     Train net output #0: loss = 1.72535 (* 1 = 1.72535 loss)
I0521 07:49:23.168092 16945 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 07:49:30.964457 16945 solver.cpp:237] Iteration 1638, loss = 1.63586
I0521 07:49:30.964490 16945 solver.cpp:253]     Train net output #0: loss = 1.63586 (* 1 = 1.63586 loss)
I0521 07:49:30.964507 16945 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 07:49:38.766865 16945 solver.cpp:237] Iteration 1656, loss = 1.66496
I0521 07:49:38.766898 16945 solver.cpp:253]     Train net output #0: loss = 1.66496 (* 1 = 1.66496 loss)
I0521 07:49:38.766913 16945 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 07:49:42.235374 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1665.caffemodel
I0521 07:49:42.581480 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1665.solverstate
I0521 07:49:46.639282 16945 solver.cpp:237] Iteration 1674, loss = 1.70355
I0521 07:49:46.639329 16945 solver.cpp:253]     Train net output #0: loss = 1.70355 (* 1 = 1.70355 loss)
I0521 07:49:46.639346 16945 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 07:49:54.443536 16945 solver.cpp:237] Iteration 1692, loss = 1.62806
I0521 07:49:54.443693 16945 solver.cpp:253]     Train net output #0: loss = 1.62806 (* 1 = 1.62806 loss)
I0521 07:49:54.443707 16945 sgd_solver.cpp:106] Iteration 1692, lr = 0.0025
I0521 07:50:02.243253 16945 solver.cpp:237] Iteration 1710, loss = 1.67925
I0521 07:50:02.243285 16945 solver.cpp:253]     Train net output #0: loss = 1.67925 (* 1 = 1.67925 loss)
I0521 07:50:02.243302 16945 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 07:50:32.307194 16945 solver.cpp:237] Iteration 1728, loss = 1.68059
I0521 07:50:32.307371 16945 solver.cpp:253]     Train net output #0: loss = 1.68059 (* 1 = 1.68059 loss)
I0521 07:50:32.307389 16945 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 07:50:40.109722 16945 solver.cpp:237] Iteration 1746, loss = 1.73572
I0521 07:50:40.109755 16945 solver.cpp:253]     Train net output #0: loss = 1.73572 (* 1 = 1.73572 loss)
I0521 07:50:40.109773 16945 sgd_solver.cpp:106] Iteration 1746, lr = 0.0025
I0521 07:50:47.911815 16945 solver.cpp:237] Iteration 1764, loss = 1.68373
I0521 07:50:47.911849 16945 solver.cpp:253]     Train net output #0: loss = 1.68373 (* 1 = 1.68373 loss)
I0521 07:50:47.911861 16945 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 07:50:55.714231 16945 solver.cpp:237] Iteration 1782, loss = 1.57257
I0521 07:50:55.714277 16945 solver.cpp:253]     Train net output #0: loss = 1.57257 (* 1 = 1.57257 loss)
I0521 07:50:55.714294 16945 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 07:51:03.511710 16945 solver.cpp:237] Iteration 1800, loss = 1.76794
I0521 07:51:03.511853 16945 solver.cpp:253]     Train net output #0: loss = 1.76794 (* 1 = 1.76794 loss)
I0521 07:51:03.511867 16945 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 07:51:11.316848 16945 solver.cpp:237] Iteration 1818, loss = 1.7016
I0521 07:51:11.316880 16945 solver.cpp:253]     Train net output #0: loss = 1.7016 (* 1 = 1.7016 loss)
I0521 07:51:11.316898 16945 sgd_solver.cpp:106] Iteration 1818, lr = 0.0025
I0521 07:51:19.119828 16945 solver.cpp:237] Iteration 1836, loss = 1.78103
I0521 07:51:19.119861 16945 solver.cpp:253]     Train net output #0: loss = 1.78103 (* 1 = 1.78103 loss)
I0521 07:51:19.119877 16945 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0521 07:51:24.755689 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1850.caffemodel
I0521 07:51:25.100832 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1850.solverstate
I0521 07:51:25.128721 16945 solver.cpp:341] Iteration 1850, Testing net (#0)
I0521 07:52:10.205224 16945 solver.cpp:409]     Test net output #0: accuracy = 0.68333
I0521 07:52:10.205400 16945 solver.cpp:409]     Test net output #1: loss = 1.10693 (* 1 = 1.10693 loss)
I0521 07:52:10.334694 16945 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1851.caffemodel
I0521 07:52:10.678056 16945 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442_iter_1851.solverstate
I0521 07:52:10.706025 16945 solver.cpp:326] Optimization Done.
I0521 07:52:10.706053 16945 caffe.cpp:215] Optimization Done.
Application 11237239 resources: utime ~1247s, stime ~226s, Rss ~5330840, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_810_2016-05-20T11.21.02.216442.solver"
	User time (seconds): 0.54
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:36.46
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15080
	Voluntary context switches: 2711
	Involuntary context switches: 115
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
