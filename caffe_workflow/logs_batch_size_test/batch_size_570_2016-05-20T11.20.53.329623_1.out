2806189
I0521 02:32:59.356524  8640 caffe.cpp:184] Using GPUs 0
I0521 02:32:59.783638  8640 solver.cpp:48] Initializing solver from parameters: 
test_iter: 263
test_interval: 526
base_lr: 0.0025
display: 26
max_iter: 2631
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 263
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623.prototxt"
I0521 02:32:59.785413  8640 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623.prototxt
I0521 02:32:59.807176  8640 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 02:32:59.807242  8640 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 02:32:59.807586  8640 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 570
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:32:59.807767  8640 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:32:59.807790  8640 net.cpp:106] Creating Layer data_hdf5
I0521 02:32:59.807806  8640 net.cpp:411] data_hdf5 -> data
I0521 02:32:59.807838  8640 net.cpp:411] data_hdf5 -> label
I0521 02:32:59.807871  8640 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 02:32:59.819653  8640 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 02:32:59.841980  8640 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 02:33:21.344358  8640 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 02:33:21.349444  8640 net.cpp:150] Setting up data_hdf5
I0521 02:33:21.349488  8640 net.cpp:157] Top shape: 570 1 127 50 (3619500)
I0521 02:33:21.349503  8640 net.cpp:157] Top shape: 570 (570)
I0521 02:33:21.349514  8640 net.cpp:165] Memory required for data: 14480280
I0521 02:33:21.349527  8640 layer_factory.hpp:77] Creating layer conv1
I0521 02:33:21.349561  8640 net.cpp:106] Creating Layer conv1
I0521 02:33:21.349573  8640 net.cpp:454] conv1 <- data
I0521 02:33:21.349596  8640 net.cpp:411] conv1 -> conv1
I0521 02:33:22.243088  8640 net.cpp:150] Setting up conv1
I0521 02:33:22.243131  8640 net.cpp:157] Top shape: 570 12 120 48 (39398400)
I0521 02:33:22.243144  8640 net.cpp:165] Memory required for data: 172073880
I0521 02:33:22.243175  8640 layer_factory.hpp:77] Creating layer relu1
I0521 02:33:22.243204  8640 net.cpp:106] Creating Layer relu1
I0521 02:33:22.243216  8640 net.cpp:454] relu1 <- conv1
I0521 02:33:22.243228  8640 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:33:22.243744  8640 net.cpp:150] Setting up relu1
I0521 02:33:22.243762  8640 net.cpp:157] Top shape: 570 12 120 48 (39398400)
I0521 02:33:22.243772  8640 net.cpp:165] Memory required for data: 329667480
I0521 02:33:22.243782  8640 layer_factory.hpp:77] Creating layer pool1
I0521 02:33:22.243799  8640 net.cpp:106] Creating Layer pool1
I0521 02:33:22.243809  8640 net.cpp:454] pool1 <- conv1
I0521 02:33:22.243823  8640 net.cpp:411] pool1 -> pool1
I0521 02:33:22.243902  8640 net.cpp:150] Setting up pool1
I0521 02:33:22.243916  8640 net.cpp:157] Top shape: 570 12 60 48 (19699200)
I0521 02:33:22.243927  8640 net.cpp:165] Memory required for data: 408464280
I0521 02:33:22.243937  8640 layer_factory.hpp:77] Creating layer conv2
I0521 02:33:22.243958  8640 net.cpp:106] Creating Layer conv2
I0521 02:33:22.243968  8640 net.cpp:454] conv2 <- pool1
I0521 02:33:22.243981  8640 net.cpp:411] conv2 -> conv2
I0521 02:33:22.246634  8640 net.cpp:150] Setting up conv2
I0521 02:33:22.246662  8640 net.cpp:157] Top shape: 570 20 54 46 (28317600)
I0521 02:33:22.246672  8640 net.cpp:165] Memory required for data: 521734680
I0521 02:33:22.246691  8640 layer_factory.hpp:77] Creating layer relu2
I0521 02:33:22.246706  8640 net.cpp:106] Creating Layer relu2
I0521 02:33:22.246716  8640 net.cpp:454] relu2 <- conv2
I0521 02:33:22.246728  8640 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:33:22.247058  8640 net.cpp:150] Setting up relu2
I0521 02:33:22.247072  8640 net.cpp:157] Top shape: 570 20 54 46 (28317600)
I0521 02:33:22.247083  8640 net.cpp:165] Memory required for data: 635005080
I0521 02:33:22.247093  8640 layer_factory.hpp:77] Creating layer pool2
I0521 02:33:22.247107  8640 net.cpp:106] Creating Layer pool2
I0521 02:33:22.247117  8640 net.cpp:454] pool2 <- conv2
I0521 02:33:22.247141  8640 net.cpp:411] pool2 -> pool2
I0521 02:33:22.247221  8640 net.cpp:150] Setting up pool2
I0521 02:33:22.247236  8640 net.cpp:157] Top shape: 570 20 27 46 (14158800)
I0521 02:33:22.247246  8640 net.cpp:165] Memory required for data: 691640280
I0521 02:33:22.247256  8640 layer_factory.hpp:77] Creating layer conv3
I0521 02:33:22.247273  8640 net.cpp:106] Creating Layer conv3
I0521 02:33:22.247283  8640 net.cpp:454] conv3 <- pool2
I0521 02:33:22.247298  8640 net.cpp:411] conv3 -> conv3
I0521 02:33:22.249209  8640 net.cpp:150] Setting up conv3
I0521 02:33:22.249228  8640 net.cpp:157] Top shape: 570 28 22 44 (15449280)
I0521 02:33:22.249239  8640 net.cpp:165] Memory required for data: 753437400
I0521 02:33:22.249258  8640 layer_factory.hpp:77] Creating layer relu3
I0521 02:33:22.249274  8640 net.cpp:106] Creating Layer relu3
I0521 02:33:22.249282  8640 net.cpp:454] relu3 <- conv3
I0521 02:33:22.249295  8640 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:33:22.249761  8640 net.cpp:150] Setting up relu3
I0521 02:33:22.249779  8640 net.cpp:157] Top shape: 570 28 22 44 (15449280)
I0521 02:33:22.249789  8640 net.cpp:165] Memory required for data: 815234520
I0521 02:33:22.249799  8640 layer_factory.hpp:77] Creating layer pool3
I0521 02:33:22.249812  8640 net.cpp:106] Creating Layer pool3
I0521 02:33:22.249822  8640 net.cpp:454] pool3 <- conv3
I0521 02:33:22.249835  8640 net.cpp:411] pool3 -> pool3
I0521 02:33:22.249903  8640 net.cpp:150] Setting up pool3
I0521 02:33:22.249917  8640 net.cpp:157] Top shape: 570 28 11 44 (7724640)
I0521 02:33:22.249927  8640 net.cpp:165] Memory required for data: 846133080
I0521 02:33:22.249935  8640 layer_factory.hpp:77] Creating layer conv4
I0521 02:33:22.249953  8640 net.cpp:106] Creating Layer conv4
I0521 02:33:22.249964  8640 net.cpp:454] conv4 <- pool3
I0521 02:33:22.249977  8640 net.cpp:411] conv4 -> conv4
I0521 02:33:22.252743  8640 net.cpp:150] Setting up conv4
I0521 02:33:22.252771  8640 net.cpp:157] Top shape: 570 36 6 42 (5171040)
I0521 02:33:22.252782  8640 net.cpp:165] Memory required for data: 866817240
I0521 02:33:22.252799  8640 layer_factory.hpp:77] Creating layer relu4
I0521 02:33:22.252812  8640 net.cpp:106] Creating Layer relu4
I0521 02:33:22.252822  8640 net.cpp:454] relu4 <- conv4
I0521 02:33:22.252835  8640 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:33:22.253309  8640 net.cpp:150] Setting up relu4
I0521 02:33:22.253324  8640 net.cpp:157] Top shape: 570 36 6 42 (5171040)
I0521 02:33:22.253335  8640 net.cpp:165] Memory required for data: 887501400
I0521 02:33:22.253345  8640 layer_factory.hpp:77] Creating layer pool4
I0521 02:33:22.253358  8640 net.cpp:106] Creating Layer pool4
I0521 02:33:22.253368  8640 net.cpp:454] pool4 <- conv4
I0521 02:33:22.253381  8640 net.cpp:411] pool4 -> pool4
I0521 02:33:22.253449  8640 net.cpp:150] Setting up pool4
I0521 02:33:22.253463  8640 net.cpp:157] Top shape: 570 36 3 42 (2585520)
I0521 02:33:22.253473  8640 net.cpp:165] Memory required for data: 897843480
I0521 02:33:22.253484  8640 layer_factory.hpp:77] Creating layer ip1
I0521 02:33:22.253504  8640 net.cpp:106] Creating Layer ip1
I0521 02:33:22.253514  8640 net.cpp:454] ip1 <- pool4
I0521 02:33:22.253525  8640 net.cpp:411] ip1 -> ip1
I0521 02:33:22.268923  8640 net.cpp:150] Setting up ip1
I0521 02:33:22.268952  8640 net.cpp:157] Top shape: 570 196 (111720)
I0521 02:33:22.268965  8640 net.cpp:165] Memory required for data: 898290360
I0521 02:33:22.268992  8640 layer_factory.hpp:77] Creating layer relu5
I0521 02:33:22.269006  8640 net.cpp:106] Creating Layer relu5
I0521 02:33:22.269017  8640 net.cpp:454] relu5 <- ip1
I0521 02:33:22.269032  8640 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:33:22.269377  8640 net.cpp:150] Setting up relu5
I0521 02:33:22.269392  8640 net.cpp:157] Top shape: 570 196 (111720)
I0521 02:33:22.269402  8640 net.cpp:165] Memory required for data: 898737240
I0521 02:33:22.269412  8640 layer_factory.hpp:77] Creating layer drop1
I0521 02:33:22.269434  8640 net.cpp:106] Creating Layer drop1
I0521 02:33:22.269444  8640 net.cpp:454] drop1 <- ip1
I0521 02:33:22.269470  8640 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:33:22.269516  8640 net.cpp:150] Setting up drop1
I0521 02:33:22.269529  8640 net.cpp:157] Top shape: 570 196 (111720)
I0521 02:33:22.269541  8640 net.cpp:165] Memory required for data: 899184120
I0521 02:33:22.269551  8640 layer_factory.hpp:77] Creating layer ip2
I0521 02:33:22.269568  8640 net.cpp:106] Creating Layer ip2
I0521 02:33:22.269578  8640 net.cpp:454] ip2 <- ip1
I0521 02:33:22.269593  8640 net.cpp:411] ip2 -> ip2
I0521 02:33:22.270056  8640 net.cpp:150] Setting up ip2
I0521 02:33:22.270069  8640 net.cpp:157] Top shape: 570 98 (55860)
I0521 02:33:22.270079  8640 net.cpp:165] Memory required for data: 899407560
I0521 02:33:22.270094  8640 layer_factory.hpp:77] Creating layer relu6
I0521 02:33:22.270107  8640 net.cpp:106] Creating Layer relu6
I0521 02:33:22.270117  8640 net.cpp:454] relu6 <- ip2
I0521 02:33:22.270128  8640 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:33:22.270648  8640 net.cpp:150] Setting up relu6
I0521 02:33:22.270665  8640 net.cpp:157] Top shape: 570 98 (55860)
I0521 02:33:22.270675  8640 net.cpp:165] Memory required for data: 899631000
I0521 02:33:22.270685  8640 layer_factory.hpp:77] Creating layer drop2
I0521 02:33:22.270699  8640 net.cpp:106] Creating Layer drop2
I0521 02:33:22.270709  8640 net.cpp:454] drop2 <- ip2
I0521 02:33:22.270721  8640 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:33:22.270763  8640 net.cpp:150] Setting up drop2
I0521 02:33:22.270776  8640 net.cpp:157] Top shape: 570 98 (55860)
I0521 02:33:22.270787  8640 net.cpp:165] Memory required for data: 899854440
I0521 02:33:22.270797  8640 layer_factory.hpp:77] Creating layer ip3
I0521 02:33:22.270809  8640 net.cpp:106] Creating Layer ip3
I0521 02:33:22.270819  8640 net.cpp:454] ip3 <- ip2
I0521 02:33:22.270833  8640 net.cpp:411] ip3 -> ip3
I0521 02:33:22.271044  8640 net.cpp:150] Setting up ip3
I0521 02:33:22.271056  8640 net.cpp:157] Top shape: 570 11 (6270)
I0521 02:33:22.271066  8640 net.cpp:165] Memory required for data: 899879520
I0521 02:33:22.271081  8640 layer_factory.hpp:77] Creating layer drop3
I0521 02:33:22.271093  8640 net.cpp:106] Creating Layer drop3
I0521 02:33:22.271103  8640 net.cpp:454] drop3 <- ip3
I0521 02:33:22.271114  8640 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:33:22.271154  8640 net.cpp:150] Setting up drop3
I0521 02:33:22.271167  8640 net.cpp:157] Top shape: 570 11 (6270)
I0521 02:33:22.271178  8640 net.cpp:165] Memory required for data: 899904600
I0521 02:33:22.271194  8640 layer_factory.hpp:77] Creating layer loss
I0521 02:33:22.271214  8640 net.cpp:106] Creating Layer loss
I0521 02:33:22.271224  8640 net.cpp:454] loss <- ip3
I0521 02:33:22.271236  8640 net.cpp:454] loss <- label
I0521 02:33:22.271248  8640 net.cpp:411] loss -> loss
I0521 02:33:22.271265  8640 layer_factory.hpp:77] Creating layer loss
I0521 02:33:22.271919  8640 net.cpp:150] Setting up loss
I0521 02:33:22.271939  8640 net.cpp:157] Top shape: (1)
I0521 02:33:22.271952  8640 net.cpp:160]     with loss weight 1
I0521 02:33:22.271994  8640 net.cpp:165] Memory required for data: 899904604
I0521 02:33:22.272006  8640 net.cpp:226] loss needs backward computation.
I0521 02:33:22.272017  8640 net.cpp:226] drop3 needs backward computation.
I0521 02:33:22.272024  8640 net.cpp:226] ip3 needs backward computation.
I0521 02:33:22.272035  8640 net.cpp:226] drop2 needs backward computation.
I0521 02:33:22.272045  8640 net.cpp:226] relu6 needs backward computation.
I0521 02:33:22.272054  8640 net.cpp:226] ip2 needs backward computation.
I0521 02:33:22.272064  8640 net.cpp:226] drop1 needs backward computation.
I0521 02:33:22.272074  8640 net.cpp:226] relu5 needs backward computation.
I0521 02:33:22.272084  8640 net.cpp:226] ip1 needs backward computation.
I0521 02:33:22.272094  8640 net.cpp:226] pool4 needs backward computation.
I0521 02:33:22.272104  8640 net.cpp:226] relu4 needs backward computation.
I0521 02:33:22.272114  8640 net.cpp:226] conv4 needs backward computation.
I0521 02:33:22.272125  8640 net.cpp:226] pool3 needs backward computation.
I0521 02:33:22.272143  8640 net.cpp:226] relu3 needs backward computation.
I0521 02:33:22.272153  8640 net.cpp:226] conv3 needs backward computation.
I0521 02:33:22.272166  8640 net.cpp:226] pool2 needs backward computation.
I0521 02:33:22.272176  8640 net.cpp:226] relu2 needs backward computation.
I0521 02:33:22.272186  8640 net.cpp:226] conv2 needs backward computation.
I0521 02:33:22.272197  8640 net.cpp:226] pool1 needs backward computation.
I0521 02:33:22.272207  8640 net.cpp:226] relu1 needs backward computation.
I0521 02:33:22.272217  8640 net.cpp:226] conv1 needs backward computation.
I0521 02:33:22.272228  8640 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:33:22.272238  8640 net.cpp:270] This network produces output loss
I0521 02:33:22.272263  8640 net.cpp:283] Network initialization done.
I0521 02:33:22.273913  8640 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623.prototxt
I0521 02:33:22.273984  8640 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 02:33:22.274338  8640 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 570
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:33:22.274525  8640 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:33:22.274540  8640 net.cpp:106] Creating Layer data_hdf5
I0521 02:33:22.274554  8640 net.cpp:411] data_hdf5 -> data
I0521 02:33:22.274570  8640 net.cpp:411] data_hdf5 -> label
I0521 02:33:22.274586  8640 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 02:33:22.275789  8640 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 02:33:43.546432  8640 net.cpp:150] Setting up data_hdf5
I0521 02:33:43.546593  8640 net.cpp:157] Top shape: 570 1 127 50 (3619500)
I0521 02:33:43.546608  8640 net.cpp:157] Top shape: 570 (570)
I0521 02:33:43.546619  8640 net.cpp:165] Memory required for data: 14480280
I0521 02:33:43.546633  8640 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 02:33:43.546661  8640 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 02:33:43.546672  8640 net.cpp:454] label_data_hdf5_1_split <- label
I0521 02:33:43.546687  8640 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 02:33:43.546708  8640 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 02:33:43.546782  8640 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 02:33:43.546795  8640 net.cpp:157] Top shape: 570 (570)
I0521 02:33:43.546808  8640 net.cpp:157] Top shape: 570 (570)
I0521 02:33:43.546816  8640 net.cpp:165] Memory required for data: 14484840
I0521 02:33:43.546828  8640 layer_factory.hpp:77] Creating layer conv1
I0521 02:33:43.546849  8640 net.cpp:106] Creating Layer conv1
I0521 02:33:43.546859  8640 net.cpp:454] conv1 <- data
I0521 02:33:43.546875  8640 net.cpp:411] conv1 -> conv1
I0521 02:33:43.548826  8640 net.cpp:150] Setting up conv1
I0521 02:33:43.548849  8640 net.cpp:157] Top shape: 570 12 120 48 (39398400)
I0521 02:33:43.548861  8640 net.cpp:165] Memory required for data: 172078440
I0521 02:33:43.548882  8640 layer_factory.hpp:77] Creating layer relu1
I0521 02:33:43.548897  8640 net.cpp:106] Creating Layer relu1
I0521 02:33:43.548907  8640 net.cpp:454] relu1 <- conv1
I0521 02:33:43.548919  8640 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:33:43.549420  8640 net.cpp:150] Setting up relu1
I0521 02:33:43.549437  8640 net.cpp:157] Top shape: 570 12 120 48 (39398400)
I0521 02:33:43.549446  8640 net.cpp:165] Memory required for data: 329672040
I0521 02:33:43.549456  8640 layer_factory.hpp:77] Creating layer pool1
I0521 02:33:43.549474  8640 net.cpp:106] Creating Layer pool1
I0521 02:33:43.549482  8640 net.cpp:454] pool1 <- conv1
I0521 02:33:43.549496  8640 net.cpp:411] pool1 -> pool1
I0521 02:33:43.549571  8640 net.cpp:150] Setting up pool1
I0521 02:33:43.549583  8640 net.cpp:157] Top shape: 570 12 60 48 (19699200)
I0521 02:33:43.549593  8640 net.cpp:165] Memory required for data: 408468840
I0521 02:33:43.549604  8640 layer_factory.hpp:77] Creating layer conv2
I0521 02:33:43.549621  8640 net.cpp:106] Creating Layer conv2
I0521 02:33:43.549633  8640 net.cpp:454] conv2 <- pool1
I0521 02:33:43.549646  8640 net.cpp:411] conv2 -> conv2
I0521 02:33:43.551570  8640 net.cpp:150] Setting up conv2
I0521 02:33:43.551592  8640 net.cpp:157] Top shape: 570 20 54 46 (28317600)
I0521 02:33:43.551606  8640 net.cpp:165] Memory required for data: 521739240
I0521 02:33:43.551623  8640 layer_factory.hpp:77] Creating layer relu2
I0521 02:33:43.551636  8640 net.cpp:106] Creating Layer relu2
I0521 02:33:43.551646  8640 net.cpp:454] relu2 <- conv2
I0521 02:33:43.551659  8640 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:33:43.551993  8640 net.cpp:150] Setting up relu2
I0521 02:33:43.552007  8640 net.cpp:157] Top shape: 570 20 54 46 (28317600)
I0521 02:33:43.552017  8640 net.cpp:165] Memory required for data: 635009640
I0521 02:33:43.552027  8640 layer_factory.hpp:77] Creating layer pool2
I0521 02:33:43.552040  8640 net.cpp:106] Creating Layer pool2
I0521 02:33:43.552050  8640 net.cpp:454] pool2 <- conv2
I0521 02:33:43.552063  8640 net.cpp:411] pool2 -> pool2
I0521 02:33:43.552134  8640 net.cpp:150] Setting up pool2
I0521 02:33:43.552147  8640 net.cpp:157] Top shape: 570 20 27 46 (14158800)
I0521 02:33:43.552157  8640 net.cpp:165] Memory required for data: 691644840
I0521 02:33:43.552166  8640 layer_factory.hpp:77] Creating layer conv3
I0521 02:33:43.552184  8640 net.cpp:106] Creating Layer conv3
I0521 02:33:43.552196  8640 net.cpp:454] conv3 <- pool2
I0521 02:33:43.552209  8640 net.cpp:411] conv3 -> conv3
I0521 02:33:43.554174  8640 net.cpp:150] Setting up conv3
I0521 02:33:43.554198  8640 net.cpp:157] Top shape: 570 28 22 44 (15449280)
I0521 02:33:43.554209  8640 net.cpp:165] Memory required for data: 753441960
I0521 02:33:43.554242  8640 layer_factory.hpp:77] Creating layer relu3
I0521 02:33:43.554255  8640 net.cpp:106] Creating Layer relu3
I0521 02:33:43.554266  8640 net.cpp:454] relu3 <- conv3
I0521 02:33:43.554280  8640 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:33:43.554749  8640 net.cpp:150] Setting up relu3
I0521 02:33:43.554765  8640 net.cpp:157] Top shape: 570 28 22 44 (15449280)
I0521 02:33:43.554776  8640 net.cpp:165] Memory required for data: 815239080
I0521 02:33:43.554786  8640 layer_factory.hpp:77] Creating layer pool3
I0521 02:33:43.554800  8640 net.cpp:106] Creating Layer pool3
I0521 02:33:43.554810  8640 net.cpp:454] pool3 <- conv3
I0521 02:33:43.554822  8640 net.cpp:411] pool3 -> pool3
I0521 02:33:43.554894  8640 net.cpp:150] Setting up pool3
I0521 02:33:43.554908  8640 net.cpp:157] Top shape: 570 28 11 44 (7724640)
I0521 02:33:43.554918  8640 net.cpp:165] Memory required for data: 846137640
I0521 02:33:43.554927  8640 layer_factory.hpp:77] Creating layer conv4
I0521 02:33:43.554944  8640 net.cpp:106] Creating Layer conv4
I0521 02:33:43.554955  8640 net.cpp:454] conv4 <- pool3
I0521 02:33:43.554968  8640 net.cpp:411] conv4 -> conv4
I0521 02:33:43.557037  8640 net.cpp:150] Setting up conv4
I0521 02:33:43.557060  8640 net.cpp:157] Top shape: 570 36 6 42 (5171040)
I0521 02:33:43.557070  8640 net.cpp:165] Memory required for data: 866821800
I0521 02:33:43.557086  8640 layer_factory.hpp:77] Creating layer relu4
I0521 02:33:43.557101  8640 net.cpp:106] Creating Layer relu4
I0521 02:33:43.557111  8640 net.cpp:454] relu4 <- conv4
I0521 02:33:43.557123  8640 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:33:43.557591  8640 net.cpp:150] Setting up relu4
I0521 02:33:43.557607  8640 net.cpp:157] Top shape: 570 36 6 42 (5171040)
I0521 02:33:43.557617  8640 net.cpp:165] Memory required for data: 887505960
I0521 02:33:43.557627  8640 layer_factory.hpp:77] Creating layer pool4
I0521 02:33:43.557641  8640 net.cpp:106] Creating Layer pool4
I0521 02:33:43.557651  8640 net.cpp:454] pool4 <- conv4
I0521 02:33:43.557663  8640 net.cpp:411] pool4 -> pool4
I0521 02:33:43.557736  8640 net.cpp:150] Setting up pool4
I0521 02:33:43.557749  8640 net.cpp:157] Top shape: 570 36 3 42 (2585520)
I0521 02:33:43.557760  8640 net.cpp:165] Memory required for data: 897848040
I0521 02:33:43.557767  8640 layer_factory.hpp:77] Creating layer ip1
I0521 02:33:43.557782  8640 net.cpp:106] Creating Layer ip1
I0521 02:33:43.557793  8640 net.cpp:454] ip1 <- pool4
I0521 02:33:43.557806  8640 net.cpp:411] ip1 -> ip1
I0521 02:33:43.573207  8640 net.cpp:150] Setting up ip1
I0521 02:33:43.573230  8640 net.cpp:157] Top shape: 570 196 (111720)
I0521 02:33:43.573241  8640 net.cpp:165] Memory required for data: 898294920
I0521 02:33:43.573263  8640 layer_factory.hpp:77] Creating layer relu5
I0521 02:33:43.573278  8640 net.cpp:106] Creating Layer relu5
I0521 02:33:43.573288  8640 net.cpp:454] relu5 <- ip1
I0521 02:33:43.573302  8640 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:33:43.573652  8640 net.cpp:150] Setting up relu5
I0521 02:33:43.573665  8640 net.cpp:157] Top shape: 570 196 (111720)
I0521 02:33:43.573676  8640 net.cpp:165] Memory required for data: 898741800
I0521 02:33:43.573686  8640 layer_factory.hpp:77] Creating layer drop1
I0521 02:33:43.573705  8640 net.cpp:106] Creating Layer drop1
I0521 02:33:43.573715  8640 net.cpp:454] drop1 <- ip1
I0521 02:33:43.573727  8640 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:33:43.573772  8640 net.cpp:150] Setting up drop1
I0521 02:33:43.573784  8640 net.cpp:157] Top shape: 570 196 (111720)
I0521 02:33:43.573794  8640 net.cpp:165] Memory required for data: 899188680
I0521 02:33:43.573803  8640 layer_factory.hpp:77] Creating layer ip2
I0521 02:33:43.573818  8640 net.cpp:106] Creating Layer ip2
I0521 02:33:43.573828  8640 net.cpp:454] ip2 <- ip1
I0521 02:33:43.573842  8640 net.cpp:411] ip2 -> ip2
I0521 02:33:43.574321  8640 net.cpp:150] Setting up ip2
I0521 02:33:43.574334  8640 net.cpp:157] Top shape: 570 98 (55860)
I0521 02:33:43.574344  8640 net.cpp:165] Memory required for data: 899412120
I0521 02:33:43.574373  8640 layer_factory.hpp:77] Creating layer relu6
I0521 02:33:43.574386  8640 net.cpp:106] Creating Layer relu6
I0521 02:33:43.574395  8640 net.cpp:454] relu6 <- ip2
I0521 02:33:43.574409  8640 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:33:43.574939  8640 net.cpp:150] Setting up relu6
I0521 02:33:43.574955  8640 net.cpp:157] Top shape: 570 98 (55860)
I0521 02:33:43.574965  8640 net.cpp:165] Memory required for data: 899635560
I0521 02:33:43.574975  8640 layer_factory.hpp:77] Creating layer drop2
I0521 02:33:43.574988  8640 net.cpp:106] Creating Layer drop2
I0521 02:33:43.575000  8640 net.cpp:454] drop2 <- ip2
I0521 02:33:43.575012  8640 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:33:43.575057  8640 net.cpp:150] Setting up drop2
I0521 02:33:43.575069  8640 net.cpp:157] Top shape: 570 98 (55860)
I0521 02:33:43.575079  8640 net.cpp:165] Memory required for data: 899859000
I0521 02:33:43.575089  8640 layer_factory.hpp:77] Creating layer ip3
I0521 02:33:43.575103  8640 net.cpp:106] Creating Layer ip3
I0521 02:33:43.575114  8640 net.cpp:454] ip3 <- ip2
I0521 02:33:43.575127  8640 net.cpp:411] ip3 -> ip3
I0521 02:33:43.575357  8640 net.cpp:150] Setting up ip3
I0521 02:33:43.575371  8640 net.cpp:157] Top shape: 570 11 (6270)
I0521 02:33:43.575381  8640 net.cpp:165] Memory required for data: 899884080
I0521 02:33:43.575395  8640 layer_factory.hpp:77] Creating layer drop3
I0521 02:33:43.575408  8640 net.cpp:106] Creating Layer drop3
I0521 02:33:43.575418  8640 net.cpp:454] drop3 <- ip3
I0521 02:33:43.575431  8640 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:33:43.575472  8640 net.cpp:150] Setting up drop3
I0521 02:33:43.575485  8640 net.cpp:157] Top shape: 570 11 (6270)
I0521 02:33:43.575495  8640 net.cpp:165] Memory required for data: 899909160
I0521 02:33:43.575505  8640 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 02:33:43.575517  8640 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 02:33:43.575527  8640 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 02:33:43.575541  8640 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 02:33:43.575556  8640 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 02:33:43.575628  8640 net.cpp:150] Setting up ip3_drop3_0_split
I0521 02:33:43.575640  8640 net.cpp:157] Top shape: 570 11 (6270)
I0521 02:33:43.575652  8640 net.cpp:157] Top shape: 570 11 (6270)
I0521 02:33:43.575662  8640 net.cpp:165] Memory required for data: 899959320
I0521 02:33:43.575670  8640 layer_factory.hpp:77] Creating layer accuracy
I0521 02:33:43.575693  8640 net.cpp:106] Creating Layer accuracy
I0521 02:33:43.575703  8640 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 02:33:43.575714  8640 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 02:33:43.575728  8640 net.cpp:411] accuracy -> accuracy
I0521 02:33:43.575752  8640 net.cpp:150] Setting up accuracy
I0521 02:33:43.575764  8640 net.cpp:157] Top shape: (1)
I0521 02:33:43.575774  8640 net.cpp:165] Memory required for data: 899959324
I0521 02:33:43.575784  8640 layer_factory.hpp:77] Creating layer loss
I0521 02:33:43.575798  8640 net.cpp:106] Creating Layer loss
I0521 02:33:43.575809  8640 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 02:33:43.575819  8640 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 02:33:43.575832  8640 net.cpp:411] loss -> loss
I0521 02:33:43.575850  8640 layer_factory.hpp:77] Creating layer loss
I0521 02:33:43.576342  8640 net.cpp:150] Setting up loss
I0521 02:33:43.576355  8640 net.cpp:157] Top shape: (1)
I0521 02:33:43.576365  8640 net.cpp:160]     with loss weight 1
I0521 02:33:43.576385  8640 net.cpp:165] Memory required for data: 899959328
I0521 02:33:43.576395  8640 net.cpp:226] loss needs backward computation.
I0521 02:33:43.576406  8640 net.cpp:228] accuracy does not need backward computation.
I0521 02:33:43.576417  8640 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 02:33:43.576427  8640 net.cpp:226] drop3 needs backward computation.
I0521 02:33:43.576437  8640 net.cpp:226] ip3 needs backward computation.
I0521 02:33:43.576448  8640 net.cpp:226] drop2 needs backward computation.
I0521 02:33:43.576467  8640 net.cpp:226] relu6 needs backward computation.
I0521 02:33:43.576478  8640 net.cpp:226] ip2 needs backward computation.
I0521 02:33:43.576488  8640 net.cpp:226] drop1 needs backward computation.
I0521 02:33:43.576498  8640 net.cpp:226] relu5 needs backward computation.
I0521 02:33:43.576506  8640 net.cpp:226] ip1 needs backward computation.
I0521 02:33:43.576517  8640 net.cpp:226] pool4 needs backward computation.
I0521 02:33:43.576529  8640 net.cpp:226] relu4 needs backward computation.
I0521 02:33:43.576536  8640 net.cpp:226] conv4 needs backward computation.
I0521 02:33:43.576547  8640 net.cpp:226] pool3 needs backward computation.
I0521 02:33:43.576557  8640 net.cpp:226] relu3 needs backward computation.
I0521 02:33:43.576568  8640 net.cpp:226] conv3 needs backward computation.
I0521 02:33:43.576580  8640 net.cpp:226] pool2 needs backward computation.
I0521 02:33:43.576589  8640 net.cpp:226] relu2 needs backward computation.
I0521 02:33:43.576599  8640 net.cpp:226] conv2 needs backward computation.
I0521 02:33:43.576609  8640 net.cpp:226] pool1 needs backward computation.
I0521 02:33:43.576619  8640 net.cpp:226] relu1 needs backward computation.
I0521 02:33:43.576629  8640 net.cpp:226] conv1 needs backward computation.
I0521 02:33:43.576640  8640 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 02:33:43.576653  8640 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:33:43.576663  8640 net.cpp:270] This network produces output accuracy
I0521 02:33:43.576671  8640 net.cpp:270] This network produces output loss
I0521 02:33:43.576700  8640 net.cpp:283] Network initialization done.
I0521 02:33:43.576833  8640 solver.cpp:60] Solver scaffolding done.
I0521 02:33:43.577968  8640 caffe.cpp:212] Starting Optimization
I0521 02:33:43.577987  8640 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 02:33:43.578001  8640 solver.cpp:289] Learning Rate Policy: fixed
I0521 02:33:43.579236  8640 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 02:34:29.597592  8640 solver.cpp:409]     Test net output #0: accuracy = 0.0934428
I0521 02:34:29.597754  8640 solver.cpp:409]     Test net output #1: loss = 2.39756 (* 1 = 2.39756 loss)
I0521 02:34:29.707890  8640 solver.cpp:237] Iteration 0, loss = 2.39759
I0521 02:34:29.707927  8640 solver.cpp:253]     Train net output #0: loss = 2.39759 (* 1 = 2.39759 loss)
I0521 02:34:29.707947  8640 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 02:34:37.668113  8640 solver.cpp:237] Iteration 26, loss = 2.3815
I0521 02:34:37.668154  8640 solver.cpp:253]     Train net output #0: loss = 2.3815 (* 1 = 2.3815 loss)
I0521 02:34:37.668171  8640 sgd_solver.cpp:106] Iteration 26, lr = 0.0025
I0521 02:34:45.624835  8640 solver.cpp:237] Iteration 52, loss = 2.36396
I0521 02:34:45.624867  8640 solver.cpp:253]     Train net output #0: loss = 2.36396 (* 1 = 2.36396 loss)
I0521 02:34:45.624884  8640 sgd_solver.cpp:106] Iteration 52, lr = 0.0025
I0521 02:34:53.585264  8640 solver.cpp:237] Iteration 78, loss = 2.35423
I0521 02:34:53.585296  8640 solver.cpp:253]     Train net output #0: loss = 2.35423 (* 1 = 2.35423 loss)
I0521 02:34:53.585310  8640 sgd_solver.cpp:106] Iteration 78, lr = 0.0025
I0521 02:35:01.555244  8640 solver.cpp:237] Iteration 104, loss = 2.34131
I0521 02:35:01.555399  8640 solver.cpp:253]     Train net output #0: loss = 2.34131 (* 1 = 2.34131 loss)
I0521 02:35:01.555414  8640 sgd_solver.cpp:106] Iteration 104, lr = 0.0025
I0521 02:35:09.517244  8640 solver.cpp:237] Iteration 130, loss = 2.35047
I0521 02:35:09.517277  8640 solver.cpp:253]     Train net output #0: loss = 2.35047 (* 1 = 2.35047 loss)
I0521 02:35:09.517294  8640 sgd_solver.cpp:106] Iteration 130, lr = 0.0025
I0521 02:35:17.478592  8640 solver.cpp:237] Iteration 156, loss = 2.32952
I0521 02:35:17.478626  8640 solver.cpp:253]     Train net output #0: loss = 2.32952 (* 1 = 2.32952 loss)
I0521 02:35:17.478641  8640 sgd_solver.cpp:106] Iteration 156, lr = 0.0025
I0521 02:35:47.515537  8640 solver.cpp:237] Iteration 182, loss = 2.32438
I0521 02:35:47.515696  8640 solver.cpp:253]     Train net output #0: loss = 2.32438 (* 1 = 2.32438 loss)
I0521 02:35:47.515712  8640 sgd_solver.cpp:106] Iteration 182, lr = 0.0025
I0521 02:35:55.482254  8640 solver.cpp:237] Iteration 208, loss = 2.30575
I0521 02:35:55.482292  8640 solver.cpp:253]     Train net output #0: loss = 2.30575 (* 1 = 2.30575 loss)
I0521 02:35:55.482313  8640 sgd_solver.cpp:106] Iteration 208, lr = 0.0025
I0521 02:36:03.443790  8640 solver.cpp:237] Iteration 234, loss = 2.30449
I0521 02:36:03.443825  8640 solver.cpp:253]     Train net output #0: loss = 2.30449 (* 1 = 2.30449 loss)
I0521 02:36:03.443837  8640 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0521 02:36:11.409049  8640 solver.cpp:237] Iteration 260, loss = 2.29906
I0521 02:36:11.409082  8640 solver.cpp:253]     Train net output #0: loss = 2.29906 (* 1 = 2.29906 loss)
I0521 02:36:11.409099  8640 sgd_solver.cpp:106] Iteration 260, lr = 0.0025
I0521 02:36:12.022478  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_263.caffemodel
I0521 02:36:12.278316  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_263.solverstate
I0521 02:36:19.435127  8640 solver.cpp:237] Iteration 286, loss = 2.26903
I0521 02:36:19.435288  8640 solver.cpp:253]     Train net output #0: loss = 2.26903 (* 1 = 2.26903 loss)
I0521 02:36:19.435303  8640 sgd_solver.cpp:106] Iteration 286, lr = 0.0025
I0521 02:36:27.399195  8640 solver.cpp:237] Iteration 312, loss = 2.2468
I0521 02:36:27.399227  8640 solver.cpp:253]     Train net output #0: loss = 2.2468 (* 1 = 2.2468 loss)
I0521 02:36:27.399242  8640 sgd_solver.cpp:106] Iteration 312, lr = 0.0025
I0521 02:36:35.363627  8640 solver.cpp:237] Iteration 338, loss = 2.19413
I0521 02:36:35.363662  8640 solver.cpp:253]     Train net output #0: loss = 2.19413 (* 1 = 2.19413 loss)
I0521 02:36:35.363675  8640 sgd_solver.cpp:106] Iteration 338, lr = 0.0025
I0521 02:37:05.449082  8640 solver.cpp:237] Iteration 364, loss = 2.13888
I0521 02:37:05.449226  8640 solver.cpp:253]     Train net output #0: loss = 2.13888 (* 1 = 2.13888 loss)
I0521 02:37:05.449241  8640 sgd_solver.cpp:106] Iteration 364, lr = 0.0025
I0521 02:37:13.409142  8640 solver.cpp:237] Iteration 390, loss = 2.13155
I0521 02:37:13.409174  8640 solver.cpp:253]     Train net output #0: loss = 2.13155 (* 1 = 2.13155 loss)
I0521 02:37:13.409191  8640 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 02:37:21.367771  8640 solver.cpp:237] Iteration 416, loss = 2.11031
I0521 02:37:21.367804  8640 solver.cpp:253]     Train net output #0: loss = 2.11031 (* 1 = 2.11031 loss)
I0521 02:37:21.367817  8640 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 02:37:29.329630  8640 solver.cpp:237] Iteration 442, loss = 2.0467
I0521 02:37:29.329664  8640 solver.cpp:253]     Train net output #0: loss = 2.0467 (* 1 = 2.0467 loss)
I0521 02:37:29.329681  8640 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0521 02:37:37.296434  8640 solver.cpp:237] Iteration 468, loss = 2.05693
I0521 02:37:37.296591  8640 solver.cpp:253]     Train net output #0: loss = 2.05693 (* 1 = 2.05693 loss)
I0521 02:37:37.296605  8640 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0521 02:37:45.262128  8640 solver.cpp:237] Iteration 494, loss = 1.97742
I0521 02:37:45.262161  8640 solver.cpp:253]     Train net output #0: loss = 1.97742 (* 1 = 1.97742 loss)
I0521 02:37:45.262179  8640 sgd_solver.cpp:106] Iteration 494, lr = 0.0025
I0521 02:37:53.228962  8640 solver.cpp:237] Iteration 520, loss = 2.06746
I0521 02:37:53.228996  8640 solver.cpp:253]     Train net output #0: loss = 2.06746 (* 1 = 2.06746 loss)
I0521 02:37:53.229012  8640 sgd_solver.cpp:106] Iteration 520, lr = 0.0025
I0521 02:37:54.760097  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_526.caffemodel
I0521 02:37:55.021940  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_526.solverstate
I0521 02:37:55.046952  8640 solver.cpp:341] Iteration 526, Testing net (#0)
I0521 02:38:40.129961  8640 solver.cpp:409]     Test net output #0: accuracy = 0.532159
I0521 02:38:40.130120  8640 solver.cpp:409]     Test net output #1: loss = 1.74201 (* 1 = 1.74201 loss)
I0521 02:39:08.424893  8640 solver.cpp:237] Iteration 546, loss = 1.93785
I0521 02:39:08.424943  8640 solver.cpp:253]     Train net output #0: loss = 1.93785 (* 1 = 1.93785 loss)
I0521 02:39:08.424960  8640 sgd_solver.cpp:106] Iteration 546, lr = 0.0025
I0521 02:39:16.381065  8640 solver.cpp:237] Iteration 572, loss = 1.97503
I0521 02:39:16.381211  8640 solver.cpp:253]     Train net output #0: loss = 1.97503 (* 1 = 1.97503 loss)
I0521 02:39:16.381224  8640 sgd_solver.cpp:106] Iteration 572, lr = 0.0025
I0521 02:39:24.337817  8640 solver.cpp:237] Iteration 598, loss = 1.99054
I0521 02:39:24.337849  8640 solver.cpp:253]     Train net output #0: loss = 1.99054 (* 1 = 1.99054 loss)
I0521 02:39:24.337864  8640 sgd_solver.cpp:106] Iteration 598, lr = 0.0025
I0521 02:39:32.293824  8640 solver.cpp:237] Iteration 624, loss = 1.89562
I0521 02:39:32.293856  8640 solver.cpp:253]     Train net output #0: loss = 1.89562 (* 1 = 1.89562 loss)
I0521 02:39:32.293874  8640 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 02:39:40.247454  8640 solver.cpp:237] Iteration 650, loss = 1.88497
I0521 02:39:40.247486  8640 solver.cpp:253]     Train net output #0: loss = 1.88497 (* 1 = 1.88497 loss)
I0521 02:39:40.247505  8640 sgd_solver.cpp:106] Iteration 650, lr = 0.0025
I0521 02:39:48.200589  8640 solver.cpp:237] Iteration 676, loss = 1.86171
I0521 02:39:48.200721  8640 solver.cpp:253]     Train net output #0: loss = 1.86171 (* 1 = 1.86171 loss)
I0521 02:39:48.200734  8640 sgd_solver.cpp:106] Iteration 676, lr = 0.0025
I0521 02:40:18.244982  8640 solver.cpp:237] Iteration 702, loss = 1.88777
I0521 02:40:18.245141  8640 solver.cpp:253]     Train net output #0: loss = 1.88777 (* 1 = 1.88777 loss)
I0521 02:40:18.245157  8640 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 02:40:26.200711  8640 solver.cpp:237] Iteration 728, loss = 1.94069
I0521 02:40:26.200748  8640 solver.cpp:253]     Train net output #0: loss = 1.94069 (* 1 = 1.94069 loss)
I0521 02:40:26.200767  8640 sgd_solver.cpp:106] Iteration 728, lr = 0.0025
I0521 02:40:34.156941  8640 solver.cpp:237] Iteration 754, loss = 1.86148
I0521 02:40:34.156975  8640 solver.cpp:253]     Train net output #0: loss = 1.86148 (* 1 = 1.86148 loss)
I0521 02:40:34.156991  8640 sgd_solver.cpp:106] Iteration 754, lr = 0.0025
I0521 02:40:42.120226  8640 solver.cpp:237] Iteration 780, loss = 1.80624
I0521 02:40:42.120260  8640 solver.cpp:253]     Train net output #0: loss = 1.80624 (* 1 = 1.80624 loss)
I0521 02:40:42.120277  8640 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 02:40:44.568166  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_789.caffemodel
I0521 02:40:44.844552  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_789.solverstate
I0521 02:40:50.170789  8640 solver.cpp:237] Iteration 806, loss = 1.88185
I0521 02:40:50.170951  8640 solver.cpp:253]     Train net output #0: loss = 1.88185 (* 1 = 1.88185 loss)
I0521 02:40:50.170965  8640 sgd_solver.cpp:106] Iteration 806, lr = 0.0025
I0521 02:40:58.128553  8640 solver.cpp:237] Iteration 832, loss = 1.84078
I0521 02:40:58.128588  8640 solver.cpp:253]     Train net output #0: loss = 1.84078 (* 1 = 1.84078 loss)
I0521 02:40:58.128610  8640 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 02:41:06.088882  8640 solver.cpp:237] Iteration 858, loss = 1.77315
I0521 02:41:06.088917  8640 solver.cpp:253]     Train net output #0: loss = 1.77315 (* 1 = 1.77315 loss)
I0521 02:41:06.088932  8640 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0521 02:41:36.219126  8640 solver.cpp:237] Iteration 884, loss = 1.83204
I0521 02:41:36.219300  8640 solver.cpp:253]     Train net output #0: loss = 1.83204 (* 1 = 1.83204 loss)
I0521 02:41:36.219316  8640 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0521 02:41:44.179782  8640 solver.cpp:237] Iteration 910, loss = 1.89709
I0521 02:41:44.179828  8640 solver.cpp:253]     Train net output #0: loss = 1.89709 (* 1 = 1.89709 loss)
I0521 02:41:44.179842  8640 sgd_solver.cpp:106] Iteration 910, lr = 0.0025
I0521 02:41:52.133673  8640 solver.cpp:237] Iteration 936, loss = 1.7992
I0521 02:41:52.133708  8640 solver.cpp:253]     Train net output #0: loss = 1.7992 (* 1 = 1.7992 loss)
I0521 02:41:52.133725  8640 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 02:42:00.090445  8640 solver.cpp:237] Iteration 962, loss = 1.79858
I0521 02:42:00.090479  8640 solver.cpp:253]     Train net output #0: loss = 1.79858 (* 1 = 1.79858 loss)
I0521 02:42:00.090497  8640 sgd_solver.cpp:106] Iteration 962, lr = 0.0025
I0521 02:42:08.044658  8640 solver.cpp:237] Iteration 988, loss = 1.82561
I0521 02:42:08.044806  8640 solver.cpp:253]     Train net output #0: loss = 1.82561 (* 1 = 1.82561 loss)
I0521 02:42:08.044818  8640 sgd_solver.cpp:106] Iteration 988, lr = 0.0025
I0521 02:42:15.997931  8640 solver.cpp:237] Iteration 1014, loss = 1.83397
I0521 02:42:15.997963  8640 solver.cpp:253]     Train net output #0: loss = 1.83397 (* 1 = 1.83397 loss)
I0521 02:42:15.997980  8640 sgd_solver.cpp:106] Iteration 1014, lr = 0.0025
I0521 02:42:23.957653  8640 solver.cpp:237] Iteration 1040, loss = 1.81391
I0521 02:42:23.957685  8640 solver.cpp:253]     Train net output #0: loss = 1.81391 (* 1 = 1.81391 loss)
I0521 02:42:23.957701  8640 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 02:42:27.320046  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1052.caffemodel
I0521 02:42:27.574566  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1052.solverstate
I0521 02:42:27.603077  8640 solver.cpp:341] Iteration 1052, Testing net (#0)
I0521 02:43:33.521670  8640 solver.cpp:409]     Test net output #0: accuracy = 0.636108
I0521 02:43:33.521836  8640 solver.cpp:409]     Test net output #1: loss = 1.29328 (* 1 = 1.29328 loss)
I0521 02:44:00.003216  8640 solver.cpp:237] Iteration 1066, loss = 1.72846
I0521 02:44:00.003265  8640 solver.cpp:253]     Train net output #0: loss = 1.72846 (* 1 = 1.72846 loss)
I0521 02:44:00.003283  8640 sgd_solver.cpp:106] Iteration 1066, lr = 0.0025
I0521 02:44:07.953526  8640 solver.cpp:237] Iteration 1092, loss = 1.7856
I0521 02:44:07.953681  8640 solver.cpp:253]     Train net output #0: loss = 1.7856 (* 1 = 1.7856 loss)
I0521 02:44:07.953696  8640 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0521 02:44:15.905750  8640 solver.cpp:237] Iteration 1118, loss = 1.7484
I0521 02:44:15.905786  8640 solver.cpp:253]     Train net output #0: loss = 1.7484 (* 1 = 1.7484 loss)
I0521 02:44:15.905807  8640 sgd_solver.cpp:106] Iteration 1118, lr = 0.0025
I0521 02:44:23.855696  8640 solver.cpp:237] Iteration 1144, loss = 1.78349
I0521 02:44:23.855728  8640 solver.cpp:253]     Train net output #0: loss = 1.78349 (* 1 = 1.78349 loss)
I0521 02:44:23.855746  8640 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0521 02:44:31.807319  8640 solver.cpp:237] Iteration 1170, loss = 1.82191
I0521 02:44:31.807351  8640 solver.cpp:253]     Train net output #0: loss = 1.82191 (* 1 = 1.82191 loss)
I0521 02:44:31.807368  8640 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 02:44:39.757941  8640 solver.cpp:237] Iteration 1196, loss = 1.77273
I0521 02:44:39.758101  8640 solver.cpp:253]     Train net output #0: loss = 1.77273 (* 1 = 1.77273 loss)
I0521 02:44:39.758116  8640 sgd_solver.cpp:106] Iteration 1196, lr = 0.0025
I0521 02:44:47.706722  8640 solver.cpp:237] Iteration 1222, loss = 1.73999
I0521 02:44:47.706753  8640 solver.cpp:253]     Train net output #0: loss = 1.73999 (* 1 = 1.73999 loss)
I0521 02:44:47.706770  8640 sgd_solver.cpp:106] Iteration 1222, lr = 0.0025
I0521 02:45:17.772605  8640 solver.cpp:237] Iteration 1248, loss = 1.73472
I0521 02:45:17.772764  8640 solver.cpp:253]     Train net output #0: loss = 1.73472 (* 1 = 1.73472 loss)
I0521 02:45:17.772779  8640 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 02:45:25.721827  8640 solver.cpp:237] Iteration 1274, loss = 1.77858
I0521 02:45:25.721868  8640 solver.cpp:253]     Train net output #0: loss = 1.77858 (* 1 = 1.77858 loss)
I0521 02:45:25.721882  8640 sgd_solver.cpp:106] Iteration 1274, lr = 0.0025
I0521 02:45:33.675909  8640 solver.cpp:237] Iteration 1300, loss = 1.68199
I0521 02:45:33.675941  8640 solver.cpp:253]     Train net output #0: loss = 1.68199 (* 1 = 1.68199 loss)
I0521 02:45:33.675959  8640 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0521 02:45:37.957790  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1315.caffemodel
I0521 02:45:38.212426  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1315.solverstate
I0521 02:45:41.698335  8640 solver.cpp:237] Iteration 1326, loss = 1.71937
I0521 02:45:41.698385  8640 solver.cpp:253]     Train net output #0: loss = 1.71937 (* 1 = 1.71937 loss)
I0521 02:45:41.698398  8640 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0521 02:45:49.647366  8640 solver.cpp:237] Iteration 1352, loss = 1.7685
I0521 02:45:49.647506  8640 solver.cpp:253]     Train net output #0: loss = 1.7685 (* 1 = 1.7685 loss)
I0521 02:45:49.647519  8640 sgd_solver.cpp:106] Iteration 1352, lr = 0.0025
I0521 02:45:57.596673  8640 solver.cpp:237] Iteration 1378, loss = 1.7513
I0521 02:45:57.596719  8640 solver.cpp:253]     Train net output #0: loss = 1.7513 (* 1 = 1.7513 loss)
I0521 02:45:57.596740  8640 sgd_solver.cpp:106] Iteration 1378, lr = 0.0025
I0521 02:46:27.641342  8640 solver.cpp:237] Iteration 1404, loss = 1.70318
I0521 02:46:27.641527  8640 solver.cpp:253]     Train net output #0: loss = 1.70318 (* 1 = 1.70318 loss)
I0521 02:46:27.641543  8640 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 02:46:35.591938  8640 solver.cpp:237] Iteration 1430, loss = 1.71007
I0521 02:46:35.591971  8640 solver.cpp:253]     Train net output #0: loss = 1.71007 (* 1 = 1.71007 loss)
I0521 02:46:35.591984  8640 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0521 02:46:43.546289  8640 solver.cpp:237] Iteration 1456, loss = 1.72588
I0521 02:46:43.546330  8640 solver.cpp:253]     Train net output #0: loss = 1.72588 (* 1 = 1.72588 loss)
I0521 02:46:43.546350  8640 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 02:46:51.499435  8640 solver.cpp:237] Iteration 1482, loss = 1.72347
I0521 02:46:51.499469  8640 solver.cpp:253]     Train net output #0: loss = 1.72347 (* 1 = 1.72347 loss)
I0521 02:46:51.499485  8640 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0521 02:46:59.448251  8640 solver.cpp:237] Iteration 1508, loss = 1.72227
I0521 02:46:59.448388  8640 solver.cpp:253]     Train net output #0: loss = 1.72227 (* 1 = 1.72227 loss)
I0521 02:46:59.448401  8640 sgd_solver.cpp:106] Iteration 1508, lr = 0.0025
I0521 02:47:07.395716  8640 solver.cpp:237] Iteration 1534, loss = 1.81524
I0521 02:47:07.395762  8640 solver.cpp:253]     Train net output #0: loss = 1.81524 (* 1 = 1.81524 loss)
I0521 02:47:07.395778  8640 sgd_solver.cpp:106] Iteration 1534, lr = 0.0025
I0521 02:47:15.345322  8640 solver.cpp:237] Iteration 1560, loss = 1.72826
I0521 02:47:15.345355  8640 solver.cpp:253]     Train net output #0: loss = 1.72826 (* 1 = 1.72826 loss)
I0521 02:47:15.345368  8640 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 02:47:20.546933  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1578.caffemodel
I0521 02:47:20.799100  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1578.solverstate
I0521 02:47:20.825320  8640 solver.cpp:341] Iteration 1578, Testing net (#0)
I0521 02:48:05.524130  8640 solver.cpp:409]     Test net output #0: accuracy = 0.662211
I0521 02:48:05.524284  8640 solver.cpp:409]     Test net output #1: loss = 1.16227 (* 1 = 1.16227 loss)
I0521 02:48:30.251108  8640 solver.cpp:237] Iteration 1586, loss = 1.69209
I0521 02:48:30.251158  8640 solver.cpp:253]     Train net output #0: loss = 1.69209 (* 1 = 1.69209 loss)
I0521 02:48:30.251174  8640 sgd_solver.cpp:106] Iteration 1586, lr = 0.0025
I0521 02:48:38.201320  8640 solver.cpp:237] Iteration 1612, loss = 1.61455
I0521 02:48:38.201468  8640 solver.cpp:253]     Train net output #0: loss = 1.61455 (* 1 = 1.61455 loss)
I0521 02:48:38.201483  8640 sgd_solver.cpp:106] Iteration 1612, lr = 0.0025
I0521 02:48:46.158424  8640 solver.cpp:237] Iteration 1638, loss = 1.69659
I0521 02:48:46.158468  8640 solver.cpp:253]     Train net output #0: loss = 1.69659 (* 1 = 1.69659 loss)
I0521 02:48:46.158484  8640 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 02:48:54.111245  8640 solver.cpp:237] Iteration 1664, loss = 1.71097
I0521 02:48:54.111279  8640 solver.cpp:253]     Train net output #0: loss = 1.71097 (* 1 = 1.71097 loss)
I0521 02:48:54.111295  8640 sgd_solver.cpp:106] Iteration 1664, lr = 0.0025
I0521 02:49:02.065891  8640 solver.cpp:237] Iteration 1690, loss = 1.65063
I0521 02:49:02.065923  8640 solver.cpp:253]     Train net output #0: loss = 1.65063 (* 1 = 1.65063 loss)
I0521 02:49:02.065940  8640 sgd_solver.cpp:106] Iteration 1690, lr = 0.0025
I0521 02:49:10.017444  8640 solver.cpp:237] Iteration 1716, loss = 1.7716
I0521 02:49:10.017583  8640 solver.cpp:253]     Train net output #0: loss = 1.7716 (* 1 = 1.7716 loss)
I0521 02:49:10.017596  8640 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0521 02:49:17.973127  8640 solver.cpp:237] Iteration 1742, loss = 1.65497
I0521 02:49:17.973163  8640 solver.cpp:253]     Train net output #0: loss = 1.65497 (* 1 = 1.65497 loss)
I0521 02:49:17.973182  8640 sgd_solver.cpp:106] Iteration 1742, lr = 0.0025
I0521 02:49:48.090416  8640 solver.cpp:237] Iteration 1768, loss = 1.7767
I0521 02:49:48.090590  8640 solver.cpp:253]     Train net output #0: loss = 1.7767 (* 1 = 1.7767 loss)
I0521 02:49:48.090605  8640 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0521 02:49:56.042628  8640 solver.cpp:237] Iteration 1794, loss = 1.6613
I0521 02:49:56.042660  8640 solver.cpp:253]     Train net output #0: loss = 1.6613 (* 1 = 1.6613 loss)
I0521 02:49:56.042677  8640 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0521 02:50:04.000526  8640 solver.cpp:237] Iteration 1820, loss = 1.68227
I0521 02:50:04.000569  8640 solver.cpp:253]     Train net output #0: loss = 1.68227 (* 1 = 1.68227 loss)
I0521 02:50:04.000586  8640 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0521 02:50:10.119338  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1841.caffemodel
I0521 02:50:10.371069  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_1841.solverstate
I0521 02:50:12.017884  8640 solver.cpp:237] Iteration 1846, loss = 1.77334
I0521 02:50:12.017930  8640 solver.cpp:253]     Train net output #0: loss = 1.77334 (* 1 = 1.77334 loss)
I0521 02:50:12.017946  8640 sgd_solver.cpp:106] Iteration 1846, lr = 0.0025
I0521 02:50:19.974205  8640 solver.cpp:237] Iteration 1872, loss = 1.62214
I0521 02:50:19.974359  8640 solver.cpp:253]     Train net output #0: loss = 1.62214 (* 1 = 1.62214 loss)
I0521 02:50:19.974372  8640 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0521 02:50:27.928372  8640 solver.cpp:237] Iteration 1898, loss = 1.73708
I0521 02:50:27.928409  8640 solver.cpp:253]     Train net output #0: loss = 1.73708 (* 1 = 1.73708 loss)
I0521 02:50:27.928429  8640 sgd_solver.cpp:106] Iteration 1898, lr = 0.0025
I0521 02:50:35.881829  8640 solver.cpp:237] Iteration 1924, loss = 1.63641
I0521 02:50:35.881861  8640 solver.cpp:253]     Train net output #0: loss = 1.63641 (* 1 = 1.63641 loss)
I0521 02:50:35.881877  8640 sgd_solver.cpp:106] Iteration 1924, lr = 0.0025
I0521 02:51:05.954385  8640 solver.cpp:237] Iteration 1950, loss = 1.634
I0521 02:51:05.954551  8640 solver.cpp:253]     Train net output #0: loss = 1.634 (* 1 = 1.634 loss)
I0521 02:51:05.954566  8640 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0521 02:51:13.910923  8640 solver.cpp:237] Iteration 1976, loss = 1.62935
I0521 02:51:13.910955  8640 solver.cpp:253]     Train net output #0: loss = 1.62935 (* 1 = 1.62935 loss)
I0521 02:51:13.910972  8640 sgd_solver.cpp:106] Iteration 1976, lr = 0.0025
I0521 02:51:21.866044  8640 solver.cpp:237] Iteration 2002, loss = 1.67401
I0521 02:51:21.866091  8640 solver.cpp:253]     Train net output #0: loss = 1.67401 (* 1 = 1.67401 loss)
I0521 02:51:21.866108  8640 sgd_solver.cpp:106] Iteration 2002, lr = 0.0025
I0521 02:51:29.818085  8640 solver.cpp:237] Iteration 2028, loss = 1.75382
I0521 02:51:29.818119  8640 solver.cpp:253]     Train net output #0: loss = 1.75382 (* 1 = 1.75382 loss)
I0521 02:51:29.818135  8640 sgd_solver.cpp:106] Iteration 2028, lr = 0.0025
I0521 02:51:37.772380  8640 solver.cpp:237] Iteration 2054, loss = 1.59095
I0521 02:51:37.772518  8640 solver.cpp:253]     Train net output #0: loss = 1.59095 (* 1 = 1.59095 loss)
I0521 02:51:37.772531  8640 sgd_solver.cpp:106] Iteration 2054, lr = 0.0025
I0521 02:51:45.724095  8640 solver.cpp:237] Iteration 2080, loss = 1.64392
I0521 02:51:45.724140  8640 solver.cpp:253]     Train net output #0: loss = 1.64392 (* 1 = 1.64392 loss)
I0521 02:51:45.724158  8640 sgd_solver.cpp:106] Iteration 2080, lr = 0.0025
I0521 02:51:52.763036  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2104.caffemodel
I0521 02:51:53.016381  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2104.solverstate
I0521 02:51:53.042516  8640 solver.cpp:341] Iteration 2104, Testing net (#0)
I0521 02:52:58.936388  8640 solver.cpp:409]     Test net output #0: accuracy = 0.682376
I0521 02:52:58.936558  8640 solver.cpp:409]     Test net output #1: loss = 1.09681 (* 1 = 1.09681 loss)
I0521 02:53:21.821921  8640 solver.cpp:237] Iteration 2106, loss = 1.67047
I0521 02:53:21.821971  8640 solver.cpp:253]     Train net output #0: loss = 1.67047 (* 1 = 1.67047 loss)
I0521 02:53:21.821988  8640 sgd_solver.cpp:106] Iteration 2106, lr = 0.0025
I0521 02:53:29.774413  8640 solver.cpp:237] Iteration 2132, loss = 1.71477
I0521 02:53:29.774567  8640 solver.cpp:253]     Train net output #0: loss = 1.71477 (* 1 = 1.71477 loss)
I0521 02:53:29.774582  8640 sgd_solver.cpp:106] Iteration 2132, lr = 0.0025
I0521 02:53:37.733608  8640 solver.cpp:237] Iteration 2158, loss = 1.63975
I0521 02:53:37.733642  8640 solver.cpp:253]     Train net output #0: loss = 1.63975 (* 1 = 1.63975 loss)
I0521 02:53:37.733659  8640 sgd_solver.cpp:106] Iteration 2158, lr = 0.0025
I0521 02:53:45.689146  8640 solver.cpp:237] Iteration 2184, loss = 1.64874
I0521 02:53:45.689183  8640 solver.cpp:253]     Train net output #0: loss = 1.64874 (* 1 = 1.64874 loss)
I0521 02:53:45.689198  8640 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0521 02:53:53.647797  8640 solver.cpp:237] Iteration 2210, loss = 1.66292
I0521 02:53:53.647830  8640 solver.cpp:253]     Train net output #0: loss = 1.66292 (* 1 = 1.66292 loss)
I0521 02:53:53.647846  8640 sgd_solver.cpp:106] Iteration 2210, lr = 0.0025
I0521 02:54:01.606736  8640 solver.cpp:237] Iteration 2236, loss = 1.71277
I0521 02:54:01.606875  8640 solver.cpp:253]     Train net output #0: loss = 1.71277 (* 1 = 1.71277 loss)
I0521 02:54:01.606889  8640 sgd_solver.cpp:106] Iteration 2236, lr = 0.0025
I0521 02:54:09.565403  8640 solver.cpp:237] Iteration 2262, loss = 1.63835
I0521 02:54:09.565435  8640 solver.cpp:253]     Train net output #0: loss = 1.63835 (* 1 = 1.63835 loss)
I0521 02:54:09.565454  8640 sgd_solver.cpp:106] Iteration 2262, lr = 0.0025
I0521 02:54:39.640105  8640 solver.cpp:237] Iteration 2288, loss = 1.62531
I0521 02:54:39.640270  8640 solver.cpp:253]     Train net output #0: loss = 1.62531 (* 1 = 1.62531 loss)
I0521 02:54:39.640285  8640 sgd_solver.cpp:106] Iteration 2288, lr = 0.0025
I0521 02:54:47.594697  8640 solver.cpp:237] Iteration 2314, loss = 1.67485
I0521 02:54:47.594729  8640 solver.cpp:253]     Train net output #0: loss = 1.67485 (* 1 = 1.67485 loss)
I0521 02:54:47.594743  8640 sgd_solver.cpp:106] Iteration 2314, lr = 0.0025
I0521 02:54:55.546597  8640 solver.cpp:237] Iteration 2340, loss = 1.87378
I0521 02:54:55.546629  8640 solver.cpp:253]     Train net output #0: loss = 1.87378 (* 1 = 1.87378 loss)
I0521 02:54:55.546643  8640 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0521 02:55:03.504582  8640 solver.cpp:237] Iteration 2366, loss = 1.59841
I0521 02:55:03.504622  8640 solver.cpp:253]     Train net output #0: loss = 1.59841 (* 1 = 1.59841 loss)
I0521 02:55:03.504637  8640 sgd_solver.cpp:106] Iteration 2366, lr = 0.0025
I0521 02:55:03.505022  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2367.caffemodel
I0521 02:55:03.758189  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2367.solverstate
I0521 02:55:11.535596  8640 solver.cpp:237] Iteration 2392, loss = 1.66222
I0521 02:55:11.535766  8640 solver.cpp:253]     Train net output #0: loss = 1.66222 (* 1 = 1.66222 loss)
I0521 02:55:11.535780  8640 sgd_solver.cpp:106] Iteration 2392, lr = 0.0025
I0521 02:55:19.488639  8640 solver.cpp:237] Iteration 2418, loss = 1.61456
I0521 02:55:19.488672  8640 solver.cpp:253]     Train net output #0: loss = 1.61456 (* 1 = 1.61456 loss)
I0521 02:55:19.488689  8640 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0521 02:55:27.443126  8640 solver.cpp:237] Iteration 2444, loss = 1.60369
I0521 02:55:27.443168  8640 solver.cpp:253]     Train net output #0: loss = 1.60369 (* 1 = 1.60369 loss)
I0521 02:55:27.443192  8640 sgd_solver.cpp:106] Iteration 2444, lr = 0.0025
I0521 02:55:57.554512  8640 solver.cpp:237] Iteration 2470, loss = 1.56833
I0521 02:55:57.554683  8640 solver.cpp:253]     Train net output #0: loss = 1.56833 (* 1 = 1.56833 loss)
I0521 02:55:57.554699  8640 sgd_solver.cpp:106] Iteration 2470, lr = 0.0025
I0521 02:56:05.508074  8640 solver.cpp:237] Iteration 2496, loss = 1.63682
I0521 02:56:05.508106  8640 solver.cpp:253]     Train net output #0: loss = 1.63682 (* 1 = 1.63682 loss)
I0521 02:56:05.508123  8640 sgd_solver.cpp:106] Iteration 2496, lr = 0.0025
I0521 02:56:13.463371  8640 solver.cpp:237] Iteration 2522, loss = 1.59646
I0521 02:56:13.463403  8640 solver.cpp:253]     Train net output #0: loss = 1.59646 (* 1 = 1.59646 loss)
I0521 02:56:13.463420  8640 sgd_solver.cpp:106] Iteration 2522, lr = 0.0025
I0521 02:56:21.419679  8640 solver.cpp:237] Iteration 2548, loss = 1.48371
I0521 02:56:21.419718  8640 solver.cpp:253]     Train net output #0: loss = 1.48371 (* 1 = 1.48371 loss)
I0521 02:56:21.419739  8640 sgd_solver.cpp:106] Iteration 2548, lr = 0.0025
I0521 02:56:29.376236  8640 solver.cpp:237] Iteration 2574, loss = 1.66641
I0521 02:56:29.376375  8640 solver.cpp:253]     Train net output #0: loss = 1.66641 (* 1 = 1.66641 loss)
I0521 02:56:29.376389  8640 sgd_solver.cpp:106] Iteration 2574, lr = 0.0025
I0521 02:56:37.331989  8640 solver.cpp:237] Iteration 2600, loss = 1.63022
I0521 02:56:37.332020  8640 solver.cpp:253]     Train net output #0: loss = 1.63022 (* 1 = 1.63022 loss)
I0521 02:56:37.332037  8640 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0521 02:56:45.287137  8640 solver.cpp:237] Iteration 2626, loss = 1.6259
I0521 02:56:45.287175  8640 solver.cpp:253]     Train net output #0: loss = 1.6259 (* 1 = 1.6259 loss)
I0521 02:56:45.287196  8640 sgd_solver.cpp:106] Iteration 2626, lr = 0.0025
I0521 02:56:46.205756  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2630.caffemodel
I0521 02:56:46.466092  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2630.solverstate
I0521 02:56:46.494567  8640 solver.cpp:341] Iteration 2630, Testing net (#0)
I0521 02:57:31.582173  8640 solver.cpp:409]     Test net output #0: accuracy = 0.707898
I0521 02:57:31.582334  8640 solver.cpp:409]     Test net output #1: loss = 1.04094 (* 1 = 1.04094 loss)
I0521 02:57:31.673925  8640 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2631.caffemodel
I0521 02:57:31.929081  8640 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623_iter_2631.solverstate
I0521 02:57:31.957265  8640 solver.cpp:326] Optimization Done.
I0521 02:57:31.957291  8640 caffe.cpp:215] Optimization Done.
Application 11236566 resources: utime ~1248s, stime ~226s, Rss ~5333104, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_570_2016-05-20T11.20.53.329623.solver"
	User time (seconds): 0.57
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:38.59
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15073
	Voluntary context switches: 2690
	Involuntary context switches: 77
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
