2806321
I0521 06:35:24.444515  2420 caffe.cpp:184] Using GPUs 0
I0521 06:35:24.867997  2420 solver.cpp:48] Initializing solver from parameters: 
test_iter: 197
test_interval: 394
base_lr: 0.0025
display: 19
max_iter: 1973
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 197
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915.prototxt"
I0521 06:35:24.872005  2420 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915.prototxt
I0521 06:35:24.884039  2420 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 06:35:24.884099  2420 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 06:35:24.884443  2420 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 760
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 06:35:24.884625  2420 layer_factory.hpp:77] Creating layer data_hdf5
I0521 06:35:24.884649  2420 net.cpp:106] Creating Layer data_hdf5
I0521 06:35:24.884665  2420 net.cpp:411] data_hdf5 -> data
I0521 06:35:24.884697  2420 net.cpp:411] data_hdf5 -> label
I0521 06:35:24.884729  2420 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 06:35:24.885946  2420 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 06:35:24.888125  2420 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 06:35:46.425783  2420 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 06:35:46.430918  2420 net.cpp:150] Setting up data_hdf5
I0521 06:35:46.430965  2420 net.cpp:157] Top shape: 760 1 127 50 (4826000)
I0521 06:35:46.430979  2420 net.cpp:157] Top shape: 760 (760)
I0521 06:35:46.430989  2420 net.cpp:165] Memory required for data: 19307040
I0521 06:35:46.431002  2420 layer_factory.hpp:77] Creating layer conv1
I0521 06:35:46.431035  2420 net.cpp:106] Creating Layer conv1
I0521 06:35:46.431047  2420 net.cpp:454] conv1 <- data
I0521 06:35:46.431071  2420 net.cpp:411] conv1 -> conv1
I0521 06:35:46.795680  2420 net.cpp:150] Setting up conv1
I0521 06:35:46.795727  2420 net.cpp:157] Top shape: 760 12 120 48 (52531200)
I0521 06:35:46.795738  2420 net.cpp:165] Memory required for data: 229431840
I0521 06:35:46.795770  2420 layer_factory.hpp:77] Creating layer relu1
I0521 06:35:46.795791  2420 net.cpp:106] Creating Layer relu1
I0521 06:35:46.795802  2420 net.cpp:454] relu1 <- conv1
I0521 06:35:46.795815  2420 net.cpp:397] relu1 -> conv1 (in-place)
I0521 06:35:46.796334  2420 net.cpp:150] Setting up relu1
I0521 06:35:46.796350  2420 net.cpp:157] Top shape: 760 12 120 48 (52531200)
I0521 06:35:46.796361  2420 net.cpp:165] Memory required for data: 439556640
I0521 06:35:46.796372  2420 layer_factory.hpp:77] Creating layer pool1
I0521 06:35:46.796388  2420 net.cpp:106] Creating Layer pool1
I0521 06:35:46.796399  2420 net.cpp:454] pool1 <- conv1
I0521 06:35:46.796413  2420 net.cpp:411] pool1 -> pool1
I0521 06:35:46.796492  2420 net.cpp:150] Setting up pool1
I0521 06:35:46.796507  2420 net.cpp:157] Top shape: 760 12 60 48 (26265600)
I0521 06:35:46.796517  2420 net.cpp:165] Memory required for data: 544619040
I0521 06:35:46.796527  2420 layer_factory.hpp:77] Creating layer conv2
I0521 06:35:46.796550  2420 net.cpp:106] Creating Layer conv2
I0521 06:35:46.796561  2420 net.cpp:454] conv2 <- pool1
I0521 06:35:46.796573  2420 net.cpp:411] conv2 -> conv2
I0521 06:35:46.799340  2420 net.cpp:150] Setting up conv2
I0521 06:35:46.799367  2420 net.cpp:157] Top shape: 760 20 54 46 (37756800)
I0521 06:35:46.799378  2420 net.cpp:165] Memory required for data: 695646240
I0521 06:35:46.799397  2420 layer_factory.hpp:77] Creating layer relu2
I0521 06:35:46.799412  2420 net.cpp:106] Creating Layer relu2
I0521 06:35:46.799422  2420 net.cpp:454] relu2 <- conv2
I0521 06:35:46.799435  2420 net.cpp:397] relu2 -> conv2 (in-place)
I0521 06:35:46.799767  2420 net.cpp:150] Setting up relu2
I0521 06:35:46.799780  2420 net.cpp:157] Top shape: 760 20 54 46 (37756800)
I0521 06:35:46.799790  2420 net.cpp:165] Memory required for data: 846673440
I0521 06:35:46.799801  2420 layer_factory.hpp:77] Creating layer pool2
I0521 06:35:46.799813  2420 net.cpp:106] Creating Layer pool2
I0521 06:35:46.799823  2420 net.cpp:454] pool2 <- conv2
I0521 06:35:46.799849  2420 net.cpp:411] pool2 -> pool2
I0521 06:35:46.799917  2420 net.cpp:150] Setting up pool2
I0521 06:35:46.799931  2420 net.cpp:157] Top shape: 760 20 27 46 (18878400)
I0521 06:35:46.799940  2420 net.cpp:165] Memory required for data: 922187040
I0521 06:35:46.799947  2420 layer_factory.hpp:77] Creating layer conv3
I0521 06:35:46.799964  2420 net.cpp:106] Creating Layer conv3
I0521 06:35:46.799975  2420 net.cpp:454] conv3 <- pool2
I0521 06:35:46.799988  2420 net.cpp:411] conv3 -> conv3
I0521 06:35:46.801909  2420 net.cpp:150] Setting up conv3
I0521 06:35:46.801933  2420 net.cpp:157] Top shape: 760 28 22 44 (20599040)
I0521 06:35:46.801945  2420 net.cpp:165] Memory required for data: 1004583200
I0521 06:35:46.801964  2420 layer_factory.hpp:77] Creating layer relu3
I0521 06:35:46.801980  2420 net.cpp:106] Creating Layer relu3
I0521 06:35:46.801990  2420 net.cpp:454] relu3 <- conv3
I0521 06:35:46.802002  2420 net.cpp:397] relu3 -> conv3 (in-place)
I0521 06:35:46.802489  2420 net.cpp:150] Setting up relu3
I0521 06:35:46.802507  2420 net.cpp:157] Top shape: 760 28 22 44 (20599040)
I0521 06:35:46.802518  2420 net.cpp:165] Memory required for data: 1086979360
I0521 06:35:46.802528  2420 layer_factory.hpp:77] Creating layer pool3
I0521 06:35:46.802541  2420 net.cpp:106] Creating Layer pool3
I0521 06:35:46.802551  2420 net.cpp:454] pool3 <- conv3
I0521 06:35:46.802564  2420 net.cpp:411] pool3 -> pool3
I0521 06:35:46.802633  2420 net.cpp:150] Setting up pool3
I0521 06:35:46.802645  2420 net.cpp:157] Top shape: 760 28 11 44 (10299520)
I0521 06:35:46.802655  2420 net.cpp:165] Memory required for data: 1128177440
I0521 06:35:46.802665  2420 layer_factory.hpp:77] Creating layer conv4
I0521 06:35:46.802682  2420 net.cpp:106] Creating Layer conv4
I0521 06:35:46.802693  2420 net.cpp:454] conv4 <- pool3
I0521 06:35:46.802707  2420 net.cpp:411] conv4 -> conv4
I0521 06:35:46.805474  2420 net.cpp:150] Setting up conv4
I0521 06:35:46.805502  2420 net.cpp:157] Top shape: 760 36 6 42 (6894720)
I0521 06:35:46.805512  2420 net.cpp:165] Memory required for data: 1155756320
I0521 06:35:46.805527  2420 layer_factory.hpp:77] Creating layer relu4
I0521 06:35:46.805541  2420 net.cpp:106] Creating Layer relu4
I0521 06:35:46.805552  2420 net.cpp:454] relu4 <- conv4
I0521 06:35:46.805565  2420 net.cpp:397] relu4 -> conv4 (in-place)
I0521 06:35:46.806035  2420 net.cpp:150] Setting up relu4
I0521 06:35:46.806051  2420 net.cpp:157] Top shape: 760 36 6 42 (6894720)
I0521 06:35:46.806062  2420 net.cpp:165] Memory required for data: 1183335200
I0521 06:35:46.806073  2420 layer_factory.hpp:77] Creating layer pool4
I0521 06:35:46.806087  2420 net.cpp:106] Creating Layer pool4
I0521 06:35:46.806097  2420 net.cpp:454] pool4 <- conv4
I0521 06:35:46.806108  2420 net.cpp:411] pool4 -> pool4
I0521 06:35:46.806176  2420 net.cpp:150] Setting up pool4
I0521 06:35:46.806190  2420 net.cpp:157] Top shape: 760 36 3 42 (3447360)
I0521 06:35:46.806201  2420 net.cpp:165] Memory required for data: 1197124640
I0521 06:35:46.806210  2420 layer_factory.hpp:77] Creating layer ip1
I0521 06:35:46.806231  2420 net.cpp:106] Creating Layer ip1
I0521 06:35:46.806241  2420 net.cpp:454] ip1 <- pool4
I0521 06:35:46.806255  2420 net.cpp:411] ip1 -> ip1
I0521 06:35:46.821689  2420 net.cpp:150] Setting up ip1
I0521 06:35:46.821718  2420 net.cpp:157] Top shape: 760 196 (148960)
I0521 06:35:46.821730  2420 net.cpp:165] Memory required for data: 1197720480
I0521 06:35:46.821753  2420 layer_factory.hpp:77] Creating layer relu5
I0521 06:35:46.821768  2420 net.cpp:106] Creating Layer relu5
I0521 06:35:46.821777  2420 net.cpp:454] relu5 <- ip1
I0521 06:35:46.821790  2420 net.cpp:397] relu5 -> ip1 (in-place)
I0521 06:35:46.822131  2420 net.cpp:150] Setting up relu5
I0521 06:35:46.822146  2420 net.cpp:157] Top shape: 760 196 (148960)
I0521 06:35:46.822156  2420 net.cpp:165] Memory required for data: 1198316320
I0521 06:35:46.822166  2420 layer_factory.hpp:77] Creating layer drop1
I0521 06:35:46.822187  2420 net.cpp:106] Creating Layer drop1
I0521 06:35:46.822197  2420 net.cpp:454] drop1 <- ip1
I0521 06:35:46.822223  2420 net.cpp:397] drop1 -> ip1 (in-place)
I0521 06:35:46.822278  2420 net.cpp:150] Setting up drop1
I0521 06:35:46.822290  2420 net.cpp:157] Top shape: 760 196 (148960)
I0521 06:35:46.822301  2420 net.cpp:165] Memory required for data: 1198912160
I0521 06:35:46.822310  2420 layer_factory.hpp:77] Creating layer ip2
I0521 06:35:46.822329  2420 net.cpp:106] Creating Layer ip2
I0521 06:35:46.822340  2420 net.cpp:454] ip2 <- ip1
I0521 06:35:46.822352  2420 net.cpp:411] ip2 -> ip2
I0521 06:35:46.822818  2420 net.cpp:150] Setting up ip2
I0521 06:35:46.822830  2420 net.cpp:157] Top shape: 760 98 (74480)
I0521 06:35:46.822840  2420 net.cpp:165] Memory required for data: 1199210080
I0521 06:35:46.822855  2420 layer_factory.hpp:77] Creating layer relu6
I0521 06:35:46.822868  2420 net.cpp:106] Creating Layer relu6
I0521 06:35:46.822877  2420 net.cpp:454] relu6 <- ip2
I0521 06:35:46.822890  2420 net.cpp:397] relu6 -> ip2 (in-place)
I0521 06:35:46.823407  2420 net.cpp:150] Setting up relu6
I0521 06:35:46.823423  2420 net.cpp:157] Top shape: 760 98 (74480)
I0521 06:35:46.823434  2420 net.cpp:165] Memory required for data: 1199508000
I0521 06:35:46.823444  2420 layer_factory.hpp:77] Creating layer drop2
I0521 06:35:46.823457  2420 net.cpp:106] Creating Layer drop2
I0521 06:35:46.823467  2420 net.cpp:454] drop2 <- ip2
I0521 06:35:46.823479  2420 net.cpp:397] drop2 -> ip2 (in-place)
I0521 06:35:46.823521  2420 net.cpp:150] Setting up drop2
I0521 06:35:46.823534  2420 net.cpp:157] Top shape: 760 98 (74480)
I0521 06:35:46.823544  2420 net.cpp:165] Memory required for data: 1199805920
I0521 06:35:46.823554  2420 layer_factory.hpp:77] Creating layer ip3
I0521 06:35:46.823567  2420 net.cpp:106] Creating Layer ip3
I0521 06:35:46.823577  2420 net.cpp:454] ip3 <- ip2
I0521 06:35:46.823591  2420 net.cpp:411] ip3 -> ip3
I0521 06:35:46.823801  2420 net.cpp:150] Setting up ip3
I0521 06:35:46.823813  2420 net.cpp:157] Top shape: 760 11 (8360)
I0521 06:35:46.823823  2420 net.cpp:165] Memory required for data: 1199839360
I0521 06:35:46.823838  2420 layer_factory.hpp:77] Creating layer drop3
I0521 06:35:46.823850  2420 net.cpp:106] Creating Layer drop3
I0521 06:35:46.823860  2420 net.cpp:454] drop3 <- ip3
I0521 06:35:46.823873  2420 net.cpp:397] drop3 -> ip3 (in-place)
I0521 06:35:46.823911  2420 net.cpp:150] Setting up drop3
I0521 06:35:46.823923  2420 net.cpp:157] Top shape: 760 11 (8360)
I0521 06:35:46.823931  2420 net.cpp:165] Memory required for data: 1199872800
I0521 06:35:46.823941  2420 layer_factory.hpp:77] Creating layer loss
I0521 06:35:46.823961  2420 net.cpp:106] Creating Layer loss
I0521 06:35:46.823971  2420 net.cpp:454] loss <- ip3
I0521 06:35:46.823982  2420 net.cpp:454] loss <- label
I0521 06:35:46.823994  2420 net.cpp:411] loss -> loss
I0521 06:35:46.824012  2420 layer_factory.hpp:77] Creating layer loss
I0521 06:35:46.824659  2420 net.cpp:150] Setting up loss
I0521 06:35:46.824681  2420 net.cpp:157] Top shape: (1)
I0521 06:35:46.824693  2420 net.cpp:160]     with loss weight 1
I0521 06:35:46.824736  2420 net.cpp:165] Memory required for data: 1199872804
I0521 06:35:46.824748  2420 net.cpp:226] loss needs backward computation.
I0521 06:35:46.824759  2420 net.cpp:226] drop3 needs backward computation.
I0521 06:35:46.824767  2420 net.cpp:226] ip3 needs backward computation.
I0521 06:35:46.824779  2420 net.cpp:226] drop2 needs backward computation.
I0521 06:35:46.824789  2420 net.cpp:226] relu6 needs backward computation.
I0521 06:35:46.824797  2420 net.cpp:226] ip2 needs backward computation.
I0521 06:35:46.824808  2420 net.cpp:226] drop1 needs backward computation.
I0521 06:35:46.824817  2420 net.cpp:226] relu5 needs backward computation.
I0521 06:35:46.824827  2420 net.cpp:226] ip1 needs backward computation.
I0521 06:35:46.824837  2420 net.cpp:226] pool4 needs backward computation.
I0521 06:35:46.824848  2420 net.cpp:226] relu4 needs backward computation.
I0521 06:35:46.824857  2420 net.cpp:226] conv4 needs backward computation.
I0521 06:35:46.824868  2420 net.cpp:226] pool3 needs backward computation.
I0521 06:35:46.824887  2420 net.cpp:226] relu3 needs backward computation.
I0521 06:35:46.824898  2420 net.cpp:226] conv3 needs backward computation.
I0521 06:35:46.824909  2420 net.cpp:226] pool2 needs backward computation.
I0521 06:35:46.824919  2420 net.cpp:226] relu2 needs backward computation.
I0521 06:35:46.824930  2420 net.cpp:226] conv2 needs backward computation.
I0521 06:35:46.824941  2420 net.cpp:226] pool1 needs backward computation.
I0521 06:35:46.824951  2420 net.cpp:226] relu1 needs backward computation.
I0521 06:35:46.824961  2420 net.cpp:226] conv1 needs backward computation.
I0521 06:35:46.824971  2420 net.cpp:228] data_hdf5 does not need backward computation.
I0521 06:35:46.824980  2420 net.cpp:270] This network produces output loss
I0521 06:35:46.825004  2420 net.cpp:283] Network initialization done.
I0521 06:35:46.826649  2420 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915.prototxt
I0521 06:35:46.826720  2420 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 06:35:46.827075  2420 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 760
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 06:35:46.827265  2420 layer_factory.hpp:77] Creating layer data_hdf5
I0521 06:35:46.827280  2420 net.cpp:106] Creating Layer data_hdf5
I0521 06:35:46.827292  2420 net.cpp:411] data_hdf5 -> data
I0521 06:35:46.827308  2420 net.cpp:411] data_hdf5 -> label
I0521 06:35:46.827324  2420 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 06:35:46.828522  2420 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 06:36:08.164464  2420 net.cpp:150] Setting up data_hdf5
I0521 06:36:08.164630  2420 net.cpp:157] Top shape: 760 1 127 50 (4826000)
I0521 06:36:08.164645  2420 net.cpp:157] Top shape: 760 (760)
I0521 06:36:08.164657  2420 net.cpp:165] Memory required for data: 19307040
I0521 06:36:08.164671  2420 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 06:36:08.164700  2420 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 06:36:08.164711  2420 net.cpp:454] label_data_hdf5_1_split <- label
I0521 06:36:08.164726  2420 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 06:36:08.164748  2420 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 06:36:08.164821  2420 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 06:36:08.164835  2420 net.cpp:157] Top shape: 760 (760)
I0521 06:36:08.164846  2420 net.cpp:157] Top shape: 760 (760)
I0521 06:36:08.164856  2420 net.cpp:165] Memory required for data: 19313120
I0521 06:36:08.164866  2420 layer_factory.hpp:77] Creating layer conv1
I0521 06:36:08.164888  2420 net.cpp:106] Creating Layer conv1
I0521 06:36:08.164898  2420 net.cpp:454] conv1 <- data
I0521 06:36:08.164913  2420 net.cpp:411] conv1 -> conv1
I0521 06:36:08.166856  2420 net.cpp:150] Setting up conv1
I0521 06:36:08.166879  2420 net.cpp:157] Top shape: 760 12 120 48 (52531200)
I0521 06:36:08.166892  2420 net.cpp:165] Memory required for data: 229437920
I0521 06:36:08.166913  2420 layer_factory.hpp:77] Creating layer relu1
I0521 06:36:08.166928  2420 net.cpp:106] Creating Layer relu1
I0521 06:36:08.166937  2420 net.cpp:454] relu1 <- conv1
I0521 06:36:08.166950  2420 net.cpp:397] relu1 -> conv1 (in-place)
I0521 06:36:08.167450  2420 net.cpp:150] Setting up relu1
I0521 06:36:08.167466  2420 net.cpp:157] Top shape: 760 12 120 48 (52531200)
I0521 06:36:08.167476  2420 net.cpp:165] Memory required for data: 439562720
I0521 06:36:08.167487  2420 layer_factory.hpp:77] Creating layer pool1
I0521 06:36:08.167503  2420 net.cpp:106] Creating Layer pool1
I0521 06:36:08.167513  2420 net.cpp:454] pool1 <- conv1
I0521 06:36:08.167526  2420 net.cpp:411] pool1 -> pool1
I0521 06:36:08.167601  2420 net.cpp:150] Setting up pool1
I0521 06:36:08.167614  2420 net.cpp:157] Top shape: 760 12 60 48 (26265600)
I0521 06:36:08.167624  2420 net.cpp:165] Memory required for data: 544625120
I0521 06:36:08.167634  2420 layer_factory.hpp:77] Creating layer conv2
I0521 06:36:08.167650  2420 net.cpp:106] Creating Layer conv2
I0521 06:36:08.167661  2420 net.cpp:454] conv2 <- pool1
I0521 06:36:08.167675  2420 net.cpp:411] conv2 -> conv2
I0521 06:36:08.169597  2420 net.cpp:150] Setting up conv2
I0521 06:36:08.169615  2420 net.cpp:157] Top shape: 760 20 54 46 (37756800)
I0521 06:36:08.169630  2420 net.cpp:165] Memory required for data: 695652320
I0521 06:36:08.169648  2420 layer_factory.hpp:77] Creating layer relu2
I0521 06:36:08.169663  2420 net.cpp:106] Creating Layer relu2
I0521 06:36:08.169672  2420 net.cpp:454] relu2 <- conv2
I0521 06:36:08.169685  2420 net.cpp:397] relu2 -> conv2 (in-place)
I0521 06:36:08.170019  2420 net.cpp:150] Setting up relu2
I0521 06:36:08.170033  2420 net.cpp:157] Top shape: 760 20 54 46 (37756800)
I0521 06:36:08.170043  2420 net.cpp:165] Memory required for data: 846679520
I0521 06:36:08.170053  2420 layer_factory.hpp:77] Creating layer pool2
I0521 06:36:08.170066  2420 net.cpp:106] Creating Layer pool2
I0521 06:36:08.170076  2420 net.cpp:454] pool2 <- conv2
I0521 06:36:08.170089  2420 net.cpp:411] pool2 -> pool2
I0521 06:36:08.170161  2420 net.cpp:150] Setting up pool2
I0521 06:36:08.170173  2420 net.cpp:157] Top shape: 760 20 27 46 (18878400)
I0521 06:36:08.170183  2420 net.cpp:165] Memory required for data: 922193120
I0521 06:36:08.170195  2420 layer_factory.hpp:77] Creating layer conv3
I0521 06:36:08.170212  2420 net.cpp:106] Creating Layer conv3
I0521 06:36:08.170222  2420 net.cpp:454] conv3 <- pool2
I0521 06:36:08.170238  2420 net.cpp:411] conv3 -> conv3
I0521 06:36:08.172219  2420 net.cpp:150] Setting up conv3
I0521 06:36:08.172237  2420 net.cpp:157] Top shape: 760 28 22 44 (20599040)
I0521 06:36:08.172248  2420 net.cpp:165] Memory required for data: 1004589280
I0521 06:36:08.172281  2420 layer_factory.hpp:77] Creating layer relu3
I0521 06:36:08.172294  2420 net.cpp:106] Creating Layer relu3
I0521 06:36:08.172305  2420 net.cpp:454] relu3 <- conv3
I0521 06:36:08.172318  2420 net.cpp:397] relu3 -> conv3 (in-place)
I0521 06:36:08.172791  2420 net.cpp:150] Setting up relu3
I0521 06:36:08.172807  2420 net.cpp:157] Top shape: 760 28 22 44 (20599040)
I0521 06:36:08.172817  2420 net.cpp:165] Memory required for data: 1086985440
I0521 06:36:08.172827  2420 layer_factory.hpp:77] Creating layer pool3
I0521 06:36:08.172840  2420 net.cpp:106] Creating Layer pool3
I0521 06:36:08.172850  2420 net.cpp:454] pool3 <- conv3
I0521 06:36:08.172863  2420 net.cpp:411] pool3 -> pool3
I0521 06:36:08.172935  2420 net.cpp:150] Setting up pool3
I0521 06:36:08.172950  2420 net.cpp:157] Top shape: 760 28 11 44 (10299520)
I0521 06:36:08.172958  2420 net.cpp:165] Memory required for data: 1128183520
I0521 06:36:08.172968  2420 layer_factory.hpp:77] Creating layer conv4
I0521 06:36:08.172986  2420 net.cpp:106] Creating Layer conv4
I0521 06:36:08.172996  2420 net.cpp:454] conv4 <- pool3
I0521 06:36:08.173010  2420 net.cpp:411] conv4 -> conv4
I0521 06:36:08.175097  2420 net.cpp:150] Setting up conv4
I0521 06:36:08.175119  2420 net.cpp:157] Top shape: 760 36 6 42 (6894720)
I0521 06:36:08.175132  2420 net.cpp:165] Memory required for data: 1155762400
I0521 06:36:08.175148  2420 layer_factory.hpp:77] Creating layer relu4
I0521 06:36:08.175161  2420 net.cpp:106] Creating Layer relu4
I0521 06:36:08.175171  2420 net.cpp:454] relu4 <- conv4
I0521 06:36:08.175184  2420 net.cpp:397] relu4 -> conv4 (in-place)
I0521 06:36:08.175660  2420 net.cpp:150] Setting up relu4
I0521 06:36:08.175676  2420 net.cpp:157] Top shape: 760 36 6 42 (6894720)
I0521 06:36:08.175686  2420 net.cpp:165] Memory required for data: 1183341280
I0521 06:36:08.175696  2420 layer_factory.hpp:77] Creating layer pool4
I0521 06:36:08.175709  2420 net.cpp:106] Creating Layer pool4
I0521 06:36:08.175719  2420 net.cpp:454] pool4 <- conv4
I0521 06:36:08.175734  2420 net.cpp:411] pool4 -> pool4
I0521 06:36:08.175806  2420 net.cpp:150] Setting up pool4
I0521 06:36:08.175818  2420 net.cpp:157] Top shape: 760 36 3 42 (3447360)
I0521 06:36:08.175828  2420 net.cpp:165] Memory required for data: 1197130720
I0521 06:36:08.175838  2420 layer_factory.hpp:77] Creating layer ip1
I0521 06:36:08.175853  2420 net.cpp:106] Creating Layer ip1
I0521 06:36:08.175863  2420 net.cpp:454] ip1 <- pool4
I0521 06:36:08.175878  2420 net.cpp:411] ip1 -> ip1
I0521 06:36:08.191412  2420 net.cpp:150] Setting up ip1
I0521 06:36:08.191442  2420 net.cpp:157] Top shape: 760 196 (148960)
I0521 06:36:08.191452  2420 net.cpp:165] Memory required for data: 1197726560
I0521 06:36:08.191475  2420 layer_factory.hpp:77] Creating layer relu5
I0521 06:36:08.191490  2420 net.cpp:106] Creating Layer relu5
I0521 06:36:08.191501  2420 net.cpp:454] relu5 <- ip1
I0521 06:36:08.191514  2420 net.cpp:397] relu5 -> ip1 (in-place)
I0521 06:36:08.191864  2420 net.cpp:150] Setting up relu5
I0521 06:36:08.191877  2420 net.cpp:157] Top shape: 760 196 (148960)
I0521 06:36:08.191886  2420 net.cpp:165] Memory required for data: 1198322400
I0521 06:36:08.191896  2420 layer_factory.hpp:77] Creating layer drop1
I0521 06:36:08.191915  2420 net.cpp:106] Creating Layer drop1
I0521 06:36:08.191926  2420 net.cpp:454] drop1 <- ip1
I0521 06:36:08.191938  2420 net.cpp:397] drop1 -> ip1 (in-place)
I0521 06:36:08.191982  2420 net.cpp:150] Setting up drop1
I0521 06:36:08.191995  2420 net.cpp:157] Top shape: 760 196 (148960)
I0521 06:36:08.192005  2420 net.cpp:165] Memory required for data: 1198918240
I0521 06:36:08.192014  2420 layer_factory.hpp:77] Creating layer ip2
I0521 06:36:08.192029  2420 net.cpp:106] Creating Layer ip2
I0521 06:36:08.192039  2420 net.cpp:454] ip2 <- ip1
I0521 06:36:08.192052  2420 net.cpp:411] ip2 -> ip2
I0521 06:36:08.192534  2420 net.cpp:150] Setting up ip2
I0521 06:36:08.192548  2420 net.cpp:157] Top shape: 760 98 (74480)
I0521 06:36:08.192558  2420 net.cpp:165] Memory required for data: 1199216160
I0521 06:36:08.192586  2420 layer_factory.hpp:77] Creating layer relu6
I0521 06:36:08.192598  2420 net.cpp:106] Creating Layer relu6
I0521 06:36:08.192610  2420 net.cpp:454] relu6 <- ip2
I0521 06:36:08.192621  2420 net.cpp:397] relu6 -> ip2 (in-place)
I0521 06:36:08.193156  2420 net.cpp:150] Setting up relu6
I0521 06:36:08.193171  2420 net.cpp:157] Top shape: 760 98 (74480)
I0521 06:36:08.193181  2420 net.cpp:165] Memory required for data: 1199514080
I0521 06:36:08.193192  2420 layer_factory.hpp:77] Creating layer drop2
I0521 06:36:08.193204  2420 net.cpp:106] Creating Layer drop2
I0521 06:36:08.193214  2420 net.cpp:454] drop2 <- ip2
I0521 06:36:08.193228  2420 net.cpp:397] drop2 -> ip2 (in-place)
I0521 06:36:08.193271  2420 net.cpp:150] Setting up drop2
I0521 06:36:08.193284  2420 net.cpp:157] Top shape: 760 98 (74480)
I0521 06:36:08.193295  2420 net.cpp:165] Memory required for data: 1199812000
I0521 06:36:08.193305  2420 layer_factory.hpp:77] Creating layer ip3
I0521 06:36:08.193318  2420 net.cpp:106] Creating Layer ip3
I0521 06:36:08.193328  2420 net.cpp:454] ip3 <- ip2
I0521 06:36:08.193342  2420 net.cpp:411] ip3 -> ip3
I0521 06:36:08.193563  2420 net.cpp:150] Setting up ip3
I0521 06:36:08.193578  2420 net.cpp:157] Top shape: 760 11 (8360)
I0521 06:36:08.193588  2420 net.cpp:165] Memory required for data: 1199845440
I0521 06:36:08.193603  2420 layer_factory.hpp:77] Creating layer drop3
I0521 06:36:08.193615  2420 net.cpp:106] Creating Layer drop3
I0521 06:36:08.193625  2420 net.cpp:454] drop3 <- ip3
I0521 06:36:08.193637  2420 net.cpp:397] drop3 -> ip3 (in-place)
I0521 06:36:08.193678  2420 net.cpp:150] Setting up drop3
I0521 06:36:08.193691  2420 net.cpp:157] Top shape: 760 11 (8360)
I0521 06:36:08.193701  2420 net.cpp:165] Memory required for data: 1199878880
I0521 06:36:08.193711  2420 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 06:36:08.193723  2420 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 06:36:08.193732  2420 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 06:36:08.193745  2420 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 06:36:08.193759  2420 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 06:36:08.193832  2420 net.cpp:150] Setting up ip3_drop3_0_split
I0521 06:36:08.193845  2420 net.cpp:157] Top shape: 760 11 (8360)
I0521 06:36:08.193857  2420 net.cpp:157] Top shape: 760 11 (8360)
I0521 06:36:08.193867  2420 net.cpp:165] Memory required for data: 1199945760
I0521 06:36:08.193876  2420 layer_factory.hpp:77] Creating layer accuracy
I0521 06:36:08.193897  2420 net.cpp:106] Creating Layer accuracy
I0521 06:36:08.193908  2420 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 06:36:08.193919  2420 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 06:36:08.193933  2420 net.cpp:411] accuracy -> accuracy
I0521 06:36:08.193956  2420 net.cpp:150] Setting up accuracy
I0521 06:36:08.193970  2420 net.cpp:157] Top shape: (1)
I0521 06:36:08.193980  2420 net.cpp:165] Memory required for data: 1199945764
I0521 06:36:08.193990  2420 layer_factory.hpp:77] Creating layer loss
I0521 06:36:08.194005  2420 net.cpp:106] Creating Layer loss
I0521 06:36:08.194015  2420 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 06:36:08.194025  2420 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 06:36:08.194037  2420 net.cpp:411] loss -> loss
I0521 06:36:08.194056  2420 layer_factory.hpp:77] Creating layer loss
I0521 06:36:08.194556  2420 net.cpp:150] Setting up loss
I0521 06:36:08.194569  2420 net.cpp:157] Top shape: (1)
I0521 06:36:08.194579  2420 net.cpp:160]     with loss weight 1
I0521 06:36:08.194597  2420 net.cpp:165] Memory required for data: 1199945768
I0521 06:36:08.194608  2420 net.cpp:226] loss needs backward computation.
I0521 06:36:08.194618  2420 net.cpp:228] accuracy does not need backward computation.
I0521 06:36:08.194630  2420 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 06:36:08.194640  2420 net.cpp:226] drop3 needs backward computation.
I0521 06:36:08.194650  2420 net.cpp:226] ip3 needs backward computation.
I0521 06:36:08.194660  2420 net.cpp:226] drop2 needs backward computation.
I0521 06:36:08.194679  2420 net.cpp:226] relu6 needs backward computation.
I0521 06:36:08.194689  2420 net.cpp:226] ip2 needs backward computation.
I0521 06:36:08.194700  2420 net.cpp:226] drop1 needs backward computation.
I0521 06:36:08.194708  2420 net.cpp:226] relu5 needs backward computation.
I0521 06:36:08.194718  2420 net.cpp:226] ip1 needs backward computation.
I0521 06:36:08.194728  2420 net.cpp:226] pool4 needs backward computation.
I0521 06:36:08.194738  2420 net.cpp:226] relu4 needs backward computation.
I0521 06:36:08.194748  2420 net.cpp:226] conv4 needs backward computation.
I0521 06:36:08.194758  2420 net.cpp:226] pool3 needs backward computation.
I0521 06:36:08.194767  2420 net.cpp:226] relu3 needs backward computation.
I0521 06:36:08.194777  2420 net.cpp:226] conv3 needs backward computation.
I0521 06:36:08.194788  2420 net.cpp:226] pool2 needs backward computation.
I0521 06:36:08.194798  2420 net.cpp:226] relu2 needs backward computation.
I0521 06:36:08.194808  2420 net.cpp:226] conv2 needs backward computation.
I0521 06:36:08.194818  2420 net.cpp:226] pool1 needs backward computation.
I0521 06:36:08.194828  2420 net.cpp:226] relu1 needs backward computation.
I0521 06:36:08.194839  2420 net.cpp:226] conv1 needs backward computation.
I0521 06:36:08.194849  2420 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 06:36:08.194860  2420 net.cpp:228] data_hdf5 does not need backward computation.
I0521 06:36:08.194870  2420 net.cpp:270] This network produces output accuracy
I0521 06:36:08.194880  2420 net.cpp:270] This network produces output loss
I0521 06:36:08.194910  2420 net.cpp:283] Network initialization done.
I0521 06:36:08.195044  2420 solver.cpp:60] Solver scaffolding done.
I0521 06:36:08.196183  2420 caffe.cpp:212] Starting Optimization
I0521 06:36:08.196197  2420 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 06:36:08.196207  2420 solver.cpp:289] Learning Rate Policy: fixed
I0521 06:36:08.197430  2420 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 06:36:54.070895  2420 solver.cpp:409]     Test net output #0: accuracy = 0.082347
I0521 06:36:54.071056  2420 solver.cpp:409]     Test net output #1: loss = 2.39737 (* 1 = 2.39737 loss)
I0521 06:36:54.212296  2420 solver.cpp:237] Iteration 0, loss = 2.3981
I0521 06:36:54.212332  2420 solver.cpp:253]     Train net output #0: loss = 2.3981 (* 1 = 2.3981 loss)
I0521 06:36:54.212349  2420 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 06:37:01.947451  2420 solver.cpp:237] Iteration 19, loss = 2.38549
I0521 06:37:01.947491  2420 solver.cpp:253]     Train net output #0: loss = 2.38549 (* 1 = 2.38549 loss)
I0521 06:37:01.947512  2420 sgd_solver.cpp:106] Iteration 19, lr = 0.0025
I0521 06:37:09.680595  2420 solver.cpp:237] Iteration 38, loss = 2.37124
I0521 06:37:09.680629  2420 solver.cpp:253]     Train net output #0: loss = 2.37124 (* 1 = 2.37124 loss)
I0521 06:37:09.680645  2420 sgd_solver.cpp:106] Iteration 38, lr = 0.0025
I0521 06:37:17.414243  2420 solver.cpp:237] Iteration 57, loss = 2.35589
I0521 06:37:17.414280  2420 solver.cpp:253]     Train net output #0: loss = 2.35589 (* 1 = 2.35589 loss)
I0521 06:37:17.414294  2420 sgd_solver.cpp:106] Iteration 57, lr = 0.0025
I0521 06:37:25.150161  2420 solver.cpp:237] Iteration 76, loss = 2.34413
I0521 06:37:25.150315  2420 solver.cpp:253]     Train net output #0: loss = 2.34413 (* 1 = 2.34413 loss)
I0521 06:37:25.150327  2420 sgd_solver.cpp:106] Iteration 76, lr = 0.0025
I0521 06:37:32.884495  2420 solver.cpp:237] Iteration 95, loss = 2.33625
I0521 06:37:32.884526  2420 solver.cpp:253]     Train net output #0: loss = 2.33625 (* 1 = 2.33625 loss)
I0521 06:37:32.884543  2420 sgd_solver.cpp:106] Iteration 95, lr = 0.0025
I0521 06:37:40.616286  2420 solver.cpp:237] Iteration 114, loss = 2.33063
I0521 06:37:40.616320  2420 solver.cpp:253]     Train net output #0: loss = 2.33063 (* 1 = 2.33063 loss)
I0521 06:37:40.616336  2420 sgd_solver.cpp:106] Iteration 114, lr = 0.0025
I0521 06:38:10.475358  2420 solver.cpp:237] Iteration 133, loss = 2.32395
I0521 06:38:10.475517  2420 solver.cpp:253]     Train net output #0: loss = 2.32395 (* 1 = 2.32395 loss)
I0521 06:38:10.475533  2420 sgd_solver.cpp:106] Iteration 133, lr = 0.0025
I0521 06:38:18.213912  2420 solver.cpp:237] Iteration 152, loss = 2.31575
I0521 06:38:18.213944  2420 solver.cpp:253]     Train net output #0: loss = 2.31575 (* 1 = 2.31575 loss)
I0521 06:38:18.213969  2420 sgd_solver.cpp:106] Iteration 152, lr = 0.0025
I0521 06:38:25.949931  2420 solver.cpp:237] Iteration 171, loss = 2.32282
I0521 06:38:25.949965  2420 solver.cpp:253]     Train net output #0: loss = 2.32282 (* 1 = 2.32282 loss)
I0521 06:38:25.949981  2420 sgd_solver.cpp:106] Iteration 171, lr = 0.0025
I0521 06:38:33.688364  2420 solver.cpp:237] Iteration 190, loss = 2.29176
I0521 06:38:33.688397  2420 solver.cpp:253]     Train net output #0: loss = 2.29176 (* 1 = 2.29176 loss)
I0521 06:38:33.688411  2420 sgd_solver.cpp:106] Iteration 190, lr = 0.0025
I0521 06:38:36.133260  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_197.caffemodel
I0521 06:38:36.459764  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_197.solverstate
I0521 06:38:41.494597  2420 solver.cpp:237] Iteration 209, loss = 2.31603
I0521 06:38:41.494762  2420 solver.cpp:253]     Train net output #0: loss = 2.31603 (* 1 = 2.31603 loss)
I0521 06:38:41.494777  2420 sgd_solver.cpp:106] Iteration 209, lr = 0.0025
I0521 06:38:49.224951  2420 solver.cpp:237] Iteration 228, loss = 2.28544
I0521 06:38:49.224983  2420 solver.cpp:253]     Train net output #0: loss = 2.28544 (* 1 = 2.28544 loss)
I0521 06:38:49.225003  2420 sgd_solver.cpp:106] Iteration 228, lr = 0.0025
I0521 06:38:56.961052  2420 solver.cpp:237] Iteration 247, loss = 2.30144
I0521 06:38:56.961086  2420 solver.cpp:253]     Train net output #0: loss = 2.30144 (* 1 = 2.30144 loss)
I0521 06:38:56.961103  2420 sgd_solver.cpp:106] Iteration 247, lr = 0.0025
I0521 06:39:26.810515  2420 solver.cpp:237] Iteration 266, loss = 2.27131
I0521 06:39:26.810669  2420 solver.cpp:253]     Train net output #0: loss = 2.27131 (* 1 = 2.27131 loss)
I0521 06:39:26.810683  2420 sgd_solver.cpp:106] Iteration 266, lr = 0.0025
I0521 06:39:34.545567  2420 solver.cpp:237] Iteration 285, loss = 2.28364
I0521 06:39:34.545603  2420 solver.cpp:253]     Train net output #0: loss = 2.28364 (* 1 = 2.28364 loss)
I0521 06:39:34.545624  2420 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 06:39:42.282104  2420 solver.cpp:237] Iteration 304, loss = 2.26049
I0521 06:39:42.282136  2420 solver.cpp:253]     Train net output #0: loss = 2.26049 (* 1 = 2.26049 loss)
I0521 06:39:42.282152  2420 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0521 06:39:50.021595  2420 solver.cpp:237] Iteration 323, loss = 2.23392
I0521 06:39:50.021627  2420 solver.cpp:253]     Train net output #0: loss = 2.23392 (* 1 = 2.23392 loss)
I0521 06:39:50.021641  2420 sgd_solver.cpp:106] Iteration 323, lr = 0.0025
I0521 06:39:57.762308  2420 solver.cpp:237] Iteration 342, loss = 2.21128
I0521 06:39:57.762466  2420 solver.cpp:253]     Train net output #0: loss = 2.21128 (* 1 = 2.21128 loss)
I0521 06:39:57.762480  2420 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0521 06:40:05.495281  2420 solver.cpp:237] Iteration 361, loss = 2.17518
I0521 06:40:05.495314  2420 solver.cpp:253]     Train net output #0: loss = 2.17518 (* 1 = 2.17518 loss)
I0521 06:40:05.495332  2420 sgd_solver.cpp:106] Iteration 361, lr = 0.0025
I0521 06:40:13.230399  2420 solver.cpp:237] Iteration 380, loss = 2.19343
I0521 06:40:13.230432  2420 solver.cpp:253]     Train net output #0: loss = 2.19343 (* 1 = 2.19343 loss)
I0521 06:40:13.230449  2420 sgd_solver.cpp:106] Iteration 380, lr = 0.0025
I0521 06:40:18.525243  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_394.caffemodel
I0521 06:40:18.849000  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_394.solverstate
I0521 06:40:18.874354  2420 solver.cpp:341] Iteration 394, Testing net (#0)
I0521 06:41:03.863028  2420 solver.cpp:409]     Test net output #0: accuracy = 0.440188
I0521 06:41:03.863198  2420 solver.cpp:409]     Test net output #1: loss = 2.00423 (* 1 = 2.00423 loss)
I0521 06:41:28.183043  2420 solver.cpp:237] Iteration 399, loss = 2.16951
I0521 06:41:28.183094  2420 solver.cpp:253]     Train net output #0: loss = 2.16951 (* 1 = 2.16951 loss)
I0521 06:41:28.183110  2420 sgd_solver.cpp:106] Iteration 399, lr = 0.0025
I0521 06:41:35.916224  2420 solver.cpp:237] Iteration 418, loss = 2.09997
I0521 06:41:35.916379  2420 solver.cpp:253]     Train net output #0: loss = 2.09997 (* 1 = 2.09997 loss)
I0521 06:41:35.916393  2420 sgd_solver.cpp:106] Iteration 418, lr = 0.0025
I0521 06:41:43.646697  2420 solver.cpp:237] Iteration 437, loss = 2.09689
I0521 06:41:43.646730  2420 solver.cpp:253]     Train net output #0: loss = 2.09689 (* 1 = 2.09689 loss)
I0521 06:41:43.646747  2420 sgd_solver.cpp:106] Iteration 437, lr = 0.0025
I0521 06:41:51.379060  2420 solver.cpp:237] Iteration 456, loss = 2.0721
I0521 06:41:51.379093  2420 solver.cpp:253]     Train net output #0: loss = 2.0721 (* 1 = 2.0721 loss)
I0521 06:41:51.379109  2420 sgd_solver.cpp:106] Iteration 456, lr = 0.0025
I0521 06:41:59.109767  2420 solver.cpp:237] Iteration 475, loss = 2.04855
I0521 06:41:59.109814  2420 solver.cpp:253]     Train net output #0: loss = 2.04855 (* 1 = 2.04855 loss)
I0521 06:41:59.109828  2420 sgd_solver.cpp:106] Iteration 475, lr = 0.0025
I0521 06:42:06.835558  2420 solver.cpp:237] Iteration 494, loss = 2.01845
I0521 06:42:06.835703  2420 solver.cpp:253]     Train net output #0: loss = 2.01845 (* 1 = 2.01845 loss)
I0521 06:42:06.835716  2420 sgd_solver.cpp:106] Iteration 494, lr = 0.0025
I0521 06:42:14.567123  2420 solver.cpp:237] Iteration 513, loss = 2.03696
I0521 06:42:14.567155  2420 solver.cpp:253]     Train net output #0: loss = 2.03696 (* 1 = 2.03696 loss)
I0521 06:42:14.567173  2420 sgd_solver.cpp:106] Iteration 513, lr = 0.0025
I0521 06:42:44.488034  2420 solver.cpp:237] Iteration 532, loss = 2.04767
I0521 06:42:44.488209  2420 solver.cpp:253]     Train net output #0: loss = 2.04767 (* 1 = 2.04767 loss)
I0521 06:42:44.488224  2420 sgd_solver.cpp:106] Iteration 532, lr = 0.0025
I0521 06:42:52.214762  2420 solver.cpp:237] Iteration 551, loss = 1.96597
I0521 06:42:52.214802  2420 solver.cpp:253]     Train net output #0: loss = 1.96597 (* 1 = 1.96597 loss)
I0521 06:42:52.214823  2420 sgd_solver.cpp:106] Iteration 551, lr = 0.0025
I0521 06:42:59.939687  2420 solver.cpp:237] Iteration 570, loss = 2.04696
I0521 06:42:59.939720  2420 solver.cpp:253]     Train net output #0: loss = 2.04696 (* 1 = 2.04696 loss)
I0521 06:42:59.939736  2420 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 06:43:07.665824  2420 solver.cpp:237] Iteration 589, loss = 1.99184
I0521 06:43:07.665856  2420 solver.cpp:253]     Train net output #0: loss = 1.99184 (* 1 = 1.99184 loss)
I0521 06:43:07.665873  2420 sgd_solver.cpp:106] Iteration 589, lr = 0.0025
I0521 06:43:08.073256  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_591.caffemodel
I0521 06:43:08.399150  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_591.solverstate
I0521 06:43:15.462939  2420 solver.cpp:237] Iteration 608, loss = 1.98137
I0521 06:43:15.463098  2420 solver.cpp:253]     Train net output #0: loss = 1.98137 (* 1 = 1.98137 loss)
I0521 06:43:15.463112  2420 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 06:43:23.192775  2420 solver.cpp:237] Iteration 627, loss = 1.92396
I0521 06:43:23.192808  2420 solver.cpp:253]     Train net output #0: loss = 1.92396 (* 1 = 1.92396 loss)
I0521 06:43:23.192826  2420 sgd_solver.cpp:106] Iteration 627, lr = 0.0025
I0521 06:43:30.928448  2420 solver.cpp:237] Iteration 646, loss = 1.90842
I0521 06:43:30.928481  2420 solver.cpp:253]     Train net output #0: loss = 1.90842 (* 1 = 1.90842 loss)
I0521 06:43:30.928498  2420 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0521 06:44:00.843125  2420 solver.cpp:237] Iteration 665, loss = 1.93509
I0521 06:44:00.843291  2420 solver.cpp:253]     Train net output #0: loss = 1.93509 (* 1 = 1.93509 loss)
I0521 06:44:00.843304  2420 sgd_solver.cpp:106] Iteration 665, lr = 0.0025
I0521 06:44:08.574985  2420 solver.cpp:237] Iteration 684, loss = 1.8912
I0521 06:44:08.575016  2420 solver.cpp:253]     Train net output #0: loss = 1.8912 (* 1 = 1.8912 loss)
I0521 06:44:08.575040  2420 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0521 06:44:16.308234  2420 solver.cpp:237] Iteration 703, loss = 1.93842
I0521 06:44:16.308269  2420 solver.cpp:253]     Train net output #0: loss = 1.93842 (* 1 = 1.93842 loss)
I0521 06:44:16.308286  2420 sgd_solver.cpp:106] Iteration 703, lr = 0.0025
I0521 06:44:24.041106  2420 solver.cpp:237] Iteration 722, loss = 1.89039
I0521 06:44:24.041139  2420 solver.cpp:253]     Train net output #0: loss = 1.89039 (* 1 = 1.89039 loss)
I0521 06:44:24.041157  2420 sgd_solver.cpp:106] Iteration 722, lr = 0.0025
I0521 06:44:31.771800  2420 solver.cpp:237] Iteration 741, loss = 1.92084
I0521 06:44:31.771937  2420 solver.cpp:253]     Train net output #0: loss = 1.92084 (* 1 = 1.92084 loss)
I0521 06:44:31.771950  2420 sgd_solver.cpp:106] Iteration 741, lr = 0.0025
I0521 06:44:39.499155  2420 solver.cpp:237] Iteration 760, loss = 1.87705
I0521 06:44:39.499186  2420 solver.cpp:253]     Train net output #0: loss = 1.87705 (* 1 = 1.87705 loss)
I0521 06:44:39.499202  2420 sgd_solver.cpp:106] Iteration 760, lr = 0.0025
I0521 06:44:47.223634  2420 solver.cpp:237] Iteration 779, loss = 1.82818
I0521 06:44:47.223667  2420 solver.cpp:253]     Train net output #0: loss = 1.82818 (* 1 = 1.82818 loss)
I0521 06:44:47.223681  2420 sgd_solver.cpp:106] Iteration 779, lr = 0.0025
I0521 06:44:50.480151  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_788.caffemodel
I0521 06:44:50.806717  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_788.solverstate
I0521 06:44:50.833379  2420 solver.cpp:341] Iteration 788, Testing net (#0)
I0521 06:45:56.673077  2420 solver.cpp:409]     Test net output #0: accuracy = 0.594497
I0521 06:45:56.673249  2420 solver.cpp:409]     Test net output #1: loss = 1.47629 (* 1 = 1.47629 loss)
I0521 06:46:23.044375  2420 solver.cpp:237] Iteration 798, loss = 1.83168
I0521 06:46:23.044425  2420 solver.cpp:253]     Train net output #0: loss = 1.83168 (* 1 = 1.83168 loss)
I0521 06:46:23.044442  2420 sgd_solver.cpp:106] Iteration 798, lr = 0.0025
I0521 06:46:30.776849  2420 solver.cpp:237] Iteration 817, loss = 1.87881
I0521 06:46:30.777005  2420 solver.cpp:253]     Train net output #0: loss = 1.87881 (* 1 = 1.87881 loss)
I0521 06:46:30.777019  2420 sgd_solver.cpp:106] Iteration 817, lr = 0.0025
I0521 06:46:38.507665  2420 solver.cpp:237] Iteration 836, loss = 1.86215
I0521 06:46:38.507709  2420 solver.cpp:253]     Train net output #0: loss = 1.86215 (* 1 = 1.86215 loss)
I0521 06:46:38.507727  2420 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0521 06:46:46.239006  2420 solver.cpp:237] Iteration 855, loss = 1.83657
I0521 06:46:46.239037  2420 solver.cpp:253]     Train net output #0: loss = 1.83657 (* 1 = 1.83657 loss)
I0521 06:46:46.239054  2420 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 06:46:53.968760  2420 solver.cpp:237] Iteration 874, loss = 1.84258
I0521 06:46:53.968792  2420 solver.cpp:253]     Train net output #0: loss = 1.84258 (* 1 = 1.84258 loss)
I0521 06:46:53.968809  2420 sgd_solver.cpp:106] Iteration 874, lr = 0.0025
I0521 06:47:01.696053  2420 solver.cpp:237] Iteration 893, loss = 1.83998
I0521 06:47:01.696214  2420 solver.cpp:253]     Train net output #0: loss = 1.83998 (* 1 = 1.83998 loss)
I0521 06:47:01.696229  2420 sgd_solver.cpp:106] Iteration 893, lr = 0.0025
I0521 06:47:09.426017  2420 solver.cpp:237] Iteration 912, loss = 1.77363
I0521 06:47:09.426048  2420 solver.cpp:253]     Train net output #0: loss = 1.77363 (* 1 = 1.77363 loss)
I0521 06:47:09.426065  2420 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 06:47:39.320528  2420 solver.cpp:237] Iteration 931, loss = 1.83735
I0521 06:47:39.320690  2420 solver.cpp:253]     Train net output #0: loss = 1.83735 (* 1 = 1.83735 loss)
I0521 06:47:39.320704  2420 sgd_solver.cpp:106] Iteration 931, lr = 0.0025
I0521 06:47:47.050351  2420 solver.cpp:237] Iteration 950, loss = 1.96008
I0521 06:47:47.050384  2420 solver.cpp:253]     Train net output #0: loss = 1.96008 (* 1 = 1.96008 loss)
I0521 06:47:47.050400  2420 sgd_solver.cpp:106] Iteration 950, lr = 0.0025
I0521 06:47:54.779238  2420 solver.cpp:237] Iteration 969, loss = 1.81782
I0521 06:47:54.779279  2420 solver.cpp:253]     Train net output #0: loss = 1.81782 (* 1 = 1.81782 loss)
I0521 06:47:54.779294  2420 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0521 06:48:00.878073  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_985.caffemodel
I0521 06:48:01.203719  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_985.solverstate
I0521 06:48:02.571774  2420 solver.cpp:237] Iteration 988, loss = 1.82505
I0521 06:48:02.571822  2420 solver.cpp:253]     Train net output #0: loss = 1.82505 (* 1 = 1.82505 loss)
I0521 06:48:02.571837  2420 sgd_solver.cpp:106] Iteration 988, lr = 0.0025
I0521 06:48:10.302999  2420 solver.cpp:237] Iteration 1007, loss = 1.91283
I0521 06:48:10.303153  2420 solver.cpp:253]     Train net output #0: loss = 1.91283 (* 1 = 1.91283 loss)
I0521 06:48:10.303166  2420 sgd_solver.cpp:106] Iteration 1007, lr = 0.0025
I0521 06:48:18.034123  2420 solver.cpp:237] Iteration 1026, loss = 1.80831
I0521 06:48:18.034170  2420 solver.cpp:253]     Train net output #0: loss = 1.80831 (* 1 = 1.80831 loss)
I0521 06:48:18.034188  2420 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 06:48:25.761126  2420 solver.cpp:237] Iteration 1045, loss = 1.87433
I0521 06:48:25.761159  2420 solver.cpp:253]     Train net output #0: loss = 1.87433 (* 1 = 1.87433 loss)
I0521 06:48:25.761175  2420 sgd_solver.cpp:106] Iteration 1045, lr = 0.0025
I0521 06:48:55.641592  2420 solver.cpp:237] Iteration 1064, loss = 1.81547
I0521 06:48:55.641754  2420 solver.cpp:253]     Train net output #0: loss = 1.81547 (* 1 = 1.81547 loss)
I0521 06:48:55.641768  2420 sgd_solver.cpp:106] Iteration 1064, lr = 0.0025
I0521 06:49:03.368934  2420 solver.cpp:237] Iteration 1083, loss = 1.76994
I0521 06:49:03.368968  2420 solver.cpp:253]     Train net output #0: loss = 1.76994 (* 1 = 1.76994 loss)
I0521 06:49:03.368985  2420 sgd_solver.cpp:106] Iteration 1083, lr = 0.0025
I0521 06:49:11.101506  2420 solver.cpp:237] Iteration 1102, loss = 1.81507
I0521 06:49:11.101542  2420 solver.cpp:253]     Train net output #0: loss = 1.81507 (* 1 = 1.81507 loss)
I0521 06:49:11.101564  2420 sgd_solver.cpp:106] Iteration 1102, lr = 0.0025
I0521 06:49:18.829705  2420 solver.cpp:237] Iteration 1121, loss = 1.7929
I0521 06:49:18.829738  2420 solver.cpp:253]     Train net output #0: loss = 1.7929 (* 1 = 1.7929 loss)
I0521 06:49:18.829752  2420 sgd_solver.cpp:106] Iteration 1121, lr = 0.0025
I0521 06:49:26.557240  2420 solver.cpp:237] Iteration 1140, loss = 1.93879
I0521 06:49:26.557375  2420 solver.cpp:253]     Train net output #0: loss = 1.93879 (* 1 = 1.93879 loss)
I0521 06:49:26.557389  2420 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 06:49:34.281529  2420 solver.cpp:237] Iteration 1159, loss = 1.81637
I0521 06:49:34.281569  2420 solver.cpp:253]     Train net output #0: loss = 1.81637 (* 1 = 1.81637 loss)
I0521 06:49:34.281589  2420 sgd_solver.cpp:106] Iteration 1159, lr = 0.0025
I0521 06:49:42.008787  2420 solver.cpp:237] Iteration 1178, loss = 1.73727
I0521 06:49:42.008821  2420 solver.cpp:253]     Train net output #0: loss = 1.73727 (* 1 = 1.73727 loss)
I0521 06:49:42.008839  2420 sgd_solver.cpp:106] Iteration 1178, lr = 0.0025
I0521 06:49:43.228102  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1182.caffemodel
I0521 06:49:43.551940  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1182.solverstate
I0521 06:49:43.577926  2420 solver.cpp:341] Iteration 1182, Testing net (#0)
I0521 06:50:28.214370  2420 solver.cpp:409]     Test net output #0: accuracy = 0.631151
I0521 06:50:28.214530  2420 solver.cpp:409]     Test net output #1: loss = 1.29762 (* 1 = 1.29762 loss)
I0521 06:50:56.621104  2420 solver.cpp:237] Iteration 1197, loss = 1.76143
I0521 06:50:56.621155  2420 solver.cpp:253]     Train net output #0: loss = 1.76143 (* 1 = 1.76143 loss)
I0521 06:50:56.621170  2420 sgd_solver.cpp:106] Iteration 1197, lr = 0.0025
I0521 06:51:04.352246  2420 solver.cpp:237] Iteration 1216, loss = 1.75662
I0521 06:51:04.352397  2420 solver.cpp:253]     Train net output #0: loss = 1.75662 (* 1 = 1.75662 loss)
I0521 06:51:04.352411  2420 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 06:51:12.084197  2420 solver.cpp:237] Iteration 1235, loss = 1.77803
I0521 06:51:12.084234  2420 solver.cpp:253]     Train net output #0: loss = 1.77803 (* 1 = 1.77803 loss)
I0521 06:51:12.084252  2420 sgd_solver.cpp:106] Iteration 1235, lr = 0.0025
I0521 06:51:19.819108  2420 solver.cpp:237] Iteration 1254, loss = 1.80097
I0521 06:51:19.819139  2420 solver.cpp:253]     Train net output #0: loss = 1.80097 (* 1 = 1.80097 loss)
I0521 06:51:19.819157  2420 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0521 06:51:27.550892  2420 solver.cpp:237] Iteration 1273, loss = 1.7757
I0521 06:51:27.550925  2420 solver.cpp:253]     Train net output #0: loss = 1.7757 (* 1 = 1.7757 loss)
I0521 06:51:27.550941  2420 sgd_solver.cpp:106] Iteration 1273, lr = 0.0025
I0521 06:51:35.276563  2420 solver.cpp:237] Iteration 1292, loss = 1.75344
I0521 06:51:35.276734  2420 solver.cpp:253]     Train net output #0: loss = 1.75344 (* 1 = 1.75344 loss)
I0521 06:51:35.276749  2420 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0521 06:51:43.010222  2420 solver.cpp:237] Iteration 1311, loss = 1.75621
I0521 06:51:43.010254  2420 solver.cpp:253]     Train net output #0: loss = 1.75621 (* 1 = 1.75621 loss)
I0521 06:51:43.010275  2420 sgd_solver.cpp:106] Iteration 1311, lr = 0.0025
I0521 06:52:12.900717  2420 solver.cpp:237] Iteration 1330, loss = 1.72726
I0521 06:52:12.900884  2420 solver.cpp:253]     Train net output #0: loss = 1.72726 (* 1 = 1.72726 loss)
I0521 06:52:12.900899  2420 sgd_solver.cpp:106] Iteration 1330, lr = 0.0025
I0521 06:52:20.633690  2420 solver.cpp:237] Iteration 1349, loss = 1.89167
I0521 06:52:20.633724  2420 solver.cpp:253]     Train net output #0: loss = 1.89167 (* 1 = 1.89167 loss)
I0521 06:52:20.633741  2420 sgd_solver.cpp:106] Iteration 1349, lr = 0.0025
I0521 06:52:28.363239  2420 solver.cpp:237] Iteration 1368, loss = 1.70005
I0521 06:52:28.363278  2420 solver.cpp:253]     Train net output #0: loss = 1.70005 (* 1 = 1.70005 loss)
I0521 06:52:28.363296  2420 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 06:52:32.430855  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1379.caffemodel
I0521 06:52:32.754019  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1379.solverstate
I0521 06:52:36.156493  2420 solver.cpp:237] Iteration 1387, loss = 1.76586
I0521 06:52:36.156533  2420 solver.cpp:253]     Train net output #0: loss = 1.76586 (* 1 = 1.76586 loss)
I0521 06:52:36.156551  2420 sgd_solver.cpp:106] Iteration 1387, lr = 0.0025
I0521 06:52:43.887328  2420 solver.cpp:237] Iteration 1406, loss = 1.74163
I0521 06:52:43.887480  2420 solver.cpp:253]     Train net output #0: loss = 1.74163 (* 1 = 1.74163 loss)
I0521 06:52:43.887495  2420 sgd_solver.cpp:106] Iteration 1406, lr = 0.0025
I0521 06:52:51.621634  2420 solver.cpp:237] Iteration 1425, loss = 1.70509
I0521 06:52:51.621678  2420 solver.cpp:253]     Train net output #0: loss = 1.70509 (* 1 = 1.70509 loss)
I0521 06:52:51.621695  2420 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 06:52:59.355026  2420 solver.cpp:237] Iteration 1444, loss = 1.74415
I0521 06:52:59.355062  2420 solver.cpp:253]     Train net output #0: loss = 1.74415 (* 1 = 1.74415 loss)
I0521 06:52:59.355077  2420 sgd_solver.cpp:106] Iteration 1444, lr = 0.0025
I0521 06:53:29.227380  2420 solver.cpp:237] Iteration 1463, loss = 1.72041
I0521 06:53:29.227547  2420 solver.cpp:253]     Train net output #0: loss = 1.72041 (* 1 = 1.72041 loss)
I0521 06:53:29.227561  2420 sgd_solver.cpp:106] Iteration 1463, lr = 0.0025
I0521 06:53:36.960026  2420 solver.cpp:237] Iteration 1482, loss = 1.71357
I0521 06:53:36.960058  2420 solver.cpp:253]     Train net output #0: loss = 1.71357 (* 1 = 1.71357 loss)
I0521 06:53:36.960075  2420 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0521 06:53:44.687558  2420 solver.cpp:237] Iteration 1501, loss = 1.69745
I0521 06:53:44.687708  2420 solver.cpp:253]     Train net output #0: loss = 1.69745 (* 1 = 1.69745 loss)
I0521 06:53:44.687724  2420 sgd_solver.cpp:106] Iteration 1501, lr = 0.0025
I0521 06:53:52.417819  2420 solver.cpp:237] Iteration 1520, loss = 1.74958
I0521 06:53:52.417851  2420 solver.cpp:253]     Train net output #0: loss = 1.74958 (* 1 = 1.74958 loss)
I0521 06:53:52.417870  2420 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 06:54:00.151605  2420 solver.cpp:237] Iteration 1539, loss = 1.7778
I0521 06:54:00.151752  2420 solver.cpp:253]     Train net output #0: loss = 1.7778 (* 1 = 1.7778 loss)
I0521 06:54:00.151767  2420 sgd_solver.cpp:106] Iteration 1539, lr = 0.0025
I0521 06:54:07.881129  2420 solver.cpp:237] Iteration 1558, loss = 1.77661
I0521 06:54:07.881167  2420 solver.cpp:253]     Train net output #0: loss = 1.77661 (* 1 = 1.77661 loss)
I0521 06:54:07.881189  2420 sgd_solver.cpp:106] Iteration 1558, lr = 0.0025
I0521 06:54:14.799540  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1576.caffemodel
I0521 06:54:15.128464  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1576.solverstate
I0521 06:54:15.154820  2420 solver.cpp:341] Iteration 1576, Testing net (#0)
I0521 06:55:21.029250  2420 solver.cpp:409]     Test net output #0: accuracy = 0.649766
I0521 06:55:21.029412  2420 solver.cpp:409]     Test net output #1: loss = 1.22065 (* 1 = 1.22065 loss)
I0521 06:55:21.556068  2420 solver.cpp:237] Iteration 1577, loss = 1.70678
I0521 06:55:21.556099  2420 solver.cpp:253]     Train net output #0: loss = 1.70678 (* 1 = 1.70678 loss)
I0521 06:55:21.556113  2420 sgd_solver.cpp:106] Iteration 1577, lr = 0.0025
I0521 06:55:51.436110  2420 solver.cpp:237] Iteration 1596, loss = 1.69448
I0521 06:55:51.436278  2420 solver.cpp:253]     Train net output #0: loss = 1.69448 (* 1 = 1.69448 loss)
I0521 06:55:51.436292  2420 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0521 06:55:59.158946  2420 solver.cpp:237] Iteration 1615, loss = 1.72483
I0521 06:55:59.158977  2420 solver.cpp:253]     Train net output #0: loss = 1.72483 (* 1 = 1.72483 loss)
I0521 06:55:59.158995  2420 sgd_solver.cpp:106] Iteration 1615, lr = 0.0025
I0521 06:56:06.879392  2420 solver.cpp:237] Iteration 1634, loss = 1.75659
I0521 06:56:06.879425  2420 solver.cpp:253]     Train net output #0: loss = 1.75659 (* 1 = 1.75659 loss)
I0521 06:56:06.879441  2420 sgd_solver.cpp:106] Iteration 1634, lr = 0.0025
I0521 06:56:14.603035  2420 solver.cpp:237] Iteration 1653, loss = 1.73833
I0521 06:56:14.603073  2420 solver.cpp:253]     Train net output #0: loss = 1.73833 (* 1 = 1.73833 loss)
I0521 06:56:14.603096  2420 sgd_solver.cpp:106] Iteration 1653, lr = 0.0025
I0521 06:56:22.334017  2420 solver.cpp:237] Iteration 1672, loss = 1.71276
I0521 06:56:22.334168  2420 solver.cpp:253]     Train net output #0: loss = 1.71276 (* 1 = 1.71276 loss)
I0521 06:56:22.334182  2420 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0521 06:56:30.067731  2420 solver.cpp:237] Iteration 1691, loss = 1.67737
I0521 06:56:30.067764  2420 solver.cpp:253]     Train net output #0: loss = 1.67737 (* 1 = 1.67737 loss)
I0521 06:56:30.067782  2420 sgd_solver.cpp:106] Iteration 1691, lr = 0.0025
I0521 06:56:59.972270  2420 solver.cpp:237] Iteration 1710, loss = 1.78856
I0521 06:56:59.972443  2420 solver.cpp:253]     Train net output #0: loss = 1.78856 (* 1 = 1.78856 loss)
I0521 06:56:59.972460  2420 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 06:57:07.711594  2420 solver.cpp:237] Iteration 1729, loss = 1.69382
I0521 06:57:07.711627  2420 solver.cpp:253]     Train net output #0: loss = 1.69382 (* 1 = 1.69382 loss)
I0521 06:57:07.711644  2420 sgd_solver.cpp:106] Iteration 1729, lr = 0.0025
I0521 06:57:15.440508  2420 solver.cpp:237] Iteration 1748, loss = 1.74113
I0521 06:57:15.440541  2420 solver.cpp:253]     Train net output #0: loss = 1.74113 (* 1 = 1.74113 loss)
I0521 06:57:15.440557  2420 sgd_solver.cpp:106] Iteration 1748, lr = 0.0025
I0521 06:57:23.166718  2420 solver.cpp:237] Iteration 1767, loss = 1.68209
I0521 06:57:23.166750  2420 solver.cpp:253]     Train net output #0: loss = 1.68209 (* 1 = 1.68209 loss)
I0521 06:57:23.166767  2420 sgd_solver.cpp:106] Iteration 1767, lr = 0.0025
I0521 06:57:25.202812  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1773.caffemodel
I0521 06:57:25.527519  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1773.solverstate
I0521 06:57:30.966828  2420 solver.cpp:237] Iteration 1786, loss = 1.62793
I0521 06:57:30.966998  2420 solver.cpp:253]     Train net output #0: loss = 1.62793 (* 1 = 1.62793 loss)
I0521 06:57:30.967012  2420 sgd_solver.cpp:106] Iteration 1786, lr = 0.0025
I0521 06:57:38.694253  2420 solver.cpp:237] Iteration 1805, loss = 1.77993
I0521 06:57:38.694290  2420 solver.cpp:253]     Train net output #0: loss = 1.77993 (* 1 = 1.77993 loss)
I0521 06:57:38.694308  2420 sgd_solver.cpp:106] Iteration 1805, lr = 0.0025
I0521 06:57:46.420171  2420 solver.cpp:237] Iteration 1824, loss = 1.80609
I0521 06:57:46.420203  2420 solver.cpp:253]     Train net output #0: loss = 1.80609 (* 1 = 1.80609 loss)
I0521 06:57:46.420217  2420 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0521 06:58:16.331596  2420 solver.cpp:237] Iteration 1843, loss = 1.66879
I0521 06:58:16.331768  2420 solver.cpp:253]     Train net output #0: loss = 1.66879 (* 1 = 1.66879 loss)
I0521 06:58:16.331784  2420 sgd_solver.cpp:106] Iteration 1843, lr = 0.0025
I0521 06:58:24.062996  2420 solver.cpp:237] Iteration 1862, loss = 1.63759
I0521 06:58:24.063030  2420 solver.cpp:253]     Train net output #0: loss = 1.63759 (* 1 = 1.63759 loss)
I0521 06:58:24.063043  2420 sgd_solver.cpp:106] Iteration 1862, lr = 0.0025
I0521 06:58:31.788331  2420 solver.cpp:237] Iteration 1881, loss = 1.69208
I0521 06:58:31.788364  2420 solver.cpp:253]     Train net output #0: loss = 1.69208 (* 1 = 1.69208 loss)
I0521 06:58:31.788381  2420 sgd_solver.cpp:106] Iteration 1881, lr = 0.0025
I0521 06:58:39.519944  2420 solver.cpp:237] Iteration 1900, loss = 1.67105
I0521 06:58:39.519994  2420 solver.cpp:253]     Train net output #0: loss = 1.67105 (* 1 = 1.67105 loss)
I0521 06:58:39.520009  2420 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0521 06:58:47.242408  2420 solver.cpp:237] Iteration 1919, loss = 1.67109
I0521 06:58:47.242565  2420 solver.cpp:253]     Train net output #0: loss = 1.67109 (* 1 = 1.67109 loss)
I0521 06:58:47.242578  2420 sgd_solver.cpp:106] Iteration 1919, lr = 0.0025
I0521 06:58:54.965836  2420 solver.cpp:237] Iteration 1938, loss = 1.67531
I0521 06:58:54.965867  2420 solver.cpp:253]     Train net output #0: loss = 1.67531 (* 1 = 1.67531 loss)
I0521 06:58:54.965885  2420 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0521 06:59:02.691439  2420 solver.cpp:237] Iteration 1957, loss = 1.66649
I0521 06:59:02.691473  2420 solver.cpp:253]     Train net output #0: loss = 1.66649 (* 1 = 1.66649 loss)
I0521 06:59:02.691489  2420 sgd_solver.cpp:106] Iteration 1957, lr = 0.0025
I0521 06:59:07.573618  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1970.caffemodel
I0521 06:59:07.898619  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1970.solverstate
I0521 06:59:07.926729  2420 solver.cpp:341] Iteration 1970, Testing net (#0)
I0521 06:59:52.877270  2420 solver.cpp:409]     Test net output #0: accuracy = 0.647041
I0521 06:59:52.877434  2420 solver.cpp:409]     Test net output #1: loss = 1.1792 (* 1 = 1.1792 loss)
I0521 06:59:53.811820  2420 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1973.caffemodel
I0521 06:59:54.136514  2420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915_iter_1973.solverstate
I0521 06:59:54.164603  2420 solver.cpp:326] Optimization Done.
I0521 06:59:54.164633  2420 caffe.cpp:215] Optimization Done.
Application 11237096 resources: utime ~1246s, stime ~226s, Rss ~5329452, inblocks ~3594475, outblocks ~194564
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_760_2016-05-20T11.21.00.373915.solver"
	User time (seconds): 0.59
	System time (seconds): 0.09
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:35.51
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15078
	Voluntary context switches: 2712
	Involuntary context switches: 90
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
