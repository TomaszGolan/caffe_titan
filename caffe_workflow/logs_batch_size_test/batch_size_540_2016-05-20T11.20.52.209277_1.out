2806141
I0521 01:48:59.526242  8072 caffe.cpp:184] Using GPUs 0
I0521 01:48:59.955293  8072 solver.cpp:48] Initializing solver from parameters: 
test_iter: 277
test_interval: 555
base_lr: 0.0025
display: 27
max_iter: 2777
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 277
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277.prototxt"
I0521 01:48:59.957365  8072 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277.prototxt
I0521 01:48:59.970396  8072 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 01:48:59.970455  8072 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 01:48:59.970801  8072 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 540
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:48:59.970983  8072 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:48:59.971007  8072 net.cpp:106] Creating Layer data_hdf5
I0521 01:48:59.971021  8072 net.cpp:411] data_hdf5 -> data
I0521 01:48:59.971055  8072 net.cpp:411] data_hdf5 -> label
I0521 01:48:59.971086  8072 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 01:48:59.972491  8072 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 01:48:59.974936  8072 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 01:49:21.480592  8072 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 01:49:21.485685  8072 net.cpp:150] Setting up data_hdf5
I0521 01:49:21.485725  8072 net.cpp:157] Top shape: 540 1 127 50 (3429000)
I0521 01:49:21.485739  8072 net.cpp:157] Top shape: 540 (540)
I0521 01:49:21.485751  8072 net.cpp:165] Memory required for data: 13718160
I0521 01:49:21.485765  8072 layer_factory.hpp:77] Creating layer conv1
I0521 01:49:21.485800  8072 net.cpp:106] Creating Layer conv1
I0521 01:49:21.485810  8072 net.cpp:454] conv1 <- data
I0521 01:49:21.485831  8072 net.cpp:411] conv1 -> conv1
I0521 01:49:23.008996  8072 net.cpp:150] Setting up conv1
I0521 01:49:23.009043  8072 net.cpp:157] Top shape: 540 12 120 48 (37324800)
I0521 01:49:23.009053  8072 net.cpp:165] Memory required for data: 163017360
I0521 01:49:23.009084  8072 layer_factory.hpp:77] Creating layer relu1
I0521 01:49:23.009104  8072 net.cpp:106] Creating Layer relu1
I0521 01:49:23.009115  8072 net.cpp:454] relu1 <- conv1
I0521 01:49:23.009130  8072 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:49:23.009644  8072 net.cpp:150] Setting up relu1
I0521 01:49:23.009660  8072 net.cpp:157] Top shape: 540 12 120 48 (37324800)
I0521 01:49:23.009672  8072 net.cpp:165] Memory required for data: 312316560
I0521 01:49:23.009682  8072 layer_factory.hpp:77] Creating layer pool1
I0521 01:49:23.009699  8072 net.cpp:106] Creating Layer pool1
I0521 01:49:23.009709  8072 net.cpp:454] pool1 <- conv1
I0521 01:49:23.009722  8072 net.cpp:411] pool1 -> pool1
I0521 01:49:23.009802  8072 net.cpp:150] Setting up pool1
I0521 01:49:23.009816  8072 net.cpp:157] Top shape: 540 12 60 48 (18662400)
I0521 01:49:23.009826  8072 net.cpp:165] Memory required for data: 386966160
I0521 01:49:23.009837  8072 layer_factory.hpp:77] Creating layer conv2
I0521 01:49:23.009860  8072 net.cpp:106] Creating Layer conv2
I0521 01:49:23.009871  8072 net.cpp:454] conv2 <- pool1
I0521 01:49:23.009883  8072 net.cpp:411] conv2 -> conv2
I0521 01:49:23.012557  8072 net.cpp:150] Setting up conv2
I0521 01:49:23.012584  8072 net.cpp:157] Top shape: 540 20 54 46 (26827200)
I0521 01:49:23.012595  8072 net.cpp:165] Memory required for data: 494274960
I0521 01:49:23.012614  8072 layer_factory.hpp:77] Creating layer relu2
I0521 01:49:23.012629  8072 net.cpp:106] Creating Layer relu2
I0521 01:49:23.012639  8072 net.cpp:454] relu2 <- conv2
I0521 01:49:23.012652  8072 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:49:23.012981  8072 net.cpp:150] Setting up relu2
I0521 01:49:23.012995  8072 net.cpp:157] Top shape: 540 20 54 46 (26827200)
I0521 01:49:23.013006  8072 net.cpp:165] Memory required for data: 601583760
I0521 01:49:23.013016  8072 layer_factory.hpp:77] Creating layer pool2
I0521 01:49:23.013028  8072 net.cpp:106] Creating Layer pool2
I0521 01:49:23.013038  8072 net.cpp:454] pool2 <- conv2
I0521 01:49:23.013063  8072 net.cpp:411] pool2 -> pool2
I0521 01:49:23.013133  8072 net.cpp:150] Setting up pool2
I0521 01:49:23.013146  8072 net.cpp:157] Top shape: 540 20 27 46 (13413600)
I0521 01:49:23.013156  8072 net.cpp:165] Memory required for data: 655238160
I0521 01:49:23.013166  8072 layer_factory.hpp:77] Creating layer conv3
I0521 01:49:23.013185  8072 net.cpp:106] Creating Layer conv3
I0521 01:49:23.013195  8072 net.cpp:454] conv3 <- pool2
I0521 01:49:23.013209  8072 net.cpp:411] conv3 -> conv3
I0521 01:49:23.015121  8072 net.cpp:150] Setting up conv3
I0521 01:49:23.015144  8072 net.cpp:157] Top shape: 540 28 22 44 (14636160)
I0521 01:49:23.015157  8072 net.cpp:165] Memory required for data: 713782800
I0521 01:49:23.015175  8072 layer_factory.hpp:77] Creating layer relu3
I0521 01:49:23.015192  8072 net.cpp:106] Creating Layer relu3
I0521 01:49:23.015202  8072 net.cpp:454] relu3 <- conv3
I0521 01:49:23.015214  8072 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:49:23.015692  8072 net.cpp:150] Setting up relu3
I0521 01:49:23.015709  8072 net.cpp:157] Top shape: 540 28 22 44 (14636160)
I0521 01:49:23.015719  8072 net.cpp:165] Memory required for data: 772327440
I0521 01:49:23.015729  8072 layer_factory.hpp:77] Creating layer pool3
I0521 01:49:23.015743  8072 net.cpp:106] Creating Layer pool3
I0521 01:49:23.015753  8072 net.cpp:454] pool3 <- conv3
I0521 01:49:23.015765  8072 net.cpp:411] pool3 -> pool3
I0521 01:49:23.015833  8072 net.cpp:150] Setting up pool3
I0521 01:49:23.015847  8072 net.cpp:157] Top shape: 540 28 11 44 (7318080)
I0521 01:49:23.015856  8072 net.cpp:165] Memory required for data: 801599760
I0521 01:49:23.015866  8072 layer_factory.hpp:77] Creating layer conv4
I0521 01:49:23.015883  8072 net.cpp:106] Creating Layer conv4
I0521 01:49:23.015894  8072 net.cpp:454] conv4 <- pool3
I0521 01:49:23.015908  8072 net.cpp:411] conv4 -> conv4
I0521 01:49:23.018715  8072 net.cpp:150] Setting up conv4
I0521 01:49:23.018743  8072 net.cpp:157] Top shape: 540 36 6 42 (4898880)
I0521 01:49:23.018754  8072 net.cpp:165] Memory required for data: 821195280
I0521 01:49:23.018770  8072 layer_factory.hpp:77] Creating layer relu4
I0521 01:49:23.018784  8072 net.cpp:106] Creating Layer relu4
I0521 01:49:23.018795  8072 net.cpp:454] relu4 <- conv4
I0521 01:49:23.018806  8072 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:49:23.019279  8072 net.cpp:150] Setting up relu4
I0521 01:49:23.019296  8072 net.cpp:157] Top shape: 540 36 6 42 (4898880)
I0521 01:49:23.019307  8072 net.cpp:165] Memory required for data: 840790800
I0521 01:49:23.019317  8072 layer_factory.hpp:77] Creating layer pool4
I0521 01:49:23.019330  8072 net.cpp:106] Creating Layer pool4
I0521 01:49:23.019340  8072 net.cpp:454] pool4 <- conv4
I0521 01:49:23.019353  8072 net.cpp:411] pool4 -> pool4
I0521 01:49:23.019421  8072 net.cpp:150] Setting up pool4
I0521 01:49:23.019435  8072 net.cpp:157] Top shape: 540 36 3 42 (2449440)
I0521 01:49:23.019446  8072 net.cpp:165] Memory required for data: 850588560
I0521 01:49:23.019455  8072 layer_factory.hpp:77] Creating layer ip1
I0521 01:49:23.019476  8072 net.cpp:106] Creating Layer ip1
I0521 01:49:23.019487  8072 net.cpp:454] ip1 <- pool4
I0521 01:49:23.019500  8072 net.cpp:411] ip1 -> ip1
I0521 01:49:23.034898  8072 net.cpp:150] Setting up ip1
I0521 01:49:23.034926  8072 net.cpp:157] Top shape: 540 196 (105840)
I0521 01:49:23.034939  8072 net.cpp:165] Memory required for data: 851011920
I0521 01:49:23.034960  8072 layer_factory.hpp:77] Creating layer relu5
I0521 01:49:23.034976  8072 net.cpp:106] Creating Layer relu5
I0521 01:49:23.034986  8072 net.cpp:454] relu5 <- ip1
I0521 01:49:23.034998  8072 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:49:23.035341  8072 net.cpp:150] Setting up relu5
I0521 01:49:23.035354  8072 net.cpp:157] Top shape: 540 196 (105840)
I0521 01:49:23.035364  8072 net.cpp:165] Memory required for data: 851435280
I0521 01:49:23.035374  8072 layer_factory.hpp:77] Creating layer drop1
I0521 01:49:23.035395  8072 net.cpp:106] Creating Layer drop1
I0521 01:49:23.035406  8072 net.cpp:454] drop1 <- ip1
I0521 01:49:23.035431  8072 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:49:23.035477  8072 net.cpp:150] Setting up drop1
I0521 01:49:23.035490  8072 net.cpp:157] Top shape: 540 196 (105840)
I0521 01:49:23.035501  8072 net.cpp:165] Memory required for data: 851858640
I0521 01:49:23.035509  8072 layer_factory.hpp:77] Creating layer ip2
I0521 01:49:23.035528  8072 net.cpp:106] Creating Layer ip2
I0521 01:49:23.035538  8072 net.cpp:454] ip2 <- ip1
I0521 01:49:23.035552  8072 net.cpp:411] ip2 -> ip2
I0521 01:49:23.036021  8072 net.cpp:150] Setting up ip2
I0521 01:49:23.036034  8072 net.cpp:157] Top shape: 540 98 (52920)
I0521 01:49:23.036044  8072 net.cpp:165] Memory required for data: 852070320
I0521 01:49:23.036061  8072 layer_factory.hpp:77] Creating layer relu6
I0521 01:49:23.036073  8072 net.cpp:106] Creating Layer relu6
I0521 01:49:23.036083  8072 net.cpp:454] relu6 <- ip2
I0521 01:49:23.036094  8072 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:49:23.036618  8072 net.cpp:150] Setting up relu6
I0521 01:49:23.036634  8072 net.cpp:157] Top shape: 540 98 (52920)
I0521 01:49:23.036644  8072 net.cpp:165] Memory required for data: 852282000
I0521 01:49:23.036654  8072 layer_factory.hpp:77] Creating layer drop2
I0521 01:49:23.036667  8072 net.cpp:106] Creating Layer drop2
I0521 01:49:23.036677  8072 net.cpp:454] drop2 <- ip2
I0521 01:49:23.036690  8072 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:49:23.036732  8072 net.cpp:150] Setting up drop2
I0521 01:49:23.036746  8072 net.cpp:157] Top shape: 540 98 (52920)
I0521 01:49:23.036756  8072 net.cpp:165] Memory required for data: 852493680
I0521 01:49:23.036766  8072 layer_factory.hpp:77] Creating layer ip3
I0521 01:49:23.036779  8072 net.cpp:106] Creating Layer ip3
I0521 01:49:23.036789  8072 net.cpp:454] ip3 <- ip2
I0521 01:49:23.036801  8072 net.cpp:411] ip3 -> ip3
I0521 01:49:23.037014  8072 net.cpp:150] Setting up ip3
I0521 01:49:23.037027  8072 net.cpp:157] Top shape: 540 11 (5940)
I0521 01:49:23.037037  8072 net.cpp:165] Memory required for data: 852517440
I0521 01:49:23.037052  8072 layer_factory.hpp:77] Creating layer drop3
I0521 01:49:23.037065  8072 net.cpp:106] Creating Layer drop3
I0521 01:49:23.037073  8072 net.cpp:454] drop3 <- ip3
I0521 01:49:23.037086  8072 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:49:23.037124  8072 net.cpp:150] Setting up drop3
I0521 01:49:23.037137  8072 net.cpp:157] Top shape: 540 11 (5940)
I0521 01:49:23.037147  8072 net.cpp:165] Memory required for data: 852541200
I0521 01:49:23.037158  8072 layer_factory.hpp:77] Creating layer loss
I0521 01:49:23.037176  8072 net.cpp:106] Creating Layer loss
I0521 01:49:23.037186  8072 net.cpp:454] loss <- ip3
I0521 01:49:23.037197  8072 net.cpp:454] loss <- label
I0521 01:49:23.037209  8072 net.cpp:411] loss -> loss
I0521 01:49:23.037225  8072 layer_factory.hpp:77] Creating layer loss
I0521 01:49:23.037875  8072 net.cpp:150] Setting up loss
I0521 01:49:23.037891  8072 net.cpp:157] Top shape: (1)
I0521 01:49:23.037902  8072 net.cpp:160]     with loss weight 1
I0521 01:49:23.037945  8072 net.cpp:165] Memory required for data: 852541204
I0521 01:49:23.037955  8072 net.cpp:226] loss needs backward computation.
I0521 01:49:23.037966  8072 net.cpp:226] drop3 needs backward computation.
I0521 01:49:23.037974  8072 net.cpp:226] ip3 needs backward computation.
I0521 01:49:23.037986  8072 net.cpp:226] drop2 needs backward computation.
I0521 01:49:23.037995  8072 net.cpp:226] relu6 needs backward computation.
I0521 01:49:23.038004  8072 net.cpp:226] ip2 needs backward computation.
I0521 01:49:23.038014  8072 net.cpp:226] drop1 needs backward computation.
I0521 01:49:23.038024  8072 net.cpp:226] relu5 needs backward computation.
I0521 01:49:23.038035  8072 net.cpp:226] ip1 needs backward computation.
I0521 01:49:23.038045  8072 net.cpp:226] pool4 needs backward computation.
I0521 01:49:23.038055  8072 net.cpp:226] relu4 needs backward computation.
I0521 01:49:23.038065  8072 net.cpp:226] conv4 needs backward computation.
I0521 01:49:23.038076  8072 net.cpp:226] pool3 needs backward computation.
I0521 01:49:23.038095  8072 net.cpp:226] relu3 needs backward computation.
I0521 01:49:23.038105  8072 net.cpp:226] conv3 needs backward computation.
I0521 01:49:23.038115  8072 net.cpp:226] pool2 needs backward computation.
I0521 01:49:23.038126  8072 net.cpp:226] relu2 needs backward computation.
I0521 01:49:23.038136  8072 net.cpp:226] conv2 needs backward computation.
I0521 01:49:23.038147  8072 net.cpp:226] pool1 needs backward computation.
I0521 01:49:23.038157  8072 net.cpp:226] relu1 needs backward computation.
I0521 01:49:23.038167  8072 net.cpp:226] conv1 needs backward computation.
I0521 01:49:23.038178  8072 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:49:23.038187  8072 net.cpp:270] This network produces output loss
I0521 01:49:23.038211  8072 net.cpp:283] Network initialization done.
I0521 01:49:23.039950  8072 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277.prototxt
I0521 01:49:23.040022  8072 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 01:49:23.040375  8072 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 540
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:49:23.040565  8072 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:49:23.040580  8072 net.cpp:106] Creating Layer data_hdf5
I0521 01:49:23.040592  8072 net.cpp:411] data_hdf5 -> data
I0521 01:49:23.040609  8072 net.cpp:411] data_hdf5 -> label
I0521 01:49:23.040624  8072 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 01:49:23.041996  8072 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 01:49:44.343211  8072 net.cpp:150] Setting up data_hdf5
I0521 01:49:44.343379  8072 net.cpp:157] Top shape: 540 1 127 50 (3429000)
I0521 01:49:44.343394  8072 net.cpp:157] Top shape: 540 (540)
I0521 01:49:44.343405  8072 net.cpp:165] Memory required for data: 13718160
I0521 01:49:44.343420  8072 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 01:49:44.343447  8072 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 01:49:44.343458  8072 net.cpp:454] label_data_hdf5_1_split <- label
I0521 01:49:44.343473  8072 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 01:49:44.343494  8072 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 01:49:44.343567  8072 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 01:49:44.343581  8072 net.cpp:157] Top shape: 540 (540)
I0521 01:49:44.343593  8072 net.cpp:157] Top shape: 540 (540)
I0521 01:49:44.343602  8072 net.cpp:165] Memory required for data: 13722480
I0521 01:49:44.343612  8072 layer_factory.hpp:77] Creating layer conv1
I0521 01:49:44.343634  8072 net.cpp:106] Creating Layer conv1
I0521 01:49:44.343653  8072 net.cpp:454] conv1 <- data
I0521 01:49:44.343668  8072 net.cpp:411] conv1 -> conv1
I0521 01:49:44.345587  8072 net.cpp:150] Setting up conv1
I0521 01:49:44.345607  8072 net.cpp:157] Top shape: 540 12 120 48 (37324800)
I0521 01:49:44.345618  8072 net.cpp:165] Memory required for data: 163021680
I0521 01:49:44.345638  8072 layer_factory.hpp:77] Creating layer relu1
I0521 01:49:44.345654  8072 net.cpp:106] Creating Layer relu1
I0521 01:49:44.345664  8072 net.cpp:454] relu1 <- conv1
I0521 01:49:44.345676  8072 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:49:44.346174  8072 net.cpp:150] Setting up relu1
I0521 01:49:44.346189  8072 net.cpp:157] Top shape: 540 12 120 48 (37324800)
I0521 01:49:44.346200  8072 net.cpp:165] Memory required for data: 312320880
I0521 01:49:44.346210  8072 layer_factory.hpp:77] Creating layer pool1
I0521 01:49:44.346226  8072 net.cpp:106] Creating Layer pool1
I0521 01:49:44.346236  8072 net.cpp:454] pool1 <- conv1
I0521 01:49:44.346249  8072 net.cpp:411] pool1 -> pool1
I0521 01:49:44.346323  8072 net.cpp:150] Setting up pool1
I0521 01:49:44.346338  8072 net.cpp:157] Top shape: 540 12 60 48 (18662400)
I0521 01:49:44.346346  8072 net.cpp:165] Memory required for data: 386970480
I0521 01:49:44.346355  8072 layer_factory.hpp:77] Creating layer conv2
I0521 01:49:44.346374  8072 net.cpp:106] Creating Layer conv2
I0521 01:49:44.346385  8072 net.cpp:454] conv2 <- pool1
I0521 01:49:44.346400  8072 net.cpp:411] conv2 -> conv2
I0521 01:49:44.348315  8072 net.cpp:150] Setting up conv2
I0521 01:49:44.348336  8072 net.cpp:157] Top shape: 540 20 54 46 (26827200)
I0521 01:49:44.348351  8072 net.cpp:165] Memory required for data: 494279280
I0521 01:49:44.348368  8072 layer_factory.hpp:77] Creating layer relu2
I0521 01:49:44.348381  8072 net.cpp:106] Creating Layer relu2
I0521 01:49:44.348392  8072 net.cpp:454] relu2 <- conv2
I0521 01:49:44.348404  8072 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:49:44.348737  8072 net.cpp:150] Setting up relu2
I0521 01:49:44.348750  8072 net.cpp:157] Top shape: 540 20 54 46 (26827200)
I0521 01:49:44.348760  8072 net.cpp:165] Memory required for data: 601588080
I0521 01:49:44.348770  8072 layer_factory.hpp:77] Creating layer pool2
I0521 01:49:44.348783  8072 net.cpp:106] Creating Layer pool2
I0521 01:49:44.348793  8072 net.cpp:454] pool2 <- conv2
I0521 01:49:44.348805  8072 net.cpp:411] pool2 -> pool2
I0521 01:49:44.348876  8072 net.cpp:150] Setting up pool2
I0521 01:49:44.348889  8072 net.cpp:157] Top shape: 540 20 27 46 (13413600)
I0521 01:49:44.348898  8072 net.cpp:165] Memory required for data: 655242480
I0521 01:49:44.348908  8072 layer_factory.hpp:77] Creating layer conv3
I0521 01:49:44.348928  8072 net.cpp:106] Creating Layer conv3
I0521 01:49:44.348938  8072 net.cpp:454] conv3 <- pool2
I0521 01:49:44.348953  8072 net.cpp:411] conv3 -> conv3
I0521 01:49:44.350991  8072 net.cpp:150] Setting up conv3
I0521 01:49:44.351014  8072 net.cpp:157] Top shape: 540 28 22 44 (14636160)
I0521 01:49:44.351027  8072 net.cpp:165] Memory required for data: 713787120
I0521 01:49:44.351059  8072 layer_factory.hpp:77] Creating layer relu3
I0521 01:49:44.351073  8072 net.cpp:106] Creating Layer relu3
I0521 01:49:44.351083  8072 net.cpp:454] relu3 <- conv3
I0521 01:49:44.351096  8072 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:49:44.351568  8072 net.cpp:150] Setting up relu3
I0521 01:49:44.351585  8072 net.cpp:157] Top shape: 540 28 22 44 (14636160)
I0521 01:49:44.351595  8072 net.cpp:165] Memory required for data: 772331760
I0521 01:49:44.351605  8072 layer_factory.hpp:77] Creating layer pool3
I0521 01:49:44.351618  8072 net.cpp:106] Creating Layer pool3
I0521 01:49:44.351627  8072 net.cpp:454] pool3 <- conv3
I0521 01:49:44.351647  8072 net.cpp:411] pool3 -> pool3
I0521 01:49:44.351721  8072 net.cpp:150] Setting up pool3
I0521 01:49:44.351733  8072 net.cpp:157] Top shape: 540 28 11 44 (7318080)
I0521 01:49:44.351742  8072 net.cpp:165] Memory required for data: 801604080
I0521 01:49:44.351752  8072 layer_factory.hpp:77] Creating layer conv4
I0521 01:49:44.351769  8072 net.cpp:106] Creating Layer conv4
I0521 01:49:44.351780  8072 net.cpp:454] conv4 <- pool3
I0521 01:49:44.351794  8072 net.cpp:411] conv4 -> conv4
I0521 01:49:44.353840  8072 net.cpp:150] Setting up conv4
I0521 01:49:44.353858  8072 net.cpp:157] Top shape: 540 36 6 42 (4898880)
I0521 01:49:44.353869  8072 net.cpp:165] Memory required for data: 821199600
I0521 01:49:44.353884  8072 layer_factory.hpp:77] Creating layer relu4
I0521 01:49:44.353899  8072 net.cpp:106] Creating Layer relu4
I0521 01:49:44.353909  8072 net.cpp:454] relu4 <- conv4
I0521 01:49:44.353920  8072 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:49:44.354393  8072 net.cpp:150] Setting up relu4
I0521 01:49:44.354409  8072 net.cpp:157] Top shape: 540 36 6 42 (4898880)
I0521 01:49:44.354419  8072 net.cpp:165] Memory required for data: 840795120
I0521 01:49:44.354429  8072 layer_factory.hpp:77] Creating layer pool4
I0521 01:49:44.354442  8072 net.cpp:106] Creating Layer pool4
I0521 01:49:44.354452  8072 net.cpp:454] pool4 <- conv4
I0521 01:49:44.354465  8072 net.cpp:411] pool4 -> pool4
I0521 01:49:44.354534  8072 net.cpp:150] Setting up pool4
I0521 01:49:44.354548  8072 net.cpp:157] Top shape: 540 36 3 42 (2449440)
I0521 01:49:44.354557  8072 net.cpp:165] Memory required for data: 850592880
I0521 01:49:44.354568  8072 layer_factory.hpp:77] Creating layer ip1
I0521 01:49:44.354580  8072 net.cpp:106] Creating Layer ip1
I0521 01:49:44.354591  8072 net.cpp:454] ip1 <- pool4
I0521 01:49:44.354604  8072 net.cpp:411] ip1 -> ip1
I0521 01:49:44.370091  8072 net.cpp:150] Setting up ip1
I0521 01:49:44.370120  8072 net.cpp:157] Top shape: 540 196 (105840)
I0521 01:49:44.370131  8072 net.cpp:165] Memory required for data: 851016240
I0521 01:49:44.370153  8072 layer_factory.hpp:77] Creating layer relu5
I0521 01:49:44.370168  8072 net.cpp:106] Creating Layer relu5
I0521 01:49:44.370178  8072 net.cpp:454] relu5 <- ip1
I0521 01:49:44.370192  8072 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:49:44.370535  8072 net.cpp:150] Setting up relu5
I0521 01:49:44.370549  8072 net.cpp:157] Top shape: 540 196 (105840)
I0521 01:49:44.370559  8072 net.cpp:165] Memory required for data: 851439600
I0521 01:49:44.370569  8072 layer_factory.hpp:77] Creating layer drop1
I0521 01:49:44.370589  8072 net.cpp:106] Creating Layer drop1
I0521 01:49:44.370599  8072 net.cpp:454] drop1 <- ip1
I0521 01:49:44.370611  8072 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:49:44.370656  8072 net.cpp:150] Setting up drop1
I0521 01:49:44.370668  8072 net.cpp:157] Top shape: 540 196 (105840)
I0521 01:49:44.370676  8072 net.cpp:165] Memory required for data: 851862960
I0521 01:49:44.370688  8072 layer_factory.hpp:77] Creating layer ip2
I0521 01:49:44.370702  8072 net.cpp:106] Creating Layer ip2
I0521 01:49:44.370712  8072 net.cpp:454] ip2 <- ip1
I0521 01:49:44.370726  8072 net.cpp:411] ip2 -> ip2
I0521 01:49:44.371204  8072 net.cpp:150] Setting up ip2
I0521 01:49:44.371218  8072 net.cpp:157] Top shape: 540 98 (52920)
I0521 01:49:44.371228  8072 net.cpp:165] Memory required for data: 852074640
I0521 01:49:44.371256  8072 layer_factory.hpp:77] Creating layer relu6
I0521 01:49:44.371269  8072 net.cpp:106] Creating Layer relu6
I0521 01:49:44.371279  8072 net.cpp:454] relu6 <- ip2
I0521 01:49:44.371291  8072 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:49:44.371832  8072 net.cpp:150] Setting up relu6
I0521 01:49:44.371853  8072 net.cpp:157] Top shape: 540 98 (52920)
I0521 01:49:44.371863  8072 net.cpp:165] Memory required for data: 852286320
I0521 01:49:44.371875  8072 layer_factory.hpp:77] Creating layer drop2
I0521 01:49:44.371887  8072 net.cpp:106] Creating Layer drop2
I0521 01:49:44.371897  8072 net.cpp:454] drop2 <- ip2
I0521 01:49:44.371911  8072 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:49:44.371953  8072 net.cpp:150] Setting up drop2
I0521 01:49:44.371968  8072 net.cpp:157] Top shape: 540 98 (52920)
I0521 01:49:44.371976  8072 net.cpp:165] Memory required for data: 852498000
I0521 01:49:44.371986  8072 layer_factory.hpp:77] Creating layer ip3
I0521 01:49:44.372001  8072 net.cpp:106] Creating Layer ip3
I0521 01:49:44.372011  8072 net.cpp:454] ip3 <- ip2
I0521 01:49:44.372025  8072 net.cpp:411] ip3 -> ip3
I0521 01:49:44.372246  8072 net.cpp:150] Setting up ip3
I0521 01:49:44.372258  8072 net.cpp:157] Top shape: 540 11 (5940)
I0521 01:49:44.372268  8072 net.cpp:165] Memory required for data: 852521760
I0521 01:49:44.372283  8072 layer_factory.hpp:77] Creating layer drop3
I0521 01:49:44.372297  8072 net.cpp:106] Creating Layer drop3
I0521 01:49:44.372306  8072 net.cpp:454] drop3 <- ip3
I0521 01:49:44.372318  8072 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:49:44.372359  8072 net.cpp:150] Setting up drop3
I0521 01:49:44.372372  8072 net.cpp:157] Top shape: 540 11 (5940)
I0521 01:49:44.372381  8072 net.cpp:165] Memory required for data: 852545520
I0521 01:49:44.372391  8072 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 01:49:44.372405  8072 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 01:49:44.372414  8072 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 01:49:44.372427  8072 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 01:49:44.372442  8072 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 01:49:44.372515  8072 net.cpp:150] Setting up ip3_drop3_0_split
I0521 01:49:44.372529  8072 net.cpp:157] Top shape: 540 11 (5940)
I0521 01:49:44.372540  8072 net.cpp:157] Top shape: 540 11 (5940)
I0521 01:49:44.372550  8072 net.cpp:165] Memory required for data: 852593040
I0521 01:49:44.372557  8072 layer_factory.hpp:77] Creating layer accuracy
I0521 01:49:44.372580  8072 net.cpp:106] Creating Layer accuracy
I0521 01:49:44.372591  8072 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 01:49:44.372602  8072 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 01:49:44.372616  8072 net.cpp:411] accuracy -> accuracy
I0521 01:49:44.372638  8072 net.cpp:150] Setting up accuracy
I0521 01:49:44.372651  8072 net.cpp:157] Top shape: (1)
I0521 01:49:44.372661  8072 net.cpp:165] Memory required for data: 852593044
I0521 01:49:44.372671  8072 layer_factory.hpp:77] Creating layer loss
I0521 01:49:44.372685  8072 net.cpp:106] Creating Layer loss
I0521 01:49:44.372695  8072 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 01:49:44.372706  8072 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 01:49:44.372720  8072 net.cpp:411] loss -> loss
I0521 01:49:44.372737  8072 layer_factory.hpp:77] Creating layer loss
I0521 01:49:44.373229  8072 net.cpp:150] Setting up loss
I0521 01:49:44.373241  8072 net.cpp:157] Top shape: (1)
I0521 01:49:44.373251  8072 net.cpp:160]     with loss weight 1
I0521 01:49:44.373270  8072 net.cpp:165] Memory required for data: 852593048
I0521 01:49:44.373280  8072 net.cpp:226] loss needs backward computation.
I0521 01:49:44.373291  8072 net.cpp:228] accuracy does not need backward computation.
I0521 01:49:44.373302  8072 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 01:49:44.373312  8072 net.cpp:226] drop3 needs backward computation.
I0521 01:49:44.373324  8072 net.cpp:226] ip3 needs backward computation.
I0521 01:49:44.373334  8072 net.cpp:226] drop2 needs backward computation.
I0521 01:49:44.373353  8072 net.cpp:226] relu6 needs backward computation.
I0521 01:49:44.373363  8072 net.cpp:226] ip2 needs backward computation.
I0521 01:49:44.373373  8072 net.cpp:226] drop1 needs backward computation.
I0521 01:49:44.373383  8072 net.cpp:226] relu5 needs backward computation.
I0521 01:49:44.373391  8072 net.cpp:226] ip1 needs backward computation.
I0521 01:49:44.373401  8072 net.cpp:226] pool4 needs backward computation.
I0521 01:49:44.373411  8072 net.cpp:226] relu4 needs backward computation.
I0521 01:49:44.373420  8072 net.cpp:226] conv4 needs backward computation.
I0521 01:49:44.373431  8072 net.cpp:226] pool3 needs backward computation.
I0521 01:49:44.373441  8072 net.cpp:226] relu3 needs backward computation.
I0521 01:49:44.373452  8072 net.cpp:226] conv3 needs backward computation.
I0521 01:49:44.373462  8072 net.cpp:226] pool2 needs backward computation.
I0521 01:49:44.373472  8072 net.cpp:226] relu2 needs backward computation.
I0521 01:49:44.373482  8072 net.cpp:226] conv2 needs backward computation.
I0521 01:49:44.373492  8072 net.cpp:226] pool1 needs backward computation.
I0521 01:49:44.373502  8072 net.cpp:226] relu1 needs backward computation.
I0521 01:49:44.373512  8072 net.cpp:226] conv1 needs backward computation.
I0521 01:49:44.373523  8072 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 01:49:44.373535  8072 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:49:44.373545  8072 net.cpp:270] This network produces output accuracy
I0521 01:49:44.373555  8072 net.cpp:270] This network produces output loss
I0521 01:49:44.373584  8072 net.cpp:283] Network initialization done.
I0521 01:49:44.373718  8072 solver.cpp:60] Solver scaffolding done.
I0521 01:49:44.374845  8072 caffe.cpp:212] Starting Optimization
I0521 01:49:44.374862  8072 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 01:49:44.374876  8072 solver.cpp:289] Learning Rate Policy: fixed
I0521 01:49:44.376103  8072 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 01:50:30.298199  8072 solver.cpp:409]     Test net output #0: accuracy = 0.108477
I0521 01:50:30.298359  8072 solver.cpp:409]     Test net output #1: loss = 2.397 (* 1 = 2.397 loss)
I0521 01:50:30.403312  8072 solver.cpp:237] Iteration 0, loss = 2.39686
I0521 01:50:30.403350  8072 solver.cpp:253]     Train net output #0: loss = 2.39686 (* 1 = 2.39686 loss)
I0521 01:50:30.403368  8072 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 01:50:38.236001  8072 solver.cpp:237] Iteration 27, loss = 2.37839
I0521 01:50:38.236038  8072 solver.cpp:253]     Train net output #0: loss = 2.37839 (* 1 = 2.37839 loss)
I0521 01:50:38.236053  8072 sgd_solver.cpp:106] Iteration 27, lr = 0.0025
I0521 01:50:46.062707  8072 solver.cpp:237] Iteration 54, loss = 2.35989
I0521 01:50:46.062739  8072 solver.cpp:253]     Train net output #0: loss = 2.35989 (* 1 = 2.35989 loss)
I0521 01:50:46.062757  8072 sgd_solver.cpp:106] Iteration 54, lr = 0.0025
I0521 01:50:53.889681  8072 solver.cpp:237] Iteration 81, loss = 2.34589
I0521 01:50:53.889724  8072 solver.cpp:253]     Train net output #0: loss = 2.34589 (* 1 = 2.34589 loss)
I0521 01:50:53.889739  8072 sgd_solver.cpp:106] Iteration 81, lr = 0.0025
I0521 01:51:01.711769  8072 solver.cpp:237] Iteration 108, loss = 2.34121
I0521 01:51:01.711915  8072 solver.cpp:253]     Train net output #0: loss = 2.34121 (* 1 = 2.34121 loss)
I0521 01:51:01.711928  8072 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0521 01:51:09.541654  8072 solver.cpp:237] Iteration 135, loss = 2.33857
I0521 01:51:09.541685  8072 solver.cpp:253]     Train net output #0: loss = 2.33857 (* 1 = 2.33857 loss)
I0521 01:51:09.541699  8072 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 01:51:17.368324  8072 solver.cpp:237] Iteration 162, loss = 2.30856
I0521 01:51:17.368367  8072 solver.cpp:253]     Train net output #0: loss = 2.30856 (* 1 = 2.30856 loss)
I0521 01:51:17.368381  8072 sgd_solver.cpp:106] Iteration 162, lr = 0.0025
I0521 01:51:47.308496  8072 solver.cpp:237] Iteration 189, loss = 2.3014
I0521 01:51:47.308657  8072 solver.cpp:253]     Train net output #0: loss = 2.3014 (* 1 = 2.3014 loss)
I0521 01:51:47.308672  8072 sgd_solver.cpp:106] Iteration 189, lr = 0.0025
I0521 01:51:55.141125  8072 solver.cpp:237] Iteration 216, loss = 2.30248
I0521 01:51:55.141158  8072 solver.cpp:253]     Train net output #0: loss = 2.30248 (* 1 = 2.30248 loss)
I0521 01:51:55.141175  8072 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 01:52:02.975095  8072 solver.cpp:237] Iteration 243, loss = 2.29917
I0521 01:52:02.975128  8072 solver.cpp:253]     Train net output #0: loss = 2.29917 (* 1 = 2.29917 loss)
I0521 01:52:02.975145  8072 sgd_solver.cpp:106] Iteration 243, lr = 0.0025
I0521 01:52:10.812104  8072 solver.cpp:237] Iteration 270, loss = 2.26511
I0521 01:52:10.812149  8072 solver.cpp:253]     Train net output #0: loss = 2.26511 (* 1 = 2.26511 loss)
I0521 01:52:10.812165  8072 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 01:52:12.551604  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_277.caffemodel
I0521 01:52:12.797116  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_277.solverstate
I0521 01:52:18.704761  8072 solver.cpp:237] Iteration 297, loss = 2.2698
I0521 01:52:18.704910  8072 solver.cpp:253]     Train net output #0: loss = 2.2698 (* 1 = 2.2698 loss)
I0521 01:52:18.704924  8072 sgd_solver.cpp:106] Iteration 297, lr = 0.0025
I0521 01:52:26.536562  8072 solver.cpp:237] Iteration 324, loss = 2.22712
I0521 01:52:26.536595  8072 solver.cpp:253]     Train net output #0: loss = 2.22712 (* 1 = 2.22712 loss)
I0521 01:52:26.536612  8072 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0521 01:52:34.369403  8072 solver.cpp:237] Iteration 351, loss = 2.20751
I0521 01:52:34.369444  8072 solver.cpp:253]     Train net output #0: loss = 2.20751 (* 1 = 2.20751 loss)
I0521 01:52:34.369462  8072 sgd_solver.cpp:106] Iteration 351, lr = 0.0025
I0521 01:53:04.338053  8072 solver.cpp:237] Iteration 378, loss = 2.11165
I0521 01:53:04.338208  8072 solver.cpp:253]     Train net output #0: loss = 2.11165 (* 1 = 2.11165 loss)
I0521 01:53:04.338223  8072 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 01:53:12.170158  8072 solver.cpp:237] Iteration 405, loss = 2.11949
I0521 01:53:12.170191  8072 solver.cpp:253]     Train net output #0: loss = 2.11949 (* 1 = 2.11949 loss)
I0521 01:53:12.170208  8072 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 01:53:20.000702  8072 solver.cpp:237] Iteration 432, loss = 2.05136
I0521 01:53:20.000737  8072 solver.cpp:253]     Train net output #0: loss = 2.05136 (* 1 = 2.05136 loss)
I0521 01:53:20.000753  8072 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 01:53:27.828672  8072 solver.cpp:237] Iteration 459, loss = 1.98462
I0521 01:53:27.828717  8072 solver.cpp:253]     Train net output #0: loss = 1.98462 (* 1 = 1.98462 loss)
I0521 01:53:27.828733  8072 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0521 01:53:35.663362  8072 solver.cpp:237] Iteration 486, loss = 1.98162
I0521 01:53:35.663509  8072 solver.cpp:253]     Train net output #0: loss = 1.98162 (* 1 = 1.98162 loss)
I0521 01:53:35.663522  8072 sgd_solver.cpp:106] Iteration 486, lr = 0.0025
I0521 01:53:43.494633  8072 solver.cpp:237] Iteration 513, loss = 2.02663
I0521 01:53:43.494666  8072 solver.cpp:253]     Train net output #0: loss = 2.02663 (* 1 = 2.02663 loss)
I0521 01:53:43.494680  8072 sgd_solver.cpp:106] Iteration 513, lr = 0.0025
I0521 01:53:51.329777  8072 solver.cpp:237] Iteration 540, loss = 1.95307
I0521 01:53:51.329815  8072 solver.cpp:253]     Train net output #0: loss = 1.95307 (* 1 = 1.95307 loss)
I0521 01:53:51.329835  8072 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 01:53:55.104982  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_554.caffemodel
I0521 01:53:55.345825  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_554.solverstate
I0521 01:53:55.458303  8072 solver.cpp:341] Iteration 555, Testing net (#0)
I0521 01:54:40.649811  8072 solver.cpp:409]     Test net output #0: accuracy = 0.540333
I0521 01:54:40.649968  8072 solver.cpp:409]     Test net output #1: loss = 1.69734 (* 1 = 1.69734 loss)
I0521 01:55:06.311852  8072 solver.cpp:237] Iteration 567, loss = 1.94303
I0521 01:55:06.311903  8072 solver.cpp:253]     Train net output #0: loss = 1.94303 (* 1 = 1.94303 loss)
I0521 01:55:06.311919  8072 sgd_solver.cpp:106] Iteration 567, lr = 0.0025
I0521 01:55:14.133018  8072 solver.cpp:237] Iteration 594, loss = 1.96009
I0521 01:55:14.133162  8072 solver.cpp:253]     Train net output #0: loss = 1.96009 (* 1 = 1.96009 loss)
I0521 01:55:14.133177  8072 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 01:55:21.954999  8072 solver.cpp:237] Iteration 621, loss = 1.92775
I0521 01:55:21.955031  8072 solver.cpp:253]     Train net output #0: loss = 1.92775 (* 1 = 1.92775 loss)
I0521 01:55:21.955049  8072 sgd_solver.cpp:106] Iteration 621, lr = 0.0025
I0521 01:55:29.777765  8072 solver.cpp:237] Iteration 648, loss = 1.94481
I0521 01:55:29.777812  8072 solver.cpp:253]     Train net output #0: loss = 1.94481 (* 1 = 1.94481 loss)
I0521 01:55:29.777825  8072 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 01:55:37.598604  8072 solver.cpp:237] Iteration 675, loss = 1.85624
I0521 01:55:37.598639  8072 solver.cpp:253]     Train net output #0: loss = 1.85624 (* 1 = 1.85624 loss)
I0521 01:55:37.598654  8072 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 01:55:45.421748  8072 solver.cpp:237] Iteration 702, loss = 1.8356
I0521 01:55:45.421885  8072 solver.cpp:253]     Train net output #0: loss = 1.8356 (* 1 = 1.8356 loss)
I0521 01:55:45.421898  8072 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 01:55:53.244485  8072 solver.cpp:237] Iteration 729, loss = 1.95106
I0521 01:55:53.244527  8072 solver.cpp:253]     Train net output #0: loss = 1.95106 (* 1 = 1.95106 loss)
I0521 01:55:53.244544  8072 sgd_solver.cpp:106] Iteration 729, lr = 0.0025
I0521 01:56:23.254159  8072 solver.cpp:237] Iteration 756, loss = 1.94846
I0521 01:56:23.254333  8072 solver.cpp:253]     Train net output #0: loss = 1.94846 (* 1 = 1.94846 loss)
I0521 01:56:23.254346  8072 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 01:56:31.076905  8072 solver.cpp:237] Iteration 783, loss = 1.82836
I0521 01:56:31.076937  8072 solver.cpp:253]     Train net output #0: loss = 1.82836 (* 1 = 1.82836 loss)
I0521 01:56:31.076954  8072 sgd_solver.cpp:106] Iteration 783, lr = 0.0025
I0521 01:56:38.895946  8072 solver.cpp:237] Iteration 810, loss = 1.80695
I0521 01:56:38.895980  8072 solver.cpp:253]     Train net output #0: loss = 1.80695 (* 1 = 1.80695 loss)
I0521 01:56:38.895993  8072 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 01:56:44.690433  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_831.caffemodel
I0521 01:56:44.933614  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_831.solverstate
I0521 01:56:46.787122  8072 solver.cpp:237] Iteration 837, loss = 1.85561
I0521 01:56:46.787171  8072 solver.cpp:253]     Train net output #0: loss = 1.85561 (* 1 = 1.85561 loss)
I0521 01:56:46.787186  8072 sgd_solver.cpp:106] Iteration 837, lr = 0.0025
I0521 01:56:54.606806  8072 solver.cpp:237] Iteration 864, loss = 1.85991
I0521 01:56:54.606950  8072 solver.cpp:253]     Train net output #0: loss = 1.85991 (* 1 = 1.85991 loss)
I0521 01:56:54.606964  8072 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 01:57:02.431092  8072 solver.cpp:237] Iteration 891, loss = 1.84254
I0521 01:57:02.431124  8072 solver.cpp:253]     Train net output #0: loss = 1.84254 (* 1 = 1.84254 loss)
I0521 01:57:02.431143  8072 sgd_solver.cpp:106] Iteration 891, lr = 0.0025
I0521 01:57:10.257155  8072 solver.cpp:237] Iteration 918, loss = 1.86652
I0521 01:57:10.257202  8072 solver.cpp:253]     Train net output #0: loss = 1.86652 (* 1 = 1.86652 loss)
I0521 01:57:10.257215  8072 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 01:57:40.316124  8072 solver.cpp:237] Iteration 945, loss = 1.86863
I0521 01:57:40.316288  8072 solver.cpp:253]     Train net output #0: loss = 1.86863 (* 1 = 1.86863 loss)
I0521 01:57:40.316304  8072 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 01:57:48.138077  8072 solver.cpp:237] Iteration 972, loss = 1.78996
I0521 01:57:48.138109  8072 solver.cpp:253]     Train net output #0: loss = 1.78996 (* 1 = 1.78996 loss)
I0521 01:57:48.138124  8072 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0521 01:57:55.963752  8072 solver.cpp:237] Iteration 999, loss = 1.83343
I0521 01:57:55.963786  8072 solver.cpp:253]     Train net output #0: loss = 1.83343 (* 1 = 1.83343 loss)
I0521 01:57:55.963804  8072 sgd_solver.cpp:106] Iteration 999, lr = 0.0025
I0521 01:58:03.788616  8072 solver.cpp:237] Iteration 1026, loss = 1.76793
I0521 01:58:03.788661  8072 solver.cpp:253]     Train net output #0: loss = 1.76793 (* 1 = 1.76793 loss)
I0521 01:58:03.788674  8072 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 01:58:11.610180  8072 solver.cpp:237] Iteration 1053, loss = 1.86683
I0521 01:58:11.610319  8072 solver.cpp:253]     Train net output #0: loss = 1.86683 (* 1 = 1.86683 loss)
I0521 01:58:11.610332  8072 sgd_solver.cpp:106] Iteration 1053, lr = 0.0025
I0521 01:58:19.433120  8072 solver.cpp:237] Iteration 1080, loss = 1.83031
I0521 01:58:19.433152  8072 solver.cpp:253]     Train net output #0: loss = 1.83031 (* 1 = 1.83031 loss)
I0521 01:58:19.433166  8072 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 01:58:27.258481  8072 solver.cpp:237] Iteration 1107, loss = 1.77937
I0521 01:58:27.258522  8072 solver.cpp:253]     Train net output #0: loss = 1.77937 (* 1 = 1.77937 loss)
I0521 01:58:27.258538  8072 sgd_solver.cpp:106] Iteration 1107, lr = 0.0025
I0521 01:58:27.258913  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1108.caffemodel
I0521 01:58:27.502859  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1108.solverstate
I0521 01:58:27.908221  8072 solver.cpp:341] Iteration 1110, Testing net (#0)
I0521 01:59:33.963250  8072 solver.cpp:409]     Test net output #0: accuracy = 0.607909
I0521 01:59:33.963415  8072 solver.cpp:409]     Test net output #1: loss = 1.36404 (* 1 = 1.36404 loss)
I0521 02:00:03.178344  8072 solver.cpp:237] Iteration 1134, loss = 1.77547
I0521 02:00:03.178392  8072 solver.cpp:253]     Train net output #0: loss = 1.77547 (* 1 = 1.77547 loss)
I0521 02:00:03.178408  8072 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 02:00:11.001999  8072 solver.cpp:237] Iteration 1161, loss = 1.78944
I0521 02:00:11.002148  8072 solver.cpp:253]     Train net output #0: loss = 1.78944 (* 1 = 1.78944 loss)
I0521 02:00:11.002161  8072 sgd_solver.cpp:106] Iteration 1161, lr = 0.0025
I0521 02:00:18.822262  8072 solver.cpp:237] Iteration 1188, loss = 1.91891
I0521 02:00:18.822295  8072 solver.cpp:253]     Train net output #0: loss = 1.91891 (* 1 = 1.91891 loss)
I0521 02:00:18.822312  8072 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 02:00:26.652377  8072 solver.cpp:237] Iteration 1215, loss = 1.79417
I0521 02:00:26.652426  8072 solver.cpp:253]     Train net output #0: loss = 1.79417 (* 1 = 1.79417 loss)
I0521 02:00:26.652441  8072 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 02:00:34.473147  8072 solver.cpp:237] Iteration 1242, loss = 1.73435
I0521 02:00:34.473181  8072 solver.cpp:253]     Train net output #0: loss = 1.73435 (* 1 = 1.73435 loss)
I0521 02:00:34.473198  8072 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 02:00:42.291960  8072 solver.cpp:237] Iteration 1269, loss = 1.7221
I0521 02:00:42.292098  8072 solver.cpp:253]     Train net output #0: loss = 1.7221 (* 1 = 1.7221 loss)
I0521 02:00:42.292111  8072 sgd_solver.cpp:106] Iteration 1269, lr = 0.0025
I0521 02:01:12.273627  8072 solver.cpp:237] Iteration 1296, loss = 1.76259
I0521 02:01:12.273677  8072 solver.cpp:253]     Train net output #0: loss = 1.76259 (* 1 = 1.76259 loss)
I0521 02:01:12.273692  8072 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 02:01:20.095304  8072 solver.cpp:237] Iteration 1323, loss = 1.87148
I0521 02:01:20.095461  8072 solver.cpp:253]     Train net output #0: loss = 1.87148 (* 1 = 1.87148 loss)
I0521 02:01:20.095475  8072 sgd_solver.cpp:106] Iteration 1323, lr = 0.0025
I0521 02:01:27.914443  8072 solver.cpp:237] Iteration 1350, loss = 1.685
I0521 02:01:27.914475  8072 solver.cpp:253]     Train net output #0: loss = 1.685 (* 1 = 1.685 loss)
I0521 02:01:27.914491  8072 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 02:01:35.735131  8072 solver.cpp:237] Iteration 1377, loss = 1.7506
I0521 02:01:35.735164  8072 solver.cpp:253]     Train net output #0: loss = 1.7506 (* 1 = 1.7506 loss)
I0521 02:01:35.735182  8072 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0521 02:01:37.767153  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1385.caffemodel
I0521 02:01:38.010618  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1385.solverstate
I0521 02:01:43.629606  8072 solver.cpp:237] Iteration 1404, loss = 1.74193
I0521 02:01:43.629650  8072 solver.cpp:253]     Train net output #0: loss = 1.74193 (* 1 = 1.74193 loss)
I0521 02:01:43.629672  8072 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 02:01:51.448266  8072 solver.cpp:237] Iteration 1431, loss = 1.81255
I0521 02:01:51.448420  8072 solver.cpp:253]     Train net output #0: loss = 1.81255 (* 1 = 1.81255 loss)
I0521 02:01:51.448433  8072 sgd_solver.cpp:106] Iteration 1431, lr = 0.0025
I0521 02:01:59.265583  8072 solver.cpp:237] Iteration 1458, loss = 1.71936
I0521 02:01:59.265616  8072 solver.cpp:253]     Train net output #0: loss = 1.71936 (* 1 = 1.71936 loss)
I0521 02:01:59.265632  8072 sgd_solver.cpp:106] Iteration 1458, lr = 0.0025
I0521 02:02:29.261122  8072 solver.cpp:237] Iteration 1485, loss = 1.72547
I0521 02:02:29.261289  8072 solver.cpp:253]     Train net output #0: loss = 1.72547 (* 1 = 1.72547 loss)
I0521 02:02:29.261303  8072 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 02:02:37.088090  8072 solver.cpp:237] Iteration 1512, loss = 1.62835
I0521 02:02:37.088129  8072 solver.cpp:253]     Train net output #0: loss = 1.62835 (* 1 = 1.62835 loss)
I0521 02:02:37.088148  8072 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 02:02:44.913663  8072 solver.cpp:237] Iteration 1539, loss = 1.75719
I0521 02:02:44.913698  8072 solver.cpp:253]     Train net output #0: loss = 1.75719 (* 1 = 1.75719 loss)
I0521 02:02:44.913714  8072 sgd_solver.cpp:106] Iteration 1539, lr = 0.0025
I0521 02:02:52.731395  8072 solver.cpp:237] Iteration 1566, loss = 1.86617
I0521 02:02:52.731428  8072 solver.cpp:253]     Train net output #0: loss = 1.86617 (* 1 = 1.86617 loss)
I0521 02:02:52.731446  8072 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 02:03:00.557004  8072 solver.cpp:237] Iteration 1593, loss = 1.69187
I0521 02:03:00.557158  8072 solver.cpp:253]     Train net output #0: loss = 1.69187 (* 1 = 1.69187 loss)
I0521 02:03:00.557173  8072 sgd_solver.cpp:106] Iteration 1593, lr = 0.0025
I0521 02:03:08.377960  8072 solver.cpp:237] Iteration 1620, loss = 1.72307
I0521 02:03:08.377992  8072 solver.cpp:253]     Train net output #0: loss = 1.72307 (* 1 = 1.72307 loss)
I0521 02:03:08.378010  8072 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 02:03:16.198500  8072 solver.cpp:237] Iteration 1647, loss = 1.77495
I0521 02:03:16.198534  8072 solver.cpp:253]     Train net output #0: loss = 1.77495 (* 1 = 1.77495 loss)
I0521 02:03:16.198551  8072 sgd_solver.cpp:106] Iteration 1647, lr = 0.0025
I0521 02:03:20.256570  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1662.caffemodel
I0521 02:03:20.497242  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1662.solverstate
I0521 02:03:21.190153  8072 solver.cpp:341] Iteration 1665, Testing net (#0)
I0521 02:04:06.078435  8072 solver.cpp:409]     Test net output #0: accuracy = 0.652707
I0521 02:04:06.078598  8072 solver.cpp:409]     Test net output #1: loss = 1.20198 (* 1 = 1.20198 loss)
I0521 02:04:30.942720  8072 solver.cpp:237] Iteration 1674, loss = 1.63364
I0521 02:04:30.942772  8072 solver.cpp:253]     Train net output #0: loss = 1.63364 (* 1 = 1.63364 loss)
I0521 02:04:30.942786  8072 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 02:04:38.762053  8072 solver.cpp:237] Iteration 1701, loss = 1.67374
I0521 02:04:38.762223  8072 solver.cpp:253]     Train net output #0: loss = 1.67374 (* 1 = 1.67374 loss)
I0521 02:04:38.762238  8072 sgd_solver.cpp:106] Iteration 1701, lr = 0.0025
I0521 02:04:46.588449  8072 solver.cpp:237] Iteration 1728, loss = 1.66821
I0521 02:04:46.588481  8072 solver.cpp:253]     Train net output #0: loss = 1.66821 (* 1 = 1.66821 loss)
I0521 02:04:46.588500  8072 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 02:04:54.411412  8072 solver.cpp:237] Iteration 1755, loss = 1.71966
I0521 02:04:54.411444  8072 solver.cpp:253]     Train net output #0: loss = 1.71966 (* 1 = 1.71966 loss)
I0521 02:04:54.411458  8072 sgd_solver.cpp:106] Iteration 1755, lr = 0.0025
I0521 02:05:02.235446  8072 solver.cpp:237] Iteration 1782, loss = 1.73717
I0521 02:05:02.235496  8072 solver.cpp:253]     Train net output #0: loss = 1.73717 (* 1 = 1.73717 loss)
I0521 02:05:02.235509  8072 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 02:05:10.052089  8072 solver.cpp:237] Iteration 1809, loss = 1.67358
I0521 02:05:10.052240  8072 solver.cpp:253]     Train net output #0: loss = 1.67358 (* 1 = 1.67358 loss)
I0521 02:05:10.052253  8072 sgd_solver.cpp:106] Iteration 1809, lr = 0.0025
I0521 02:05:17.875277  8072 solver.cpp:237] Iteration 1836, loss = 1.69938
I0521 02:05:17.875309  8072 solver.cpp:253]     Train net output #0: loss = 1.69938 (* 1 = 1.69938 loss)
I0521 02:05:17.875327  8072 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0521 02:05:47.865223  8072 solver.cpp:237] Iteration 1863, loss = 1.72328
I0521 02:05:47.865392  8072 solver.cpp:253]     Train net output #0: loss = 1.72328 (* 1 = 1.72328 loss)
I0521 02:05:47.865406  8072 sgd_solver.cpp:106] Iteration 1863, lr = 0.0025
I0521 02:05:55.688257  8072 solver.cpp:237] Iteration 1890, loss = 1.62805
I0521 02:05:55.688299  8072 solver.cpp:253]     Train net output #0: loss = 1.62805 (* 1 = 1.62805 loss)
I0521 02:05:55.688318  8072 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0521 02:06:03.514250  8072 solver.cpp:237] Iteration 1917, loss = 1.69055
I0521 02:06:03.514283  8072 solver.cpp:253]     Train net output #0: loss = 1.69055 (* 1 = 1.69055 loss)
I0521 02:06:03.514299  8072 sgd_solver.cpp:106] Iteration 1917, lr = 0.0025
I0521 02:06:09.597470  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1939.caffemodel
I0521 02:06:09.838430  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_1939.solverstate
I0521 02:06:11.401124  8072 solver.cpp:237] Iteration 1944, loss = 1.69299
I0521 02:06:11.401170  8072 solver.cpp:253]     Train net output #0: loss = 1.69299 (* 1 = 1.69299 loss)
I0521 02:06:11.401186  8072 sgd_solver.cpp:106] Iteration 1944, lr = 0.0025
I0521 02:06:19.217519  8072 solver.cpp:237] Iteration 1971, loss = 1.58493
I0521 02:06:19.217665  8072 solver.cpp:253]     Train net output #0: loss = 1.58493 (* 1 = 1.58493 loss)
I0521 02:06:19.217679  8072 sgd_solver.cpp:106] Iteration 1971, lr = 0.0025
I0521 02:06:27.035740  8072 solver.cpp:237] Iteration 1998, loss = 1.67038
I0521 02:06:27.035773  8072 solver.cpp:253]     Train net output #0: loss = 1.67038 (* 1 = 1.67038 loss)
I0521 02:06:27.035787  8072 sgd_solver.cpp:106] Iteration 1998, lr = 0.0025
I0521 02:06:34.859690  8072 solver.cpp:237] Iteration 2025, loss = 1.76729
I0521 02:06:34.859724  8072 solver.cpp:253]     Train net output #0: loss = 1.76729 (* 1 = 1.76729 loss)
I0521 02:06:34.859740  8072 sgd_solver.cpp:106] Iteration 2025, lr = 0.0025
I0521 02:07:04.843700  8072 solver.cpp:237] Iteration 2052, loss = 1.65729
I0521 02:07:04.843869  8072 solver.cpp:253]     Train net output #0: loss = 1.65729 (* 1 = 1.65729 loss)
I0521 02:07:04.843883  8072 sgd_solver.cpp:106] Iteration 2052, lr = 0.0025
I0521 02:07:12.660612  8072 solver.cpp:237] Iteration 2079, loss = 1.69969
I0521 02:07:12.660646  8072 solver.cpp:253]     Train net output #0: loss = 1.69969 (* 1 = 1.69969 loss)
I0521 02:07:12.660665  8072 sgd_solver.cpp:106] Iteration 2079, lr = 0.0025
I0521 02:07:20.481803  8072 solver.cpp:237] Iteration 2106, loss = 1.68893
I0521 02:07:20.481837  8072 solver.cpp:253]     Train net output #0: loss = 1.68893 (* 1 = 1.68893 loss)
I0521 02:07:20.481853  8072 sgd_solver.cpp:106] Iteration 2106, lr = 0.0025
I0521 02:07:28.302072  8072 solver.cpp:237] Iteration 2133, loss = 1.59551
I0521 02:07:28.302105  8072 solver.cpp:253]     Train net output #0: loss = 1.59551 (* 1 = 1.59551 loss)
I0521 02:07:28.302124  8072 sgd_solver.cpp:106] Iteration 2133, lr = 0.0025
I0521 02:07:36.125033  8072 solver.cpp:237] Iteration 2160, loss = 1.60743
I0521 02:07:36.125180  8072 solver.cpp:253]     Train net output #0: loss = 1.60743 (* 1 = 1.60743 loss)
I0521 02:07:36.125195  8072 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0521 02:07:43.943496  8072 solver.cpp:237] Iteration 2187, loss = 1.75602
I0521 02:07:43.943529  8072 solver.cpp:253]     Train net output #0: loss = 1.75602 (* 1 = 1.75602 loss)
I0521 02:07:43.943543  8072 sgd_solver.cpp:106] Iteration 2187, lr = 0.0025
I0521 02:07:51.761775  8072 solver.cpp:237] Iteration 2214, loss = 1.65171
I0521 02:07:51.761809  8072 solver.cpp:253]     Train net output #0: loss = 1.65171 (* 1 = 1.65171 loss)
I0521 02:07:51.761826  8072 sgd_solver.cpp:106] Iteration 2214, lr = 0.0025
I0521 02:07:52.051373  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2216.caffemodel
I0521 02:07:52.293633  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2216.solverstate
I0521 02:07:53.276235  8072 solver.cpp:341] Iteration 2220, Testing net (#0)
I0521 02:08:59.367527  8072 solver.cpp:409]     Test net output #0: accuracy = 0.674348
I0521 02:08:59.367709  8072 solver.cpp:409]     Test net output #1: loss = 1.10241 (* 1 = 1.10241 loss)
I0521 02:09:27.637419  8072 solver.cpp:237] Iteration 2241, loss = 1.70892
I0521 02:09:27.637470  8072 solver.cpp:253]     Train net output #0: loss = 1.70892 (* 1 = 1.70892 loss)
I0521 02:09:27.637486  8072 sgd_solver.cpp:106] Iteration 2241, lr = 0.0025
I0521 02:09:35.461685  8072 solver.cpp:237] Iteration 2268, loss = 1.63879
I0521 02:09:35.461840  8072 solver.cpp:253]     Train net output #0: loss = 1.63879 (* 1 = 1.63879 loss)
I0521 02:09:35.461854  8072 sgd_solver.cpp:106] Iteration 2268, lr = 0.0025
I0521 02:09:43.277257  8072 solver.cpp:237] Iteration 2295, loss = 1.66752
I0521 02:09:43.277293  8072 solver.cpp:253]     Train net output #0: loss = 1.66752 (* 1 = 1.66752 loss)
I0521 02:09:43.277314  8072 sgd_solver.cpp:106] Iteration 2295, lr = 0.0025
I0521 02:09:51.099562  8072 solver.cpp:237] Iteration 2322, loss = 1.66625
I0521 02:09:51.099596  8072 solver.cpp:253]     Train net output #0: loss = 1.66625 (* 1 = 1.66625 loss)
I0521 02:09:51.099612  8072 sgd_solver.cpp:106] Iteration 2322, lr = 0.0025
I0521 02:09:58.915593  8072 solver.cpp:237] Iteration 2349, loss = 1.7069
I0521 02:09:58.915627  8072 solver.cpp:253]     Train net output #0: loss = 1.7069 (* 1 = 1.7069 loss)
I0521 02:09:58.915649  8072 sgd_solver.cpp:106] Iteration 2349, lr = 0.0025
I0521 02:10:06.732231  8072 solver.cpp:237] Iteration 2376, loss = 1.67123
I0521 02:10:06.732389  8072 solver.cpp:253]     Train net output #0: loss = 1.67123 (* 1 = 1.67123 loss)
I0521 02:10:06.732403  8072 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0521 02:10:14.550798  8072 solver.cpp:237] Iteration 2403, loss = 1.64526
I0521 02:10:14.550830  8072 solver.cpp:253]     Train net output #0: loss = 1.64526 (* 1 = 1.64526 loss)
I0521 02:10:14.550848  8072 sgd_solver.cpp:106] Iteration 2403, lr = 0.0025
I0521 02:10:44.542877  8072 solver.cpp:237] Iteration 2430, loss = 1.58632
I0521 02:10:44.543046  8072 solver.cpp:253]     Train net output #0: loss = 1.58632 (* 1 = 1.58632 loss)
I0521 02:10:44.543061  8072 sgd_solver.cpp:106] Iteration 2430, lr = 0.0025
I0521 02:10:52.362283  8072 solver.cpp:237] Iteration 2457, loss = 1.64395
I0521 02:10:52.362328  8072 solver.cpp:253]     Train net output #0: loss = 1.64395 (* 1 = 1.64395 loss)
I0521 02:10:52.362344  8072 sgd_solver.cpp:106] Iteration 2457, lr = 0.0025
I0521 02:11:00.183989  8072 solver.cpp:237] Iteration 2484, loss = 1.66259
I0521 02:11:00.184021  8072 solver.cpp:253]     Train net output #0: loss = 1.66259 (* 1 = 1.66259 loss)
I0521 02:11:00.184038  8072 sgd_solver.cpp:106] Iteration 2484, lr = 0.0025
I0521 02:11:02.503041  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2493.caffemodel
I0521 02:11:02.746367  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2493.solverstate
I0521 02:11:08.071878  8072 solver.cpp:237] Iteration 2511, loss = 1.71896
I0521 02:11:08.071925  8072 solver.cpp:253]     Train net output #0: loss = 1.71896 (* 1 = 1.71896 loss)
I0521 02:11:08.071943  8072 sgd_solver.cpp:106] Iteration 2511, lr = 0.0025
I0521 02:11:15.893249  8072 solver.cpp:237] Iteration 2538, loss = 1.63897
I0521 02:11:15.893407  8072 solver.cpp:253]     Train net output #0: loss = 1.63897 (* 1 = 1.63897 loss)
I0521 02:11:15.893421  8072 sgd_solver.cpp:106] Iteration 2538, lr = 0.0025
I0521 02:11:23.711760  8072 solver.cpp:237] Iteration 2565, loss = 1.66468
I0521 02:11:23.711796  8072 solver.cpp:253]     Train net output #0: loss = 1.66468 (* 1 = 1.66468 loss)
I0521 02:11:23.711819  8072 sgd_solver.cpp:106] Iteration 2565, lr = 0.0025
I0521 02:11:53.702235  8072 solver.cpp:237] Iteration 2592, loss = 1.74994
I0521 02:11:53.702414  8072 solver.cpp:253]     Train net output #0: loss = 1.74994 (* 1 = 1.74994 loss)
I0521 02:11:53.702428  8072 sgd_solver.cpp:106] Iteration 2592, lr = 0.0025
I0521 02:12:01.524430  8072 solver.cpp:237] Iteration 2619, loss = 1.69454
I0521 02:12:01.524462  8072 solver.cpp:253]     Train net output #0: loss = 1.69454 (* 1 = 1.69454 loss)
I0521 02:12:01.524479  8072 sgd_solver.cpp:106] Iteration 2619, lr = 0.0025
I0521 02:12:09.342615  8072 solver.cpp:237] Iteration 2646, loss = 1.64536
I0521 02:12:09.342659  8072 solver.cpp:253]     Train net output #0: loss = 1.64536 (* 1 = 1.64536 loss)
I0521 02:12:09.342675  8072 sgd_solver.cpp:106] Iteration 2646, lr = 0.0025
I0521 02:12:17.160568  8072 solver.cpp:237] Iteration 2673, loss = 1.58012
I0521 02:12:17.160600  8072 solver.cpp:253]     Train net output #0: loss = 1.58012 (* 1 = 1.58012 loss)
I0521 02:12:17.160617  8072 sgd_solver.cpp:106] Iteration 2673, lr = 0.0025
I0521 02:12:24.977279  8072 solver.cpp:237] Iteration 2700, loss = 1.60392
I0521 02:12:24.977427  8072 solver.cpp:253]     Train net output #0: loss = 1.60392 (* 1 = 1.60392 loss)
I0521 02:12:24.977439  8072 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0521 02:12:32.800127  8072 solver.cpp:237] Iteration 2727, loss = 1.66835
I0521 02:12:32.800173  8072 solver.cpp:253]     Train net output #0: loss = 1.66835 (* 1 = 1.66835 loss)
I0521 02:12:32.800189  8072 sgd_solver.cpp:106] Iteration 2727, lr = 0.0025
I0521 02:12:40.621017  8072 solver.cpp:237] Iteration 2754, loss = 1.64351
I0521 02:12:40.621052  8072 solver.cpp:253]     Train net output #0: loss = 1.64351 (* 1 = 1.64351 loss)
I0521 02:12:40.621068  8072 sgd_solver.cpp:106] Iteration 2754, lr = 0.0025
I0521 02:12:44.964443  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2770.caffemodel
I0521 02:12:45.208946  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2770.solverstate
I0521 02:12:46.482067  8072 solver.cpp:341] Iteration 2775, Testing net (#0)
I0521 02:13:31.685261  8072 solver.cpp:409]     Test net output #0: accuracy = 0.686663
I0521 02:13:31.685439  8072 solver.cpp:409]     Test net output #1: loss = 1.06868 (* 1 = 1.06868 loss)
I0521 02:13:32.061779  8072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2777.caffemodel
I0521 02:13:32.305598  8072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277_iter_2777.solverstate
I0521 02:13:32.334584  8072 solver.cpp:326] Optimization Done.
I0521 02:13:32.334611  8072 caffe.cpp:215] Optimization Done.
Application 11236465 resources: utime ~1247s, stime ~227s, Rss ~5329532, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_540_2016-05-20T11.20.52.209277.solver"
	User time (seconds): 0.54
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.58
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15071
	Voluntary context switches: 2717
	Involuntary context switches: 58
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
