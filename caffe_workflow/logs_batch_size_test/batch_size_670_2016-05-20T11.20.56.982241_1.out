2806268
I0521 04:55:05.138906 25138 caffe.cpp:184] Using GPUs 0
I0521 04:55:05.568708 25138 solver.cpp:48] Initializing solver from parameters: 
test_iter: 223
test_interval: 447
base_lr: 0.0025
display: 22
max_iter: 2238
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 223
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241.prototxt"
I0521 04:55:05.570833 25138 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241.prototxt
I0521 04:55:05.582010 25138 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 04:55:05.582068 25138 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 04:55:05.582414 25138 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 670
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:55:05.582590 25138 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:55:05.582614 25138 net.cpp:106] Creating Layer data_hdf5
I0521 04:55:05.582628 25138 net.cpp:411] data_hdf5 -> data
I0521 04:55:05.582662 25138 net.cpp:411] data_hdf5 -> label
I0521 04:55:05.582695 25138 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 04:55:05.584465 25138 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 04:55:05.586725 25138 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 04:55:27.122953 25138 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 04:55:27.128070 25138 net.cpp:150] Setting up data_hdf5
I0521 04:55:27.128110 25138 net.cpp:157] Top shape: 670 1 127 50 (4254500)
I0521 04:55:27.128124 25138 net.cpp:157] Top shape: 670 (670)
I0521 04:55:27.128139 25138 net.cpp:165] Memory required for data: 17020680
I0521 04:55:27.128151 25138 layer_factory.hpp:77] Creating layer conv1
I0521 04:55:27.128185 25138 net.cpp:106] Creating Layer conv1
I0521 04:55:27.128196 25138 net.cpp:454] conv1 <- data
I0521 04:55:27.128216 25138 net.cpp:411] conv1 -> conv1
I0521 04:55:27.538431 25138 net.cpp:150] Setting up conv1
I0521 04:55:27.538477 25138 net.cpp:157] Top shape: 670 12 120 48 (46310400)
I0521 04:55:27.538488 25138 net.cpp:165] Memory required for data: 202262280
I0521 04:55:27.538518 25138 layer_factory.hpp:77] Creating layer relu1
I0521 04:55:27.538538 25138 net.cpp:106] Creating Layer relu1
I0521 04:55:27.538550 25138 net.cpp:454] relu1 <- conv1
I0521 04:55:27.538563 25138 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:55:27.539077 25138 net.cpp:150] Setting up relu1
I0521 04:55:27.539093 25138 net.cpp:157] Top shape: 670 12 120 48 (46310400)
I0521 04:55:27.539103 25138 net.cpp:165] Memory required for data: 387503880
I0521 04:55:27.539114 25138 layer_factory.hpp:77] Creating layer pool1
I0521 04:55:27.539132 25138 net.cpp:106] Creating Layer pool1
I0521 04:55:27.539141 25138 net.cpp:454] pool1 <- conv1
I0521 04:55:27.539155 25138 net.cpp:411] pool1 -> pool1
I0521 04:55:27.539233 25138 net.cpp:150] Setting up pool1
I0521 04:55:27.539247 25138 net.cpp:157] Top shape: 670 12 60 48 (23155200)
I0521 04:55:27.539258 25138 net.cpp:165] Memory required for data: 480124680
I0521 04:55:27.539265 25138 layer_factory.hpp:77] Creating layer conv2
I0521 04:55:27.539288 25138 net.cpp:106] Creating Layer conv2
I0521 04:55:27.539299 25138 net.cpp:454] conv2 <- pool1
I0521 04:55:27.539311 25138 net.cpp:411] conv2 -> conv2
I0521 04:55:27.541986 25138 net.cpp:150] Setting up conv2
I0521 04:55:27.542008 25138 net.cpp:157] Top shape: 670 20 54 46 (33285600)
I0521 04:55:27.542024 25138 net.cpp:165] Memory required for data: 613267080
I0521 04:55:27.542044 25138 layer_factory.hpp:77] Creating layer relu2
I0521 04:55:27.542058 25138 net.cpp:106] Creating Layer relu2
I0521 04:55:27.542068 25138 net.cpp:454] relu2 <- conv2
I0521 04:55:27.542081 25138 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:55:27.542412 25138 net.cpp:150] Setting up relu2
I0521 04:55:27.542425 25138 net.cpp:157] Top shape: 670 20 54 46 (33285600)
I0521 04:55:27.542435 25138 net.cpp:165] Memory required for data: 746409480
I0521 04:55:27.542445 25138 layer_factory.hpp:77] Creating layer pool2
I0521 04:55:27.542459 25138 net.cpp:106] Creating Layer pool2
I0521 04:55:27.542469 25138 net.cpp:454] pool2 <- conv2
I0521 04:55:27.542493 25138 net.cpp:411] pool2 -> pool2
I0521 04:55:27.542562 25138 net.cpp:150] Setting up pool2
I0521 04:55:27.542575 25138 net.cpp:157] Top shape: 670 20 27 46 (16642800)
I0521 04:55:27.542585 25138 net.cpp:165] Memory required for data: 812980680
I0521 04:55:27.542595 25138 layer_factory.hpp:77] Creating layer conv3
I0521 04:55:27.542613 25138 net.cpp:106] Creating Layer conv3
I0521 04:55:27.542624 25138 net.cpp:454] conv3 <- pool2
I0521 04:55:27.542637 25138 net.cpp:411] conv3 -> conv3
I0521 04:55:27.544546 25138 net.cpp:150] Setting up conv3
I0521 04:55:27.544569 25138 net.cpp:157] Top shape: 670 28 22 44 (18159680)
I0521 04:55:27.544580 25138 net.cpp:165] Memory required for data: 885619400
I0521 04:55:27.544598 25138 layer_factory.hpp:77] Creating layer relu3
I0521 04:55:27.544615 25138 net.cpp:106] Creating Layer relu3
I0521 04:55:27.544625 25138 net.cpp:454] relu3 <- conv3
I0521 04:55:27.544636 25138 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:55:27.545112 25138 net.cpp:150] Setting up relu3
I0521 04:55:27.545130 25138 net.cpp:157] Top shape: 670 28 22 44 (18159680)
I0521 04:55:27.545140 25138 net.cpp:165] Memory required for data: 958258120
I0521 04:55:27.545150 25138 layer_factory.hpp:77] Creating layer pool3
I0521 04:55:27.545162 25138 net.cpp:106] Creating Layer pool3
I0521 04:55:27.545172 25138 net.cpp:454] pool3 <- conv3
I0521 04:55:27.545186 25138 net.cpp:411] pool3 -> pool3
I0521 04:55:27.545253 25138 net.cpp:150] Setting up pool3
I0521 04:55:27.545266 25138 net.cpp:157] Top shape: 670 28 11 44 (9079840)
I0521 04:55:27.545276 25138 net.cpp:165] Memory required for data: 994577480
I0521 04:55:27.545284 25138 layer_factory.hpp:77] Creating layer conv4
I0521 04:55:27.545301 25138 net.cpp:106] Creating Layer conv4
I0521 04:55:27.545311 25138 net.cpp:454] conv4 <- pool3
I0521 04:55:27.545325 25138 net.cpp:411] conv4 -> conv4
I0521 04:55:27.548116 25138 net.cpp:150] Setting up conv4
I0521 04:55:27.548146 25138 net.cpp:157] Top shape: 670 36 6 42 (6078240)
I0521 04:55:27.548156 25138 net.cpp:165] Memory required for data: 1018890440
I0521 04:55:27.548171 25138 layer_factory.hpp:77] Creating layer relu4
I0521 04:55:27.548185 25138 net.cpp:106] Creating Layer relu4
I0521 04:55:27.548195 25138 net.cpp:454] relu4 <- conv4
I0521 04:55:27.548208 25138 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:55:27.548678 25138 net.cpp:150] Setting up relu4
I0521 04:55:27.548693 25138 net.cpp:157] Top shape: 670 36 6 42 (6078240)
I0521 04:55:27.548704 25138 net.cpp:165] Memory required for data: 1043203400
I0521 04:55:27.548714 25138 layer_factory.hpp:77] Creating layer pool4
I0521 04:55:27.548727 25138 net.cpp:106] Creating Layer pool4
I0521 04:55:27.548738 25138 net.cpp:454] pool4 <- conv4
I0521 04:55:27.548750 25138 net.cpp:411] pool4 -> pool4
I0521 04:55:27.548820 25138 net.cpp:150] Setting up pool4
I0521 04:55:27.548832 25138 net.cpp:157] Top shape: 670 36 3 42 (3039120)
I0521 04:55:27.548843 25138 net.cpp:165] Memory required for data: 1055359880
I0521 04:55:27.548853 25138 layer_factory.hpp:77] Creating layer ip1
I0521 04:55:27.548873 25138 net.cpp:106] Creating Layer ip1
I0521 04:55:27.548883 25138 net.cpp:454] ip1 <- pool4
I0521 04:55:27.548897 25138 net.cpp:411] ip1 -> ip1
I0521 04:55:27.564265 25138 net.cpp:150] Setting up ip1
I0521 04:55:27.564293 25138 net.cpp:157] Top shape: 670 196 (131320)
I0521 04:55:27.564306 25138 net.cpp:165] Memory required for data: 1055885160
I0521 04:55:27.564327 25138 layer_factory.hpp:77] Creating layer relu5
I0521 04:55:27.564342 25138 net.cpp:106] Creating Layer relu5
I0521 04:55:27.564352 25138 net.cpp:454] relu5 <- ip1
I0521 04:55:27.564364 25138 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:55:27.564707 25138 net.cpp:150] Setting up relu5
I0521 04:55:27.564720 25138 net.cpp:157] Top shape: 670 196 (131320)
I0521 04:55:27.564730 25138 net.cpp:165] Memory required for data: 1056410440
I0521 04:55:27.564740 25138 layer_factory.hpp:77] Creating layer drop1
I0521 04:55:27.564761 25138 net.cpp:106] Creating Layer drop1
I0521 04:55:27.564771 25138 net.cpp:454] drop1 <- ip1
I0521 04:55:27.564797 25138 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:55:27.564843 25138 net.cpp:150] Setting up drop1
I0521 04:55:27.564857 25138 net.cpp:157] Top shape: 670 196 (131320)
I0521 04:55:27.564867 25138 net.cpp:165] Memory required for data: 1056935720
I0521 04:55:27.564877 25138 layer_factory.hpp:77] Creating layer ip2
I0521 04:55:27.564895 25138 net.cpp:106] Creating Layer ip2
I0521 04:55:27.564913 25138 net.cpp:454] ip2 <- ip1
I0521 04:55:27.564925 25138 net.cpp:411] ip2 -> ip2
I0521 04:55:27.565389 25138 net.cpp:150] Setting up ip2
I0521 04:55:27.565402 25138 net.cpp:157] Top shape: 670 98 (65660)
I0521 04:55:27.565412 25138 net.cpp:165] Memory required for data: 1057198360
I0521 04:55:27.565428 25138 layer_factory.hpp:77] Creating layer relu6
I0521 04:55:27.565440 25138 net.cpp:106] Creating Layer relu6
I0521 04:55:27.565450 25138 net.cpp:454] relu6 <- ip2
I0521 04:55:27.565462 25138 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:55:27.565981 25138 net.cpp:150] Setting up relu6
I0521 04:55:27.565997 25138 net.cpp:157] Top shape: 670 98 (65660)
I0521 04:55:27.566007 25138 net.cpp:165] Memory required for data: 1057461000
I0521 04:55:27.566017 25138 layer_factory.hpp:77] Creating layer drop2
I0521 04:55:27.566030 25138 net.cpp:106] Creating Layer drop2
I0521 04:55:27.566040 25138 net.cpp:454] drop2 <- ip2
I0521 04:55:27.566052 25138 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:55:27.566094 25138 net.cpp:150] Setting up drop2
I0521 04:55:27.566107 25138 net.cpp:157] Top shape: 670 98 (65660)
I0521 04:55:27.566118 25138 net.cpp:165] Memory required for data: 1057723640
I0521 04:55:27.566128 25138 layer_factory.hpp:77] Creating layer ip3
I0521 04:55:27.566140 25138 net.cpp:106] Creating Layer ip3
I0521 04:55:27.566149 25138 net.cpp:454] ip3 <- ip2
I0521 04:55:27.566162 25138 net.cpp:411] ip3 -> ip3
I0521 04:55:27.566371 25138 net.cpp:150] Setting up ip3
I0521 04:55:27.566385 25138 net.cpp:157] Top shape: 670 11 (7370)
I0521 04:55:27.566395 25138 net.cpp:165] Memory required for data: 1057753120
I0521 04:55:27.566409 25138 layer_factory.hpp:77] Creating layer drop3
I0521 04:55:27.566421 25138 net.cpp:106] Creating Layer drop3
I0521 04:55:27.566431 25138 net.cpp:454] drop3 <- ip3
I0521 04:55:27.566442 25138 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:55:27.566483 25138 net.cpp:150] Setting up drop3
I0521 04:55:27.566494 25138 net.cpp:157] Top shape: 670 11 (7370)
I0521 04:55:27.566504 25138 net.cpp:165] Memory required for data: 1057782600
I0521 04:55:27.566514 25138 layer_factory.hpp:77] Creating layer loss
I0521 04:55:27.566532 25138 net.cpp:106] Creating Layer loss
I0521 04:55:27.566542 25138 net.cpp:454] loss <- ip3
I0521 04:55:27.566553 25138 net.cpp:454] loss <- label
I0521 04:55:27.566566 25138 net.cpp:411] loss -> loss
I0521 04:55:27.566583 25138 layer_factory.hpp:77] Creating layer loss
I0521 04:55:27.567229 25138 net.cpp:150] Setting up loss
I0521 04:55:27.567250 25138 net.cpp:157] Top shape: (1)
I0521 04:55:27.567262 25138 net.cpp:160]     with loss weight 1
I0521 04:55:27.567304 25138 net.cpp:165] Memory required for data: 1057782604
I0521 04:55:27.567315 25138 net.cpp:226] loss needs backward computation.
I0521 04:55:27.567325 25138 net.cpp:226] drop3 needs backward computation.
I0521 04:55:27.567335 25138 net.cpp:226] ip3 needs backward computation.
I0521 04:55:27.567347 25138 net.cpp:226] drop2 needs backward computation.
I0521 04:55:27.567355 25138 net.cpp:226] relu6 needs backward computation.
I0521 04:55:27.567365 25138 net.cpp:226] ip2 needs backward computation.
I0521 04:55:27.567375 25138 net.cpp:226] drop1 needs backward computation.
I0521 04:55:27.567385 25138 net.cpp:226] relu5 needs backward computation.
I0521 04:55:27.567394 25138 net.cpp:226] ip1 needs backward computation.
I0521 04:55:27.567404 25138 net.cpp:226] pool4 needs backward computation.
I0521 04:55:27.567414 25138 net.cpp:226] relu4 needs backward computation.
I0521 04:55:27.567425 25138 net.cpp:226] conv4 needs backward computation.
I0521 04:55:27.567435 25138 net.cpp:226] pool3 needs backward computation.
I0521 04:55:27.567453 25138 net.cpp:226] relu3 needs backward computation.
I0521 04:55:27.567463 25138 net.cpp:226] conv3 needs backward computation.
I0521 04:55:27.567474 25138 net.cpp:226] pool2 needs backward computation.
I0521 04:55:27.567484 25138 net.cpp:226] relu2 needs backward computation.
I0521 04:55:27.567494 25138 net.cpp:226] conv2 needs backward computation.
I0521 04:55:27.567505 25138 net.cpp:226] pool1 needs backward computation.
I0521 04:55:27.567515 25138 net.cpp:226] relu1 needs backward computation.
I0521 04:55:27.567525 25138 net.cpp:226] conv1 needs backward computation.
I0521 04:55:27.567536 25138 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:55:27.567545 25138 net.cpp:270] This network produces output loss
I0521 04:55:27.567569 25138 net.cpp:283] Network initialization done.
I0521 04:55:27.569165 25138 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241.prototxt
I0521 04:55:27.569236 25138 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 04:55:27.569591 25138 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 670
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:55:27.569779 25138 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:55:27.569794 25138 net.cpp:106] Creating Layer data_hdf5
I0521 04:55:27.569807 25138 net.cpp:411] data_hdf5 -> data
I0521 04:55:27.569823 25138 net.cpp:411] data_hdf5 -> label
I0521 04:55:27.569839 25138 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 04:55:27.571075 25138 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 04:55:48.921259 25138 net.cpp:150] Setting up data_hdf5
I0521 04:55:48.921425 25138 net.cpp:157] Top shape: 670 1 127 50 (4254500)
I0521 04:55:48.921440 25138 net.cpp:157] Top shape: 670 (670)
I0521 04:55:48.921452 25138 net.cpp:165] Memory required for data: 17020680
I0521 04:55:48.921466 25138 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 04:55:48.921494 25138 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 04:55:48.921505 25138 net.cpp:454] label_data_hdf5_1_split <- label
I0521 04:55:48.921519 25138 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 04:55:48.921541 25138 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 04:55:48.921613 25138 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 04:55:48.921627 25138 net.cpp:157] Top shape: 670 (670)
I0521 04:55:48.921638 25138 net.cpp:157] Top shape: 670 (670)
I0521 04:55:48.921648 25138 net.cpp:165] Memory required for data: 17026040
I0521 04:55:48.921658 25138 layer_factory.hpp:77] Creating layer conv1
I0521 04:55:48.921677 25138 net.cpp:106] Creating Layer conv1
I0521 04:55:48.921689 25138 net.cpp:454] conv1 <- data
I0521 04:55:48.921703 25138 net.cpp:411] conv1 -> conv1
I0521 04:55:48.923638 25138 net.cpp:150] Setting up conv1
I0521 04:55:48.923662 25138 net.cpp:157] Top shape: 670 12 120 48 (46310400)
I0521 04:55:48.923674 25138 net.cpp:165] Memory required for data: 202267640
I0521 04:55:48.923694 25138 layer_factory.hpp:77] Creating layer relu1
I0521 04:55:48.923709 25138 net.cpp:106] Creating Layer relu1
I0521 04:55:48.923719 25138 net.cpp:454] relu1 <- conv1
I0521 04:55:48.923732 25138 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:55:48.924231 25138 net.cpp:150] Setting up relu1
I0521 04:55:48.924247 25138 net.cpp:157] Top shape: 670 12 120 48 (46310400)
I0521 04:55:48.924257 25138 net.cpp:165] Memory required for data: 387509240
I0521 04:55:48.924268 25138 layer_factory.hpp:77] Creating layer pool1
I0521 04:55:48.924283 25138 net.cpp:106] Creating Layer pool1
I0521 04:55:48.924293 25138 net.cpp:454] pool1 <- conv1
I0521 04:55:48.924307 25138 net.cpp:411] pool1 -> pool1
I0521 04:55:48.924381 25138 net.cpp:150] Setting up pool1
I0521 04:55:48.924394 25138 net.cpp:157] Top shape: 670 12 60 48 (23155200)
I0521 04:55:48.924404 25138 net.cpp:165] Memory required for data: 480130040
I0521 04:55:48.924414 25138 layer_factory.hpp:77] Creating layer conv2
I0521 04:55:48.924432 25138 net.cpp:106] Creating Layer conv2
I0521 04:55:48.924443 25138 net.cpp:454] conv2 <- pool1
I0521 04:55:48.924456 25138 net.cpp:411] conv2 -> conv2
I0521 04:55:48.926373 25138 net.cpp:150] Setting up conv2
I0521 04:55:48.926393 25138 net.cpp:157] Top shape: 670 20 54 46 (33285600)
I0521 04:55:48.926408 25138 net.cpp:165] Memory required for data: 613272440
I0521 04:55:48.926427 25138 layer_factory.hpp:77] Creating layer relu2
I0521 04:55:48.926440 25138 net.cpp:106] Creating Layer relu2
I0521 04:55:48.926450 25138 net.cpp:454] relu2 <- conv2
I0521 04:55:48.926462 25138 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:55:48.926794 25138 net.cpp:150] Setting up relu2
I0521 04:55:48.926806 25138 net.cpp:157] Top shape: 670 20 54 46 (33285600)
I0521 04:55:48.926817 25138 net.cpp:165] Memory required for data: 746414840
I0521 04:55:48.926828 25138 layer_factory.hpp:77] Creating layer pool2
I0521 04:55:48.926841 25138 net.cpp:106] Creating Layer pool2
I0521 04:55:48.926851 25138 net.cpp:454] pool2 <- conv2
I0521 04:55:48.926863 25138 net.cpp:411] pool2 -> pool2
I0521 04:55:48.926934 25138 net.cpp:150] Setting up pool2
I0521 04:55:48.926946 25138 net.cpp:157] Top shape: 670 20 27 46 (16642800)
I0521 04:55:48.926956 25138 net.cpp:165] Memory required for data: 812986040
I0521 04:55:48.926964 25138 layer_factory.hpp:77] Creating layer conv3
I0521 04:55:48.926985 25138 net.cpp:106] Creating Layer conv3
I0521 04:55:48.926995 25138 net.cpp:454] conv3 <- pool2
I0521 04:55:48.927008 25138 net.cpp:411] conv3 -> conv3
I0521 04:55:48.928977 25138 net.cpp:150] Setting up conv3
I0521 04:55:48.929000 25138 net.cpp:157] Top shape: 670 28 22 44 (18159680)
I0521 04:55:48.929013 25138 net.cpp:165] Memory required for data: 885624760
I0521 04:55:48.929044 25138 layer_factory.hpp:77] Creating layer relu3
I0521 04:55:48.929057 25138 net.cpp:106] Creating Layer relu3
I0521 04:55:48.929067 25138 net.cpp:454] relu3 <- conv3
I0521 04:55:48.929081 25138 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:55:48.929556 25138 net.cpp:150] Setting up relu3
I0521 04:55:48.929572 25138 net.cpp:157] Top shape: 670 28 22 44 (18159680)
I0521 04:55:48.929582 25138 net.cpp:165] Memory required for data: 958263480
I0521 04:55:48.929592 25138 layer_factory.hpp:77] Creating layer pool3
I0521 04:55:48.929605 25138 net.cpp:106] Creating Layer pool3
I0521 04:55:48.929615 25138 net.cpp:454] pool3 <- conv3
I0521 04:55:48.929628 25138 net.cpp:411] pool3 -> pool3
I0521 04:55:48.929700 25138 net.cpp:150] Setting up pool3
I0521 04:55:48.929714 25138 net.cpp:157] Top shape: 670 28 11 44 (9079840)
I0521 04:55:48.929723 25138 net.cpp:165] Memory required for data: 994582840
I0521 04:55:48.929733 25138 layer_factory.hpp:77] Creating layer conv4
I0521 04:55:48.929750 25138 net.cpp:106] Creating Layer conv4
I0521 04:55:48.929760 25138 net.cpp:454] conv4 <- pool3
I0521 04:55:48.929774 25138 net.cpp:411] conv4 -> conv4
I0521 04:55:48.931819 25138 net.cpp:150] Setting up conv4
I0521 04:55:48.931841 25138 net.cpp:157] Top shape: 670 36 6 42 (6078240)
I0521 04:55:48.931854 25138 net.cpp:165] Memory required for data: 1018895800
I0521 04:55:48.931869 25138 layer_factory.hpp:77] Creating layer relu4
I0521 04:55:48.931882 25138 net.cpp:106] Creating Layer relu4
I0521 04:55:48.931892 25138 net.cpp:454] relu4 <- conv4
I0521 04:55:48.931905 25138 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:55:48.932373 25138 net.cpp:150] Setting up relu4
I0521 04:55:48.932389 25138 net.cpp:157] Top shape: 670 36 6 42 (6078240)
I0521 04:55:48.932399 25138 net.cpp:165] Memory required for data: 1043208760
I0521 04:55:48.932409 25138 layer_factory.hpp:77] Creating layer pool4
I0521 04:55:48.932421 25138 net.cpp:106] Creating Layer pool4
I0521 04:55:48.932431 25138 net.cpp:454] pool4 <- conv4
I0521 04:55:48.932445 25138 net.cpp:411] pool4 -> pool4
I0521 04:55:48.932515 25138 net.cpp:150] Setting up pool4
I0521 04:55:48.932529 25138 net.cpp:157] Top shape: 670 36 3 42 (3039120)
I0521 04:55:48.932539 25138 net.cpp:165] Memory required for data: 1055365240
I0521 04:55:48.932548 25138 layer_factory.hpp:77] Creating layer ip1
I0521 04:55:48.932564 25138 net.cpp:106] Creating Layer ip1
I0521 04:55:48.932574 25138 net.cpp:454] ip1 <- pool4
I0521 04:55:48.932588 25138 net.cpp:411] ip1 -> ip1
I0521 04:55:48.948027 25138 net.cpp:150] Setting up ip1
I0521 04:55:48.948056 25138 net.cpp:157] Top shape: 670 196 (131320)
I0521 04:55:48.948067 25138 net.cpp:165] Memory required for data: 1055890520
I0521 04:55:48.948089 25138 layer_factory.hpp:77] Creating layer relu5
I0521 04:55:48.948104 25138 net.cpp:106] Creating Layer relu5
I0521 04:55:48.948114 25138 net.cpp:454] relu5 <- ip1
I0521 04:55:48.948128 25138 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:55:48.948475 25138 net.cpp:150] Setting up relu5
I0521 04:55:48.948489 25138 net.cpp:157] Top shape: 670 196 (131320)
I0521 04:55:48.948499 25138 net.cpp:165] Memory required for data: 1056415800
I0521 04:55:48.948510 25138 layer_factory.hpp:77] Creating layer drop1
I0521 04:55:48.948529 25138 net.cpp:106] Creating Layer drop1
I0521 04:55:48.948539 25138 net.cpp:454] drop1 <- ip1
I0521 04:55:48.948552 25138 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:55:48.948596 25138 net.cpp:150] Setting up drop1
I0521 04:55:48.948609 25138 net.cpp:157] Top shape: 670 196 (131320)
I0521 04:55:48.948618 25138 net.cpp:165] Memory required for data: 1056941080
I0521 04:55:48.948628 25138 layer_factory.hpp:77] Creating layer ip2
I0521 04:55:48.948643 25138 net.cpp:106] Creating Layer ip2
I0521 04:55:48.948652 25138 net.cpp:454] ip2 <- ip1
I0521 04:55:48.948668 25138 net.cpp:411] ip2 -> ip2
I0521 04:55:48.949154 25138 net.cpp:150] Setting up ip2
I0521 04:55:48.949167 25138 net.cpp:157] Top shape: 670 98 (65660)
I0521 04:55:48.949177 25138 net.cpp:165] Memory required for data: 1057203720
I0521 04:55:48.949204 25138 layer_factory.hpp:77] Creating layer relu6
I0521 04:55:48.949218 25138 net.cpp:106] Creating Layer relu6
I0521 04:55:48.949228 25138 net.cpp:454] relu6 <- ip2
I0521 04:55:48.949240 25138 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:55:48.949769 25138 net.cpp:150] Setting up relu6
I0521 04:55:48.949785 25138 net.cpp:157] Top shape: 670 98 (65660)
I0521 04:55:48.949795 25138 net.cpp:165] Memory required for data: 1057466360
I0521 04:55:48.949805 25138 layer_factory.hpp:77] Creating layer drop2
I0521 04:55:48.949818 25138 net.cpp:106] Creating Layer drop2
I0521 04:55:48.949828 25138 net.cpp:454] drop2 <- ip2
I0521 04:55:48.949841 25138 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:55:48.949885 25138 net.cpp:150] Setting up drop2
I0521 04:55:48.949898 25138 net.cpp:157] Top shape: 670 98 (65660)
I0521 04:55:48.949908 25138 net.cpp:165] Memory required for data: 1057729000
I0521 04:55:48.949918 25138 layer_factory.hpp:77] Creating layer ip3
I0521 04:55:48.949933 25138 net.cpp:106] Creating Layer ip3
I0521 04:55:48.949942 25138 net.cpp:454] ip3 <- ip2
I0521 04:55:48.949956 25138 net.cpp:411] ip3 -> ip3
I0521 04:55:48.950181 25138 net.cpp:150] Setting up ip3
I0521 04:55:48.950193 25138 net.cpp:157] Top shape: 670 11 (7370)
I0521 04:55:48.950203 25138 net.cpp:165] Memory required for data: 1057758480
I0521 04:55:48.950218 25138 layer_factory.hpp:77] Creating layer drop3
I0521 04:55:48.950232 25138 net.cpp:106] Creating Layer drop3
I0521 04:55:48.950242 25138 net.cpp:454] drop3 <- ip3
I0521 04:55:48.950254 25138 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:55:48.950295 25138 net.cpp:150] Setting up drop3
I0521 04:55:48.950309 25138 net.cpp:157] Top shape: 670 11 (7370)
I0521 04:55:48.950319 25138 net.cpp:165] Memory required for data: 1057787960
I0521 04:55:48.950328 25138 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 04:55:48.950340 25138 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 04:55:48.950350 25138 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 04:55:48.950363 25138 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 04:55:48.950378 25138 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 04:55:48.950460 25138 net.cpp:150] Setting up ip3_drop3_0_split
I0521 04:55:48.950474 25138 net.cpp:157] Top shape: 670 11 (7370)
I0521 04:55:48.950485 25138 net.cpp:157] Top shape: 670 11 (7370)
I0521 04:55:48.950495 25138 net.cpp:165] Memory required for data: 1057846920
I0521 04:55:48.950503 25138 layer_factory.hpp:77] Creating layer accuracy
I0521 04:55:48.950525 25138 net.cpp:106] Creating Layer accuracy
I0521 04:55:48.950536 25138 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 04:55:48.950546 25138 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 04:55:48.950561 25138 net.cpp:411] accuracy -> accuracy
I0521 04:55:48.950583 25138 net.cpp:150] Setting up accuracy
I0521 04:55:48.950597 25138 net.cpp:157] Top shape: (1)
I0521 04:55:48.950605 25138 net.cpp:165] Memory required for data: 1057846924
I0521 04:55:48.950615 25138 layer_factory.hpp:77] Creating layer loss
I0521 04:55:48.950629 25138 net.cpp:106] Creating Layer loss
I0521 04:55:48.950639 25138 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 04:55:48.950650 25138 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 04:55:48.950664 25138 net.cpp:411] loss -> loss
I0521 04:55:48.950680 25138 layer_factory.hpp:77] Creating layer loss
I0521 04:55:48.951174 25138 net.cpp:150] Setting up loss
I0521 04:55:48.951187 25138 net.cpp:157] Top shape: (1)
I0521 04:55:48.951198 25138 net.cpp:160]     with loss weight 1
I0521 04:55:48.951216 25138 net.cpp:165] Memory required for data: 1057846928
I0521 04:55:48.951227 25138 net.cpp:226] loss needs backward computation.
I0521 04:55:48.951238 25138 net.cpp:228] accuracy does not need backward computation.
I0521 04:55:48.951249 25138 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 04:55:48.951259 25138 net.cpp:226] drop3 needs backward computation.
I0521 04:55:48.951269 25138 net.cpp:226] ip3 needs backward computation.
I0521 04:55:48.951282 25138 net.cpp:226] drop2 needs backward computation.
I0521 04:55:48.951298 25138 net.cpp:226] relu6 needs backward computation.
I0521 04:55:48.951308 25138 net.cpp:226] ip2 needs backward computation.
I0521 04:55:48.951318 25138 net.cpp:226] drop1 needs backward computation.
I0521 04:55:48.951328 25138 net.cpp:226] relu5 needs backward computation.
I0521 04:55:48.951338 25138 net.cpp:226] ip1 needs backward computation.
I0521 04:55:48.951347 25138 net.cpp:226] pool4 needs backward computation.
I0521 04:55:48.951359 25138 net.cpp:226] relu4 needs backward computation.
I0521 04:55:48.951369 25138 net.cpp:226] conv4 needs backward computation.
I0521 04:55:48.951378 25138 net.cpp:226] pool3 needs backward computation.
I0521 04:55:48.951386 25138 net.cpp:226] relu3 needs backward computation.
I0521 04:55:48.951396 25138 net.cpp:226] conv3 needs backward computation.
I0521 04:55:48.951407 25138 net.cpp:226] pool2 needs backward computation.
I0521 04:55:48.951418 25138 net.cpp:226] relu2 needs backward computation.
I0521 04:55:48.951428 25138 net.cpp:226] conv2 needs backward computation.
I0521 04:55:48.951438 25138 net.cpp:226] pool1 needs backward computation.
I0521 04:55:48.951448 25138 net.cpp:226] relu1 needs backward computation.
I0521 04:55:48.951458 25138 net.cpp:226] conv1 needs backward computation.
I0521 04:55:48.951469 25138 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 04:55:48.951481 25138 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:55:48.951491 25138 net.cpp:270] This network produces output accuracy
I0521 04:55:48.951501 25138 net.cpp:270] This network produces output loss
I0521 04:55:48.951529 25138 net.cpp:283] Network initialization done.
I0521 04:55:48.951663 25138 solver.cpp:60] Solver scaffolding done.
I0521 04:55:48.952795 25138 caffe.cpp:212] Starting Optimization
I0521 04:55:48.952813 25138 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 04:55:48.952826 25138 solver.cpp:289] Learning Rate Policy: fixed
I0521 04:55:48.954059 25138 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 04:56:34.929719 25138 solver.cpp:409]     Test net output #0: accuracy = 0.101533
I0521 04:56:34.929882 25138 solver.cpp:409]     Test net output #1: loss = 2.39801 (* 1 = 2.39801 loss)
I0521 04:56:35.057133 25138 solver.cpp:237] Iteration 0, loss = 2.39833
I0521 04:56:35.057169 25138 solver.cpp:253]     Train net output #0: loss = 2.39833 (* 1 = 2.39833 loss)
I0521 04:56:35.057188 25138 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 04:56:43.001613 25138 solver.cpp:237] Iteration 22, loss = 2.38491
I0521 04:56:43.001649 25138 solver.cpp:253]     Train net output #0: loss = 2.38491 (* 1 = 2.38491 loss)
I0521 04:56:43.001665 25138 sgd_solver.cpp:106] Iteration 22, lr = 0.0025
I0521 04:56:50.944068 25138 solver.cpp:237] Iteration 44, loss = 2.37664
I0521 04:56:50.944102 25138 solver.cpp:253]     Train net output #0: loss = 2.37664 (* 1 = 2.37664 loss)
I0521 04:56:50.944118 25138 sgd_solver.cpp:106] Iteration 44, lr = 0.0025
I0521 04:56:58.884232 25138 solver.cpp:237] Iteration 66, loss = 2.35542
I0521 04:56:58.884268 25138 solver.cpp:253]     Train net output #0: loss = 2.35542 (* 1 = 2.35542 loss)
I0521 04:56:58.884282 25138 sgd_solver.cpp:106] Iteration 66, lr = 0.0025
I0521 04:57:06.827888 25138 solver.cpp:237] Iteration 88, loss = 2.35148
I0521 04:57:06.828032 25138 solver.cpp:253]     Train net output #0: loss = 2.35148 (* 1 = 2.35148 loss)
I0521 04:57:06.828044 25138 sgd_solver.cpp:106] Iteration 88, lr = 0.0025
I0521 04:57:14.776082 25138 solver.cpp:237] Iteration 110, loss = 2.31988
I0521 04:57:14.776113 25138 solver.cpp:253]     Train net output #0: loss = 2.31988 (* 1 = 2.31988 loss)
I0521 04:57:14.776129 25138 sgd_solver.cpp:106] Iteration 110, lr = 0.0025
I0521 04:57:22.716392 25138 solver.cpp:237] Iteration 132, loss = 2.33082
I0521 04:57:22.716428 25138 solver.cpp:253]     Train net output #0: loss = 2.33082 (* 1 = 2.33082 loss)
I0521 04:57:22.716444 25138 sgd_solver.cpp:106] Iteration 132, lr = 0.0025
I0521 04:57:52.805179 25138 solver.cpp:237] Iteration 154, loss = 2.32164
I0521 04:57:52.805352 25138 solver.cpp:253]     Train net output #0: loss = 2.32164 (* 1 = 2.32164 loss)
I0521 04:57:52.805368 25138 sgd_solver.cpp:106] Iteration 154, lr = 0.0025
I0521 04:58:00.749342 25138 solver.cpp:237] Iteration 176, loss = 2.32198
I0521 04:58:00.749375 25138 solver.cpp:253]     Train net output #0: loss = 2.32198 (* 1 = 2.32198 loss)
I0521 04:58:00.749393 25138 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 04:58:08.697221 25138 solver.cpp:237] Iteration 198, loss = 2.32108
I0521 04:58:08.697254 25138 solver.cpp:253]     Train net output #0: loss = 2.32108 (* 1 = 2.32108 loss)
I0521 04:58:08.697270 25138 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 04:58:16.638713 25138 solver.cpp:237] Iteration 220, loss = 2.30331
I0521 04:58:16.638749 25138 solver.cpp:253]     Train net output #0: loss = 2.30331 (* 1 = 2.30331 loss)
I0521 04:58:16.638767 25138 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0521 04:58:17.363028 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_223.caffemodel
I0521 04:58:17.658496 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_223.solverstate
I0521 04:58:24.655990 25138 solver.cpp:237] Iteration 242, loss = 2.25245
I0521 04:58:24.656142 25138 solver.cpp:253]     Train net output #0: loss = 2.25245 (* 1 = 2.25245 loss)
I0521 04:58:24.656157 25138 sgd_solver.cpp:106] Iteration 242, lr = 0.0025
I0521 04:58:32.596588 25138 solver.cpp:237] Iteration 264, loss = 2.25126
I0521 04:58:32.596621 25138 solver.cpp:253]     Train net output #0: loss = 2.25126 (* 1 = 2.25126 loss)
I0521 04:58:32.596638 25138 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0521 04:58:40.537921 25138 solver.cpp:237] Iteration 286, loss = 2.22168
I0521 04:58:40.537966 25138 solver.cpp:253]     Train net output #0: loss = 2.22168 (* 1 = 2.22168 loss)
I0521 04:58:40.537981 25138 sgd_solver.cpp:106] Iteration 286, lr = 0.0025
I0521 04:59:10.637434 25138 solver.cpp:237] Iteration 308, loss = 2.15611
I0521 04:59:10.637590 25138 solver.cpp:253]     Train net output #0: loss = 2.15611 (* 1 = 2.15611 loss)
I0521 04:59:10.637606 25138 sgd_solver.cpp:106] Iteration 308, lr = 0.0025
I0521 04:59:18.583394 25138 solver.cpp:237] Iteration 330, loss = 2.15817
I0521 04:59:18.583426 25138 solver.cpp:253]     Train net output #0: loss = 2.15817 (* 1 = 2.15817 loss)
I0521 04:59:18.583444 25138 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 04:59:26.533869 25138 solver.cpp:237] Iteration 352, loss = 2.11048
I0521 04:59:26.533902 25138 solver.cpp:253]     Train net output #0: loss = 2.11048 (* 1 = 2.11048 loss)
I0521 04:59:26.533921 25138 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 04:59:34.478060 25138 solver.cpp:237] Iteration 374, loss = 2.12625
I0521 04:59:34.478096 25138 solver.cpp:253]     Train net output #0: loss = 2.12625 (* 1 = 2.12625 loss)
I0521 04:59:34.478117 25138 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 04:59:42.428668 25138 solver.cpp:237] Iteration 396, loss = 2.03799
I0521 04:59:42.428812 25138 solver.cpp:253]     Train net output #0: loss = 2.03799 (* 1 = 2.03799 loss)
I0521 04:59:42.428825 25138 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 04:59:50.371999 25138 solver.cpp:237] Iteration 418, loss = 2.0248
I0521 04:59:50.372031 25138 solver.cpp:253]     Train net output #0: loss = 2.0248 (* 1 = 2.0248 loss)
I0521 04:59:50.372050 25138 sgd_solver.cpp:106] Iteration 418, lr = 0.0025
I0521 04:59:58.321518 25138 solver.cpp:237] Iteration 440, loss = 2.06905
I0521 04:59:58.321557 25138 solver.cpp:253]     Train net output #0: loss = 2.06905 (* 1 = 2.06905 loss)
I0521 04:59:58.321573 25138 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0521 05:00:00.129272 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_446.caffemodel
I0521 05:00:00.422312 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_446.solverstate
I0521 05:00:00.555202 25138 solver.cpp:341] Iteration 447, Testing net (#0)
I0521 05:00:45.815843 25138 solver.cpp:409]     Test net output #0: accuracy = 0.537133
I0521 05:00:45.816004 25138 solver.cpp:409]     Test net output #1: loss = 1.79243 (* 1 = 1.79243 loss)
I0521 05:01:13.510789 25138 solver.cpp:237] Iteration 462, loss = 1.97518
I0521 05:01:13.510840 25138 solver.cpp:253]     Train net output #0: loss = 1.97518 (* 1 = 1.97518 loss)
I0521 05:01:13.510857 25138 sgd_solver.cpp:106] Iteration 462, lr = 0.0025
I0521 05:01:21.456660 25138 solver.cpp:237] Iteration 484, loss = 1.95659
I0521 05:01:21.456807 25138 solver.cpp:253]     Train net output #0: loss = 1.95659 (* 1 = 1.95659 loss)
I0521 05:01:21.456820 25138 sgd_solver.cpp:106] Iteration 484, lr = 0.0025
I0521 05:01:29.399005 25138 solver.cpp:237] Iteration 506, loss = 2.03288
I0521 05:01:29.399037 25138 solver.cpp:253]     Train net output #0: loss = 2.03288 (* 1 = 2.03288 loss)
I0521 05:01:29.399055 25138 sgd_solver.cpp:106] Iteration 506, lr = 0.0025
I0521 05:01:37.338512 25138 solver.cpp:237] Iteration 528, loss = 1.98501
I0521 05:01:37.338552 25138 solver.cpp:253]     Train net output #0: loss = 1.98501 (* 1 = 1.98501 loss)
I0521 05:01:37.338574 25138 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 05:01:45.276862 25138 solver.cpp:237] Iteration 550, loss = 1.93929
I0521 05:01:45.276895 25138 solver.cpp:253]     Train net output #0: loss = 1.93929 (* 1 = 1.93929 loss)
I0521 05:01:45.276916 25138 sgd_solver.cpp:106] Iteration 550, lr = 0.0025
I0521 05:01:53.219665 25138 solver.cpp:237] Iteration 572, loss = 1.93022
I0521 05:01:53.219799 25138 solver.cpp:253]     Train net output #0: loss = 1.93022 (* 1 = 1.93022 loss)
I0521 05:01:53.219812 25138 sgd_solver.cpp:106] Iteration 572, lr = 0.0025
I0521 05:02:01.158010 25138 solver.cpp:237] Iteration 594, loss = 1.89017
I0521 05:02:01.158049 25138 solver.cpp:253]     Train net output #0: loss = 1.89017 (* 1 = 1.89017 loss)
I0521 05:02:01.158069 25138 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 05:02:31.295408 25138 solver.cpp:237] Iteration 616, loss = 1.91218
I0521 05:02:31.295580 25138 solver.cpp:253]     Train net output #0: loss = 1.91218 (* 1 = 1.91218 loss)
I0521 05:02:31.295596 25138 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0521 05:02:39.237282 25138 solver.cpp:237] Iteration 638, loss = 1.90648
I0521 05:02:39.237314 25138 solver.cpp:253]     Train net output #0: loss = 1.90648 (* 1 = 1.90648 loss)
I0521 05:02:39.237329 25138 sgd_solver.cpp:106] Iteration 638, lr = 0.0025
I0521 05:02:47.181516 25138 solver.cpp:237] Iteration 660, loss = 1.86715
I0521 05:02:47.181556 25138 solver.cpp:253]     Train net output #0: loss = 1.86715 (* 1 = 1.86715 loss)
I0521 05:02:47.181574 25138 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 05:02:50.069417 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_669.caffemodel
I0521 05:02:50.364603 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_669.solverstate
I0521 05:02:55.192425 25138 solver.cpp:237] Iteration 682, loss = 1.83185
I0521 05:02:55.192469 25138 solver.cpp:253]     Train net output #0: loss = 1.83185 (* 1 = 1.83185 loss)
I0521 05:02:55.192488 25138 sgd_solver.cpp:106] Iteration 682, lr = 0.0025
I0521 05:03:03.134810 25138 solver.cpp:237] Iteration 704, loss = 1.85855
I0521 05:03:03.134965 25138 solver.cpp:253]     Train net output #0: loss = 1.85855 (* 1 = 1.85855 loss)
I0521 05:03:03.134979 25138 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 05:03:11.078496 25138 solver.cpp:237] Iteration 726, loss = 1.88988
I0521 05:03:11.078528 25138 solver.cpp:253]     Train net output #0: loss = 1.88988 (* 1 = 1.88988 loss)
I0521 05:03:11.078547 25138 sgd_solver.cpp:106] Iteration 726, lr = 0.0025
I0521 05:03:41.214543 25138 solver.cpp:237] Iteration 748, loss = 1.87671
I0521 05:03:41.214705 25138 solver.cpp:253]     Train net output #0: loss = 1.87671 (* 1 = 1.87671 loss)
I0521 05:03:41.214721 25138 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 05:03:49.155431 25138 solver.cpp:237] Iteration 770, loss = 1.80902
I0521 05:03:49.155462 25138 solver.cpp:253]     Train net output #0: loss = 1.80902 (* 1 = 1.80902 loss)
I0521 05:03:49.155478 25138 sgd_solver.cpp:106] Iteration 770, lr = 0.0025
I0521 05:03:57.100371 25138 solver.cpp:237] Iteration 792, loss = 1.82417
I0521 05:03:57.100405 25138 solver.cpp:253]     Train net output #0: loss = 1.82417 (* 1 = 1.82417 loss)
I0521 05:03:57.100421 25138 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 05:04:05.040405 25138 solver.cpp:237] Iteration 814, loss = 1.79916
I0521 05:04:05.040443 25138 solver.cpp:253]     Train net output #0: loss = 1.79916 (* 1 = 1.79916 loss)
I0521 05:04:05.040462 25138 sgd_solver.cpp:106] Iteration 814, lr = 0.0025
I0521 05:04:12.975523 25138 solver.cpp:237] Iteration 836, loss = 1.84177
I0521 05:04:12.975656 25138 solver.cpp:253]     Train net output #0: loss = 1.84177 (* 1 = 1.84177 loss)
I0521 05:04:12.975671 25138 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0521 05:04:20.915851 25138 solver.cpp:237] Iteration 858, loss = 1.80966
I0521 05:04:20.915884 25138 solver.cpp:253]     Train net output #0: loss = 1.80966 (* 1 = 1.80966 loss)
I0521 05:04:20.915897 25138 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0521 05:04:28.857647 25138 solver.cpp:237] Iteration 880, loss = 1.82399
I0521 05:04:28.857687 25138 solver.cpp:253]     Train net output #0: loss = 1.82399 (* 1 = 1.82399 loss)
I0521 05:04:28.857708 25138 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 05:04:32.828577 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_892.caffemodel
I0521 05:04:33.121402 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_892.solverstate
I0521 05:04:33.620820 25138 solver.cpp:341] Iteration 894, Testing net (#0)
I0521 05:05:39.786384 25138 solver.cpp:409]     Test net output #0: accuracy = 0.615862
I0521 05:05:39.786556 25138 solver.cpp:409]     Test net output #1: loss = 1.35395 (* 1 = 1.35395 loss)
I0521 05:06:04.943235 25138 solver.cpp:237] Iteration 902, loss = 1.8174
I0521 05:06:04.943286 25138 solver.cpp:253]     Train net output #0: loss = 1.8174 (* 1 = 1.8174 loss)
I0521 05:06:04.943301 25138 sgd_solver.cpp:106] Iteration 902, lr = 0.0025
I0521 05:06:12.882531 25138 solver.cpp:237] Iteration 924, loss = 1.82467
I0521 05:06:12.882688 25138 solver.cpp:253]     Train net output #0: loss = 1.82467 (* 1 = 1.82467 loss)
I0521 05:06:12.882701 25138 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 05:06:20.824349 25138 solver.cpp:237] Iteration 946, loss = 1.76796
I0521 05:06:20.824383 25138 solver.cpp:253]     Train net output #0: loss = 1.76796 (* 1 = 1.76796 loss)
I0521 05:06:20.824400 25138 sgd_solver.cpp:106] Iteration 946, lr = 0.0025
I0521 05:06:28.765162 25138 solver.cpp:237] Iteration 968, loss = 1.74799
I0521 05:06:28.765194 25138 solver.cpp:253]     Train net output #0: loss = 1.74799 (* 1 = 1.74799 loss)
I0521 05:06:28.765213 25138 sgd_solver.cpp:106] Iteration 968, lr = 0.0025
I0521 05:06:36.708304 25138 solver.cpp:237] Iteration 990, loss = 1.81181
I0521 05:06:36.708343 25138 solver.cpp:253]     Train net output #0: loss = 1.81181 (* 1 = 1.81181 loss)
I0521 05:06:36.708362 25138 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 05:06:44.652163 25138 solver.cpp:237] Iteration 1012, loss = 1.70361
I0521 05:06:44.652298 25138 solver.cpp:253]     Train net output #0: loss = 1.70361 (* 1 = 1.70361 loss)
I0521 05:06:44.652312 25138 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0521 05:06:52.589123 25138 solver.cpp:237] Iteration 1034, loss = 1.74171
I0521 05:06:52.589150 25138 solver.cpp:253]     Train net output #0: loss = 1.74171 (* 1 = 1.74171 loss)
I0521 05:06:52.589164 25138 sgd_solver.cpp:106] Iteration 1034, lr = 0.0025
I0521 05:07:22.724827 25138 solver.cpp:237] Iteration 1056, loss = 1.75403
I0521 05:07:22.724998 25138 solver.cpp:253]     Train net output #0: loss = 1.75403 (* 1 = 1.75403 loss)
I0521 05:07:22.725013 25138 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 05:07:30.668429 25138 solver.cpp:237] Iteration 1078, loss = 1.81117
I0521 05:07:30.668462 25138 solver.cpp:253]     Train net output #0: loss = 1.81117 (* 1 = 1.81117 loss)
I0521 05:07:30.668478 25138 sgd_solver.cpp:106] Iteration 1078, lr = 0.0025
I0521 05:07:38.607295 25138 solver.cpp:237] Iteration 1100, loss = 1.77219
I0521 05:07:38.607327 25138 solver.cpp:253]     Train net output #0: loss = 1.77219 (* 1 = 1.77219 loss)
I0521 05:07:38.607345 25138 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 05:07:43.661645 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1115.caffemodel
I0521 05:07:43.956650 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1115.solverstate
I0521 05:07:46.616705 25138 solver.cpp:237] Iteration 1122, loss = 1.75997
I0521 05:07:46.616753 25138 solver.cpp:253]     Train net output #0: loss = 1.75997 (* 1 = 1.75997 loss)
I0521 05:07:46.616773 25138 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 05:07:54.549885 25138 solver.cpp:237] Iteration 1144, loss = 1.82704
I0521 05:07:54.550026 25138 solver.cpp:253]     Train net output #0: loss = 1.82704 (* 1 = 1.82704 loss)
I0521 05:07:54.550040 25138 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0521 05:08:02.491741 25138 solver.cpp:237] Iteration 1166, loss = 1.77234
I0521 05:08:02.491773 25138 solver.cpp:253]     Train net output #0: loss = 1.77234 (* 1 = 1.77234 loss)
I0521 05:08:02.491788 25138 sgd_solver.cpp:106] Iteration 1166, lr = 0.0025
I0521 05:08:10.432534 25138 solver.cpp:237] Iteration 1188, loss = 1.80269
I0521 05:08:10.432567 25138 solver.cpp:253]     Train net output #0: loss = 1.80269 (* 1 = 1.80269 loss)
I0521 05:08:10.432585 25138 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 05:08:40.497122 25138 solver.cpp:237] Iteration 1210, loss = 1.75286
I0521 05:08:40.497292 25138 solver.cpp:253]     Train net output #0: loss = 1.75286 (* 1 = 1.75286 loss)
I0521 05:08:40.497309 25138 sgd_solver.cpp:106] Iteration 1210, lr = 0.0025
I0521 05:08:48.433076 25138 solver.cpp:237] Iteration 1232, loss = 1.70584
I0521 05:08:48.433109 25138 solver.cpp:253]     Train net output #0: loss = 1.70584 (* 1 = 1.70584 loss)
I0521 05:08:48.433126 25138 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 05:08:56.371750 25138 solver.cpp:237] Iteration 1254, loss = 1.75131
I0521 05:08:56.371784 25138 solver.cpp:253]     Train net output #0: loss = 1.75131 (* 1 = 1.75131 loss)
I0521 05:08:56.371800 25138 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0521 05:09:04.312302 25138 solver.cpp:237] Iteration 1276, loss = 1.74879
I0521 05:09:04.312342 25138 solver.cpp:253]     Train net output #0: loss = 1.74879 (* 1 = 1.74879 loss)
I0521 05:09:04.312362 25138 sgd_solver.cpp:106] Iteration 1276, lr = 0.0025
I0521 05:09:12.251993 25138 solver.cpp:237] Iteration 1298, loss = 1.72626
I0521 05:09:12.252131 25138 solver.cpp:253]     Train net output #0: loss = 1.72626 (* 1 = 1.72626 loss)
I0521 05:09:12.252145 25138 sgd_solver.cpp:106] Iteration 1298, lr = 0.0025
I0521 05:09:20.193781 25138 solver.cpp:237] Iteration 1320, loss = 1.69955
I0521 05:09:20.193814 25138 solver.cpp:253]     Train net output #0: loss = 1.69955 (* 1 = 1.69955 loss)
I0521 05:09:20.193832 25138 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 05:09:26.332456 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1338.caffemodel
I0521 05:09:26.624393 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1338.solverstate
I0521 05:09:27.480794 25138 solver.cpp:341] Iteration 1341, Testing net (#0)
I0521 05:10:12.370273 25138 solver.cpp:409]     Test net output #0: accuracy = 0.643163
I0521 05:10:12.370431 25138 solver.cpp:409]     Test net output #1: loss = 1.21364 (* 1 = 1.21364 loss)
I0521 05:10:12.837419 25138 solver.cpp:237] Iteration 1342, loss = 1.69042
I0521 05:10:12.837447 25138 solver.cpp:253]     Train net output #0: loss = 1.69042 (* 1 = 1.69042 loss)
I0521 05:10:12.837461 25138 sgd_solver.cpp:106] Iteration 1342, lr = 0.0025
I0521 05:10:42.928555 25138 solver.cpp:237] Iteration 1364, loss = 1.74786
I0521 05:10:42.928715 25138 solver.cpp:253]     Train net output #0: loss = 1.74786 (* 1 = 1.74786 loss)
I0521 05:10:42.928731 25138 sgd_solver.cpp:106] Iteration 1364, lr = 0.0025
I0521 05:10:50.866209 25138 solver.cpp:237] Iteration 1386, loss = 1.65249
I0521 05:10:50.866241 25138 solver.cpp:253]     Train net output #0: loss = 1.65249 (* 1 = 1.65249 loss)
I0521 05:10:50.866258 25138 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 05:10:58.813292 25138 solver.cpp:237] Iteration 1408, loss = 1.71509
I0521 05:10:58.813325 25138 solver.cpp:253]     Train net output #0: loss = 1.71509 (* 1 = 1.71509 loss)
I0521 05:10:58.813339 25138 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 05:11:06.753347 25138 solver.cpp:237] Iteration 1430, loss = 1.70789
I0521 05:11:06.753387 25138 solver.cpp:253]     Train net output #0: loss = 1.70789 (* 1 = 1.70789 loss)
I0521 05:11:06.753409 25138 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0521 05:11:14.702055 25138 solver.cpp:237] Iteration 1452, loss = 1.68778
I0521 05:11:14.702203 25138 solver.cpp:253]     Train net output #0: loss = 1.68778 (* 1 = 1.68778 loss)
I0521 05:11:14.702219 25138 sgd_solver.cpp:106] Iteration 1452, lr = 0.0025
I0521 05:11:22.645934 25138 solver.cpp:237] Iteration 1474, loss = 1.80722
I0521 05:11:22.645967 25138 solver.cpp:253]     Train net output #0: loss = 1.80722 (* 1 = 1.80722 loss)
I0521 05:11:22.645985 25138 sgd_solver.cpp:106] Iteration 1474, lr = 0.0025
I0521 05:11:52.796036 25138 solver.cpp:237] Iteration 1496, loss = 1.72568
I0521 05:11:52.796201 25138 solver.cpp:253]     Train net output #0: loss = 1.72568 (* 1 = 1.72568 loss)
I0521 05:11:52.796216 25138 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 05:12:00.739419 25138 solver.cpp:237] Iteration 1518, loss = 1.70931
I0521 05:12:00.739464 25138 solver.cpp:253]     Train net output #0: loss = 1.70931 (* 1 = 1.70931 loss)
I0521 05:12:00.739481 25138 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0521 05:12:08.681885 25138 solver.cpp:237] Iteration 1540, loss = 1.71107
I0521 05:12:08.681920 25138 solver.cpp:253]     Train net output #0: loss = 1.71107 (* 1 = 1.71107 loss)
I0521 05:12:08.681936 25138 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 05:12:15.905486 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1561.caffemodel
I0521 05:12:16.203783 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1561.solverstate
I0521 05:12:16.699981 25138 solver.cpp:237] Iteration 1562, loss = 1.74517
I0521 05:12:16.700027 25138 solver.cpp:253]     Train net output #0: loss = 1.74517 (* 1 = 1.74517 loss)
I0521 05:12:16.700042 25138 sgd_solver.cpp:106] Iteration 1562, lr = 0.0025
I0521 05:12:24.644582 25138 solver.cpp:237] Iteration 1584, loss = 1.76005
I0521 05:12:24.644742 25138 solver.cpp:253]     Train net output #0: loss = 1.76005 (* 1 = 1.76005 loss)
I0521 05:12:24.644757 25138 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 05:12:32.583642 25138 solver.cpp:237] Iteration 1606, loss = 1.65647
I0521 05:12:32.583673 25138 solver.cpp:253]     Train net output #0: loss = 1.65647 (* 1 = 1.65647 loss)
I0521 05:12:32.583688 25138 sgd_solver.cpp:106] Iteration 1606, lr = 0.0025
I0521 05:12:40.527851 25138 solver.cpp:237] Iteration 1628, loss = 1.72863
I0521 05:12:40.527884 25138 solver.cpp:253]     Train net output #0: loss = 1.72863 (* 1 = 1.72863 loss)
I0521 05:12:40.527900 25138 sgd_solver.cpp:106] Iteration 1628, lr = 0.0025
I0521 05:13:10.651849 25138 solver.cpp:237] Iteration 1650, loss = 1.66238
I0521 05:13:10.652019 25138 solver.cpp:253]     Train net output #0: loss = 1.66238 (* 1 = 1.66238 loss)
I0521 05:13:10.652035 25138 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 05:13:18.597265 25138 solver.cpp:237] Iteration 1672, loss = 1.8265
I0521 05:13:18.597306 25138 solver.cpp:253]     Train net output #0: loss = 1.8265 (* 1 = 1.8265 loss)
I0521 05:13:18.597323 25138 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0521 05:13:26.541003 25138 solver.cpp:237] Iteration 1694, loss = 1.69261
I0521 05:13:26.541036 25138 solver.cpp:253]     Train net output #0: loss = 1.69261 (* 1 = 1.69261 loss)
I0521 05:13:26.541052 25138 sgd_solver.cpp:106] Iteration 1694, lr = 0.0025
I0521 05:13:34.483366 25138 solver.cpp:237] Iteration 1716, loss = 1.71899
I0521 05:13:34.483399 25138 solver.cpp:253]     Train net output #0: loss = 1.71899 (* 1 = 1.71899 loss)
I0521 05:13:34.483415 25138 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0521 05:13:42.426460 25138 solver.cpp:237] Iteration 1738, loss = 1.66628
I0521 05:13:42.426614 25138 solver.cpp:253]     Train net output #0: loss = 1.66628 (* 1 = 1.66628 loss)
I0521 05:13:42.426627 25138 sgd_solver.cpp:106] Iteration 1738, lr = 0.0025
I0521 05:13:50.368676 25138 solver.cpp:237] Iteration 1760, loss = 1.65537
I0521 05:13:50.368707 25138 solver.cpp:253]     Train net output #0: loss = 1.65537 (* 1 = 1.65537 loss)
I0521 05:13:50.368726 25138 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0521 05:13:58.308341 25138 solver.cpp:237] Iteration 1782, loss = 1.6715
I0521 05:13:58.308373 25138 solver.cpp:253]     Train net output #0: loss = 1.6715 (* 1 = 1.6715 loss)
I0521 05:13:58.308390 25138 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 05:13:58.669317 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1784.caffemodel
I0521 05:13:58.960893 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_1784.solverstate
I0521 05:14:00.179327 25138 solver.cpp:341] Iteration 1788, Testing net (#0)
I0521 05:15:06.297593 25138 solver.cpp:409]     Test net output #0: accuracy = 0.6645
I0521 05:15:06.297768 25138 solver.cpp:409]     Test net output #1: loss = 1.15134 (* 1 = 1.15134 loss)
I0521 05:15:34.349828 25138 solver.cpp:237] Iteration 1804, loss = 1.67396
I0521 05:15:34.349879 25138 solver.cpp:253]     Train net output #0: loss = 1.67396 (* 1 = 1.67396 loss)
I0521 05:15:34.349895 25138 sgd_solver.cpp:106] Iteration 1804, lr = 0.0025
I0521 05:15:42.288266 25138 solver.cpp:237] Iteration 1826, loss = 1.71709
I0521 05:15:42.288430 25138 solver.cpp:253]     Train net output #0: loss = 1.71709 (* 1 = 1.71709 loss)
I0521 05:15:42.288445 25138 sgd_solver.cpp:106] Iteration 1826, lr = 0.0025
I0521 05:15:50.226470 25138 solver.cpp:237] Iteration 1848, loss = 1.73387
I0521 05:15:50.226503 25138 solver.cpp:253]     Train net output #0: loss = 1.73387 (* 1 = 1.73387 loss)
I0521 05:15:50.226518 25138 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 05:15:58.167002 25138 solver.cpp:237] Iteration 1870, loss = 1.72386
I0521 05:15:58.167035 25138 solver.cpp:253]     Train net output #0: loss = 1.72386 (* 1 = 1.72386 loss)
I0521 05:15:58.167052 25138 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0521 05:16:06.103049 25138 solver.cpp:237] Iteration 1892, loss = 1.67002
I0521 05:16:06.103081 25138 solver.cpp:253]     Train net output #0: loss = 1.67002 (* 1 = 1.67002 loss)
I0521 05:16:06.103097 25138 sgd_solver.cpp:106] Iteration 1892, lr = 0.0025
I0521 05:16:14.045658 25138 solver.cpp:237] Iteration 1914, loss = 1.63798
I0521 05:16:14.045814 25138 solver.cpp:253]     Train net output #0: loss = 1.63798 (* 1 = 1.63798 loss)
I0521 05:16:14.045828 25138 sgd_solver.cpp:106] Iteration 1914, lr = 0.0025
I0521 05:16:21.981266 25138 solver.cpp:237] Iteration 1936, loss = 1.6514
I0521 05:16:21.981298 25138 solver.cpp:253]     Train net output #0: loss = 1.6514 (* 1 = 1.6514 loss)
I0521 05:16:21.981317 25138 sgd_solver.cpp:106] Iteration 1936, lr = 0.0025
I0521 05:16:52.096271 25138 solver.cpp:237] Iteration 1958, loss = 1.59269
I0521 05:16:52.096442 25138 solver.cpp:253]     Train net output #0: loss = 1.59269 (* 1 = 1.59269 loss)
I0521 05:16:52.096458 25138 sgd_solver.cpp:106] Iteration 1958, lr = 0.0025
I0521 05:17:00.036296 25138 solver.cpp:237] Iteration 1980, loss = 1.70804
I0521 05:17:00.036339 25138 solver.cpp:253]     Train net output #0: loss = 1.70804 (* 1 = 1.70804 loss)
I0521 05:17:00.036353 25138 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 05:17:07.974033 25138 solver.cpp:237] Iteration 2002, loss = 1.70936
I0521 05:17:07.974067 25138 solver.cpp:253]     Train net output #0: loss = 1.70936 (* 1 = 1.70936 loss)
I0521 05:17:07.974081 25138 sgd_solver.cpp:106] Iteration 2002, lr = 0.0025
I0521 05:17:09.415956 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_2007.caffemodel
I0521 05:17:09.708586 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_2007.solverstate
I0521 05:17:15.975476 25138 solver.cpp:237] Iteration 2024, loss = 1.62907
I0521 05:17:15.975524 25138 solver.cpp:253]     Train net output #0: loss = 1.62907 (* 1 = 1.62907 loss)
I0521 05:17:15.975538 25138 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0521 05:17:23.919812 25138 solver.cpp:237] Iteration 2046, loss = 1.64277
I0521 05:17:23.919987 25138 solver.cpp:253]     Train net output #0: loss = 1.64277 (* 1 = 1.64277 loss)
I0521 05:17:23.920002 25138 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0521 05:17:31.855473 25138 solver.cpp:237] Iteration 2068, loss = 1.61526
I0521 05:17:31.855505 25138 solver.cpp:253]     Train net output #0: loss = 1.61526 (* 1 = 1.61526 loss)
I0521 05:17:31.855522 25138 sgd_solver.cpp:106] Iteration 2068, lr = 0.0025
I0521 05:18:02.014132 25138 solver.cpp:237] Iteration 2090, loss = 1.78847
I0521 05:18:02.014308 25138 solver.cpp:253]     Train net output #0: loss = 1.78847 (* 1 = 1.78847 loss)
I0521 05:18:02.014324 25138 sgd_solver.cpp:106] Iteration 2090, lr = 0.0025
I0521 05:18:09.951522 25138 solver.cpp:237] Iteration 2112, loss = 1.62647
I0521 05:18:09.951555 25138 solver.cpp:253]     Train net output #0: loss = 1.62647 (* 1 = 1.62647 loss)
I0521 05:18:09.951572 25138 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 05:18:17.888917 25138 solver.cpp:237] Iteration 2134, loss = 1.6914
I0521 05:18:17.888962 25138 solver.cpp:253]     Train net output #0: loss = 1.6914 (* 1 = 1.6914 loss)
I0521 05:18:17.888978 25138 sgd_solver.cpp:106] Iteration 2134, lr = 0.0025
I0521 05:18:25.830175 25138 solver.cpp:237] Iteration 2156, loss = 1.64231
I0521 05:18:25.830209 25138 solver.cpp:253]     Train net output #0: loss = 1.64231 (* 1 = 1.64231 loss)
I0521 05:18:25.830225 25138 sgd_solver.cpp:106] Iteration 2156, lr = 0.0025
I0521 05:18:33.768899 25138 solver.cpp:237] Iteration 2178, loss = 1.60399
I0521 05:18:33.769050 25138 solver.cpp:253]     Train net output #0: loss = 1.60399 (* 1 = 1.60399 loss)
I0521 05:18:33.769063 25138 sgd_solver.cpp:106] Iteration 2178, lr = 0.0025
I0521 05:18:41.711195 25138 solver.cpp:237] Iteration 2200, loss = 1.65713
I0521 05:18:41.711225 25138 solver.cpp:253]     Train net output #0: loss = 1.65713 (* 1 = 1.65713 loss)
I0521 05:18:41.711242 25138 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0521 05:18:49.653412 25138 solver.cpp:237] Iteration 2222, loss = 1.66618
I0521 05:18:49.653445 25138 solver.cpp:253]     Train net output #0: loss = 1.66618 (* 1 = 1.66618 loss)
I0521 05:18:49.653460 25138 sgd_solver.cpp:106] Iteration 2222, lr = 0.0025
I0521 05:18:52.180850 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_2230.caffemodel
I0521 05:18:52.472669 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_2230.solverstate
I0521 05:18:54.054366 25138 solver.cpp:341] Iteration 2235, Testing net (#0)
I0521 05:19:39.334534 25138 solver.cpp:409]     Test net output #0: accuracy = 0.682752
I0521 05:19:39.334698 25138 solver.cpp:409]     Test net output #1: loss = 1.12924 (* 1 = 1.12924 loss)
I0521 05:19:40.165345 25138 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_2238.caffemodel
I0521 05:19:40.467499 25138 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241_iter_2238.solverstate
I0521 05:19:40.496719 25138 solver.cpp:326] Optimization Done.
I0521 05:19:40.496745 25138 caffe.cpp:215] Optimization Done.
Application 11236828 resources: utime ~1251s, stime ~226s, Rss ~5329844, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_670_2016-05-20T11.20.56.982241.solver"
	User time (seconds): 0.56
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:41.31
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15077
	Voluntary context switches: 2708
	Involuntary context switches: 80
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
