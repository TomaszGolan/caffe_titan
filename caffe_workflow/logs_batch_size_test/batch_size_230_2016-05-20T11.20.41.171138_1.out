2805877
I0520 19:05:00.752854 30346 caffe.cpp:184] Using GPUs 0
I0520 19:05:01.180910 30346 solver.cpp:48] Initializing solver from parameters: 
test_iter: 652
test_interval: 1304
base_lr: 0.0025
display: 65
max_iter: 6521
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 652
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138.prototxt"
I0520 19:05:01.182466 30346 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138.prototxt
I0520 19:05:01.198523 30346 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 19:05:01.198582 30346 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 19:05:01.198926 30346 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 230
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 19:05:01.199105 30346 layer_factory.hpp:77] Creating layer data_hdf5
I0520 19:05:01.199128 30346 net.cpp:106] Creating Layer data_hdf5
I0520 19:05:01.199143 30346 net.cpp:411] data_hdf5 -> data
I0520 19:05:01.199177 30346 net.cpp:411] data_hdf5 -> label
I0520 19:05:01.199210 30346 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 19:05:01.200433 30346 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 19:05:01.202620 30346 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 19:05:22.710480 30346 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 19:05:22.715556 30346 net.cpp:150] Setting up data_hdf5
I0520 19:05:22.715597 30346 net.cpp:157] Top shape: 230 1 127 50 (1460500)
I0520 19:05:22.715612 30346 net.cpp:157] Top shape: 230 (230)
I0520 19:05:22.715622 30346 net.cpp:165] Memory required for data: 5842920
I0520 19:05:22.715636 30346 layer_factory.hpp:77] Creating layer conv1
I0520 19:05:22.715669 30346 net.cpp:106] Creating Layer conv1
I0520 19:05:22.715682 30346 net.cpp:454] conv1 <- data
I0520 19:05:22.715704 30346 net.cpp:411] conv1 -> conv1
I0520 19:05:23.817095 30346 net.cpp:150] Setting up conv1
I0520 19:05:23.817144 30346 net.cpp:157] Top shape: 230 12 120 48 (15897600)
I0520 19:05:23.817155 30346 net.cpp:165] Memory required for data: 69433320
I0520 19:05:23.817183 30346 layer_factory.hpp:77] Creating layer relu1
I0520 19:05:23.817204 30346 net.cpp:106] Creating Layer relu1
I0520 19:05:23.817215 30346 net.cpp:454] relu1 <- conv1
I0520 19:05:23.817229 30346 net.cpp:397] relu1 -> conv1 (in-place)
I0520 19:05:23.817746 30346 net.cpp:150] Setting up relu1
I0520 19:05:23.817762 30346 net.cpp:157] Top shape: 230 12 120 48 (15897600)
I0520 19:05:23.817772 30346 net.cpp:165] Memory required for data: 133023720
I0520 19:05:23.817782 30346 layer_factory.hpp:77] Creating layer pool1
I0520 19:05:23.817800 30346 net.cpp:106] Creating Layer pool1
I0520 19:05:23.817809 30346 net.cpp:454] pool1 <- conv1
I0520 19:05:23.817822 30346 net.cpp:411] pool1 -> pool1
I0520 19:05:23.817914 30346 net.cpp:150] Setting up pool1
I0520 19:05:23.817929 30346 net.cpp:157] Top shape: 230 12 60 48 (7948800)
I0520 19:05:23.817937 30346 net.cpp:165] Memory required for data: 164818920
I0520 19:05:23.817947 30346 layer_factory.hpp:77] Creating layer conv2
I0520 19:05:23.817970 30346 net.cpp:106] Creating Layer conv2
I0520 19:05:23.817981 30346 net.cpp:454] conv2 <- pool1
I0520 19:05:23.817993 30346 net.cpp:411] conv2 -> conv2
I0520 19:05:23.820677 30346 net.cpp:150] Setting up conv2
I0520 19:05:23.820703 30346 net.cpp:157] Top shape: 230 20 54 46 (11426400)
I0520 19:05:23.820714 30346 net.cpp:165] Memory required for data: 210524520
I0520 19:05:23.820734 30346 layer_factory.hpp:77] Creating layer relu2
I0520 19:05:23.820747 30346 net.cpp:106] Creating Layer relu2
I0520 19:05:23.820757 30346 net.cpp:454] relu2 <- conv2
I0520 19:05:23.820770 30346 net.cpp:397] relu2 -> conv2 (in-place)
I0520 19:05:23.821099 30346 net.cpp:150] Setting up relu2
I0520 19:05:23.821113 30346 net.cpp:157] Top shape: 230 20 54 46 (11426400)
I0520 19:05:23.821125 30346 net.cpp:165] Memory required for data: 256230120
I0520 19:05:23.821135 30346 layer_factory.hpp:77] Creating layer pool2
I0520 19:05:23.821147 30346 net.cpp:106] Creating Layer pool2
I0520 19:05:23.821157 30346 net.cpp:454] pool2 <- conv2
I0520 19:05:23.821182 30346 net.cpp:411] pool2 -> pool2
I0520 19:05:23.821251 30346 net.cpp:150] Setting up pool2
I0520 19:05:23.821264 30346 net.cpp:157] Top shape: 230 20 27 46 (5713200)
I0520 19:05:23.821274 30346 net.cpp:165] Memory required for data: 279082920
I0520 19:05:23.821285 30346 layer_factory.hpp:77] Creating layer conv3
I0520 19:05:23.821302 30346 net.cpp:106] Creating Layer conv3
I0520 19:05:23.821315 30346 net.cpp:454] conv3 <- pool2
I0520 19:05:23.821327 30346 net.cpp:411] conv3 -> conv3
I0520 19:05:23.823269 30346 net.cpp:150] Setting up conv3
I0520 19:05:23.823292 30346 net.cpp:157] Top shape: 230 28 22 44 (6233920)
I0520 19:05:23.823304 30346 net.cpp:165] Memory required for data: 304018600
I0520 19:05:23.823323 30346 layer_factory.hpp:77] Creating layer relu3
I0520 19:05:23.823339 30346 net.cpp:106] Creating Layer relu3
I0520 19:05:23.823349 30346 net.cpp:454] relu3 <- conv3
I0520 19:05:23.823361 30346 net.cpp:397] relu3 -> conv3 (in-place)
I0520 19:05:23.823829 30346 net.cpp:150] Setting up relu3
I0520 19:05:23.823846 30346 net.cpp:157] Top shape: 230 28 22 44 (6233920)
I0520 19:05:23.823858 30346 net.cpp:165] Memory required for data: 328954280
I0520 19:05:23.823868 30346 layer_factory.hpp:77] Creating layer pool3
I0520 19:05:23.823880 30346 net.cpp:106] Creating Layer pool3
I0520 19:05:23.823890 30346 net.cpp:454] pool3 <- conv3
I0520 19:05:23.823902 30346 net.cpp:411] pool3 -> pool3
I0520 19:05:23.823971 30346 net.cpp:150] Setting up pool3
I0520 19:05:23.823983 30346 net.cpp:157] Top shape: 230 28 11 44 (3116960)
I0520 19:05:23.823993 30346 net.cpp:165] Memory required for data: 341422120
I0520 19:05:23.824002 30346 layer_factory.hpp:77] Creating layer conv4
I0520 19:05:23.824020 30346 net.cpp:106] Creating Layer conv4
I0520 19:05:23.824031 30346 net.cpp:454] conv4 <- pool3
I0520 19:05:23.824044 30346 net.cpp:411] conv4 -> conv4
I0520 19:05:23.826761 30346 net.cpp:150] Setting up conv4
I0520 19:05:23.826789 30346 net.cpp:157] Top shape: 230 36 6 42 (2086560)
I0520 19:05:23.826800 30346 net.cpp:165] Memory required for data: 349768360
I0520 19:05:23.826815 30346 layer_factory.hpp:77] Creating layer relu4
I0520 19:05:23.826830 30346 net.cpp:106] Creating Layer relu4
I0520 19:05:23.826840 30346 net.cpp:454] relu4 <- conv4
I0520 19:05:23.826853 30346 net.cpp:397] relu4 -> conv4 (in-place)
I0520 19:05:23.827316 30346 net.cpp:150] Setting up relu4
I0520 19:05:23.827332 30346 net.cpp:157] Top shape: 230 36 6 42 (2086560)
I0520 19:05:23.827342 30346 net.cpp:165] Memory required for data: 358114600
I0520 19:05:23.827353 30346 layer_factory.hpp:77] Creating layer pool4
I0520 19:05:23.827363 30346 net.cpp:106] Creating Layer pool4
I0520 19:05:23.827373 30346 net.cpp:454] pool4 <- conv4
I0520 19:05:23.827386 30346 net.cpp:411] pool4 -> pool4
I0520 19:05:23.827455 30346 net.cpp:150] Setting up pool4
I0520 19:05:23.827468 30346 net.cpp:157] Top shape: 230 36 3 42 (1043280)
I0520 19:05:23.827478 30346 net.cpp:165] Memory required for data: 362287720
I0520 19:05:23.827488 30346 layer_factory.hpp:77] Creating layer ip1
I0520 19:05:23.827508 30346 net.cpp:106] Creating Layer ip1
I0520 19:05:23.827519 30346 net.cpp:454] ip1 <- pool4
I0520 19:05:23.827533 30346 net.cpp:411] ip1 -> ip1
I0520 19:05:23.842962 30346 net.cpp:150] Setting up ip1
I0520 19:05:23.842991 30346 net.cpp:157] Top shape: 230 196 (45080)
I0520 19:05:23.843003 30346 net.cpp:165] Memory required for data: 362468040
I0520 19:05:23.843025 30346 layer_factory.hpp:77] Creating layer relu5
I0520 19:05:23.843040 30346 net.cpp:106] Creating Layer relu5
I0520 19:05:23.843051 30346 net.cpp:454] relu5 <- ip1
I0520 19:05:23.843065 30346 net.cpp:397] relu5 -> ip1 (in-place)
I0520 19:05:23.843407 30346 net.cpp:150] Setting up relu5
I0520 19:05:23.843421 30346 net.cpp:157] Top shape: 230 196 (45080)
I0520 19:05:23.843432 30346 net.cpp:165] Memory required for data: 362648360
I0520 19:05:23.843442 30346 layer_factory.hpp:77] Creating layer drop1
I0520 19:05:23.843463 30346 net.cpp:106] Creating Layer drop1
I0520 19:05:23.843474 30346 net.cpp:454] drop1 <- ip1
I0520 19:05:23.843500 30346 net.cpp:397] drop1 -> ip1 (in-place)
I0520 19:05:23.843549 30346 net.cpp:150] Setting up drop1
I0520 19:05:23.843562 30346 net.cpp:157] Top shape: 230 196 (45080)
I0520 19:05:23.843572 30346 net.cpp:165] Memory required for data: 362828680
I0520 19:05:23.843582 30346 layer_factory.hpp:77] Creating layer ip2
I0520 19:05:23.843601 30346 net.cpp:106] Creating Layer ip2
I0520 19:05:23.843611 30346 net.cpp:454] ip2 <- ip1
I0520 19:05:23.843624 30346 net.cpp:411] ip2 -> ip2
I0520 19:05:23.844089 30346 net.cpp:150] Setting up ip2
I0520 19:05:23.844102 30346 net.cpp:157] Top shape: 230 98 (22540)
I0520 19:05:23.844112 30346 net.cpp:165] Memory required for data: 362918840
I0520 19:05:23.844127 30346 layer_factory.hpp:77] Creating layer relu6
I0520 19:05:23.844140 30346 net.cpp:106] Creating Layer relu6
I0520 19:05:23.844149 30346 net.cpp:454] relu6 <- ip2
I0520 19:05:23.844161 30346 net.cpp:397] relu6 -> ip2 (in-place)
I0520 19:05:23.844677 30346 net.cpp:150] Setting up relu6
I0520 19:05:23.844693 30346 net.cpp:157] Top shape: 230 98 (22540)
I0520 19:05:23.844704 30346 net.cpp:165] Memory required for data: 363009000
I0520 19:05:23.844714 30346 layer_factory.hpp:77] Creating layer drop2
I0520 19:05:23.844727 30346 net.cpp:106] Creating Layer drop2
I0520 19:05:23.844738 30346 net.cpp:454] drop2 <- ip2
I0520 19:05:23.844749 30346 net.cpp:397] drop2 -> ip2 (in-place)
I0520 19:05:23.844791 30346 net.cpp:150] Setting up drop2
I0520 19:05:23.844805 30346 net.cpp:157] Top shape: 230 98 (22540)
I0520 19:05:23.844815 30346 net.cpp:165] Memory required for data: 363099160
I0520 19:05:23.844825 30346 layer_factory.hpp:77] Creating layer ip3
I0520 19:05:23.844838 30346 net.cpp:106] Creating Layer ip3
I0520 19:05:23.844848 30346 net.cpp:454] ip3 <- ip2
I0520 19:05:23.844861 30346 net.cpp:411] ip3 -> ip3
I0520 19:05:23.845072 30346 net.cpp:150] Setting up ip3
I0520 19:05:23.845084 30346 net.cpp:157] Top shape: 230 11 (2530)
I0520 19:05:23.845094 30346 net.cpp:165] Memory required for data: 363109280
I0520 19:05:23.845109 30346 layer_factory.hpp:77] Creating layer drop3
I0520 19:05:23.845121 30346 net.cpp:106] Creating Layer drop3
I0520 19:05:23.845131 30346 net.cpp:454] drop3 <- ip3
I0520 19:05:23.845144 30346 net.cpp:397] drop3 -> ip3 (in-place)
I0520 19:05:23.845182 30346 net.cpp:150] Setting up drop3
I0520 19:05:23.845194 30346 net.cpp:157] Top shape: 230 11 (2530)
I0520 19:05:23.845206 30346 net.cpp:165] Memory required for data: 363119400
I0520 19:05:23.845214 30346 layer_factory.hpp:77] Creating layer loss
I0520 19:05:23.845233 30346 net.cpp:106] Creating Layer loss
I0520 19:05:23.845243 30346 net.cpp:454] loss <- ip3
I0520 19:05:23.845254 30346 net.cpp:454] loss <- label
I0520 19:05:23.845268 30346 net.cpp:411] loss -> loss
I0520 19:05:23.845284 30346 layer_factory.hpp:77] Creating layer loss
I0520 19:05:23.845928 30346 net.cpp:150] Setting up loss
I0520 19:05:23.845949 30346 net.cpp:157] Top shape: (1)
I0520 19:05:23.845963 30346 net.cpp:160]     with loss weight 1
I0520 19:05:23.846009 30346 net.cpp:165] Memory required for data: 363119404
I0520 19:05:23.846020 30346 net.cpp:226] loss needs backward computation.
I0520 19:05:23.846031 30346 net.cpp:226] drop3 needs backward computation.
I0520 19:05:23.846041 30346 net.cpp:226] ip3 needs backward computation.
I0520 19:05:23.846052 30346 net.cpp:226] drop2 needs backward computation.
I0520 19:05:23.846061 30346 net.cpp:226] relu6 needs backward computation.
I0520 19:05:23.846071 30346 net.cpp:226] ip2 needs backward computation.
I0520 19:05:23.846081 30346 net.cpp:226] drop1 needs backward computation.
I0520 19:05:23.846091 30346 net.cpp:226] relu5 needs backward computation.
I0520 19:05:23.846101 30346 net.cpp:226] ip1 needs backward computation.
I0520 19:05:23.846112 30346 net.cpp:226] pool4 needs backward computation.
I0520 19:05:23.846122 30346 net.cpp:226] relu4 needs backward computation.
I0520 19:05:23.846132 30346 net.cpp:226] conv4 needs backward computation.
I0520 19:05:23.846141 30346 net.cpp:226] pool3 needs backward computation.
I0520 19:05:23.846161 30346 net.cpp:226] relu3 needs backward computation.
I0520 19:05:23.846171 30346 net.cpp:226] conv3 needs backward computation.
I0520 19:05:23.846182 30346 net.cpp:226] pool2 needs backward computation.
I0520 19:05:23.846194 30346 net.cpp:226] relu2 needs backward computation.
I0520 19:05:23.846204 30346 net.cpp:226] conv2 needs backward computation.
I0520 19:05:23.846213 30346 net.cpp:226] pool1 needs backward computation.
I0520 19:05:23.846225 30346 net.cpp:226] relu1 needs backward computation.
I0520 19:05:23.846233 30346 net.cpp:226] conv1 needs backward computation.
I0520 19:05:23.846245 30346 net.cpp:228] data_hdf5 does not need backward computation.
I0520 19:05:23.846254 30346 net.cpp:270] This network produces output loss
I0520 19:05:23.846278 30346 net.cpp:283] Network initialization done.
I0520 19:05:23.847878 30346 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138.prototxt
I0520 19:05:23.847949 30346 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 19:05:23.848304 30346 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 230
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 19:05:23.848491 30346 layer_factory.hpp:77] Creating layer data_hdf5
I0520 19:05:23.848507 30346 net.cpp:106] Creating Layer data_hdf5
I0520 19:05:23.848520 30346 net.cpp:411] data_hdf5 -> data
I0520 19:05:23.848536 30346 net.cpp:411] data_hdf5 -> label
I0520 19:05:23.848552 30346 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 19:05:23.849748 30346 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 19:05:45.150058 30346 net.cpp:150] Setting up data_hdf5
I0520 19:05:45.150225 30346 net.cpp:157] Top shape: 230 1 127 50 (1460500)
I0520 19:05:45.150240 30346 net.cpp:157] Top shape: 230 (230)
I0520 19:05:45.150252 30346 net.cpp:165] Memory required for data: 5842920
I0520 19:05:45.150265 30346 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 19:05:45.150293 30346 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 19:05:45.150305 30346 net.cpp:454] label_data_hdf5_1_split <- label
I0520 19:05:45.150319 30346 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 19:05:45.150341 30346 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 19:05:45.150413 30346 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 19:05:45.150427 30346 net.cpp:157] Top shape: 230 (230)
I0520 19:05:45.150439 30346 net.cpp:157] Top shape: 230 (230)
I0520 19:05:45.150447 30346 net.cpp:165] Memory required for data: 5844760
I0520 19:05:45.150457 30346 layer_factory.hpp:77] Creating layer conv1
I0520 19:05:45.150480 30346 net.cpp:106] Creating Layer conv1
I0520 19:05:45.150490 30346 net.cpp:454] conv1 <- data
I0520 19:05:45.150504 30346 net.cpp:411] conv1 -> conv1
I0520 19:05:45.152443 30346 net.cpp:150] Setting up conv1
I0520 19:05:45.152467 30346 net.cpp:157] Top shape: 230 12 120 48 (15897600)
I0520 19:05:45.152478 30346 net.cpp:165] Memory required for data: 69435160
I0520 19:05:45.152499 30346 layer_factory.hpp:77] Creating layer relu1
I0520 19:05:45.152514 30346 net.cpp:106] Creating Layer relu1
I0520 19:05:45.152524 30346 net.cpp:454] relu1 <- conv1
I0520 19:05:45.152537 30346 net.cpp:397] relu1 -> conv1 (in-place)
I0520 19:05:45.153033 30346 net.cpp:150] Setting up relu1
I0520 19:05:45.153049 30346 net.cpp:157] Top shape: 230 12 120 48 (15897600)
I0520 19:05:45.153060 30346 net.cpp:165] Memory required for data: 133025560
I0520 19:05:45.153070 30346 layer_factory.hpp:77] Creating layer pool1
I0520 19:05:45.153086 30346 net.cpp:106] Creating Layer pool1
I0520 19:05:45.153096 30346 net.cpp:454] pool1 <- conv1
I0520 19:05:45.153108 30346 net.cpp:411] pool1 -> pool1
I0520 19:05:45.153183 30346 net.cpp:150] Setting up pool1
I0520 19:05:45.153197 30346 net.cpp:157] Top shape: 230 12 60 48 (7948800)
I0520 19:05:45.153206 30346 net.cpp:165] Memory required for data: 164820760
I0520 19:05:45.153216 30346 layer_factory.hpp:77] Creating layer conv2
I0520 19:05:45.153234 30346 net.cpp:106] Creating Layer conv2
I0520 19:05:45.153245 30346 net.cpp:454] conv2 <- pool1
I0520 19:05:45.153259 30346 net.cpp:411] conv2 -> conv2
I0520 19:05:45.155195 30346 net.cpp:150] Setting up conv2
I0520 19:05:45.155217 30346 net.cpp:157] Top shape: 230 20 54 46 (11426400)
I0520 19:05:45.155230 30346 net.cpp:165] Memory required for data: 210526360
I0520 19:05:45.155247 30346 layer_factory.hpp:77] Creating layer relu2
I0520 19:05:45.155261 30346 net.cpp:106] Creating Layer relu2
I0520 19:05:45.155270 30346 net.cpp:454] relu2 <- conv2
I0520 19:05:45.155283 30346 net.cpp:397] relu2 -> conv2 (in-place)
I0520 19:05:45.155616 30346 net.cpp:150] Setting up relu2
I0520 19:05:45.155630 30346 net.cpp:157] Top shape: 230 20 54 46 (11426400)
I0520 19:05:45.155640 30346 net.cpp:165] Memory required for data: 256231960
I0520 19:05:45.155650 30346 layer_factory.hpp:77] Creating layer pool2
I0520 19:05:45.155663 30346 net.cpp:106] Creating Layer pool2
I0520 19:05:45.155673 30346 net.cpp:454] pool2 <- conv2
I0520 19:05:45.155685 30346 net.cpp:411] pool2 -> pool2
I0520 19:05:45.155757 30346 net.cpp:150] Setting up pool2
I0520 19:05:45.155771 30346 net.cpp:157] Top shape: 230 20 27 46 (5713200)
I0520 19:05:45.155781 30346 net.cpp:165] Memory required for data: 279084760
I0520 19:05:45.155791 30346 layer_factory.hpp:77] Creating layer conv3
I0520 19:05:45.155807 30346 net.cpp:106] Creating Layer conv3
I0520 19:05:45.155818 30346 net.cpp:454] conv3 <- pool2
I0520 19:05:45.155832 30346 net.cpp:411] conv3 -> conv3
I0520 19:05:45.157799 30346 net.cpp:150] Setting up conv3
I0520 19:05:45.157822 30346 net.cpp:157] Top shape: 230 28 22 44 (6233920)
I0520 19:05:45.157833 30346 net.cpp:165] Memory required for data: 304020440
I0520 19:05:45.157873 30346 layer_factory.hpp:77] Creating layer relu3
I0520 19:05:45.157887 30346 net.cpp:106] Creating Layer relu3
I0520 19:05:45.157897 30346 net.cpp:454] relu3 <- conv3
I0520 19:05:45.157908 30346 net.cpp:397] relu3 -> conv3 (in-place)
I0520 19:05:45.158382 30346 net.cpp:150] Setting up relu3
I0520 19:05:45.158398 30346 net.cpp:157] Top shape: 230 28 22 44 (6233920)
I0520 19:05:45.158409 30346 net.cpp:165] Memory required for data: 328956120
I0520 19:05:45.158419 30346 layer_factory.hpp:77] Creating layer pool3
I0520 19:05:45.158432 30346 net.cpp:106] Creating Layer pool3
I0520 19:05:45.158442 30346 net.cpp:454] pool3 <- conv3
I0520 19:05:45.158455 30346 net.cpp:411] pool3 -> pool3
I0520 19:05:45.158526 30346 net.cpp:150] Setting up pool3
I0520 19:05:45.158540 30346 net.cpp:157] Top shape: 230 28 11 44 (3116960)
I0520 19:05:45.158550 30346 net.cpp:165] Memory required for data: 341423960
I0520 19:05:45.158560 30346 layer_factory.hpp:77] Creating layer conv4
I0520 19:05:45.158577 30346 net.cpp:106] Creating Layer conv4
I0520 19:05:45.158587 30346 net.cpp:454] conv4 <- pool3
I0520 19:05:45.158601 30346 net.cpp:411] conv4 -> conv4
I0520 19:05:45.160650 30346 net.cpp:150] Setting up conv4
I0520 19:05:45.160673 30346 net.cpp:157] Top shape: 230 36 6 42 (2086560)
I0520 19:05:45.160686 30346 net.cpp:165] Memory required for data: 349770200
I0520 19:05:45.160701 30346 layer_factory.hpp:77] Creating layer relu4
I0520 19:05:45.160714 30346 net.cpp:106] Creating Layer relu4
I0520 19:05:45.160724 30346 net.cpp:454] relu4 <- conv4
I0520 19:05:45.160737 30346 net.cpp:397] relu4 -> conv4 (in-place)
I0520 19:05:45.161203 30346 net.cpp:150] Setting up relu4
I0520 19:05:45.161219 30346 net.cpp:157] Top shape: 230 36 6 42 (2086560)
I0520 19:05:45.161229 30346 net.cpp:165] Memory required for data: 358116440
I0520 19:05:45.161239 30346 layer_factory.hpp:77] Creating layer pool4
I0520 19:05:45.161252 30346 net.cpp:106] Creating Layer pool4
I0520 19:05:45.161262 30346 net.cpp:454] pool4 <- conv4
I0520 19:05:45.161275 30346 net.cpp:411] pool4 -> pool4
I0520 19:05:45.161347 30346 net.cpp:150] Setting up pool4
I0520 19:05:45.161361 30346 net.cpp:157] Top shape: 230 36 3 42 (1043280)
I0520 19:05:45.161370 30346 net.cpp:165] Memory required for data: 362289560
I0520 19:05:45.161381 30346 layer_factory.hpp:77] Creating layer ip1
I0520 19:05:45.161396 30346 net.cpp:106] Creating Layer ip1
I0520 19:05:45.161406 30346 net.cpp:454] ip1 <- pool4
I0520 19:05:45.161418 30346 net.cpp:411] ip1 -> ip1
I0520 19:05:45.176908 30346 net.cpp:150] Setting up ip1
I0520 19:05:45.176936 30346 net.cpp:157] Top shape: 230 196 (45080)
I0520 19:05:45.176949 30346 net.cpp:165] Memory required for data: 362469880
I0520 19:05:45.176971 30346 layer_factory.hpp:77] Creating layer relu5
I0520 19:05:45.176987 30346 net.cpp:106] Creating Layer relu5
I0520 19:05:45.176998 30346 net.cpp:454] relu5 <- ip1
I0520 19:05:45.177011 30346 net.cpp:397] relu5 -> ip1 (in-place)
I0520 19:05:45.177356 30346 net.cpp:150] Setting up relu5
I0520 19:05:45.177371 30346 net.cpp:157] Top shape: 230 196 (45080)
I0520 19:05:45.177381 30346 net.cpp:165] Memory required for data: 362650200
I0520 19:05:45.177392 30346 layer_factory.hpp:77] Creating layer drop1
I0520 19:05:45.177410 30346 net.cpp:106] Creating Layer drop1
I0520 19:05:45.177420 30346 net.cpp:454] drop1 <- ip1
I0520 19:05:45.177433 30346 net.cpp:397] drop1 -> ip1 (in-place)
I0520 19:05:45.177480 30346 net.cpp:150] Setting up drop1
I0520 19:05:45.177494 30346 net.cpp:157] Top shape: 230 196 (45080)
I0520 19:05:45.177503 30346 net.cpp:165] Memory required for data: 362830520
I0520 19:05:45.177513 30346 layer_factory.hpp:77] Creating layer ip2
I0520 19:05:45.177527 30346 net.cpp:106] Creating Layer ip2
I0520 19:05:45.177537 30346 net.cpp:454] ip2 <- ip1
I0520 19:05:45.177551 30346 net.cpp:411] ip2 -> ip2
I0520 19:05:45.178037 30346 net.cpp:150] Setting up ip2
I0520 19:05:45.178050 30346 net.cpp:157] Top shape: 230 98 (22540)
I0520 19:05:45.178061 30346 net.cpp:165] Memory required for data: 362920680
I0520 19:05:45.178088 30346 layer_factory.hpp:77] Creating layer relu6
I0520 19:05:45.178102 30346 net.cpp:106] Creating Layer relu6
I0520 19:05:45.178112 30346 net.cpp:454] relu6 <- ip2
I0520 19:05:45.178123 30346 net.cpp:397] relu6 -> ip2 (in-place)
I0520 19:05:45.178660 30346 net.cpp:150] Setting up relu6
I0520 19:05:45.178683 30346 net.cpp:157] Top shape: 230 98 (22540)
I0520 19:05:45.178691 30346 net.cpp:165] Memory required for data: 363010840
I0520 19:05:45.178701 30346 layer_factory.hpp:77] Creating layer drop2
I0520 19:05:45.178716 30346 net.cpp:106] Creating Layer drop2
I0520 19:05:45.178726 30346 net.cpp:454] drop2 <- ip2
I0520 19:05:45.178740 30346 net.cpp:397] drop2 -> ip2 (in-place)
I0520 19:05:45.178783 30346 net.cpp:150] Setting up drop2
I0520 19:05:45.178797 30346 net.cpp:157] Top shape: 230 98 (22540)
I0520 19:05:45.178807 30346 net.cpp:165] Memory required for data: 363101000
I0520 19:05:45.178817 30346 layer_factory.hpp:77] Creating layer ip3
I0520 19:05:45.178831 30346 net.cpp:106] Creating Layer ip3
I0520 19:05:45.178841 30346 net.cpp:454] ip3 <- ip2
I0520 19:05:45.178855 30346 net.cpp:411] ip3 -> ip3
I0520 19:05:45.179077 30346 net.cpp:150] Setting up ip3
I0520 19:05:45.179090 30346 net.cpp:157] Top shape: 230 11 (2530)
I0520 19:05:45.179100 30346 net.cpp:165] Memory required for data: 363111120
I0520 19:05:45.179116 30346 layer_factory.hpp:77] Creating layer drop3
I0520 19:05:45.179128 30346 net.cpp:106] Creating Layer drop3
I0520 19:05:45.179138 30346 net.cpp:454] drop3 <- ip3
I0520 19:05:45.179152 30346 net.cpp:397] drop3 -> ip3 (in-place)
I0520 19:05:45.179193 30346 net.cpp:150] Setting up drop3
I0520 19:05:45.179205 30346 net.cpp:157] Top shape: 230 11 (2530)
I0520 19:05:45.179215 30346 net.cpp:165] Memory required for data: 363121240
I0520 19:05:45.179224 30346 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 19:05:45.179237 30346 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 19:05:45.179247 30346 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 19:05:45.179260 30346 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 19:05:45.179275 30346 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 19:05:45.179349 30346 net.cpp:150] Setting up ip3_drop3_0_split
I0520 19:05:45.179363 30346 net.cpp:157] Top shape: 230 11 (2530)
I0520 19:05:45.179375 30346 net.cpp:157] Top shape: 230 11 (2530)
I0520 19:05:45.179385 30346 net.cpp:165] Memory required for data: 363141480
I0520 19:05:45.179396 30346 layer_factory.hpp:77] Creating layer accuracy
I0520 19:05:45.179419 30346 net.cpp:106] Creating Layer accuracy
I0520 19:05:45.179428 30346 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 19:05:45.179440 30346 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 19:05:45.179453 30346 net.cpp:411] accuracy -> accuracy
I0520 19:05:45.179476 30346 net.cpp:150] Setting up accuracy
I0520 19:05:45.179489 30346 net.cpp:157] Top shape: (1)
I0520 19:05:45.179499 30346 net.cpp:165] Memory required for data: 363141484
I0520 19:05:45.179509 30346 layer_factory.hpp:77] Creating layer loss
I0520 19:05:45.179522 30346 net.cpp:106] Creating Layer loss
I0520 19:05:45.179533 30346 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 19:05:45.179543 30346 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 19:05:45.179556 30346 net.cpp:411] loss -> loss
I0520 19:05:45.179574 30346 layer_factory.hpp:77] Creating layer loss
I0520 19:05:45.180058 30346 net.cpp:150] Setting up loss
I0520 19:05:45.180071 30346 net.cpp:157] Top shape: (1)
I0520 19:05:45.180081 30346 net.cpp:160]     with loss weight 1
I0520 19:05:45.180102 30346 net.cpp:165] Memory required for data: 363141488
I0520 19:05:45.180112 30346 net.cpp:226] loss needs backward computation.
I0520 19:05:45.180124 30346 net.cpp:228] accuracy does not need backward computation.
I0520 19:05:45.180135 30346 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 19:05:45.180145 30346 net.cpp:226] drop3 needs backward computation.
I0520 19:05:45.180155 30346 net.cpp:226] ip3 needs backward computation.
I0520 19:05:45.180166 30346 net.cpp:226] drop2 needs backward computation.
I0520 19:05:45.180184 30346 net.cpp:226] relu6 needs backward computation.
I0520 19:05:45.180194 30346 net.cpp:226] ip2 needs backward computation.
I0520 19:05:45.180204 30346 net.cpp:226] drop1 needs backward computation.
I0520 19:05:45.180213 30346 net.cpp:226] relu5 needs backward computation.
I0520 19:05:45.180223 30346 net.cpp:226] ip1 needs backward computation.
I0520 19:05:45.180234 30346 net.cpp:226] pool4 needs backward computation.
I0520 19:05:45.180244 30346 net.cpp:226] relu4 needs backward computation.
I0520 19:05:45.180254 30346 net.cpp:226] conv4 needs backward computation.
I0520 19:05:45.180265 30346 net.cpp:226] pool3 needs backward computation.
I0520 19:05:45.180276 30346 net.cpp:226] relu3 needs backward computation.
I0520 19:05:45.180286 30346 net.cpp:226] conv3 needs backward computation.
I0520 19:05:45.180297 30346 net.cpp:226] pool2 needs backward computation.
I0520 19:05:45.180307 30346 net.cpp:226] relu2 needs backward computation.
I0520 19:05:45.180317 30346 net.cpp:226] conv2 needs backward computation.
I0520 19:05:45.180327 30346 net.cpp:226] pool1 needs backward computation.
I0520 19:05:45.180337 30346 net.cpp:226] relu1 needs backward computation.
I0520 19:05:45.180347 30346 net.cpp:226] conv1 needs backward computation.
I0520 19:05:45.180359 30346 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 19:05:45.180371 30346 net.cpp:228] data_hdf5 does not need backward computation.
I0520 19:05:45.180382 30346 net.cpp:270] This network produces output accuracy
I0520 19:05:45.180392 30346 net.cpp:270] This network produces output loss
I0520 19:05:45.180420 30346 net.cpp:283] Network initialization done.
I0520 19:05:45.180553 30346 solver.cpp:60] Solver scaffolding done.
I0520 19:05:45.181689 30346 caffe.cpp:212] Starting Optimization
I0520 19:05:45.181707 30346 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 19:05:45.181720 30346 solver.cpp:289] Learning Rate Policy: fixed
I0520 19:05:45.182950 30346 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 19:06:31.801234 30346 solver.cpp:409]     Test net output #0: accuracy = 0.111463
I0520 19:06:31.801399 30346 solver.cpp:409]     Test net output #1: loss = 2.39658 (* 1 = 2.39658 loss)
I0520 19:06:31.855834 30346 solver.cpp:237] Iteration 0, loss = 2.39623
I0520 19:06:31.855872 30346 solver.cpp:253]     Train net output #0: loss = 2.39623 (* 1 = 2.39623 loss)
I0520 19:06:31.855893 30346 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 19:06:40.123344 30346 solver.cpp:237] Iteration 65, loss = 2.33726
I0520 19:06:40.123380 30346 solver.cpp:253]     Train net output #0: loss = 2.33726 (* 1 = 2.33726 loss)
I0520 19:06:40.123396 30346 sgd_solver.cpp:106] Iteration 65, lr = 0.0025
I0520 19:06:48.381304 30346 solver.cpp:237] Iteration 130, loss = 2.32304
I0520 19:06:48.381338 30346 solver.cpp:253]     Train net output #0: loss = 2.32304 (* 1 = 2.32304 loss)
I0520 19:06:48.381355 30346 sgd_solver.cpp:106] Iteration 130, lr = 0.0025
I0520 19:06:56.643044 30346 solver.cpp:237] Iteration 195, loss = 2.33563
I0520 19:06:56.643082 30346 solver.cpp:253]     Train net output #0: loss = 2.33563 (* 1 = 2.33563 loss)
I0520 19:06:56.643105 30346 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0520 19:07:04.903050 30346 solver.cpp:237] Iteration 260, loss = 2.24749
I0520 19:07:04.903197 30346 solver.cpp:253]     Train net output #0: loss = 2.24749 (* 1 = 2.24749 loss)
I0520 19:07:04.903210 30346 sgd_solver.cpp:106] Iteration 260, lr = 0.0025
I0520 19:07:13.163161 30346 solver.cpp:237] Iteration 325, loss = 2.2132
I0520 19:07:13.163194 30346 solver.cpp:253]     Train net output #0: loss = 2.2132 (* 1 = 2.2132 loss)
I0520 19:07:13.163211 30346 sgd_solver.cpp:106] Iteration 325, lr = 0.0025
I0520 19:07:21.429471 30346 solver.cpp:237] Iteration 390, loss = 2.16307
I0520 19:07:21.429518 30346 solver.cpp:253]     Train net output #0: loss = 2.16307 (* 1 = 2.16307 loss)
I0520 19:07:21.429538 30346 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0520 19:07:51.828635 30346 solver.cpp:237] Iteration 455, loss = 2.12233
I0520 19:07:51.828800 30346 solver.cpp:253]     Train net output #0: loss = 2.12233 (* 1 = 2.12233 loss)
I0520 19:07:51.828815 30346 sgd_solver.cpp:106] Iteration 455, lr = 0.0025
I0520 19:08:00.092258 30346 solver.cpp:237] Iteration 520, loss = 2.08136
I0520 19:08:00.092293 30346 solver.cpp:253]     Train net output #0: loss = 2.08136 (* 1 = 2.08136 loss)
I0520 19:08:00.092309 30346 sgd_solver.cpp:106] Iteration 520, lr = 0.0025
I0520 19:08:08.364454 30346 solver.cpp:237] Iteration 585, loss = 2.03739
I0520 19:08:08.364496 30346 solver.cpp:253]     Train net output #0: loss = 2.03739 (* 1 = 2.03739 loss)
I0520 19:08:08.364517 30346 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0520 19:08:16.637260 30346 solver.cpp:237] Iteration 650, loss = 1.96393
I0520 19:08:16.637295 30346 solver.cpp:253]     Train net output #0: loss = 1.96393 (* 1 = 1.96393 loss)
I0520 19:08:16.637312 30346 sgd_solver.cpp:106] Iteration 650, lr = 0.0025
I0520 19:08:16.765843 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_652.caffemodel
I0520 19:08:16.896165 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_652.solverstate
I0520 19:08:24.971429 30346 solver.cpp:237] Iteration 715, loss = 1.89974
I0520 19:08:24.971587 30346 solver.cpp:253]     Train net output #0: loss = 1.89974 (* 1 = 1.89974 loss)
I0520 19:08:24.971601 30346 sgd_solver.cpp:106] Iteration 715, lr = 0.0025
I0520 19:08:33.241387 30346 solver.cpp:237] Iteration 780, loss = 1.84814
I0520 19:08:33.241436 30346 solver.cpp:253]     Train net output #0: loss = 1.84814 (* 1 = 1.84814 loss)
I0520 19:08:33.241451 30346 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0520 19:08:41.508105 30346 solver.cpp:237] Iteration 845, loss = 1.80996
I0520 19:08:41.508139 30346 solver.cpp:253]     Train net output #0: loss = 1.80996 (* 1 = 1.80996 loss)
I0520 19:08:41.508152 30346 sgd_solver.cpp:106] Iteration 845, lr = 0.0025
I0520 19:09:11.922288 30346 solver.cpp:237] Iteration 910, loss = 1.93484
I0520 19:09:11.922446 30346 solver.cpp:253]     Train net output #0: loss = 1.93484 (* 1 = 1.93484 loss)
I0520 19:09:11.922462 30346 sgd_solver.cpp:106] Iteration 910, lr = 0.0025
I0520 19:09:20.192564 30346 solver.cpp:237] Iteration 975, loss = 1.84522
I0520 19:09:20.192596 30346 solver.cpp:253]     Train net output #0: loss = 1.84522 (* 1 = 1.84522 loss)
I0520 19:09:20.192615 30346 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0520 19:09:28.455879 30346 solver.cpp:237] Iteration 1040, loss = 1.80478
I0520 19:09:28.455925 30346 solver.cpp:253]     Train net output #0: loss = 1.80478 (* 1 = 1.80478 loss)
I0520 19:09:28.455942 30346 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0520 19:09:36.727792 30346 solver.cpp:237] Iteration 1105, loss = 1.9021
I0520 19:09:36.727825 30346 solver.cpp:253]     Train net output #0: loss = 1.9021 (* 1 = 1.9021 loss)
I0520 19:09:36.727839 30346 sgd_solver.cpp:106] Iteration 1105, lr = 0.0025
I0520 19:09:44.992061 30346 solver.cpp:237] Iteration 1170, loss = 1.74946
I0520 19:09:44.992208 30346 solver.cpp:253]     Train net output #0: loss = 1.74946 (* 1 = 1.74946 loss)
I0520 19:09:44.992221 30346 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0520 19:09:53.273224 30346 solver.cpp:237] Iteration 1235, loss = 1.74277
I0520 19:09:53.273265 30346 solver.cpp:253]     Train net output #0: loss = 1.74277 (* 1 = 1.74277 loss)
I0520 19:09:53.273283 30346 sgd_solver.cpp:106] Iteration 1235, lr = 0.0025
I0520 19:10:01.556113 30346 solver.cpp:237] Iteration 1300, loss = 1.86441
I0520 19:10:01.556148 30346 solver.cpp:253]     Train net output #0: loss = 1.86441 (* 1 = 1.86441 loss)
I0520 19:10:01.556161 30346 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0520 19:10:01.940073 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_1304.caffemodel
I0520 19:10:02.067788 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_1304.solverstate
I0520 19:10:02.094127 30346 solver.cpp:341] Iteration 1304, Testing net (#0)
I0520 19:10:47.714453 30346 solver.cpp:409]     Test net output #0: accuracy = 0.637877
I0520 19:10:47.714617 30346 solver.cpp:409]     Test net output #1: loss = 1.26524 (* 1 = 1.26524 loss)
I0520 19:11:17.700770 30346 solver.cpp:237] Iteration 1365, loss = 1.86283
I0520 19:11:17.700819 30346 solver.cpp:253]     Train net output #0: loss = 1.86283 (* 1 = 1.86283 loss)
I0520 19:11:17.700839 30346 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0520 19:11:25.974416 30346 solver.cpp:237] Iteration 1430, loss = 1.76635
I0520 19:11:25.974566 30346 solver.cpp:253]     Train net output #0: loss = 1.76635 (* 1 = 1.76635 loss)
I0520 19:11:25.974581 30346 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0520 19:11:34.246616 30346 solver.cpp:237] Iteration 1495, loss = 1.7247
I0520 19:11:34.246650 30346 solver.cpp:253]     Train net output #0: loss = 1.7247 (* 1 = 1.7247 loss)
I0520 19:11:34.246667 30346 sgd_solver.cpp:106] Iteration 1495, lr = 0.0025
I0520 19:11:42.508414 30346 solver.cpp:237] Iteration 1560, loss = 1.84035
I0520 19:11:42.508447 30346 solver.cpp:253]     Train net output #0: loss = 1.84035 (* 1 = 1.84035 loss)
I0520 19:11:42.508461 30346 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0520 19:11:50.772050 30346 solver.cpp:237] Iteration 1625, loss = 1.80979
I0520 19:11:50.772084 30346 solver.cpp:253]     Train net output #0: loss = 1.80979 (* 1 = 1.80979 loss)
I0520 19:11:50.772100 30346 sgd_solver.cpp:106] Iteration 1625, lr = 0.0025
I0520 19:11:59.041913 30346 solver.cpp:237] Iteration 1690, loss = 1.65936
I0520 19:11:59.042053 30346 solver.cpp:253]     Train net output #0: loss = 1.65936 (* 1 = 1.65936 loss)
I0520 19:11:59.042068 30346 sgd_solver.cpp:106] Iteration 1690, lr = 0.0025
I0520 19:12:29.500308 30346 solver.cpp:237] Iteration 1755, loss = 1.65316
I0520 19:12:29.500474 30346 solver.cpp:253]     Train net output #0: loss = 1.65316 (* 1 = 1.65316 loss)
I0520 19:12:29.500490 30346 sgd_solver.cpp:106] Iteration 1755, lr = 0.0025
I0520 19:12:37.775084 30346 solver.cpp:237] Iteration 1820, loss = 1.81751
I0520 19:12:37.775116 30346 solver.cpp:253]     Train net output #0: loss = 1.81751 (* 1 = 1.81751 loss)
I0520 19:12:37.775133 30346 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0520 19:12:46.050999 30346 solver.cpp:237] Iteration 1885, loss = 1.76451
I0520 19:12:46.051039 30346 solver.cpp:253]     Train net output #0: loss = 1.76451 (* 1 = 1.76451 loss)
I0520 19:12:46.051056 30346 sgd_solver.cpp:106] Iteration 1885, lr = 0.0025
I0520 19:12:54.330435 30346 solver.cpp:237] Iteration 1950, loss = 1.76426
I0520 19:12:54.330469 30346 solver.cpp:253]     Train net output #0: loss = 1.76426 (* 1 = 1.76426 loss)
I0520 19:12:54.330482 30346 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0520 19:12:54.965368 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_1956.caffemodel
I0520 19:12:55.095006 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_1956.solverstate
I0520 19:13:02.666482 30346 solver.cpp:237] Iteration 2015, loss = 1.70614
I0520 19:13:02.666656 30346 solver.cpp:253]     Train net output #0: loss = 1.70614 (* 1 = 1.70614 loss)
I0520 19:13:02.666671 30346 sgd_solver.cpp:106] Iteration 2015, lr = 0.0025
I0520 19:13:10.944766 30346 solver.cpp:237] Iteration 2080, loss = 1.68873
I0520 19:13:10.944803 30346 solver.cpp:253]     Train net output #0: loss = 1.68873 (* 1 = 1.68873 loss)
I0520 19:13:10.944821 30346 sgd_solver.cpp:106] Iteration 2080, lr = 0.0025
I0520 19:13:19.216897 30346 solver.cpp:237] Iteration 2145, loss = 1.65393
I0520 19:13:19.216930 30346 solver.cpp:253]     Train net output #0: loss = 1.65393 (* 1 = 1.65393 loss)
I0520 19:13:19.216946 30346 sgd_solver.cpp:106] Iteration 2145, lr = 0.0025
I0520 19:13:49.655735 30346 solver.cpp:237] Iteration 2210, loss = 1.70206
I0520 19:13:49.655897 30346 solver.cpp:253]     Train net output #0: loss = 1.70206 (* 1 = 1.70206 loss)
I0520 19:13:49.655912 30346 sgd_solver.cpp:106] Iteration 2210, lr = 0.0025
I0520 19:13:57.937160 30346 solver.cpp:237] Iteration 2275, loss = 1.70086
I0520 19:13:57.937207 30346 solver.cpp:253]     Train net output #0: loss = 1.70086 (* 1 = 1.70086 loss)
I0520 19:13:57.937224 30346 sgd_solver.cpp:106] Iteration 2275, lr = 0.0025
I0520 19:14:06.213930 30346 solver.cpp:237] Iteration 2340, loss = 1.53224
I0520 19:14:06.213959 30346 solver.cpp:253]     Train net output #0: loss = 1.53224 (* 1 = 1.53224 loss)
I0520 19:14:06.213975 30346 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0520 19:14:14.482178 30346 solver.cpp:237] Iteration 2405, loss = 1.47324
I0520 19:14:14.482206 30346 solver.cpp:253]     Train net output #0: loss = 1.47324 (* 1 = 1.47324 loss)
I0520 19:14:14.482219 30346 sgd_solver.cpp:106] Iteration 2405, lr = 0.0025
I0520 19:14:22.744195 30346 solver.cpp:237] Iteration 2470, loss = 1.62843
I0520 19:14:22.744345 30346 solver.cpp:253]     Train net output #0: loss = 1.62843 (* 1 = 1.62843 loss)
I0520 19:14:22.744359 30346 sgd_solver.cpp:106] Iteration 2470, lr = 0.0025
I0520 19:14:31.010192 30346 solver.cpp:237] Iteration 2535, loss = 1.68633
I0520 19:14:31.010226 30346 solver.cpp:253]     Train net output #0: loss = 1.68633 (* 1 = 1.68633 loss)
I0520 19:14:31.010239 30346 sgd_solver.cpp:106] Iteration 2535, lr = 0.0025
I0520 19:14:39.270730 30346 solver.cpp:237] Iteration 2600, loss = 1.77152
I0520 19:14:39.270763 30346 solver.cpp:253]     Train net output #0: loss = 1.77152 (* 1 = 1.77152 loss)
I0520 19:14:39.270777 30346 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0520 19:14:40.161250 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_2608.caffemodel
I0520 19:14:40.290025 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_2608.solverstate
I0520 19:14:40.318516 30346 solver.cpp:341] Iteration 2608, Testing net (#0)
I0520 19:15:46.802470 30346 solver.cpp:409]     Test net output #0: accuracy = 0.690357
I0520 19:15:46.802641 30346 solver.cpp:409]     Test net output #1: loss = 1.07222 (* 1 = 1.07222 loss)
I0520 19:16:16.254676 30346 solver.cpp:237] Iteration 2665, loss = 1.53456
I0520 19:16:16.254732 30346 solver.cpp:253]     Train net output #0: loss = 1.53456 (* 1 = 1.53456 loss)
I0520 19:16:16.254748 30346 sgd_solver.cpp:106] Iteration 2665, lr = 0.0025
I0520 19:16:24.520905 30346 solver.cpp:237] Iteration 2730, loss = 1.6362
I0520 19:16:24.521054 30346 solver.cpp:253]     Train net output #0: loss = 1.6362 (* 1 = 1.6362 loss)
I0520 19:16:24.521067 30346 sgd_solver.cpp:106] Iteration 2730, lr = 0.0025
I0520 19:16:32.787588 30346 solver.cpp:237] Iteration 2795, loss = 1.69563
I0520 19:16:32.787636 30346 solver.cpp:253]     Train net output #0: loss = 1.69563 (* 1 = 1.69563 loss)
I0520 19:16:32.787652 30346 sgd_solver.cpp:106] Iteration 2795, lr = 0.0025
I0520 19:16:41.051554 30346 solver.cpp:237] Iteration 2860, loss = 1.64564
I0520 19:16:41.051589 30346 solver.cpp:253]     Train net output #0: loss = 1.64564 (* 1 = 1.64564 loss)
I0520 19:16:41.051604 30346 sgd_solver.cpp:106] Iteration 2860, lr = 0.0025
I0520 19:16:49.324450 30346 solver.cpp:237] Iteration 2925, loss = 1.73339
I0520 19:16:49.324484 30346 solver.cpp:253]     Train net output #0: loss = 1.73339 (* 1 = 1.73339 loss)
I0520 19:16:49.324501 30346 sgd_solver.cpp:106] Iteration 2925, lr = 0.0025
I0520 19:16:57.607208 30346 solver.cpp:237] Iteration 2990, loss = 1.57748
I0520 19:16:57.607359 30346 solver.cpp:253]     Train net output #0: loss = 1.57748 (* 1 = 1.57748 loss)
I0520 19:16:57.607373 30346 sgd_solver.cpp:106] Iteration 2990, lr = 0.0025
I0520 19:17:28.076781 30346 solver.cpp:237] Iteration 3055, loss = 1.57268
I0520 19:17:28.076952 30346 solver.cpp:253]     Train net output #0: loss = 1.57268 (* 1 = 1.57268 loss)
I0520 19:17:28.076967 30346 sgd_solver.cpp:106] Iteration 3055, lr = 0.0025
I0520 19:17:36.351541 30346 solver.cpp:237] Iteration 3120, loss = 1.47587
I0520 19:17:36.351574 30346 solver.cpp:253]     Train net output #0: loss = 1.47587 (* 1 = 1.47587 loss)
I0520 19:17:36.351593 30346 sgd_solver.cpp:106] Iteration 3120, lr = 0.0025
I0520 19:17:44.623491 30346 solver.cpp:237] Iteration 3185, loss = 1.56629
I0520 19:17:44.623538 30346 solver.cpp:253]     Train net output #0: loss = 1.56629 (* 1 = 1.56629 loss)
I0520 19:17:44.623554 30346 sgd_solver.cpp:106] Iteration 3185, lr = 0.0025
I0520 19:17:52.893741 30346 solver.cpp:237] Iteration 3250, loss = 1.72031
I0520 19:17:52.893775 30346 solver.cpp:253]     Train net output #0: loss = 1.72031 (* 1 = 1.72031 loss)
I0520 19:17:52.893790 30346 sgd_solver.cpp:106] Iteration 3250, lr = 0.0025
I0520 19:17:54.037942 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_3260.caffemodel
I0520 19:17:54.166437 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_3260.solverstate
I0520 19:18:01.227373 30346 solver.cpp:237] Iteration 3315, loss = 1.5488
I0520 19:18:01.227536 30346 solver.cpp:253]     Train net output #0: loss = 1.5488 (* 1 = 1.5488 loss)
I0520 19:18:01.227550 30346 sgd_solver.cpp:106] Iteration 3315, lr = 0.0025
I0520 19:18:09.500180 30346 solver.cpp:237] Iteration 3380, loss = 1.60012
I0520 19:18:09.500228 30346 solver.cpp:253]     Train net output #0: loss = 1.60012 (* 1 = 1.60012 loss)
I0520 19:18:09.500247 30346 sgd_solver.cpp:106] Iteration 3380, lr = 0.0025
I0520 19:18:17.773938 30346 solver.cpp:237] Iteration 3445, loss = 1.56475
I0520 19:18:17.773972 30346 solver.cpp:253]     Train net output #0: loss = 1.56475 (* 1 = 1.56475 loss)
I0520 19:18:17.773988 30346 sgd_solver.cpp:106] Iteration 3445, lr = 0.0025
I0520 19:18:48.248608 30346 solver.cpp:237] Iteration 3510, loss = 1.50783
I0520 19:18:48.248790 30346 solver.cpp:253]     Train net output #0: loss = 1.50783 (* 1 = 1.50783 loss)
I0520 19:18:48.248805 30346 sgd_solver.cpp:106] Iteration 3510, lr = 0.0025
I0520 19:18:56.525445 30346 solver.cpp:237] Iteration 3575, loss = 1.5604
I0520 19:18:56.525485 30346 solver.cpp:253]     Train net output #0: loss = 1.5604 (* 1 = 1.5604 loss)
I0520 19:18:56.525507 30346 sgd_solver.cpp:106] Iteration 3575, lr = 0.0025
I0520 19:19:04.802665 30346 solver.cpp:237] Iteration 3640, loss = 1.57351
I0520 19:19:04.802700 30346 solver.cpp:253]     Train net output #0: loss = 1.57351 (* 1 = 1.57351 loss)
I0520 19:19:04.802716 30346 sgd_solver.cpp:106] Iteration 3640, lr = 0.0025
I0520 19:19:13.073420 30346 solver.cpp:237] Iteration 3705, loss = 1.54776
I0520 19:19:13.073453 30346 solver.cpp:253]     Train net output #0: loss = 1.54776 (* 1 = 1.54776 loss)
I0520 19:19:13.073470 30346 sgd_solver.cpp:106] Iteration 3705, lr = 0.0025
I0520 19:19:21.337419 30346 solver.cpp:237] Iteration 3770, loss = 1.59386
I0520 19:19:21.337574 30346 solver.cpp:253]     Train net output #0: loss = 1.59386 (* 1 = 1.59386 loss)
I0520 19:19:21.337589 30346 sgd_solver.cpp:106] Iteration 3770, lr = 0.0025
I0520 19:19:29.618933 30346 solver.cpp:237] Iteration 3835, loss = 1.55071
I0520 19:19:29.618968 30346 solver.cpp:253]     Train net output #0: loss = 1.55071 (* 1 = 1.55071 loss)
I0520 19:19:29.618984 30346 sgd_solver.cpp:106] Iteration 3835, lr = 0.0025
I0520 19:19:37.886513 30346 solver.cpp:237] Iteration 3900, loss = 1.42744
I0520 19:19:37.886548 30346 solver.cpp:253]     Train net output #0: loss = 1.42744 (* 1 = 1.42744 loss)
I0520 19:19:37.886561 30346 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0520 19:19:39.286937 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_3912.caffemodel
I0520 19:19:39.413036 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_3912.solverstate
I0520 19:19:39.439199 30346 solver.cpp:341] Iteration 3912, Testing net (#0)
I0520 19:20:24.735656 30346 solver.cpp:409]     Test net output #0: accuracy = 0.749193
I0520 19:20:24.735829 30346 solver.cpp:409]     Test net output #1: loss = 0.89774 (* 1 = 0.89774 loss)
I0520 19:20:53.668066 30346 solver.cpp:237] Iteration 3965, loss = 1.53491
I0520 19:20:53.668115 30346 solver.cpp:253]     Train net output #0: loss = 1.53491 (* 1 = 1.53491 loss)
I0520 19:20:53.668133 30346 sgd_solver.cpp:106] Iteration 3965, lr = 0.0025
I0520 19:21:01.944644 30346 solver.cpp:237] Iteration 4030, loss = 1.43437
I0520 19:21:01.944809 30346 solver.cpp:253]     Train net output #0: loss = 1.43437 (* 1 = 1.43437 loss)
I0520 19:21:01.944823 30346 sgd_solver.cpp:106] Iteration 4030, lr = 0.0025
I0520 19:21:10.216420 30346 solver.cpp:237] Iteration 4095, loss = 1.41559
I0520 19:21:10.216454 30346 solver.cpp:253]     Train net output #0: loss = 1.41559 (* 1 = 1.41559 loss)
I0520 19:21:10.216470 30346 sgd_solver.cpp:106] Iteration 4095, lr = 0.0025
I0520 19:21:18.489418 30346 solver.cpp:237] Iteration 4160, loss = 1.43676
I0520 19:21:18.489452 30346 solver.cpp:253]     Train net output #0: loss = 1.43676 (* 1 = 1.43676 loss)
I0520 19:21:18.489469 30346 sgd_solver.cpp:106] Iteration 4160, lr = 0.0025
I0520 19:21:26.758653 30346 solver.cpp:237] Iteration 4225, loss = 1.42213
I0520 19:21:26.758697 30346 solver.cpp:253]     Train net output #0: loss = 1.42213 (* 1 = 1.42213 loss)
I0520 19:21:26.758715 30346 sgd_solver.cpp:106] Iteration 4225, lr = 0.0025
I0520 19:21:35.018761 30346 solver.cpp:237] Iteration 4290, loss = 1.42399
I0520 19:21:35.018921 30346 solver.cpp:253]     Train net output #0: loss = 1.42399 (* 1 = 1.42399 loss)
I0520 19:21:35.018935 30346 sgd_solver.cpp:106] Iteration 4290, lr = 0.0025
I0520 19:22:05.473459 30346 solver.cpp:237] Iteration 4355, loss = 1.58037
I0520 19:22:05.473623 30346 solver.cpp:253]     Train net output #0: loss = 1.58037 (* 1 = 1.58037 loss)
I0520 19:22:05.473639 30346 sgd_solver.cpp:106] Iteration 4355, lr = 0.0025
I0520 19:22:13.743605 30346 solver.cpp:237] Iteration 4420, loss = 1.36161
I0520 19:22:13.743639 30346 solver.cpp:253]     Train net output #0: loss = 1.36161 (* 1 = 1.36161 loss)
I0520 19:22:13.743655 30346 sgd_solver.cpp:106] Iteration 4420, lr = 0.0025
I0520 19:22:22.010571 30346 solver.cpp:237] Iteration 4485, loss = 1.34518
I0520 19:22:22.010604 30346 solver.cpp:253]     Train net output #0: loss = 1.34518 (* 1 = 1.34518 loss)
I0520 19:22:22.010624 30346 sgd_solver.cpp:106] Iteration 4485, lr = 0.0025
I0520 19:22:30.279551 30346 solver.cpp:237] Iteration 4550, loss = 1.40831
I0520 19:22:30.279587 30346 solver.cpp:253]     Train net output #0: loss = 1.40831 (* 1 = 1.40831 loss)
I0520 19:22:30.279600 30346 sgd_solver.cpp:106] Iteration 4550, lr = 0.0025
I0520 19:22:31.933405 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_4564.caffemodel
I0520 19:22:32.060228 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_4564.solverstate
I0520 19:22:38.613513 30346 solver.cpp:237] Iteration 4615, loss = 1.48114
I0520 19:22:38.613675 30346 solver.cpp:253]     Train net output #0: loss = 1.48114 (* 1 = 1.48114 loss)
I0520 19:22:38.613689 30346 sgd_solver.cpp:106] Iteration 4615, lr = 0.0025
I0520 19:22:46.885660 30346 solver.cpp:237] Iteration 4680, loss = 1.50666
I0520 19:22:46.885706 30346 solver.cpp:253]     Train net output #0: loss = 1.50666 (* 1 = 1.50666 loss)
I0520 19:22:46.885725 30346 sgd_solver.cpp:106] Iteration 4680, lr = 0.0025
I0520 19:22:55.154883 30346 solver.cpp:237] Iteration 4745, loss = 1.41573
I0520 19:22:55.154917 30346 solver.cpp:253]     Train net output #0: loss = 1.41573 (* 1 = 1.41573 loss)
I0520 19:22:55.154934 30346 sgd_solver.cpp:106] Iteration 4745, lr = 0.0025
I0520 19:23:25.620462 30346 solver.cpp:237] Iteration 4810, loss = 1.4679
I0520 19:23:25.620636 30346 solver.cpp:253]     Train net output #0: loss = 1.4679 (* 1 = 1.4679 loss)
I0520 19:23:25.620651 30346 sgd_solver.cpp:106] Iteration 4810, lr = 0.0025
I0520 19:23:33.893389 30346 solver.cpp:237] Iteration 4875, loss = 1.52424
I0520 19:23:33.893437 30346 solver.cpp:253]     Train net output #0: loss = 1.52424 (* 1 = 1.52424 loss)
I0520 19:23:33.893455 30346 sgd_solver.cpp:106] Iteration 4875, lr = 0.0025
I0520 19:23:42.157120 30346 solver.cpp:237] Iteration 4940, loss = 1.52864
I0520 19:23:42.157155 30346 solver.cpp:253]     Train net output #0: loss = 1.52864 (* 1 = 1.52864 loss)
I0520 19:23:42.157168 30346 sgd_solver.cpp:106] Iteration 4940, lr = 0.0025
I0520 19:23:50.419701 30346 solver.cpp:237] Iteration 5005, loss = 1.28588
I0520 19:23:50.419735 30346 solver.cpp:253]     Train net output #0: loss = 1.28588 (* 1 = 1.28588 loss)
I0520 19:23:50.419751 30346 sgd_solver.cpp:106] Iteration 5005, lr = 0.0025
I0520 19:23:58.685938 30346 solver.cpp:237] Iteration 5070, loss = 1.29906
I0520 19:23:58.686112 30346 solver.cpp:253]     Train net output #0: loss = 1.29906 (* 1 = 1.29906 loss)
I0520 19:23:58.686127 30346 sgd_solver.cpp:106] Iteration 5070, lr = 0.0025
I0520 19:24:06.946331 30346 solver.cpp:237] Iteration 5135, loss = 1.41243
I0520 19:24:06.946364 30346 solver.cpp:253]     Train net output #0: loss = 1.41243 (* 1 = 1.41243 loss)
I0520 19:24:06.946382 30346 sgd_solver.cpp:106] Iteration 5135, lr = 0.0025
I0520 19:24:15.213907 30346 solver.cpp:237] Iteration 5200, loss = 1.37678
I0520 19:24:15.213942 30346 solver.cpp:253]     Train net output #0: loss = 1.37678 (* 1 = 1.37678 loss)
I0520 19:24:15.213958 30346 sgd_solver.cpp:106] Iteration 5200, lr = 0.0025
I0520 19:24:17.122932 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_5216.caffemodel
I0520 19:24:17.250231 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_5216.solverstate
I0520 19:24:17.276727 30346 solver.cpp:341] Iteration 5216, Testing net (#0)
I0520 19:25:23.782347 30346 solver.cpp:409]     Test net output #0: accuracy = 0.790551
I0520 19:25:23.782528 30346 solver.cpp:409]     Test net output #1: loss = 0.771279 (* 1 = 0.771279 loss)
I0520 19:25:52.174223 30346 solver.cpp:237] Iteration 5265, loss = 1.53417
I0520 19:25:52.174278 30346 solver.cpp:253]     Train net output #0: loss = 1.53417 (* 1 = 1.53417 loss)
I0520 19:25:52.174293 30346 sgd_solver.cpp:106] Iteration 5265, lr = 0.0025
I0520 19:26:00.434644 30346 solver.cpp:237] Iteration 5330, loss = 1.3633
I0520 19:26:00.434789 30346 solver.cpp:253]     Train net output #0: loss = 1.3633 (* 1 = 1.3633 loss)
I0520 19:26:00.434803 30346 sgd_solver.cpp:106] Iteration 5330, lr = 0.0025
I0520 19:26:08.703548 30346 solver.cpp:237] Iteration 5395, loss = 1.47107
I0520 19:26:08.703579 30346 solver.cpp:253]     Train net output #0: loss = 1.47107 (* 1 = 1.47107 loss)
I0520 19:26:08.703605 30346 sgd_solver.cpp:106] Iteration 5395, lr = 0.0025
I0520 19:26:16.968639 30346 solver.cpp:237] Iteration 5460, loss = 1.43993
I0520 19:26:16.968672 30346 solver.cpp:253]     Train net output #0: loss = 1.43993 (* 1 = 1.43993 loss)
I0520 19:26:16.968689 30346 sgd_solver.cpp:106] Iteration 5460, lr = 0.0025
I0520 19:26:25.237025 30346 solver.cpp:237] Iteration 5525, loss = 1.50468
I0520 19:26:25.237057 30346 solver.cpp:253]     Train net output #0: loss = 1.50468 (* 1 = 1.50468 loss)
I0520 19:26:25.237073 30346 sgd_solver.cpp:106] Iteration 5525, lr = 0.0025
I0520 19:26:33.502305 30346 solver.cpp:237] Iteration 5590, loss = 1.38588
I0520 19:26:33.502451 30346 solver.cpp:253]     Train net output #0: loss = 1.38588 (* 1 = 1.38588 loss)
I0520 19:26:33.502465 30346 sgd_solver.cpp:106] Iteration 5590, lr = 0.0025
I0520 19:27:03.959182 30346 solver.cpp:237] Iteration 5655, loss = 1.35321
I0520 19:27:03.959364 30346 solver.cpp:253]     Train net output #0: loss = 1.35321 (* 1 = 1.35321 loss)
I0520 19:27:03.959380 30346 sgd_solver.cpp:106] Iteration 5655, lr = 0.0025
I0520 19:27:12.228780 30346 solver.cpp:237] Iteration 5720, loss = 1.36648
I0520 19:27:12.228814 30346 solver.cpp:253]     Train net output #0: loss = 1.36648 (* 1 = 1.36648 loss)
I0520 19:27:12.228832 30346 sgd_solver.cpp:106] Iteration 5720, lr = 0.0025
I0520 19:27:20.496130 30346 solver.cpp:237] Iteration 5785, loss = 1.47859
I0520 19:27:20.496181 30346 solver.cpp:253]     Train net output #0: loss = 1.47859 (* 1 = 1.47859 loss)
I0520 19:27:20.496196 30346 sgd_solver.cpp:106] Iteration 5785, lr = 0.0025
I0520 19:27:28.764645 30346 solver.cpp:237] Iteration 5850, loss = 1.50068
I0520 19:27:28.764679 30346 solver.cpp:253]     Train net output #0: loss = 1.50068 (* 1 = 1.50068 loss)
I0520 19:27:28.764693 30346 sgd_solver.cpp:106] Iteration 5850, lr = 0.0025
I0520 19:27:30.930387 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_5868.caffemodel
I0520 19:27:31.059429 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_5868.solverstate
I0520 19:27:37.109364 30346 solver.cpp:237] Iteration 5915, loss = 1.51742
I0520 19:27:37.109531 30346 solver.cpp:253]     Train net output #0: loss = 1.51742 (* 1 = 1.51742 loss)
I0520 19:27:37.109545 30346 sgd_solver.cpp:106] Iteration 5915, lr = 0.0025
I0520 19:27:45.378746 30346 solver.cpp:237] Iteration 5980, loss = 1.52071
I0520 19:27:45.378790 30346 solver.cpp:253]     Train net output #0: loss = 1.52071 (* 1 = 1.52071 loss)
I0520 19:27:45.378810 30346 sgd_solver.cpp:106] Iteration 5980, lr = 0.0025
I0520 19:27:53.646981 30346 solver.cpp:237] Iteration 6045, loss = 1.29699
I0520 19:27:53.647014 30346 solver.cpp:253]     Train net output #0: loss = 1.29699 (* 1 = 1.29699 loss)
I0520 19:27:53.647027 30346 sgd_solver.cpp:106] Iteration 6045, lr = 0.0025
I0520 19:28:24.090559 30346 solver.cpp:237] Iteration 6110, loss = 1.46183
I0520 19:28:24.090742 30346 solver.cpp:253]     Train net output #0: loss = 1.46183 (* 1 = 1.46183 loss)
I0520 19:28:24.090759 30346 sgd_solver.cpp:106] Iteration 6110, lr = 0.0025
I0520 19:28:32.359297 30346 solver.cpp:237] Iteration 6175, loss = 1.56976
I0520 19:28:32.359344 30346 solver.cpp:253]     Train net output #0: loss = 1.56976 (* 1 = 1.56976 loss)
I0520 19:28:32.359360 30346 sgd_solver.cpp:106] Iteration 6175, lr = 0.0025
I0520 19:28:40.638784 30346 solver.cpp:237] Iteration 6240, loss = 1.32368
I0520 19:28:40.638819 30346 solver.cpp:253]     Train net output #0: loss = 1.32368 (* 1 = 1.32368 loss)
I0520 19:28:40.638836 30346 sgd_solver.cpp:106] Iteration 6240, lr = 0.0025
I0520 19:28:48.914383 30346 solver.cpp:237] Iteration 6305, loss = 1.37264
I0520 19:28:48.914417 30346 solver.cpp:253]     Train net output #0: loss = 1.37264 (* 1 = 1.37264 loss)
I0520 19:28:48.914435 30346 sgd_solver.cpp:106] Iteration 6305, lr = 0.0025
I0520 19:28:57.181552 30346 solver.cpp:237] Iteration 6370, loss = 1.35946
I0520 19:28:57.181709 30346 solver.cpp:253]     Train net output #0: loss = 1.35946 (* 1 = 1.35946 loss)
I0520 19:28:57.181722 30346 sgd_solver.cpp:106] Iteration 6370, lr = 0.0025
I0520 19:29:05.441429 30346 solver.cpp:237] Iteration 6435, loss = 1.4614
I0520 19:29:05.441462 30346 solver.cpp:253]     Train net output #0: loss = 1.4614 (* 1 = 1.4614 loss)
I0520 19:29:05.441479 30346 sgd_solver.cpp:106] Iteration 6435, lr = 0.0025
I0520 19:29:13.715749 30346 solver.cpp:237] Iteration 6500, loss = 1.43764
I0520 19:29:13.715783 30346 solver.cpp:253]     Train net output #0: loss = 1.43764 (* 1 = 1.43764 loss)
I0520 19:29:13.715801 30346 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0520 19:29:16.131009 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_6520.caffemodel
I0520 19:29:16.259500 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_6520.solverstate
I0520 19:29:16.287933 30346 solver.cpp:341] Iteration 6520, Testing net (#0)
I0520 19:30:01.951197 30346 solver.cpp:409]     Test net output #0: accuracy = 0.811189
I0520 19:30:01.951377 30346 solver.cpp:409]     Test net output #1: loss = 0.699342 (* 1 = 0.699342 loss)
I0520 19:30:01.989959 30346 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_6521.caffemodel
I0520 19:30:02.119712 30346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138_iter_6521.solverstate
I0520 19:30:02.147992 30346 solver.cpp:326] Optimization Done.
I0520 19:30:02.148023 30346 caffe.cpp:215] Optimization Done.
Application 11234718 resources: utime ~1272s, stime ~230s, Rss ~5328940, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_230_2016-05-20T11.20.41.171138.solver"
	User time (seconds): 0.53
	System time (seconds): 0.17
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:07.35
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2779
	Involuntary context switches: 75
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
