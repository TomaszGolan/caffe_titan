2805279
I0520 15:28:45.388164 22173 caffe.cpp:184] Using GPUs 0
I0520 15:28:45.817692 22173 solver.cpp:48] Initializing solver from parameters: 
test_iter: 882
test_interval: 1764
base_lr: 0.0025
display: 88
max_iter: 8823
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 882
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113.prototxt"
I0520 15:28:45.819461 22173 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113.prototxt
I0520 15:28:45.833281 22173 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 15:28:45.833343 22173 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 15:28:45.833686 22173 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 170
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:28:45.833863 22173 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:28:45.833886 22173 net.cpp:106] Creating Layer data_hdf5
I0520 15:28:45.833900 22173 net.cpp:411] data_hdf5 -> data
I0520 15:28:45.833935 22173 net.cpp:411] data_hdf5 -> label
I0520 15:28:45.833966 22173 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 15:28:45.835188 22173 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 15:28:45.837355 22173 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 15:29:07.346627 22173 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 15:29:07.351796 22173 net.cpp:150] Setting up data_hdf5
I0520 15:29:07.351836 22173 net.cpp:157] Top shape: 170 1 127 50 (1079500)
I0520 15:29:07.351851 22173 net.cpp:157] Top shape: 170 (170)
I0520 15:29:07.351863 22173 net.cpp:165] Memory required for data: 4318680
I0520 15:29:07.351877 22173 layer_factory.hpp:77] Creating layer conv1
I0520 15:29:07.351912 22173 net.cpp:106] Creating Layer conv1
I0520 15:29:07.351922 22173 net.cpp:454] conv1 <- data
I0520 15:29:07.351944 22173 net.cpp:411] conv1 -> conv1
I0520 15:29:09.160297 22173 net.cpp:150] Setting up conv1
I0520 15:29:09.160344 22173 net.cpp:157] Top shape: 170 12 120 48 (11750400)
I0520 15:29:09.160356 22173 net.cpp:165] Memory required for data: 51320280
I0520 15:29:09.160384 22173 layer_factory.hpp:77] Creating layer relu1
I0520 15:29:09.160405 22173 net.cpp:106] Creating Layer relu1
I0520 15:29:09.160416 22173 net.cpp:454] relu1 <- conv1
I0520 15:29:09.160429 22173 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:29:09.160954 22173 net.cpp:150] Setting up relu1
I0520 15:29:09.160971 22173 net.cpp:157] Top shape: 170 12 120 48 (11750400)
I0520 15:29:09.160982 22173 net.cpp:165] Memory required for data: 98321880
I0520 15:29:09.160992 22173 layer_factory.hpp:77] Creating layer pool1
I0520 15:29:09.161008 22173 net.cpp:106] Creating Layer pool1
I0520 15:29:09.161020 22173 net.cpp:454] pool1 <- conv1
I0520 15:29:09.161032 22173 net.cpp:411] pool1 -> pool1
I0520 15:29:09.161113 22173 net.cpp:150] Setting up pool1
I0520 15:29:09.161128 22173 net.cpp:157] Top shape: 170 12 60 48 (5875200)
I0520 15:29:09.161136 22173 net.cpp:165] Memory required for data: 121822680
I0520 15:29:09.161146 22173 layer_factory.hpp:77] Creating layer conv2
I0520 15:29:09.161169 22173 net.cpp:106] Creating Layer conv2
I0520 15:29:09.161180 22173 net.cpp:454] conv2 <- pool1
I0520 15:29:09.161192 22173 net.cpp:411] conv2 -> conv2
I0520 15:29:09.163889 22173 net.cpp:150] Setting up conv2
I0520 15:29:09.163916 22173 net.cpp:157] Top shape: 170 20 54 46 (8445600)
I0520 15:29:09.163930 22173 net.cpp:165] Memory required for data: 155605080
I0520 15:29:09.163951 22173 layer_factory.hpp:77] Creating layer relu2
I0520 15:29:09.163965 22173 net.cpp:106] Creating Layer relu2
I0520 15:29:09.163975 22173 net.cpp:454] relu2 <- conv2
I0520 15:29:09.163988 22173 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:29:09.164317 22173 net.cpp:150] Setting up relu2
I0520 15:29:09.164331 22173 net.cpp:157] Top shape: 170 20 54 46 (8445600)
I0520 15:29:09.164342 22173 net.cpp:165] Memory required for data: 189387480
I0520 15:29:09.164352 22173 layer_factory.hpp:77] Creating layer pool2
I0520 15:29:09.164364 22173 net.cpp:106] Creating Layer pool2
I0520 15:29:09.164374 22173 net.cpp:454] pool2 <- conv2
I0520 15:29:09.164399 22173 net.cpp:411] pool2 -> pool2
I0520 15:29:09.164468 22173 net.cpp:150] Setting up pool2
I0520 15:29:09.164481 22173 net.cpp:157] Top shape: 170 20 27 46 (4222800)
I0520 15:29:09.164490 22173 net.cpp:165] Memory required for data: 206278680
I0520 15:29:09.164500 22173 layer_factory.hpp:77] Creating layer conv3
I0520 15:29:09.164517 22173 net.cpp:106] Creating Layer conv3
I0520 15:29:09.164528 22173 net.cpp:454] conv3 <- pool2
I0520 15:29:09.164541 22173 net.cpp:411] conv3 -> conv3
I0520 15:29:09.166515 22173 net.cpp:150] Setting up conv3
I0520 15:29:09.166539 22173 net.cpp:157] Top shape: 170 28 22 44 (4607680)
I0520 15:29:09.166550 22173 net.cpp:165] Memory required for data: 224709400
I0520 15:29:09.166570 22173 layer_factory.hpp:77] Creating layer relu3
I0520 15:29:09.166586 22173 net.cpp:106] Creating Layer relu3
I0520 15:29:09.166596 22173 net.cpp:454] relu3 <- conv3
I0520 15:29:09.166609 22173 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:29:09.167079 22173 net.cpp:150] Setting up relu3
I0520 15:29:09.167098 22173 net.cpp:157] Top shape: 170 28 22 44 (4607680)
I0520 15:29:09.167107 22173 net.cpp:165] Memory required for data: 243140120
I0520 15:29:09.167117 22173 layer_factory.hpp:77] Creating layer pool3
I0520 15:29:09.167130 22173 net.cpp:106] Creating Layer pool3
I0520 15:29:09.167140 22173 net.cpp:454] pool3 <- conv3
I0520 15:29:09.167153 22173 net.cpp:411] pool3 -> pool3
I0520 15:29:09.167222 22173 net.cpp:150] Setting up pool3
I0520 15:29:09.167234 22173 net.cpp:157] Top shape: 170 28 11 44 (2303840)
I0520 15:29:09.167244 22173 net.cpp:165] Memory required for data: 252355480
I0520 15:29:09.167253 22173 layer_factory.hpp:77] Creating layer conv4
I0520 15:29:09.167269 22173 net.cpp:106] Creating Layer conv4
I0520 15:29:09.167280 22173 net.cpp:454] conv4 <- pool3
I0520 15:29:09.167294 22173 net.cpp:411] conv4 -> conv4
I0520 15:29:09.170264 22173 net.cpp:150] Setting up conv4
I0520 15:29:09.170291 22173 net.cpp:157] Top shape: 170 36 6 42 (1542240)
I0520 15:29:09.170302 22173 net.cpp:165] Memory required for data: 258524440
I0520 15:29:09.170317 22173 layer_factory.hpp:77] Creating layer relu4
I0520 15:29:09.170332 22173 net.cpp:106] Creating Layer relu4
I0520 15:29:09.170342 22173 net.cpp:454] relu4 <- conv4
I0520 15:29:09.170356 22173 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:29:09.170816 22173 net.cpp:150] Setting up relu4
I0520 15:29:09.170832 22173 net.cpp:157] Top shape: 170 36 6 42 (1542240)
I0520 15:29:09.170843 22173 net.cpp:165] Memory required for data: 264693400
I0520 15:29:09.170852 22173 layer_factory.hpp:77] Creating layer pool4
I0520 15:29:09.170866 22173 net.cpp:106] Creating Layer pool4
I0520 15:29:09.170876 22173 net.cpp:454] pool4 <- conv4
I0520 15:29:09.170888 22173 net.cpp:411] pool4 -> pool4
I0520 15:29:09.170956 22173 net.cpp:150] Setting up pool4
I0520 15:29:09.170970 22173 net.cpp:157] Top shape: 170 36 3 42 (771120)
I0520 15:29:09.170980 22173 net.cpp:165] Memory required for data: 267777880
I0520 15:29:09.170989 22173 layer_factory.hpp:77] Creating layer ip1
I0520 15:29:09.171010 22173 net.cpp:106] Creating Layer ip1
I0520 15:29:09.171020 22173 net.cpp:454] ip1 <- pool4
I0520 15:29:09.171033 22173 net.cpp:411] ip1 -> ip1
I0520 15:29:09.186456 22173 net.cpp:150] Setting up ip1
I0520 15:29:09.186486 22173 net.cpp:157] Top shape: 170 196 (33320)
I0520 15:29:09.186496 22173 net.cpp:165] Memory required for data: 267911160
I0520 15:29:09.186517 22173 layer_factory.hpp:77] Creating layer relu5
I0520 15:29:09.186532 22173 net.cpp:106] Creating Layer relu5
I0520 15:29:09.186543 22173 net.cpp:454] relu5 <- ip1
I0520 15:29:09.186560 22173 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:29:09.186902 22173 net.cpp:150] Setting up relu5
I0520 15:29:09.186914 22173 net.cpp:157] Top shape: 170 196 (33320)
I0520 15:29:09.186924 22173 net.cpp:165] Memory required for data: 268044440
I0520 15:29:09.186934 22173 layer_factory.hpp:77] Creating layer drop1
I0520 15:29:09.186956 22173 net.cpp:106] Creating Layer drop1
I0520 15:29:09.186966 22173 net.cpp:454] drop1 <- ip1
I0520 15:29:09.186990 22173 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:29:09.187038 22173 net.cpp:150] Setting up drop1
I0520 15:29:09.187052 22173 net.cpp:157] Top shape: 170 196 (33320)
I0520 15:29:09.187062 22173 net.cpp:165] Memory required for data: 268177720
I0520 15:29:09.187072 22173 layer_factory.hpp:77] Creating layer ip2
I0520 15:29:09.187090 22173 net.cpp:106] Creating Layer ip2
I0520 15:29:09.187100 22173 net.cpp:454] ip2 <- ip1
I0520 15:29:09.187114 22173 net.cpp:411] ip2 -> ip2
I0520 15:29:09.187579 22173 net.cpp:150] Setting up ip2
I0520 15:29:09.187592 22173 net.cpp:157] Top shape: 170 98 (16660)
I0520 15:29:09.187603 22173 net.cpp:165] Memory required for data: 268244360
I0520 15:29:09.187618 22173 layer_factory.hpp:77] Creating layer relu6
I0520 15:29:09.187630 22173 net.cpp:106] Creating Layer relu6
I0520 15:29:09.187640 22173 net.cpp:454] relu6 <- ip2
I0520 15:29:09.187652 22173 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:29:09.188172 22173 net.cpp:150] Setting up relu6
I0520 15:29:09.188189 22173 net.cpp:157] Top shape: 170 98 (16660)
I0520 15:29:09.188199 22173 net.cpp:165] Memory required for data: 268311000
I0520 15:29:09.188210 22173 layer_factory.hpp:77] Creating layer drop2
I0520 15:29:09.188221 22173 net.cpp:106] Creating Layer drop2
I0520 15:29:09.188231 22173 net.cpp:454] drop2 <- ip2
I0520 15:29:09.188244 22173 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:29:09.188287 22173 net.cpp:150] Setting up drop2
I0520 15:29:09.188299 22173 net.cpp:157] Top shape: 170 98 (16660)
I0520 15:29:09.188309 22173 net.cpp:165] Memory required for data: 268377640
I0520 15:29:09.188319 22173 layer_factory.hpp:77] Creating layer ip3
I0520 15:29:09.188333 22173 net.cpp:106] Creating Layer ip3
I0520 15:29:09.188343 22173 net.cpp:454] ip3 <- ip2
I0520 15:29:09.188354 22173 net.cpp:411] ip3 -> ip3
I0520 15:29:09.188565 22173 net.cpp:150] Setting up ip3
I0520 15:29:09.188577 22173 net.cpp:157] Top shape: 170 11 (1870)
I0520 15:29:09.188587 22173 net.cpp:165] Memory required for data: 268385120
I0520 15:29:09.188602 22173 layer_factory.hpp:77] Creating layer drop3
I0520 15:29:09.188616 22173 net.cpp:106] Creating Layer drop3
I0520 15:29:09.188624 22173 net.cpp:454] drop3 <- ip3
I0520 15:29:09.188637 22173 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:29:09.188675 22173 net.cpp:150] Setting up drop3
I0520 15:29:09.188688 22173 net.cpp:157] Top shape: 170 11 (1870)
I0520 15:29:09.188699 22173 net.cpp:165] Memory required for data: 268392600
I0520 15:29:09.188709 22173 layer_factory.hpp:77] Creating layer loss
I0520 15:29:09.188726 22173 net.cpp:106] Creating Layer loss
I0520 15:29:09.188737 22173 net.cpp:454] loss <- ip3
I0520 15:29:09.188747 22173 net.cpp:454] loss <- label
I0520 15:29:09.188760 22173 net.cpp:411] loss -> loss
I0520 15:29:09.188776 22173 layer_factory.hpp:77] Creating layer loss
I0520 15:29:09.189455 22173 net.cpp:150] Setting up loss
I0520 15:29:09.189476 22173 net.cpp:157] Top shape: (1)
I0520 15:29:09.189488 22173 net.cpp:160]     with loss weight 1
I0520 15:29:09.189533 22173 net.cpp:165] Memory required for data: 268392604
I0520 15:29:09.189544 22173 net.cpp:226] loss needs backward computation.
I0520 15:29:09.189555 22173 net.cpp:226] drop3 needs backward computation.
I0520 15:29:09.189563 22173 net.cpp:226] ip3 needs backward computation.
I0520 15:29:09.189574 22173 net.cpp:226] drop2 needs backward computation.
I0520 15:29:09.189584 22173 net.cpp:226] relu6 needs backward computation.
I0520 15:29:09.189594 22173 net.cpp:226] ip2 needs backward computation.
I0520 15:29:09.189604 22173 net.cpp:226] drop1 needs backward computation.
I0520 15:29:09.189613 22173 net.cpp:226] relu5 needs backward computation.
I0520 15:29:09.189622 22173 net.cpp:226] ip1 needs backward computation.
I0520 15:29:09.189633 22173 net.cpp:226] pool4 needs backward computation.
I0520 15:29:09.189643 22173 net.cpp:226] relu4 needs backward computation.
I0520 15:29:09.189653 22173 net.cpp:226] conv4 needs backward computation.
I0520 15:29:09.189663 22173 net.cpp:226] pool3 needs backward computation.
I0520 15:29:09.189674 22173 net.cpp:226] relu3 needs backward computation.
I0520 15:29:09.189692 22173 net.cpp:226] conv3 needs backward computation.
I0520 15:29:09.189704 22173 net.cpp:226] pool2 needs backward computation.
I0520 15:29:09.189715 22173 net.cpp:226] relu2 needs backward computation.
I0520 15:29:09.189725 22173 net.cpp:226] conv2 needs backward computation.
I0520 15:29:09.189736 22173 net.cpp:226] pool1 needs backward computation.
I0520 15:29:09.189746 22173 net.cpp:226] relu1 needs backward computation.
I0520 15:29:09.189756 22173 net.cpp:226] conv1 needs backward computation.
I0520 15:29:09.189767 22173 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:29:09.189777 22173 net.cpp:270] This network produces output loss
I0520 15:29:09.189801 22173 net.cpp:283] Network initialization done.
I0520 15:29:09.191403 22173 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113.prototxt
I0520 15:29:09.191474 22173 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 15:29:09.191828 22173 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 170
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:29:09.192018 22173 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:29:09.192033 22173 net.cpp:106] Creating Layer data_hdf5
I0520 15:29:09.192045 22173 net.cpp:411] data_hdf5 -> data
I0520 15:29:09.192062 22173 net.cpp:411] data_hdf5 -> label
I0520 15:29:09.192078 22173 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 15:29:09.193202 22173 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 15:29:30.495689 22173 net.cpp:150] Setting up data_hdf5
I0520 15:29:30.495857 22173 net.cpp:157] Top shape: 170 1 127 50 (1079500)
I0520 15:29:30.495872 22173 net.cpp:157] Top shape: 170 (170)
I0520 15:29:30.495882 22173 net.cpp:165] Memory required for data: 4318680
I0520 15:29:30.495896 22173 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 15:29:30.495924 22173 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 15:29:30.495934 22173 net.cpp:454] label_data_hdf5_1_split <- label
I0520 15:29:30.495950 22173 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 15:29:30.495971 22173 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 15:29:30.496044 22173 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 15:29:30.496058 22173 net.cpp:157] Top shape: 170 (170)
I0520 15:29:30.496070 22173 net.cpp:157] Top shape: 170 (170)
I0520 15:29:30.496079 22173 net.cpp:165] Memory required for data: 4320040
I0520 15:29:30.496089 22173 layer_factory.hpp:77] Creating layer conv1
I0520 15:29:30.496111 22173 net.cpp:106] Creating Layer conv1
I0520 15:29:30.496121 22173 net.cpp:454] conv1 <- data
I0520 15:29:30.496136 22173 net.cpp:411] conv1 -> conv1
I0520 15:29:30.498088 22173 net.cpp:150] Setting up conv1
I0520 15:29:30.498108 22173 net.cpp:157] Top shape: 170 12 120 48 (11750400)
I0520 15:29:30.498118 22173 net.cpp:165] Memory required for data: 51321640
I0520 15:29:30.498138 22173 layer_factory.hpp:77] Creating layer relu1
I0520 15:29:30.498152 22173 net.cpp:106] Creating Layer relu1
I0520 15:29:30.498162 22173 net.cpp:454] relu1 <- conv1
I0520 15:29:30.498175 22173 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:29:30.498670 22173 net.cpp:150] Setting up relu1
I0520 15:29:30.498687 22173 net.cpp:157] Top shape: 170 12 120 48 (11750400)
I0520 15:29:30.498697 22173 net.cpp:165] Memory required for data: 98323240
I0520 15:29:30.498708 22173 layer_factory.hpp:77] Creating layer pool1
I0520 15:29:30.498723 22173 net.cpp:106] Creating Layer pool1
I0520 15:29:30.498733 22173 net.cpp:454] pool1 <- conv1
I0520 15:29:30.498745 22173 net.cpp:411] pool1 -> pool1
I0520 15:29:30.498821 22173 net.cpp:150] Setting up pool1
I0520 15:29:30.498834 22173 net.cpp:157] Top shape: 170 12 60 48 (5875200)
I0520 15:29:30.498844 22173 net.cpp:165] Memory required for data: 121824040
I0520 15:29:30.498854 22173 layer_factory.hpp:77] Creating layer conv2
I0520 15:29:30.498869 22173 net.cpp:106] Creating Layer conv2
I0520 15:29:30.498880 22173 net.cpp:454] conv2 <- pool1
I0520 15:29:30.498894 22173 net.cpp:411] conv2 -> conv2
I0520 15:29:30.500810 22173 net.cpp:150] Setting up conv2
I0520 15:29:30.500833 22173 net.cpp:157] Top shape: 170 20 54 46 (8445600)
I0520 15:29:30.500845 22173 net.cpp:165] Memory required for data: 155606440
I0520 15:29:30.500864 22173 layer_factory.hpp:77] Creating layer relu2
I0520 15:29:30.500876 22173 net.cpp:106] Creating Layer relu2
I0520 15:29:30.500895 22173 net.cpp:454] relu2 <- conv2
I0520 15:29:30.500907 22173 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:29:30.501242 22173 net.cpp:150] Setting up relu2
I0520 15:29:30.501256 22173 net.cpp:157] Top shape: 170 20 54 46 (8445600)
I0520 15:29:30.501266 22173 net.cpp:165] Memory required for data: 189388840
I0520 15:29:30.501276 22173 layer_factory.hpp:77] Creating layer pool2
I0520 15:29:30.501289 22173 net.cpp:106] Creating Layer pool2
I0520 15:29:30.501299 22173 net.cpp:454] pool2 <- conv2
I0520 15:29:30.501312 22173 net.cpp:411] pool2 -> pool2
I0520 15:29:30.501384 22173 net.cpp:150] Setting up pool2
I0520 15:29:30.501397 22173 net.cpp:157] Top shape: 170 20 27 46 (4222800)
I0520 15:29:30.501406 22173 net.cpp:165] Memory required for data: 206280040
I0520 15:29:30.501417 22173 layer_factory.hpp:77] Creating layer conv3
I0520 15:29:30.501436 22173 net.cpp:106] Creating Layer conv3
I0520 15:29:30.501446 22173 net.cpp:454] conv3 <- pool2
I0520 15:29:30.501461 22173 net.cpp:411] conv3 -> conv3
I0520 15:29:30.503453 22173 net.cpp:150] Setting up conv3
I0520 15:29:30.503476 22173 net.cpp:157] Top shape: 170 28 22 44 (4607680)
I0520 15:29:30.503487 22173 net.cpp:165] Memory required for data: 224710760
I0520 15:29:30.503520 22173 layer_factory.hpp:77] Creating layer relu3
I0520 15:29:30.503535 22173 net.cpp:106] Creating Layer relu3
I0520 15:29:30.503545 22173 net.cpp:454] relu3 <- conv3
I0520 15:29:30.503557 22173 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:29:30.504027 22173 net.cpp:150] Setting up relu3
I0520 15:29:30.504043 22173 net.cpp:157] Top shape: 170 28 22 44 (4607680)
I0520 15:29:30.504055 22173 net.cpp:165] Memory required for data: 243141480
I0520 15:29:30.504065 22173 layer_factory.hpp:77] Creating layer pool3
I0520 15:29:30.504077 22173 net.cpp:106] Creating Layer pool3
I0520 15:29:30.504087 22173 net.cpp:454] pool3 <- conv3
I0520 15:29:30.504101 22173 net.cpp:411] pool3 -> pool3
I0520 15:29:30.504170 22173 net.cpp:150] Setting up pool3
I0520 15:29:30.504184 22173 net.cpp:157] Top shape: 170 28 11 44 (2303840)
I0520 15:29:30.504194 22173 net.cpp:165] Memory required for data: 252356840
I0520 15:29:30.504204 22173 layer_factory.hpp:77] Creating layer conv4
I0520 15:29:30.504218 22173 net.cpp:106] Creating Layer conv4
I0520 15:29:30.504228 22173 net.cpp:454] conv4 <- pool3
I0520 15:29:30.504243 22173 net.cpp:411] conv4 -> conv4
I0520 15:29:30.506330 22173 net.cpp:150] Setting up conv4
I0520 15:29:30.506351 22173 net.cpp:157] Top shape: 170 36 6 42 (1542240)
I0520 15:29:30.506364 22173 net.cpp:165] Memory required for data: 258525800
I0520 15:29:30.506379 22173 layer_factory.hpp:77] Creating layer relu4
I0520 15:29:30.506393 22173 net.cpp:106] Creating Layer relu4
I0520 15:29:30.506403 22173 net.cpp:454] relu4 <- conv4
I0520 15:29:30.506417 22173 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:29:30.506888 22173 net.cpp:150] Setting up relu4
I0520 15:29:30.506904 22173 net.cpp:157] Top shape: 170 36 6 42 (1542240)
I0520 15:29:30.506914 22173 net.cpp:165] Memory required for data: 264694760
I0520 15:29:30.506924 22173 layer_factory.hpp:77] Creating layer pool4
I0520 15:29:30.506937 22173 net.cpp:106] Creating Layer pool4
I0520 15:29:30.506947 22173 net.cpp:454] pool4 <- conv4
I0520 15:29:30.506960 22173 net.cpp:411] pool4 -> pool4
I0520 15:29:30.507031 22173 net.cpp:150] Setting up pool4
I0520 15:29:30.507045 22173 net.cpp:157] Top shape: 170 36 3 42 (771120)
I0520 15:29:30.507055 22173 net.cpp:165] Memory required for data: 267779240
I0520 15:29:30.507064 22173 layer_factory.hpp:77] Creating layer ip1
I0520 15:29:30.507078 22173 net.cpp:106] Creating Layer ip1
I0520 15:29:30.507088 22173 net.cpp:454] ip1 <- pool4
I0520 15:29:30.507102 22173 net.cpp:411] ip1 -> ip1
I0520 15:29:30.522596 22173 net.cpp:150] Setting up ip1
I0520 15:29:30.522624 22173 net.cpp:157] Top shape: 170 196 (33320)
I0520 15:29:30.522636 22173 net.cpp:165] Memory required for data: 267912520
I0520 15:29:30.522658 22173 layer_factory.hpp:77] Creating layer relu5
I0520 15:29:30.522673 22173 net.cpp:106] Creating Layer relu5
I0520 15:29:30.522685 22173 net.cpp:454] relu5 <- ip1
I0520 15:29:30.522697 22173 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:29:30.523046 22173 net.cpp:150] Setting up relu5
I0520 15:29:30.523061 22173 net.cpp:157] Top shape: 170 196 (33320)
I0520 15:29:30.523071 22173 net.cpp:165] Memory required for data: 268045800
I0520 15:29:30.523082 22173 layer_factory.hpp:77] Creating layer drop1
I0520 15:29:30.523100 22173 net.cpp:106] Creating Layer drop1
I0520 15:29:30.523110 22173 net.cpp:454] drop1 <- ip1
I0520 15:29:30.523123 22173 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:29:30.523169 22173 net.cpp:150] Setting up drop1
I0520 15:29:30.523181 22173 net.cpp:157] Top shape: 170 196 (33320)
I0520 15:29:30.523192 22173 net.cpp:165] Memory required for data: 268179080
I0520 15:29:30.523201 22173 layer_factory.hpp:77] Creating layer ip2
I0520 15:29:30.523216 22173 net.cpp:106] Creating Layer ip2
I0520 15:29:30.523226 22173 net.cpp:454] ip2 <- ip1
I0520 15:29:30.523239 22173 net.cpp:411] ip2 -> ip2
I0520 15:29:30.523715 22173 net.cpp:150] Setting up ip2
I0520 15:29:30.523730 22173 net.cpp:157] Top shape: 170 98 (16660)
I0520 15:29:30.523739 22173 net.cpp:165] Memory required for data: 268245720
I0520 15:29:30.523768 22173 layer_factory.hpp:77] Creating layer relu6
I0520 15:29:30.523782 22173 net.cpp:106] Creating Layer relu6
I0520 15:29:30.523792 22173 net.cpp:454] relu6 <- ip2
I0520 15:29:30.523803 22173 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:29:30.524345 22173 net.cpp:150] Setting up relu6
I0520 15:29:30.524365 22173 net.cpp:157] Top shape: 170 98 (16660)
I0520 15:29:30.524375 22173 net.cpp:165] Memory required for data: 268312360
I0520 15:29:30.524387 22173 layer_factory.hpp:77] Creating layer drop2
I0520 15:29:30.524401 22173 net.cpp:106] Creating Layer drop2
I0520 15:29:30.524410 22173 net.cpp:454] drop2 <- ip2
I0520 15:29:30.524423 22173 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:29:30.524467 22173 net.cpp:150] Setting up drop2
I0520 15:29:30.524480 22173 net.cpp:157] Top shape: 170 98 (16660)
I0520 15:29:30.524490 22173 net.cpp:165] Memory required for data: 268379000
I0520 15:29:30.524500 22173 layer_factory.hpp:77] Creating layer ip3
I0520 15:29:30.524514 22173 net.cpp:106] Creating Layer ip3
I0520 15:29:30.524523 22173 net.cpp:454] ip3 <- ip2
I0520 15:29:30.524538 22173 net.cpp:411] ip3 -> ip3
I0520 15:29:30.524761 22173 net.cpp:150] Setting up ip3
I0520 15:29:30.524775 22173 net.cpp:157] Top shape: 170 11 (1870)
I0520 15:29:30.524785 22173 net.cpp:165] Memory required for data: 268386480
I0520 15:29:30.524801 22173 layer_factory.hpp:77] Creating layer drop3
I0520 15:29:30.524813 22173 net.cpp:106] Creating Layer drop3
I0520 15:29:30.524823 22173 net.cpp:454] drop3 <- ip3
I0520 15:29:30.524837 22173 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:29:30.524878 22173 net.cpp:150] Setting up drop3
I0520 15:29:30.524899 22173 net.cpp:157] Top shape: 170 11 (1870)
I0520 15:29:30.524909 22173 net.cpp:165] Memory required for data: 268393960
I0520 15:29:30.524917 22173 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 15:29:30.524931 22173 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 15:29:30.524940 22173 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 15:29:30.524952 22173 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 15:29:30.524967 22173 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 15:29:30.525043 22173 net.cpp:150] Setting up ip3_drop3_0_split
I0520 15:29:30.525054 22173 net.cpp:157] Top shape: 170 11 (1870)
I0520 15:29:30.525068 22173 net.cpp:157] Top shape: 170 11 (1870)
I0520 15:29:30.525079 22173 net.cpp:165] Memory required for data: 268408920
I0520 15:29:30.525089 22173 layer_factory.hpp:77] Creating layer accuracy
I0520 15:29:30.525110 22173 net.cpp:106] Creating Layer accuracy
I0520 15:29:30.525120 22173 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 15:29:30.525130 22173 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 15:29:30.525144 22173 net.cpp:411] accuracy -> accuracy
I0520 15:29:30.525167 22173 net.cpp:150] Setting up accuracy
I0520 15:29:30.525179 22173 net.cpp:157] Top shape: (1)
I0520 15:29:30.525189 22173 net.cpp:165] Memory required for data: 268408924
I0520 15:29:30.525199 22173 layer_factory.hpp:77] Creating layer loss
I0520 15:29:30.525213 22173 net.cpp:106] Creating Layer loss
I0520 15:29:30.525223 22173 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 15:29:30.525233 22173 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 15:29:30.525248 22173 net.cpp:411] loss -> loss
I0520 15:29:30.525265 22173 layer_factory.hpp:77] Creating layer loss
I0520 15:29:30.525753 22173 net.cpp:150] Setting up loss
I0520 15:29:30.525766 22173 net.cpp:157] Top shape: (1)
I0520 15:29:30.525776 22173 net.cpp:160]     with loss weight 1
I0520 15:29:30.525797 22173 net.cpp:165] Memory required for data: 268408928
I0520 15:29:30.525807 22173 net.cpp:226] loss needs backward computation.
I0520 15:29:30.525818 22173 net.cpp:228] accuracy does not need backward computation.
I0520 15:29:30.525830 22173 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 15:29:30.525840 22173 net.cpp:226] drop3 needs backward computation.
I0520 15:29:30.525851 22173 net.cpp:226] ip3 needs backward computation.
I0520 15:29:30.525861 22173 net.cpp:226] drop2 needs backward computation.
I0520 15:29:30.525879 22173 net.cpp:226] relu6 needs backward computation.
I0520 15:29:30.525889 22173 net.cpp:226] ip2 needs backward computation.
I0520 15:29:30.525899 22173 net.cpp:226] drop1 needs backward computation.
I0520 15:29:30.525908 22173 net.cpp:226] relu5 needs backward computation.
I0520 15:29:30.525918 22173 net.cpp:226] ip1 needs backward computation.
I0520 15:29:30.525928 22173 net.cpp:226] pool4 needs backward computation.
I0520 15:29:30.525938 22173 net.cpp:226] relu4 needs backward computation.
I0520 15:29:30.525948 22173 net.cpp:226] conv4 needs backward computation.
I0520 15:29:30.525957 22173 net.cpp:226] pool3 needs backward computation.
I0520 15:29:30.525969 22173 net.cpp:226] relu3 needs backward computation.
I0520 15:29:30.525977 22173 net.cpp:226] conv3 needs backward computation.
I0520 15:29:30.525988 22173 net.cpp:226] pool2 needs backward computation.
I0520 15:29:30.525998 22173 net.cpp:226] relu2 needs backward computation.
I0520 15:29:30.526007 22173 net.cpp:226] conv2 needs backward computation.
I0520 15:29:30.526018 22173 net.cpp:226] pool1 needs backward computation.
I0520 15:29:30.526028 22173 net.cpp:226] relu1 needs backward computation.
I0520 15:29:30.526038 22173 net.cpp:226] conv1 needs backward computation.
I0520 15:29:30.526049 22173 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 15:29:30.526062 22173 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:29:30.526072 22173 net.cpp:270] This network produces output accuracy
I0520 15:29:30.526082 22173 net.cpp:270] This network produces output loss
I0520 15:29:30.526110 22173 net.cpp:283] Network initialization done.
I0520 15:29:30.526244 22173 solver.cpp:60] Solver scaffolding done.
I0520 15:29:30.527379 22173 caffe.cpp:212] Starting Optimization
I0520 15:29:30.527397 22173 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 15:29:30.527412 22173 solver.cpp:289] Learning Rate Policy: fixed
I0520 15:29:30.528478 22173 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 15:30:17.415659 22173 solver.cpp:409]     Test net output #0: accuracy = 0.114005
I0520 15:30:17.415824 22173 solver.cpp:409]     Test net output #1: loss = 2.39676 (* 1 = 2.39676 loss)
I0520 15:30:17.460413 22173 solver.cpp:237] Iteration 0, loss = 2.397
I0520 15:30:17.460449 22173 solver.cpp:253]     Train net output #0: loss = 2.397 (* 1 = 2.397 loss)
I0520 15:30:17.460467 22173 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 15:30:25.780594 22173 solver.cpp:237] Iteration 88, loss = 2.35084
I0520 15:30:25.780632 22173 solver.cpp:253]     Train net output #0: loss = 2.35084 (* 1 = 2.35084 loss)
I0520 15:30:25.780647 22173 sgd_solver.cpp:106] Iteration 88, lr = 0.0025
I0520 15:30:34.099426 22173 solver.cpp:237] Iteration 176, loss = 2.33321
I0520 15:30:34.099470 22173 solver.cpp:253]     Train net output #0: loss = 2.33321 (* 1 = 2.33321 loss)
I0520 15:30:34.099490 22173 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0520 15:30:42.390900 22173 solver.cpp:237] Iteration 264, loss = 2.33182
I0520 15:30:42.390931 22173 solver.cpp:253]     Train net output #0: loss = 2.33182 (* 1 = 2.33182 loss)
I0520 15:30:42.390944 22173 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0520 15:30:50.685808 22173 solver.cpp:237] Iteration 352, loss = 2.19975
I0520 15:30:50.685964 22173 solver.cpp:253]     Train net output #0: loss = 2.19975 (* 1 = 2.19975 loss)
I0520 15:30:50.685978 22173 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0520 15:30:58.984797 22173 solver.cpp:237] Iteration 440, loss = 2.15067
I0520 15:30:58.984841 22173 solver.cpp:253]     Train net output #0: loss = 2.15067 (* 1 = 2.15067 loss)
I0520 15:30:58.984859 22173 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0520 15:31:07.278100 22173 solver.cpp:237] Iteration 528, loss = 2.12855
I0520 15:31:07.278136 22173 solver.cpp:253]     Train net output #0: loss = 2.12855 (* 1 = 2.12855 loss)
I0520 15:31:07.278152 22173 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0520 15:31:37.678481 22173 solver.cpp:237] Iteration 616, loss = 2.06111
I0520 15:31:37.678658 22173 solver.cpp:253]     Train net output #0: loss = 2.06111 (* 1 = 2.06111 loss)
I0520 15:31:37.678673 22173 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0520 15:31:45.981037 22173 solver.cpp:237] Iteration 704, loss = 1.87236
I0520 15:31:45.981070 22173 solver.cpp:253]     Train net output #0: loss = 1.87236 (* 1 = 1.87236 loss)
I0520 15:31:45.981088 22173 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0520 15:31:54.296090 22173 solver.cpp:237] Iteration 792, loss = 1.86869
I0520 15:31:54.296128 22173 solver.cpp:253]     Train net output #0: loss = 1.86869 (* 1 = 1.86869 loss)
I0520 15:31:54.296147 22173 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0520 15:32:02.611135 22173 solver.cpp:237] Iteration 880, loss = 1.93493
I0520 15:32:02.611169 22173 solver.cpp:253]     Train net output #0: loss = 1.93493 (* 1 = 1.93493 loss)
I0520 15:32:02.611186 22173 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0520 15:32:02.705839 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_882.caffemodel
I0520 15:32:02.814453 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_882.solverstate
I0520 15:32:10.984841 22173 solver.cpp:237] Iteration 968, loss = 1.8821
I0520 15:32:10.985003 22173 solver.cpp:253]     Train net output #0: loss = 1.8821 (* 1 = 1.8821 loss)
I0520 15:32:10.985016 22173 sgd_solver.cpp:106] Iteration 968, lr = 0.0025
I0520 15:32:19.300993 22173 solver.cpp:237] Iteration 1056, loss = 1.9514
I0520 15:32:19.301038 22173 solver.cpp:253]     Train net output #0: loss = 1.9514 (* 1 = 1.9514 loss)
I0520 15:32:19.301053 22173 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0520 15:32:27.606307 22173 solver.cpp:237] Iteration 1144, loss = 1.65039
I0520 15:32:27.606340 22173 solver.cpp:253]     Train net output #0: loss = 1.65039 (* 1 = 1.65039 loss)
I0520 15:32:27.606356 22173 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0520 15:32:58.055644 22173 solver.cpp:237] Iteration 1232, loss = 1.83062
I0520 15:32:58.055802 22173 solver.cpp:253]     Train net output #0: loss = 1.83062 (* 1 = 1.83062 loss)
I0520 15:32:58.055816 22173 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0520 15:33:06.354974 22173 solver.cpp:237] Iteration 1320, loss = 1.86417
I0520 15:33:06.355015 22173 solver.cpp:253]     Train net output #0: loss = 1.86417 (* 1 = 1.86417 loss)
I0520 15:33:06.355032 22173 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0520 15:33:14.671195 22173 solver.cpp:237] Iteration 1408, loss = 1.76132
I0520 15:33:14.671231 22173 solver.cpp:253]     Train net output #0: loss = 1.76132 (* 1 = 1.76132 loss)
I0520 15:33:14.671247 22173 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0520 15:33:22.969344 22173 solver.cpp:237] Iteration 1496, loss = 1.8351
I0520 15:33:22.969378 22173 solver.cpp:253]     Train net output #0: loss = 1.8351 (* 1 = 1.8351 loss)
I0520 15:33:22.969391 22173 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 15:33:31.269625 22173 solver.cpp:237] Iteration 1584, loss = 1.69623
I0520 15:33:31.269793 22173 solver.cpp:253]     Train net output #0: loss = 1.69623 (* 1 = 1.69623 loss)
I0520 15:33:31.269807 22173 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0520 15:33:39.567906 22173 solver.cpp:237] Iteration 1672, loss = 1.71705
I0520 15:33:39.567940 22173 solver.cpp:253]     Train net output #0: loss = 1.71705 (* 1 = 1.71705 loss)
I0520 15:33:39.567957 22173 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0520 15:33:47.862577 22173 solver.cpp:237] Iteration 1760, loss = 1.71098
I0520 15:33:47.862612 22173 solver.cpp:253]     Train net output #0: loss = 1.71098 (* 1 = 1.71098 loss)
I0520 15:33:47.862628 22173 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0520 15:33:48.145957 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_1764.caffemodel
I0520 15:33:48.250578 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_1764.solverstate
I0520 15:33:48.277127 22173 solver.cpp:341] Iteration 1764, Testing net (#0)
I0520 15:34:34.232357 22173 solver.cpp:409]     Test net output #0: accuracy = 0.626685
I0520 15:34:34.232522 22173 solver.cpp:409]     Test net output #1: loss = 1.27086 (* 1 = 1.27086 loss)
I0520 15:35:04.284526 22173 solver.cpp:237] Iteration 1848, loss = 1.71654
I0520 15:35:04.284688 22173 solver.cpp:253]     Train net output #0: loss = 1.71654 (* 1 = 1.71654 loss)
I0520 15:35:04.284703 22173 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0520 15:35:12.587098 22173 solver.cpp:237] Iteration 1936, loss = 1.7224
I0520 15:35:12.587143 22173 solver.cpp:253]     Train net output #0: loss = 1.7224 (* 1 = 1.7224 loss)
I0520 15:35:12.587159 22173 sgd_solver.cpp:106] Iteration 1936, lr = 0.0025
I0520 15:35:20.881516 22173 solver.cpp:237] Iteration 2024, loss = 1.75267
I0520 15:35:20.881551 22173 solver.cpp:253]     Train net output #0: loss = 1.75267 (* 1 = 1.75267 loss)
I0520 15:35:20.881567 22173 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0520 15:35:29.188107 22173 solver.cpp:237] Iteration 2112, loss = 1.69956
I0520 15:35:29.188141 22173 solver.cpp:253]     Train net output #0: loss = 1.69956 (* 1 = 1.69956 loss)
I0520 15:35:29.188158 22173 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0520 15:35:37.486320 22173 solver.cpp:237] Iteration 2200, loss = 1.56239
I0520 15:35:37.486474 22173 solver.cpp:253]     Train net output #0: loss = 1.56239 (* 1 = 1.56239 loss)
I0520 15:35:37.486488 22173 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0520 15:35:45.782132 22173 solver.cpp:237] Iteration 2288, loss = 1.52872
I0520 15:35:45.782166 22173 solver.cpp:253]     Train net output #0: loss = 1.52872 (* 1 = 1.52872 loss)
I0520 15:35:45.782182 22173 sgd_solver.cpp:106] Iteration 2288, lr = 0.0025
I0520 15:36:16.279314 22173 solver.cpp:237] Iteration 2376, loss = 1.63763
I0520 15:36:16.279485 22173 solver.cpp:253]     Train net output #0: loss = 1.63763 (* 1 = 1.63763 loss)
I0520 15:36:16.279500 22173 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0520 15:36:24.606439 22173 solver.cpp:237] Iteration 2464, loss = 1.5366
I0520 15:36:24.606487 22173 solver.cpp:253]     Train net output #0: loss = 1.5366 (* 1 = 1.5366 loss)
I0520 15:36:24.606503 22173 sgd_solver.cpp:106] Iteration 2464, lr = 0.0025
I0520 15:36:32.922953 22173 solver.cpp:237] Iteration 2552, loss = 1.68817
I0520 15:36:32.922989 22173 solver.cpp:253]     Train net output #0: loss = 1.68817 (* 1 = 1.68817 loss)
I0520 15:36:32.923005 22173 sgd_solver.cpp:106] Iteration 2552, lr = 0.0025
I0520 15:36:41.224709 22173 solver.cpp:237] Iteration 2640, loss = 1.60048
I0520 15:36:41.224742 22173 solver.cpp:253]     Train net output #0: loss = 1.60048 (* 1 = 1.60048 loss)
I0520 15:36:41.224759 22173 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0520 15:36:41.696635 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_2646.caffemodel
I0520 15:36:41.803184 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_2646.solverstate
I0520 15:36:49.592247 22173 solver.cpp:237] Iteration 2728, loss = 1.5569
I0520 15:36:49.592422 22173 solver.cpp:253]     Train net output #0: loss = 1.5569 (* 1 = 1.5569 loss)
I0520 15:36:49.592435 22173 sgd_solver.cpp:106] Iteration 2728, lr = 0.0025
I0520 15:36:57.890084 22173 solver.cpp:237] Iteration 2816, loss = 1.70403
I0520 15:36:57.890118 22173 solver.cpp:253]     Train net output #0: loss = 1.70403 (* 1 = 1.70403 loss)
I0520 15:36:57.890132 22173 sgd_solver.cpp:106] Iteration 2816, lr = 0.0025
I0520 15:37:06.185716 22173 solver.cpp:237] Iteration 2904, loss = 1.59271
I0520 15:37:06.185751 22173 solver.cpp:253]     Train net output #0: loss = 1.59271 (* 1 = 1.59271 loss)
I0520 15:37:06.185765 22173 sgd_solver.cpp:106] Iteration 2904, lr = 0.0025
I0520 15:37:36.652495 22173 solver.cpp:237] Iteration 2992, loss = 1.56598
I0520 15:37:36.652662 22173 solver.cpp:253]     Train net output #0: loss = 1.56598 (* 1 = 1.56598 loss)
I0520 15:37:36.652678 22173 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0520 15:37:44.967644 22173 solver.cpp:237] Iteration 3080, loss = 1.47806
I0520 15:37:44.967687 22173 solver.cpp:253]     Train net output #0: loss = 1.47806 (* 1 = 1.47806 loss)
I0520 15:37:44.967708 22173 sgd_solver.cpp:106] Iteration 3080, lr = 0.0025
I0520 15:37:53.263244 22173 solver.cpp:237] Iteration 3168, loss = 1.64586
I0520 15:37:53.263279 22173 solver.cpp:253]     Train net output #0: loss = 1.64586 (* 1 = 1.64586 loss)
I0520 15:37:53.263293 22173 sgd_solver.cpp:106] Iteration 3168, lr = 0.0025
I0520 15:38:01.567373 22173 solver.cpp:237] Iteration 3256, loss = 1.6642
I0520 15:38:01.567409 22173 solver.cpp:253]     Train net output #0: loss = 1.6642 (* 1 = 1.6642 loss)
I0520 15:38:01.567421 22173 sgd_solver.cpp:106] Iteration 3256, lr = 0.0025
I0520 15:38:09.866765 22173 solver.cpp:237] Iteration 3344, loss = 1.53497
I0520 15:38:09.866914 22173 solver.cpp:253]     Train net output #0: loss = 1.53497 (* 1 = 1.53497 loss)
I0520 15:38:09.866927 22173 sgd_solver.cpp:106] Iteration 3344, lr = 0.0025
I0520 15:38:18.157712 22173 solver.cpp:237] Iteration 3432, loss = 1.6548
I0520 15:38:18.157747 22173 solver.cpp:253]     Train net output #0: loss = 1.6548 (* 1 = 1.6548 loss)
I0520 15:38:18.157763 22173 sgd_solver.cpp:106] Iteration 3432, lr = 0.0025
I0520 15:38:26.461364 22173 solver.cpp:237] Iteration 3520, loss = 1.49004
I0520 15:38:26.461398 22173 solver.cpp:253]     Train net output #0: loss = 1.49004 (* 1 = 1.49004 loss)
I0520 15:38:26.461415 22173 sgd_solver.cpp:106] Iteration 3520, lr = 0.0025
I0520 15:38:27.121665 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_3528.caffemodel
I0520 15:38:27.227016 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_3528.solverstate
I0520 15:38:27.255213 22173 solver.cpp:341] Iteration 3528, Testing net (#0)
I0520 15:39:34.010308 22173 solver.cpp:409]     Test net output #0: accuracy = 0.712692
I0520 15:39:34.010483 22173 solver.cpp:409]     Test net output #1: loss = 0.995665 (* 1 = 0.995665 loss)
I0520 15:40:03.769384 22173 solver.cpp:237] Iteration 3608, loss = 1.66601
I0520 15:40:03.769438 22173 solver.cpp:253]     Train net output #0: loss = 1.66601 (* 1 = 1.66601 loss)
I0520 15:40:03.769455 22173 sgd_solver.cpp:106] Iteration 3608, lr = 0.0025
I0520 15:40:12.065883 22173 solver.cpp:237] Iteration 3696, loss = 1.63778
I0520 15:40:12.066046 22173 solver.cpp:253]     Train net output #0: loss = 1.63778 (* 1 = 1.63778 loss)
I0520 15:40:12.066061 22173 sgd_solver.cpp:106] Iteration 3696, lr = 0.0025
I0520 15:40:20.375401 22173 solver.cpp:237] Iteration 3784, loss = 1.49579
I0520 15:40:20.375434 22173 solver.cpp:253]     Train net output #0: loss = 1.49579 (* 1 = 1.49579 loss)
I0520 15:40:20.375452 22173 sgd_solver.cpp:106] Iteration 3784, lr = 0.0025
I0520 15:40:28.680189 22173 solver.cpp:237] Iteration 3872, loss = 1.58015
I0520 15:40:28.680224 22173 solver.cpp:253]     Train net output #0: loss = 1.58015 (* 1 = 1.58015 loss)
I0520 15:40:28.680240 22173 sgd_solver.cpp:106] Iteration 3872, lr = 0.0025
I0520 15:40:36.983067 22173 solver.cpp:237] Iteration 3960, loss = 1.55515
I0520 15:40:36.983108 22173 solver.cpp:253]     Train net output #0: loss = 1.55515 (* 1 = 1.55515 loss)
I0520 15:40:36.983129 22173 sgd_solver.cpp:106] Iteration 3960, lr = 0.0025
I0520 15:40:45.292274 22173 solver.cpp:237] Iteration 4048, loss = 1.50015
I0520 15:40:45.292430 22173 solver.cpp:253]     Train net output #0: loss = 1.50015 (* 1 = 1.50015 loss)
I0520 15:40:45.292444 22173 sgd_solver.cpp:106] Iteration 4048, lr = 0.0025
I0520 15:41:15.759093 22173 solver.cpp:237] Iteration 4136, loss = 1.5345
I0520 15:41:15.759255 22173 solver.cpp:253]     Train net output #0: loss = 1.5345 (* 1 = 1.5345 loss)
I0520 15:41:15.759270 22173 sgd_solver.cpp:106] Iteration 4136, lr = 0.0025
I0520 15:41:24.062918 22173 solver.cpp:237] Iteration 4224, loss = 1.53791
I0520 15:41:24.062959 22173 solver.cpp:253]     Train net output #0: loss = 1.53791 (* 1 = 1.53791 loss)
I0520 15:41:24.062980 22173 sgd_solver.cpp:106] Iteration 4224, lr = 0.0025
I0520 15:41:32.360074 22173 solver.cpp:237] Iteration 4312, loss = 1.40473
I0520 15:41:32.360110 22173 solver.cpp:253]     Train net output #0: loss = 1.40473 (* 1 = 1.40473 loss)
I0520 15:41:32.360126 22173 sgd_solver.cpp:106] Iteration 4312, lr = 0.0025
I0520 15:41:40.644359 22173 solver.cpp:237] Iteration 4400, loss = 1.45951
I0520 15:41:40.644393 22173 solver.cpp:253]     Train net output #0: loss = 1.45951 (* 1 = 1.45951 loss)
I0520 15:41:40.644409 22173 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0520 15:41:41.492269 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_4410.caffemodel
I0520 15:41:41.598824 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_4410.solverstate
I0520 15:41:49.019402 22173 solver.cpp:237] Iteration 4488, loss = 1.54765
I0520 15:41:49.019568 22173 solver.cpp:253]     Train net output #0: loss = 1.54765 (* 1 = 1.54765 loss)
I0520 15:41:49.019582 22173 sgd_solver.cpp:106] Iteration 4488, lr = 0.0025
I0520 15:41:57.323072 22173 solver.cpp:237] Iteration 4576, loss = 1.48675
I0520 15:41:57.323106 22173 solver.cpp:253]     Train net output #0: loss = 1.48675 (* 1 = 1.48675 loss)
I0520 15:41:57.323122 22173 sgd_solver.cpp:106] Iteration 4576, lr = 0.0025
I0520 15:42:05.620445 22173 solver.cpp:237] Iteration 4664, loss = 1.60471
I0520 15:42:05.620481 22173 solver.cpp:253]     Train net output #0: loss = 1.60471 (* 1 = 1.60471 loss)
I0520 15:42:05.620498 22173 sgd_solver.cpp:106] Iteration 4664, lr = 0.0025
I0520 15:42:36.042938 22173 solver.cpp:237] Iteration 4752, loss = 1.48519
I0520 15:42:36.043118 22173 solver.cpp:253]     Train net output #0: loss = 1.48519 (* 1 = 1.48519 loss)
I0520 15:42:36.043133 22173 sgd_solver.cpp:106] Iteration 4752, lr = 0.0025
I0520 15:42:44.343067 22173 solver.cpp:237] Iteration 4840, loss = 1.53503
I0520 15:42:44.343112 22173 solver.cpp:253]     Train net output #0: loss = 1.53503 (* 1 = 1.53503 loss)
I0520 15:42:44.343130 22173 sgd_solver.cpp:106] Iteration 4840, lr = 0.0025
I0520 15:42:52.662602 22173 solver.cpp:237] Iteration 4928, loss = 1.67693
I0520 15:42:52.662638 22173 solver.cpp:253]     Train net output #0: loss = 1.67693 (* 1 = 1.67693 loss)
I0520 15:42:52.662654 22173 sgd_solver.cpp:106] Iteration 4928, lr = 0.0025
I0520 15:43:00.964486 22173 solver.cpp:237] Iteration 5016, loss = 1.46676
I0520 15:43:00.964521 22173 solver.cpp:253]     Train net output #0: loss = 1.46676 (* 1 = 1.46676 loss)
I0520 15:43:00.964537 22173 sgd_solver.cpp:106] Iteration 5016, lr = 0.0025
I0520 15:43:09.283704 22173 solver.cpp:237] Iteration 5104, loss = 1.49902
I0520 15:43:09.283854 22173 solver.cpp:253]     Train net output #0: loss = 1.49902 (* 1 = 1.49902 loss)
I0520 15:43:09.283869 22173 sgd_solver.cpp:106] Iteration 5104, lr = 0.0025
I0520 15:43:17.577114 22173 solver.cpp:237] Iteration 5192, loss = 1.36782
I0520 15:43:17.577148 22173 solver.cpp:253]     Train net output #0: loss = 1.36782 (* 1 = 1.36782 loss)
I0520 15:43:17.577165 22173 sgd_solver.cpp:106] Iteration 5192, lr = 0.0025
I0520 15:43:25.879598 22173 solver.cpp:237] Iteration 5280, loss = 1.3273
I0520 15:43:25.879633 22173 solver.cpp:253]     Train net output #0: loss = 1.3273 (* 1 = 1.3273 loss)
I0520 15:43:25.879649 22173 sgd_solver.cpp:106] Iteration 5280, lr = 0.0025
I0520 15:43:26.914890 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_5292.caffemodel
I0520 15:43:27.018282 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_5292.solverstate
I0520 15:43:27.044688 22173 solver.cpp:341] Iteration 5292, Testing net (#0)
I0520 15:44:12.667584 22173 solver.cpp:409]     Test net output #0: accuracy = 0.787795
I0520 15:44:12.667762 22173 solver.cpp:409]     Test net output #1: loss = 0.749617 (* 1 = 0.749617 loss)
I0520 15:44:42.035800 22173 solver.cpp:237] Iteration 5368, loss = 1.40885
I0520 15:44:42.035856 22173 solver.cpp:253]     Train net output #0: loss = 1.40885 (* 1 = 1.40885 loss)
I0520 15:44:42.035871 22173 sgd_solver.cpp:106] Iteration 5368, lr = 0.0025
I0520 15:44:50.352838 22173 solver.cpp:237] Iteration 5456, loss = 1.45917
I0520 15:44:50.352994 22173 solver.cpp:253]     Train net output #0: loss = 1.45917 (* 1 = 1.45917 loss)
I0520 15:44:50.353008 22173 sgd_solver.cpp:106] Iteration 5456, lr = 0.0025
I0520 15:44:58.658494 22173 solver.cpp:237] Iteration 5544, loss = 1.4371
I0520 15:44:58.658529 22173 solver.cpp:253]     Train net output #0: loss = 1.4371 (* 1 = 1.4371 loss)
I0520 15:44:58.658545 22173 sgd_solver.cpp:106] Iteration 5544, lr = 0.0025
I0520 15:45:06.967458 22173 solver.cpp:237] Iteration 5632, loss = 1.44858
I0520 15:45:06.967491 22173 solver.cpp:253]     Train net output #0: loss = 1.44858 (* 1 = 1.44858 loss)
I0520 15:45:06.967507 22173 sgd_solver.cpp:106] Iteration 5632, lr = 0.0025
I0520 15:45:15.268662 22173 solver.cpp:237] Iteration 5720, loss = 1.2996
I0520 15:45:15.268709 22173 solver.cpp:253]     Train net output #0: loss = 1.2996 (* 1 = 1.2996 loss)
I0520 15:45:15.268725 22173 sgd_solver.cpp:106] Iteration 5720, lr = 0.0025
I0520 15:45:23.572720 22173 solver.cpp:237] Iteration 5808, loss = 1.39755
I0520 15:45:23.572870 22173 solver.cpp:253]     Train net output #0: loss = 1.39755 (* 1 = 1.39755 loss)
I0520 15:45:23.572890 22173 sgd_solver.cpp:106] Iteration 5808, lr = 0.0025
I0520 15:45:53.999243 22173 solver.cpp:237] Iteration 5896, loss = 1.29226
I0520 15:45:53.999418 22173 solver.cpp:253]     Train net output #0: loss = 1.29226 (* 1 = 1.29226 loss)
I0520 15:45:53.999433 22173 sgd_solver.cpp:106] Iteration 5896, lr = 0.0025
I0520 15:46:02.297814 22173 solver.cpp:237] Iteration 5984, loss = 1.54664
I0520 15:46:02.297857 22173 solver.cpp:253]     Train net output #0: loss = 1.54664 (* 1 = 1.54664 loss)
I0520 15:46:02.297875 22173 sgd_solver.cpp:106] Iteration 5984, lr = 0.0025
I0520 15:46:10.595299 22173 solver.cpp:237] Iteration 6072, loss = 1.30892
I0520 15:46:10.595335 22173 solver.cpp:253]     Train net output #0: loss = 1.30892 (* 1 = 1.30892 loss)
I0520 15:46:10.595350 22173 sgd_solver.cpp:106] Iteration 6072, lr = 0.0025
I0520 15:46:18.909600 22173 solver.cpp:237] Iteration 6160, loss = 1.45844
I0520 15:46:18.909634 22173 solver.cpp:253]     Train net output #0: loss = 1.45844 (* 1 = 1.45844 loss)
I0520 15:46:18.909649 22173 sgd_solver.cpp:106] Iteration 6160, lr = 0.0025
I0520 15:46:20.133594 22173 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_6174.caffemodel
I0520 15:46:20.242085 22173 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_170_2016-05-20T11.20.39.039113_iter_6174.solverstate
I0520 15:46:27.275250 22173 solver.cpp:237] Iteration 6248, loss = 1.52082
I0520 15:46:27.275419 22173 solver.cpp:253]     Train net output #0: loss = 1.52082 (* 1 = 1.52082 loss)
I0520 15:46:27.275434 22173 sgd_solver.cpp:106] Iteration 6248, lr = 0.0025
I0520 15:46:35.584704 22173 solver.cpp:237] Iteration 6336, loss = 1.457
I0520 15:46:35.584738 22173 solver.cpp:253]     Train net output #0: loss = 1.457 (* 1 = 1.457 loss)
I0520 15:46:35.584756 22173 sgd_solver.cpp:106] Iteration 6336, lr = 0.0025
aprun: Apid 11232917: Caught signal Terminated, sending to application
*** Aborted at 1463773598 (unix time) try "date -d @1463773598" if you are using GNU date ***
PC: @     0x2aaaaaaca834 ([vdso]+0x833)
*** SIGTERM (@0x569a) received by PID 22173 (TID 0x2aaac746f900) from PID 22170; stack trace: ***
    @     0x2aaab7c78850 (unknown)
    @     0x2aaaaaaca834 ([vdso]+0x833)
    @     0x2aaab82072d0 maybe_syscall_gettime_cpu
    @     0x2aaab82074b0 __GI_clock_gettime
    @     0x2aaab9898f3e (unknown)
    @     0x2aaab928ec5b (unknown)
    @     0x2aaab926d723 (unknown)
    @     0x2aaab92655e1 (unknown)
aprun: Apid 11232917: Caught signal Terminated, sending to application
    @     0x2aaab9266356 (unknown)
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
    @     0x2aaab91d5562 (unknown)
    @     0x2aaab91d56ba (unknown)
    @     0x2aaab91b8715 cuMemcpy
    @     0x2aaaaacf9e92 (unknown)
    @     0x2aaaaacde306 (unknown)
    @     0x2aaaaad00328 cudaMemcpy
    @           0x4d6a10 caffe::caffe_copy<>()
    @           0x626ea9 caffe::HDF5DataLayer<>::Forward_gpu()
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
    @           0x5ca109 caffe::Solver<>::Step()
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
    @           0x438669 (unknown)
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
aprun: Apid 11232917: Caught signal Terminated, sending to application
