2805211
I0520 14:36:42.849515 23205 caffe.cpp:184] Using GPUs 0
I0520 14:36:43.271469 23205 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1153
test_interval: 2307
base_lr: 0.0025
display: 115
max_iter: 11538
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1153
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715.prototxt"
I0520 14:36:43.273064 23205 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715.prototxt
I0520 14:36:43.296020 23205 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 14:36:43.296085 23205 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 14:36:43.296461 23205 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 130
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:36:43.296694 23205 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:36:43.296730 23205 net.cpp:106] Creating Layer data_hdf5
I0520 14:36:43.296746 23205 net.cpp:411] data_hdf5 -> data
I0520 14:36:43.296782 23205 net.cpp:411] data_hdf5 -> label
I0520 14:36:43.296824 23205 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 14:36:43.298068 23205 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 14:36:43.300247 23205 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 14:37:04.843755 23205 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 14:37:04.848973 23205 net.cpp:150] Setting up data_hdf5
I0520 14:37:04.849012 23205 net.cpp:157] Top shape: 130 1 127 50 (825500)
I0520 14:37:04.849030 23205 net.cpp:157] Top shape: 130 (130)
I0520 14:37:04.849043 23205 net.cpp:165] Memory required for data: 3302520
I0520 14:37:04.849076 23205 layer_factory.hpp:77] Creating layer conv1
I0520 14:37:04.849110 23205 net.cpp:106] Creating Layer conv1
I0520 14:37:04.849138 23205 net.cpp:454] conv1 <- data
I0520 14:37:04.849162 23205 net.cpp:411] conv1 -> conv1
I0520 14:37:05.249697 23205 net.cpp:150] Setting up conv1
I0520 14:37:05.249749 23205 net.cpp:157] Top shape: 130 12 120 48 (8985600)
I0520 14:37:05.249764 23205 net.cpp:165] Memory required for data: 39244920
I0520 14:37:05.249795 23205 layer_factory.hpp:77] Creating layer relu1
I0520 14:37:05.249817 23205 net.cpp:106] Creating Layer relu1
I0520 14:37:05.249837 23205 net.cpp:454] relu1 <- conv1
I0520 14:37:05.249853 23205 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:37:05.250407 23205 net.cpp:150] Setting up relu1
I0520 14:37:05.250432 23205 net.cpp:157] Top shape: 130 12 120 48 (8985600)
I0520 14:37:05.250445 23205 net.cpp:165] Memory required for data: 75187320
I0520 14:37:05.250461 23205 layer_factory.hpp:77] Creating layer pool1
I0520 14:37:05.250488 23205 net.cpp:106] Creating Layer pool1
I0520 14:37:05.250502 23205 net.cpp:454] pool1 <- conv1
I0520 14:37:05.250519 23205 net.cpp:411] pool1 -> pool1
I0520 14:37:05.250612 23205 net.cpp:150] Setting up pool1
I0520 14:37:05.250630 23205 net.cpp:157] Top shape: 130 12 60 48 (4492800)
I0520 14:37:05.250645 23205 net.cpp:165] Memory required for data: 93158520
I0520 14:37:05.250668 23205 layer_factory.hpp:77] Creating layer conv2
I0520 14:37:05.250691 23205 net.cpp:106] Creating Layer conv2
I0520 14:37:05.250705 23205 net.cpp:454] conv2 <- pool1
I0520 14:37:05.250723 23205 net.cpp:411] conv2 -> conv2
I0520 14:37:05.253481 23205 net.cpp:150] Setting up conv2
I0520 14:37:05.253512 23205 net.cpp:157] Top shape: 130 20 54 46 (6458400)
I0520 14:37:05.253528 23205 net.cpp:165] Memory required for data: 118992120
I0520 14:37:05.253556 23205 layer_factory.hpp:77] Creating layer relu2
I0520 14:37:05.253583 23205 net.cpp:106] Creating Layer relu2
I0520 14:37:05.253597 23205 net.cpp:454] relu2 <- conv2
I0520 14:37:05.253613 23205 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:37:05.253968 23205 net.cpp:150] Setting up relu2
I0520 14:37:05.253988 23205 net.cpp:157] Top shape: 130 20 54 46 (6458400)
I0520 14:37:05.254003 23205 net.cpp:165] Memory required for data: 144825720
I0520 14:37:05.254014 23205 layer_factory.hpp:77] Creating layer pool2
I0520 14:37:05.254040 23205 net.cpp:106] Creating Layer pool2
I0520 14:37:05.254053 23205 net.cpp:454] pool2 <- conv2
I0520 14:37:05.254089 23205 net.cpp:411] pool2 -> pool2
I0520 14:37:05.254173 23205 net.cpp:150] Setting up pool2
I0520 14:37:05.254194 23205 net.cpp:157] Top shape: 130 20 27 46 (3229200)
I0520 14:37:05.254209 23205 net.cpp:165] Memory required for data: 157742520
I0520 14:37:05.254220 23205 layer_factory.hpp:77] Creating layer conv3
I0520 14:37:05.254250 23205 net.cpp:106] Creating Layer conv3
I0520 14:37:05.254262 23205 net.cpp:454] conv3 <- pool2
I0520 14:37:05.254281 23205 net.cpp:411] conv3 -> conv3
I0520 14:37:05.256217 23205 net.cpp:150] Setting up conv3
I0520 14:37:05.256242 23205 net.cpp:157] Top shape: 130 28 22 44 (3523520)
I0520 14:37:05.256263 23205 net.cpp:165] Memory required for data: 171836600
I0520 14:37:05.256286 23205 layer_factory.hpp:77] Creating layer relu3
I0520 14:37:05.256309 23205 net.cpp:106] Creating Layer relu3
I0520 14:37:05.256331 23205 net.cpp:454] relu3 <- conv3
I0520 14:37:05.256347 23205 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:37:05.256840 23205 net.cpp:150] Setting up relu3
I0520 14:37:05.256865 23205 net.cpp:157] Top shape: 130 28 22 44 (3523520)
I0520 14:37:05.256878 23205 net.cpp:165] Memory required for data: 185930680
I0520 14:37:05.256894 23205 layer_factory.hpp:77] Creating layer pool3
I0520 14:37:05.256911 23205 net.cpp:106] Creating Layer pool3
I0520 14:37:05.256933 23205 net.cpp:454] pool3 <- conv3
I0520 14:37:05.256950 23205 net.cpp:411] pool3 -> pool3
I0520 14:37:05.257040 23205 net.cpp:150] Setting up pool3
I0520 14:37:05.257057 23205 net.cpp:157] Top shape: 130 28 11 44 (1761760)
I0520 14:37:05.257071 23205 net.cpp:165] Memory required for data: 192977720
I0520 14:37:05.257083 23205 layer_factory.hpp:77] Creating layer conv4
I0520 14:37:05.257105 23205 net.cpp:106] Creating Layer conv4
I0520 14:37:05.257118 23205 net.cpp:454] conv4 <- pool3
I0520 14:37:05.257141 23205 net.cpp:411] conv4 -> conv4
I0520 14:37:05.260103 23205 net.cpp:150] Setting up conv4
I0520 14:37:05.260135 23205 net.cpp:157] Top shape: 130 36 6 42 (1179360)
I0520 14:37:05.260151 23205 net.cpp:165] Memory required for data: 197695160
I0520 14:37:05.260170 23205 layer_factory.hpp:77] Creating layer relu4
I0520 14:37:05.260192 23205 net.cpp:106] Creating Layer relu4
I0520 14:37:05.260218 23205 net.cpp:454] relu4 <- conv4
I0520 14:37:05.260234 23205 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:37:05.260735 23205 net.cpp:150] Setting up relu4
I0520 14:37:05.260759 23205 net.cpp:157] Top shape: 130 36 6 42 (1179360)
I0520 14:37:05.260772 23205 net.cpp:165] Memory required for data: 202412600
I0520 14:37:05.260788 23205 layer_factory.hpp:77] Creating layer pool4
I0520 14:37:05.260812 23205 net.cpp:106] Creating Layer pool4
I0520 14:37:05.260825 23205 net.cpp:454] pool4 <- conv4
I0520 14:37:05.260841 23205 net.cpp:411] pool4 -> pool4
I0520 14:37:05.260929 23205 net.cpp:150] Setting up pool4
I0520 14:37:05.260946 23205 net.cpp:157] Top shape: 130 36 3 42 (589680)
I0520 14:37:05.260959 23205 net.cpp:165] Memory required for data: 204771320
I0520 14:37:05.260974 23205 layer_factory.hpp:77] Creating layer ip1
I0520 14:37:05.261001 23205 net.cpp:106] Creating Layer ip1
I0520 14:37:05.261015 23205 net.cpp:454] ip1 <- pool4
I0520 14:37:05.261031 23205 net.cpp:411] ip1 -> ip1
I0520 14:37:05.276481 23205 net.cpp:150] Setting up ip1
I0520 14:37:05.276511 23205 net.cpp:157] Top shape: 130 196 (25480)
I0520 14:37:05.276532 23205 net.cpp:165] Memory required for data: 204873240
I0520 14:37:05.276558 23205 layer_factory.hpp:77] Creating layer relu5
I0520 14:37:05.276579 23205 net.cpp:106] Creating Layer relu5
I0520 14:37:05.276604 23205 net.cpp:454] relu5 <- ip1
I0520 14:37:05.276621 23205 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:37:05.276983 23205 net.cpp:150] Setting up relu5
I0520 14:37:05.277003 23205 net.cpp:157] Top shape: 130 196 (25480)
I0520 14:37:05.277015 23205 net.cpp:165] Memory required for data: 204975160
I0520 14:37:05.277030 23205 layer_factory.hpp:77] Creating layer drop1
I0520 14:37:05.277061 23205 net.cpp:106] Creating Layer drop1
I0520 14:37:05.277076 23205 net.cpp:454] drop1 <- ip1
I0520 14:37:05.277104 23205 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:37:05.277164 23205 net.cpp:150] Setting up drop1
I0520 14:37:05.277186 23205 net.cpp:157] Top shape: 130 196 (25480)
I0520 14:37:05.277199 23205 net.cpp:165] Memory required for data: 205077080
I0520 14:37:05.277211 23205 layer_factory.hpp:77] Creating layer ip2
I0520 14:37:05.277242 23205 net.cpp:106] Creating Layer ip2
I0520 14:37:05.277256 23205 net.cpp:454] ip2 <- ip1
I0520 14:37:05.277271 23205 net.cpp:411] ip2 -> ip2
I0520 14:37:05.277760 23205 net.cpp:150] Setting up ip2
I0520 14:37:05.277779 23205 net.cpp:157] Top shape: 130 98 (12740)
I0520 14:37:05.277792 23205 net.cpp:165] Memory required for data: 205128040
I0520 14:37:05.277812 23205 layer_factory.hpp:77] Creating layer relu6
I0520 14:37:05.277834 23205 net.cpp:106] Creating Layer relu6
I0520 14:37:05.277848 23205 net.cpp:454] relu6 <- ip2
I0520 14:37:05.277863 23205 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:37:05.278408 23205 net.cpp:150] Setting up relu6
I0520 14:37:05.278430 23205 net.cpp:157] Top shape: 130 98 (12740)
I0520 14:37:05.278444 23205 net.cpp:165] Memory required for data: 205179000
I0520 14:37:05.278460 23205 layer_factory.hpp:77] Creating layer drop2
I0520 14:37:05.278483 23205 net.cpp:106] Creating Layer drop2
I0520 14:37:05.278496 23205 net.cpp:454] drop2 <- ip2
I0520 14:37:05.278512 23205 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:37:05.278568 23205 net.cpp:150] Setting up drop2
I0520 14:37:05.278584 23205 net.cpp:157] Top shape: 130 98 (12740)
I0520 14:37:05.278597 23205 net.cpp:165] Memory required for data: 205229960
I0520 14:37:05.278611 23205 layer_factory.hpp:77] Creating layer ip3
I0520 14:37:05.278627 23205 net.cpp:106] Creating Layer ip3
I0520 14:37:05.278642 23205 net.cpp:454] ip3 <- ip2
I0520 14:37:05.278663 23205 net.cpp:411] ip3 -> ip3
I0520 14:37:05.278888 23205 net.cpp:150] Setting up ip3
I0520 14:37:05.278908 23205 net.cpp:157] Top shape: 130 11 (1430)
I0520 14:37:05.278920 23205 net.cpp:165] Memory required for data: 205235680
I0520 14:37:05.278940 23205 layer_factory.hpp:77] Creating layer drop3
I0520 14:37:05.278962 23205 net.cpp:106] Creating Layer drop3
I0520 14:37:05.278975 23205 net.cpp:454] drop3 <- ip3
I0520 14:37:05.278990 23205 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:37:05.279036 23205 net.cpp:150] Setting up drop3
I0520 14:37:05.279058 23205 net.cpp:157] Top shape: 130 11 (1430)
I0520 14:37:05.279072 23205 net.cpp:165] Memory required for data: 205241400
I0520 14:37:05.279090 23205 layer_factory.hpp:77] Creating layer loss
I0520 14:37:05.279111 23205 net.cpp:106] Creating Layer loss
I0520 14:37:05.279127 23205 net.cpp:454] loss <- ip3
I0520 14:37:05.279140 23205 net.cpp:454] loss <- label
I0520 14:37:05.279162 23205 net.cpp:411] loss -> loss
I0520 14:37:05.279181 23205 layer_factory.hpp:77] Creating layer loss
I0520 14:37:05.279846 23205 net.cpp:150] Setting up loss
I0520 14:37:05.279867 23205 net.cpp:157] Top shape: (1)
I0520 14:37:05.279883 23205 net.cpp:160]     with loss weight 1
I0520 14:37:05.279942 23205 net.cpp:165] Memory required for data: 205241404
I0520 14:37:05.279958 23205 net.cpp:226] loss needs backward computation.
I0520 14:37:05.279971 23205 net.cpp:226] drop3 needs backward computation.
I0520 14:37:05.279984 23205 net.cpp:226] ip3 needs backward computation.
I0520 14:37:05.279999 23205 net.cpp:226] drop2 needs backward computation.
I0520 14:37:05.280009 23205 net.cpp:226] relu6 needs backward computation.
I0520 14:37:05.280025 23205 net.cpp:226] ip2 needs backward computation.
I0520 14:37:05.280045 23205 net.cpp:226] drop1 needs backward computation.
I0520 14:37:05.280057 23205 net.cpp:226] relu5 needs backward computation.
I0520 14:37:05.280069 23205 net.cpp:226] ip1 needs backward computation.
I0520 14:37:05.280086 23205 net.cpp:226] pool4 needs backward computation.
I0520 14:37:05.280098 23205 net.cpp:226] relu4 needs backward computation.
I0520 14:37:05.280110 23205 net.cpp:226] conv4 needs backward computation.
I0520 14:37:05.280123 23205 net.cpp:226] pool3 needs backward computation.
I0520 14:37:05.280145 23205 net.cpp:226] relu3 needs backward computation.
I0520 14:37:05.280167 23205 net.cpp:226] conv3 needs backward computation.
I0520 14:37:05.280181 23205 net.cpp:226] pool2 needs backward computation.
I0520 14:37:05.280195 23205 net.cpp:226] relu2 needs backward computation.
I0520 14:37:05.280207 23205 net.cpp:226] conv2 needs backward computation.
I0520 14:37:05.280220 23205 net.cpp:226] pool1 needs backward computation.
I0520 14:37:05.280236 23205 net.cpp:226] relu1 needs backward computation.
I0520 14:37:05.280254 23205 net.cpp:226] conv1 needs backward computation.
I0520 14:37:05.280268 23205 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:37:05.280282 23205 net.cpp:270] This network produces output loss
I0520 14:37:05.280308 23205 net.cpp:283] Network initialization done.
I0520 14:37:05.281946 23205 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715.prototxt
I0520 14:37:05.282026 23205 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 14:37:05.282407 23205 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 130
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:37:05.282627 23205 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:37:05.282649 23205 net.cpp:106] Creating Layer data_hdf5
I0520 14:37:05.282663 23205 net.cpp:411] data_hdf5 -> data
I0520 14:37:05.282686 23205 net.cpp:411] data_hdf5 -> label
I0520 14:37:05.282707 23205 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 14:37:05.283897 23205 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 14:37:26.598644 23205 net.cpp:150] Setting up data_hdf5
I0520 14:37:26.598815 23205 net.cpp:157] Top shape: 130 1 127 50 (825500)
I0520 14:37:26.598835 23205 net.cpp:157] Top shape: 130 (130)
I0520 14:37:26.598847 23205 net.cpp:165] Memory required for data: 3302520
I0520 14:37:26.598862 23205 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 14:37:26.598896 23205 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 14:37:26.598928 23205 net.cpp:454] label_data_hdf5_1_split <- label
I0520 14:37:26.598947 23205 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 14:37:26.598968 23205 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 14:37:26.599056 23205 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 14:37:26.599072 23205 net.cpp:157] Top shape: 130 (130)
I0520 14:37:26.599087 23205 net.cpp:157] Top shape: 130 (130)
I0520 14:37:26.599100 23205 net.cpp:165] Memory required for data: 3303560
I0520 14:37:26.599112 23205 layer_factory.hpp:77] Creating layer conv1
I0520 14:37:26.599143 23205 net.cpp:106] Creating Layer conv1
I0520 14:37:26.599158 23205 net.cpp:454] conv1 <- data
I0520 14:37:26.599174 23205 net.cpp:411] conv1 -> conv1
I0520 14:37:26.601145 23205 net.cpp:150] Setting up conv1
I0520 14:37:26.601171 23205 net.cpp:157] Top shape: 130 12 120 48 (8985600)
I0520 14:37:26.601184 23205 net.cpp:165] Memory required for data: 39245960
I0520 14:37:26.601212 23205 layer_factory.hpp:77] Creating layer relu1
I0520 14:37:26.601238 23205 net.cpp:106] Creating Layer relu1
I0520 14:37:26.601251 23205 net.cpp:454] relu1 <- conv1
I0520 14:37:26.601267 23205 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:37:26.601794 23205 net.cpp:150] Setting up relu1
I0520 14:37:26.601816 23205 net.cpp:157] Top shape: 130 12 120 48 (8985600)
I0520 14:37:26.601830 23205 net.cpp:165] Memory required for data: 75188360
I0520 14:37:26.601845 23205 layer_factory.hpp:77] Creating layer pool1
I0520 14:37:26.601871 23205 net.cpp:106] Creating Layer pool1
I0520 14:37:26.601886 23205 net.cpp:454] pool1 <- conv1
I0520 14:37:26.601900 23205 net.cpp:411] pool1 -> pool1
I0520 14:37:26.601990 23205 net.cpp:150] Setting up pool1
I0520 14:37:26.602007 23205 net.cpp:157] Top shape: 130 12 60 48 (4492800)
I0520 14:37:26.602022 23205 net.cpp:165] Memory required for data: 93159560
I0520 14:37:26.602041 23205 layer_factory.hpp:77] Creating layer conv2
I0520 14:37:26.602062 23205 net.cpp:106] Creating Layer conv2
I0520 14:37:26.602083 23205 net.cpp:454] conv2 <- pool1
I0520 14:37:26.602100 23205 net.cpp:411] conv2 -> conv2
I0520 14:37:26.604051 23205 net.cpp:150] Setting up conv2
I0520 14:37:26.604075 23205 net.cpp:157] Top shape: 130 20 54 46 (6458400)
I0520 14:37:26.604096 23205 net.cpp:165] Memory required for data: 118993160
I0520 14:37:26.604117 23205 layer_factory.hpp:77] Creating layer relu2
I0520 14:37:26.604137 23205 net.cpp:106] Creating Layer relu2
I0520 14:37:26.604158 23205 net.cpp:454] relu2 <- conv2
I0520 14:37:26.604176 23205 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:37:26.604527 23205 net.cpp:150] Setting up relu2
I0520 14:37:26.604547 23205 net.cpp:157] Top shape: 130 20 54 46 (6458400)
I0520 14:37:26.604559 23205 net.cpp:165] Memory required for data: 144826760
I0520 14:37:26.604571 23205 layer_factory.hpp:77] Creating layer pool2
I0520 14:37:26.604598 23205 net.cpp:106] Creating Layer pool2
I0520 14:37:26.604610 23205 net.cpp:454] pool2 <- conv2
I0520 14:37:26.604626 23205 net.cpp:411] pool2 -> pool2
I0520 14:37:26.604732 23205 net.cpp:150] Setting up pool2
I0520 14:37:26.604748 23205 net.cpp:157] Top shape: 130 20 27 46 (3229200)
I0520 14:37:26.604761 23205 net.cpp:165] Memory required for data: 157743560
I0520 14:37:26.604776 23205 layer_factory.hpp:77] Creating layer conv3
I0520 14:37:26.604799 23205 net.cpp:106] Creating Layer conv3
I0520 14:37:26.604818 23205 net.cpp:454] conv3 <- pool2
I0520 14:37:26.604835 23205 net.cpp:411] conv3 -> conv3
I0520 14:37:26.606930 23205 net.cpp:150] Setting up conv3
I0520 14:37:26.606955 23205 net.cpp:157] Top shape: 130 28 22 44 (3523520)
I0520 14:37:26.606976 23205 net.cpp:165] Memory required for data: 171837640
I0520 14:37:26.607014 23205 layer_factory.hpp:77] Creating layer relu3
I0520 14:37:26.607039 23205 net.cpp:106] Creating Layer relu3
I0520 14:37:26.607053 23205 net.cpp:454] relu3 <- conv3
I0520 14:37:26.607070 23205 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:37:26.607586 23205 net.cpp:150] Setting up relu3
I0520 14:37:26.607609 23205 net.cpp:157] Top shape: 130 28 22 44 (3523520)
I0520 14:37:26.607621 23205 net.cpp:165] Memory required for data: 185931720
I0520 14:37:26.607635 23205 layer_factory.hpp:77] Creating layer pool3
I0520 14:37:26.607653 23205 net.cpp:106] Creating Layer pool3
I0520 14:37:26.607676 23205 net.cpp:454] pool3 <- conv3
I0520 14:37:26.607692 23205 net.cpp:411] pool3 -> pool3
I0520 14:37:26.607780 23205 net.cpp:150] Setting up pool3
I0520 14:37:26.607802 23205 net.cpp:157] Top shape: 130 28 11 44 (1761760)
I0520 14:37:26.607815 23205 net.cpp:165] Memory required for data: 192978760
I0520 14:37:26.607827 23205 layer_factory.hpp:77] Creating layer conv4
I0520 14:37:26.607858 23205 net.cpp:106] Creating Layer conv4
I0520 14:37:26.607872 23205 net.cpp:454] conv4 <- pool3
I0520 14:37:26.607887 23205 net.cpp:411] conv4 -> conv4
I0520 14:37:26.610007 23205 net.cpp:150] Setting up conv4
I0520 14:37:26.610030 23205 net.cpp:157] Top shape: 130 36 6 42 (1179360)
I0520 14:37:26.610051 23205 net.cpp:165] Memory required for data: 197696200
I0520 14:37:26.610070 23205 layer_factory.hpp:77] Creating layer relu4
I0520 14:37:26.610090 23205 net.cpp:106] Creating Layer relu4
I0520 14:37:26.610110 23205 net.cpp:454] relu4 <- conv4
I0520 14:37:26.610128 23205 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:37:26.610627 23205 net.cpp:150] Setting up relu4
I0520 14:37:26.610651 23205 net.cpp:157] Top shape: 130 36 6 42 (1179360)
I0520 14:37:26.610663 23205 net.cpp:165] Memory required for data: 202413640
I0520 14:37:26.610680 23205 layer_factory.hpp:77] Creating layer pool4
I0520 14:37:26.610704 23205 net.cpp:106] Creating Layer pool4
I0520 14:37:26.610718 23205 net.cpp:454] pool4 <- conv4
I0520 14:37:26.610733 23205 net.cpp:411] pool4 -> pool4
I0520 14:37:26.610819 23205 net.cpp:150] Setting up pool4
I0520 14:37:26.610836 23205 net.cpp:157] Top shape: 130 36 3 42 (589680)
I0520 14:37:26.610852 23205 net.cpp:165] Memory required for data: 204772360
I0520 14:37:26.610863 23205 layer_factory.hpp:77] Creating layer ip1
I0520 14:37:26.610889 23205 net.cpp:106] Creating Layer ip1
I0520 14:37:26.610903 23205 net.cpp:454] ip1 <- pool4
I0520 14:37:26.610919 23205 net.cpp:411] ip1 -> ip1
I0520 14:37:26.626389 23205 net.cpp:150] Setting up ip1
I0520 14:37:26.626420 23205 net.cpp:157] Top shape: 130 196 (25480)
I0520 14:37:26.626442 23205 net.cpp:165] Memory required for data: 204874280
I0520 14:37:26.626468 23205 layer_factory.hpp:77] Creating layer relu5
I0520 14:37:26.626490 23205 net.cpp:106] Creating Layer relu5
I0520 14:37:26.626514 23205 net.cpp:454] relu5 <- ip1
I0520 14:37:26.626531 23205 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:37:26.626899 23205 net.cpp:150] Setting up relu5
I0520 14:37:26.626919 23205 net.cpp:157] Top shape: 130 196 (25480)
I0520 14:37:26.626934 23205 net.cpp:165] Memory required for data: 204976200
I0520 14:37:26.626950 23205 layer_factory.hpp:77] Creating layer drop1
I0520 14:37:26.626979 23205 net.cpp:106] Creating Layer drop1
I0520 14:37:26.626993 23205 net.cpp:454] drop1 <- ip1
I0520 14:37:26.627017 23205 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:37:26.627068 23205 net.cpp:150] Setting up drop1
I0520 14:37:26.627092 23205 net.cpp:157] Top shape: 130 196 (25480)
I0520 14:37:26.627104 23205 net.cpp:165] Memory required for data: 205078120
I0520 14:37:26.627118 23205 layer_factory.hpp:77] Creating layer ip2
I0520 14:37:26.627135 23205 net.cpp:106] Creating Layer ip2
I0520 14:37:26.627151 23205 net.cpp:454] ip2 <- ip1
I0520 14:37:26.627174 23205 net.cpp:411] ip2 -> ip2
I0520 14:37:26.627670 23205 net.cpp:150] Setting up ip2
I0520 14:37:26.627688 23205 net.cpp:157] Top shape: 130 98 (12740)
I0520 14:37:26.627701 23205 net.cpp:165] Memory required for data: 205129080
I0520 14:37:26.627740 23205 layer_factory.hpp:77] Creating layer relu6
I0520 14:37:26.627758 23205 net.cpp:106] Creating Layer relu6
I0520 14:37:26.627770 23205 net.cpp:454] relu6 <- ip2
I0520 14:37:26.627785 23205 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:37:26.628345 23205 net.cpp:150] Setting up relu6
I0520 14:37:26.628368 23205 net.cpp:157] Top shape: 130 98 (12740)
I0520 14:37:26.628381 23205 net.cpp:165] Memory required for data: 205180040
I0520 14:37:26.628393 23205 layer_factory.hpp:77] Creating layer drop2
I0520 14:37:26.628413 23205 net.cpp:106] Creating Layer drop2
I0520 14:37:26.628437 23205 net.cpp:454] drop2 <- ip2
I0520 14:37:26.628453 23205 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:37:26.628504 23205 net.cpp:150] Setting up drop2
I0520 14:37:26.628526 23205 net.cpp:157] Top shape: 130 98 (12740)
I0520 14:37:26.628538 23205 net.cpp:165] Memory required for data: 205231000
I0520 14:37:26.628557 23205 layer_factory.hpp:77] Creating layer ip3
I0520 14:37:26.628574 23205 net.cpp:106] Creating Layer ip3
I0520 14:37:26.628587 23205 net.cpp:454] ip3 <- ip2
I0520 14:37:26.628605 23205 net.cpp:411] ip3 -> ip3
I0520 14:37:26.628856 23205 net.cpp:150] Setting up ip3
I0520 14:37:26.628875 23205 net.cpp:157] Top shape: 130 11 (1430)
I0520 14:37:26.628888 23205 net.cpp:165] Memory required for data: 205236720
I0520 14:37:26.628906 23205 layer_factory.hpp:77] Creating layer drop3
I0520 14:37:26.628926 23205 net.cpp:106] Creating Layer drop3
I0520 14:37:26.628947 23205 net.cpp:454] drop3 <- ip3
I0520 14:37:26.628963 23205 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:37:26.629016 23205 net.cpp:150] Setting up drop3
I0520 14:37:26.629034 23205 net.cpp:157] Top shape: 130 11 (1430)
I0520 14:37:26.629053 23205 net.cpp:165] Memory required for data: 205242440
I0520 14:37:26.629066 23205 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 14:37:26.629082 23205 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 14:37:26.629096 23205 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 14:37:26.629111 23205 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 14:37:26.629132 23205 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 14:37:26.629218 23205 net.cpp:150] Setting up ip3_drop3_0_split
I0520 14:37:26.629243 23205 net.cpp:157] Top shape: 130 11 (1430)
I0520 14:37:26.629259 23205 net.cpp:157] Top shape: 130 11 (1430)
I0520 14:37:26.629271 23205 net.cpp:165] Memory required for data: 205253880
I0520 14:37:26.629283 23205 layer_factory.hpp:77] Creating layer accuracy
I0520 14:37:26.629308 23205 net.cpp:106] Creating Layer accuracy
I0520 14:37:26.629328 23205 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 14:37:26.629343 23205 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 14:37:26.629359 23205 net.cpp:411] accuracy -> accuracy
I0520 14:37:26.629386 23205 net.cpp:150] Setting up accuracy
I0520 14:37:26.629407 23205 net.cpp:157] Top shape: (1)
I0520 14:37:26.629420 23205 net.cpp:165] Memory required for data: 205253884
I0520 14:37:26.629432 23205 layer_factory.hpp:77] Creating layer loss
I0520 14:37:26.629449 23205 net.cpp:106] Creating Layer loss
I0520 14:37:26.629462 23205 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 14:37:26.629477 23205 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 14:37:26.629493 23205 net.cpp:411] loss -> loss
I0520 14:37:26.629519 23205 layer_factory.hpp:77] Creating layer loss
I0520 14:37:26.630038 23205 net.cpp:150] Setting up loss
I0520 14:37:26.630058 23205 net.cpp:157] Top shape: (1)
I0520 14:37:26.630069 23205 net.cpp:160]     with loss weight 1
I0520 14:37:26.630096 23205 net.cpp:165] Memory required for data: 205253888
I0520 14:37:26.630116 23205 net.cpp:226] loss needs backward computation.
I0520 14:37:26.630131 23205 net.cpp:228] accuracy does not need backward computation.
I0520 14:37:26.630148 23205 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 14:37:26.630162 23205 net.cpp:226] drop3 needs backward computation.
I0520 14:37:26.630173 23205 net.cpp:226] ip3 needs backward computation.
I0520 14:37:26.630189 23205 net.cpp:226] drop2 needs backward computation.
I0520 14:37:26.630216 23205 net.cpp:226] relu6 needs backward computation.
I0520 14:37:26.630229 23205 net.cpp:226] ip2 needs backward computation.
I0520 14:37:26.630245 23205 net.cpp:226] drop1 needs backward computation.
I0520 14:37:26.630259 23205 net.cpp:226] relu5 needs backward computation.
I0520 14:37:26.630270 23205 net.cpp:226] ip1 needs backward computation.
I0520 14:37:26.630285 23205 net.cpp:226] pool4 needs backward computation.
I0520 14:37:26.630298 23205 net.cpp:226] relu4 needs backward computation.
I0520 14:37:26.630317 23205 net.cpp:226] conv4 needs backward computation.
I0520 14:37:26.630331 23205 net.cpp:226] pool3 needs backward computation.
I0520 14:37:26.630344 23205 net.cpp:226] relu3 needs backward computation.
I0520 14:37:26.630357 23205 net.cpp:226] conv3 needs backward computation.
I0520 14:37:26.630369 23205 net.cpp:226] pool2 needs backward computation.
I0520 14:37:26.630381 23205 net.cpp:226] relu2 needs backward computation.
I0520 14:37:26.630396 23205 net.cpp:226] conv2 needs backward computation.
I0520 14:37:26.630415 23205 net.cpp:226] pool1 needs backward computation.
I0520 14:37:26.630429 23205 net.cpp:226] relu1 needs backward computation.
I0520 14:37:26.630441 23205 net.cpp:226] conv1 needs backward computation.
I0520 14:37:26.630455 23205 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 14:37:26.630470 23205 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:37:26.630481 23205 net.cpp:270] This network produces output accuracy
I0520 14:37:26.630496 23205 net.cpp:270] This network produces output loss
I0520 14:37:26.630527 23205 net.cpp:283] Network initialization done.
I0520 14:37:26.630661 23205 solver.cpp:60] Solver scaffolding done.
I0520 14:37:26.631808 23205 caffe.cpp:212] Starting Optimization
I0520 14:37:26.631824 23205 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 14:37:26.631839 23205 solver.cpp:289] Learning Rate Policy: fixed
I0520 14:37:26.632925 23205 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 14:38:14.004556 23205 solver.cpp:409]     Test net output #0: accuracy = 0.0826403
I0520 14:38:14.004734 23205 solver.cpp:409]     Test net output #1: loss = 2.39762 (* 1 = 2.39762 loss)
I0520 14:38:14.042546 23205 solver.cpp:237] Iteration 0, loss = 2.39993
I0520 14:38:14.042585 23205 solver.cpp:253]     Train net output #0: loss = 2.39993 (* 1 = 2.39993 loss)
I0520 14:38:14.042606 23205 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 14:38:22.566324 23205 solver.cpp:237] Iteration 115, loss = 2.28385
I0520 14:38:22.566382 23205 solver.cpp:253]     Train net output #0: loss = 2.28385 (* 1 = 2.28385 loss)
I0520 14:38:22.566409 23205 sgd_solver.cpp:106] Iteration 115, lr = 0.0025
I0520 14:38:31.092176 23205 solver.cpp:237] Iteration 230, loss = 2.2659
I0520 14:38:31.092213 23205 solver.cpp:253]     Train net output #0: loss = 2.2659 (* 1 = 2.2659 loss)
I0520 14:38:31.092232 23205 sgd_solver.cpp:106] Iteration 230, lr = 0.0025
I0520 14:38:39.611806 23205 solver.cpp:237] Iteration 345, loss = 2.15569
I0520 14:38:39.611843 23205 solver.cpp:253]     Train net output #0: loss = 2.15569 (* 1 = 2.15569 loss)
I0520 14:38:39.611866 23205 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0520 14:38:48.140208 23205 solver.cpp:237] Iteration 460, loss = 2.06159
I0520 14:38:48.140383 23205 solver.cpp:253]     Train net output #0: loss = 2.06159 (* 1 = 2.06159 loss)
I0520 14:38:48.140403 23205 sgd_solver.cpp:106] Iteration 460, lr = 0.0025
I0520 14:38:56.658701 23205 solver.cpp:237] Iteration 575, loss = 1.89111
I0520 14:38:56.658738 23205 solver.cpp:253]     Train net output #0: loss = 1.89111 (* 1 = 1.89111 loss)
I0520 14:38:56.658757 23205 sgd_solver.cpp:106] Iteration 575, lr = 0.0025
I0520 14:39:05.178026 23205 solver.cpp:237] Iteration 690, loss = 2.0233
I0520 14:39:05.178064 23205 solver.cpp:253]     Train net output #0: loss = 2.0233 (* 1 = 2.0233 loss)
I0520 14:39:05.178083 23205 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0520 14:39:35.869834 23205 solver.cpp:237] Iteration 805, loss = 1.89212
I0520 14:39:35.869994 23205 solver.cpp:253]     Train net output #0: loss = 1.89212 (* 1 = 1.89212 loss)
I0520 14:39:35.870012 23205 sgd_solver.cpp:106] Iteration 805, lr = 0.0025
I0520 14:39:44.396067 23205 solver.cpp:237] Iteration 920, loss = 1.85132
I0520 14:39:44.396103 23205 solver.cpp:253]     Train net output #0: loss = 1.85132 (* 1 = 1.85132 loss)
I0520 14:39:44.396127 23205 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0520 14:39:52.925526 23205 solver.cpp:237] Iteration 1035, loss = 1.79525
I0520 14:39:52.925562 23205 solver.cpp:253]     Train net output #0: loss = 1.79525 (* 1 = 1.79525 loss)
I0520 14:39:52.925586 23205 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0520 14:40:01.456919 23205 solver.cpp:237] Iteration 1150, loss = 1.90432
I0520 14:40:01.456974 23205 solver.cpp:253]     Train net output #0: loss = 1.90432 (* 1 = 1.90432 loss)
I0520 14:40:01.456991 23205 sgd_solver.cpp:106] Iteration 1150, lr = 0.0025
I0520 14:40:01.604884 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_1153.caffemodel
I0520 14:40:01.697903 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_1153.solverstate
I0520 14:40:10.047087 23205 solver.cpp:237] Iteration 1265, loss = 1.6957
I0520 14:40:10.047250 23205 solver.cpp:253]     Train net output #0: loss = 1.6957 (* 1 = 1.6957 loss)
I0520 14:40:10.047267 23205 sgd_solver.cpp:106] Iteration 1265, lr = 0.0025
I0520 14:40:18.574553 23205 solver.cpp:237] Iteration 1380, loss = 1.65449
I0520 14:40:18.574589 23205 solver.cpp:253]     Train net output #0: loss = 1.65449 (* 1 = 1.65449 loss)
I0520 14:40:18.574615 23205 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0520 14:40:27.100191 23205 solver.cpp:237] Iteration 1495, loss = 1.69186
I0520 14:40:27.100244 23205 solver.cpp:253]     Train net output #0: loss = 1.69186 (* 1 = 1.69186 loss)
I0520 14:40:27.100260 23205 sgd_solver.cpp:106] Iteration 1495, lr = 0.0025
I0520 14:40:57.737665 23205 solver.cpp:237] Iteration 1610, loss = 1.6949
I0520 14:40:57.737828 23205 solver.cpp:253]     Train net output #0: loss = 1.6949 (* 1 = 1.6949 loss)
I0520 14:40:57.737846 23205 sgd_solver.cpp:106] Iteration 1610, lr = 0.0025
I0520 14:41:06.262114 23205 solver.cpp:237] Iteration 1725, loss = 1.76172
I0520 14:41:06.262151 23205 solver.cpp:253]     Train net output #0: loss = 1.76172 (* 1 = 1.76172 loss)
I0520 14:41:06.262174 23205 sgd_solver.cpp:106] Iteration 1725, lr = 0.0025
I0520 14:41:14.788995 23205 solver.cpp:237] Iteration 1840, loss = 1.65295
I0520 14:41:14.789031 23205 solver.cpp:253]     Train net output #0: loss = 1.65295 (* 1 = 1.65295 loss)
I0520 14:41:14.789054 23205 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0520 14:41:23.323297 23205 solver.cpp:237] Iteration 1955, loss = 1.74705
I0520 14:41:23.323348 23205 solver.cpp:253]     Train net output #0: loss = 1.74705 (* 1 = 1.74705 loss)
I0520 14:41:23.323365 23205 sgd_solver.cpp:106] Iteration 1955, lr = 0.0025
I0520 14:41:31.846999 23205 solver.cpp:237] Iteration 2070, loss = 1.64822
I0520 14:41:31.847151 23205 solver.cpp:253]     Train net output #0: loss = 1.64822 (* 1 = 1.64822 loss)
I0520 14:41:31.847168 23205 sgd_solver.cpp:106] Iteration 2070, lr = 0.0025
I0520 14:41:40.365181 23205 solver.cpp:237] Iteration 2185, loss = 1.72939
I0520 14:41:40.365241 23205 solver.cpp:253]     Train net output #0: loss = 1.72939 (* 1 = 1.72939 loss)
I0520 14:41:40.365265 23205 sgd_solver.cpp:106] Iteration 2185, lr = 0.0025
I0520 14:41:48.891362 23205 solver.cpp:237] Iteration 2300, loss = 1.79715
I0520 14:41:48.891402 23205 solver.cpp:253]     Train net output #0: loss = 1.79715 (* 1 = 1.79715 loss)
I0520 14:41:48.891419 23205 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0520 14:41:49.262938 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_2306.caffemodel
I0520 14:41:49.353061 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_2306.solverstate
I0520 14:41:49.402817 23205 solver.cpp:341] Iteration 2307, Testing net (#0)
I0520 14:42:35.844442 23205 solver.cpp:409]     Test net output #0: accuracy = 0.664524
I0520 14:42:35.844619 23205 solver.cpp:409]     Test net output #1: loss = 1.12689 (* 1 = 1.12689 loss)
I0520 14:43:06.079385 23205 solver.cpp:237] Iteration 2415, loss = 1.66211
I0520 14:43:06.079551 23205 solver.cpp:253]     Train net output #0: loss = 1.66211 (* 1 = 1.66211 loss)
I0520 14:43:06.079568 23205 sgd_solver.cpp:106] Iteration 2415, lr = 0.0025
I0520 14:43:14.607529 23205 solver.cpp:237] Iteration 2530, loss = 1.71559
I0520 14:43:14.607565 23205 solver.cpp:253]     Train net output #0: loss = 1.71559 (* 1 = 1.71559 loss)
I0520 14:43:14.607589 23205 sgd_solver.cpp:106] Iteration 2530, lr = 0.0025
I0520 14:43:23.136636 23205 solver.cpp:237] Iteration 2645, loss = 1.56533
I0520 14:43:23.136698 23205 solver.cpp:253]     Train net output #0: loss = 1.56533 (* 1 = 1.56533 loss)
I0520 14:43:23.136721 23205 sgd_solver.cpp:106] Iteration 2645, lr = 0.0025
I0520 14:43:31.660537 23205 solver.cpp:237] Iteration 2760, loss = 1.82421
I0520 14:43:31.660573 23205 solver.cpp:253]     Train net output #0: loss = 1.82421 (* 1 = 1.82421 loss)
I0520 14:43:31.660596 23205 sgd_solver.cpp:106] Iteration 2760, lr = 0.0025
I0520 14:43:40.184581 23205 solver.cpp:237] Iteration 2875, loss = 1.65819
I0520 14:43:40.184727 23205 solver.cpp:253]     Train net output #0: loss = 1.65819 (* 1 = 1.65819 loss)
I0520 14:43:40.184744 23205 sgd_solver.cpp:106] Iteration 2875, lr = 0.0025
I0520 14:43:48.710793 23205 solver.cpp:237] Iteration 2990, loss = 1.4399
I0520 14:43:48.710847 23205 solver.cpp:253]     Train net output #0: loss = 1.4399 (* 1 = 1.4399 loss)
I0520 14:43:48.710865 23205 sgd_solver.cpp:106] Iteration 2990, lr = 0.0025
I0520 14:44:19.394003 23205 solver.cpp:237] Iteration 3105, loss = 1.50816
I0520 14:44:19.394182 23205 solver.cpp:253]     Train net output #0: loss = 1.50816 (* 1 = 1.50816 loss)
I0520 14:44:19.394201 23205 sgd_solver.cpp:106] Iteration 3105, lr = 0.0025
I0520 14:44:27.915148 23205 solver.cpp:237] Iteration 3220, loss = 1.58602
I0520 14:44:27.915185 23205 solver.cpp:253]     Train net output #0: loss = 1.58602 (* 1 = 1.58602 loss)
I0520 14:44:27.915210 23205 sgd_solver.cpp:106] Iteration 3220, lr = 0.0025
I0520 14:44:36.438508 23205 solver.cpp:237] Iteration 3335, loss = 1.6859
I0520 14:44:36.438565 23205 solver.cpp:253]     Train net output #0: loss = 1.6859 (* 1 = 1.6859 loss)
I0520 14:44:36.438591 23205 sgd_solver.cpp:106] Iteration 3335, lr = 0.0025
I0520 14:44:44.966523 23205 solver.cpp:237] Iteration 3450, loss = 1.64992
I0520 14:44:44.966560 23205 solver.cpp:253]     Train net output #0: loss = 1.64992 (* 1 = 1.64992 loss)
I0520 14:44:44.966583 23205 sgd_solver.cpp:106] Iteration 3450, lr = 0.0025
I0520 14:44:45.559922 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_3459.caffemodel
I0520 14:44:45.652711 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_3459.solverstate
I0520 14:44:53.558142 23205 solver.cpp:237] Iteration 3565, loss = 1.52994
I0520 14:44:53.558318 23205 solver.cpp:253]     Train net output #0: loss = 1.52994 (* 1 = 1.52994 loss)
I0520 14:44:53.558336 23205 sgd_solver.cpp:106] Iteration 3565, lr = 0.0025
I0520 14:45:02.086858 23205 solver.cpp:237] Iteration 3680, loss = 1.62751
I0520 14:45:02.086920 23205 solver.cpp:253]     Train net output #0: loss = 1.62751 (* 1 = 1.62751 loss)
I0520 14:45:02.086946 23205 sgd_solver.cpp:106] Iteration 3680, lr = 0.0025
I0520 14:45:10.606482 23205 solver.cpp:237] Iteration 3795, loss = 1.66461
I0520 14:45:10.606520 23205 solver.cpp:253]     Train net output #0: loss = 1.66461 (* 1 = 1.66461 loss)
I0520 14:45:10.606539 23205 sgd_solver.cpp:106] Iteration 3795, lr = 0.0025
I0520 14:45:41.303840 23205 solver.cpp:237] Iteration 3910, loss = 1.54989
I0520 14:45:41.304013 23205 solver.cpp:253]     Train net output #0: loss = 1.54989 (* 1 = 1.54989 loss)
I0520 14:45:41.304030 23205 sgd_solver.cpp:106] Iteration 3910, lr = 0.0025
I0520 14:45:49.826268 23205 solver.cpp:237] Iteration 4025, loss = 1.61815
I0520 14:45:49.826305 23205 solver.cpp:253]     Train net output #0: loss = 1.61815 (* 1 = 1.61815 loss)
I0520 14:45:49.826323 23205 sgd_solver.cpp:106] Iteration 4025, lr = 0.0025
I0520 14:45:58.350711 23205 solver.cpp:237] Iteration 4140, loss = 1.42379
I0520 14:45:58.350770 23205 solver.cpp:253]     Train net output #0: loss = 1.42379 (* 1 = 1.42379 loss)
I0520 14:45:58.350790 23205 sgd_solver.cpp:106] Iteration 4140, lr = 0.0025
I0520 14:46:06.877182 23205 solver.cpp:237] Iteration 4255, loss = 1.52906
I0520 14:46:06.877220 23205 solver.cpp:253]     Train net output #0: loss = 1.52906 (* 1 = 1.52906 loss)
I0520 14:46:06.877238 23205 sgd_solver.cpp:106] Iteration 4255, lr = 0.0025
I0520 14:46:15.404608 23205 solver.cpp:237] Iteration 4370, loss = 1.43333
I0520 14:46:15.404779 23205 solver.cpp:253]     Train net output #0: loss = 1.43333 (* 1 = 1.43333 loss)
I0520 14:46:15.404798 23205 sgd_solver.cpp:106] Iteration 4370, lr = 0.0025
I0520 14:46:23.929514 23205 solver.cpp:237] Iteration 4485, loss = 1.63014
I0520 14:46:23.929550 23205 solver.cpp:253]     Train net output #0: loss = 1.63014 (* 1 = 1.63014 loss)
I0520 14:46:23.929575 23205 sgd_solver.cpp:106] Iteration 4485, lr = 0.0025
I0520 14:46:32.451800 23205 solver.cpp:237] Iteration 4600, loss = 1.55746
I0520 14:46:32.451838 23205 solver.cpp:253]     Train net output #0: loss = 1.55746 (* 1 = 1.55746 loss)
I0520 14:46:32.451860 23205 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0520 14:46:33.267431 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_4612.caffemodel
I0520 14:46:33.359704 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_4612.solverstate
I0520 14:46:33.486268 23205 solver.cpp:341] Iteration 4614, Testing net (#0)
I0520 14:47:40.826500 23205 solver.cpp:409]     Test net output #0: accuracy = 0.74999
I0520 14:47:40.826676 23205 solver.cpp:409]     Test net output #1: loss = 0.910529 (* 1 = 0.910529 loss)
I0520 14:48:10.508708 23205 solver.cpp:237] Iteration 4715, loss = 1.50367
I0520 14:48:10.508764 23205 solver.cpp:253]     Train net output #0: loss = 1.50367 (* 1 = 1.50367 loss)
I0520 14:48:10.508792 23205 sgd_solver.cpp:106] Iteration 4715, lr = 0.0025
I0520 14:48:19.031951 23205 solver.cpp:237] Iteration 4830, loss = 1.4791
I0520 14:48:19.032099 23205 solver.cpp:253]     Train net output #0: loss = 1.4791 (* 1 = 1.4791 loss)
I0520 14:48:19.032115 23205 sgd_solver.cpp:106] Iteration 4830, lr = 0.0025
I0520 14:48:27.562676 23205 solver.cpp:237] Iteration 4945, loss = 1.42336
I0520 14:48:27.562732 23205 solver.cpp:253]     Train net output #0: loss = 1.42336 (* 1 = 1.42336 loss)
I0520 14:48:27.562758 23205 sgd_solver.cpp:106] Iteration 4945, lr = 0.0025
I0520 14:48:36.088907 23205 solver.cpp:237] Iteration 5060, loss = 1.41428
I0520 14:48:36.088943 23205 solver.cpp:253]     Train net output #0: loss = 1.41428 (* 1 = 1.41428 loss)
I0520 14:48:36.088968 23205 sgd_solver.cpp:106] Iteration 5060, lr = 0.0025
I0520 14:48:44.609488 23205 solver.cpp:237] Iteration 5175, loss = 1.52491
I0520 14:48:44.609524 23205 solver.cpp:253]     Train net output #0: loss = 1.52491 (* 1 = 1.52491 loss)
I0520 14:48:44.609547 23205 sgd_solver.cpp:106] Iteration 5175, lr = 0.0025
I0520 14:48:53.130198 23205 solver.cpp:237] Iteration 5290, loss = 1.46266
I0520 14:48:53.130359 23205 solver.cpp:253]     Train net output #0: loss = 1.46266 (* 1 = 1.46266 loss)
I0520 14:48:53.130378 23205 sgd_solver.cpp:106] Iteration 5290, lr = 0.0025
I0520 14:49:23.824692 23205 solver.cpp:237] Iteration 5405, loss = 1.51422
I0520 14:49:23.824861 23205 solver.cpp:253]     Train net output #0: loss = 1.51422 (* 1 = 1.51422 loss)
I0520 14:49:23.824878 23205 sgd_solver.cpp:106] Iteration 5405, lr = 0.0025
I0520 14:49:32.351632 23205 solver.cpp:237] Iteration 5520, loss = 1.43508
I0520 14:49:32.351668 23205 solver.cpp:253]     Train net output #0: loss = 1.43508 (* 1 = 1.43508 loss)
I0520 14:49:32.351687 23205 sgd_solver.cpp:106] Iteration 5520, lr = 0.0025
I0520 14:49:40.877311 23205 solver.cpp:237] Iteration 5635, loss = 1.40019
I0520 14:49:40.877365 23205 solver.cpp:253]     Train net output #0: loss = 1.40019 (* 1 = 1.40019 loss)
I0520 14:49:40.877383 23205 sgd_solver.cpp:106] Iteration 5635, lr = 0.0025
I0520 14:49:49.406263 23205 solver.cpp:237] Iteration 5750, loss = 1.48778
I0520 14:49:49.406299 23205 solver.cpp:253]     Train net output #0: loss = 1.48778 (* 1 = 1.48778 loss)
I0520 14:49:49.406323 23205 sgd_solver.cpp:106] Iteration 5750, lr = 0.0025
I0520 14:49:50.445961 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_5765.caffemodel
I0520 14:49:50.538166 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_5765.solverstate
I0520 14:49:57.999893 23205 solver.cpp:237] Iteration 5865, loss = 1.34625
I0520 14:49:58.000063 23205 solver.cpp:253]     Train net output #0: loss = 1.34625 (* 1 = 1.34625 loss)
I0520 14:49:58.000082 23205 sgd_solver.cpp:106] Iteration 5865, lr = 0.0025
I0520 14:50:06.522989 23205 solver.cpp:237] Iteration 5980, loss = 1.51604
I0520 14:50:06.523046 23205 solver.cpp:253]     Train net output #0: loss = 1.51604 (* 1 = 1.51604 loss)
I0520 14:50:06.523072 23205 sgd_solver.cpp:106] Iteration 5980, lr = 0.0025
I0520 14:50:15.049480 23205 solver.cpp:237] Iteration 6095, loss = 1.44263
I0520 14:50:15.049518 23205 solver.cpp:253]     Train net output #0: loss = 1.44263 (* 1 = 1.44263 loss)
I0520 14:50:15.049537 23205 sgd_solver.cpp:106] Iteration 6095, lr = 0.0025
I0520 14:50:45.743614 23205 solver.cpp:237] Iteration 6210, loss = 1.5814
I0520 14:50:45.743806 23205 solver.cpp:253]     Train net output #0: loss = 1.5814 (* 1 = 1.5814 loss)
I0520 14:50:45.743824 23205 sgd_solver.cpp:106] Iteration 6210, lr = 0.0025
I0520 14:50:54.266772 23205 solver.cpp:237] Iteration 6325, loss = 1.48355
I0520 14:50:54.266826 23205 solver.cpp:253]     Train net output #0: loss = 1.48355 (* 1 = 1.48355 loss)
I0520 14:50:54.266844 23205 sgd_solver.cpp:106] Iteration 6325, lr = 0.0025
I0520 14:51:02.794860 23205 solver.cpp:237] Iteration 6440, loss = 1.40658
I0520 14:51:02.794896 23205 solver.cpp:253]     Train net output #0: loss = 1.40658 (* 1 = 1.40658 loss)
I0520 14:51:02.794920 23205 sgd_solver.cpp:106] Iteration 6440, lr = 0.0025
I0520 14:51:11.318653 23205 solver.cpp:237] Iteration 6555, loss = 1.45648
I0520 14:51:11.318689 23205 solver.cpp:253]     Train net output #0: loss = 1.45648 (* 1 = 1.45648 loss)
I0520 14:51:11.318708 23205 sgd_solver.cpp:106] Iteration 6555, lr = 0.0025
I0520 14:51:19.845607 23205 solver.cpp:237] Iteration 6670, loss = 1.44479
I0520 14:51:19.845775 23205 solver.cpp:253]     Train net output #0: loss = 1.44479 (* 1 = 1.44479 loss)
I0520 14:51:19.845794 23205 sgd_solver.cpp:106] Iteration 6670, lr = 0.0025
I0520 14:51:28.373006 23205 solver.cpp:237] Iteration 6785, loss = 1.37318
I0520 14:51:28.373042 23205 solver.cpp:253]     Train net output #0: loss = 1.37318 (* 1 = 1.37318 loss)
I0520 14:51:28.373066 23205 sgd_solver.cpp:106] Iteration 6785, lr = 0.0025
I0520 14:51:36.898671 23205 solver.cpp:237] Iteration 6900, loss = 1.45609
I0520 14:51:36.898707 23205 solver.cpp:253]     Train net output #0: loss = 1.45609 (* 1 = 1.45609 loss)
I0520 14:51:36.898730 23205 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0520 14:51:38.158794 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_6918.caffemodel
I0520 14:51:38.248037 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_6918.solverstate
I0520 14:51:38.445921 23205 solver.cpp:341] Iteration 6921, Testing net (#0)
I0520 14:52:24.554728 23205 solver.cpp:409]     Test net output #0: accuracy = 0.79748
I0520 14:52:24.554898 23205 solver.cpp:409]     Test net output #1: loss = 0.727148 (* 1 = 0.727148 loss)
I0520 14:52:53.792522 23205 solver.cpp:237] Iteration 7015, loss = 1.34028
I0520 14:52:53.792583 23205 solver.cpp:253]     Train net output #0: loss = 1.34028 (* 1 = 1.34028 loss)
I0520 14:52:53.792601 23205 sgd_solver.cpp:106] Iteration 7015, lr = 0.0025
I0520 14:53:02.312158 23205 solver.cpp:237] Iteration 7130, loss = 1.33981
I0520 14:53:02.312330 23205 solver.cpp:253]     Train net output #0: loss = 1.33981 (* 1 = 1.33981 loss)
I0520 14:53:02.312350 23205 sgd_solver.cpp:106] Iteration 7130, lr = 0.0025
I0520 14:53:10.833578 23205 solver.cpp:237] Iteration 7245, loss = 1.46545
I0520 14:53:10.833616 23205 solver.cpp:253]     Train net output #0: loss = 1.46545 (* 1 = 1.46545 loss)
I0520 14:53:10.833634 23205 sgd_solver.cpp:106] Iteration 7245, lr = 0.0025
I0520 14:53:19.357729 23205 solver.cpp:237] Iteration 7360, loss = 1.52863
I0520 14:53:19.357766 23205 solver.cpp:253]     Train net output #0: loss = 1.52863 (* 1 = 1.52863 loss)
I0520 14:53:19.357789 23205 sgd_solver.cpp:106] Iteration 7360, lr = 0.0025
I0520 14:53:27.881124 23205 solver.cpp:237] Iteration 7475, loss = 1.43824
I0520 14:53:27.881177 23205 solver.cpp:253]     Train net output #0: loss = 1.43824 (* 1 = 1.43824 loss)
I0520 14:53:27.881203 23205 sgd_solver.cpp:106] Iteration 7475, lr = 0.0025
I0520 14:53:36.406399 23205 solver.cpp:237] Iteration 7590, loss = 1.27291
I0520 14:53:36.406556 23205 solver.cpp:253]     Train net output #0: loss = 1.27291 (* 1 = 1.27291 loss)
I0520 14:53:36.406574 23205 sgd_solver.cpp:106] Iteration 7590, lr = 0.0025
I0520 14:54:07.116242 23205 solver.cpp:237] Iteration 7705, loss = 1.69853
I0520 14:54:07.116420 23205 solver.cpp:253]     Train net output #0: loss = 1.69853 (* 1 = 1.69853 loss)
I0520 14:54:07.116438 23205 sgd_solver.cpp:106] Iteration 7705, lr = 0.0025
I0520 14:54:15.641517 23205 solver.cpp:237] Iteration 7820, loss = 1.35274
I0520 14:54:15.641571 23205 solver.cpp:253]     Train net output #0: loss = 1.35274 (* 1 = 1.35274 loss)
I0520 14:54:15.641587 23205 sgd_solver.cpp:106] Iteration 7820, lr = 0.0025
I0520 14:54:24.166987 23205 solver.cpp:237] Iteration 7935, loss = 1.34467
I0520 14:54:24.167026 23205 solver.cpp:253]     Train net output #0: loss = 1.34467 (* 1 = 1.34467 loss)
I0520 14:54:24.167044 23205 sgd_solver.cpp:106] Iteration 7935, lr = 0.0025
I0520 14:54:32.693363 23205 solver.cpp:237] Iteration 8050, loss = 1.31366
I0520 14:54:32.693399 23205 solver.cpp:253]     Train net output #0: loss = 1.31366 (* 1 = 1.31366 loss)
I0520 14:54:32.693419 23205 sgd_solver.cpp:106] Iteration 8050, lr = 0.0025
I0520 14:54:34.176151 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_8071.caffemodel
I0520 14:54:34.265456 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_8071.solverstate
I0520 14:54:41.285228 23205 solver.cpp:237] Iteration 8165, loss = 1.4386
I0520 14:54:41.285401 23205 solver.cpp:253]     Train net output #0: loss = 1.4386 (* 1 = 1.4386 loss)
I0520 14:54:41.285420 23205 sgd_solver.cpp:106] Iteration 8165, lr = 0.0025
I0520 14:54:49.805166 23205 solver.cpp:237] Iteration 8280, loss = 1.54702
I0520 14:54:49.805202 23205 solver.cpp:253]     Train net output #0: loss = 1.54702 (* 1 = 1.54702 loss)
I0520 14:54:49.805227 23205 sgd_solver.cpp:106] Iteration 8280, lr = 0.0025
I0520 14:54:58.329582 23205 solver.cpp:237] Iteration 8395, loss = 1.15163
I0520 14:54:58.329618 23205 solver.cpp:253]     Train net output #0: loss = 1.15163 (* 1 = 1.15163 loss)
I0520 14:54:58.329643 23205 sgd_solver.cpp:106] Iteration 8395, lr = 0.0025
I0520 14:55:29.012094 23205 solver.cpp:237] Iteration 8510, loss = 1.43231
I0520 14:55:29.012265 23205 solver.cpp:253]     Train net output #0: loss = 1.43231 (* 1 = 1.43231 loss)
I0520 14:55:29.012282 23205 sgd_solver.cpp:106] Iteration 8510, lr = 0.0025
I0520 14:55:37.533707 23205 solver.cpp:237] Iteration 8625, loss = 1.26276
I0520 14:55:37.533743 23205 solver.cpp:253]     Train net output #0: loss = 1.26276 (* 1 = 1.26276 loss)
I0520 14:55:37.533768 23205 sgd_solver.cpp:106] Iteration 8625, lr = 0.0025
I0520 14:55:46.057029 23205 solver.cpp:237] Iteration 8740, loss = 1.59289
I0520 14:55:46.057066 23205 solver.cpp:253]     Train net output #0: loss = 1.59289 (* 1 = 1.59289 loss)
I0520 14:55:46.057085 23205 sgd_solver.cpp:106] Iteration 8740, lr = 0.0025
I0520 14:55:54.584270 23205 solver.cpp:237] Iteration 8855, loss = 1.14437
I0520 14:55:54.584322 23205 solver.cpp:253]     Train net output #0: loss = 1.14437 (* 1 = 1.14437 loss)
I0520 14:55:54.584339 23205 sgd_solver.cpp:106] Iteration 8855, lr = 0.0025
I0520 14:56:03.119040 23205 solver.cpp:237] Iteration 8970, loss = 1.21828
I0520 14:56:03.119190 23205 solver.cpp:253]     Train net output #0: loss = 1.21828 (* 1 = 1.21828 loss)
I0520 14:56:03.119207 23205 sgd_solver.cpp:106] Iteration 8970, lr = 0.0025
I0520 14:56:11.639935 23205 solver.cpp:237] Iteration 9085, loss = 1.20146
I0520 14:56:11.639972 23205 solver.cpp:253]     Train net output #0: loss = 1.20146 (* 1 = 1.20146 loss)
I0520 14:56:11.639991 23205 sgd_solver.cpp:106] Iteration 9085, lr = 0.0025
I0520 14:56:20.168339 23205 solver.cpp:237] Iteration 9200, loss = 1.34033
I0520 14:56:20.168393 23205 solver.cpp:253]     Train net output #0: loss = 1.34033 (* 1 = 1.34033 loss)
I0520 14:56:20.168411 23205 sgd_solver.cpp:106] Iteration 9200, lr = 0.0025
I0520 14:56:21.874011 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_9224.caffemodel
I0520 14:56:21.963424 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_9224.solverstate
I0520 14:56:22.234799 23205 solver.cpp:341] Iteration 9228, Testing net (#0)
I0520 14:57:29.489063 23205 solver.cpp:409]     Test net output #0: accuracy = 0.809821
I0520 14:57:29.489255 23205 solver.cpp:409]     Test net output #1: loss = 0.615651 (* 1 = 0.615651 loss)
I0520 14:57:58.154767 23205 solver.cpp:237] Iteration 9315, loss = 1.56305
I0520 14:57:58.154829 23205 solver.cpp:253]     Train net output #0: loss = 1.56305 (* 1 = 1.56305 loss)
I0520 14:57:58.154856 23205 sgd_solver.cpp:106] Iteration 9315, lr = 0.0025
I0520 14:58:06.689973 23205 solver.cpp:237] Iteration 9430, loss = 1.43328
I0520 14:58:06.690132 23205 solver.cpp:253]     Train net output #0: loss = 1.43328 (* 1 = 1.43328 loss)
I0520 14:58:06.690150 23205 sgd_solver.cpp:106] Iteration 9430, lr = 0.0025
I0520 14:58:15.225082 23205 solver.cpp:237] Iteration 9545, loss = 1.47729
I0520 14:58:15.225118 23205 solver.cpp:253]     Train net output #0: loss = 1.47729 (* 1 = 1.47729 loss)
I0520 14:58:15.225142 23205 sgd_solver.cpp:106] Iteration 9545, lr = 0.0025
I0520 14:58:23.749621 23205 solver.cpp:237] Iteration 9660, loss = 1.47325
I0520 14:58:23.749676 23205 solver.cpp:253]     Train net output #0: loss = 1.47325 (* 1 = 1.47325 loss)
I0520 14:58:23.749693 23205 sgd_solver.cpp:106] Iteration 9660, lr = 0.0025
I0520 14:58:32.275404 23205 solver.cpp:237] Iteration 9775, loss = 1.3651
I0520 14:58:32.275447 23205 solver.cpp:253]     Train net output #0: loss = 1.3651 (* 1 = 1.3651 loss)
I0520 14:58:32.275465 23205 sgd_solver.cpp:106] Iteration 9775, lr = 0.0025
I0520 14:58:40.797416 23205 solver.cpp:237] Iteration 9890, loss = 1.29108
I0520 14:58:40.797567 23205 solver.cpp:253]     Train net output #0: loss = 1.29108 (* 1 = 1.29108 loss)
I0520 14:58:40.797583 23205 sgd_solver.cpp:106] Iteration 9890, lr = 0.0025
I0520 14:59:11.532270 23205 solver.cpp:237] Iteration 10005, loss = 1.3132
I0520 14:59:11.532444 23205 solver.cpp:253]     Train net output #0: loss = 1.3132 (* 1 = 1.3132 loss)
I0520 14:59:11.532462 23205 sgd_solver.cpp:106] Iteration 10005, lr = 0.0025
I0520 14:59:20.055377 23205 solver.cpp:237] Iteration 10120, loss = 1.25768
I0520 14:59:20.055414 23205 solver.cpp:253]     Train net output #0: loss = 1.25768 (* 1 = 1.25768 loss)
I0520 14:59:20.055439 23205 sgd_solver.cpp:106] Iteration 10120, lr = 0.0025
I0520 14:59:28.579717 23205 solver.cpp:237] Iteration 10235, loss = 1.41784
I0520 14:59:28.579754 23205 solver.cpp:253]     Train net output #0: loss = 1.41784 (* 1 = 1.41784 loss)
I0520 14:59:28.579778 23205 sgd_solver.cpp:106] Iteration 10235, lr = 0.0025
I0520 14:59:37.108906 23205 solver.cpp:237] Iteration 10350, loss = 1.50477
I0520 14:59:37.108963 23205 solver.cpp:253]     Train net output #0: loss = 1.50477 (* 1 = 1.50477 loss)
I0520 14:59:37.108979 23205 sgd_solver.cpp:106] Iteration 10350, lr = 0.0025
I0520 14:59:39.035459 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_10377.caffemodel
I0520 14:59:39.127004 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_10377.solverstate
I0520 14:59:45.701731 23205 solver.cpp:237] Iteration 10465, loss = 1.48079
I0520 14:59:45.701903 23205 solver.cpp:253]     Train net output #0: loss = 1.48079 (* 1 = 1.48079 loss)
I0520 14:59:45.701922 23205 sgd_solver.cpp:106] Iteration 10465, lr = 0.0025
I0520 14:59:54.231284 23205 solver.cpp:237] Iteration 10580, loss = 1.18652
I0520 14:59:54.231322 23205 solver.cpp:253]     Train net output #0: loss = 1.18652 (* 1 = 1.18652 loss)
I0520 14:59:54.231345 23205 sgd_solver.cpp:106] Iteration 10580, lr = 0.0025
I0520 15:00:02.760098 23205 solver.cpp:237] Iteration 10695, loss = 1.20042
I0520 15:00:02.760159 23205 solver.cpp:253]     Train net output #0: loss = 1.20042 (* 1 = 1.20042 loss)
I0520 15:00:02.760185 23205 sgd_solver.cpp:106] Iteration 10695, lr = 0.0025
I0520 15:00:33.481072 23205 solver.cpp:237] Iteration 10810, loss = 1.38775
I0520 15:00:33.481253 23205 solver.cpp:253]     Train net output #0: loss = 1.38775 (* 1 = 1.38775 loss)
I0520 15:00:33.481271 23205 sgd_solver.cpp:106] Iteration 10810, lr = 0.0025
I0520 15:00:42.005384 23205 solver.cpp:237] Iteration 10925, loss = 1.35523
I0520 15:00:42.005421 23205 solver.cpp:253]     Train net output #0: loss = 1.35523 (* 1 = 1.35523 loss)
I0520 15:00:42.005439 23205 sgd_solver.cpp:106] Iteration 10925, lr = 0.0025
I0520 15:00:50.532670 23205 solver.cpp:237] Iteration 11040, loss = 1.29903
I0520 15:00:50.532726 23205 solver.cpp:253]     Train net output #0: loss = 1.29903 (* 1 = 1.29903 loss)
I0520 15:00:50.532742 23205 sgd_solver.cpp:106] Iteration 11040, lr = 0.0025
I0520 15:00:59.059003 23205 solver.cpp:237] Iteration 11155, loss = 1.3043
I0520 15:00:59.059041 23205 solver.cpp:253]     Train net output #0: loss = 1.3043 (* 1 = 1.3043 loss)
I0520 15:00:59.059065 23205 sgd_solver.cpp:106] Iteration 11155, lr = 0.0025
I0520 15:01:07.577595 23205 solver.cpp:237] Iteration 11270, loss = 1.27201
I0520 15:01:07.577749 23205 solver.cpp:253]     Train net output #0: loss = 1.27201 (* 1 = 1.27201 loss)
I0520 15:01:07.577765 23205 sgd_solver.cpp:106] Iteration 11270, lr = 0.0025
I0520 15:01:16.101480 23205 solver.cpp:237] Iteration 11385, loss = 1.29361
I0520 15:01:16.101536 23205 solver.cpp:253]     Train net output #0: loss = 1.29361 (* 1 = 1.29361 loss)
I0520 15:01:16.101562 23205 sgd_solver.cpp:106] Iteration 11385, lr = 0.0025
I0520 15:01:24.633036 23205 solver.cpp:237] Iteration 11500, loss = 1.17022
I0520 15:01:24.633074 23205 solver.cpp:253]     Train net output #0: loss = 1.17022 (* 1 = 1.17022 loss)
I0520 15:01:24.633097 23205 sgd_solver.cpp:106] Iteration 11500, lr = 0.0025
I0520 15:01:26.783448 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_11530.caffemodel
I0520 15:01:26.875926 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_11530.solverstate
I0520 15:01:27.224236 23205 solver.cpp:341] Iteration 11535, Testing net (#0)
I0520 15:02:13.669590 23205 solver.cpp:409]     Test net output #0: accuracy = 0.82748
I0520 15:02:13.669762 23205 solver.cpp:409]     Test net output #1: loss = 0.59356 (* 1 = 0.59356 loss)
I0520 15:02:13.840309 23205 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_11538.caffemodel
I0520 15:02:13.931815 23205 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715_iter_11538.solverstate
I0520 15:02:13.959286 23205 solver.cpp:326] Optimization Done.
I0520 15:02:13.959323 23205 caffe.cpp:215] Optimization Done.
Application 11232609 resources: utime ~1303s, stime ~230s, Rss ~5329372, inblocks ~3594474, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_130_2016-05-20T11.20.37.405715.solver"
	User time (seconds): 0.53
	System time (seconds): 0.17
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:37.52
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15078
	Voluntary context switches: 2786
	Involuntary context switches: 73
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
