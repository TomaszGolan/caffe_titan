2806368
I0521 08:40:23.682041 27336 caffe.cpp:184] Using GPUs 0
I0521 08:40:24.107386 27336 solver.cpp:48] Initializing solver from parameters: 
test_iter: 174
test_interval: 348
base_lr: 0.0025
display: 17
max_iter: 1744
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 174
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492.prototxt"
I0521 08:40:24.108935 27336 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492.prototxt
I0521 08:40:24.122004 27336 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 08:40:24.122063 27336 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 08:40:24.122406 27336 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 860
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 08:40:24.122586 27336 layer_factory.hpp:77] Creating layer data_hdf5
I0521 08:40:24.122609 27336 net.cpp:106] Creating Layer data_hdf5
I0521 08:40:24.122623 27336 net.cpp:411] data_hdf5 -> data
I0521 08:40:24.122658 27336 net.cpp:411] data_hdf5 -> label
I0521 08:40:24.122690 27336 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 08:40:24.123906 27336 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 08:40:24.126102 27336 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 08:40:45.674990 27336 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 08:40:45.680115 27336 net.cpp:150] Setting up data_hdf5
I0521 08:40:45.680160 27336 net.cpp:157] Top shape: 860 1 127 50 (5461000)
I0521 08:40:45.680176 27336 net.cpp:157] Top shape: 860 (860)
I0521 08:40:45.680186 27336 net.cpp:165] Memory required for data: 21847440
I0521 08:40:45.680198 27336 layer_factory.hpp:77] Creating layer conv1
I0521 08:40:45.680233 27336 net.cpp:106] Creating Layer conv1
I0521 08:40:45.680244 27336 net.cpp:454] conv1 <- data
I0521 08:40:45.680268 27336 net.cpp:411] conv1 -> conv1
I0521 08:40:46.040122 27336 net.cpp:150] Setting up conv1
I0521 08:40:46.040171 27336 net.cpp:157] Top shape: 860 12 120 48 (59443200)
I0521 08:40:46.040182 27336 net.cpp:165] Memory required for data: 259620240
I0521 08:40:46.040210 27336 layer_factory.hpp:77] Creating layer relu1
I0521 08:40:46.040232 27336 net.cpp:106] Creating Layer relu1
I0521 08:40:46.040243 27336 net.cpp:454] relu1 <- conv1
I0521 08:40:46.040257 27336 net.cpp:397] relu1 -> conv1 (in-place)
I0521 08:40:46.040777 27336 net.cpp:150] Setting up relu1
I0521 08:40:46.040794 27336 net.cpp:157] Top shape: 860 12 120 48 (59443200)
I0521 08:40:46.040805 27336 net.cpp:165] Memory required for data: 497393040
I0521 08:40:46.040815 27336 layer_factory.hpp:77] Creating layer pool1
I0521 08:40:46.040832 27336 net.cpp:106] Creating Layer pool1
I0521 08:40:46.040841 27336 net.cpp:454] pool1 <- conv1
I0521 08:40:46.040854 27336 net.cpp:411] pool1 -> pool1
I0521 08:40:46.040935 27336 net.cpp:150] Setting up pool1
I0521 08:40:46.040951 27336 net.cpp:157] Top shape: 860 12 60 48 (29721600)
I0521 08:40:46.040961 27336 net.cpp:165] Memory required for data: 616279440
I0521 08:40:46.040971 27336 layer_factory.hpp:77] Creating layer conv2
I0521 08:40:46.040993 27336 net.cpp:106] Creating Layer conv2
I0521 08:40:46.041004 27336 net.cpp:454] conv2 <- pool1
I0521 08:40:46.041018 27336 net.cpp:411] conv2 -> conv2
I0521 08:40:46.043689 27336 net.cpp:150] Setting up conv2
I0521 08:40:46.043718 27336 net.cpp:157] Top shape: 860 20 54 46 (42724800)
I0521 08:40:46.043728 27336 net.cpp:165] Memory required for data: 787178640
I0521 08:40:46.043748 27336 layer_factory.hpp:77] Creating layer relu2
I0521 08:40:46.043762 27336 net.cpp:106] Creating Layer relu2
I0521 08:40:46.043772 27336 net.cpp:454] relu2 <- conv2
I0521 08:40:46.043786 27336 net.cpp:397] relu2 -> conv2 (in-place)
I0521 08:40:46.044126 27336 net.cpp:150] Setting up relu2
I0521 08:40:46.044139 27336 net.cpp:157] Top shape: 860 20 54 46 (42724800)
I0521 08:40:46.044150 27336 net.cpp:165] Memory required for data: 958077840
I0521 08:40:46.044160 27336 layer_factory.hpp:77] Creating layer pool2
I0521 08:40:46.044173 27336 net.cpp:106] Creating Layer pool2
I0521 08:40:46.044183 27336 net.cpp:454] pool2 <- conv2
I0521 08:40:46.044210 27336 net.cpp:411] pool2 -> pool2
I0521 08:40:46.044280 27336 net.cpp:150] Setting up pool2
I0521 08:40:46.044292 27336 net.cpp:157] Top shape: 860 20 27 46 (21362400)
I0521 08:40:46.044303 27336 net.cpp:165] Memory required for data: 1043527440
I0521 08:40:46.044311 27336 layer_factory.hpp:77] Creating layer conv3
I0521 08:40:46.044329 27336 net.cpp:106] Creating Layer conv3
I0521 08:40:46.044340 27336 net.cpp:454] conv3 <- pool2
I0521 08:40:46.044354 27336 net.cpp:411] conv3 -> conv3
I0521 08:40:46.046283 27336 net.cpp:150] Setting up conv3
I0521 08:40:46.046300 27336 net.cpp:157] Top shape: 860 28 22 44 (23309440)
I0521 08:40:46.046313 27336 net.cpp:165] Memory required for data: 1136765200
I0521 08:40:46.046331 27336 layer_factory.hpp:77] Creating layer relu3
I0521 08:40:46.046347 27336 net.cpp:106] Creating Layer relu3
I0521 08:40:46.046357 27336 net.cpp:454] relu3 <- conv3
I0521 08:40:46.046370 27336 net.cpp:397] relu3 -> conv3 (in-place)
I0521 08:40:46.046840 27336 net.cpp:150] Setting up relu3
I0521 08:40:46.046857 27336 net.cpp:157] Top shape: 860 28 22 44 (23309440)
I0521 08:40:46.046867 27336 net.cpp:165] Memory required for data: 1230002960
I0521 08:40:46.046878 27336 layer_factory.hpp:77] Creating layer pool3
I0521 08:40:46.046891 27336 net.cpp:106] Creating Layer pool3
I0521 08:40:46.046900 27336 net.cpp:454] pool3 <- conv3
I0521 08:40:46.046913 27336 net.cpp:411] pool3 -> pool3
I0521 08:40:46.046980 27336 net.cpp:150] Setting up pool3
I0521 08:40:46.046993 27336 net.cpp:157] Top shape: 860 28 11 44 (11654720)
I0521 08:40:46.047003 27336 net.cpp:165] Memory required for data: 1276621840
I0521 08:40:46.047013 27336 layer_factory.hpp:77] Creating layer conv4
I0521 08:40:46.047029 27336 net.cpp:106] Creating Layer conv4
I0521 08:40:46.047039 27336 net.cpp:454] conv4 <- pool3
I0521 08:40:46.047052 27336 net.cpp:411] conv4 -> conv4
I0521 08:40:46.049768 27336 net.cpp:150] Setting up conv4
I0521 08:40:46.049795 27336 net.cpp:157] Top shape: 860 36 6 42 (7801920)
I0521 08:40:46.049806 27336 net.cpp:165] Memory required for data: 1307829520
I0521 08:40:46.049823 27336 layer_factory.hpp:77] Creating layer relu4
I0521 08:40:46.049836 27336 net.cpp:106] Creating Layer relu4
I0521 08:40:46.049846 27336 net.cpp:454] relu4 <- conv4
I0521 08:40:46.049860 27336 net.cpp:397] relu4 -> conv4 (in-place)
I0521 08:40:46.050323 27336 net.cpp:150] Setting up relu4
I0521 08:40:46.050339 27336 net.cpp:157] Top shape: 860 36 6 42 (7801920)
I0521 08:40:46.050349 27336 net.cpp:165] Memory required for data: 1339037200
I0521 08:40:46.050359 27336 layer_factory.hpp:77] Creating layer pool4
I0521 08:40:46.050372 27336 net.cpp:106] Creating Layer pool4
I0521 08:40:46.050382 27336 net.cpp:454] pool4 <- conv4
I0521 08:40:46.050395 27336 net.cpp:411] pool4 -> pool4
I0521 08:40:46.050463 27336 net.cpp:150] Setting up pool4
I0521 08:40:46.050477 27336 net.cpp:157] Top shape: 860 36 3 42 (3900960)
I0521 08:40:46.050488 27336 net.cpp:165] Memory required for data: 1354641040
I0521 08:40:46.050498 27336 layer_factory.hpp:77] Creating layer ip1
I0521 08:40:46.050518 27336 net.cpp:106] Creating Layer ip1
I0521 08:40:46.050529 27336 net.cpp:454] ip1 <- pool4
I0521 08:40:46.050541 27336 net.cpp:411] ip1 -> ip1
I0521 08:40:46.066018 27336 net.cpp:150] Setting up ip1
I0521 08:40:46.066042 27336 net.cpp:157] Top shape: 860 196 (168560)
I0521 08:40:46.066054 27336 net.cpp:165] Memory required for data: 1355315280
I0521 08:40:46.066076 27336 layer_factory.hpp:77] Creating layer relu5
I0521 08:40:46.066092 27336 net.cpp:106] Creating Layer relu5
I0521 08:40:46.066102 27336 net.cpp:454] relu5 <- ip1
I0521 08:40:46.066114 27336 net.cpp:397] relu5 -> ip1 (in-place)
I0521 08:40:46.066457 27336 net.cpp:150] Setting up relu5
I0521 08:40:46.066471 27336 net.cpp:157] Top shape: 860 196 (168560)
I0521 08:40:46.066483 27336 net.cpp:165] Memory required for data: 1355989520
I0521 08:40:46.066493 27336 layer_factory.hpp:77] Creating layer drop1
I0521 08:40:46.066514 27336 net.cpp:106] Creating Layer drop1
I0521 08:40:46.066524 27336 net.cpp:454] drop1 <- ip1
I0521 08:40:46.066548 27336 net.cpp:397] drop1 -> ip1 (in-place)
I0521 08:40:46.066596 27336 net.cpp:150] Setting up drop1
I0521 08:40:46.066608 27336 net.cpp:157] Top shape: 860 196 (168560)
I0521 08:40:46.066618 27336 net.cpp:165] Memory required for data: 1356663760
I0521 08:40:46.066628 27336 layer_factory.hpp:77] Creating layer ip2
I0521 08:40:46.066647 27336 net.cpp:106] Creating Layer ip2
I0521 08:40:46.066658 27336 net.cpp:454] ip2 <- ip1
I0521 08:40:46.066670 27336 net.cpp:411] ip2 -> ip2
I0521 08:40:46.067137 27336 net.cpp:150] Setting up ip2
I0521 08:40:46.067149 27336 net.cpp:157] Top shape: 860 98 (84280)
I0521 08:40:46.067159 27336 net.cpp:165] Memory required for data: 1357000880
I0521 08:40:46.067174 27336 layer_factory.hpp:77] Creating layer relu6
I0521 08:40:46.067186 27336 net.cpp:106] Creating Layer relu6
I0521 08:40:46.067196 27336 net.cpp:454] relu6 <- ip2
I0521 08:40:46.067209 27336 net.cpp:397] relu6 -> ip2 (in-place)
I0521 08:40:46.067723 27336 net.cpp:150] Setting up relu6
I0521 08:40:46.067739 27336 net.cpp:157] Top shape: 860 98 (84280)
I0521 08:40:46.067750 27336 net.cpp:165] Memory required for data: 1357338000
I0521 08:40:46.067760 27336 layer_factory.hpp:77] Creating layer drop2
I0521 08:40:46.067773 27336 net.cpp:106] Creating Layer drop2
I0521 08:40:46.067783 27336 net.cpp:454] drop2 <- ip2
I0521 08:40:46.067795 27336 net.cpp:397] drop2 -> ip2 (in-place)
I0521 08:40:46.067837 27336 net.cpp:150] Setting up drop2
I0521 08:40:46.067850 27336 net.cpp:157] Top shape: 860 98 (84280)
I0521 08:40:46.067860 27336 net.cpp:165] Memory required for data: 1357675120
I0521 08:40:46.067870 27336 layer_factory.hpp:77] Creating layer ip3
I0521 08:40:46.067884 27336 net.cpp:106] Creating Layer ip3
I0521 08:40:46.067894 27336 net.cpp:454] ip3 <- ip2
I0521 08:40:46.067908 27336 net.cpp:411] ip3 -> ip3
I0521 08:40:46.068127 27336 net.cpp:150] Setting up ip3
I0521 08:40:46.068140 27336 net.cpp:157] Top shape: 860 11 (9460)
I0521 08:40:46.068150 27336 net.cpp:165] Memory required for data: 1357712960
I0521 08:40:46.068166 27336 layer_factory.hpp:77] Creating layer drop3
I0521 08:40:46.068178 27336 net.cpp:106] Creating Layer drop3
I0521 08:40:46.068188 27336 net.cpp:454] drop3 <- ip3
I0521 08:40:46.068199 27336 net.cpp:397] drop3 -> ip3 (in-place)
I0521 08:40:46.068238 27336 net.cpp:150] Setting up drop3
I0521 08:40:46.068251 27336 net.cpp:157] Top shape: 860 11 (9460)
I0521 08:40:46.068260 27336 net.cpp:165] Memory required for data: 1357750800
I0521 08:40:46.068271 27336 layer_factory.hpp:77] Creating layer loss
I0521 08:40:46.068289 27336 net.cpp:106] Creating Layer loss
I0521 08:40:46.068300 27336 net.cpp:454] loss <- ip3
I0521 08:40:46.068310 27336 net.cpp:454] loss <- label
I0521 08:40:46.068322 27336 net.cpp:411] loss -> loss
I0521 08:40:46.068339 27336 layer_factory.hpp:77] Creating layer loss
I0521 08:40:46.068994 27336 net.cpp:150] Setting up loss
I0521 08:40:46.069015 27336 net.cpp:157] Top shape: (1)
I0521 08:40:46.069028 27336 net.cpp:160]     with loss weight 1
I0521 08:40:46.069073 27336 net.cpp:165] Memory required for data: 1357750804
I0521 08:40:46.069083 27336 net.cpp:226] loss needs backward computation.
I0521 08:40:46.069094 27336 net.cpp:226] drop3 needs backward computation.
I0521 08:40:46.069103 27336 net.cpp:226] ip3 needs backward computation.
I0521 08:40:46.069113 27336 net.cpp:226] drop2 needs backward computation.
I0521 08:40:46.069123 27336 net.cpp:226] relu6 needs backward computation.
I0521 08:40:46.069133 27336 net.cpp:226] ip2 needs backward computation.
I0521 08:40:46.069142 27336 net.cpp:226] drop1 needs backward computation.
I0521 08:40:46.069152 27336 net.cpp:226] relu5 needs backward computation.
I0521 08:40:46.069161 27336 net.cpp:226] ip1 needs backward computation.
I0521 08:40:46.069171 27336 net.cpp:226] pool4 needs backward computation.
I0521 08:40:46.069181 27336 net.cpp:226] relu4 needs backward computation.
I0521 08:40:46.069191 27336 net.cpp:226] conv4 needs backward computation.
I0521 08:40:46.069202 27336 net.cpp:226] pool3 needs backward computation.
I0521 08:40:46.069221 27336 net.cpp:226] relu3 needs backward computation.
I0521 08:40:46.069231 27336 net.cpp:226] conv3 needs backward computation.
I0521 08:40:46.069242 27336 net.cpp:226] pool2 needs backward computation.
I0521 08:40:46.069252 27336 net.cpp:226] relu2 needs backward computation.
I0521 08:40:46.069262 27336 net.cpp:226] conv2 needs backward computation.
I0521 08:40:46.069273 27336 net.cpp:226] pool1 needs backward computation.
I0521 08:40:46.069283 27336 net.cpp:226] relu1 needs backward computation.
I0521 08:40:46.069293 27336 net.cpp:226] conv1 needs backward computation.
I0521 08:40:46.069304 27336 net.cpp:228] data_hdf5 does not need backward computation.
I0521 08:40:46.069314 27336 net.cpp:270] This network produces output loss
I0521 08:40:46.069339 27336 net.cpp:283] Network initialization done.
I0521 08:40:46.070911 27336 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492.prototxt
I0521 08:40:46.070983 27336 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 08:40:46.071337 27336 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 860
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 08:40:46.071527 27336 layer_factory.hpp:77] Creating layer data_hdf5
I0521 08:40:46.071542 27336 net.cpp:106] Creating Layer data_hdf5
I0521 08:40:46.071554 27336 net.cpp:411] data_hdf5 -> data
I0521 08:40:46.071571 27336 net.cpp:411] data_hdf5 -> label
I0521 08:40:46.071586 27336 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 08:40:46.072782 27336 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 08:41:07.382838 27336 net.cpp:150] Setting up data_hdf5
I0521 08:41:07.383004 27336 net.cpp:157] Top shape: 860 1 127 50 (5461000)
I0521 08:41:07.383019 27336 net.cpp:157] Top shape: 860 (860)
I0521 08:41:07.383029 27336 net.cpp:165] Memory required for data: 21847440
I0521 08:41:07.383044 27336 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 08:41:07.383072 27336 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 08:41:07.383083 27336 net.cpp:454] label_data_hdf5_1_split <- label
I0521 08:41:07.383098 27336 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 08:41:07.383121 27336 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 08:41:07.383193 27336 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 08:41:07.383208 27336 net.cpp:157] Top shape: 860 (860)
I0521 08:41:07.383219 27336 net.cpp:157] Top shape: 860 (860)
I0521 08:41:07.383229 27336 net.cpp:165] Memory required for data: 21854320
I0521 08:41:07.383239 27336 layer_factory.hpp:77] Creating layer conv1
I0521 08:41:07.383260 27336 net.cpp:106] Creating Layer conv1
I0521 08:41:07.383271 27336 net.cpp:454] conv1 <- data
I0521 08:41:07.383286 27336 net.cpp:411] conv1 -> conv1
I0521 08:41:07.385208 27336 net.cpp:150] Setting up conv1
I0521 08:41:07.385232 27336 net.cpp:157] Top shape: 860 12 120 48 (59443200)
I0521 08:41:07.385244 27336 net.cpp:165] Memory required for data: 259627120
I0521 08:41:07.385264 27336 layer_factory.hpp:77] Creating layer relu1
I0521 08:41:07.385279 27336 net.cpp:106] Creating Layer relu1
I0521 08:41:07.385289 27336 net.cpp:454] relu1 <- conv1
I0521 08:41:07.385303 27336 net.cpp:397] relu1 -> conv1 (in-place)
I0521 08:41:07.385797 27336 net.cpp:150] Setting up relu1
I0521 08:41:07.385813 27336 net.cpp:157] Top shape: 860 12 120 48 (59443200)
I0521 08:41:07.385824 27336 net.cpp:165] Memory required for data: 497399920
I0521 08:41:07.385834 27336 layer_factory.hpp:77] Creating layer pool1
I0521 08:41:07.385850 27336 net.cpp:106] Creating Layer pool1
I0521 08:41:07.385860 27336 net.cpp:454] pool1 <- conv1
I0521 08:41:07.385874 27336 net.cpp:411] pool1 -> pool1
I0521 08:41:07.385946 27336 net.cpp:150] Setting up pool1
I0521 08:41:07.385959 27336 net.cpp:157] Top shape: 860 12 60 48 (29721600)
I0521 08:41:07.385968 27336 net.cpp:165] Memory required for data: 616286320
I0521 08:41:07.385979 27336 layer_factory.hpp:77] Creating layer conv2
I0521 08:41:07.385996 27336 net.cpp:106] Creating Layer conv2
I0521 08:41:07.386008 27336 net.cpp:454] conv2 <- pool1
I0521 08:41:07.386021 27336 net.cpp:411] conv2 -> conv2
I0521 08:41:07.387923 27336 net.cpp:150] Setting up conv2
I0521 08:41:07.387944 27336 net.cpp:157] Top shape: 860 20 54 46 (42724800)
I0521 08:41:07.387958 27336 net.cpp:165] Memory required for data: 787185520
I0521 08:41:07.387975 27336 layer_factory.hpp:77] Creating layer relu2
I0521 08:41:07.387989 27336 net.cpp:106] Creating Layer relu2
I0521 08:41:07.388010 27336 net.cpp:454] relu2 <- conv2
I0521 08:41:07.388022 27336 net.cpp:397] relu2 -> conv2 (in-place)
I0521 08:41:07.388356 27336 net.cpp:150] Setting up relu2
I0521 08:41:07.388370 27336 net.cpp:157] Top shape: 860 20 54 46 (42724800)
I0521 08:41:07.388381 27336 net.cpp:165] Memory required for data: 958084720
I0521 08:41:07.388391 27336 layer_factory.hpp:77] Creating layer pool2
I0521 08:41:07.388404 27336 net.cpp:106] Creating Layer pool2
I0521 08:41:07.388414 27336 net.cpp:454] pool2 <- conv2
I0521 08:41:07.388427 27336 net.cpp:411] pool2 -> pool2
I0521 08:41:07.388499 27336 net.cpp:150] Setting up pool2
I0521 08:41:07.388512 27336 net.cpp:157] Top shape: 860 20 27 46 (21362400)
I0521 08:41:07.388521 27336 net.cpp:165] Memory required for data: 1043534320
I0521 08:41:07.388531 27336 layer_factory.hpp:77] Creating layer conv3
I0521 08:41:07.388551 27336 net.cpp:106] Creating Layer conv3
I0521 08:41:07.388561 27336 net.cpp:454] conv3 <- pool2
I0521 08:41:07.388576 27336 net.cpp:411] conv3 -> conv3
I0521 08:41:07.390553 27336 net.cpp:150] Setting up conv3
I0521 08:41:07.390576 27336 net.cpp:157] Top shape: 860 28 22 44 (23309440)
I0521 08:41:07.390588 27336 net.cpp:165] Memory required for data: 1136772080
I0521 08:41:07.390621 27336 layer_factory.hpp:77] Creating layer relu3
I0521 08:41:07.390635 27336 net.cpp:106] Creating Layer relu3
I0521 08:41:07.390645 27336 net.cpp:454] relu3 <- conv3
I0521 08:41:07.390658 27336 net.cpp:397] relu3 -> conv3 (in-place)
I0521 08:41:07.391129 27336 net.cpp:150] Setting up relu3
I0521 08:41:07.391145 27336 net.cpp:157] Top shape: 860 28 22 44 (23309440)
I0521 08:41:07.391155 27336 net.cpp:165] Memory required for data: 1230009840
I0521 08:41:07.391165 27336 layer_factory.hpp:77] Creating layer pool3
I0521 08:41:07.391178 27336 net.cpp:106] Creating Layer pool3
I0521 08:41:07.391188 27336 net.cpp:454] pool3 <- conv3
I0521 08:41:07.391201 27336 net.cpp:411] pool3 -> pool3
I0521 08:41:07.391274 27336 net.cpp:150] Setting up pool3
I0521 08:41:07.391288 27336 net.cpp:157] Top shape: 860 28 11 44 (11654720)
I0521 08:41:07.391297 27336 net.cpp:165] Memory required for data: 1276628720
I0521 08:41:07.391307 27336 layer_factory.hpp:77] Creating layer conv4
I0521 08:41:07.391324 27336 net.cpp:106] Creating Layer conv4
I0521 08:41:07.391335 27336 net.cpp:454] conv4 <- pool3
I0521 08:41:07.391350 27336 net.cpp:411] conv4 -> conv4
I0521 08:41:07.393404 27336 net.cpp:150] Setting up conv4
I0521 08:41:07.393426 27336 net.cpp:157] Top shape: 860 36 6 42 (7801920)
I0521 08:41:07.393438 27336 net.cpp:165] Memory required for data: 1307836400
I0521 08:41:07.393455 27336 layer_factory.hpp:77] Creating layer relu4
I0521 08:41:07.393467 27336 net.cpp:106] Creating Layer relu4
I0521 08:41:07.393477 27336 net.cpp:454] relu4 <- conv4
I0521 08:41:07.393491 27336 net.cpp:397] relu4 -> conv4 (in-place)
I0521 08:41:07.393960 27336 net.cpp:150] Setting up relu4
I0521 08:41:07.393976 27336 net.cpp:157] Top shape: 860 36 6 42 (7801920)
I0521 08:41:07.393986 27336 net.cpp:165] Memory required for data: 1339044080
I0521 08:41:07.393996 27336 layer_factory.hpp:77] Creating layer pool4
I0521 08:41:07.394009 27336 net.cpp:106] Creating Layer pool4
I0521 08:41:07.394018 27336 net.cpp:454] pool4 <- conv4
I0521 08:41:07.394032 27336 net.cpp:411] pool4 -> pool4
I0521 08:41:07.394104 27336 net.cpp:150] Setting up pool4
I0521 08:41:07.394117 27336 net.cpp:157] Top shape: 860 36 3 42 (3900960)
I0521 08:41:07.394126 27336 net.cpp:165] Memory required for data: 1354647920
I0521 08:41:07.394136 27336 layer_factory.hpp:77] Creating layer ip1
I0521 08:41:07.394151 27336 net.cpp:106] Creating Layer ip1
I0521 08:41:07.394162 27336 net.cpp:454] ip1 <- pool4
I0521 08:41:07.394176 27336 net.cpp:411] ip1 -> ip1
I0521 08:41:07.409700 27336 net.cpp:150] Setting up ip1
I0521 08:41:07.409729 27336 net.cpp:157] Top shape: 860 196 (168560)
I0521 08:41:07.409739 27336 net.cpp:165] Memory required for data: 1355322160
I0521 08:41:07.409762 27336 layer_factory.hpp:77] Creating layer relu5
I0521 08:41:07.409777 27336 net.cpp:106] Creating Layer relu5
I0521 08:41:07.409787 27336 net.cpp:454] relu5 <- ip1
I0521 08:41:07.409804 27336 net.cpp:397] relu5 -> ip1 (in-place)
I0521 08:41:07.410148 27336 net.cpp:150] Setting up relu5
I0521 08:41:07.410162 27336 net.cpp:157] Top shape: 860 196 (168560)
I0521 08:41:07.410172 27336 net.cpp:165] Memory required for data: 1355996400
I0521 08:41:07.410182 27336 layer_factory.hpp:77] Creating layer drop1
I0521 08:41:07.410202 27336 net.cpp:106] Creating Layer drop1
I0521 08:41:07.410212 27336 net.cpp:454] drop1 <- ip1
I0521 08:41:07.410224 27336 net.cpp:397] drop1 -> ip1 (in-place)
I0521 08:41:07.410269 27336 net.cpp:150] Setting up drop1
I0521 08:41:07.410281 27336 net.cpp:157] Top shape: 860 196 (168560)
I0521 08:41:07.410292 27336 net.cpp:165] Memory required for data: 1356670640
I0521 08:41:07.410302 27336 layer_factory.hpp:77] Creating layer ip2
I0521 08:41:07.410316 27336 net.cpp:106] Creating Layer ip2
I0521 08:41:07.410326 27336 net.cpp:454] ip2 <- ip1
I0521 08:41:07.410341 27336 net.cpp:411] ip2 -> ip2
I0521 08:41:07.410823 27336 net.cpp:150] Setting up ip2
I0521 08:41:07.410836 27336 net.cpp:157] Top shape: 860 98 (84280)
I0521 08:41:07.410846 27336 net.cpp:165] Memory required for data: 1357007760
I0521 08:41:07.410876 27336 layer_factory.hpp:77] Creating layer relu6
I0521 08:41:07.410888 27336 net.cpp:106] Creating Layer relu6
I0521 08:41:07.410898 27336 net.cpp:454] relu6 <- ip2
I0521 08:41:07.410912 27336 net.cpp:397] relu6 -> ip2 (in-place)
I0521 08:41:07.411437 27336 net.cpp:150] Setting up relu6
I0521 08:41:07.411459 27336 net.cpp:157] Top shape: 860 98 (84280)
I0521 08:41:07.411468 27336 net.cpp:165] Memory required for data: 1357344880
I0521 08:41:07.411479 27336 layer_factory.hpp:77] Creating layer drop2
I0521 08:41:07.411492 27336 net.cpp:106] Creating Layer drop2
I0521 08:41:07.411502 27336 net.cpp:454] drop2 <- ip2
I0521 08:41:07.411515 27336 net.cpp:397] drop2 -> ip2 (in-place)
I0521 08:41:07.411558 27336 net.cpp:150] Setting up drop2
I0521 08:41:07.411571 27336 net.cpp:157] Top shape: 860 98 (84280)
I0521 08:41:07.411581 27336 net.cpp:165] Memory required for data: 1357682000
I0521 08:41:07.411592 27336 layer_factory.hpp:77] Creating layer ip3
I0521 08:41:07.411605 27336 net.cpp:106] Creating Layer ip3
I0521 08:41:07.411615 27336 net.cpp:454] ip3 <- ip2
I0521 08:41:07.411629 27336 net.cpp:411] ip3 -> ip3
I0521 08:41:07.411852 27336 net.cpp:150] Setting up ip3
I0521 08:41:07.411865 27336 net.cpp:157] Top shape: 860 11 (9460)
I0521 08:41:07.411875 27336 net.cpp:165] Memory required for data: 1357719840
I0521 08:41:07.411891 27336 layer_factory.hpp:77] Creating layer drop3
I0521 08:41:07.411905 27336 net.cpp:106] Creating Layer drop3
I0521 08:41:07.411914 27336 net.cpp:454] drop3 <- ip3
I0521 08:41:07.411926 27336 net.cpp:397] drop3 -> ip3 (in-place)
I0521 08:41:07.411968 27336 net.cpp:150] Setting up drop3
I0521 08:41:07.411981 27336 net.cpp:157] Top shape: 860 11 (9460)
I0521 08:41:07.411998 27336 net.cpp:165] Memory required for data: 1357757680
I0521 08:41:07.412009 27336 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 08:41:07.412022 27336 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 08:41:07.412032 27336 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 08:41:07.412045 27336 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 08:41:07.412060 27336 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 08:41:07.412133 27336 net.cpp:150] Setting up ip3_drop3_0_split
I0521 08:41:07.412147 27336 net.cpp:157] Top shape: 860 11 (9460)
I0521 08:41:07.412159 27336 net.cpp:157] Top shape: 860 11 (9460)
I0521 08:41:07.412169 27336 net.cpp:165] Memory required for data: 1357833360
I0521 08:41:07.412179 27336 layer_factory.hpp:77] Creating layer accuracy
I0521 08:41:07.412202 27336 net.cpp:106] Creating Layer accuracy
I0521 08:41:07.412212 27336 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 08:41:07.412223 27336 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 08:41:07.412237 27336 net.cpp:411] accuracy -> accuracy
I0521 08:41:07.412261 27336 net.cpp:150] Setting up accuracy
I0521 08:41:07.412274 27336 net.cpp:157] Top shape: (1)
I0521 08:41:07.412286 27336 net.cpp:165] Memory required for data: 1357833364
I0521 08:41:07.412294 27336 layer_factory.hpp:77] Creating layer loss
I0521 08:41:07.412308 27336 net.cpp:106] Creating Layer loss
I0521 08:41:07.412318 27336 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 08:41:07.412329 27336 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 08:41:07.412343 27336 net.cpp:411] loss -> loss
I0521 08:41:07.412360 27336 layer_factory.hpp:77] Creating layer loss
I0521 08:41:07.412860 27336 net.cpp:150] Setting up loss
I0521 08:41:07.412874 27336 net.cpp:157] Top shape: (1)
I0521 08:41:07.412884 27336 net.cpp:160]     with loss weight 1
I0521 08:41:07.412905 27336 net.cpp:165] Memory required for data: 1357833368
I0521 08:41:07.412915 27336 net.cpp:226] loss needs backward computation.
I0521 08:41:07.412927 27336 net.cpp:228] accuracy does not need backward computation.
I0521 08:41:07.412938 27336 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 08:41:07.412950 27336 net.cpp:226] drop3 needs backward computation.
I0521 08:41:07.412960 27336 net.cpp:226] ip3 needs backward computation.
I0521 08:41:07.412971 27336 net.cpp:226] drop2 needs backward computation.
I0521 08:41:07.412988 27336 net.cpp:226] relu6 needs backward computation.
I0521 08:41:07.412998 27336 net.cpp:226] ip2 needs backward computation.
I0521 08:41:07.413008 27336 net.cpp:226] drop1 needs backward computation.
I0521 08:41:07.413017 27336 net.cpp:226] relu5 needs backward computation.
I0521 08:41:07.413028 27336 net.cpp:226] ip1 needs backward computation.
I0521 08:41:07.413038 27336 net.cpp:226] pool4 needs backward computation.
I0521 08:41:07.413048 27336 net.cpp:226] relu4 needs backward computation.
I0521 08:41:07.413058 27336 net.cpp:226] conv4 needs backward computation.
I0521 08:41:07.413069 27336 net.cpp:226] pool3 needs backward computation.
I0521 08:41:07.413079 27336 net.cpp:226] relu3 needs backward computation.
I0521 08:41:07.413090 27336 net.cpp:226] conv3 needs backward computation.
I0521 08:41:07.413101 27336 net.cpp:226] pool2 needs backward computation.
I0521 08:41:07.413111 27336 net.cpp:226] relu2 needs backward computation.
I0521 08:41:07.413121 27336 net.cpp:226] conv2 needs backward computation.
I0521 08:41:07.413131 27336 net.cpp:226] pool1 needs backward computation.
I0521 08:41:07.413142 27336 net.cpp:226] relu1 needs backward computation.
I0521 08:41:07.413152 27336 net.cpp:226] conv1 needs backward computation.
I0521 08:41:07.413164 27336 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 08:41:07.413175 27336 net.cpp:228] data_hdf5 does not need backward computation.
I0521 08:41:07.413185 27336 net.cpp:270] This network produces output accuracy
I0521 08:41:07.413197 27336 net.cpp:270] This network produces output loss
I0521 08:41:07.413226 27336 net.cpp:283] Network initialization done.
I0521 08:41:07.413359 27336 solver.cpp:60] Solver scaffolding done.
I0521 08:41:07.414504 27336 caffe.cpp:212] Starting Optimization
I0521 08:41:07.414521 27336 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 08:41:07.414535 27336 solver.cpp:289] Learning Rate Policy: fixed
I0521 08:41:07.415753 27336 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 08:41:53.365767 27336 solver.cpp:409]     Test net output #0: accuracy = 0.101738
I0521 08:41:53.365942 27336 solver.cpp:409]     Test net output #1: loss = 2.39841 (* 1 = 2.39841 loss)
I0521 08:41:53.524157 27336 solver.cpp:237] Iteration 0, loss = 2.40065
I0521 08:41:53.524193 27336 solver.cpp:253]     Train net output #0: loss = 2.40065 (* 1 = 2.40065 loss)
I0521 08:41:53.524210 27336 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 08:42:01.366062 27336 solver.cpp:237] Iteration 17, loss = 2.38966
I0521 08:42:01.366097 27336 solver.cpp:253]     Train net output #0: loss = 2.38966 (* 1 = 2.38966 loss)
I0521 08:42:01.366113 27336 sgd_solver.cpp:106] Iteration 17, lr = 0.0025
I0521 08:42:09.205016 27336 solver.cpp:237] Iteration 34, loss = 2.37383
I0521 08:42:09.205062 27336 solver.cpp:253]     Train net output #0: loss = 2.37383 (* 1 = 2.37383 loss)
I0521 08:42:09.205080 27336 sgd_solver.cpp:106] Iteration 34, lr = 0.0025
I0521 08:42:17.045279 27336 solver.cpp:237] Iteration 51, loss = 2.35997
I0521 08:42:17.045310 27336 solver.cpp:253]     Train net output #0: loss = 2.35997 (* 1 = 2.35997 loss)
I0521 08:42:17.045328 27336 sgd_solver.cpp:106] Iteration 51, lr = 0.0025
I0521 08:42:24.888911 27336 solver.cpp:237] Iteration 68, loss = 2.34146
I0521 08:42:24.889063 27336 solver.cpp:253]     Train net output #0: loss = 2.34146 (* 1 = 2.34146 loss)
I0521 08:42:24.889076 27336 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0521 08:42:32.727182 27336 solver.cpp:237] Iteration 85, loss = 2.33116
I0521 08:42:32.727232 27336 solver.cpp:253]     Train net output #0: loss = 2.33116 (* 1 = 2.33116 loss)
I0521 08:42:32.727249 27336 sgd_solver.cpp:106] Iteration 85, lr = 0.0025
I0521 08:42:40.566015 27336 solver.cpp:237] Iteration 102, loss = 2.32328
I0521 08:42:40.566046 27336 solver.cpp:253]     Train net output #0: loss = 2.32328 (* 1 = 2.32328 loss)
I0521 08:42:40.566063 27336 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0521 08:43:10.556828 27336 solver.cpp:237] Iteration 119, loss = 2.31029
I0521 08:43:10.556994 27336 solver.cpp:253]     Train net output #0: loss = 2.31029 (* 1 = 2.31029 loss)
I0521 08:43:10.557008 27336 sgd_solver.cpp:106] Iteration 119, lr = 0.0025
I0521 08:43:18.398236 27336 solver.cpp:237] Iteration 136, loss = 2.29504
I0521 08:43:18.398268 27336 solver.cpp:253]     Train net output #0: loss = 2.29504 (* 1 = 2.29504 loss)
I0521 08:43:18.398288 27336 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0521 08:43:26.239094 27336 solver.cpp:237] Iteration 153, loss = 2.3082
I0521 08:43:26.239138 27336 solver.cpp:253]     Train net output #0: loss = 2.3082 (* 1 = 2.3082 loss)
I0521 08:43:26.239158 27336 sgd_solver.cpp:106] Iteration 153, lr = 0.0025
I0521 08:43:34.079296 27336 solver.cpp:237] Iteration 170, loss = 2.2982
I0521 08:43:34.079329 27336 solver.cpp:253]     Train net output #0: loss = 2.2982 (* 1 = 2.2982 loss)
I0521 08:43:34.079345 27336 sgd_solver.cpp:106] Iteration 170, lr = 0.0025
I0521 08:43:35.463474 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_174.caffemodel
I0521 08:43:35.827826 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_174.solverstate
I0521 08:43:41.983487 27336 solver.cpp:237] Iteration 187, loss = 2.28265
I0521 08:43:41.983642 27336 solver.cpp:253]     Train net output #0: loss = 2.28265 (* 1 = 2.28265 loss)
I0521 08:43:41.983656 27336 sgd_solver.cpp:106] Iteration 187, lr = 0.0025
I0521 08:43:49.820763 27336 solver.cpp:237] Iteration 204, loss = 2.27541
I0521 08:43:49.820798 27336 solver.cpp:253]     Train net output #0: loss = 2.27541 (* 1 = 2.27541 loss)
I0521 08:43:49.820822 27336 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0521 08:43:57.658546 27336 solver.cpp:237] Iteration 221, loss = 2.25378
I0521 08:43:57.658578 27336 solver.cpp:253]     Train net output #0: loss = 2.25378 (* 1 = 2.25378 loss)
I0521 08:43:57.658596 27336 sgd_solver.cpp:106] Iteration 221, lr = 0.0025
I0521 08:44:27.680979 27336 solver.cpp:237] Iteration 238, loss = 2.20794
I0521 08:44:27.681138 27336 solver.cpp:253]     Train net output #0: loss = 2.20794 (* 1 = 2.20794 loss)
I0521 08:44:27.681154 27336 sgd_solver.cpp:106] Iteration 238, lr = 0.0025
I0521 08:44:35.525925 27336 solver.cpp:237] Iteration 255, loss = 2.16417
I0521 08:44:35.525956 27336 solver.cpp:253]     Train net output #0: loss = 2.16417 (* 1 = 2.16417 loss)
I0521 08:44:35.525974 27336 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 08:44:43.367576 27336 solver.cpp:237] Iteration 272, loss = 2.1816
I0521 08:44:43.367620 27336 solver.cpp:253]     Train net output #0: loss = 2.1816 (* 1 = 2.1816 loss)
I0521 08:44:43.367637 27336 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 08:44:51.209373 27336 solver.cpp:237] Iteration 289, loss = 2.13849
I0521 08:44:51.209408 27336 solver.cpp:253]     Train net output #0: loss = 2.13849 (* 1 = 2.13849 loss)
I0521 08:44:51.209424 27336 sgd_solver.cpp:106] Iteration 289, lr = 0.0025
I0521 08:44:59.054316 27336 solver.cpp:237] Iteration 306, loss = 2.16601
I0521 08:44:59.054461 27336 solver.cpp:253]     Train net output #0: loss = 2.16601 (* 1 = 2.16601 loss)
I0521 08:44:59.054473 27336 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 08:45:06.888612 27336 solver.cpp:237] Iteration 323, loss = 2.15034
I0521 08:45:06.888643 27336 solver.cpp:253]     Train net output #0: loss = 2.15034 (* 1 = 2.15034 loss)
I0521 08:45:06.888669 27336 sgd_solver.cpp:106] Iteration 323, lr = 0.0025
I0521 08:45:14.725977 27336 solver.cpp:237] Iteration 340, loss = 2.09357
I0521 08:45:14.726011 27336 solver.cpp:253]     Train net output #0: loss = 2.09357 (* 1 = 2.09357 loss)
I0521 08:45:14.726027 27336 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0521 08:45:17.956281 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_348.caffemodel
I0521 08:45:18.319720 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_348.solverstate
I0521 08:45:18.344270 27336 solver.cpp:341] Iteration 348, Testing net (#0)
I0521 08:46:03.350778 27336 solver.cpp:409]     Test net output #0: accuracy = 0.476009
I0521 08:46:03.350940 27336 solver.cpp:409]     Test net output #1: loss = 1.87461 (* 1 = 1.87461 loss)
I0521 08:46:29.803477 27336 solver.cpp:237] Iteration 357, loss = 2.07903
I0521 08:46:29.803531 27336 solver.cpp:253]     Train net output #0: loss = 2.07903 (* 1 = 2.07903 loss)
I0521 08:46:29.803547 27336 sgd_solver.cpp:106] Iteration 357, lr = 0.0025
I0521 08:46:37.643203 27336 solver.cpp:237] Iteration 374, loss = 2.00002
I0521 08:46:37.643347 27336 solver.cpp:253]     Train net output #0: loss = 2.00002 (* 1 = 2.00002 loss)
I0521 08:46:37.643360 27336 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 08:46:45.486708 27336 solver.cpp:237] Iteration 391, loss = 2.04682
I0521 08:46:45.486747 27336 solver.cpp:253]     Train net output #0: loss = 2.04682 (* 1 = 2.04682 loss)
I0521 08:46:45.486766 27336 sgd_solver.cpp:106] Iteration 391, lr = 0.0025
I0521 08:46:53.326596 27336 solver.cpp:237] Iteration 408, loss = 2.06162
I0521 08:46:53.326628 27336 solver.cpp:253]     Train net output #0: loss = 2.06162 (* 1 = 2.06162 loss)
I0521 08:46:53.326647 27336 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0521 08:47:01.164312 27336 solver.cpp:237] Iteration 425, loss = 1.94728
I0521 08:47:01.164345 27336 solver.cpp:253]     Train net output #0: loss = 1.94728 (* 1 = 1.94728 loss)
I0521 08:47:01.164361 27336 sgd_solver.cpp:106] Iteration 425, lr = 0.0025
I0521 08:47:09.000859 27336 solver.cpp:237] Iteration 442, loss = 2.00941
I0521 08:47:09.001009 27336 solver.cpp:253]     Train net output #0: loss = 2.00941 (* 1 = 2.00941 loss)
I0521 08:47:09.001024 27336 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0521 08:47:16.841282 27336 solver.cpp:237] Iteration 459, loss = 1.98314
I0521 08:47:16.841313 27336 solver.cpp:253]     Train net output #0: loss = 1.98314 (* 1 = 1.98314 loss)
I0521 08:47:16.841331 27336 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0521 08:47:46.862705 27336 solver.cpp:237] Iteration 476, loss = 1.957
I0521 08:47:46.862884 27336 solver.cpp:253]     Train net output #0: loss = 1.957 (* 1 = 1.957 loss)
I0521 08:47:46.862900 27336 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0521 08:47:54.703789 27336 solver.cpp:237] Iteration 493, loss = 1.95733
I0521 08:47:54.703821 27336 solver.cpp:253]     Train net output #0: loss = 1.95733 (* 1 = 1.95733 loss)
I0521 08:47:54.703838 27336 sgd_solver.cpp:106] Iteration 493, lr = 0.0025
I0521 08:48:02.543732 27336 solver.cpp:237] Iteration 510, loss = 1.93998
I0521 08:48:02.543781 27336 solver.cpp:253]     Train net output #0: loss = 1.93998 (* 1 = 1.93998 loss)
I0521 08:48:02.543795 27336 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 08:48:07.615111 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_522.caffemodel
I0521 08:48:07.980868 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_522.solverstate
I0521 08:48:10.450129 27336 solver.cpp:237] Iteration 527, loss = 1.93969
I0521 08:48:10.450178 27336 solver.cpp:253]     Train net output #0: loss = 1.93969 (* 1 = 1.93969 loss)
I0521 08:48:10.450197 27336 sgd_solver.cpp:106] Iteration 527, lr = 0.0025
I0521 08:48:18.291462 27336 solver.cpp:237] Iteration 544, loss = 1.91044
I0521 08:48:18.291613 27336 solver.cpp:253]     Train net output #0: loss = 1.91044 (* 1 = 1.91044 loss)
I0521 08:48:18.291627 27336 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 08:48:26.125957 27336 solver.cpp:237] Iteration 561, loss = 1.90116
I0521 08:48:26.125996 27336 solver.cpp:253]     Train net output #0: loss = 1.90116 (* 1 = 1.90116 loss)
I0521 08:48:26.126015 27336 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0521 08:48:33.965982 27336 solver.cpp:237] Iteration 578, loss = 1.86055
I0521 08:48:33.966014 27336 solver.cpp:253]     Train net output #0: loss = 1.86055 (* 1 = 1.86055 loss)
I0521 08:48:33.966032 27336 sgd_solver.cpp:106] Iteration 578, lr = 0.0025
I0521 08:49:04.004704 27336 solver.cpp:237] Iteration 595, loss = 1.89066
I0521 08:49:04.004868 27336 solver.cpp:253]     Train net output #0: loss = 1.89066 (* 1 = 1.89066 loss)
I0521 08:49:04.004884 27336 sgd_solver.cpp:106] Iteration 595, lr = 0.0025
I0521 08:49:11.841181 27336 solver.cpp:237] Iteration 612, loss = 1.91972
I0521 08:49:11.841229 27336 solver.cpp:253]     Train net output #0: loss = 1.91972 (* 1 = 1.91972 loss)
I0521 08:49:11.841246 27336 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 08:49:19.679765 27336 solver.cpp:237] Iteration 629, loss = 1.84764
I0521 08:49:19.679798 27336 solver.cpp:253]     Train net output #0: loss = 1.84764 (* 1 = 1.84764 loss)
I0521 08:49:19.679814 27336 sgd_solver.cpp:106] Iteration 629, lr = 0.0025
I0521 08:49:27.517329 27336 solver.cpp:237] Iteration 646, loss = 1.84207
I0521 08:49:27.517362 27336 solver.cpp:253]     Train net output #0: loss = 1.84207 (* 1 = 1.84207 loss)
I0521 08:49:27.517379 27336 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0521 08:49:35.354285 27336 solver.cpp:237] Iteration 663, loss = 1.86012
I0521 08:49:35.354418 27336 solver.cpp:253]     Train net output #0: loss = 1.86012 (* 1 = 1.86012 loss)
I0521 08:49:35.354430 27336 sgd_solver.cpp:106] Iteration 663, lr = 0.0025
I0521 08:49:43.188854 27336 solver.cpp:237] Iteration 680, loss = 1.89138
I0521 08:49:43.188894 27336 solver.cpp:253]     Train net output #0: loss = 1.89138 (* 1 = 1.89138 loss)
I0521 08:49:43.188912 27336 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0521 08:49:50.103874 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_696.caffemodel
I0521 08:49:50.467989 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_696.solverstate
I0521 08:49:50.496201 27336 solver.cpp:341] Iteration 696, Testing net (#0)
I0521 08:50:56.323235 27336 solver.cpp:409]     Test net output #0: accuracy = 0.596739
I0521 08:50:56.323412 27336 solver.cpp:409]     Test net output #1: loss = 1.45543 (* 1 = 1.45543 loss)
I0521 08:51:19.117420 27336 solver.cpp:237] Iteration 697, loss = 1.89246
I0521 08:51:19.117475 27336 solver.cpp:253]     Train net output #0: loss = 1.89246 (* 1 = 1.89246 loss)
I0521 08:51:19.117492 27336 sgd_solver.cpp:106] Iteration 697, lr = 0.0025
I0521 08:51:26.960539 27336 solver.cpp:237] Iteration 714, loss = 1.86257
I0521 08:51:26.960685 27336 solver.cpp:253]     Train net output #0: loss = 1.86257 (* 1 = 1.86257 loss)
I0521 08:51:26.960698 27336 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0521 08:51:34.806090 27336 solver.cpp:237] Iteration 731, loss = 1.83356
I0521 08:51:34.806123 27336 solver.cpp:253]     Train net output #0: loss = 1.83356 (* 1 = 1.83356 loss)
I0521 08:51:34.806139 27336 sgd_solver.cpp:106] Iteration 731, lr = 0.0025
I0521 08:51:42.653090 27336 solver.cpp:237] Iteration 748, loss = 1.80364
I0521 08:51:42.653134 27336 solver.cpp:253]     Train net output #0: loss = 1.80364 (* 1 = 1.80364 loss)
I0521 08:51:42.653154 27336 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 08:51:50.490455 27336 solver.cpp:237] Iteration 765, loss = 1.82114
I0521 08:51:50.490489 27336 solver.cpp:253]     Train net output #0: loss = 1.82114 (* 1 = 1.82114 loss)
I0521 08:51:50.490504 27336 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 08:51:58.331332 27336 solver.cpp:237] Iteration 782, loss = 1.84728
I0521 08:51:58.331470 27336 solver.cpp:253]     Train net output #0: loss = 1.84728 (* 1 = 1.84728 loss)
I0521 08:51:58.331485 27336 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0521 08:52:06.166554 27336 solver.cpp:237] Iteration 799, loss = 1.81967
I0521 08:52:06.166585 27336 solver.cpp:253]     Train net output #0: loss = 1.81967 (* 1 = 1.81967 loss)
I0521 08:52:06.166604 27336 sgd_solver.cpp:106] Iteration 799, lr = 0.0025
I0521 08:52:36.189652 27336 solver.cpp:237] Iteration 816, loss = 1.76806
I0521 08:52:36.189823 27336 solver.cpp:253]     Train net output #0: loss = 1.76806 (* 1 = 1.76806 loss)
I0521 08:52:36.189837 27336 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 08:52:44.034842 27336 solver.cpp:237] Iteration 833, loss = 1.84651
I0521 08:52:44.034874 27336 solver.cpp:253]     Train net output #0: loss = 1.84651 (* 1 = 1.84651 loss)
I0521 08:52:44.034893 27336 sgd_solver.cpp:106] Iteration 833, lr = 0.0025
I0521 08:52:51.872498 27336 solver.cpp:237] Iteration 850, loss = 1.79526
I0521 08:52:51.872531 27336 solver.cpp:253]     Train net output #0: loss = 1.79526 (* 1 = 1.79526 loss)
I0521 08:52:51.872548 27336 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0521 08:52:59.706477 27336 solver.cpp:237] Iteration 867, loss = 1.80412
I0521 08:52:59.706526 27336 solver.cpp:253]     Train net output #0: loss = 1.80412 (* 1 = 1.80412 loss)
I0521 08:52:59.706542 27336 sgd_solver.cpp:106] Iteration 867, lr = 0.0025
I0521 08:53:00.629976 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_870.caffemodel
I0521 08:53:00.995540 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_870.solverstate
I0521 08:53:07.622095 27336 solver.cpp:237] Iteration 884, loss = 1.83859
I0521 08:53:07.622254 27336 solver.cpp:253]     Train net output #0: loss = 1.83859 (* 1 = 1.83859 loss)
I0521 08:53:07.622268 27336 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0521 08:53:15.462106 27336 solver.cpp:237] Iteration 901, loss = 1.8072
I0521 08:53:15.462137 27336 solver.cpp:253]     Train net output #0: loss = 1.8072 (* 1 = 1.8072 loss)
I0521 08:53:15.462154 27336 sgd_solver.cpp:106] Iteration 901, lr = 0.0025
I0521 08:53:23.302121 27336 solver.cpp:237] Iteration 918, loss = 1.7877
I0521 08:53:23.302173 27336 solver.cpp:253]     Train net output #0: loss = 1.7877 (* 1 = 1.7877 loss)
I0521 08:53:23.302187 27336 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 08:53:53.371182 27336 solver.cpp:237] Iteration 935, loss = 1.75336
I0521 08:53:53.371420 27336 solver.cpp:253]     Train net output #0: loss = 1.75336 (* 1 = 1.75336 loss)
I0521 08:53:53.371435 27336 sgd_solver.cpp:106] Iteration 935, lr = 0.0025
I0521 08:54:01.215642 27336 solver.cpp:237] Iteration 952, loss = 1.76848
I0521 08:54:01.215674 27336 solver.cpp:253]     Train net output #0: loss = 1.76848 (* 1 = 1.76848 loss)
I0521 08:54:01.215694 27336 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0521 08:54:09.055027 27336 solver.cpp:237] Iteration 969, loss = 1.80692
I0521 08:54:09.055059 27336 solver.cpp:253]     Train net output #0: loss = 1.80692 (* 1 = 1.80692 loss)
I0521 08:54:09.055076 27336 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0521 08:54:16.900193 27336 solver.cpp:237] Iteration 986, loss = 1.83881
I0521 08:54:16.900233 27336 solver.cpp:253]     Train net output #0: loss = 1.83881 (* 1 = 1.83881 loss)
I0521 08:54:16.900251 27336 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0521 08:54:24.744134 27336 solver.cpp:237] Iteration 1003, loss = 1.86969
I0521 08:54:24.744279 27336 solver.cpp:253]     Train net output #0: loss = 1.86969 (* 1 = 1.86969 loss)
I0521 08:54:24.744293 27336 sgd_solver.cpp:106] Iteration 1003, lr = 0.0025
I0521 08:54:32.581993 27336 solver.cpp:237] Iteration 1020, loss = 1.76784
I0521 08:54:32.582025 27336 solver.cpp:253]     Train net output #0: loss = 1.76784 (* 1 = 1.76784 loss)
I0521 08:54:32.582042 27336 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 08:54:40.428033 27336 solver.cpp:237] Iteration 1037, loss = 1.77919
I0521 08:54:40.428072 27336 solver.cpp:253]     Train net output #0: loss = 1.77919 (* 1 = 1.77919 loss)
I0521 08:54:40.428086 27336 sgd_solver.cpp:106] Iteration 1037, lr = 0.0025
I0521 08:54:43.193013 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1044.caffemodel
I0521 08:54:43.554999 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1044.solverstate
I0521 08:54:43.581010 27336 solver.cpp:341] Iteration 1044, Testing net (#0)
I0521 08:55:28.299674 27336 solver.cpp:409]     Test net output #0: accuracy = 0.631937
I0521 08:55:28.299839 27336 solver.cpp:409]     Test net output #1: loss = 1.38563 (* 1 = 1.38563 loss)
I0521 08:55:55.213933 27336 solver.cpp:237] Iteration 1054, loss = 1.7695
I0521 08:55:55.213989 27336 solver.cpp:253]     Train net output #0: loss = 1.7695 (* 1 = 1.7695 loss)
I0521 08:55:55.214004 27336 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0521 08:56:03.047371 27336 solver.cpp:237] Iteration 1071, loss = 1.79658
I0521 08:56:03.047519 27336 solver.cpp:253]     Train net output #0: loss = 1.79658 (* 1 = 1.79658 loss)
I0521 08:56:03.047533 27336 sgd_solver.cpp:106] Iteration 1071, lr = 0.0025
I0521 08:56:10.881933 27336 solver.cpp:237] Iteration 1088, loss = 1.75427
I0521 08:56:10.881965 27336 solver.cpp:253]     Train net output #0: loss = 1.75427 (* 1 = 1.75427 loss)
I0521 08:56:10.881983 27336 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 08:56:18.719439 27336 solver.cpp:237] Iteration 1105, loss = 1.70714
I0521 08:56:18.719480 27336 solver.cpp:253]     Train net output #0: loss = 1.70714 (* 1 = 1.70714 loss)
I0521 08:56:18.719498 27336 sgd_solver.cpp:106] Iteration 1105, lr = 0.0025
I0521 08:56:26.553007 27336 solver.cpp:237] Iteration 1122, loss = 1.70513
I0521 08:56:26.553040 27336 solver.cpp:253]     Train net output #0: loss = 1.70513 (* 1 = 1.70513 loss)
I0521 08:56:26.553056 27336 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 08:56:34.389786 27336 solver.cpp:237] Iteration 1139, loss = 1.7939
I0521 08:56:34.389941 27336 solver.cpp:253]     Train net output #0: loss = 1.7939 (* 1 = 1.7939 loss)
I0521 08:56:34.389955 27336 sgd_solver.cpp:106] Iteration 1139, lr = 0.0025
I0521 08:56:42.224985 27336 solver.cpp:237] Iteration 1156, loss = 1.82159
I0521 08:56:42.225021 27336 solver.cpp:253]     Train net output #0: loss = 1.82159 (* 1 = 1.82159 loss)
I0521 08:56:42.225047 27336 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0521 08:57:12.269631 27336 solver.cpp:237] Iteration 1173, loss = 1.76379
I0521 08:57:12.269798 27336 solver.cpp:253]     Train net output #0: loss = 1.76379 (* 1 = 1.76379 loss)
I0521 08:57:12.269814 27336 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0521 08:57:20.102048 27336 solver.cpp:237] Iteration 1190, loss = 1.73543
I0521 08:57:20.102079 27336 solver.cpp:253]     Train net output #0: loss = 1.73543 (* 1 = 1.73543 loss)
I0521 08:57:20.102094 27336 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0521 08:57:27.933174 27336 solver.cpp:237] Iteration 1207, loss = 1.72812
I0521 08:57:27.933205 27336 solver.cpp:253]     Train net output #0: loss = 1.72812 (* 1 = 1.72812 loss)
I0521 08:57:27.933223 27336 sgd_solver.cpp:106] Iteration 1207, lr = 0.0025
I0521 08:57:32.539013 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1218.caffemodel
I0521 08:57:32.900483 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1218.solverstate
I0521 08:57:35.832036 27336 solver.cpp:237] Iteration 1224, loss = 1.78563
I0521 08:57:35.832080 27336 solver.cpp:253]     Train net output #0: loss = 1.78563 (* 1 = 1.78563 loss)
I0521 08:57:35.832101 27336 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 08:57:43.660274 27336 solver.cpp:237] Iteration 1241, loss = 1.68614
I0521 08:57:43.660414 27336 solver.cpp:253]     Train net output #0: loss = 1.68614 (* 1 = 1.68614 loss)
I0521 08:57:43.660428 27336 sgd_solver.cpp:106] Iteration 1241, lr = 0.0025
I0521 08:57:51.491089 27336 solver.cpp:237] Iteration 1258, loss = 1.76318
I0521 08:57:51.491120 27336 solver.cpp:253]     Train net output #0: loss = 1.76318 (* 1 = 1.76318 loss)
I0521 08:57:51.491138 27336 sgd_solver.cpp:106] Iteration 1258, lr = 0.0025
I0521 08:57:59.321334 27336 solver.cpp:237] Iteration 1275, loss = 1.69034
I0521 08:57:59.321377 27336 solver.cpp:253]     Train net output #0: loss = 1.69034 (* 1 = 1.69034 loss)
I0521 08:57:59.321399 27336 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 08:58:29.325353 27336 solver.cpp:237] Iteration 1292, loss = 1.73738
I0521 08:58:29.325526 27336 solver.cpp:253]     Train net output #0: loss = 1.73738 (* 1 = 1.73738 loss)
I0521 08:58:29.325539 27336 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0521 08:58:37.164216 27336 solver.cpp:237] Iteration 1309, loss = 1.82608
I0521 08:58:37.164244 27336 solver.cpp:253]     Train net output #0: loss = 1.82608 (* 1 = 1.82608 loss)
I0521 08:58:37.164258 27336 sgd_solver.cpp:106] Iteration 1309, lr = 0.0025
I0521 08:58:44.998653 27336 solver.cpp:237] Iteration 1326, loss = 1.74735
I0521 08:58:44.998684 27336 solver.cpp:253]     Train net output #0: loss = 1.74735 (* 1 = 1.74735 loss)
I0521 08:58:44.998702 27336 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0521 08:58:52.834748 27336 solver.cpp:237] Iteration 1343, loss = 1.79039
I0521 08:58:52.834789 27336 solver.cpp:253]     Train net output #0: loss = 1.79039 (* 1 = 1.79039 loss)
I0521 08:58:52.834810 27336 sgd_solver.cpp:106] Iteration 1343, lr = 0.0025
I0521 08:59:00.666822 27336 solver.cpp:237] Iteration 1360, loss = 1.73857
I0521 08:59:00.666960 27336 solver.cpp:253]     Train net output #0: loss = 1.73857 (* 1 = 1.73857 loss)
I0521 08:59:00.666972 27336 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 08:59:08.499892 27336 solver.cpp:237] Iteration 1377, loss = 1.71033
I0521 08:59:08.499922 27336 solver.cpp:253]     Train net output #0: loss = 1.71033 (* 1 = 1.71033 loss)
I0521 08:59:08.499940 27336 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0521 08:59:14.951580 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1392.caffemodel
I0521 08:59:15.313382 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1392.solverstate
I0521 08:59:15.339284 27336 solver.cpp:341] Iteration 1392, Testing net (#0)
I0521 09:00:21.240892 27336 solver.cpp:409]     Test net output #0: accuracy = 0.655179
I0521 09:00:21.241080 27336 solver.cpp:409]     Test net output #1: loss = 1.21234 (* 1 = 1.21234 loss)
I0521 09:00:22.298859 27336 solver.cpp:237] Iteration 1394, loss = 1.71178
I0521 09:00:22.298888 27336 solver.cpp:253]     Train net output #0: loss = 1.71178 (* 1 = 1.71178 loss)
I0521 09:00:22.298903 27336 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0521 09:00:52.348191 27336 solver.cpp:237] Iteration 1411, loss = 1.68075
I0521 09:00:52.348357 27336 solver.cpp:253]     Train net output #0: loss = 1.68075 (* 1 = 1.68075 loss)
I0521 09:00:52.348373 27336 sgd_solver.cpp:106] Iteration 1411, lr = 0.0025
I0521 09:01:00.180418 27336 solver.cpp:237] Iteration 1428, loss = 1.77602
I0521 09:01:00.180445 27336 solver.cpp:253]     Train net output #0: loss = 1.77602 (* 1 = 1.77602 loss)
I0521 09:01:00.180461 27336 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 09:01:08.019634 27336 solver.cpp:237] Iteration 1445, loss = 1.69802
I0521 09:01:08.019665 27336 solver.cpp:253]     Train net output #0: loss = 1.69802 (* 1 = 1.69802 loss)
I0521 09:01:08.019682 27336 sgd_solver.cpp:106] Iteration 1445, lr = 0.0025
I0521 09:01:15.852460 27336 solver.cpp:237] Iteration 1462, loss = 1.70371
I0521 09:01:15.852491 27336 solver.cpp:253]     Train net output #0: loss = 1.70371 (* 1 = 1.70371 loss)
I0521 09:01:15.852509 27336 sgd_solver.cpp:106] Iteration 1462, lr = 0.0025
I0521 09:01:23.686663 27336 solver.cpp:237] Iteration 1479, loss = 1.71884
I0521 09:01:23.686815 27336 solver.cpp:253]     Train net output #0: loss = 1.71884 (* 1 = 1.71884 loss)
I0521 09:01:23.686828 27336 sgd_solver.cpp:106] Iteration 1479, lr = 0.0025
I0521 09:01:31.525313 27336 solver.cpp:237] Iteration 1496, loss = 1.70625
I0521 09:01:31.525346 27336 solver.cpp:253]     Train net output #0: loss = 1.70625 (* 1 = 1.70625 loss)
I0521 09:01:31.525362 27336 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 09:02:01.562532 27336 solver.cpp:237] Iteration 1513, loss = 1.75424
I0521 09:02:01.562706 27336 solver.cpp:253]     Train net output #0: loss = 1.75424 (* 1 = 1.75424 loss)
I0521 09:02:01.562722 27336 sgd_solver.cpp:106] Iteration 1513, lr = 0.0025
I0521 09:02:09.398995 27336 solver.cpp:237] Iteration 1530, loss = 1.70706
I0521 09:02:09.399042 27336 solver.cpp:253]     Train net output #0: loss = 1.70706 (* 1 = 1.70706 loss)
I0521 09:02:09.399062 27336 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 09:02:17.227690 27336 solver.cpp:237] Iteration 1547, loss = 1.68942
I0521 09:02:17.227723 27336 solver.cpp:253]     Train net output #0: loss = 1.68942 (* 1 = 1.68942 loss)
I0521 09:02:17.227743 27336 sgd_solver.cpp:106] Iteration 1547, lr = 0.0025
I0521 09:02:25.062975 27336 solver.cpp:237] Iteration 1564, loss = 1.7243
I0521 09:02:25.063007 27336 solver.cpp:253]     Train net output #0: loss = 1.7243 (* 1 = 1.7243 loss)
I0521 09:02:25.063025 27336 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0521 09:02:25.524631 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1566.caffemodel
I0521 09:02:25.889276 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1566.solverstate
I0521 09:02:32.967094 27336 solver.cpp:237] Iteration 1581, loss = 1.68963
I0521 09:02:32.967277 27336 solver.cpp:253]     Train net output #0: loss = 1.68963 (* 1 = 1.68963 loss)
I0521 09:02:32.967291 27336 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0521 09:02:40.801668 27336 solver.cpp:237] Iteration 1598, loss = 1.73794
I0521 09:02:40.801700 27336 solver.cpp:253]     Train net output #0: loss = 1.73794 (* 1 = 1.73794 loss)
I0521 09:02:40.801717 27336 sgd_solver.cpp:106] Iteration 1598, lr = 0.0025
I0521 09:02:48.635946 27336 solver.cpp:237] Iteration 1615, loss = 1.71168
I0521 09:02:48.635977 27336 solver.cpp:253]     Train net output #0: loss = 1.71168 (* 1 = 1.71168 loss)
I0521 09:02:48.636003 27336 sgd_solver.cpp:106] Iteration 1615, lr = 0.0025
I0521 09:03:18.646647 27336 solver.cpp:237] Iteration 1632, loss = 1.6228
I0521 09:03:18.646818 27336 solver.cpp:253]     Train net output #0: loss = 1.6228 (* 1 = 1.6228 loss)
I0521 09:03:18.646832 27336 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 09:03:26.486434 27336 solver.cpp:237] Iteration 1649, loss = 1.73698
I0521 09:03:26.486470 27336 solver.cpp:253]     Train net output #0: loss = 1.73698 (* 1 = 1.73698 loss)
I0521 09:03:26.486487 27336 sgd_solver.cpp:106] Iteration 1649, lr = 0.0025
I0521 09:03:34.319448 27336 solver.cpp:237] Iteration 1666, loss = 1.6597
I0521 09:03:34.319483 27336 solver.cpp:253]     Train net output #0: loss = 1.6597 (* 1 = 1.6597 loss)
I0521 09:03:34.319499 27336 sgd_solver.cpp:106] Iteration 1666, lr = 0.0025
I0521 09:03:42.154553 27336 solver.cpp:237] Iteration 1683, loss = 1.70272
I0521 09:03:42.154587 27336 solver.cpp:253]     Train net output #0: loss = 1.70272 (* 1 = 1.70272 loss)
I0521 09:03:42.154603 27336 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0521 09:03:49.989131 27336 solver.cpp:237] Iteration 1700, loss = 1.6917
I0521 09:03:49.989284 27336 solver.cpp:253]     Train net output #0: loss = 1.6917 (* 1 = 1.6917 loss)
I0521 09:03:49.989298 27336 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 09:03:57.828713 27336 solver.cpp:237] Iteration 1717, loss = 1.66514
I0521 09:03:57.828744 27336 solver.cpp:253]     Train net output #0: loss = 1.66514 (* 1 = 1.66514 loss)
I0521 09:03:57.828763 27336 sgd_solver.cpp:106] Iteration 1717, lr = 0.0025
I0521 09:04:05.664912 27336 solver.cpp:237] Iteration 1734, loss = 1.62447
I0521 09:04:05.664944 27336 solver.cpp:253]     Train net output #0: loss = 1.62447 (* 1 = 1.62447 loss)
I0521 09:04:05.664963 27336 sgd_solver.cpp:106] Iteration 1734, lr = 0.0025
I0521 09:04:07.971141 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1740.caffemodel
I0521 09:04:08.334727 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1740.solverstate
I0521 09:04:08.363133 27336 solver.cpp:341] Iteration 1740, Testing net (#0)
I0521 09:04:53.411646 27336 solver.cpp:409]     Test net output #0: accuracy = 0.660659
I0521 09:04:53.411823 27336 solver.cpp:409]     Test net output #1: loss = 1.15546 (* 1 = 1.15546 loss)
I0521 09:04:54.932000 27336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1744.caffemodel
I0521 09:04:55.295109 27336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492_iter_1744.solverstate
I0521 09:04:55.323154 27336 solver.cpp:326] Optimization Done.
I0521 09:04:55.323184 27336 caffe.cpp:215] Optimization Done.
Application 11237323 resources: utime ~1248s, stime ~225s, Rss ~5329920, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_860_2016-05-20T11.21.04.079492.solver"
	User time (seconds): 0.57
	System time (seconds): 0.10
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:37.60
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2662
	Involuntary context switches: 70
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
