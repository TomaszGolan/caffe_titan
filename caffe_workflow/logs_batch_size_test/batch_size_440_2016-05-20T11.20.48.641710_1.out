2806053
I0520 23:43:10.099402  9674 caffe.cpp:184] Using GPUs 0
I0520 23:43:10.523689  9674 solver.cpp:48] Initializing solver from parameters: 
test_iter: 340
test_interval: 681
base_lr: 0.0025
display: 34
max_iter: 3409
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 340
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710.prototxt"
I0520 23:43:11.645967  9674 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710.prototxt
I0520 23:43:11.695219  9674 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 23:43:11.695281  9674 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 23:43:11.695636  9674 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 440
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:43:11.695814  9674 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:43:11.695838  9674 net.cpp:106] Creating Layer data_hdf5
I0520 23:43:11.695852  9674 net.cpp:411] data_hdf5 -> data
I0520 23:43:11.695886  9674 net.cpp:411] data_hdf5 -> label
I0520 23:43:11.695919  9674 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 23:43:11.697863  9674 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 23:43:11.724136  9674 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 23:43:33.289260  9674 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 23:43:33.296402  9674 net.cpp:150] Setting up data_hdf5
I0520 23:43:33.296443  9674 net.cpp:157] Top shape: 440 1 127 50 (2794000)
I0520 23:43:33.296458  9674 net.cpp:157] Top shape: 440 (440)
I0520 23:43:33.296471  9674 net.cpp:165] Memory required for data: 11177760
I0520 23:43:33.296484  9674 layer_factory.hpp:77] Creating layer conv1
I0520 23:43:33.296517  9674 net.cpp:106] Creating Layer conv1
I0520 23:43:33.296528  9674 net.cpp:454] conv1 <- data
I0520 23:43:33.296551  9674 net.cpp:411] conv1 -> conv1
I0520 23:43:33.665796  9674 net.cpp:150] Setting up conv1
I0520 23:43:33.665844  9674 net.cpp:157] Top shape: 440 12 120 48 (30412800)
I0520 23:43:33.665855  9674 net.cpp:165] Memory required for data: 132828960
I0520 23:43:33.665882  9674 layer_factory.hpp:77] Creating layer relu1
I0520 23:43:33.665904  9674 net.cpp:106] Creating Layer relu1
I0520 23:43:33.665915  9674 net.cpp:454] relu1 <- conv1
I0520 23:43:33.665930  9674 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:43:33.666443  9674 net.cpp:150] Setting up relu1
I0520 23:43:33.666460  9674 net.cpp:157] Top shape: 440 12 120 48 (30412800)
I0520 23:43:33.666471  9674 net.cpp:165] Memory required for data: 254480160
I0520 23:43:33.666481  9674 layer_factory.hpp:77] Creating layer pool1
I0520 23:43:33.666497  9674 net.cpp:106] Creating Layer pool1
I0520 23:43:33.666507  9674 net.cpp:454] pool1 <- conv1
I0520 23:43:33.666520  9674 net.cpp:411] pool1 -> pool1
I0520 23:43:33.666600  9674 net.cpp:150] Setting up pool1
I0520 23:43:33.666615  9674 net.cpp:157] Top shape: 440 12 60 48 (15206400)
I0520 23:43:33.666625  9674 net.cpp:165] Memory required for data: 315305760
I0520 23:43:33.666632  9674 layer_factory.hpp:77] Creating layer conv2
I0520 23:43:33.666656  9674 net.cpp:106] Creating Layer conv2
I0520 23:43:33.666666  9674 net.cpp:454] conv2 <- pool1
I0520 23:43:33.666681  9674 net.cpp:411] conv2 -> conv2
I0520 23:43:33.669375  9674 net.cpp:150] Setting up conv2
I0520 23:43:33.669397  9674 net.cpp:157] Top shape: 440 20 54 46 (21859200)
I0520 23:43:33.669409  9674 net.cpp:165] Memory required for data: 402742560
I0520 23:43:33.669427  9674 layer_factory.hpp:77] Creating layer relu2
I0520 23:43:33.669441  9674 net.cpp:106] Creating Layer relu2
I0520 23:43:33.669451  9674 net.cpp:454] relu2 <- conv2
I0520 23:43:33.669463  9674 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:43:33.669793  9674 net.cpp:150] Setting up relu2
I0520 23:43:33.669808  9674 net.cpp:157] Top shape: 440 20 54 46 (21859200)
I0520 23:43:33.669818  9674 net.cpp:165] Memory required for data: 490179360
I0520 23:43:33.669828  9674 layer_factory.hpp:77] Creating layer pool2
I0520 23:43:33.669841  9674 net.cpp:106] Creating Layer pool2
I0520 23:43:33.669850  9674 net.cpp:454] pool2 <- conv2
I0520 23:43:33.669877  9674 net.cpp:411] pool2 -> pool2
I0520 23:43:33.669945  9674 net.cpp:150] Setting up pool2
I0520 23:43:33.669958  9674 net.cpp:157] Top shape: 440 20 27 46 (10929600)
I0520 23:43:33.669968  9674 net.cpp:165] Memory required for data: 533897760
I0520 23:43:33.669978  9674 layer_factory.hpp:77] Creating layer conv3
I0520 23:43:33.669996  9674 net.cpp:106] Creating Layer conv3
I0520 23:43:33.670007  9674 net.cpp:454] conv3 <- pool2
I0520 23:43:33.670020  9674 net.cpp:411] conv3 -> conv3
I0520 23:43:33.671952  9674 net.cpp:150] Setting up conv3
I0520 23:43:33.671974  9674 net.cpp:157] Top shape: 440 28 22 44 (11925760)
I0520 23:43:33.671988  9674 net.cpp:165] Memory required for data: 581600800
I0520 23:43:33.672006  9674 layer_factory.hpp:77] Creating layer relu3
I0520 23:43:33.672022  9674 net.cpp:106] Creating Layer relu3
I0520 23:43:33.672032  9674 net.cpp:454] relu3 <- conv3
I0520 23:43:33.672045  9674 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:43:33.672513  9674 net.cpp:150] Setting up relu3
I0520 23:43:33.672529  9674 net.cpp:157] Top shape: 440 28 22 44 (11925760)
I0520 23:43:33.672539  9674 net.cpp:165] Memory required for data: 629303840
I0520 23:43:33.672549  9674 layer_factory.hpp:77] Creating layer pool3
I0520 23:43:33.672562  9674 net.cpp:106] Creating Layer pool3
I0520 23:43:33.672572  9674 net.cpp:454] pool3 <- conv3
I0520 23:43:33.672585  9674 net.cpp:411] pool3 -> pool3
I0520 23:43:33.672652  9674 net.cpp:150] Setting up pool3
I0520 23:43:33.672665  9674 net.cpp:157] Top shape: 440 28 11 44 (5962880)
I0520 23:43:33.672675  9674 net.cpp:165] Memory required for data: 653155360
I0520 23:43:33.672685  9674 layer_factory.hpp:77] Creating layer conv4
I0520 23:43:33.672703  9674 net.cpp:106] Creating Layer conv4
I0520 23:43:33.672713  9674 net.cpp:454] conv4 <- pool3
I0520 23:43:33.672726  9674 net.cpp:411] conv4 -> conv4
I0520 23:43:33.675498  9674 net.cpp:150] Setting up conv4
I0520 23:43:33.675524  9674 net.cpp:157] Top shape: 440 36 6 42 (3991680)
I0520 23:43:33.675536  9674 net.cpp:165] Memory required for data: 669122080
I0520 23:43:33.675552  9674 layer_factory.hpp:77] Creating layer relu4
I0520 23:43:33.675566  9674 net.cpp:106] Creating Layer relu4
I0520 23:43:33.675577  9674 net.cpp:454] relu4 <- conv4
I0520 23:43:33.675590  9674 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:43:33.676064  9674 net.cpp:150] Setting up relu4
I0520 23:43:33.676081  9674 net.cpp:157] Top shape: 440 36 6 42 (3991680)
I0520 23:43:33.676091  9674 net.cpp:165] Memory required for data: 685088800
I0520 23:43:33.676101  9674 layer_factory.hpp:77] Creating layer pool4
I0520 23:43:33.676115  9674 net.cpp:106] Creating Layer pool4
I0520 23:43:33.676125  9674 net.cpp:454] pool4 <- conv4
I0520 23:43:33.676137  9674 net.cpp:411] pool4 -> pool4
I0520 23:43:33.676205  9674 net.cpp:150] Setting up pool4
I0520 23:43:33.676218  9674 net.cpp:157] Top shape: 440 36 3 42 (1995840)
I0520 23:43:33.676229  9674 net.cpp:165] Memory required for data: 693072160
I0520 23:43:33.676239  9674 layer_factory.hpp:77] Creating layer ip1
I0520 23:43:33.676259  9674 net.cpp:106] Creating Layer ip1
I0520 23:43:33.676270  9674 net.cpp:454] ip1 <- pool4
I0520 23:43:33.676283  9674 net.cpp:411] ip1 -> ip1
I0520 23:43:33.691716  9674 net.cpp:150] Setting up ip1
I0520 23:43:33.691745  9674 net.cpp:157] Top shape: 440 196 (86240)
I0520 23:43:33.691759  9674 net.cpp:165] Memory required for data: 693417120
I0520 23:43:33.691781  9674 layer_factory.hpp:77] Creating layer relu5
I0520 23:43:33.691795  9674 net.cpp:106] Creating Layer relu5
I0520 23:43:33.691807  9674 net.cpp:454] relu5 <- ip1
I0520 23:43:33.691819  9674 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:43:33.692160  9674 net.cpp:150] Setting up relu5
I0520 23:43:33.692174  9674 net.cpp:157] Top shape: 440 196 (86240)
I0520 23:43:33.692185  9674 net.cpp:165] Memory required for data: 693762080
I0520 23:43:33.692195  9674 layer_factory.hpp:77] Creating layer drop1
I0520 23:43:33.692216  9674 net.cpp:106] Creating Layer drop1
I0520 23:43:33.692227  9674 net.cpp:454] drop1 <- ip1
I0520 23:43:33.692252  9674 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:43:33.692298  9674 net.cpp:150] Setting up drop1
I0520 23:43:33.692312  9674 net.cpp:157] Top shape: 440 196 (86240)
I0520 23:43:33.692322  9674 net.cpp:165] Memory required for data: 694107040
I0520 23:43:33.692332  9674 layer_factory.hpp:77] Creating layer ip2
I0520 23:43:33.692350  9674 net.cpp:106] Creating Layer ip2
I0520 23:43:33.692360  9674 net.cpp:454] ip2 <- ip1
I0520 23:43:33.692373  9674 net.cpp:411] ip2 -> ip2
I0520 23:43:33.692837  9674 net.cpp:150] Setting up ip2
I0520 23:43:33.692850  9674 net.cpp:157] Top shape: 440 98 (43120)
I0520 23:43:33.692860  9674 net.cpp:165] Memory required for data: 694279520
I0520 23:43:33.692875  9674 layer_factory.hpp:77] Creating layer relu6
I0520 23:43:33.692888  9674 net.cpp:106] Creating Layer relu6
I0520 23:43:33.692898  9674 net.cpp:454] relu6 <- ip2
I0520 23:43:33.692909  9674 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:43:33.693424  9674 net.cpp:150] Setting up relu6
I0520 23:43:33.693440  9674 net.cpp:157] Top shape: 440 98 (43120)
I0520 23:43:33.693451  9674 net.cpp:165] Memory required for data: 694452000
I0520 23:43:33.693461  9674 layer_factory.hpp:77] Creating layer drop2
I0520 23:43:33.693475  9674 net.cpp:106] Creating Layer drop2
I0520 23:43:33.693485  9674 net.cpp:454] drop2 <- ip2
I0520 23:43:33.693496  9674 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:43:33.693538  9674 net.cpp:150] Setting up drop2
I0520 23:43:33.693552  9674 net.cpp:157] Top shape: 440 98 (43120)
I0520 23:43:33.693562  9674 net.cpp:165] Memory required for data: 694624480
I0520 23:43:33.693572  9674 layer_factory.hpp:77] Creating layer ip3
I0520 23:43:33.693585  9674 net.cpp:106] Creating Layer ip3
I0520 23:43:33.693594  9674 net.cpp:454] ip3 <- ip2
I0520 23:43:33.693608  9674 net.cpp:411] ip3 -> ip3
I0520 23:43:33.693816  9674 net.cpp:150] Setting up ip3
I0520 23:43:33.693830  9674 net.cpp:157] Top shape: 440 11 (4840)
I0520 23:43:33.693840  9674 net.cpp:165] Memory required for data: 694643840
I0520 23:43:33.693855  9674 layer_factory.hpp:77] Creating layer drop3
I0520 23:43:33.693867  9674 net.cpp:106] Creating Layer drop3
I0520 23:43:33.693877  9674 net.cpp:454] drop3 <- ip3
I0520 23:43:33.693889  9674 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:43:33.693928  9674 net.cpp:150] Setting up drop3
I0520 23:43:33.693941  9674 net.cpp:157] Top shape: 440 11 (4840)
I0520 23:43:33.693950  9674 net.cpp:165] Memory required for data: 694663200
I0520 23:43:33.693960  9674 layer_factory.hpp:77] Creating layer loss
I0520 23:43:33.693979  9674 net.cpp:106] Creating Layer loss
I0520 23:43:33.693989  9674 net.cpp:454] loss <- ip3
I0520 23:43:33.694000  9674 net.cpp:454] loss <- label
I0520 23:43:33.694012  9674 net.cpp:411] loss -> loss
I0520 23:43:33.694030  9674 layer_factory.hpp:77] Creating layer loss
I0520 23:43:33.694675  9674 net.cpp:150] Setting up loss
I0520 23:43:33.694696  9674 net.cpp:157] Top shape: (1)
I0520 23:43:33.694710  9674 net.cpp:160]     with loss weight 1
I0520 23:43:33.694751  9674 net.cpp:165] Memory required for data: 694663204
I0520 23:43:33.694762  9674 net.cpp:226] loss needs backward computation.
I0520 23:43:33.694773  9674 net.cpp:226] drop3 needs backward computation.
I0520 23:43:33.694782  9674 net.cpp:226] ip3 needs backward computation.
I0520 23:43:33.694792  9674 net.cpp:226] drop2 needs backward computation.
I0520 23:43:33.694802  9674 net.cpp:226] relu6 needs backward computation.
I0520 23:43:33.694813  9674 net.cpp:226] ip2 needs backward computation.
I0520 23:43:33.694823  9674 net.cpp:226] drop1 needs backward computation.
I0520 23:43:33.694831  9674 net.cpp:226] relu5 needs backward computation.
I0520 23:43:33.694841  9674 net.cpp:226] ip1 needs backward computation.
I0520 23:43:33.694851  9674 net.cpp:226] pool4 needs backward computation.
I0520 23:43:33.694861  9674 net.cpp:226] relu4 needs backward computation.
I0520 23:43:33.694871  9674 net.cpp:226] conv4 needs backward computation.
I0520 23:43:33.694882  9674 net.cpp:226] pool3 needs backward computation.
I0520 23:43:33.694900  9674 net.cpp:226] relu3 needs backward computation.
I0520 23:43:33.694908  9674 net.cpp:226] conv3 needs backward computation.
I0520 23:43:33.694919  9674 net.cpp:226] pool2 needs backward computation.
I0520 23:43:33.694931  9674 net.cpp:226] relu2 needs backward computation.
I0520 23:43:33.694941  9674 net.cpp:226] conv2 needs backward computation.
I0520 23:43:33.694950  9674 net.cpp:226] pool1 needs backward computation.
I0520 23:43:33.694962  9674 net.cpp:226] relu1 needs backward computation.
I0520 23:43:33.694970  9674 net.cpp:226] conv1 needs backward computation.
I0520 23:43:33.694983  9674 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:43:33.694991  9674 net.cpp:270] This network produces output loss
I0520 23:43:33.695015  9674 net.cpp:283] Network initialization done.
I0520 23:43:33.735198  9674 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710.prototxt
I0520 23:43:33.735276  9674 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 23:43:33.735635  9674 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 440
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:43:33.735827  9674 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:43:33.735843  9674 net.cpp:106] Creating Layer data_hdf5
I0520 23:43:33.735857  9674 net.cpp:411] data_hdf5 -> data
I0520 23:43:33.735874  9674 net.cpp:411] data_hdf5 -> label
I0520 23:43:33.735891  9674 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 23:43:33.763564  9674 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 23:43:55.094386  9674 net.cpp:150] Setting up data_hdf5
I0520 23:43:55.094552  9674 net.cpp:157] Top shape: 440 1 127 50 (2794000)
I0520 23:43:55.094565  9674 net.cpp:157] Top shape: 440 (440)
I0520 23:43:55.094578  9674 net.cpp:165] Memory required for data: 11177760
I0520 23:43:55.094591  9674 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 23:43:55.094619  9674 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 23:43:55.094630  9674 net.cpp:454] label_data_hdf5_1_split <- label
I0520 23:43:55.094645  9674 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 23:43:55.094666  9674 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 23:43:55.094739  9674 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 23:43:55.094753  9674 net.cpp:157] Top shape: 440 (440)
I0520 23:43:55.094765  9674 net.cpp:157] Top shape: 440 (440)
I0520 23:43:55.094774  9674 net.cpp:165] Memory required for data: 11181280
I0520 23:43:55.094784  9674 layer_factory.hpp:77] Creating layer conv1
I0520 23:43:55.094805  9674 net.cpp:106] Creating Layer conv1
I0520 23:43:55.094816  9674 net.cpp:454] conv1 <- data
I0520 23:43:55.094830  9674 net.cpp:411] conv1 -> conv1
I0520 23:43:55.096742  9674 net.cpp:150] Setting up conv1
I0520 23:43:55.096766  9674 net.cpp:157] Top shape: 440 12 120 48 (30412800)
I0520 23:43:55.096777  9674 net.cpp:165] Memory required for data: 132832480
I0520 23:43:55.096799  9674 layer_factory.hpp:77] Creating layer relu1
I0520 23:43:55.096813  9674 net.cpp:106] Creating Layer relu1
I0520 23:43:55.096824  9674 net.cpp:454] relu1 <- conv1
I0520 23:43:55.096837  9674 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:43:55.097335  9674 net.cpp:150] Setting up relu1
I0520 23:43:55.097352  9674 net.cpp:157] Top shape: 440 12 120 48 (30412800)
I0520 23:43:55.097362  9674 net.cpp:165] Memory required for data: 254483680
I0520 23:43:55.097373  9674 layer_factory.hpp:77] Creating layer pool1
I0520 23:43:55.097389  9674 net.cpp:106] Creating Layer pool1
I0520 23:43:55.097398  9674 net.cpp:454] pool1 <- conv1
I0520 23:43:55.097411  9674 net.cpp:411] pool1 -> pool1
I0520 23:43:55.097486  9674 net.cpp:150] Setting up pool1
I0520 23:43:55.097501  9674 net.cpp:157] Top shape: 440 12 60 48 (15206400)
I0520 23:43:55.097510  9674 net.cpp:165] Memory required for data: 315309280
I0520 23:43:55.097520  9674 layer_factory.hpp:77] Creating layer conv2
I0520 23:43:55.097538  9674 net.cpp:106] Creating Layer conv2
I0520 23:43:55.097548  9674 net.cpp:454] conv2 <- pool1
I0520 23:43:55.097561  9674 net.cpp:411] conv2 -> conv2
I0520 23:43:55.099478  9674 net.cpp:150] Setting up conv2
I0520 23:43:55.099500  9674 net.cpp:157] Top shape: 440 20 54 46 (21859200)
I0520 23:43:55.099514  9674 net.cpp:165] Memory required for data: 402746080
I0520 23:43:55.099531  9674 layer_factory.hpp:77] Creating layer relu2
I0520 23:43:55.099545  9674 net.cpp:106] Creating Layer relu2
I0520 23:43:55.099555  9674 net.cpp:454] relu2 <- conv2
I0520 23:43:55.099566  9674 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:43:55.099898  9674 net.cpp:150] Setting up relu2
I0520 23:43:55.099912  9674 net.cpp:157] Top shape: 440 20 54 46 (21859200)
I0520 23:43:55.099921  9674 net.cpp:165] Memory required for data: 490182880
I0520 23:43:55.099931  9674 layer_factory.hpp:77] Creating layer pool2
I0520 23:43:55.099944  9674 net.cpp:106] Creating Layer pool2
I0520 23:43:55.099954  9674 net.cpp:454] pool2 <- conv2
I0520 23:43:55.099967  9674 net.cpp:411] pool2 -> pool2
I0520 23:43:55.100039  9674 net.cpp:150] Setting up pool2
I0520 23:43:55.100052  9674 net.cpp:157] Top shape: 440 20 27 46 (10929600)
I0520 23:43:55.100062  9674 net.cpp:165] Memory required for data: 533901280
I0520 23:43:55.100072  9674 layer_factory.hpp:77] Creating layer conv3
I0520 23:43:55.100090  9674 net.cpp:106] Creating Layer conv3
I0520 23:43:55.100100  9674 net.cpp:454] conv3 <- pool2
I0520 23:43:55.100114  9674 net.cpp:411] conv3 -> conv3
I0520 23:43:55.102083  9674 net.cpp:150] Setting up conv3
I0520 23:43:55.102107  9674 net.cpp:157] Top shape: 440 28 22 44 (11925760)
I0520 23:43:55.102118  9674 net.cpp:165] Memory required for data: 581604320
I0520 23:43:55.102151  9674 layer_factory.hpp:77] Creating layer relu3
I0520 23:43:55.102164  9674 net.cpp:106] Creating Layer relu3
I0520 23:43:55.102174  9674 net.cpp:454] relu3 <- conv3
I0520 23:43:55.102187  9674 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:43:55.102658  9674 net.cpp:150] Setting up relu3
I0520 23:43:55.102674  9674 net.cpp:157] Top shape: 440 28 22 44 (11925760)
I0520 23:43:55.102684  9674 net.cpp:165] Memory required for data: 629307360
I0520 23:43:55.102694  9674 layer_factory.hpp:77] Creating layer pool3
I0520 23:43:55.102707  9674 net.cpp:106] Creating Layer pool3
I0520 23:43:55.102718  9674 net.cpp:454] pool3 <- conv3
I0520 23:43:55.102730  9674 net.cpp:411] pool3 -> pool3
I0520 23:43:55.102803  9674 net.cpp:150] Setting up pool3
I0520 23:43:55.102816  9674 net.cpp:157] Top shape: 440 28 11 44 (5962880)
I0520 23:43:55.102826  9674 net.cpp:165] Memory required for data: 653158880
I0520 23:43:55.102834  9674 layer_factory.hpp:77] Creating layer conv4
I0520 23:43:55.102852  9674 net.cpp:106] Creating Layer conv4
I0520 23:43:55.102862  9674 net.cpp:454] conv4 <- pool3
I0520 23:43:55.102877  9674 net.cpp:411] conv4 -> conv4
I0520 23:43:55.104997  9674 net.cpp:150] Setting up conv4
I0520 23:43:55.105015  9674 net.cpp:157] Top shape: 440 36 6 42 (3991680)
I0520 23:43:55.105026  9674 net.cpp:165] Memory required for data: 669125600
I0520 23:43:55.105041  9674 layer_factory.hpp:77] Creating layer relu4
I0520 23:43:55.105056  9674 net.cpp:106] Creating Layer relu4
I0520 23:43:55.105065  9674 net.cpp:454] relu4 <- conv4
I0520 23:43:55.105078  9674 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:43:55.105548  9674 net.cpp:150] Setting up relu4
I0520 23:43:55.105564  9674 net.cpp:157] Top shape: 440 36 6 42 (3991680)
I0520 23:43:55.105574  9674 net.cpp:165] Memory required for data: 685092320
I0520 23:43:55.105584  9674 layer_factory.hpp:77] Creating layer pool4
I0520 23:43:55.105598  9674 net.cpp:106] Creating Layer pool4
I0520 23:43:55.105608  9674 net.cpp:454] pool4 <- conv4
I0520 23:43:55.105621  9674 net.cpp:411] pool4 -> pool4
I0520 23:43:55.105693  9674 net.cpp:150] Setting up pool4
I0520 23:43:55.105707  9674 net.cpp:157] Top shape: 440 36 3 42 (1995840)
I0520 23:43:55.105716  9674 net.cpp:165] Memory required for data: 693075680
I0520 23:43:55.105725  9674 layer_factory.hpp:77] Creating layer ip1
I0520 23:43:55.105741  9674 net.cpp:106] Creating Layer ip1
I0520 23:43:55.105751  9674 net.cpp:454] ip1 <- pool4
I0520 23:43:55.105765  9674 net.cpp:411] ip1 -> ip1
I0520 23:43:55.121254  9674 net.cpp:150] Setting up ip1
I0520 23:43:55.121278  9674 net.cpp:157] Top shape: 440 196 (86240)
I0520 23:43:55.121289  9674 net.cpp:165] Memory required for data: 693420640
I0520 23:43:55.121310  9674 layer_factory.hpp:77] Creating layer relu5
I0520 23:43:55.121326  9674 net.cpp:106] Creating Layer relu5
I0520 23:43:55.121337  9674 net.cpp:454] relu5 <- ip1
I0520 23:43:55.121353  9674 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:43:55.121701  9674 net.cpp:150] Setting up relu5
I0520 23:43:55.121714  9674 net.cpp:157] Top shape: 440 196 (86240)
I0520 23:43:55.121724  9674 net.cpp:165] Memory required for data: 693765600
I0520 23:43:55.121734  9674 layer_factory.hpp:77] Creating layer drop1
I0520 23:43:55.121753  9674 net.cpp:106] Creating Layer drop1
I0520 23:43:55.121763  9674 net.cpp:454] drop1 <- ip1
I0520 23:43:55.121778  9674 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:43:55.121821  9674 net.cpp:150] Setting up drop1
I0520 23:43:55.121834  9674 net.cpp:157] Top shape: 440 196 (86240)
I0520 23:43:55.121845  9674 net.cpp:165] Memory required for data: 694110560
I0520 23:43:55.121855  9674 layer_factory.hpp:77] Creating layer ip2
I0520 23:43:55.121868  9674 net.cpp:106] Creating Layer ip2
I0520 23:43:55.121878  9674 net.cpp:454] ip2 <- ip1
I0520 23:43:55.121892  9674 net.cpp:411] ip2 -> ip2
I0520 23:43:55.122375  9674 net.cpp:150] Setting up ip2
I0520 23:43:55.122387  9674 net.cpp:157] Top shape: 440 98 (43120)
I0520 23:43:55.122397  9674 net.cpp:165] Memory required for data: 694283040
I0520 23:43:55.122426  9674 layer_factory.hpp:77] Creating layer relu6
I0520 23:43:55.122438  9674 net.cpp:106] Creating Layer relu6
I0520 23:43:55.122448  9674 net.cpp:454] relu6 <- ip2
I0520 23:43:55.122460  9674 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:43:55.122994  9674 net.cpp:150] Setting up relu6
I0520 23:43:55.123015  9674 net.cpp:157] Top shape: 440 98 (43120)
I0520 23:43:55.123025  9674 net.cpp:165] Memory required for data: 694455520
I0520 23:43:55.123036  9674 layer_factory.hpp:77] Creating layer drop2
I0520 23:43:55.123049  9674 net.cpp:106] Creating Layer drop2
I0520 23:43:55.123059  9674 net.cpp:454] drop2 <- ip2
I0520 23:43:55.123073  9674 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:43:55.123117  9674 net.cpp:150] Setting up drop2
I0520 23:43:55.123137  9674 net.cpp:157] Top shape: 440 98 (43120)
I0520 23:43:55.123147  9674 net.cpp:165] Memory required for data: 694628000
I0520 23:43:55.123157  9674 layer_factory.hpp:77] Creating layer ip3
I0520 23:43:55.123172  9674 net.cpp:106] Creating Layer ip3
I0520 23:43:55.123181  9674 net.cpp:454] ip3 <- ip2
I0520 23:43:55.123194  9674 net.cpp:411] ip3 -> ip3
I0520 23:43:55.123415  9674 net.cpp:150] Setting up ip3
I0520 23:43:55.123430  9674 net.cpp:157] Top shape: 440 11 (4840)
I0520 23:43:55.123440  9674 net.cpp:165] Memory required for data: 694647360
I0520 23:43:55.123455  9674 layer_factory.hpp:77] Creating layer drop3
I0520 23:43:55.123467  9674 net.cpp:106] Creating Layer drop3
I0520 23:43:55.123477  9674 net.cpp:454] drop3 <- ip3
I0520 23:43:55.123489  9674 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:43:55.123530  9674 net.cpp:150] Setting up drop3
I0520 23:43:55.123543  9674 net.cpp:157] Top shape: 440 11 (4840)
I0520 23:43:55.123553  9674 net.cpp:165] Memory required for data: 694666720
I0520 23:43:55.123564  9674 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 23:43:55.123576  9674 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 23:43:55.123586  9674 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 23:43:55.123599  9674 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 23:43:55.123613  9674 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 23:43:55.123685  9674 net.cpp:150] Setting up ip3_drop3_0_split
I0520 23:43:55.123698  9674 net.cpp:157] Top shape: 440 11 (4840)
I0520 23:43:55.123711  9674 net.cpp:157] Top shape: 440 11 (4840)
I0520 23:43:55.123721  9674 net.cpp:165] Memory required for data: 694705440
I0520 23:43:55.123731  9674 layer_factory.hpp:77] Creating layer accuracy
I0520 23:43:55.123752  9674 net.cpp:106] Creating Layer accuracy
I0520 23:43:55.123762  9674 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 23:43:55.123774  9674 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 23:43:55.123786  9674 net.cpp:411] accuracy -> accuracy
I0520 23:43:55.123811  9674 net.cpp:150] Setting up accuracy
I0520 23:43:55.123823  9674 net.cpp:157] Top shape: (1)
I0520 23:43:55.123833  9674 net.cpp:165] Memory required for data: 694705444
I0520 23:43:55.123843  9674 layer_factory.hpp:77] Creating layer loss
I0520 23:43:55.123857  9674 net.cpp:106] Creating Layer loss
I0520 23:43:55.123867  9674 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 23:43:55.123878  9674 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 23:43:55.123891  9674 net.cpp:411] loss -> loss
I0520 23:43:55.123909  9674 layer_factory.hpp:77] Creating layer loss
I0520 23:43:55.124400  9674 net.cpp:150] Setting up loss
I0520 23:43:55.124414  9674 net.cpp:157] Top shape: (1)
I0520 23:43:55.124424  9674 net.cpp:160]     with loss weight 1
I0520 23:43:55.124444  9674 net.cpp:165] Memory required for data: 694705448
I0520 23:43:55.124454  9674 net.cpp:226] loss needs backward computation.
I0520 23:43:55.124465  9674 net.cpp:228] accuracy does not need backward computation.
I0520 23:43:55.124476  9674 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 23:43:55.124486  9674 net.cpp:226] drop3 needs backward computation.
I0520 23:43:55.124497  9674 net.cpp:226] ip3 needs backward computation.
I0520 23:43:55.124508  9674 net.cpp:226] drop2 needs backward computation.
I0520 23:43:55.124526  9674 net.cpp:226] relu6 needs backward computation.
I0520 23:43:55.124536  9674 net.cpp:226] ip2 needs backward computation.
I0520 23:43:55.124547  9674 net.cpp:226] drop1 needs backward computation.
I0520 23:43:55.124557  9674 net.cpp:226] relu5 needs backward computation.
I0520 23:43:55.124567  9674 net.cpp:226] ip1 needs backward computation.
I0520 23:43:55.124577  9674 net.cpp:226] pool4 needs backward computation.
I0520 23:43:55.124586  9674 net.cpp:226] relu4 needs backward computation.
I0520 23:43:55.124594  9674 net.cpp:226] conv4 needs backward computation.
I0520 23:43:55.124605  9674 net.cpp:226] pool3 needs backward computation.
I0520 23:43:55.124616  9674 net.cpp:226] relu3 needs backward computation.
I0520 23:43:55.124626  9674 net.cpp:226] conv3 needs backward computation.
I0520 23:43:55.124636  9674 net.cpp:226] pool2 needs backward computation.
I0520 23:43:55.124646  9674 net.cpp:226] relu2 needs backward computation.
I0520 23:43:55.124656  9674 net.cpp:226] conv2 needs backward computation.
I0520 23:43:55.124666  9674 net.cpp:226] pool1 needs backward computation.
I0520 23:43:55.124677  9674 net.cpp:226] relu1 needs backward computation.
I0520 23:43:55.124687  9674 net.cpp:226] conv1 needs backward computation.
I0520 23:43:55.124698  9674 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 23:43:55.124711  9674 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:43:55.124721  9674 net.cpp:270] This network produces output accuracy
I0520 23:43:55.124732  9674 net.cpp:270] This network produces output loss
I0520 23:43:55.124760  9674 net.cpp:283] Network initialization done.
I0520 23:43:55.124892  9674 solver.cpp:60] Solver scaffolding done.
I0520 23:43:55.126019  9674 caffe.cpp:212] Starting Optimization
I0520 23:43:55.126037  9674 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 23:43:55.126050  9674 solver.cpp:289] Learning Rate Policy: fixed
I0520 23:43:55.127279  9674 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 23:44:41.157063  9674 solver.cpp:409]     Test net output #0: accuracy = 0.0955748
I0520 23:44:41.157222  9674 solver.cpp:409]     Test net output #1: loss = 2.39808 (* 1 = 2.39808 loss)
I0520 23:44:41.246125  9674 solver.cpp:237] Iteration 0, loss = 2.39801
I0520 23:44:41.246161  9674 solver.cpp:253]     Train net output #0: loss = 2.39801 (* 1 = 2.39801 loss)
I0520 23:44:41.246181  9674 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 23:44:49.320212  9674 solver.cpp:237] Iteration 34, loss = 2.37313
I0520 23:44:49.320247  9674 solver.cpp:253]     Train net output #0: loss = 2.37313 (* 1 = 2.37313 loss)
I0520 23:44:49.320263  9674 sgd_solver.cpp:106] Iteration 34, lr = 0.0025
I0520 23:44:57.400795  9674 solver.cpp:237] Iteration 68, loss = 2.33776
I0520 23:44:57.400843  9674 solver.cpp:253]     Train net output #0: loss = 2.33776 (* 1 = 2.33776 loss)
I0520 23:44:57.400856  9674 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0520 23:45:05.475657  9674 solver.cpp:237] Iteration 102, loss = 2.33783
I0520 23:45:05.475690  9674 solver.cpp:253]     Train net output #0: loss = 2.33783 (* 1 = 2.33783 loss)
I0520 23:45:05.475708  9674 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0520 23:45:13.552731  9674 solver.cpp:237] Iteration 136, loss = 2.32248
I0520 23:45:13.552872  9674 solver.cpp:253]     Train net output #0: loss = 2.32248 (* 1 = 2.32248 loss)
I0520 23:45:13.552887  9674 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0520 23:45:21.630386  9674 solver.cpp:237] Iteration 170, loss = 2.33309
I0520 23:45:21.630427  9674 solver.cpp:253]     Train net output #0: loss = 2.33309 (* 1 = 2.33309 loss)
I0520 23:45:21.630446  9674 sgd_solver.cpp:106] Iteration 170, lr = 0.0025
I0520 23:45:29.703254  9674 solver.cpp:237] Iteration 204, loss = 2.26944
I0520 23:45:29.703286  9674 solver.cpp:253]     Train net output #0: loss = 2.26944 (* 1 = 2.26944 loss)
I0520 23:45:29.703305  9674 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0520 23:46:01.717349  9674 solver.cpp:237] Iteration 238, loss = 2.24136
I0520 23:46:01.717509  9674 solver.cpp:253]     Train net output #0: loss = 2.24136 (* 1 = 2.24136 loss)
I0520 23:46:01.717524  9674 sgd_solver.cpp:106] Iteration 238, lr = 0.0025
I0520 23:46:09.796444  9674 solver.cpp:237] Iteration 272, loss = 2.14046
I0520 23:46:09.796489  9674 solver.cpp:253]     Train net output #0: loss = 2.14046 (* 1 = 2.14046 loss)
I0520 23:46:09.796506  9674 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0520 23:46:17.875475  9674 solver.cpp:237] Iteration 306, loss = 2.11725
I0520 23:46:17.875509  9674 solver.cpp:253]     Train net output #0: loss = 2.11725 (* 1 = 2.11725 loss)
I0520 23:46:17.875524  9674 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0520 23:46:25.716867  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_340.caffemodel
I0520 23:46:25.951616  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_340.solverstate
I0520 23:46:26.048076  9674 solver.cpp:237] Iteration 340, loss = 2.1444
I0520 23:46:26.048120  9674 solver.cpp:253]     Train net output #0: loss = 2.1444 (* 1 = 2.1444 loss)
I0520 23:46:26.048138  9674 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0520 23:46:34.125002  9674 solver.cpp:237] Iteration 374, loss = 2.03969
I0520 23:46:34.125167  9674 solver.cpp:253]     Train net output #0: loss = 2.03969 (* 1 = 2.03969 loss)
I0520 23:46:34.125182  9674 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0520 23:46:42.192704  9674 solver.cpp:237] Iteration 408, loss = 2.00857
I0520 23:46:42.192739  9674 solver.cpp:253]     Train net output #0: loss = 2.00857 (* 1 = 2.00857 loss)
I0520 23:46:42.192752  9674 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0520 23:46:50.268599  9674 solver.cpp:237] Iteration 442, loss = 2.02535
I0520 23:46:50.268632  9674 solver.cpp:253]     Train net output #0: loss = 2.02535 (* 1 = 2.02535 loss)
I0520 23:46:50.268645  9674 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0520 23:47:20.458328  9674 solver.cpp:237] Iteration 476, loss = 2.04905
I0520 23:47:20.458483  9674 solver.cpp:253]     Train net output #0: loss = 2.04905 (* 1 = 2.04905 loss)
I0520 23:47:20.458499  9674 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0520 23:47:28.535542  9674 solver.cpp:237] Iteration 510, loss = 2.0035
I0520 23:47:28.535586  9674 solver.cpp:253]     Train net output #0: loss = 2.0035 (* 1 = 2.0035 loss)
I0520 23:47:28.535604  9674 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0520 23:47:36.615746  9674 solver.cpp:237] Iteration 544, loss = 1.88853
I0520 23:47:36.615780  9674 solver.cpp:253]     Train net output #0: loss = 1.88853 (* 1 = 1.88853 loss)
I0520 23:47:36.615794  9674 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0520 23:47:44.690382  9674 solver.cpp:237] Iteration 578, loss = 1.93564
I0520 23:47:44.690415  9674 solver.cpp:253]     Train net output #0: loss = 1.93564 (* 1 = 1.93564 loss)
I0520 23:47:44.690431  9674 sgd_solver.cpp:106] Iteration 578, lr = 0.0025
I0520 23:47:52.768200  9674 solver.cpp:237] Iteration 612, loss = 1.79935
I0520 23:47:52.768352  9674 solver.cpp:253]     Train net output #0: loss = 1.79935 (* 1 = 1.79935 loss)
I0520 23:47:52.768367  9674 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0520 23:48:00.845839  9674 solver.cpp:237] Iteration 646, loss = 1.84066
I0520 23:48:00.845872  9674 solver.cpp:253]     Train net output #0: loss = 1.84066 (* 1 = 1.84066 loss)
I0520 23:48:00.845890  9674 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0520 23:48:08.685937  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_680.caffemodel
I0520 23:48:09.421650  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_680.solverstate
I0520 23:48:09.547845  9674 solver.cpp:237] Iteration 680, loss = 1.82935
I0520 23:48:09.547891  9674 solver.cpp:253]     Train net output #0: loss = 1.82935 (* 1 = 1.82935 loss)
I0520 23:48:09.547907  9674 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0520 23:48:09.548401  9674 solver.cpp:341] Iteration 681, Testing net (#0)
I0520 23:48:56.631961  9674 solver.cpp:409]     Test net output #0: accuracy = 0.601377
I0520 23:48:56.632122  9674 solver.cpp:409]     Test net output #1: loss = 1.46602 (* 1 = 1.46602 loss)
I0520 23:49:26.740614  9674 solver.cpp:237] Iteration 714, loss = 1.77521
I0520 23:49:26.740777  9674 solver.cpp:253]     Train net output #0: loss = 1.77521 (* 1 = 1.77521 loss)
I0520 23:49:26.740793  9674 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0520 23:49:34.810223  9674 solver.cpp:237] Iteration 748, loss = 1.79349
I0520 23:49:34.810267  9674 solver.cpp:253]     Train net output #0: loss = 1.79349 (* 1 = 1.79349 loss)
I0520 23:49:34.810286  9674 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0520 23:49:42.879420  9674 solver.cpp:237] Iteration 782, loss = 1.80637
I0520 23:49:42.879452  9674 solver.cpp:253]     Train net output #0: loss = 1.80637 (* 1 = 1.80637 loss)
I0520 23:49:42.879468  9674 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0520 23:49:50.951251  9674 solver.cpp:237] Iteration 816, loss = 1.77387
I0520 23:49:50.951283  9674 solver.cpp:253]     Train net output #0: loss = 1.77387 (* 1 = 1.77387 loss)
I0520 23:49:50.951300  9674 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0520 23:49:59.024929  9674 solver.cpp:237] Iteration 850, loss = 1.8766
I0520 23:49:59.025074  9674 solver.cpp:253]     Train net output #0: loss = 1.8766 (* 1 = 1.8766 loss)
I0520 23:49:59.025089  9674 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0520 23:50:07.095098  9674 solver.cpp:237] Iteration 884, loss = 1.81221
I0520 23:50:07.095135  9674 solver.cpp:253]     Train net output #0: loss = 1.81221 (* 1 = 1.81221 loss)
I0520 23:50:07.095149  9674 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0520 23:50:39.689631  9674 solver.cpp:237] Iteration 918, loss = 1.73612
I0520 23:50:39.689796  9674 solver.cpp:253]     Train net output #0: loss = 1.73612 (* 1 = 1.73612 loss)
I0520 23:50:39.689812  9674 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0520 23:50:47.761592  9674 solver.cpp:237] Iteration 952, loss = 1.76664
I0520 23:50:47.761634  9674 solver.cpp:253]     Train net output #0: loss = 1.76664 (* 1 = 1.76664 loss)
I0520 23:50:47.761651  9674 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0520 23:50:55.832759  9674 solver.cpp:237] Iteration 986, loss = 1.89806
I0520 23:50:55.832793  9674 solver.cpp:253]     Train net output #0: loss = 1.89806 (* 1 = 1.89806 loss)
I0520 23:50:55.832810  9674 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0520 23:51:03.666128  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_1020.caffemodel
I0520 23:51:03.939090  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_1020.solverstate
I0520 23:51:04.071579  9674 solver.cpp:237] Iteration 1020, loss = 1.72478
I0520 23:51:04.071630  9674 solver.cpp:253]     Train net output #0: loss = 1.72478 (* 1 = 1.72478 loss)
I0520 23:51:04.071645  9674 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0520 23:51:12.143362  9674 solver.cpp:237] Iteration 1054, loss = 1.84875
I0520 23:51:12.143527  9674 solver.cpp:253]     Train net output #0: loss = 1.84875 (* 1 = 1.84875 loss)
I0520 23:51:12.143540  9674 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0520 23:51:20.213582  9674 solver.cpp:237] Iteration 1088, loss = 1.68085
I0520 23:51:20.213614  9674 solver.cpp:253]     Train net output #0: loss = 1.68085 (* 1 = 1.68085 loss)
I0520 23:51:20.213632  9674 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0520 23:51:28.287051  9674 solver.cpp:237] Iteration 1122, loss = 1.77186
I0520 23:51:28.287083  9674 solver.cpp:253]     Train net output #0: loss = 1.77186 (* 1 = 1.77186 loss)
I0520 23:51:28.287101  9674 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0520 23:51:58.650933  9674 solver.cpp:237] Iteration 1156, loss = 1.78387
I0520 23:51:58.651093  9674 solver.cpp:253]     Train net output #0: loss = 1.78387 (* 1 = 1.78387 loss)
I0520 23:51:58.651108  9674 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0520 23:52:06.724145  9674 solver.cpp:237] Iteration 1190, loss = 1.69203
I0520 23:52:06.724179  9674 solver.cpp:253]     Train net output #0: loss = 1.69203 (* 1 = 1.69203 loss)
I0520 23:52:06.724196  9674 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0520 23:52:14.797646  9674 solver.cpp:237] Iteration 1224, loss = 1.6969
I0520 23:52:14.797679  9674 solver.cpp:253]     Train net output #0: loss = 1.6969 (* 1 = 1.6969 loss)
I0520 23:52:14.797696  9674 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0520 23:52:22.872300  9674 solver.cpp:237] Iteration 1258, loss = 1.75295
I0520 23:52:22.872334  9674 solver.cpp:253]     Train net output #0: loss = 1.75295 (* 1 = 1.75295 loss)
I0520 23:52:22.872350  9674 sgd_solver.cpp:106] Iteration 1258, lr = 0.0025
I0520 23:52:30.946362  9674 solver.cpp:237] Iteration 1292, loss = 1.75604
I0520 23:52:30.946513  9674 solver.cpp:253]     Train net output #0: loss = 1.75604 (* 1 = 1.75604 loss)
I0520 23:52:30.946527  9674 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0520 23:52:39.020448  9674 solver.cpp:237] Iteration 1326, loss = 1.60098
I0520 23:52:39.020481  9674 solver.cpp:253]     Train net output #0: loss = 1.60098 (* 1 = 1.60098 loss)
I0520 23:52:39.020500  9674 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0520 23:52:46.857024  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_1360.caffemodel
I0520 23:52:47.334990  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_1360.solverstate
I0520 23:52:47.461432  9674 solver.cpp:237] Iteration 1360, loss = 1.69855
I0520 23:52:47.461482  9674 solver.cpp:253]     Train net output #0: loss = 1.69855 (* 1 = 1.69855 loss)
I0520 23:52:47.461495  9674 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0520 23:52:47.699015  9674 solver.cpp:341] Iteration 1362, Testing net (#0)
I0520 23:53:55.305951  9674 solver.cpp:409]     Test net output #0: accuracy = 0.654104
I0520 23:53:55.306120  9674 solver.cpp:409]     Test net output #1: loss = 1.18743 (* 1 = 1.18743 loss)
I0520 23:54:25.610494  9674 solver.cpp:237] Iteration 1394, loss = 1.77569
I0520 23:54:25.610661  9674 solver.cpp:253]     Train net output #0: loss = 1.77569 (* 1 = 1.77569 loss)
I0520 23:54:25.610677  9674 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0520 23:54:33.680990  9674 solver.cpp:237] Iteration 1428, loss = 1.71839
I0520 23:54:33.681025  9674 solver.cpp:253]     Train net output #0: loss = 1.71839 (* 1 = 1.71839 loss)
I0520 23:54:33.681043  9674 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0520 23:54:41.750751  9674 solver.cpp:237] Iteration 1462, loss = 1.71901
I0520 23:54:41.750784  9674 solver.cpp:253]     Train net output #0: loss = 1.71901 (* 1 = 1.71901 loss)
I0520 23:54:41.750800  9674 sgd_solver.cpp:106] Iteration 1462, lr = 0.0025
I0520 23:54:49.820878  9674 solver.cpp:237] Iteration 1496, loss = 1.68922
I0520 23:54:49.820910  9674 solver.cpp:253]     Train net output #0: loss = 1.68922 (* 1 = 1.68922 loss)
I0520 23:54:49.820926  9674 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 23:54:57.897048  9674 solver.cpp:237] Iteration 1530, loss = 1.79229
I0520 23:54:57.897202  9674 solver.cpp:253]     Train net output #0: loss = 1.79229 (* 1 = 1.79229 loss)
I0520 23:54:57.897217  9674 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0520 23:55:05.967412  9674 solver.cpp:237] Iteration 1564, loss = 1.72868
I0520 23:55:05.967444  9674 solver.cpp:253]     Train net output #0: loss = 1.72868 (* 1 = 1.72868 loss)
I0520 23:55:05.967463  9674 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0520 23:55:38.506299  9674 solver.cpp:237] Iteration 1598, loss = 1.68296
I0520 23:55:38.506464  9674 solver.cpp:253]     Train net output #0: loss = 1.68296 (* 1 = 1.68296 loss)
I0520 23:55:38.506480  9674 sgd_solver.cpp:106] Iteration 1598, lr = 0.0025
I0520 23:55:46.579941  9674 solver.cpp:237] Iteration 1632, loss = 1.76468
I0520 23:55:46.579973  9674 solver.cpp:253]     Train net output #0: loss = 1.76468 (* 1 = 1.76468 loss)
I0520 23:55:46.579999  9674 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0520 23:55:54.650310  9674 solver.cpp:237] Iteration 1666, loss = 1.66804
I0520 23:55:54.650343  9674 solver.cpp:253]     Train net output #0: loss = 1.66804 (* 1 = 1.66804 loss)
I0520 23:55:54.650357  9674 sgd_solver.cpp:106] Iteration 1666, lr = 0.0025
I0520 23:56:02.484284  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_1700.caffemodel
I0520 23:56:02.730348  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_1700.solverstate
I0520 23:56:02.913452  9674 solver.cpp:237] Iteration 1700, loss = 1.61401
I0520 23:56:02.913496  9674 solver.cpp:253]     Train net output #0: loss = 1.61401 (* 1 = 1.61401 loss)
I0520 23:56:02.913518  9674 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0520 23:56:10.981384  9674 solver.cpp:237] Iteration 1734, loss = 1.69142
I0520 23:56:10.981526  9674 solver.cpp:253]     Train net output #0: loss = 1.69142 (* 1 = 1.69142 loss)
I0520 23:56:10.981540  9674 sgd_solver.cpp:106] Iteration 1734, lr = 0.0025
I0520 23:56:19.052686  9674 solver.cpp:237] Iteration 1768, loss = 1.69232
I0520 23:56:19.052718  9674 solver.cpp:253]     Train net output #0: loss = 1.69232 (* 1 = 1.69232 loss)
I0520 23:56:19.052736  9674 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0520 23:56:27.122786  9674 solver.cpp:237] Iteration 1802, loss = 1.71336
I0520 23:56:27.122818  9674 solver.cpp:253]     Train net output #0: loss = 1.71336 (* 1 = 1.71336 loss)
I0520 23:56:27.122835  9674 sgd_solver.cpp:106] Iteration 1802, lr = 0.0025
I0520 23:56:58.004215  9674 solver.cpp:237] Iteration 1836, loss = 1.67689
I0520 23:56:58.004380  9674 solver.cpp:253]     Train net output #0: loss = 1.67689 (* 1 = 1.67689 loss)
I0520 23:56:58.004395  9674 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0520 23:57:06.080523  9674 solver.cpp:237] Iteration 1870, loss = 1.65814
I0520 23:57:06.080555  9674 solver.cpp:253]     Train net output #0: loss = 1.65814 (* 1 = 1.65814 loss)
I0520 23:57:06.080575  9674 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0520 23:57:14.150454  9674 solver.cpp:237] Iteration 1904, loss = 1.71773
I0520 23:57:14.150490  9674 solver.cpp:253]     Train net output #0: loss = 1.71773 (* 1 = 1.71773 loss)
I0520 23:57:14.150506  9674 sgd_solver.cpp:106] Iteration 1904, lr = 0.0025
I0520 23:57:22.219637  9674 solver.cpp:237] Iteration 1938, loss = 1.75788
I0520 23:57:22.219671  9674 solver.cpp:253]     Train net output #0: loss = 1.75788 (* 1 = 1.75788 loss)
I0520 23:57:22.219687  9674 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0520 23:57:30.284274  9674 solver.cpp:237] Iteration 1972, loss = 1.70585
I0520 23:57:30.284422  9674 solver.cpp:253]     Train net output #0: loss = 1.70585 (* 1 = 1.70585 loss)
I0520 23:57:30.284436  9674 sgd_solver.cpp:106] Iteration 1972, lr = 0.0025
I0520 23:57:38.351222  9674 solver.cpp:237] Iteration 2006, loss = 1.6006
I0520 23:57:38.351254  9674 solver.cpp:253]     Train net output #0: loss = 1.6006 (* 1 = 1.6006 loss)
I0520 23:57:38.351269  9674 sgd_solver.cpp:106] Iteration 2006, lr = 0.0025
I0520 23:57:46.182247  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_2040.caffemodel
I0520 23:57:46.432696  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_2040.solverstate
I0520 23:57:46.567276  9674 solver.cpp:237] Iteration 2040, loss = 1.66015
I0520 23:57:46.567317  9674 solver.cpp:253]     Train net output #0: loss = 1.66015 (* 1 = 1.66015 loss)
I0520 23:57:46.567332  9674 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0520 23:57:47.043172  9674 solver.cpp:341] Iteration 2043, Testing net (#0)
I0520 23:58:32.047413  9674 solver.cpp:409]     Test net output #0: accuracy = 0.673208
I0520 23:58:32.047574  9674 solver.cpp:409]     Test net output #1: loss = 1.12988 (* 1 = 1.12988 loss)
I0520 23:59:01.680377  9674 solver.cpp:237] Iteration 2074, loss = 1.66649
I0520 23:59:01.680428  9674 solver.cpp:253]     Train net output #0: loss = 1.66649 (* 1 = 1.66649 loss)
I0520 23:59:01.680444  9674 sgd_solver.cpp:106] Iteration 2074, lr = 0.0025
I0520 23:59:09.753288  9674 solver.cpp:237] Iteration 2108, loss = 1.69855
I0520 23:59:09.753449  9674 solver.cpp:253]     Train net output #0: loss = 1.69855 (* 1 = 1.69855 loss)
I0520 23:59:09.753463  9674 sgd_solver.cpp:106] Iteration 2108, lr = 0.0025
I0520 23:59:17.824848  9674 solver.cpp:237] Iteration 2142, loss = 1.70266
I0520 23:59:17.824880  9674 solver.cpp:253]     Train net output #0: loss = 1.70266 (* 1 = 1.70266 loss)
I0520 23:59:17.824898  9674 sgd_solver.cpp:106] Iteration 2142, lr = 0.0025
I0520 23:59:25.896924  9674 solver.cpp:237] Iteration 2176, loss = 1.65735
I0520 23:59:25.896957  9674 solver.cpp:253]     Train net output #0: loss = 1.65735 (* 1 = 1.65735 loss)
I0520 23:59:25.896973  9674 sgd_solver.cpp:106] Iteration 2176, lr = 0.0025
I0520 23:59:33.970857  9674 solver.cpp:237] Iteration 2210, loss = 1.58742
I0520 23:59:33.970897  9674 solver.cpp:253]     Train net output #0: loss = 1.58742 (* 1 = 1.58742 loss)
I0520 23:59:33.970917  9674 sgd_solver.cpp:106] Iteration 2210, lr = 0.0025
I0520 23:59:42.043419  9674 solver.cpp:237] Iteration 2244, loss = 1.58646
I0520 23:59:42.043583  9674 solver.cpp:253]     Train net output #0: loss = 1.58646 (* 1 = 1.58646 loss)
I0520 23:59:42.043596  9674 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0521 00:00:12.348382  9674 solver.cpp:237] Iteration 2278, loss = 1.60778
I0521 00:00:12.348558  9674 solver.cpp:253]     Train net output #0: loss = 1.60778 (* 1 = 1.60778 loss)
I0521 00:00:12.348572  9674 sgd_solver.cpp:106] Iteration 2278, lr = 0.0025
I0521 00:00:20.420652  9674 solver.cpp:237] Iteration 2312, loss = 1.63146
I0521 00:00:20.420696  9674 solver.cpp:253]     Train net output #0: loss = 1.63146 (* 1 = 1.63146 loss)
I0521 00:00:20.420713  9674 sgd_solver.cpp:106] Iteration 2312, lr = 0.0025
I0521 00:00:28.486088  9674 solver.cpp:237] Iteration 2346, loss = 1.58597
I0521 00:00:28.486122  9674 solver.cpp:253]     Train net output #0: loss = 1.58597 (* 1 = 1.58597 loss)
I0521 00:00:28.486138  9674 sgd_solver.cpp:106] Iteration 2346, lr = 0.0025
I0521 00:00:36.323269  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_2380.caffemodel
I0521 00:00:36.533918  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_2380.solverstate
I0521 00:00:36.633718  9674 solver.cpp:237] Iteration 2380, loss = 1.59702
I0521 00:00:36.633765  9674 solver.cpp:253]     Train net output #0: loss = 1.59702 (* 1 = 1.59702 loss)
I0521 00:00:36.633782  9674 sgd_solver.cpp:106] Iteration 2380, lr = 0.0025
I0521 00:00:44.703627  9674 solver.cpp:237] Iteration 2414, loss = 1.66661
I0521 00:00:44.703791  9674 solver.cpp:253]     Train net output #0: loss = 1.66661 (* 1 = 1.66661 loss)
I0521 00:00:44.703805  9674 sgd_solver.cpp:106] Iteration 2414, lr = 0.0025
I0521 00:00:52.772452  9674 solver.cpp:237] Iteration 2448, loss = 1.67089
I0521 00:00:52.772485  9674 solver.cpp:253]     Train net output #0: loss = 1.67089 (* 1 = 1.67089 loss)
I0521 00:00:52.772502  9674 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0521 00:01:00.843588  9674 solver.cpp:237] Iteration 2482, loss = 1.65106
I0521 00:01:00.843621  9674 solver.cpp:253]     Train net output #0: loss = 1.65106 (* 1 = 1.65106 loss)
I0521 00:01:00.843639  9674 sgd_solver.cpp:106] Iteration 2482, lr = 0.0025
I0521 00:01:31.136747  9674 solver.cpp:237] Iteration 2516, loss = 1.6636
I0521 00:01:31.136916  9674 solver.cpp:253]     Train net output #0: loss = 1.6636 (* 1 = 1.6636 loss)
I0521 00:01:31.136932  9674 sgd_solver.cpp:106] Iteration 2516, lr = 0.0025
I0521 00:01:39.212352  9674 solver.cpp:237] Iteration 2550, loss = 1.51467
I0521 00:01:39.212394  9674 solver.cpp:253]     Train net output #0: loss = 1.51467 (* 1 = 1.51467 loss)
I0521 00:01:39.212414  9674 sgd_solver.cpp:106] Iteration 2550, lr = 0.0025
I0521 00:01:47.285096  9674 solver.cpp:237] Iteration 2584, loss = 1.64412
I0521 00:01:47.285130  9674 solver.cpp:253]     Train net output #0: loss = 1.64412 (* 1 = 1.64412 loss)
I0521 00:01:47.285145  9674 sgd_solver.cpp:106] Iteration 2584, lr = 0.0025
I0521 00:01:55.355939  9674 solver.cpp:237] Iteration 2618, loss = 1.60541
I0521 00:01:55.355974  9674 solver.cpp:253]     Train net output #0: loss = 1.60541 (* 1 = 1.60541 loss)
I0521 00:01:55.355988  9674 sgd_solver.cpp:106] Iteration 2618, lr = 0.0025
I0521 00:02:03.427103  9674 solver.cpp:237] Iteration 2652, loss = 1.66079
I0521 00:02:03.427258  9674 solver.cpp:253]     Train net output #0: loss = 1.66079 (* 1 = 1.66079 loss)
I0521 00:02:03.427270  9674 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0521 00:02:11.499274  9674 solver.cpp:237] Iteration 2686, loss = 1.68459
I0521 00:02:11.499306  9674 solver.cpp:253]     Train net output #0: loss = 1.68459 (* 1 = 1.68459 loss)
I0521 00:02:11.499321  9674 sgd_solver.cpp:106] Iteration 2686, lr = 0.0025
I0521 00:02:19.334336  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_2720.caffemodel
I0521 00:02:19.539113  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_2720.solverstate
I0521 00:02:19.636281  9674 solver.cpp:237] Iteration 2720, loss = 1.57109
I0521 00:02:19.636327  9674 solver.cpp:253]     Train net output #0: loss = 1.57109 (* 1 = 1.57109 loss)
I0521 00:02:19.636343  9674 sgd_solver.cpp:106] Iteration 2720, lr = 0.0025
I0521 00:02:20.348578  9674 solver.cpp:341] Iteration 2724, Testing net (#0)
I0521 00:03:26.466756  9674 solver.cpp:409]     Test net output #0: accuracy = 0.686471
I0521 00:03:26.466928  9674 solver.cpp:409]     Test net output #1: loss = 1.04379 (* 1 = 1.04379 loss)
I0521 00:03:55.856360  9674 solver.cpp:237] Iteration 2754, loss = 1.66219
I0521 00:03:55.856411  9674 solver.cpp:253]     Train net output #0: loss = 1.66219 (* 1 = 1.66219 loss)
I0521 00:03:55.856427  9674 sgd_solver.cpp:106] Iteration 2754, lr = 0.0025
I0521 00:04:03.926719  9674 solver.cpp:237] Iteration 2788, loss = 1.69026
I0521 00:04:03.926882  9674 solver.cpp:253]     Train net output #0: loss = 1.69026 (* 1 = 1.69026 loss)
I0521 00:04:03.926895  9674 sgd_solver.cpp:106] Iteration 2788, lr = 0.0025
I0521 00:04:12.002940  9674 solver.cpp:237] Iteration 2822, loss = 1.61434
I0521 00:04:12.002974  9674 solver.cpp:253]     Train net output #0: loss = 1.61434 (* 1 = 1.61434 loss)
I0521 00:04:12.002988  9674 sgd_solver.cpp:106] Iteration 2822, lr = 0.0025
I0521 00:04:20.075297  9674 solver.cpp:237] Iteration 2856, loss = 1.5206
I0521 00:04:20.075331  9674 solver.cpp:253]     Train net output #0: loss = 1.5206 (* 1 = 1.5206 loss)
I0521 00:04:20.075345  9674 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0521 00:04:28.149677  9674 solver.cpp:237] Iteration 2890, loss = 1.7111
I0521 00:04:28.149718  9674 solver.cpp:253]     Train net output #0: loss = 1.7111 (* 1 = 1.7111 loss)
I0521 00:04:28.149734  9674 sgd_solver.cpp:106] Iteration 2890, lr = 0.0025
I0521 00:04:36.221914  9674 solver.cpp:237] Iteration 2924, loss = 1.5622
I0521 00:04:36.222053  9674 solver.cpp:253]     Train net output #0: loss = 1.5622 (* 1 = 1.5622 loss)
I0521 00:04:36.222066  9674 sgd_solver.cpp:106] Iteration 2924, lr = 0.0025
I0521 00:05:06.487042  9674 solver.cpp:237] Iteration 2958, loss = 1.61433
I0521 00:05:06.487218  9674 solver.cpp:253]     Train net output #0: loss = 1.61433 (* 1 = 1.61433 loss)
I0521 00:05:06.487233  9674 sgd_solver.cpp:106] Iteration 2958, lr = 0.0025
I0521 00:05:14.557189  9674 solver.cpp:237] Iteration 2992, loss = 1.61349
I0521 00:05:14.557222  9674 solver.cpp:253]     Train net output #0: loss = 1.61349 (* 1 = 1.61349 loss)
I0521 00:05:14.557240  9674 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0521 00:05:22.631522  9674 solver.cpp:237] Iteration 3026, loss = 1.67424
I0521 00:05:22.631566  9674 solver.cpp:253]     Train net output #0: loss = 1.67424 (* 1 = 1.67424 loss)
I0521 00:05:22.631582  9674 sgd_solver.cpp:106] Iteration 3026, lr = 0.0025
I0521 00:05:30.468072  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_3060.caffemodel
I0521 00:05:30.674654  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_3060.solverstate
I0521 00:05:30.774176  9674 solver.cpp:237] Iteration 3060, loss = 1.56245
I0521 00:05:30.774226  9674 solver.cpp:253]     Train net output #0: loss = 1.56245 (* 1 = 1.56245 loss)
I0521 00:05:30.774242  9674 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0521 00:05:38.848644  9674 solver.cpp:237] Iteration 3094, loss = 1.56726
I0521 00:05:38.848793  9674 solver.cpp:253]     Train net output #0: loss = 1.56726 (* 1 = 1.56726 loss)
I0521 00:05:38.848808  9674 sgd_solver.cpp:106] Iteration 3094, lr = 0.0025
I0521 00:05:46.921859  9674 solver.cpp:237] Iteration 3128, loss = 1.64794
I0521 00:05:46.921910  9674 solver.cpp:253]     Train net output #0: loss = 1.64794 (* 1 = 1.64794 loss)
I0521 00:05:46.921922  9674 sgd_solver.cpp:106] Iteration 3128, lr = 0.0025
I0521 00:05:54.996320  9674 solver.cpp:237] Iteration 3162, loss = 1.62464
I0521 00:05:54.996354  9674 solver.cpp:253]     Train net output #0: loss = 1.62464 (* 1 = 1.62464 loss)
I0521 00:05:54.996368  9674 sgd_solver.cpp:106] Iteration 3162, lr = 0.0025
I0521 00:06:25.306946  9674 solver.cpp:237] Iteration 3196, loss = 1.61985
I0521 00:06:25.307132  9674 solver.cpp:253]     Train net output #0: loss = 1.61985 (* 1 = 1.61985 loss)
I0521 00:06:25.307147  9674 sgd_solver.cpp:106] Iteration 3196, lr = 0.0025
I0521 00:06:33.377424  9674 solver.cpp:237] Iteration 3230, loss = 1.58123
I0521 00:06:33.377470  9674 solver.cpp:253]     Train net output #0: loss = 1.58123 (* 1 = 1.58123 loss)
I0521 00:06:33.377485  9674 sgd_solver.cpp:106] Iteration 3230, lr = 0.0025
I0521 00:06:41.448873  9674 solver.cpp:237] Iteration 3264, loss = 1.50476
I0521 00:06:41.448907  9674 solver.cpp:253]     Train net output #0: loss = 1.50476 (* 1 = 1.50476 loss)
I0521 00:06:41.448925  9674 sgd_solver.cpp:106] Iteration 3264, lr = 0.0025
I0521 00:06:49.520601  9674 solver.cpp:237] Iteration 3298, loss = 1.61787
I0521 00:06:49.520635  9674 solver.cpp:253]     Train net output #0: loss = 1.61787 (* 1 = 1.61787 loss)
I0521 00:06:49.520648  9674 sgd_solver.cpp:106] Iteration 3298, lr = 0.0025
I0521 00:06:57.593227  9674 solver.cpp:237] Iteration 3332, loss = 1.55873
I0521 00:06:57.593369  9674 solver.cpp:253]     Train net output #0: loss = 1.55873 (* 1 = 1.55873 loss)
I0521 00:06:57.593384  9674 sgd_solver.cpp:106] Iteration 3332, lr = 0.0025
I0521 00:07:05.664935  9674 solver.cpp:237] Iteration 3366, loss = 1.66044
I0521 00:07:05.664978  9674 solver.cpp:253]     Train net output #0: loss = 1.66044 (* 1 = 1.66044 loss)
I0521 00:07:05.664994  9674 sgd_solver.cpp:106] Iteration 3366, lr = 0.0025
I0521 00:07:13.500618  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_3400.caffemodel
I0521 00:07:13.707744  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_3400.solverstate
I0521 00:07:13.807060  9674 solver.cpp:237] Iteration 3400, loss = 1.60206
I0521 00:07:13.807109  9674 solver.cpp:253]     Train net output #0: loss = 1.60206 (* 1 = 1.60206 loss)
I0521 00:07:13.807132  9674 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0521 00:07:14.756072  9674 solver.cpp:341] Iteration 3405, Testing net (#0)
I0521 00:08:00.017604  9674 solver.cpp:409]     Test net output #0: accuracy = 0.703643
I0521 00:08:00.017770  9674 solver.cpp:409]     Test net output #1: loss = 1.0214 (* 1 = 1.0214 loss)
I0521 00:08:00.799384  9674 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_3409.caffemodel
I0521 00:08:01.006283  9674 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710_iter_3409.solverstate
I0521 00:08:01.034675  9674 solver.cpp:326] Optimization Done.
I0521 00:08:01.034703  9674 caffe.cpp:215] Optimization Done.
Application 11236004 resources: utime ~1255s, stime ~225s, Rss ~5328968, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_440_2016-05-20T11.20.48.641710.solver"
	User time (seconds): 0.58
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:59.55
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15081
	Voluntary context switches: 2798
	Involuntary context switches: 74
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
