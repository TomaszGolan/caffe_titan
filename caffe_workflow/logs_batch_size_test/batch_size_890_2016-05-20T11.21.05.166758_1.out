2806384
I0521 09:07:20.739276  6389 caffe.cpp:184] Using GPUs 0
I0521 09:07:21.165086  6389 solver.cpp:48] Initializing solver from parameters: 
test_iter: 168
test_interval: 337
base_lr: 0.0025
display: 16
max_iter: 1685
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 168
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758.prototxt"
I0521 09:07:21.166797  6389 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758.prototxt
I0521 09:07:21.177209  6389 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 09:07:21.177269  6389 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 09:07:21.177614  6389 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 890
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:07:21.177824  6389 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:07:21.177850  6389 net.cpp:106] Creating Layer data_hdf5
I0521 09:07:21.177865  6389 net.cpp:411] data_hdf5 -> data
I0521 09:07:21.177898  6389 net.cpp:411] data_hdf5 -> label
I0521 09:07:21.177930  6389 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 09:07:21.179230  6389 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 09:07:21.181430  6389 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 09:07:42.787538  6389 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 09:07:42.792620  6389 net.cpp:150] Setting up data_hdf5
I0521 09:07:42.792660  6389 net.cpp:157] Top shape: 890 1 127 50 (5651500)
I0521 09:07:42.792675  6389 net.cpp:157] Top shape: 890 (890)
I0521 09:07:42.792686  6389 net.cpp:165] Memory required for data: 22609560
I0521 09:07:42.792701  6389 layer_factory.hpp:77] Creating layer conv1
I0521 09:07:42.792734  6389 net.cpp:106] Creating Layer conv1
I0521 09:07:42.792745  6389 net.cpp:454] conv1 <- data
I0521 09:07:42.792768  6389 net.cpp:411] conv1 -> conv1
I0521 09:07:43.163066  6389 net.cpp:150] Setting up conv1
I0521 09:07:43.163113  6389 net.cpp:157] Top shape: 890 12 120 48 (61516800)
I0521 09:07:43.163125  6389 net.cpp:165] Memory required for data: 268676760
I0521 09:07:43.163154  6389 layer_factory.hpp:77] Creating layer relu1
I0521 09:07:43.163177  6389 net.cpp:106] Creating Layer relu1
I0521 09:07:43.163187  6389 net.cpp:454] relu1 <- conv1
I0521 09:07:43.163200  6389 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:07:43.163714  6389 net.cpp:150] Setting up relu1
I0521 09:07:43.163730  6389 net.cpp:157] Top shape: 890 12 120 48 (61516800)
I0521 09:07:43.163741  6389 net.cpp:165] Memory required for data: 514743960
I0521 09:07:43.163753  6389 layer_factory.hpp:77] Creating layer pool1
I0521 09:07:43.163769  6389 net.cpp:106] Creating Layer pool1
I0521 09:07:43.163779  6389 net.cpp:454] pool1 <- conv1
I0521 09:07:43.163794  6389 net.cpp:411] pool1 -> pool1
I0521 09:07:43.163875  6389 net.cpp:150] Setting up pool1
I0521 09:07:43.163889  6389 net.cpp:157] Top shape: 890 12 60 48 (30758400)
I0521 09:07:43.163899  6389 net.cpp:165] Memory required for data: 637777560
I0521 09:07:43.163910  6389 layer_factory.hpp:77] Creating layer conv2
I0521 09:07:43.163933  6389 net.cpp:106] Creating Layer conv2
I0521 09:07:43.163943  6389 net.cpp:454] conv2 <- pool1
I0521 09:07:43.163955  6389 net.cpp:411] conv2 -> conv2
I0521 09:07:43.166620  6389 net.cpp:150] Setting up conv2
I0521 09:07:43.166648  6389 net.cpp:157] Top shape: 890 20 54 46 (44215200)
I0521 09:07:43.166659  6389 net.cpp:165] Memory required for data: 814638360
I0521 09:07:43.166678  6389 layer_factory.hpp:77] Creating layer relu2
I0521 09:07:43.166693  6389 net.cpp:106] Creating Layer relu2
I0521 09:07:43.166703  6389 net.cpp:454] relu2 <- conv2
I0521 09:07:43.166717  6389 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:07:43.167045  6389 net.cpp:150] Setting up relu2
I0521 09:07:43.167060  6389 net.cpp:157] Top shape: 890 20 54 46 (44215200)
I0521 09:07:43.167070  6389 net.cpp:165] Memory required for data: 991499160
I0521 09:07:43.167080  6389 layer_factory.hpp:77] Creating layer pool2
I0521 09:07:43.167093  6389 net.cpp:106] Creating Layer pool2
I0521 09:07:43.167103  6389 net.cpp:454] pool2 <- conv2
I0521 09:07:43.167129  6389 net.cpp:411] pool2 -> pool2
I0521 09:07:43.167197  6389 net.cpp:150] Setting up pool2
I0521 09:07:43.167212  6389 net.cpp:157] Top shape: 890 20 27 46 (22107600)
I0521 09:07:43.167222  6389 net.cpp:165] Memory required for data: 1079929560
I0521 09:07:43.167229  6389 layer_factory.hpp:77] Creating layer conv3
I0521 09:07:43.167248  6389 net.cpp:106] Creating Layer conv3
I0521 09:07:43.167258  6389 net.cpp:454] conv3 <- pool2
I0521 09:07:43.167271  6389 net.cpp:411] conv3 -> conv3
I0521 09:07:43.169181  6389 net.cpp:150] Setting up conv3
I0521 09:07:43.169205  6389 net.cpp:157] Top shape: 890 28 22 44 (24122560)
I0521 09:07:43.169217  6389 net.cpp:165] Memory required for data: 1176419800
I0521 09:07:43.169235  6389 layer_factory.hpp:77] Creating layer relu3
I0521 09:07:43.169251  6389 net.cpp:106] Creating Layer relu3
I0521 09:07:43.169261  6389 net.cpp:454] relu3 <- conv3
I0521 09:07:43.169275  6389 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:07:43.169752  6389 net.cpp:150] Setting up relu3
I0521 09:07:43.169770  6389 net.cpp:157] Top shape: 890 28 22 44 (24122560)
I0521 09:07:43.169780  6389 net.cpp:165] Memory required for data: 1272910040
I0521 09:07:43.169790  6389 layer_factory.hpp:77] Creating layer pool3
I0521 09:07:43.169803  6389 net.cpp:106] Creating Layer pool3
I0521 09:07:43.169813  6389 net.cpp:454] pool3 <- conv3
I0521 09:07:43.169826  6389 net.cpp:411] pool3 -> pool3
I0521 09:07:43.169893  6389 net.cpp:150] Setting up pool3
I0521 09:07:43.169908  6389 net.cpp:157] Top shape: 890 28 11 44 (12061280)
I0521 09:07:43.169917  6389 net.cpp:165] Memory required for data: 1321155160
I0521 09:07:43.169925  6389 layer_factory.hpp:77] Creating layer conv4
I0521 09:07:43.169944  6389 net.cpp:106] Creating Layer conv4
I0521 09:07:43.169953  6389 net.cpp:454] conv4 <- pool3
I0521 09:07:43.169966  6389 net.cpp:411] conv4 -> conv4
I0521 09:07:43.172689  6389 net.cpp:150] Setting up conv4
I0521 09:07:43.172718  6389 net.cpp:157] Top shape: 890 36 6 42 (8074080)
I0521 09:07:43.172727  6389 net.cpp:165] Memory required for data: 1353451480
I0521 09:07:43.172744  6389 layer_factory.hpp:77] Creating layer relu4
I0521 09:07:43.172758  6389 net.cpp:106] Creating Layer relu4
I0521 09:07:43.172768  6389 net.cpp:454] relu4 <- conv4
I0521 09:07:43.172780  6389 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:07:43.173243  6389 net.cpp:150] Setting up relu4
I0521 09:07:43.173259  6389 net.cpp:157] Top shape: 890 36 6 42 (8074080)
I0521 09:07:43.173269  6389 net.cpp:165] Memory required for data: 1385747800
I0521 09:07:43.173280  6389 layer_factory.hpp:77] Creating layer pool4
I0521 09:07:43.173291  6389 net.cpp:106] Creating Layer pool4
I0521 09:07:43.173301  6389 net.cpp:454] pool4 <- conv4
I0521 09:07:43.173315  6389 net.cpp:411] pool4 -> pool4
I0521 09:07:43.173382  6389 net.cpp:150] Setting up pool4
I0521 09:07:43.173395  6389 net.cpp:157] Top shape: 890 36 3 42 (4037040)
I0521 09:07:43.173406  6389 net.cpp:165] Memory required for data: 1401895960
I0521 09:07:43.173416  6389 layer_factory.hpp:77] Creating layer ip1
I0521 09:07:43.173436  6389 net.cpp:106] Creating Layer ip1
I0521 09:07:43.173447  6389 net.cpp:454] ip1 <- pool4
I0521 09:07:43.173460  6389 net.cpp:411] ip1 -> ip1
I0521 09:07:43.188905  6389 net.cpp:150] Setting up ip1
I0521 09:07:43.188932  6389 net.cpp:157] Top shape: 890 196 (174440)
I0521 09:07:43.188947  6389 net.cpp:165] Memory required for data: 1402593720
I0521 09:07:43.188969  6389 layer_factory.hpp:77] Creating layer relu5
I0521 09:07:43.188984  6389 net.cpp:106] Creating Layer relu5
I0521 09:07:43.188995  6389 net.cpp:454] relu5 <- ip1
I0521 09:07:43.189008  6389 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:07:43.189352  6389 net.cpp:150] Setting up relu5
I0521 09:07:43.189368  6389 net.cpp:157] Top shape: 890 196 (174440)
I0521 09:07:43.189376  6389 net.cpp:165] Memory required for data: 1403291480
I0521 09:07:43.189386  6389 layer_factory.hpp:77] Creating layer drop1
I0521 09:07:43.189407  6389 net.cpp:106] Creating Layer drop1
I0521 09:07:43.189417  6389 net.cpp:454] drop1 <- ip1
I0521 09:07:43.189443  6389 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:07:43.189489  6389 net.cpp:150] Setting up drop1
I0521 09:07:43.189502  6389 net.cpp:157] Top shape: 890 196 (174440)
I0521 09:07:43.189513  6389 net.cpp:165] Memory required for data: 1403989240
I0521 09:07:43.189523  6389 layer_factory.hpp:77] Creating layer ip2
I0521 09:07:43.189541  6389 net.cpp:106] Creating Layer ip2
I0521 09:07:43.189551  6389 net.cpp:454] ip2 <- ip1
I0521 09:07:43.189564  6389 net.cpp:411] ip2 -> ip2
I0521 09:07:43.190037  6389 net.cpp:150] Setting up ip2
I0521 09:07:43.190050  6389 net.cpp:157] Top shape: 890 98 (87220)
I0521 09:07:43.190060  6389 net.cpp:165] Memory required for data: 1404338120
I0521 09:07:43.190075  6389 layer_factory.hpp:77] Creating layer relu6
I0521 09:07:43.190088  6389 net.cpp:106] Creating Layer relu6
I0521 09:07:43.190098  6389 net.cpp:454] relu6 <- ip2
I0521 09:07:43.190109  6389 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:07:43.190628  6389 net.cpp:150] Setting up relu6
I0521 09:07:43.190644  6389 net.cpp:157] Top shape: 890 98 (87220)
I0521 09:07:43.190654  6389 net.cpp:165] Memory required for data: 1404687000
I0521 09:07:43.190665  6389 layer_factory.hpp:77] Creating layer drop2
I0521 09:07:43.190676  6389 net.cpp:106] Creating Layer drop2
I0521 09:07:43.190686  6389 net.cpp:454] drop2 <- ip2
I0521 09:07:43.190698  6389 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:07:43.190740  6389 net.cpp:150] Setting up drop2
I0521 09:07:43.190753  6389 net.cpp:157] Top shape: 890 98 (87220)
I0521 09:07:43.190764  6389 net.cpp:165] Memory required for data: 1405035880
I0521 09:07:43.190774  6389 layer_factory.hpp:77] Creating layer ip3
I0521 09:07:43.190788  6389 net.cpp:106] Creating Layer ip3
I0521 09:07:43.190798  6389 net.cpp:454] ip3 <- ip2
I0521 09:07:43.190810  6389 net.cpp:411] ip3 -> ip3
I0521 09:07:43.191023  6389 net.cpp:150] Setting up ip3
I0521 09:07:43.191036  6389 net.cpp:157] Top shape: 890 11 (9790)
I0521 09:07:43.191045  6389 net.cpp:165] Memory required for data: 1405075040
I0521 09:07:43.191061  6389 layer_factory.hpp:77] Creating layer drop3
I0521 09:07:43.191072  6389 net.cpp:106] Creating Layer drop3
I0521 09:07:43.191082  6389 net.cpp:454] drop3 <- ip3
I0521 09:07:43.191094  6389 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:07:43.191133  6389 net.cpp:150] Setting up drop3
I0521 09:07:43.191146  6389 net.cpp:157] Top shape: 890 11 (9790)
I0521 09:07:43.191155  6389 net.cpp:165] Memory required for data: 1405114200
I0521 09:07:43.191165  6389 layer_factory.hpp:77] Creating layer loss
I0521 09:07:43.191184  6389 net.cpp:106] Creating Layer loss
I0521 09:07:43.191193  6389 net.cpp:454] loss <- ip3
I0521 09:07:43.191205  6389 net.cpp:454] loss <- label
I0521 09:07:43.191216  6389 net.cpp:411] loss -> loss
I0521 09:07:43.191233  6389 layer_factory.hpp:77] Creating layer loss
I0521 09:07:43.191887  6389 net.cpp:150] Setting up loss
I0521 09:07:43.191903  6389 net.cpp:157] Top shape: (1)
I0521 09:07:43.191915  6389 net.cpp:160]     with loss weight 1
I0521 09:07:43.191956  6389 net.cpp:165] Memory required for data: 1405114204
I0521 09:07:43.191967  6389 net.cpp:226] loss needs backward computation.
I0521 09:07:43.191977  6389 net.cpp:226] drop3 needs backward computation.
I0521 09:07:43.191987  6389 net.cpp:226] ip3 needs backward computation.
I0521 09:07:43.191998  6389 net.cpp:226] drop2 needs backward computation.
I0521 09:07:43.192008  6389 net.cpp:226] relu6 needs backward computation.
I0521 09:07:43.192018  6389 net.cpp:226] ip2 needs backward computation.
I0521 09:07:43.192028  6389 net.cpp:226] drop1 needs backward computation.
I0521 09:07:43.192036  6389 net.cpp:226] relu5 needs backward computation.
I0521 09:07:43.192046  6389 net.cpp:226] ip1 needs backward computation.
I0521 09:07:43.192056  6389 net.cpp:226] pool4 needs backward computation.
I0521 09:07:43.192067  6389 net.cpp:226] relu4 needs backward computation.
I0521 09:07:43.192076  6389 net.cpp:226] conv4 needs backward computation.
I0521 09:07:43.192087  6389 net.cpp:226] pool3 needs backward computation.
I0521 09:07:43.192106  6389 net.cpp:226] relu3 needs backward computation.
I0521 09:07:43.192113  6389 net.cpp:226] conv3 needs backward computation.
I0521 09:07:43.192126  6389 net.cpp:226] pool2 needs backward computation.
I0521 09:07:43.192136  6389 net.cpp:226] relu2 needs backward computation.
I0521 09:07:43.192145  6389 net.cpp:226] conv2 needs backward computation.
I0521 09:07:43.192157  6389 net.cpp:226] pool1 needs backward computation.
I0521 09:07:43.192167  6389 net.cpp:226] relu1 needs backward computation.
I0521 09:07:43.192176  6389 net.cpp:226] conv1 needs backward computation.
I0521 09:07:43.192188  6389 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:07:43.192196  6389 net.cpp:270] This network produces output loss
I0521 09:07:43.192220  6389 net.cpp:283] Network initialization done.
I0521 09:07:43.193842  6389 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758.prototxt
I0521 09:07:43.193913  6389 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 09:07:43.194267  6389 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 890
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:07:43.194456  6389 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:07:43.194471  6389 net.cpp:106] Creating Layer data_hdf5
I0521 09:07:43.194484  6389 net.cpp:411] data_hdf5 -> data
I0521 09:07:43.194500  6389 net.cpp:411] data_hdf5 -> label
I0521 09:07:43.194517  6389 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 09:07:43.196241  6389 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 09:08:04.579347  6389 net.cpp:150] Setting up data_hdf5
I0521 09:08:04.579514  6389 net.cpp:157] Top shape: 890 1 127 50 (5651500)
I0521 09:08:04.579529  6389 net.cpp:157] Top shape: 890 (890)
I0521 09:08:04.579541  6389 net.cpp:165] Memory required for data: 22609560
I0521 09:08:04.579555  6389 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 09:08:04.579583  6389 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 09:08:04.579594  6389 net.cpp:454] label_data_hdf5_1_split <- label
I0521 09:08:04.579608  6389 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 09:08:04.579630  6389 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 09:08:04.579704  6389 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 09:08:04.579718  6389 net.cpp:157] Top shape: 890 (890)
I0521 09:08:04.579730  6389 net.cpp:157] Top shape: 890 (890)
I0521 09:08:04.579740  6389 net.cpp:165] Memory required for data: 22616680
I0521 09:08:04.579749  6389 layer_factory.hpp:77] Creating layer conv1
I0521 09:08:04.579771  6389 net.cpp:106] Creating Layer conv1
I0521 09:08:04.579780  6389 net.cpp:454] conv1 <- data
I0521 09:08:04.579795  6389 net.cpp:411] conv1 -> conv1
I0521 09:08:04.581708  6389 net.cpp:150] Setting up conv1
I0521 09:08:04.581745  6389 net.cpp:157] Top shape: 890 12 120 48 (61516800)
I0521 09:08:04.581756  6389 net.cpp:165] Memory required for data: 268683880
I0521 09:08:04.581778  6389 layer_factory.hpp:77] Creating layer relu1
I0521 09:08:04.581792  6389 net.cpp:106] Creating Layer relu1
I0521 09:08:04.581802  6389 net.cpp:454] relu1 <- conv1
I0521 09:08:04.581815  6389 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:08:04.582314  6389 net.cpp:150] Setting up relu1
I0521 09:08:04.582330  6389 net.cpp:157] Top shape: 890 12 120 48 (61516800)
I0521 09:08:04.582340  6389 net.cpp:165] Memory required for data: 514751080
I0521 09:08:04.582350  6389 layer_factory.hpp:77] Creating layer pool1
I0521 09:08:04.582366  6389 net.cpp:106] Creating Layer pool1
I0521 09:08:04.582376  6389 net.cpp:454] pool1 <- conv1
I0521 09:08:04.582389  6389 net.cpp:411] pool1 -> pool1
I0521 09:08:04.582464  6389 net.cpp:150] Setting up pool1
I0521 09:08:04.582478  6389 net.cpp:157] Top shape: 890 12 60 48 (30758400)
I0521 09:08:04.582487  6389 net.cpp:165] Memory required for data: 637784680
I0521 09:08:04.582495  6389 layer_factory.hpp:77] Creating layer conv2
I0521 09:08:04.582514  6389 net.cpp:106] Creating Layer conv2
I0521 09:08:04.582523  6389 net.cpp:454] conv2 <- pool1
I0521 09:08:04.582538  6389 net.cpp:411] conv2 -> conv2
I0521 09:08:04.584451  6389 net.cpp:150] Setting up conv2
I0521 09:08:04.584475  6389 net.cpp:157] Top shape: 890 20 54 46 (44215200)
I0521 09:08:04.584486  6389 net.cpp:165] Memory required for data: 814645480
I0521 09:08:04.584504  6389 layer_factory.hpp:77] Creating layer relu2
I0521 09:08:04.584518  6389 net.cpp:106] Creating Layer relu2
I0521 09:08:04.584528  6389 net.cpp:454] relu2 <- conv2
I0521 09:08:04.584540  6389 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:08:04.584877  6389 net.cpp:150] Setting up relu2
I0521 09:08:04.584890  6389 net.cpp:157] Top shape: 890 20 54 46 (44215200)
I0521 09:08:04.584900  6389 net.cpp:165] Memory required for data: 991506280
I0521 09:08:04.584910  6389 layer_factory.hpp:77] Creating layer pool2
I0521 09:08:04.584923  6389 net.cpp:106] Creating Layer pool2
I0521 09:08:04.584933  6389 net.cpp:454] pool2 <- conv2
I0521 09:08:04.584945  6389 net.cpp:411] pool2 -> pool2
I0521 09:08:04.585017  6389 net.cpp:150] Setting up pool2
I0521 09:08:04.585031  6389 net.cpp:157] Top shape: 890 20 27 46 (22107600)
I0521 09:08:04.585041  6389 net.cpp:165] Memory required for data: 1079936680
I0521 09:08:04.585050  6389 layer_factory.hpp:77] Creating layer conv3
I0521 09:08:04.585070  6389 net.cpp:106] Creating Layer conv3
I0521 09:08:04.585080  6389 net.cpp:454] conv3 <- pool2
I0521 09:08:04.585094  6389 net.cpp:411] conv3 -> conv3
I0521 09:08:04.587067  6389 net.cpp:150] Setting up conv3
I0521 09:08:04.587091  6389 net.cpp:157] Top shape: 890 28 22 44 (24122560)
I0521 09:08:04.587103  6389 net.cpp:165] Memory required for data: 1176426920
I0521 09:08:04.587136  6389 layer_factory.hpp:77] Creating layer relu3
I0521 09:08:04.587149  6389 net.cpp:106] Creating Layer relu3
I0521 09:08:04.587159  6389 net.cpp:454] relu3 <- conv3
I0521 09:08:04.587172  6389 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:08:04.587641  6389 net.cpp:150] Setting up relu3
I0521 09:08:04.587656  6389 net.cpp:157] Top shape: 890 28 22 44 (24122560)
I0521 09:08:04.587667  6389 net.cpp:165] Memory required for data: 1272917160
I0521 09:08:04.587677  6389 layer_factory.hpp:77] Creating layer pool3
I0521 09:08:04.587690  6389 net.cpp:106] Creating Layer pool3
I0521 09:08:04.587699  6389 net.cpp:454] pool3 <- conv3
I0521 09:08:04.587713  6389 net.cpp:411] pool3 -> pool3
I0521 09:08:04.587785  6389 net.cpp:150] Setting up pool3
I0521 09:08:04.587797  6389 net.cpp:157] Top shape: 890 28 11 44 (12061280)
I0521 09:08:04.587807  6389 net.cpp:165] Memory required for data: 1321162280
I0521 09:08:04.587816  6389 layer_factory.hpp:77] Creating layer conv4
I0521 09:08:04.587832  6389 net.cpp:106] Creating Layer conv4
I0521 09:08:04.587842  6389 net.cpp:454] conv4 <- pool3
I0521 09:08:04.587857  6389 net.cpp:411] conv4 -> conv4
I0521 09:08:04.589920  6389 net.cpp:150] Setting up conv4
I0521 09:08:04.589943  6389 net.cpp:157] Top shape: 890 36 6 42 (8074080)
I0521 09:08:04.589956  6389 net.cpp:165] Memory required for data: 1353458600
I0521 09:08:04.589972  6389 layer_factory.hpp:77] Creating layer relu4
I0521 09:08:04.589984  6389 net.cpp:106] Creating Layer relu4
I0521 09:08:04.589994  6389 net.cpp:454] relu4 <- conv4
I0521 09:08:04.590008  6389 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:08:04.590477  6389 net.cpp:150] Setting up relu4
I0521 09:08:04.590493  6389 net.cpp:157] Top shape: 890 36 6 42 (8074080)
I0521 09:08:04.590503  6389 net.cpp:165] Memory required for data: 1385754920
I0521 09:08:04.590513  6389 layer_factory.hpp:77] Creating layer pool4
I0521 09:08:04.590526  6389 net.cpp:106] Creating Layer pool4
I0521 09:08:04.590536  6389 net.cpp:454] pool4 <- conv4
I0521 09:08:04.590549  6389 net.cpp:411] pool4 -> pool4
I0521 09:08:04.590621  6389 net.cpp:150] Setting up pool4
I0521 09:08:04.590634  6389 net.cpp:157] Top shape: 890 36 3 42 (4037040)
I0521 09:08:04.590644  6389 net.cpp:165] Memory required for data: 1401903080
I0521 09:08:04.590654  6389 layer_factory.hpp:77] Creating layer ip1
I0521 09:08:04.590667  6389 net.cpp:106] Creating Layer ip1
I0521 09:08:04.590678  6389 net.cpp:454] ip1 <- pool4
I0521 09:08:04.590692  6389 net.cpp:411] ip1 -> ip1
I0521 09:08:04.606161  6389 net.cpp:150] Setting up ip1
I0521 09:08:04.606189  6389 net.cpp:157] Top shape: 890 196 (174440)
I0521 09:08:04.606201  6389 net.cpp:165] Memory required for data: 1402600840
I0521 09:08:04.606223  6389 layer_factory.hpp:77] Creating layer relu5
I0521 09:08:04.606238  6389 net.cpp:106] Creating Layer relu5
I0521 09:08:04.606248  6389 net.cpp:454] relu5 <- ip1
I0521 09:08:04.606262  6389 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:08:04.606606  6389 net.cpp:150] Setting up relu5
I0521 09:08:04.606621  6389 net.cpp:157] Top shape: 890 196 (174440)
I0521 09:08:04.606631  6389 net.cpp:165] Memory required for data: 1403298600
I0521 09:08:04.606640  6389 layer_factory.hpp:77] Creating layer drop1
I0521 09:08:04.606659  6389 net.cpp:106] Creating Layer drop1
I0521 09:08:04.606669  6389 net.cpp:454] drop1 <- ip1
I0521 09:08:04.606683  6389 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:08:04.606726  6389 net.cpp:150] Setting up drop1
I0521 09:08:04.606739  6389 net.cpp:157] Top shape: 890 196 (174440)
I0521 09:08:04.606748  6389 net.cpp:165] Memory required for data: 1403996360
I0521 09:08:04.606758  6389 layer_factory.hpp:77] Creating layer ip2
I0521 09:08:04.606773  6389 net.cpp:106] Creating Layer ip2
I0521 09:08:04.606783  6389 net.cpp:454] ip2 <- ip1
I0521 09:08:04.606797  6389 net.cpp:411] ip2 -> ip2
I0521 09:08:04.607276  6389 net.cpp:150] Setting up ip2
I0521 09:08:04.607290  6389 net.cpp:157] Top shape: 890 98 (87220)
I0521 09:08:04.607300  6389 net.cpp:165] Memory required for data: 1404345240
I0521 09:08:04.607327  6389 layer_factory.hpp:77] Creating layer relu6
I0521 09:08:04.607341  6389 net.cpp:106] Creating Layer relu6
I0521 09:08:04.607350  6389 net.cpp:454] relu6 <- ip2
I0521 09:08:04.607362  6389 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:08:04.607890  6389 net.cpp:150] Setting up relu6
I0521 09:08:04.607908  6389 net.cpp:157] Top shape: 890 98 (87220)
I0521 09:08:04.607918  6389 net.cpp:165] Memory required for data: 1404694120
I0521 09:08:04.607928  6389 layer_factory.hpp:77] Creating layer drop2
I0521 09:08:04.607941  6389 net.cpp:106] Creating Layer drop2
I0521 09:08:04.607950  6389 net.cpp:454] drop2 <- ip2
I0521 09:08:04.607964  6389 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:08:04.608007  6389 net.cpp:150] Setting up drop2
I0521 09:08:04.608021  6389 net.cpp:157] Top shape: 890 98 (87220)
I0521 09:08:04.608031  6389 net.cpp:165] Memory required for data: 1405043000
I0521 09:08:04.608041  6389 layer_factory.hpp:77] Creating layer ip3
I0521 09:08:04.608054  6389 net.cpp:106] Creating Layer ip3
I0521 09:08:04.608064  6389 net.cpp:454] ip3 <- ip2
I0521 09:08:04.608078  6389 net.cpp:411] ip3 -> ip3
I0521 09:08:04.608299  6389 net.cpp:150] Setting up ip3
I0521 09:08:04.608314  6389 net.cpp:157] Top shape: 890 11 (9790)
I0521 09:08:04.608322  6389 net.cpp:165] Memory required for data: 1405082160
I0521 09:08:04.608337  6389 layer_factory.hpp:77] Creating layer drop3
I0521 09:08:04.608350  6389 net.cpp:106] Creating Layer drop3
I0521 09:08:04.608361  6389 net.cpp:454] drop3 <- ip3
I0521 09:08:04.608373  6389 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:08:04.608414  6389 net.cpp:150] Setting up drop3
I0521 09:08:04.608427  6389 net.cpp:157] Top shape: 890 11 (9790)
I0521 09:08:04.608436  6389 net.cpp:165] Memory required for data: 1405121320
I0521 09:08:04.608445  6389 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 09:08:04.608458  6389 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 09:08:04.608467  6389 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 09:08:04.608480  6389 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 09:08:04.608495  6389 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 09:08:04.608567  6389 net.cpp:150] Setting up ip3_drop3_0_split
I0521 09:08:04.608580  6389 net.cpp:157] Top shape: 890 11 (9790)
I0521 09:08:04.608593  6389 net.cpp:157] Top shape: 890 11 (9790)
I0521 09:08:04.608603  6389 net.cpp:165] Memory required for data: 1405199640
I0521 09:08:04.608610  6389 layer_factory.hpp:77] Creating layer accuracy
I0521 09:08:04.608633  6389 net.cpp:106] Creating Layer accuracy
I0521 09:08:04.608644  6389 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 09:08:04.608654  6389 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 09:08:04.608667  6389 net.cpp:411] accuracy -> accuracy
I0521 09:08:04.608691  6389 net.cpp:150] Setting up accuracy
I0521 09:08:04.608705  6389 net.cpp:157] Top shape: (1)
I0521 09:08:04.608713  6389 net.cpp:165] Memory required for data: 1405199644
I0521 09:08:04.608723  6389 layer_factory.hpp:77] Creating layer loss
I0521 09:08:04.608737  6389 net.cpp:106] Creating Layer loss
I0521 09:08:04.608747  6389 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 09:08:04.608758  6389 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 09:08:04.608770  6389 net.cpp:411] loss -> loss
I0521 09:08:04.608788  6389 layer_factory.hpp:77] Creating layer loss
I0521 09:08:04.609283  6389 net.cpp:150] Setting up loss
I0521 09:08:04.609297  6389 net.cpp:157] Top shape: (1)
I0521 09:08:04.609308  6389 net.cpp:160]     with loss weight 1
I0521 09:08:04.609325  6389 net.cpp:165] Memory required for data: 1405199648
I0521 09:08:04.609335  6389 net.cpp:226] loss needs backward computation.
I0521 09:08:04.609346  6389 net.cpp:228] accuracy does not need backward computation.
I0521 09:08:04.609357  6389 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 09:08:04.609369  6389 net.cpp:226] drop3 needs backward computation.
I0521 09:08:04.609376  6389 net.cpp:226] ip3 needs backward computation.
I0521 09:08:04.609386  6389 net.cpp:226] drop2 needs backward computation.
I0521 09:08:04.609405  6389 net.cpp:226] relu6 needs backward computation.
I0521 09:08:04.609416  6389 net.cpp:226] ip2 needs backward computation.
I0521 09:08:04.609426  6389 net.cpp:226] drop1 needs backward computation.
I0521 09:08:04.609434  6389 net.cpp:226] relu5 needs backward computation.
I0521 09:08:04.609444  6389 net.cpp:226] ip1 needs backward computation.
I0521 09:08:04.609454  6389 net.cpp:226] pool4 needs backward computation.
I0521 09:08:04.609464  6389 net.cpp:226] relu4 needs backward computation.
I0521 09:08:04.609474  6389 net.cpp:226] conv4 needs backward computation.
I0521 09:08:04.609484  6389 net.cpp:226] pool3 needs backward computation.
I0521 09:08:04.609493  6389 net.cpp:226] relu3 needs backward computation.
I0521 09:08:04.609503  6389 net.cpp:226] conv3 needs backward computation.
I0521 09:08:04.609513  6389 net.cpp:226] pool2 needs backward computation.
I0521 09:08:04.609524  6389 net.cpp:226] relu2 needs backward computation.
I0521 09:08:04.609534  6389 net.cpp:226] conv2 needs backward computation.
I0521 09:08:04.609544  6389 net.cpp:226] pool1 needs backward computation.
I0521 09:08:04.609555  6389 net.cpp:226] relu1 needs backward computation.
I0521 09:08:04.609563  6389 net.cpp:226] conv1 needs backward computation.
I0521 09:08:04.609575  6389 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 09:08:04.609586  6389 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:08:04.609596  6389 net.cpp:270] This network produces output accuracy
I0521 09:08:04.609606  6389 net.cpp:270] This network produces output loss
I0521 09:08:04.609633  6389 net.cpp:283] Network initialization done.
I0521 09:08:04.609812  6389 solver.cpp:60] Solver scaffolding done.
I0521 09:08:04.610947  6389 caffe.cpp:212] Starting Optimization
I0521 09:08:04.610965  6389 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 09:08:04.610980  6389 solver.cpp:289] Learning Rate Policy: fixed
I0521 09:08:04.612220  6389 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 09:08:50.548359  6389 solver.cpp:409]     Test net output #0: accuracy = 0.144369
I0521 09:08:50.548521  6389 solver.cpp:409]     Test net output #1: loss = 2.39905 (* 1 = 2.39905 loss)
I0521 09:08:50.711720  6389 solver.cpp:237] Iteration 0, loss = 2.40139
I0521 09:08:50.711756  6389 solver.cpp:253]     Train net output #0: loss = 2.40139 (* 1 = 2.40139 loss)
I0521 09:08:50.711774  6389 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 09:08:58.360617  6389 solver.cpp:237] Iteration 16, loss = 2.3911
I0521 09:08:58.360661  6389 solver.cpp:253]     Train net output #0: loss = 2.3911 (* 1 = 2.3911 loss)
I0521 09:08:58.360678  6389 sgd_solver.cpp:106] Iteration 16, lr = 0.0025
I0521 09:09:06.014019  6389 solver.cpp:237] Iteration 32, loss = 2.37786
I0521 09:09:06.014052  6389 solver.cpp:253]     Train net output #0: loss = 2.37786 (* 1 = 2.37786 loss)
I0521 09:09:06.014071  6389 sgd_solver.cpp:106] Iteration 32, lr = 0.0025
I0521 09:09:13.660653  6389 solver.cpp:237] Iteration 48, loss = 2.36295
I0521 09:09:13.660686  6389 solver.cpp:253]     Train net output #0: loss = 2.36295 (* 1 = 2.36295 loss)
I0521 09:09:13.660703  6389 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0521 09:09:21.308522  6389 solver.cpp:237] Iteration 64, loss = 2.35405
I0521 09:09:21.308666  6389 solver.cpp:253]     Train net output #0: loss = 2.35405 (* 1 = 2.35405 loss)
I0521 09:09:21.308681  6389 sgd_solver.cpp:106] Iteration 64, lr = 0.0025
I0521 09:09:28.959856  6389 solver.cpp:237] Iteration 80, loss = 2.33502
I0521 09:09:28.959887  6389 solver.cpp:253]     Train net output #0: loss = 2.33502 (* 1 = 2.33502 loss)
I0521 09:09:28.959904  6389 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0521 09:09:36.609616  6389 solver.cpp:237] Iteration 96, loss = 2.32875
I0521 09:09:36.609648  6389 solver.cpp:253]     Train net output #0: loss = 2.32875 (* 1 = 2.32875 loss)
I0521 09:09:36.609663  6389 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 09:10:06.422297  6389 solver.cpp:237] Iteration 112, loss = 2.32553
I0521 09:10:06.422466  6389 solver.cpp:253]     Train net output #0: loss = 2.32553 (* 1 = 2.32553 loss)
I0521 09:10:06.422482  6389 sgd_solver.cpp:106] Iteration 112, lr = 0.0025
I0521 09:10:14.070708  6389 solver.cpp:237] Iteration 128, loss = 2.3097
I0521 09:10:14.070740  6389 solver.cpp:253]     Train net output #0: loss = 2.3097 (* 1 = 2.3097 loss)
I0521 09:10:14.070762  6389 sgd_solver.cpp:106] Iteration 128, lr = 0.0025
I0521 09:10:21.720216  6389 solver.cpp:237] Iteration 144, loss = 2.28912
I0521 09:10:21.720249  6389 solver.cpp:253]     Train net output #0: loss = 2.28912 (* 1 = 2.28912 loss)
I0521 09:10:21.720265  6389 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 09:10:29.369642  6389 solver.cpp:237] Iteration 160, loss = 2.28957
I0521 09:10:29.369675  6389 solver.cpp:253]     Train net output #0: loss = 2.28957 (* 1 = 2.28957 loss)
I0521 09:10:29.369691  6389 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 09:10:32.716326  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_168.caffemodel
I0521 09:10:33.095091  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_168.solverstate
I0521 09:10:37.090334  6389 solver.cpp:237] Iteration 176, loss = 2.27462
I0521 09:10:37.090483  6389 solver.cpp:253]     Train net output #0: loss = 2.27462 (* 1 = 2.27462 loss)
I0521 09:10:37.090497  6389 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 09:10:44.744846  6389 solver.cpp:237] Iteration 192, loss = 2.24448
I0521 09:10:44.744879  6389 solver.cpp:253]     Train net output #0: loss = 2.24448 (* 1 = 2.24448 loss)
I0521 09:10:44.744892  6389 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 09:10:52.397716  6389 solver.cpp:237] Iteration 208, loss = 2.26023
I0521 09:10:52.397754  6389 solver.cpp:253]     Train net output #0: loss = 2.26023 (* 1 = 2.26023 loss)
I0521 09:10:52.397770  6389 sgd_solver.cpp:106] Iteration 208, lr = 0.0025
I0521 09:11:22.205512  6389 solver.cpp:237] Iteration 224, loss = 2.20983
I0521 09:11:22.205663  6389 solver.cpp:253]     Train net output #0: loss = 2.20983 (* 1 = 2.20983 loss)
I0521 09:11:22.205678  6389 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 09:11:29.857524  6389 solver.cpp:237] Iteration 240, loss = 2.22432
I0521 09:11:29.857558  6389 solver.cpp:253]     Train net output #0: loss = 2.22432 (* 1 = 2.22432 loss)
I0521 09:11:29.857573  6389 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 09:11:37.514381  6389 solver.cpp:237] Iteration 256, loss = 2.14071
I0521 09:11:37.514415  6389 solver.cpp:253]     Train net output #0: loss = 2.14071 (* 1 = 2.14071 loss)
I0521 09:11:37.514431  6389 sgd_solver.cpp:106] Iteration 256, lr = 0.0025
I0521 09:11:45.168174  6389 solver.cpp:237] Iteration 272, loss = 2.1528
I0521 09:11:45.168207  6389 solver.cpp:253]     Train net output #0: loss = 2.1528 (* 1 = 2.1528 loss)
I0521 09:11:45.168225  6389 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 09:11:52.828213  6389 solver.cpp:237] Iteration 288, loss = 2.12431
I0521 09:11:52.828369  6389 solver.cpp:253]     Train net output #0: loss = 2.12431 (* 1 = 2.12431 loss)
I0521 09:11:52.828383  6389 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 09:12:00.478117  6389 solver.cpp:237] Iteration 304, loss = 2.11525
I0521 09:12:00.478150  6389 solver.cpp:253]     Train net output #0: loss = 2.11525 (* 1 = 2.11525 loss)
I0521 09:12:00.478164  6389 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0521 09:12:08.138278  6389 solver.cpp:237] Iteration 320, loss = 2.06074
I0521 09:12:08.138310  6389 solver.cpp:253]     Train net output #0: loss = 2.06074 (* 1 = 2.06074 loss)
I0521 09:12:08.138327  6389 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 09:12:15.314470  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_336.caffemodel
I0521 09:12:15.689970  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_336.solverstate
I0521 09:12:15.857590  6389 solver.cpp:237] Iteration 336, loss = 2.05206
I0521 09:12:15.857637  6389 solver.cpp:253]     Train net output #0: loss = 2.05206 (* 1 = 2.05206 loss)
I0521 09:12:15.857651  6389 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 09:12:15.858157  6389 solver.cpp:341] Iteration 337, Testing net (#0)
I0521 09:13:01.247282  6389 solver.cpp:409]     Test net output #0: accuracy = 0.519556
I0521 09:13:01.247442  6389 solver.cpp:409]     Test net output #1: loss = 1.83265 (* 1 = 1.83265 loss)
I0521 09:13:30.694336  6389 solver.cpp:237] Iteration 352, loss = 2.03487
I0521 09:13:30.694386  6389 solver.cpp:253]     Train net output #0: loss = 2.03487 (* 1 = 2.03487 loss)
I0521 09:13:30.694402  6389 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 09:13:38.334658  6389 solver.cpp:237] Iteration 368, loss = 2.05946
I0521 09:13:38.334802  6389 solver.cpp:253]     Train net output #0: loss = 2.05946 (* 1 = 2.05946 loss)
I0521 09:13:38.334816  6389 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0521 09:13:45.984472  6389 solver.cpp:237] Iteration 384, loss = 1.95783
I0521 09:13:45.984503  6389 solver.cpp:253]     Train net output #0: loss = 1.95783 (* 1 = 1.95783 loss)
I0521 09:13:45.984519  6389 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 09:13:53.631911  6389 solver.cpp:237] Iteration 400, loss = 1.98548
I0521 09:13:53.631945  6389 solver.cpp:253]     Train net output #0: loss = 1.98548 (* 1 = 1.98548 loss)
I0521 09:13:53.631968  6389 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 09:14:01.284768  6389 solver.cpp:237] Iteration 416, loss = 1.95221
I0521 09:14:01.284801  6389 solver.cpp:253]     Train net output #0: loss = 1.95221 (* 1 = 1.95221 loss)
I0521 09:14:01.284818  6389 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 09:14:08.931278  6389 solver.cpp:237] Iteration 432, loss = 1.97266
I0521 09:14:08.931418  6389 solver.cpp:253]     Train net output #0: loss = 1.97266 (* 1 = 1.97266 loss)
I0521 09:14:08.931432  6389 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 09:14:16.569368  6389 solver.cpp:237] Iteration 448, loss = 1.9596
I0521 09:14:16.569401  6389 solver.cpp:253]     Train net output #0: loss = 1.9596 (* 1 = 1.9596 loss)
I0521 09:14:16.569418  6389 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 09:14:46.433478  6389 solver.cpp:237] Iteration 464, loss = 1.95295
I0521 09:14:46.433650  6389 solver.cpp:253]     Train net output #0: loss = 1.95295 (* 1 = 1.95295 loss)
I0521 09:14:46.433665  6389 sgd_solver.cpp:106] Iteration 464, lr = 0.0025
I0521 09:14:54.081425  6389 solver.cpp:237] Iteration 480, loss = 1.8752
I0521 09:14:54.081459  6389 solver.cpp:253]     Train net output #0: loss = 1.8752 (* 1 = 1.8752 loss)
I0521 09:14:54.081478  6389 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 09:15:01.725136  6389 solver.cpp:237] Iteration 496, loss = 1.91104
I0521 09:15:01.725168  6389 solver.cpp:253]     Train net output #0: loss = 1.91104 (* 1 = 1.91104 loss)
I0521 09:15:01.725185  6389 sgd_solver.cpp:106] Iteration 496, lr = 0.0025
I0521 09:15:05.072331  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_504.caffemodel
I0521 09:15:05.451180  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_504.solverstate
I0521 09:15:09.443717  6389 solver.cpp:237] Iteration 512, loss = 1.92361
I0521 09:15:09.443765  6389 solver.cpp:253]     Train net output #0: loss = 1.92361 (* 1 = 1.92361 loss)
I0521 09:15:09.443783  6389 sgd_solver.cpp:106] Iteration 512, lr = 0.0025
I0521 09:15:17.085494  6389 solver.cpp:237] Iteration 528, loss = 1.87265
I0521 09:15:17.085635  6389 solver.cpp:253]     Train net output #0: loss = 1.87265 (* 1 = 1.87265 loss)
I0521 09:15:17.085649  6389 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 09:15:24.733253  6389 solver.cpp:237] Iteration 544, loss = 1.91825
I0521 09:15:24.733286  6389 solver.cpp:253]     Train net output #0: loss = 1.91825 (* 1 = 1.91825 loss)
I0521 09:15:24.733304  6389 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 09:15:32.379400  6389 solver.cpp:237] Iteration 560, loss = 1.94692
I0521 09:15:32.379444  6389 solver.cpp:253]     Train net output #0: loss = 1.94692 (* 1 = 1.94692 loss)
I0521 09:15:32.379464  6389 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 09:16:02.226399  6389 solver.cpp:237] Iteration 576, loss = 1.88283
I0521 09:16:02.226557  6389 solver.cpp:253]     Train net output #0: loss = 1.88283 (* 1 = 1.88283 loss)
I0521 09:16:02.226572  6389 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 09:16:09.874114  6389 solver.cpp:237] Iteration 592, loss = 1.83824
I0521 09:16:09.874146  6389 solver.cpp:253]     Train net output #0: loss = 1.83824 (* 1 = 1.83824 loss)
I0521 09:16:09.874163  6389 sgd_solver.cpp:106] Iteration 592, lr = 0.0025
I0521 09:16:17.519948  6389 solver.cpp:237] Iteration 608, loss = 1.8817
I0521 09:16:17.519980  6389 solver.cpp:253]     Train net output #0: loss = 1.8817 (* 1 = 1.8817 loss)
I0521 09:16:17.519994  6389 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 09:16:25.165299  6389 solver.cpp:237] Iteration 624, loss = 1.8428
I0521 09:16:25.165334  6389 solver.cpp:253]     Train net output #0: loss = 1.8428 (* 1 = 1.8428 loss)
I0521 09:16:25.165352  6389 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 09:16:32.808010  6389 solver.cpp:237] Iteration 640, loss = 1.9132
I0521 09:16:32.808153  6389 solver.cpp:253]     Train net output #0: loss = 1.9132 (* 1 = 1.9132 loss)
I0521 09:16:32.808166  6389 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 09:16:40.456142  6389 solver.cpp:237] Iteration 656, loss = 1.84352
I0521 09:16:40.456173  6389 solver.cpp:253]     Train net output #0: loss = 1.84352 (* 1 = 1.84352 loss)
I0521 09:16:40.456192  6389 sgd_solver.cpp:106] Iteration 656, lr = 0.0025
I0521 09:16:47.628262  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_672.caffemodel
I0521 09:16:48.004890  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_672.solverstate
I0521 09:16:48.174373  6389 solver.cpp:237] Iteration 672, loss = 1.79488
I0521 09:16:48.174422  6389 solver.cpp:253]     Train net output #0: loss = 1.79488 (* 1 = 1.79488 loss)
I0521 09:16:48.174438  6389 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 09:16:48.655140  6389 solver.cpp:341] Iteration 674, Testing net (#0)
I0521 09:17:54.993878  6389 solver.cpp:409]     Test net output #0: accuracy = 0.590001
I0521 09:17:54.994046  6389 solver.cpp:409]     Test net output #1: loss = 1.45466 (* 1 = 1.45466 loss)
I0521 09:18:24.021394  6389 solver.cpp:237] Iteration 688, loss = 1.81173
I0521 09:18:24.021443  6389 solver.cpp:253]     Train net output #0: loss = 1.81173 (* 1 = 1.81173 loss)
I0521 09:18:24.021458  6389 sgd_solver.cpp:106] Iteration 688, lr = 0.0025
I0521 09:18:31.662598  6389 solver.cpp:237] Iteration 704, loss = 1.81049
I0521 09:18:31.662751  6389 solver.cpp:253]     Train net output #0: loss = 1.81049 (* 1 = 1.81049 loss)
I0521 09:18:31.662765  6389 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 09:18:39.303354  6389 solver.cpp:237] Iteration 720, loss = 1.821
I0521 09:18:39.303386  6389 solver.cpp:253]     Train net output #0: loss = 1.821 (* 1 = 1.821 loss)
I0521 09:18:39.303401  6389 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 09:18:46.944308  6389 solver.cpp:237] Iteration 736, loss = 1.79237
I0521 09:18:46.944340  6389 solver.cpp:253]     Train net output #0: loss = 1.79237 (* 1 = 1.79237 loss)
I0521 09:18:46.944358  6389 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 09:18:54.588913  6389 solver.cpp:237] Iteration 752, loss = 1.80767
I0521 09:18:54.588953  6389 solver.cpp:253]     Train net output #0: loss = 1.80767 (* 1 = 1.80767 loss)
I0521 09:18:54.588976  6389 sgd_solver.cpp:106] Iteration 752, lr = 0.0025
I0521 09:19:02.229799  6389 solver.cpp:237] Iteration 768, loss = 1.82335
I0521 09:19:02.229936  6389 solver.cpp:253]     Train net output #0: loss = 1.82335 (* 1 = 1.82335 loss)
I0521 09:19:02.229949  6389 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 09:19:09.871500  6389 solver.cpp:237] Iteration 784, loss = 1.82638
I0521 09:19:09.871531  6389 solver.cpp:253]     Train net output #0: loss = 1.82638 (* 1 = 1.82638 loss)
I0521 09:19:09.871548  6389 sgd_solver.cpp:106] Iteration 784, lr = 0.0025
I0521 09:19:39.699331  6389 solver.cpp:237] Iteration 800, loss = 1.80005
I0521 09:19:39.699498  6389 solver.cpp:253]     Train net output #0: loss = 1.80005 (* 1 = 1.80005 loss)
I0521 09:19:39.699512  6389 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 09:19:47.336308  6389 solver.cpp:237] Iteration 816, loss = 1.78327
I0521 09:19:47.336344  6389 solver.cpp:253]     Train net output #0: loss = 1.78327 (* 1 = 1.78327 loss)
I0521 09:19:47.336364  6389 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 09:19:54.973930  6389 solver.cpp:237] Iteration 832, loss = 1.82719
I0521 09:19:54.973963  6389 solver.cpp:253]     Train net output #0: loss = 1.82719 (* 1 = 1.82719 loss)
I0521 09:19:54.973976  6389 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 09:19:58.318109  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_840.caffemodel
I0521 09:19:58.693178  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_840.solverstate
I0521 09:20:02.682083  6389 solver.cpp:237] Iteration 848, loss = 1.84116
I0521 09:20:02.682132  6389 solver.cpp:253]     Train net output #0: loss = 1.84116 (* 1 = 1.84116 loss)
I0521 09:20:02.682147  6389 sgd_solver.cpp:106] Iteration 848, lr = 0.0025
I0521 09:20:10.321938  6389 solver.cpp:237] Iteration 864, loss = 1.79644
I0521 09:20:10.322110  6389 solver.cpp:253]     Train net output #0: loss = 1.79644 (* 1 = 1.79644 loss)
I0521 09:20:10.322124  6389 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 09:20:17.964951  6389 solver.cpp:237] Iteration 880, loss = 1.72714
I0521 09:20:17.964980  6389 solver.cpp:253]     Train net output #0: loss = 1.72714 (* 1 = 1.72714 loss)
I0521 09:20:17.965000  6389 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 09:20:25.598423  6389 solver.cpp:237] Iteration 896, loss = 1.76831
I0521 09:20:25.598455  6389 solver.cpp:253]     Train net output #0: loss = 1.76831 (* 1 = 1.76831 loss)
I0521 09:20:25.598471  6389 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 09:20:55.451113  6389 solver.cpp:237] Iteration 912, loss = 1.72047
I0521 09:20:55.451282  6389 solver.cpp:253]     Train net output #0: loss = 1.72047 (* 1 = 1.72047 loss)
I0521 09:20:55.451297  6389 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 09:21:03.090505  6389 solver.cpp:237] Iteration 928, loss = 1.75395
I0521 09:21:03.090546  6389 solver.cpp:253]     Train net output #0: loss = 1.75395 (* 1 = 1.75395 loss)
I0521 09:21:03.090564  6389 sgd_solver.cpp:106] Iteration 928, lr = 0.0025
I0521 09:21:10.734390  6389 solver.cpp:237] Iteration 944, loss = 1.79612
I0521 09:21:10.734423  6389 solver.cpp:253]     Train net output #0: loss = 1.79612 (* 1 = 1.79612 loss)
I0521 09:21:10.734441  6389 sgd_solver.cpp:106] Iteration 944, lr = 0.0025
I0521 09:21:18.376191  6389 solver.cpp:237] Iteration 960, loss = 1.70805
I0521 09:21:18.376224  6389 solver.cpp:253]     Train net output #0: loss = 1.70805 (* 1 = 1.70805 loss)
I0521 09:21:18.376237  6389 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 09:21:26.018266  6389 solver.cpp:237] Iteration 976, loss = 1.78266
I0521 09:21:26.018415  6389 solver.cpp:253]     Train net output #0: loss = 1.78266 (* 1 = 1.78266 loss)
I0521 09:21:26.018429  6389 sgd_solver.cpp:106] Iteration 976, lr = 0.0025
I0521 09:21:33.660749  6389 solver.cpp:237] Iteration 992, loss = 1.73499
I0521 09:21:33.660780  6389 solver.cpp:253]     Train net output #0: loss = 1.73499 (* 1 = 1.73499 loss)
I0521 09:21:33.660799  6389 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0521 09:21:40.827638  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1008.caffemodel
I0521 09:21:41.200999  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1008.solverstate
I0521 09:21:41.369861  6389 solver.cpp:237] Iteration 1008, loss = 1.69246
I0521 09:21:41.369904  6389 solver.cpp:253]     Train net output #0: loss = 1.69246 (* 1 = 1.69246 loss)
I0521 09:21:41.369920  6389 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 09:21:42.326450  6389 solver.cpp:341] Iteration 1011, Testing net (#0)
I0521 09:22:27.422619  6389 solver.cpp:409]     Test net output #0: accuracy = 0.651217
I0521 09:22:27.422780  6389 solver.cpp:409]     Test net output #1: loss = 1.32111 (* 1 = 1.32111 loss)
I0521 09:22:55.980926  6389 solver.cpp:237] Iteration 1024, loss = 1.68434
I0521 09:22:55.980975  6389 solver.cpp:253]     Train net output #0: loss = 1.68434 (* 1 = 1.68434 loss)
I0521 09:22:55.980991  6389 sgd_solver.cpp:106] Iteration 1024, lr = 0.0025
I0521 09:23:03.626807  6389 solver.cpp:237] Iteration 1040, loss = 1.77
I0521 09:23:03.626970  6389 solver.cpp:253]     Train net output #0: loss = 1.77 (* 1 = 1.77 loss)
I0521 09:23:03.626983  6389 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 09:23:11.272546  6389 solver.cpp:237] Iteration 1056, loss = 1.71674
I0521 09:23:11.272578  6389 solver.cpp:253]     Train net output #0: loss = 1.71674 (* 1 = 1.71674 loss)
I0521 09:23:11.272595  6389 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 09:23:18.915096  6389 solver.cpp:237] Iteration 1072, loss = 1.75908
I0521 09:23:18.915127  6389 solver.cpp:253]     Train net output #0: loss = 1.75908 (* 1 = 1.75908 loss)
I0521 09:23:18.915144  6389 sgd_solver.cpp:106] Iteration 1072, lr = 0.0025
I0521 09:23:26.557487  6389 solver.cpp:237] Iteration 1088, loss = 1.70793
I0521 09:23:26.557520  6389 solver.cpp:253]     Train net output #0: loss = 1.70793 (* 1 = 1.70793 loss)
I0521 09:23:26.557535  6389 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 09:23:34.202857  6389 solver.cpp:237] Iteration 1104, loss = 1.63787
I0521 09:23:34.203016  6389 solver.cpp:253]     Train net output #0: loss = 1.63787 (* 1 = 1.63787 loss)
I0521 09:23:34.203029  6389 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 09:23:41.843552  6389 solver.cpp:237] Iteration 1120, loss = 1.7684
I0521 09:23:41.843583  6389 solver.cpp:253]     Train net output #0: loss = 1.7684 (* 1 = 1.7684 loss)
I0521 09:23:41.843602  6389 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 09:24:11.663770  6389 solver.cpp:237] Iteration 1136, loss = 1.69194
I0521 09:24:11.663939  6389 solver.cpp:253]     Train net output #0: loss = 1.69194 (* 1 = 1.69194 loss)
I0521 09:24:11.663954  6389 sgd_solver.cpp:106] Iteration 1136, lr = 0.0025
I0521 09:24:19.308012  6389 solver.cpp:237] Iteration 1152, loss = 1.78707
I0521 09:24:19.308055  6389 solver.cpp:253]     Train net output #0: loss = 1.78707 (* 1 = 1.78707 loss)
I0521 09:24:19.308073  6389 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 09:24:26.946084  6389 solver.cpp:237] Iteration 1168, loss = 1.71106
I0521 09:24:26.946115  6389 solver.cpp:253]     Train net output #0: loss = 1.71106 (* 1 = 1.71106 loss)
I0521 09:24:26.946132  6389 sgd_solver.cpp:106] Iteration 1168, lr = 0.0025
I0521 09:24:30.288566  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1176.caffemodel
I0521 09:24:30.662044  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1176.solverstate
I0521 09:24:34.649395  6389 solver.cpp:237] Iteration 1184, loss = 1.74592
I0521 09:24:34.649437  6389 solver.cpp:253]     Train net output #0: loss = 1.74592 (* 1 = 1.74592 loss)
I0521 09:24:34.649461  6389 sgd_solver.cpp:106] Iteration 1184, lr = 0.0025
I0521 09:24:42.290630  6389 solver.cpp:237] Iteration 1200, loss = 1.74104
I0521 09:24:42.290779  6389 solver.cpp:253]     Train net output #0: loss = 1.74104 (* 1 = 1.74104 loss)
I0521 09:24:42.290794  6389 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 09:24:49.935441  6389 solver.cpp:237] Iteration 1216, loss = 1.68338
I0521 09:24:49.935472  6389 solver.cpp:253]     Train net output #0: loss = 1.68338 (* 1 = 1.68338 loss)
I0521 09:24:49.935488  6389 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 09:24:57.583770  6389 solver.cpp:237] Iteration 1232, loss = 1.6919
I0521 09:24:57.583802  6389 solver.cpp:253]     Train net output #0: loss = 1.6919 (* 1 = 1.6919 loss)
I0521 09:24:57.583817  6389 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 09:25:27.439723  6389 solver.cpp:237] Iteration 1248, loss = 1.72683
I0521 09:25:27.439903  6389 solver.cpp:253]     Train net output #0: loss = 1.72683 (* 1 = 1.72683 loss)
I0521 09:25:27.439918  6389 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 09:25:35.089845  6389 solver.cpp:237] Iteration 1264, loss = 1.70875
I0521 09:25:35.089877  6389 solver.cpp:253]     Train net output #0: loss = 1.70875 (* 1 = 1.70875 loss)
I0521 09:25:35.089895  6389 sgd_solver.cpp:106] Iteration 1264, lr = 0.0025
I0521 09:25:42.735754  6389 solver.cpp:237] Iteration 1280, loss = 1.68233
I0521 09:25:42.735787  6389 solver.cpp:253]     Train net output #0: loss = 1.68233 (* 1 = 1.68233 loss)
I0521 09:25:42.735802  6389 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 09:25:50.380260  6389 solver.cpp:237] Iteration 1296, loss = 1.77351
I0521 09:25:50.380292  6389 solver.cpp:253]     Train net output #0: loss = 1.77351 (* 1 = 1.77351 loss)
I0521 09:25:50.380309  6389 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 09:25:58.019901  6389 solver.cpp:237] Iteration 1312, loss = 1.76169
I0521 09:25:58.020062  6389 solver.cpp:253]     Train net output #0: loss = 1.76169 (* 1 = 1.76169 loss)
I0521 09:25:58.020076  6389 sgd_solver.cpp:106] Iteration 1312, lr = 0.0025
I0521 09:26:05.660184  6389 solver.cpp:237] Iteration 1328, loss = 1.76312
I0521 09:26:05.660218  6389 solver.cpp:253]     Train net output #0: loss = 1.76312 (* 1 = 1.76312 loss)
I0521 09:26:05.660234  6389 sgd_solver.cpp:106] Iteration 1328, lr = 0.0025
I0521 09:26:12.822926  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1344.caffemodel
I0521 09:26:13.195281  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1344.solverstate
I0521 09:26:13.364207  6389 solver.cpp:237] Iteration 1344, loss = 1.76952
I0521 09:26:13.364254  6389 solver.cpp:253]     Train net output #0: loss = 1.76952 (* 1 = 1.76952 loss)
I0521 09:26:13.364269  6389 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 09:26:14.796252  6389 solver.cpp:341] Iteration 1348, Testing net (#0)
I0521 09:27:21.104452  6389 solver.cpp:409]     Test net output #0: accuracy = 0.659464
I0521 09:27:21.104615  6389 solver.cpp:409]     Test net output #1: loss = 1.19238 (* 1 = 1.19238 loss)
I0521 09:27:49.172065  6389 solver.cpp:237] Iteration 1360, loss = 1.75915
I0521 09:27:49.172116  6389 solver.cpp:253]     Train net output #0: loss = 1.75915 (* 1 = 1.75915 loss)
I0521 09:27:49.172129  6389 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 09:27:56.820482  6389 solver.cpp:237] Iteration 1376, loss = 1.74082
I0521 09:27:56.820636  6389 solver.cpp:253]     Train net output #0: loss = 1.74082 (* 1 = 1.74082 loss)
I0521 09:27:56.820648  6389 sgd_solver.cpp:106] Iteration 1376, lr = 0.0025
I0521 09:28:04.471547  6389 solver.cpp:237] Iteration 1392, loss = 1.66827
I0521 09:28:04.471585  6389 solver.cpp:253]     Train net output #0: loss = 1.66827 (* 1 = 1.66827 loss)
I0521 09:28:04.471606  6389 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 09:28:12.118614  6389 solver.cpp:237] Iteration 1408, loss = 1.67954
I0521 09:28:12.118648  6389 solver.cpp:253]     Train net output #0: loss = 1.67954 (* 1 = 1.67954 loss)
I0521 09:28:12.118664  6389 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 09:28:19.759158  6389 solver.cpp:237] Iteration 1424, loss = 1.70392
I0521 09:28:19.759191  6389 solver.cpp:253]     Train net output #0: loss = 1.70392 (* 1 = 1.70392 loss)
I0521 09:28:19.759207  6389 sgd_solver.cpp:106] Iteration 1424, lr = 0.0025
I0521 09:28:27.402832  6389 solver.cpp:237] Iteration 1440, loss = 1.70054
I0521 09:28:27.402988  6389 solver.cpp:253]     Train net output #0: loss = 1.70054 (* 1 = 1.70054 loss)
I0521 09:28:27.403002  6389 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 09:28:35.047617  6389 solver.cpp:237] Iteration 1456, loss = 1.65454
I0521 09:28:35.047649  6389 solver.cpp:253]     Train net output #0: loss = 1.65454 (* 1 = 1.65454 loss)
I0521 09:28:35.047668  6389 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 09:29:04.841233  6389 solver.cpp:237] Iteration 1472, loss = 1.73585
I0521 09:29:04.841401  6389 solver.cpp:253]     Train net output #0: loss = 1.73585 (* 1 = 1.73585 loss)
I0521 09:29:04.841416  6389 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 09:29:12.493150  6389 solver.cpp:237] Iteration 1488, loss = 1.66519
I0521 09:29:12.493183  6389 solver.cpp:253]     Train net output #0: loss = 1.66519 (* 1 = 1.66519 loss)
I0521 09:29:12.493196  6389 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 09:29:20.136451  6389 solver.cpp:237] Iteration 1504, loss = 1.66883
I0521 09:29:20.136481  6389 solver.cpp:253]     Train net output #0: loss = 1.66883 (* 1 = 1.66883 loss)
I0521 09:29:20.136498  6389 sgd_solver.cpp:106] Iteration 1504, lr = 0.0025
I0521 09:29:23.479676  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1512.caffemodel
I0521 09:29:23.854830  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1512.solverstate
I0521 09:29:27.845139  6389 solver.cpp:237] Iteration 1520, loss = 1.69688
I0521 09:29:27.845194  6389 solver.cpp:253]     Train net output #0: loss = 1.69688 (* 1 = 1.69688 loss)
I0521 09:29:27.845207  6389 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 09:29:35.490372  6389 solver.cpp:237] Iteration 1536, loss = 1.66776
I0521 09:29:35.490531  6389 solver.cpp:253]     Train net output #0: loss = 1.66776 (* 1 = 1.66776 loss)
I0521 09:29:35.490545  6389 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 09:29:43.137814  6389 solver.cpp:237] Iteration 1552, loss = 1.65864
I0521 09:29:43.137857  6389 solver.cpp:253]     Train net output #0: loss = 1.65864 (* 1 = 1.65864 loss)
I0521 09:29:43.137876  6389 sgd_solver.cpp:106] Iteration 1552, lr = 0.0025
I0521 09:29:50.780032  6389 solver.cpp:237] Iteration 1568, loss = 1.75059
I0521 09:29:50.780066  6389 solver.cpp:253]     Train net output #0: loss = 1.75059 (* 1 = 1.75059 loss)
I0521 09:29:50.780081  6389 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 09:30:20.622747  6389 solver.cpp:237] Iteration 1584, loss = 1.64422
I0521 09:30:20.622918  6389 solver.cpp:253]     Train net output #0: loss = 1.64422 (* 1 = 1.64422 loss)
I0521 09:30:20.622932  6389 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 09:30:28.267204  6389 solver.cpp:237] Iteration 1600, loss = 1.61807
I0521 09:30:28.267235  6389 solver.cpp:253]     Train net output #0: loss = 1.61807 (* 1 = 1.61807 loss)
I0521 09:30:28.267253  6389 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 09:30:35.914734  6389 solver.cpp:237] Iteration 1616, loss = 1.67024
I0521 09:30:35.914772  6389 solver.cpp:253]     Train net output #0: loss = 1.67024 (* 1 = 1.67024 loss)
I0521 09:30:35.914794  6389 sgd_solver.cpp:106] Iteration 1616, lr = 0.0025
I0521 09:30:43.565260  6389 solver.cpp:237] Iteration 1632, loss = 1.73267
I0521 09:30:43.565294  6389 solver.cpp:253]     Train net output #0: loss = 1.73267 (* 1 = 1.73267 loss)
I0521 09:30:43.565310  6389 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 09:30:51.212159  6389 solver.cpp:237] Iteration 1648, loss = 1.66332
I0521 09:30:51.212301  6389 solver.cpp:253]     Train net output #0: loss = 1.66332 (* 1 = 1.66332 loss)
I0521 09:30:51.212314  6389 sgd_solver.cpp:106] Iteration 1648, lr = 0.0025
I0521 09:30:58.858649  6389 solver.cpp:237] Iteration 1664, loss = 1.68095
I0521 09:30:58.858692  6389 solver.cpp:253]     Train net output #0: loss = 1.68095 (* 1 = 1.68095 loss)
I0521 09:30:58.858712  6389 sgd_solver.cpp:106] Iteration 1664, lr = 0.0025
I0521 09:31:06.026075  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1680.caffemodel
I0521 09:31:06.401690  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1680.solverstate
I0521 09:31:06.573559  6389 solver.cpp:237] Iteration 1680, loss = 1.71562
I0521 09:31:06.573607  6389 solver.cpp:253]     Train net output #0: loss = 1.71562 (* 1 = 1.71562 loss)
I0521 09:31:06.573624  6389 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 09:31:08.487676  6389 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1685.caffemodel
I0521 09:31:08.867506  6389 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758_iter_1685.solverstate
I0521 09:31:08.896087  6389 solver.cpp:341] Iteration 1685, Testing net (#0)
I0521 09:31:53.976541  6389 solver.cpp:409]     Test net output #0: accuracy = 0.677113
I0521 09:31:53.976714  6389 solver.cpp:409]     Test net output #1: loss = 1.16468 (* 1 = 1.16468 loss)
I0521 09:31:53.976729  6389 solver.cpp:326] Optimization Done.
I0521 09:31:53.976742  6389 caffe.cpp:215] Optimization Done.
Application 11237449 resources: utime ~1250s, stime ~226s, Rss ~5329280, inblocks ~3594475, outblocks ~194566
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_890_2016-05-20T11.21.05.166758.solver"
	User time (seconds): 0.56
	System time (seconds): 0.19
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.10
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 1
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 3364
	Involuntary context switches: 561
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
