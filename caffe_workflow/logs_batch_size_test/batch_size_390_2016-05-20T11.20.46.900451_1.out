2806012
I0520 22:46:24.511783  5513 caffe.cpp:184] Using GPUs 0
I0520 22:46:24.942662  5513 solver.cpp:48] Initializing solver from parameters: 
test_iter: 384
test_interval: 769
base_lr: 0.0025
display: 38
max_iter: 3846
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 384
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451.prototxt"
I0520 22:46:24.944247  5513 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451.prototxt
I0520 22:46:24.956612  5513 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 22:46:24.956671  5513 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 22:46:24.957015  5513 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 390
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:46:24.957195  5513 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:46:24.957219  5513 net.cpp:106] Creating Layer data_hdf5
I0520 22:46:24.957233  5513 net.cpp:411] data_hdf5 -> data
I0520 22:46:24.957267  5513 net.cpp:411] data_hdf5 -> label
I0520 22:46:24.957299  5513 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 22:46:24.972697  5513 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 22:46:24.974905  5513 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 22:46:46.539042  5513 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 22:46:46.544152  5513 net.cpp:150] Setting up data_hdf5
I0520 22:46:46.544191  5513 net.cpp:157] Top shape: 390 1 127 50 (2476500)
I0520 22:46:46.544206  5513 net.cpp:157] Top shape: 390 (390)
I0520 22:46:46.544216  5513 net.cpp:165] Memory required for data: 9907560
I0520 22:46:46.544229  5513 layer_factory.hpp:77] Creating layer conv1
I0520 22:46:46.544265  5513 net.cpp:106] Creating Layer conv1
I0520 22:46:46.544276  5513 net.cpp:454] conv1 <- data
I0520 22:46:46.544299  5513 net.cpp:411] conv1 -> conv1
I0520 22:46:47.614929  5513 net.cpp:150] Setting up conv1
I0520 22:46:47.614976  5513 net.cpp:157] Top shape: 390 12 120 48 (26956800)
I0520 22:46:47.614987  5513 net.cpp:165] Memory required for data: 117734760
I0520 22:46:47.615016  5513 layer_factory.hpp:77] Creating layer relu1
I0520 22:46:47.615037  5513 net.cpp:106] Creating Layer relu1
I0520 22:46:47.615048  5513 net.cpp:454] relu1 <- conv1
I0520 22:46:47.615061  5513 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:46:47.615578  5513 net.cpp:150] Setting up relu1
I0520 22:46:47.615594  5513 net.cpp:157] Top shape: 390 12 120 48 (26956800)
I0520 22:46:47.615604  5513 net.cpp:165] Memory required for data: 225561960
I0520 22:46:47.615615  5513 layer_factory.hpp:77] Creating layer pool1
I0520 22:46:47.615630  5513 net.cpp:106] Creating Layer pool1
I0520 22:46:47.615640  5513 net.cpp:454] pool1 <- conv1
I0520 22:46:47.615653  5513 net.cpp:411] pool1 -> pool1
I0520 22:46:47.615733  5513 net.cpp:150] Setting up pool1
I0520 22:46:47.615747  5513 net.cpp:157] Top shape: 390 12 60 48 (13478400)
I0520 22:46:47.615758  5513 net.cpp:165] Memory required for data: 279475560
I0520 22:46:47.615768  5513 layer_factory.hpp:77] Creating layer conv2
I0520 22:46:47.615790  5513 net.cpp:106] Creating Layer conv2
I0520 22:46:47.615800  5513 net.cpp:454] conv2 <- pool1
I0520 22:46:47.615815  5513 net.cpp:411] conv2 -> conv2
I0520 22:46:47.618520  5513 net.cpp:150] Setting up conv2
I0520 22:46:47.618547  5513 net.cpp:157] Top shape: 390 20 54 46 (19375200)
I0520 22:46:47.618557  5513 net.cpp:165] Memory required for data: 356976360
I0520 22:46:47.618577  5513 layer_factory.hpp:77] Creating layer relu2
I0520 22:46:47.618592  5513 net.cpp:106] Creating Layer relu2
I0520 22:46:47.618602  5513 net.cpp:454] relu2 <- conv2
I0520 22:46:47.618614  5513 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:46:47.618945  5513 net.cpp:150] Setting up relu2
I0520 22:46:47.618959  5513 net.cpp:157] Top shape: 390 20 54 46 (19375200)
I0520 22:46:47.618969  5513 net.cpp:165] Memory required for data: 434477160
I0520 22:46:47.618979  5513 layer_factory.hpp:77] Creating layer pool2
I0520 22:46:47.618993  5513 net.cpp:106] Creating Layer pool2
I0520 22:46:47.619002  5513 net.cpp:454] pool2 <- conv2
I0520 22:46:47.619027  5513 net.cpp:411] pool2 -> pool2
I0520 22:46:47.619096  5513 net.cpp:150] Setting up pool2
I0520 22:46:47.619109  5513 net.cpp:157] Top shape: 390 20 27 46 (9687600)
I0520 22:46:47.619119  5513 net.cpp:165] Memory required for data: 473227560
I0520 22:46:47.619128  5513 layer_factory.hpp:77] Creating layer conv3
I0520 22:46:47.619148  5513 net.cpp:106] Creating Layer conv3
I0520 22:46:47.619158  5513 net.cpp:454] conv3 <- pool2
I0520 22:46:47.619171  5513 net.cpp:411] conv3 -> conv3
I0520 22:46:47.621090  5513 net.cpp:150] Setting up conv3
I0520 22:46:47.621114  5513 net.cpp:157] Top shape: 390 28 22 44 (10570560)
I0520 22:46:47.621125  5513 net.cpp:165] Memory required for data: 515509800
I0520 22:46:47.621145  5513 layer_factory.hpp:77] Creating layer relu3
I0520 22:46:47.621160  5513 net.cpp:106] Creating Layer relu3
I0520 22:46:47.621170  5513 net.cpp:454] relu3 <- conv3
I0520 22:46:47.621182  5513 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:46:47.621650  5513 net.cpp:150] Setting up relu3
I0520 22:46:47.621667  5513 net.cpp:157] Top shape: 390 28 22 44 (10570560)
I0520 22:46:47.621678  5513 net.cpp:165] Memory required for data: 557792040
I0520 22:46:47.621688  5513 layer_factory.hpp:77] Creating layer pool3
I0520 22:46:47.621701  5513 net.cpp:106] Creating Layer pool3
I0520 22:46:47.621711  5513 net.cpp:454] pool3 <- conv3
I0520 22:46:47.621723  5513 net.cpp:411] pool3 -> pool3
I0520 22:46:47.621790  5513 net.cpp:150] Setting up pool3
I0520 22:46:47.621804  5513 net.cpp:157] Top shape: 390 28 11 44 (5285280)
I0520 22:46:47.621814  5513 net.cpp:165] Memory required for data: 578933160
I0520 22:46:47.621824  5513 layer_factory.hpp:77] Creating layer conv4
I0520 22:46:47.621840  5513 net.cpp:106] Creating Layer conv4
I0520 22:46:47.621850  5513 net.cpp:454] conv4 <- pool3
I0520 22:46:47.621862  5513 net.cpp:411] conv4 -> conv4
I0520 22:46:47.624631  5513 net.cpp:150] Setting up conv4
I0520 22:46:47.624655  5513 net.cpp:157] Top shape: 390 36 6 42 (3538080)
I0520 22:46:47.624665  5513 net.cpp:165] Memory required for data: 593085480
I0520 22:46:47.624680  5513 layer_factory.hpp:77] Creating layer relu4
I0520 22:46:47.624696  5513 net.cpp:106] Creating Layer relu4
I0520 22:46:47.624706  5513 net.cpp:454] relu4 <- conv4
I0520 22:46:47.624718  5513 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:46:47.625186  5513 net.cpp:150] Setting up relu4
I0520 22:46:47.625203  5513 net.cpp:157] Top shape: 390 36 6 42 (3538080)
I0520 22:46:47.625213  5513 net.cpp:165] Memory required for data: 607237800
I0520 22:46:47.625223  5513 layer_factory.hpp:77] Creating layer pool4
I0520 22:46:47.625236  5513 net.cpp:106] Creating Layer pool4
I0520 22:46:47.625247  5513 net.cpp:454] pool4 <- conv4
I0520 22:46:47.625259  5513 net.cpp:411] pool4 -> pool4
I0520 22:46:47.625326  5513 net.cpp:150] Setting up pool4
I0520 22:46:47.625340  5513 net.cpp:157] Top shape: 390 36 3 42 (1769040)
I0520 22:46:47.625351  5513 net.cpp:165] Memory required for data: 614313960
I0520 22:46:47.625361  5513 layer_factory.hpp:77] Creating layer ip1
I0520 22:46:47.625382  5513 net.cpp:106] Creating Layer ip1
I0520 22:46:47.625392  5513 net.cpp:454] ip1 <- pool4
I0520 22:46:47.625406  5513 net.cpp:411] ip1 -> ip1
I0520 22:46:47.640877  5513 net.cpp:150] Setting up ip1
I0520 22:46:47.640904  5513 net.cpp:157] Top shape: 390 196 (76440)
I0520 22:46:47.640918  5513 net.cpp:165] Memory required for data: 614619720
I0520 22:46:47.640939  5513 layer_factory.hpp:77] Creating layer relu5
I0520 22:46:47.640954  5513 net.cpp:106] Creating Layer relu5
I0520 22:46:47.640964  5513 net.cpp:454] relu5 <- ip1
I0520 22:46:47.640976  5513 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:46:47.641319  5513 net.cpp:150] Setting up relu5
I0520 22:46:47.641332  5513 net.cpp:157] Top shape: 390 196 (76440)
I0520 22:46:47.641342  5513 net.cpp:165] Memory required for data: 614925480
I0520 22:46:47.641352  5513 layer_factory.hpp:77] Creating layer drop1
I0520 22:46:47.641374  5513 net.cpp:106] Creating Layer drop1
I0520 22:46:47.641384  5513 net.cpp:454] drop1 <- ip1
I0520 22:46:47.641409  5513 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:46:47.641456  5513 net.cpp:150] Setting up drop1
I0520 22:46:47.641470  5513 net.cpp:157] Top shape: 390 196 (76440)
I0520 22:46:47.641480  5513 net.cpp:165] Memory required for data: 615231240
I0520 22:46:47.641489  5513 layer_factory.hpp:77] Creating layer ip2
I0520 22:46:47.641508  5513 net.cpp:106] Creating Layer ip2
I0520 22:46:47.641518  5513 net.cpp:454] ip2 <- ip1
I0520 22:46:47.641532  5513 net.cpp:411] ip2 -> ip2
I0520 22:46:47.641996  5513 net.cpp:150] Setting up ip2
I0520 22:46:47.642009  5513 net.cpp:157] Top shape: 390 98 (38220)
I0520 22:46:47.642019  5513 net.cpp:165] Memory required for data: 615384120
I0520 22:46:47.642035  5513 layer_factory.hpp:77] Creating layer relu6
I0520 22:46:47.642046  5513 net.cpp:106] Creating Layer relu6
I0520 22:46:47.642056  5513 net.cpp:454] relu6 <- ip2
I0520 22:46:47.642068  5513 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:46:47.642591  5513 net.cpp:150] Setting up relu6
I0520 22:46:47.642607  5513 net.cpp:157] Top shape: 390 98 (38220)
I0520 22:46:47.642617  5513 net.cpp:165] Memory required for data: 615537000
I0520 22:46:47.642627  5513 layer_factory.hpp:77] Creating layer drop2
I0520 22:46:47.642640  5513 net.cpp:106] Creating Layer drop2
I0520 22:46:47.642650  5513 net.cpp:454] drop2 <- ip2
I0520 22:46:47.642663  5513 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:46:47.642705  5513 net.cpp:150] Setting up drop2
I0520 22:46:47.642719  5513 net.cpp:157] Top shape: 390 98 (38220)
I0520 22:46:47.642729  5513 net.cpp:165] Memory required for data: 615689880
I0520 22:46:47.642738  5513 layer_factory.hpp:77] Creating layer ip3
I0520 22:46:47.642752  5513 net.cpp:106] Creating Layer ip3
I0520 22:46:47.642761  5513 net.cpp:454] ip3 <- ip2
I0520 22:46:47.642774  5513 net.cpp:411] ip3 -> ip3
I0520 22:46:47.642983  5513 net.cpp:150] Setting up ip3
I0520 22:46:47.642997  5513 net.cpp:157] Top shape: 390 11 (4290)
I0520 22:46:47.643007  5513 net.cpp:165] Memory required for data: 615707040
I0520 22:46:47.643021  5513 layer_factory.hpp:77] Creating layer drop3
I0520 22:46:47.643033  5513 net.cpp:106] Creating Layer drop3
I0520 22:46:47.643043  5513 net.cpp:454] drop3 <- ip3
I0520 22:46:47.643055  5513 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:46:47.643095  5513 net.cpp:150] Setting up drop3
I0520 22:46:47.643107  5513 net.cpp:157] Top shape: 390 11 (4290)
I0520 22:46:47.643118  5513 net.cpp:165] Memory required for data: 615724200
I0520 22:46:47.643127  5513 layer_factory.hpp:77] Creating layer loss
I0520 22:46:47.643146  5513 net.cpp:106] Creating Layer loss
I0520 22:46:47.643157  5513 net.cpp:454] loss <- ip3
I0520 22:46:47.643167  5513 net.cpp:454] loss <- label
I0520 22:46:47.643179  5513 net.cpp:411] loss -> loss
I0520 22:46:47.643198  5513 layer_factory.hpp:77] Creating layer loss
I0520 22:46:47.643842  5513 net.cpp:150] Setting up loss
I0520 22:46:47.643863  5513 net.cpp:157] Top shape: (1)
I0520 22:46:47.643877  5513 net.cpp:160]     with loss weight 1
I0520 22:46:47.643918  5513 net.cpp:165] Memory required for data: 615724204
I0520 22:46:47.643929  5513 net.cpp:226] loss needs backward computation.
I0520 22:46:47.643939  5513 net.cpp:226] drop3 needs backward computation.
I0520 22:46:47.643949  5513 net.cpp:226] ip3 needs backward computation.
I0520 22:46:47.643957  5513 net.cpp:226] drop2 needs backward computation.
I0520 22:46:47.643973  5513 net.cpp:226] relu6 needs backward computation.
I0520 22:46:47.643982  5513 net.cpp:226] ip2 needs backward computation.
I0520 22:46:47.643993  5513 net.cpp:226] drop1 needs backward computation.
I0520 22:46:47.644003  5513 net.cpp:226] relu5 needs backward computation.
I0520 22:46:47.644012  5513 net.cpp:226] ip1 needs backward computation.
I0520 22:46:47.644023  5513 net.cpp:226] pool4 needs backward computation.
I0520 22:46:47.644033  5513 net.cpp:226] relu4 needs backward computation.
I0520 22:46:47.644043  5513 net.cpp:226] conv4 needs backward computation.
I0520 22:46:47.644053  5513 net.cpp:226] pool3 needs backward computation.
I0520 22:46:47.644071  5513 net.cpp:226] relu3 needs backward computation.
I0520 22:46:47.644081  5513 net.cpp:226] conv3 needs backward computation.
I0520 22:46:47.644093  5513 net.cpp:226] pool2 needs backward computation.
I0520 22:46:47.644104  5513 net.cpp:226] relu2 needs backward computation.
I0520 22:46:47.644112  5513 net.cpp:226] conv2 needs backward computation.
I0520 22:46:47.644124  5513 net.cpp:226] pool1 needs backward computation.
I0520 22:46:47.644134  5513 net.cpp:226] relu1 needs backward computation.
I0520 22:46:47.644145  5513 net.cpp:226] conv1 needs backward computation.
I0520 22:46:47.644155  5513 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:46:47.644165  5513 net.cpp:270] This network produces output loss
I0520 22:46:47.644188  5513 net.cpp:283] Network initialization done.
I0520 22:46:47.645778  5513 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451.prototxt
I0520 22:46:47.645849  5513 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 22:46:47.646203  5513 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 390
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:46:47.646392  5513 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:46:47.646407  5513 net.cpp:106] Creating Layer data_hdf5
I0520 22:46:47.646420  5513 net.cpp:411] data_hdf5 -> data
I0520 22:46:47.646437  5513 net.cpp:411] data_hdf5 -> label
I0520 22:46:47.646452  5513 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 22:46:47.647639  5513 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 22:47:09.070654  5513 net.cpp:150] Setting up data_hdf5
I0520 22:47:09.070817  5513 net.cpp:157] Top shape: 390 1 127 50 (2476500)
I0520 22:47:09.070832  5513 net.cpp:157] Top shape: 390 (390)
I0520 22:47:09.070844  5513 net.cpp:165] Memory required for data: 9907560
I0520 22:47:09.070858  5513 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 22:47:09.070886  5513 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 22:47:09.070897  5513 net.cpp:454] label_data_hdf5_1_split <- label
I0520 22:47:09.070912  5513 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 22:47:09.070935  5513 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 22:47:09.071007  5513 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 22:47:09.071020  5513 net.cpp:157] Top shape: 390 (390)
I0520 22:47:09.071033  5513 net.cpp:157] Top shape: 390 (390)
I0520 22:47:09.071043  5513 net.cpp:165] Memory required for data: 9910680
I0520 22:47:09.071053  5513 layer_factory.hpp:77] Creating layer conv1
I0520 22:47:09.071074  5513 net.cpp:106] Creating Layer conv1
I0520 22:47:09.071084  5513 net.cpp:454] conv1 <- data
I0520 22:47:09.071099  5513 net.cpp:411] conv1 -> conv1
I0520 22:47:09.073019  5513 net.cpp:150] Setting up conv1
I0520 22:47:09.073043  5513 net.cpp:157] Top shape: 390 12 120 48 (26956800)
I0520 22:47:09.073055  5513 net.cpp:165] Memory required for data: 117737880
I0520 22:47:09.073076  5513 layer_factory.hpp:77] Creating layer relu1
I0520 22:47:09.073091  5513 net.cpp:106] Creating Layer relu1
I0520 22:47:09.073101  5513 net.cpp:454] relu1 <- conv1
I0520 22:47:09.073113  5513 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:47:09.073611  5513 net.cpp:150] Setting up relu1
I0520 22:47:09.073626  5513 net.cpp:157] Top shape: 390 12 120 48 (26956800)
I0520 22:47:09.073637  5513 net.cpp:165] Memory required for data: 225565080
I0520 22:47:09.073648  5513 layer_factory.hpp:77] Creating layer pool1
I0520 22:47:09.073664  5513 net.cpp:106] Creating Layer pool1
I0520 22:47:09.073675  5513 net.cpp:454] pool1 <- conv1
I0520 22:47:09.073688  5513 net.cpp:411] pool1 -> pool1
I0520 22:47:09.073762  5513 net.cpp:150] Setting up pool1
I0520 22:47:09.073776  5513 net.cpp:157] Top shape: 390 12 60 48 (13478400)
I0520 22:47:09.073786  5513 net.cpp:165] Memory required for data: 279478680
I0520 22:47:09.073796  5513 layer_factory.hpp:77] Creating layer conv2
I0520 22:47:09.073813  5513 net.cpp:106] Creating Layer conv2
I0520 22:47:09.073824  5513 net.cpp:454] conv2 <- pool1
I0520 22:47:09.073838  5513 net.cpp:411] conv2 -> conv2
I0520 22:47:09.075758  5513 net.cpp:150] Setting up conv2
I0520 22:47:09.075780  5513 net.cpp:157] Top shape: 390 20 54 46 (19375200)
I0520 22:47:09.075793  5513 net.cpp:165] Memory required for data: 356979480
I0520 22:47:09.075811  5513 layer_factory.hpp:77] Creating layer relu2
I0520 22:47:09.075825  5513 net.cpp:106] Creating Layer relu2
I0520 22:47:09.075834  5513 net.cpp:454] relu2 <- conv2
I0520 22:47:09.075847  5513 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:47:09.076179  5513 net.cpp:150] Setting up relu2
I0520 22:47:09.076194  5513 net.cpp:157] Top shape: 390 20 54 46 (19375200)
I0520 22:47:09.076203  5513 net.cpp:165] Memory required for data: 434480280
I0520 22:47:09.076213  5513 layer_factory.hpp:77] Creating layer pool2
I0520 22:47:09.076225  5513 net.cpp:106] Creating Layer pool2
I0520 22:47:09.076236  5513 net.cpp:454] pool2 <- conv2
I0520 22:47:09.076248  5513 net.cpp:411] pool2 -> pool2
I0520 22:47:09.076318  5513 net.cpp:150] Setting up pool2
I0520 22:47:09.076333  5513 net.cpp:157] Top shape: 390 20 27 46 (9687600)
I0520 22:47:09.076342  5513 net.cpp:165] Memory required for data: 473230680
I0520 22:47:09.076352  5513 layer_factory.hpp:77] Creating layer conv3
I0520 22:47:09.076371  5513 net.cpp:106] Creating Layer conv3
I0520 22:47:09.076381  5513 net.cpp:454] conv3 <- pool2
I0520 22:47:09.076395  5513 net.cpp:411] conv3 -> conv3
I0520 22:47:09.078358  5513 net.cpp:150] Setting up conv3
I0520 22:47:09.078382  5513 net.cpp:157] Top shape: 390 28 22 44 (10570560)
I0520 22:47:09.078392  5513 net.cpp:165] Memory required for data: 515512920
I0520 22:47:09.078425  5513 layer_factory.hpp:77] Creating layer relu3
I0520 22:47:09.078439  5513 net.cpp:106] Creating Layer relu3
I0520 22:47:09.078449  5513 net.cpp:454] relu3 <- conv3
I0520 22:47:09.078469  5513 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:47:09.078943  5513 net.cpp:150] Setting up relu3
I0520 22:47:09.078959  5513 net.cpp:157] Top shape: 390 28 22 44 (10570560)
I0520 22:47:09.078969  5513 net.cpp:165] Memory required for data: 557795160
I0520 22:47:09.078979  5513 layer_factory.hpp:77] Creating layer pool3
I0520 22:47:09.078992  5513 net.cpp:106] Creating Layer pool3
I0520 22:47:09.079002  5513 net.cpp:454] pool3 <- conv3
I0520 22:47:09.079015  5513 net.cpp:411] pool3 -> pool3
I0520 22:47:09.079087  5513 net.cpp:150] Setting up pool3
I0520 22:47:09.079102  5513 net.cpp:157] Top shape: 390 28 11 44 (5285280)
I0520 22:47:09.079112  5513 net.cpp:165] Memory required for data: 578936280
I0520 22:47:09.079120  5513 layer_factory.hpp:77] Creating layer conv4
I0520 22:47:09.079138  5513 net.cpp:106] Creating Layer conv4
I0520 22:47:09.079149  5513 net.cpp:454] conv4 <- pool3
I0520 22:47:09.079162  5513 net.cpp:411] conv4 -> conv4
I0520 22:47:09.081215  5513 net.cpp:150] Setting up conv4
I0520 22:47:09.081238  5513 net.cpp:157] Top shape: 390 36 6 42 (3538080)
I0520 22:47:09.081250  5513 net.cpp:165] Memory required for data: 593088600
I0520 22:47:09.081265  5513 layer_factory.hpp:77] Creating layer relu4
I0520 22:47:09.081279  5513 net.cpp:106] Creating Layer relu4
I0520 22:47:09.081289  5513 net.cpp:454] relu4 <- conv4
I0520 22:47:09.081301  5513 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:47:09.081770  5513 net.cpp:150] Setting up relu4
I0520 22:47:09.081786  5513 net.cpp:157] Top shape: 390 36 6 42 (3538080)
I0520 22:47:09.081796  5513 net.cpp:165] Memory required for data: 607240920
I0520 22:47:09.081806  5513 layer_factory.hpp:77] Creating layer pool4
I0520 22:47:09.081820  5513 net.cpp:106] Creating Layer pool4
I0520 22:47:09.081830  5513 net.cpp:454] pool4 <- conv4
I0520 22:47:09.081842  5513 net.cpp:411] pool4 -> pool4
I0520 22:47:09.081914  5513 net.cpp:150] Setting up pool4
I0520 22:47:09.081928  5513 net.cpp:157] Top shape: 390 36 3 42 (1769040)
I0520 22:47:09.081938  5513 net.cpp:165] Memory required for data: 614317080
I0520 22:47:09.081948  5513 layer_factory.hpp:77] Creating layer ip1
I0520 22:47:09.081962  5513 net.cpp:106] Creating Layer ip1
I0520 22:47:09.081974  5513 net.cpp:454] ip1 <- pool4
I0520 22:47:09.081989  5513 net.cpp:411] ip1 -> ip1
I0520 22:47:09.097482  5513 net.cpp:150] Setting up ip1
I0520 22:47:09.097510  5513 net.cpp:157] Top shape: 390 196 (76440)
I0520 22:47:09.097522  5513 net.cpp:165] Memory required for data: 614622840
I0520 22:47:09.097543  5513 layer_factory.hpp:77] Creating layer relu5
I0520 22:47:09.097558  5513 net.cpp:106] Creating Layer relu5
I0520 22:47:09.097569  5513 net.cpp:454] relu5 <- ip1
I0520 22:47:09.097584  5513 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:47:09.097929  5513 net.cpp:150] Setting up relu5
I0520 22:47:09.097942  5513 net.cpp:157] Top shape: 390 196 (76440)
I0520 22:47:09.097952  5513 net.cpp:165] Memory required for data: 614928600
I0520 22:47:09.097964  5513 layer_factory.hpp:77] Creating layer drop1
I0520 22:47:09.097982  5513 net.cpp:106] Creating Layer drop1
I0520 22:47:09.097992  5513 net.cpp:454] drop1 <- ip1
I0520 22:47:09.098006  5513 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:47:09.098050  5513 net.cpp:150] Setting up drop1
I0520 22:47:09.098062  5513 net.cpp:157] Top shape: 390 196 (76440)
I0520 22:47:09.098073  5513 net.cpp:165] Memory required for data: 615234360
I0520 22:47:09.098083  5513 layer_factory.hpp:77] Creating layer ip2
I0520 22:47:09.098098  5513 net.cpp:106] Creating Layer ip2
I0520 22:47:09.098107  5513 net.cpp:454] ip2 <- ip1
I0520 22:47:09.098121  5513 net.cpp:411] ip2 -> ip2
I0520 22:47:09.098608  5513 net.cpp:150] Setting up ip2
I0520 22:47:09.098623  5513 net.cpp:157] Top shape: 390 98 (38220)
I0520 22:47:09.098633  5513 net.cpp:165] Memory required for data: 615387240
I0520 22:47:09.098659  5513 layer_factory.hpp:77] Creating layer relu6
I0520 22:47:09.098672  5513 net.cpp:106] Creating Layer relu6
I0520 22:47:09.098683  5513 net.cpp:454] relu6 <- ip2
I0520 22:47:09.098695  5513 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:47:09.099225  5513 net.cpp:150] Setting up relu6
I0520 22:47:09.099248  5513 net.cpp:157] Top shape: 390 98 (38220)
I0520 22:47:09.099258  5513 net.cpp:165] Memory required for data: 615540120
I0520 22:47:09.099269  5513 layer_factory.hpp:77] Creating layer drop2
I0520 22:47:09.099282  5513 net.cpp:106] Creating Layer drop2
I0520 22:47:09.099292  5513 net.cpp:454] drop2 <- ip2
I0520 22:47:09.099304  5513 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:47:09.099349  5513 net.cpp:150] Setting up drop2
I0520 22:47:09.099361  5513 net.cpp:157] Top shape: 390 98 (38220)
I0520 22:47:09.099372  5513 net.cpp:165] Memory required for data: 615693000
I0520 22:47:09.099382  5513 layer_factory.hpp:77] Creating layer ip3
I0520 22:47:09.099396  5513 net.cpp:106] Creating Layer ip3
I0520 22:47:09.099406  5513 net.cpp:454] ip3 <- ip2
I0520 22:47:09.099421  5513 net.cpp:411] ip3 -> ip3
I0520 22:47:09.099642  5513 net.cpp:150] Setting up ip3
I0520 22:47:09.099655  5513 net.cpp:157] Top shape: 390 11 (4290)
I0520 22:47:09.099665  5513 net.cpp:165] Memory required for data: 615710160
I0520 22:47:09.099680  5513 layer_factory.hpp:77] Creating layer drop3
I0520 22:47:09.099694  5513 net.cpp:106] Creating Layer drop3
I0520 22:47:09.099704  5513 net.cpp:454] drop3 <- ip3
I0520 22:47:09.099716  5513 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:47:09.099757  5513 net.cpp:150] Setting up drop3
I0520 22:47:09.099769  5513 net.cpp:157] Top shape: 390 11 (4290)
I0520 22:47:09.099779  5513 net.cpp:165] Memory required for data: 615727320
I0520 22:47:09.099789  5513 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 22:47:09.099802  5513 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 22:47:09.099812  5513 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 22:47:09.099825  5513 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 22:47:09.099840  5513 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 22:47:09.099913  5513 net.cpp:150] Setting up ip3_drop3_0_split
I0520 22:47:09.099927  5513 net.cpp:157] Top shape: 390 11 (4290)
I0520 22:47:09.099939  5513 net.cpp:157] Top shape: 390 11 (4290)
I0520 22:47:09.099949  5513 net.cpp:165] Memory required for data: 615761640
I0520 22:47:09.099957  5513 layer_factory.hpp:77] Creating layer accuracy
I0520 22:47:09.099983  5513 net.cpp:106] Creating Layer accuracy
I0520 22:47:09.099995  5513 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 22:47:09.100009  5513 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 22:47:09.100023  5513 net.cpp:411] accuracy -> accuracy
I0520 22:47:09.100047  5513 net.cpp:150] Setting up accuracy
I0520 22:47:09.100060  5513 net.cpp:157] Top shape: (1)
I0520 22:47:09.100070  5513 net.cpp:165] Memory required for data: 615761644
I0520 22:47:09.100080  5513 layer_factory.hpp:77] Creating layer loss
I0520 22:47:09.100095  5513 net.cpp:106] Creating Layer loss
I0520 22:47:09.100105  5513 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 22:47:09.100116  5513 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 22:47:09.100129  5513 net.cpp:411] loss -> loss
I0520 22:47:09.100147  5513 layer_factory.hpp:77] Creating layer loss
I0520 22:47:09.100636  5513 net.cpp:150] Setting up loss
I0520 22:47:09.100649  5513 net.cpp:157] Top shape: (1)
I0520 22:47:09.100659  5513 net.cpp:160]     with loss weight 1
I0520 22:47:09.100677  5513 net.cpp:165] Memory required for data: 615761648
I0520 22:47:09.100687  5513 net.cpp:226] loss needs backward computation.
I0520 22:47:09.100698  5513 net.cpp:228] accuracy does not need backward computation.
I0520 22:47:09.100709  5513 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 22:47:09.100720  5513 net.cpp:226] drop3 needs backward computation.
I0520 22:47:09.100728  5513 net.cpp:226] ip3 needs backward computation.
I0520 22:47:09.100739  5513 net.cpp:226] drop2 needs backward computation.
I0520 22:47:09.100756  5513 net.cpp:226] relu6 needs backward computation.
I0520 22:47:09.100766  5513 net.cpp:226] ip2 needs backward computation.
I0520 22:47:09.100776  5513 net.cpp:226] drop1 needs backward computation.
I0520 22:47:09.100786  5513 net.cpp:226] relu5 needs backward computation.
I0520 22:47:09.100795  5513 net.cpp:226] ip1 needs backward computation.
I0520 22:47:09.100805  5513 net.cpp:226] pool4 needs backward computation.
I0520 22:47:09.100816  5513 net.cpp:226] relu4 needs backward computation.
I0520 22:47:09.100826  5513 net.cpp:226] conv4 needs backward computation.
I0520 22:47:09.100836  5513 net.cpp:226] pool3 needs backward computation.
I0520 22:47:09.100847  5513 net.cpp:226] relu3 needs backward computation.
I0520 22:47:09.100855  5513 net.cpp:226] conv3 needs backward computation.
I0520 22:47:09.100867  5513 net.cpp:226] pool2 needs backward computation.
I0520 22:47:09.100877  5513 net.cpp:226] relu2 needs backward computation.
I0520 22:47:09.100886  5513 net.cpp:226] conv2 needs backward computation.
I0520 22:47:09.100898  5513 net.cpp:226] pool1 needs backward computation.
I0520 22:47:09.100908  5513 net.cpp:226] relu1 needs backward computation.
I0520 22:47:09.100917  5513 net.cpp:226] conv1 needs backward computation.
I0520 22:47:09.100929  5513 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 22:47:09.100940  5513 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:47:09.100950  5513 net.cpp:270] This network produces output accuracy
I0520 22:47:09.100960  5513 net.cpp:270] This network produces output loss
I0520 22:47:09.100991  5513 net.cpp:283] Network initialization done.
I0520 22:47:09.101124  5513 solver.cpp:60] Solver scaffolding done.
I0520 22:47:09.102254  5513 caffe.cpp:212] Starting Optimization
I0520 22:47:09.102272  5513 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 22:47:09.102285  5513 solver.cpp:289] Learning Rate Policy: fixed
I0520 22:47:09.103543  5513 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 22:47:55.217269  5513 solver.cpp:409]     Test net output #0: accuracy = 0.0529781
I0520 22:47:55.217428  5513 solver.cpp:409]     Test net output #1: loss = 2.39825 (* 1 = 2.39825 loss)
I0520 22:47:55.298279  5513 solver.cpp:237] Iteration 0, loss = 2.40091
I0520 22:47:55.298315  5513 solver.cpp:253]     Train net output #0: loss = 2.40091 (* 1 = 2.40091 loss)
I0520 22:47:55.298332  5513 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 22:48:03.339465  5513 solver.cpp:237] Iteration 38, loss = 2.37485
I0520 22:48:03.339501  5513 solver.cpp:253]     Train net output #0: loss = 2.37485 (* 1 = 2.37485 loss)
I0520 22:48:03.339519  5513 sgd_solver.cpp:106] Iteration 38, lr = 0.0025
I0520 22:48:11.378517  5513 solver.cpp:237] Iteration 76, loss = 2.34132
I0520 22:48:11.378551  5513 solver.cpp:253]     Train net output #0: loss = 2.34132 (* 1 = 2.34132 loss)
I0520 22:48:11.378568  5513 sgd_solver.cpp:106] Iteration 76, lr = 0.0025
I0520 22:48:19.418165  5513 solver.cpp:237] Iteration 114, loss = 2.3422
I0520 22:48:19.418212  5513 solver.cpp:253]     Train net output #0: loss = 2.3422 (* 1 = 2.3422 loss)
I0520 22:48:19.418229  5513 sgd_solver.cpp:106] Iteration 114, lr = 0.0025
I0520 22:48:27.452124  5513 solver.cpp:237] Iteration 152, loss = 2.33283
I0520 22:48:27.452278  5513 solver.cpp:253]     Train net output #0: loss = 2.33283 (* 1 = 2.33283 loss)
I0520 22:48:27.452292  5513 sgd_solver.cpp:106] Iteration 152, lr = 0.0025
I0520 22:48:35.487411  5513 solver.cpp:237] Iteration 190, loss = 2.33109
I0520 22:48:35.487443  5513 solver.cpp:253]     Train net output #0: loss = 2.33109 (* 1 = 2.33109 loss)
I0520 22:48:35.487460  5513 sgd_solver.cpp:106] Iteration 190, lr = 0.0025
I0520 22:48:43.524997  5513 solver.cpp:237] Iteration 228, loss = 2.3383
I0520 22:48:43.525039  5513 solver.cpp:253]     Train net output #0: loss = 2.3383 (* 1 = 2.3383 loss)
I0520 22:48:43.525058  5513 sgd_solver.cpp:106] Iteration 228, lr = 0.0025
I0520 22:49:13.691923  5513 solver.cpp:237] Iteration 266, loss = 2.29619
I0520 22:49:13.692085  5513 solver.cpp:253]     Train net output #0: loss = 2.29619 (* 1 = 2.29619 loss)
I0520 22:49:13.692101  5513 sgd_solver.cpp:106] Iteration 266, lr = 0.0025
I0520 22:49:21.734061  5513 solver.cpp:237] Iteration 304, loss = 2.27414
I0520 22:49:21.734096  5513 solver.cpp:253]     Train net output #0: loss = 2.27414 (* 1 = 2.27414 loss)
I0520 22:49:21.734112  5513 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0520 22:49:29.774464  5513 solver.cpp:237] Iteration 342, loss = 2.25456
I0520 22:49:29.774505  5513 solver.cpp:253]     Train net output #0: loss = 2.25456 (* 1 = 2.25456 loss)
I0520 22:49:29.774523  5513 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0520 22:49:37.818850  5513 solver.cpp:237] Iteration 380, loss = 2.22171
I0520 22:49:37.818877  5513 solver.cpp:253]     Train net output #0: loss = 2.22171 (* 1 = 2.22171 loss)
I0520 22:49:37.818891  5513 sgd_solver.cpp:106] Iteration 380, lr = 0.0025
I0520 22:49:38.455222  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_384.caffemodel
I0520 22:49:38.644130  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_384.solverstate
I0520 22:49:45.925024  5513 solver.cpp:237] Iteration 418, loss = 2.18844
I0520 22:49:45.925173  5513 solver.cpp:253]     Train net output #0: loss = 2.18844 (* 1 = 2.18844 loss)
I0520 22:49:45.925186  5513 sgd_solver.cpp:106] Iteration 418, lr = 0.0025
I0520 22:49:53.964721  5513 solver.cpp:237] Iteration 456, loss = 2.09636
I0520 22:49:53.964761  5513 solver.cpp:253]     Train net output #0: loss = 2.09636 (* 1 = 2.09636 loss)
I0520 22:49:53.964774  5513 sgd_solver.cpp:106] Iteration 456, lr = 0.0025
I0520 22:50:02.009896  5513 solver.cpp:237] Iteration 494, loss = 2.11722
I0520 22:50:02.009928  5513 solver.cpp:253]     Train net output #0: loss = 2.11722 (* 1 = 2.11722 loss)
I0520 22:50:02.009944  5513 sgd_solver.cpp:106] Iteration 494, lr = 0.0025
I0520 22:50:32.210402  5513 solver.cpp:237] Iteration 532, loss = 2.05234
I0520 22:50:32.210561  5513 solver.cpp:253]     Train net output #0: loss = 2.05234 (* 1 = 2.05234 loss)
I0520 22:50:32.210577  5513 sgd_solver.cpp:106] Iteration 532, lr = 0.0025
I0520 22:50:40.257443  5513 solver.cpp:237] Iteration 570, loss = 1.98861
I0520 22:50:40.257477  5513 solver.cpp:253]     Train net output #0: loss = 1.98861 (* 1 = 1.98861 loss)
I0520 22:50:40.257495  5513 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0520 22:50:48.298555  5513 solver.cpp:237] Iteration 608, loss = 1.99673
I0520 22:50:48.298604  5513 solver.cpp:253]     Train net output #0: loss = 1.99673 (* 1 = 1.99673 loss)
I0520 22:50:48.298619  5513 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0520 22:50:56.339349  5513 solver.cpp:237] Iteration 646, loss = 1.92885
I0520 22:50:56.339381  5513 solver.cpp:253]     Train net output #0: loss = 1.92885 (* 1 = 1.92885 loss)
I0520 22:50:56.339399  5513 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0520 22:51:04.380234  5513 solver.cpp:237] Iteration 684, loss = 1.96272
I0520 22:51:04.380381  5513 solver.cpp:253]     Train net output #0: loss = 1.96272 (* 1 = 1.96272 loss)
I0520 22:51:04.380395  5513 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0520 22:51:12.420596  5513 solver.cpp:237] Iteration 722, loss = 1.9419
I0520 22:51:12.420634  5513 solver.cpp:253]     Train net output #0: loss = 1.9419 (* 1 = 1.9419 loss)
I0520 22:51:12.420650  5513 sgd_solver.cpp:106] Iteration 722, lr = 0.0025
I0520 22:51:20.459875  5513 solver.cpp:237] Iteration 760, loss = 1.88974
I0520 22:51:20.459909  5513 solver.cpp:253]     Train net output #0: loss = 1.88974 (* 1 = 1.88974 loss)
I0520 22:51:20.460021  5513 sgd_solver.cpp:106] Iteration 760, lr = 0.0025
I0520 22:51:21.940917  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_768.caffemodel
I0520 22:51:22.127727  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_768.solverstate
I0520 22:51:22.216727  5513 solver.cpp:341] Iteration 769, Testing net (#0)
I0520 22:52:07.588084  5513 solver.cpp:409]     Test net output #0: accuracy = 0.581197
I0520 22:52:07.588243  5513 solver.cpp:409]     Test net output #1: loss = 1.51133 (* 1 = 1.51133 loss)
I0520 22:52:35.989274  5513 solver.cpp:237] Iteration 798, loss = 1.8101
I0520 22:52:35.989325  5513 solver.cpp:253]     Train net output #0: loss = 1.8101 (* 1 = 1.8101 loss)
I0520 22:52:35.989339  5513 sgd_solver.cpp:106] Iteration 798, lr = 0.0025
I0520 22:52:44.033006  5513 solver.cpp:237] Iteration 836, loss = 1.93406
I0520 22:52:44.033151  5513 solver.cpp:253]     Train net output #0: loss = 1.93406 (* 1 = 1.93406 loss)
I0520 22:52:44.033165  5513 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0520 22:52:52.080554  5513 solver.cpp:237] Iteration 874, loss = 1.92237
I0520 22:52:52.080598  5513 solver.cpp:253]     Train net output #0: loss = 1.92237 (* 1 = 1.92237 loss)
I0520 22:52:52.080615  5513 sgd_solver.cpp:106] Iteration 874, lr = 0.0025
I0520 22:53:00.123394  5513 solver.cpp:237] Iteration 912, loss = 1.77769
I0520 22:53:00.123428  5513 solver.cpp:253]     Train net output #0: loss = 1.77769 (* 1 = 1.77769 loss)
I0520 22:53:00.123445  5513 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0520 22:53:08.168126  5513 solver.cpp:237] Iteration 950, loss = 1.83531
I0520 22:53:08.168159  5513 solver.cpp:253]     Train net output #0: loss = 1.83531 (* 1 = 1.83531 loss)
I0520 22:53:08.168175  5513 sgd_solver.cpp:106] Iteration 950, lr = 0.0025
I0520 22:53:16.215657  5513 solver.cpp:237] Iteration 988, loss = 1.84341
I0520 22:53:16.215793  5513 solver.cpp:253]     Train net output #0: loss = 1.84341 (* 1 = 1.84341 loss)
I0520 22:53:16.215807  5513 sgd_solver.cpp:106] Iteration 988, lr = 0.0025
I0520 22:53:46.428829  5513 solver.cpp:237] Iteration 1026, loss = 1.7905
I0520 22:53:46.428995  5513 solver.cpp:253]     Train net output #0: loss = 1.7905 (* 1 = 1.7905 loss)
I0520 22:53:46.429010  5513 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0520 22:53:54.474316  5513 solver.cpp:237] Iteration 1064, loss = 1.86216
I0520 22:53:54.474350  5513 solver.cpp:253]     Train net output #0: loss = 1.86216 (* 1 = 1.86216 loss)
I0520 22:53:54.474367  5513 sgd_solver.cpp:106] Iteration 1064, lr = 0.0025
I0520 22:54:02.518437  5513 solver.cpp:237] Iteration 1102, loss = 1.74191
I0520 22:54:02.518477  5513 solver.cpp:253]     Train net output #0: loss = 1.74191 (* 1 = 1.74191 loss)
I0520 22:54:02.518491  5513 sgd_solver.cpp:106] Iteration 1102, lr = 0.0025
I0520 22:54:10.563880  5513 solver.cpp:237] Iteration 1140, loss = 1.8049
I0520 22:54:10.563916  5513 solver.cpp:253]     Train net output #0: loss = 1.8049 (* 1 = 1.8049 loss)
I0520 22:54:10.563936  5513 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0520 22:54:12.894675  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_1152.caffemodel
I0520 22:54:13.084682  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_1152.solverstate
I0520 22:54:18.680732  5513 solver.cpp:237] Iteration 1178, loss = 1.86854
I0520 22:54:18.680897  5513 solver.cpp:253]     Train net output #0: loss = 1.86854 (* 1 = 1.86854 loss)
I0520 22:54:18.680912  5513 sgd_solver.cpp:106] Iteration 1178, lr = 0.0025
I0520 22:54:26.727584  5513 solver.cpp:237] Iteration 1216, loss = 1.69188
I0520 22:54:26.727617  5513 solver.cpp:253]     Train net output #0: loss = 1.69188 (* 1 = 1.69188 loss)
I0520 22:54:26.727635  5513 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0520 22:54:34.771781  5513 solver.cpp:237] Iteration 1254, loss = 1.71726
I0520 22:54:34.771829  5513 solver.cpp:253]     Train net output #0: loss = 1.71726 (* 1 = 1.71726 loss)
I0520 22:54:34.771845  5513 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0520 22:55:05.042918  5513 solver.cpp:237] Iteration 1292, loss = 1.7187
I0520 22:55:05.043077  5513 solver.cpp:253]     Train net output #0: loss = 1.7187 (* 1 = 1.7187 loss)
I0520 22:55:05.043093  5513 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0520 22:55:13.086925  5513 solver.cpp:237] Iteration 1330, loss = 1.77906
I0520 22:55:13.086959  5513 solver.cpp:253]     Train net output #0: loss = 1.77906 (* 1 = 1.77906 loss)
I0520 22:55:13.086977  5513 sgd_solver.cpp:106] Iteration 1330, lr = 0.0025
I0520 22:55:21.132189  5513 solver.cpp:237] Iteration 1368, loss = 1.88504
I0520 22:55:21.132235  5513 solver.cpp:253]     Train net output #0: loss = 1.88504 (* 1 = 1.88504 loss)
I0520 22:55:21.132249  5513 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0520 22:55:29.177546  5513 solver.cpp:237] Iteration 1406, loss = 1.76262
I0520 22:55:29.177580  5513 solver.cpp:253]     Train net output #0: loss = 1.76262 (* 1 = 1.76262 loss)
I0520 22:55:29.177595  5513 sgd_solver.cpp:106] Iteration 1406, lr = 0.0025
I0520 22:55:37.224485  5513 solver.cpp:237] Iteration 1444, loss = 1.72201
I0520 22:55:37.224630  5513 solver.cpp:253]     Train net output #0: loss = 1.72201 (* 1 = 1.72201 loss)
I0520 22:55:37.224644  5513 sgd_solver.cpp:106] Iteration 1444, lr = 0.0025
I0520 22:55:45.271327  5513 solver.cpp:237] Iteration 1482, loss = 1.8294
I0520 22:55:45.271369  5513 solver.cpp:253]     Train net output #0: loss = 1.8294 (* 1 = 1.8294 loss)
I0520 22:55:45.271386  5513 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0520 22:55:53.313571  5513 solver.cpp:237] Iteration 1520, loss = 1.71389
I0520 22:55:53.313606  5513 solver.cpp:253]     Train net output #0: loss = 1.71389 (* 1 = 1.71389 loss)
I0520 22:55:53.313622  5513 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0520 22:55:56.490515  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_1536.caffemodel
I0520 22:55:56.679126  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_1536.solverstate
I0520 22:55:56.982811  5513 solver.cpp:341] Iteration 1538, Testing net (#0)
I0520 22:57:03.273041  5513 solver.cpp:409]     Test net output #0: accuracy = 0.640071
I0520 22:57:03.273212  5513 solver.cpp:409]     Test net output #1: loss = 1.23835 (* 1 = 1.23835 loss)
I0520 22:57:29.747846  5513 solver.cpp:237] Iteration 1558, loss = 1.67895
I0520 22:57:29.747895  5513 solver.cpp:253]     Train net output #0: loss = 1.67895 (* 1 = 1.67895 loss)
I0520 22:57:29.747911  5513 sgd_solver.cpp:106] Iteration 1558, lr = 0.0025
I0520 22:57:37.793934  5513 solver.cpp:237] Iteration 1596, loss = 1.71584
I0520 22:57:37.794081  5513 solver.cpp:253]     Train net output #0: loss = 1.71584 (* 1 = 1.71584 loss)
I0520 22:57:37.794095  5513 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0520 22:57:45.843304  5513 solver.cpp:237] Iteration 1634, loss = 1.67754
I0520 22:57:45.843338  5513 solver.cpp:253]     Train net output #0: loss = 1.67754 (* 1 = 1.67754 loss)
I0520 22:57:45.843354  5513 sgd_solver.cpp:106] Iteration 1634, lr = 0.0025
I0520 22:57:53.894127  5513 solver.cpp:237] Iteration 1672, loss = 1.64135
I0520 22:57:53.894166  5513 solver.cpp:253]     Train net output #0: loss = 1.64135 (* 1 = 1.64135 loss)
I0520 22:57:53.894187  5513 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0520 22:58:01.940067  5513 solver.cpp:237] Iteration 1710, loss = 1.7634
I0520 22:58:01.940099  5513 solver.cpp:253]     Train net output #0: loss = 1.7634 (* 1 = 1.7634 loss)
I0520 22:58:01.940116  5513 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0520 22:58:09.990326  5513 solver.cpp:237] Iteration 1748, loss = 1.73536
I0520 22:58:09.990468  5513 solver.cpp:253]     Train net output #0: loss = 1.73536 (* 1 = 1.73536 loss)
I0520 22:58:09.990483  5513 sgd_solver.cpp:106] Iteration 1748, lr = 0.0025
I0520 22:58:18.038323  5513 solver.cpp:237] Iteration 1786, loss = 1.63306
I0520 22:58:18.038369  5513 solver.cpp:253]     Train net output #0: loss = 1.63306 (* 1 = 1.63306 loss)
I0520 22:58:18.038385  5513 sgd_solver.cpp:106] Iteration 1786, lr = 0.0025
I0520 22:58:48.225291  5513 solver.cpp:237] Iteration 1824, loss = 1.737
I0520 22:58:48.225455  5513 solver.cpp:253]     Train net output #0: loss = 1.737 (* 1 = 1.737 loss)
I0520 22:58:48.225469  5513 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0520 22:58:56.272450  5513 solver.cpp:237] Iteration 1862, loss = 1.74969
I0520 22:58:56.272485  5513 solver.cpp:253]     Train net output #0: loss = 1.74969 (* 1 = 1.74969 loss)
I0520 22:58:56.272502  5513 sgd_solver.cpp:106] Iteration 1862, lr = 0.0025
I0520 22:59:04.318248  5513 solver.cpp:237] Iteration 1900, loss = 1.56911
I0520 22:59:04.318289  5513 solver.cpp:253]     Train net output #0: loss = 1.56911 (* 1 = 1.56911 loss)
I0520 22:59:04.318305  5513 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0520 22:59:08.342169  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_1920.caffemodel
I0520 22:59:08.530901  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_1920.solverstate
I0520 22:59:12.437175  5513 solver.cpp:237] Iteration 1938, loss = 1.6861
I0520 22:59:12.437222  5513 solver.cpp:253]     Train net output #0: loss = 1.6861 (* 1 = 1.6861 loss)
I0520 22:59:12.437238  5513 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0520 22:59:20.484009  5513 solver.cpp:237] Iteration 1976, loss = 1.7076
I0520 22:59:20.484151  5513 solver.cpp:253]     Train net output #0: loss = 1.7076 (* 1 = 1.7076 loss)
I0520 22:59:20.484164  5513 sgd_solver.cpp:106] Iteration 1976, lr = 0.0025
I0520 22:59:28.533283  5513 solver.cpp:237] Iteration 2014, loss = 1.72035
I0520 22:59:28.533324  5513 solver.cpp:253]     Train net output #0: loss = 1.72035 (* 1 = 1.72035 loss)
I0520 22:59:28.533340  5513 sgd_solver.cpp:106] Iteration 2014, lr = 0.0025
I0520 22:59:58.735292  5513 solver.cpp:237] Iteration 2052, loss = 1.76535
I0520 22:59:58.735462  5513 solver.cpp:253]     Train net output #0: loss = 1.76535 (* 1 = 1.76535 loss)
I0520 22:59:58.735478  5513 sgd_solver.cpp:106] Iteration 2052, lr = 0.0025
I0520 23:00:06.784817  5513 solver.cpp:237] Iteration 2090, loss = 1.62949
I0520 23:00:06.784850  5513 solver.cpp:253]     Train net output #0: loss = 1.62949 (* 1 = 1.62949 loss)
I0520 23:00:06.784868  5513 sgd_solver.cpp:106] Iteration 2090, lr = 0.0025
I0520 23:00:14.832734  5513 solver.cpp:237] Iteration 2128, loss = 1.74131
I0520 23:00:14.832764  5513 solver.cpp:253]     Train net output #0: loss = 1.74131 (* 1 = 1.74131 loss)
I0520 23:00:14.832782  5513 sgd_solver.cpp:106] Iteration 2128, lr = 0.0025
I0520 23:00:22.884315  5513 solver.cpp:237] Iteration 2166, loss = 1.68313
I0520 23:00:22.884353  5513 solver.cpp:253]     Train net output #0: loss = 1.68313 (* 1 = 1.68313 loss)
I0520 23:00:22.884374  5513 sgd_solver.cpp:106] Iteration 2166, lr = 0.0025
I0520 23:00:30.934345  5513 solver.cpp:237] Iteration 2204, loss = 1.59964
I0520 23:00:30.934490  5513 solver.cpp:253]     Train net output #0: loss = 1.59964 (* 1 = 1.59964 loss)
I0520 23:00:30.934504  5513 sgd_solver.cpp:106] Iteration 2204, lr = 0.0025
I0520 23:00:38.986882  5513 solver.cpp:237] Iteration 2242, loss = 1.60864
I0520 23:00:38.986915  5513 solver.cpp:253]     Train net output #0: loss = 1.60864 (* 1 = 1.60864 loss)
I0520 23:00:38.986933  5513 sgd_solver.cpp:106] Iteration 2242, lr = 0.0025
I0520 23:00:47.039849  5513 solver.cpp:237] Iteration 2280, loss = 1.65258
I0520 23:00:47.039896  5513 solver.cpp:253]     Train net output #0: loss = 1.65258 (* 1 = 1.65258 loss)
I0520 23:00:47.039909  5513 sgd_solver.cpp:106] Iteration 2280, lr = 0.0025
I0520 23:00:51.911725  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_2304.caffemodel
I0520 23:00:52.099035  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_2304.solverstate
I0520 23:00:52.611594  5513 solver.cpp:341] Iteration 2307, Testing net (#0)
I0520 23:01:37.648787  5513 solver.cpp:409]     Test net output #0: accuracy = 0.674199
I0520 23:01:37.648947  5513 solver.cpp:409]     Test net output #1: loss = 1.10529 (* 1 = 1.10529 loss)
I0520 23:02:02.158166  5513 solver.cpp:237] Iteration 2318, loss = 1.66226
I0520 23:02:02.158216  5513 solver.cpp:253]     Train net output #0: loss = 1.66226 (* 1 = 1.66226 loss)
I0520 23:02:02.158231  5513 sgd_solver.cpp:106] Iteration 2318, lr = 0.0025
I0520 23:02:10.203178  5513 solver.cpp:237] Iteration 2356, loss = 1.72836
I0520 23:02:10.203328  5513 solver.cpp:253]     Train net output #0: loss = 1.72836 (* 1 = 1.72836 loss)
I0520 23:02:10.203342  5513 sgd_solver.cpp:106] Iteration 2356, lr = 0.0025
I0520 23:02:18.243235  5513 solver.cpp:237] Iteration 2394, loss = 1.6396
I0520 23:02:18.243268  5513 solver.cpp:253]     Train net output #0: loss = 1.6396 (* 1 = 1.6396 loss)
I0520 23:02:18.243284  5513 sgd_solver.cpp:106] Iteration 2394, lr = 0.0025
I0520 23:02:26.284471  5513 solver.cpp:237] Iteration 2432, loss = 1.61009
I0520 23:02:26.284512  5513 solver.cpp:253]     Train net output #0: loss = 1.61009 (* 1 = 1.61009 loss)
I0520 23:02:26.284528  5513 sgd_solver.cpp:106] Iteration 2432, lr = 0.0025
I0520 23:02:34.326185  5513 solver.cpp:237] Iteration 2470, loss = 1.65523
I0520 23:02:34.326220  5513 solver.cpp:253]     Train net output #0: loss = 1.65523 (* 1 = 1.65523 loss)
I0520 23:02:34.326236  5513 sgd_solver.cpp:106] Iteration 2470, lr = 0.0025
I0520 23:02:42.364799  5513 solver.cpp:237] Iteration 2508, loss = 1.72429
I0520 23:02:42.364933  5513 solver.cpp:253]     Train net output #0: loss = 1.72429 (* 1 = 1.72429 loss)
I0520 23:02:42.364948  5513 sgd_solver.cpp:106] Iteration 2508, lr = 0.0025
I0520 23:02:50.407747  5513 solver.cpp:237] Iteration 2546, loss = 1.72939
I0520 23:02:50.407784  5513 solver.cpp:253]     Train net output #0: loss = 1.72939 (* 1 = 1.72939 loss)
I0520 23:02:50.407806  5513 sgd_solver.cpp:106] Iteration 2546, lr = 0.0025
I0520 23:03:20.668192  5513 solver.cpp:237] Iteration 2584, loss = 1.56385
I0520 23:03:20.668361  5513 solver.cpp:253]     Train net output #0: loss = 1.56385 (* 1 = 1.56385 loss)
I0520 23:03:20.668376  5513 sgd_solver.cpp:106] Iteration 2584, lr = 0.0025
I0520 23:03:28.713129  5513 solver.cpp:237] Iteration 2622, loss = 1.57706
I0520 23:03:28.713162  5513 solver.cpp:253]     Train net output #0: loss = 1.57706 (* 1 = 1.57706 loss)
I0520 23:03:28.713179  5513 sgd_solver.cpp:106] Iteration 2622, lr = 0.0025
I0520 23:03:36.756753  5513 solver.cpp:237] Iteration 2660, loss = 1.71818
I0520 23:03:36.756786  5513 solver.cpp:253]     Train net output #0: loss = 1.71818 (* 1 = 1.71818 loss)
I0520 23:03:36.756803  5513 sgd_solver.cpp:106] Iteration 2660, lr = 0.0025
I0520 23:03:42.473703  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_2688.caffemodel
I0520 23:03:42.660362  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_2688.solverstate
I0520 23:03:44.866580  5513 solver.cpp:237] Iteration 2698, loss = 1.65207
I0520 23:03:44.866621  5513 solver.cpp:253]     Train net output #0: loss = 1.65207 (* 1 = 1.65207 loss)
I0520 23:03:44.866642  5513 sgd_solver.cpp:106] Iteration 2698, lr = 0.0025
I0520 23:03:52.907526  5513 solver.cpp:237] Iteration 2736, loss = 1.61862
I0520 23:03:52.907671  5513 solver.cpp:253]     Train net output #0: loss = 1.61862 (* 1 = 1.61862 loss)
I0520 23:03:52.907685  5513 sgd_solver.cpp:106] Iteration 2736, lr = 0.0025
I0520 23:04:00.948416  5513 solver.cpp:237] Iteration 2774, loss = 1.6739
I0520 23:04:00.948449  5513 solver.cpp:253]     Train net output #0: loss = 1.6739 (* 1 = 1.6739 loss)
I0520 23:04:00.948467  5513 sgd_solver.cpp:106] Iteration 2774, lr = 0.0025
I0520 23:04:08.988121  5513 solver.cpp:237] Iteration 2812, loss = 1.60255
I0520 23:04:08.988168  5513 solver.cpp:253]     Train net output #0: loss = 1.60255 (* 1 = 1.60255 loss)
I0520 23:04:08.988184  5513 sgd_solver.cpp:106] Iteration 2812, lr = 0.0025
I0520 23:04:39.171969  5513 solver.cpp:237] Iteration 2850, loss = 1.52335
I0520 23:04:39.172135  5513 solver.cpp:253]     Train net output #0: loss = 1.52335 (* 1 = 1.52335 loss)
I0520 23:04:39.172150  5513 sgd_solver.cpp:106] Iteration 2850, lr = 0.0025
I0520 23:04:47.217630  5513 solver.cpp:237] Iteration 2888, loss = 1.66247
I0520 23:04:47.217664  5513 solver.cpp:253]     Train net output #0: loss = 1.66247 (* 1 = 1.66247 loss)
I0520 23:04:47.217681  5513 sgd_solver.cpp:106] Iteration 2888, lr = 0.0025
I0520 23:04:55.259251  5513 solver.cpp:237] Iteration 2926, loss = 1.6351
I0520 23:04:55.259294  5513 solver.cpp:253]     Train net output #0: loss = 1.6351 (* 1 = 1.6351 loss)
I0520 23:04:55.259313  5513 sgd_solver.cpp:106] Iteration 2926, lr = 0.0025
I0520 23:05:03.300894  5513 solver.cpp:237] Iteration 2964, loss = 1.61655
I0520 23:05:03.300927  5513 solver.cpp:253]     Train net output #0: loss = 1.61655 (* 1 = 1.61655 loss)
I0520 23:05:03.300943  5513 sgd_solver.cpp:106] Iteration 2964, lr = 0.0025
I0520 23:05:11.342031  5513 solver.cpp:237] Iteration 3002, loss = 1.63145
I0520 23:05:11.342170  5513 solver.cpp:253]     Train net output #0: loss = 1.63145 (* 1 = 1.63145 loss)
I0520 23:05:11.342185  5513 sgd_solver.cpp:106] Iteration 3002, lr = 0.0025
I0520 23:05:19.387280  5513 solver.cpp:237] Iteration 3040, loss = 1.65228
I0520 23:05:19.387323  5513 solver.cpp:253]     Train net output #0: loss = 1.65228 (* 1 = 1.65228 loss)
I0520 23:05:19.387341  5513 sgd_solver.cpp:106] Iteration 3040, lr = 0.0025
I0520 23:05:25.948729  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3072.caffemodel
I0520 23:05:26.134702  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3072.solverstate
I0520 23:05:26.859679  5513 solver.cpp:341] Iteration 3076, Testing net (#0)
I0520 23:06:33.052672  5513 solver.cpp:409]     Test net output #0: accuracy = 0.694865
I0520 23:06:33.052851  5513 solver.cpp:409]     Test net output #1: loss = 1.02483 (* 1 = 1.02483 loss)
I0520 23:06:55.686357  5513 solver.cpp:237] Iteration 3078, loss = 1.58571
I0520 23:06:55.686408  5513 solver.cpp:253]     Train net output #0: loss = 1.58571 (* 1 = 1.58571 loss)
I0520 23:06:55.686424  5513 sgd_solver.cpp:106] Iteration 3078, lr = 0.0025
I0520 23:07:03.729406  5513 solver.cpp:237] Iteration 3116, loss = 1.61375
I0520 23:07:03.729568  5513 solver.cpp:253]     Train net output #0: loss = 1.61375 (* 1 = 1.61375 loss)
I0520 23:07:03.729583  5513 sgd_solver.cpp:106] Iteration 3116, lr = 0.0025
I0520 23:07:11.773550  5513 solver.cpp:237] Iteration 3154, loss = 1.55665
I0520 23:07:11.773582  5513 solver.cpp:253]     Train net output #0: loss = 1.55665 (* 1 = 1.55665 loss)
I0520 23:07:11.773599  5513 sgd_solver.cpp:106] Iteration 3154, lr = 0.0025
I0520 23:07:19.815129  5513 solver.cpp:237] Iteration 3192, loss = 1.58471
I0520 23:07:19.815162  5513 solver.cpp:253]     Train net output #0: loss = 1.58471 (* 1 = 1.58471 loss)
I0520 23:07:19.815178  5513 sgd_solver.cpp:106] Iteration 3192, lr = 0.0025
I0520 23:07:27.860640  5513 solver.cpp:237] Iteration 3230, loss = 1.59237
I0520 23:07:27.860677  5513 solver.cpp:253]     Train net output #0: loss = 1.59237 (* 1 = 1.59237 loss)
I0520 23:07:27.860697  5513 sgd_solver.cpp:106] Iteration 3230, lr = 0.0025
I0520 23:07:35.902856  5513 solver.cpp:237] Iteration 3268, loss = 1.57916
I0520 23:07:35.902995  5513 solver.cpp:253]     Train net output #0: loss = 1.57916 (* 1 = 1.57916 loss)
I0520 23:07:35.903008  5513 sgd_solver.cpp:106] Iteration 3268, lr = 0.0025
I0520 23:07:43.948287  5513 solver.cpp:237] Iteration 3306, loss = 1.63255
I0520 23:07:43.948318  5513 solver.cpp:253]     Train net output #0: loss = 1.63255 (* 1 = 1.63255 loss)
I0520 23:07:43.948335  5513 sgd_solver.cpp:106] Iteration 3306, lr = 0.0025
I0520 23:08:14.169242  5513 solver.cpp:237] Iteration 3344, loss = 1.60219
I0520 23:08:14.169409  5513 solver.cpp:253]     Train net output #0: loss = 1.60219 (* 1 = 1.60219 loss)
I0520 23:08:14.169425  5513 sgd_solver.cpp:106] Iteration 3344, lr = 0.0025
I0520 23:08:22.211807  5513 solver.cpp:237] Iteration 3382, loss = 1.60624
I0520 23:08:22.211839  5513 solver.cpp:253]     Train net output #0: loss = 1.60624 (* 1 = 1.60624 loss)
I0520 23:08:22.211858  5513 sgd_solver.cpp:106] Iteration 3382, lr = 0.0025
I0520 23:08:30.254823  5513 solver.cpp:237] Iteration 3420, loss = 1.56832
I0520 23:08:30.254858  5513 solver.cpp:253]     Train net output #0: loss = 1.56832 (* 1 = 1.56832 loss)
I0520 23:08:30.254874  5513 sgd_solver.cpp:106] Iteration 3420, lr = 0.0025
I0520 23:08:37.665868  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3456.caffemodel
I0520 23:08:37.856895  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3456.solverstate
I0520 23:08:38.373657  5513 solver.cpp:237] Iteration 3458, loss = 1.57343
I0520 23:08:38.373705  5513 solver.cpp:253]     Train net output #0: loss = 1.57343 (* 1 = 1.57343 loss)
I0520 23:08:38.373723  5513 sgd_solver.cpp:106] Iteration 3458, lr = 0.0025
I0520 23:08:46.419461  5513 solver.cpp:237] Iteration 3496, loss = 1.58224
I0520 23:08:46.419618  5513 solver.cpp:253]     Train net output #0: loss = 1.58224 (* 1 = 1.58224 loss)
I0520 23:08:46.419632  5513 sgd_solver.cpp:106] Iteration 3496, lr = 0.0025
I0520 23:08:54.462718  5513 solver.cpp:237] Iteration 3534, loss = 1.52552
I0520 23:08:54.462750  5513 solver.cpp:253]     Train net output #0: loss = 1.52552 (* 1 = 1.52552 loss)
I0520 23:08:54.462767  5513 sgd_solver.cpp:106] Iteration 3534, lr = 0.0025
I0520 23:09:02.509160  5513 solver.cpp:237] Iteration 3572, loss = 1.48239
I0520 23:09:02.509192  5513 solver.cpp:253]     Train net output #0: loss = 1.48239 (* 1 = 1.48239 loss)
I0520 23:09:02.509209  5513 sgd_solver.cpp:106] Iteration 3572, lr = 0.0025
I0520 23:09:32.742772  5513 solver.cpp:237] Iteration 3610, loss = 1.54323
I0520 23:09:32.742938  5513 solver.cpp:253]     Train net output #0: loss = 1.54323 (* 1 = 1.54323 loss)
I0520 23:09:32.742954  5513 sgd_solver.cpp:106] Iteration 3610, lr = 0.0025
I0520 23:09:40.790902  5513 solver.cpp:237] Iteration 3648, loss = 1.62169
I0520 23:09:40.790935  5513 solver.cpp:253]     Train net output #0: loss = 1.62169 (* 1 = 1.62169 loss)
I0520 23:09:40.790951  5513 sgd_solver.cpp:106] Iteration 3648, lr = 0.0025
I0520 23:09:48.833950  5513 solver.cpp:237] Iteration 3686, loss = 1.47673
I0520 23:09:48.833984  5513 solver.cpp:253]     Train net output #0: loss = 1.47673 (* 1 = 1.47673 loss)
I0520 23:09:48.833998  5513 sgd_solver.cpp:106] Iteration 3686, lr = 0.0025
I0520 23:09:56.878551  5513 solver.cpp:237] Iteration 3724, loss = 1.54174
I0520 23:09:56.878593  5513 solver.cpp:253]     Train net output #0: loss = 1.54174 (* 1 = 1.54174 loss)
I0520 23:09:56.878609  5513 sgd_solver.cpp:106] Iteration 3724, lr = 0.0025
I0520 23:10:04.922947  5513 solver.cpp:237] Iteration 3762, loss = 1.6023
I0520 23:10:04.923089  5513 solver.cpp:253]     Train net output #0: loss = 1.6023 (* 1 = 1.6023 loss)
I0520 23:10:04.923102  5513 sgd_solver.cpp:106] Iteration 3762, lr = 0.0025
I0520 23:10:12.965543  5513 solver.cpp:237] Iteration 3800, loss = 1.5105
I0520 23:10:12.965577  5513 solver.cpp:253]     Train net output #0: loss = 1.5105 (* 1 = 1.5105 loss)
I0520 23:10:12.965593  5513 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0520 23:10:21.010651  5513 solver.cpp:237] Iteration 3838, loss = 1.51956
I0520 23:10:21.010700  5513 solver.cpp:253]     Train net output #0: loss = 1.51956 (* 1 = 1.51956 loss)
I0520 23:10:21.010715  5513 sgd_solver.cpp:106] Iteration 3838, lr = 0.0025
I0520 23:10:21.222241  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3840.caffemodel
I0520 23:10:21.416951  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3840.solverstate
I0520 23:10:22.354576  5513 solver.cpp:341] Iteration 3845, Testing net (#0)
I0520 23:11:07.731729  5513 solver.cpp:409]     Test net output #0: accuracy = 0.71664
I0520 23:11:07.731892  5513 solver.cpp:409]     Test net output #1: loss = 0.958462 (* 1 = 0.958462 loss)
I0520 23:11:07.795469  5513 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3846.caffemodel
I0520 23:11:08.003550  5513 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451_iter_3846.solverstate
I0520 23:11:08.031913  5513 solver.cpp:326] Optimization Done.
I0520 23:11:08.031940  5513 caffe.cpp:215] Optimization Done.
Application 11235718 resources: utime ~1258s, stime ~226s, Rss ~5329452, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_390_2016-05-20T11.20.46.900451.solver"
	User time (seconds): 0.56
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:49.91
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15084
	Voluntary context switches: 2780
	Involuntary context switches: 170
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
