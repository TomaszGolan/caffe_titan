2805107
I0520 13:24:00.966768 31949 caffe.cpp:184] Using GPUs 0
I0520 13:24:01.392004 31949 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1875
test_interval: 3750
base_lr: 0.0025
display: 187
max_iter: 18750
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1875
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834.prototxt"
I0520 13:24:01.393961 31949 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834.prototxt
I0520 13:24:01.411478 31949 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 13:24:01.411538 31949 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 13:24:01.411885 31949 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 80
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 13:24:01.412061 31949 layer_factory.hpp:77] Creating layer data_hdf5
I0520 13:24:01.412086 31949 net.cpp:106] Creating Layer data_hdf5
I0520 13:24:01.412099 31949 net.cpp:411] data_hdf5 -> data
I0520 13:24:01.412132 31949 net.cpp:411] data_hdf5 -> label
I0520 13:24:01.412164 31949 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 13:24:01.413393 31949 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 13:24:01.415596 31949 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 13:24:22.947504 31949 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 13:24:22.952625 31949 net.cpp:150] Setting up data_hdf5
I0520 13:24:22.952664 31949 net.cpp:157] Top shape: 80 1 127 50 (508000)
I0520 13:24:22.952679 31949 net.cpp:157] Top shape: 80 (80)
I0520 13:24:22.952692 31949 net.cpp:165] Memory required for data: 2032320
I0520 13:24:22.952705 31949 layer_factory.hpp:77] Creating layer conv1
I0520 13:24:22.952739 31949 net.cpp:106] Creating Layer conv1
I0520 13:24:22.952750 31949 net.cpp:454] conv1 <- data
I0520 13:24:22.952770 31949 net.cpp:411] conv1 -> conv1
I0520 13:24:24.599205 31949 net.cpp:150] Setting up conv1
I0520 13:24:24.599251 31949 net.cpp:157] Top shape: 80 12 120 48 (5529600)
I0520 13:24:24.599262 31949 net.cpp:165] Memory required for data: 24150720
I0520 13:24:24.599292 31949 layer_factory.hpp:77] Creating layer relu1
I0520 13:24:24.599313 31949 net.cpp:106] Creating Layer relu1
I0520 13:24:24.599324 31949 net.cpp:454] relu1 <- conv1
I0520 13:24:24.599337 31949 net.cpp:397] relu1 -> conv1 (in-place)
I0520 13:24:24.599850 31949 net.cpp:150] Setting up relu1
I0520 13:24:24.599866 31949 net.cpp:157] Top shape: 80 12 120 48 (5529600)
I0520 13:24:24.599877 31949 net.cpp:165] Memory required for data: 46269120
I0520 13:24:24.599887 31949 layer_factory.hpp:77] Creating layer pool1
I0520 13:24:24.599905 31949 net.cpp:106] Creating Layer pool1
I0520 13:24:24.599915 31949 net.cpp:454] pool1 <- conv1
I0520 13:24:24.599927 31949 net.cpp:411] pool1 -> pool1
I0520 13:24:24.600008 31949 net.cpp:150] Setting up pool1
I0520 13:24:24.600021 31949 net.cpp:157] Top shape: 80 12 60 48 (2764800)
I0520 13:24:24.600031 31949 net.cpp:165] Memory required for data: 57328320
I0520 13:24:24.600042 31949 layer_factory.hpp:77] Creating layer conv2
I0520 13:24:24.600064 31949 net.cpp:106] Creating Layer conv2
I0520 13:24:24.600075 31949 net.cpp:454] conv2 <- pool1
I0520 13:24:24.600088 31949 net.cpp:411] conv2 -> conv2
I0520 13:24:24.602823 31949 net.cpp:150] Setting up conv2
I0520 13:24:24.602851 31949 net.cpp:157] Top shape: 80 20 54 46 (3974400)
I0520 13:24:24.602861 31949 net.cpp:165] Memory required for data: 73225920
I0520 13:24:24.602881 31949 layer_factory.hpp:77] Creating layer relu2
I0520 13:24:24.602895 31949 net.cpp:106] Creating Layer relu2
I0520 13:24:24.602905 31949 net.cpp:454] relu2 <- conv2
I0520 13:24:24.602917 31949 net.cpp:397] relu2 -> conv2 (in-place)
I0520 13:24:24.603247 31949 net.cpp:150] Setting up relu2
I0520 13:24:24.603261 31949 net.cpp:157] Top shape: 80 20 54 46 (3974400)
I0520 13:24:24.603271 31949 net.cpp:165] Memory required for data: 89123520
I0520 13:24:24.603281 31949 layer_factory.hpp:77] Creating layer pool2
I0520 13:24:24.603293 31949 net.cpp:106] Creating Layer pool2
I0520 13:24:24.603303 31949 net.cpp:454] pool2 <- conv2
I0520 13:24:24.603328 31949 net.cpp:411] pool2 -> pool2
I0520 13:24:24.603397 31949 net.cpp:150] Setting up pool2
I0520 13:24:24.603410 31949 net.cpp:157] Top shape: 80 20 27 46 (1987200)
I0520 13:24:24.603420 31949 net.cpp:165] Memory required for data: 97072320
I0520 13:24:24.603430 31949 layer_factory.hpp:77] Creating layer conv3
I0520 13:24:24.603451 31949 net.cpp:106] Creating Layer conv3
I0520 13:24:24.603461 31949 net.cpp:454] conv3 <- pool2
I0520 13:24:24.603474 31949 net.cpp:411] conv3 -> conv3
I0520 13:24:24.605396 31949 net.cpp:150] Setting up conv3
I0520 13:24:24.605420 31949 net.cpp:157] Top shape: 80 28 22 44 (2168320)
I0520 13:24:24.605432 31949 net.cpp:165] Memory required for data: 105745600
I0520 13:24:24.605450 31949 layer_factory.hpp:77] Creating layer relu3
I0520 13:24:24.605468 31949 net.cpp:106] Creating Layer relu3
I0520 13:24:24.605476 31949 net.cpp:454] relu3 <- conv3
I0520 13:24:24.605489 31949 net.cpp:397] relu3 -> conv3 (in-place)
I0520 13:24:24.605957 31949 net.cpp:150] Setting up relu3
I0520 13:24:24.605973 31949 net.cpp:157] Top shape: 80 28 22 44 (2168320)
I0520 13:24:24.605983 31949 net.cpp:165] Memory required for data: 114418880
I0520 13:24:24.605994 31949 layer_factory.hpp:77] Creating layer pool3
I0520 13:24:24.606006 31949 net.cpp:106] Creating Layer pool3
I0520 13:24:24.606016 31949 net.cpp:454] pool3 <- conv3
I0520 13:24:24.606029 31949 net.cpp:411] pool3 -> pool3
I0520 13:24:24.606097 31949 net.cpp:150] Setting up pool3
I0520 13:24:24.606111 31949 net.cpp:157] Top shape: 80 28 11 44 (1084160)
I0520 13:24:24.606120 31949 net.cpp:165] Memory required for data: 118755520
I0520 13:24:24.606127 31949 layer_factory.hpp:77] Creating layer conv4
I0520 13:24:24.606144 31949 net.cpp:106] Creating Layer conv4
I0520 13:24:24.606155 31949 net.cpp:454] conv4 <- pool3
I0520 13:24:24.606168 31949 net.cpp:411] conv4 -> conv4
I0520 13:24:24.609141 31949 net.cpp:150] Setting up conv4
I0520 13:24:24.609170 31949 net.cpp:157] Top shape: 80 36 6 42 (725760)
I0520 13:24:24.609180 31949 net.cpp:165] Memory required for data: 121658560
I0520 13:24:24.609196 31949 layer_factory.hpp:77] Creating layer relu4
I0520 13:24:24.609210 31949 net.cpp:106] Creating Layer relu4
I0520 13:24:24.609220 31949 net.cpp:454] relu4 <- conv4
I0520 13:24:24.609233 31949 net.cpp:397] relu4 -> conv4 (in-place)
I0520 13:24:24.609695 31949 net.cpp:150] Setting up relu4
I0520 13:24:24.609711 31949 net.cpp:157] Top shape: 80 36 6 42 (725760)
I0520 13:24:24.609721 31949 net.cpp:165] Memory required for data: 124561600
I0520 13:24:24.609732 31949 layer_factory.hpp:77] Creating layer pool4
I0520 13:24:24.609745 31949 net.cpp:106] Creating Layer pool4
I0520 13:24:24.609755 31949 net.cpp:454] pool4 <- conv4
I0520 13:24:24.609768 31949 net.cpp:411] pool4 -> pool4
I0520 13:24:24.609836 31949 net.cpp:150] Setting up pool4
I0520 13:24:24.609850 31949 net.cpp:157] Top shape: 80 36 3 42 (362880)
I0520 13:24:24.609859 31949 net.cpp:165] Memory required for data: 126013120
I0520 13:24:24.609870 31949 layer_factory.hpp:77] Creating layer ip1
I0520 13:24:24.609890 31949 net.cpp:106] Creating Layer ip1
I0520 13:24:24.609900 31949 net.cpp:454] ip1 <- pool4
I0520 13:24:24.609912 31949 net.cpp:411] ip1 -> ip1
I0520 13:24:24.625358 31949 net.cpp:150] Setting up ip1
I0520 13:24:24.625387 31949 net.cpp:157] Top shape: 80 196 (15680)
I0520 13:24:24.625401 31949 net.cpp:165] Memory required for data: 126075840
I0520 13:24:24.625423 31949 layer_factory.hpp:77] Creating layer relu5
I0520 13:24:24.625438 31949 net.cpp:106] Creating Layer relu5
I0520 13:24:24.625448 31949 net.cpp:454] relu5 <- ip1
I0520 13:24:24.625463 31949 net.cpp:397] relu5 -> ip1 (in-place)
I0520 13:24:24.625803 31949 net.cpp:150] Setting up relu5
I0520 13:24:24.625818 31949 net.cpp:157] Top shape: 80 196 (15680)
I0520 13:24:24.625828 31949 net.cpp:165] Memory required for data: 126138560
I0520 13:24:24.625838 31949 layer_factory.hpp:77] Creating layer drop1
I0520 13:24:24.625859 31949 net.cpp:106] Creating Layer drop1
I0520 13:24:24.625869 31949 net.cpp:454] drop1 <- ip1
I0520 13:24:24.625880 31949 net.cpp:397] drop1 -> ip1 (in-place)
I0520 13:24:24.625942 31949 net.cpp:150] Setting up drop1
I0520 13:24:24.625957 31949 net.cpp:157] Top shape: 80 196 (15680)
I0520 13:24:24.625965 31949 net.cpp:165] Memory required for data: 126201280
I0520 13:24:24.625975 31949 layer_factory.hpp:77] Creating layer ip2
I0520 13:24:24.625998 31949 net.cpp:106] Creating Layer ip2
I0520 13:24:24.626008 31949 net.cpp:454] ip2 <- ip1
I0520 13:24:24.626021 31949 net.cpp:411] ip2 -> ip2
I0520 13:24:24.626487 31949 net.cpp:150] Setting up ip2
I0520 13:24:24.626499 31949 net.cpp:157] Top shape: 80 98 (7840)
I0520 13:24:24.626510 31949 net.cpp:165] Memory required for data: 126232640
I0520 13:24:24.626525 31949 layer_factory.hpp:77] Creating layer relu6
I0520 13:24:24.626538 31949 net.cpp:106] Creating Layer relu6
I0520 13:24:24.626548 31949 net.cpp:454] relu6 <- ip2
I0520 13:24:24.626559 31949 net.cpp:397] relu6 -> ip2 (in-place)
I0520 13:24:24.627079 31949 net.cpp:150] Setting up relu6
I0520 13:24:24.627094 31949 net.cpp:157] Top shape: 80 98 (7840)
I0520 13:24:24.627104 31949 net.cpp:165] Memory required for data: 126264000
I0520 13:24:24.627115 31949 layer_factory.hpp:77] Creating layer drop2
I0520 13:24:24.627127 31949 net.cpp:106] Creating Layer drop2
I0520 13:24:24.627137 31949 net.cpp:454] drop2 <- ip2
I0520 13:24:24.627149 31949 net.cpp:397] drop2 -> ip2 (in-place)
I0520 13:24:24.627192 31949 net.cpp:150] Setting up drop2
I0520 13:24:24.627205 31949 net.cpp:157] Top shape: 80 98 (7840)
I0520 13:24:24.627216 31949 net.cpp:165] Memory required for data: 126295360
I0520 13:24:24.627226 31949 layer_factory.hpp:77] Creating layer ip3
I0520 13:24:24.627239 31949 net.cpp:106] Creating Layer ip3
I0520 13:24:24.627249 31949 net.cpp:454] ip3 <- ip2
I0520 13:24:24.627262 31949 net.cpp:411] ip3 -> ip3
I0520 13:24:24.627470 31949 net.cpp:150] Setting up ip3
I0520 13:24:24.627483 31949 net.cpp:157] Top shape: 80 11 (880)
I0520 13:24:24.627492 31949 net.cpp:165] Memory required for data: 126298880
I0520 13:24:24.627508 31949 layer_factory.hpp:77] Creating layer drop3
I0520 13:24:24.627521 31949 net.cpp:106] Creating Layer drop3
I0520 13:24:24.627531 31949 net.cpp:454] drop3 <- ip3
I0520 13:24:24.627542 31949 net.cpp:397] drop3 -> ip3 (in-place)
I0520 13:24:24.627581 31949 net.cpp:150] Setting up drop3
I0520 13:24:24.627594 31949 net.cpp:157] Top shape: 80 11 (880)
I0520 13:24:24.627604 31949 net.cpp:165] Memory required for data: 126302400
I0520 13:24:24.627614 31949 layer_factory.hpp:77] Creating layer loss
I0520 13:24:24.627632 31949 net.cpp:106] Creating Layer loss
I0520 13:24:24.627642 31949 net.cpp:454] loss <- ip3
I0520 13:24:24.627655 31949 net.cpp:454] loss <- label
I0520 13:24:24.627666 31949 net.cpp:411] loss -> loss
I0520 13:24:24.627683 31949 layer_factory.hpp:77] Creating layer loss
I0520 13:24:24.628327 31949 net.cpp:150] Setting up loss
I0520 13:24:24.628347 31949 net.cpp:157] Top shape: (1)
I0520 13:24:24.628361 31949 net.cpp:160]     with loss weight 1
I0520 13:24:24.628403 31949 net.cpp:165] Memory required for data: 126302404
I0520 13:24:24.628414 31949 net.cpp:226] loss needs backward computation.
I0520 13:24:24.628425 31949 net.cpp:226] drop3 needs backward computation.
I0520 13:24:24.628433 31949 net.cpp:226] ip3 needs backward computation.
I0520 13:24:24.628444 31949 net.cpp:226] drop2 needs backward computation.
I0520 13:24:24.628454 31949 net.cpp:226] relu6 needs backward computation.
I0520 13:24:24.628464 31949 net.cpp:226] ip2 needs backward computation.
I0520 13:24:24.628474 31949 net.cpp:226] drop1 needs backward computation.
I0520 13:24:24.628495 31949 net.cpp:226] relu5 needs backward computation.
I0520 13:24:24.628505 31949 net.cpp:226] ip1 needs backward computation.
I0520 13:24:24.628515 31949 net.cpp:226] pool4 needs backward computation.
I0520 13:24:24.628525 31949 net.cpp:226] relu4 needs backward computation.
I0520 13:24:24.628535 31949 net.cpp:226] conv4 needs backward computation.
I0520 13:24:24.628545 31949 net.cpp:226] pool3 needs backward computation.
I0520 13:24:24.628553 31949 net.cpp:226] relu3 needs backward computation.
I0520 13:24:24.628572 31949 net.cpp:226] conv3 needs backward computation.
I0520 13:24:24.628583 31949 net.cpp:226] pool2 needs backward computation.
I0520 13:24:24.628594 31949 net.cpp:226] relu2 needs backward computation.
I0520 13:24:24.628604 31949 net.cpp:226] conv2 needs backward computation.
I0520 13:24:24.628614 31949 net.cpp:226] pool1 needs backward computation.
I0520 13:24:24.628625 31949 net.cpp:226] relu1 needs backward computation.
I0520 13:24:24.628634 31949 net.cpp:226] conv1 needs backward computation.
I0520 13:24:24.628645 31949 net.cpp:228] data_hdf5 does not need backward computation.
I0520 13:24:24.628655 31949 net.cpp:270] This network produces output loss
I0520 13:24:24.628679 31949 net.cpp:283] Network initialization done.
I0520 13:24:24.630297 31949 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834.prototxt
I0520 13:24:24.630368 31949 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 13:24:24.630720 31949 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 80
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 13:24:24.630908 31949 layer_factory.hpp:77] Creating layer data_hdf5
I0520 13:24:24.630923 31949 net.cpp:106] Creating Layer data_hdf5
I0520 13:24:24.630935 31949 net.cpp:411] data_hdf5 -> data
I0520 13:24:24.630954 31949 net.cpp:411] data_hdf5 -> label
I0520 13:24:24.630970 31949 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 13:24:24.632349 31949 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 13:24:45.944254 31949 net.cpp:150] Setting up data_hdf5
I0520 13:24:45.944419 31949 net.cpp:157] Top shape: 80 1 127 50 (508000)
I0520 13:24:45.944434 31949 net.cpp:157] Top shape: 80 (80)
I0520 13:24:45.944445 31949 net.cpp:165] Memory required for data: 2032320
I0520 13:24:45.944459 31949 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 13:24:45.944494 31949 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 13:24:45.944505 31949 net.cpp:454] label_data_hdf5_1_split <- label
I0520 13:24:45.944520 31949 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 13:24:45.944542 31949 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 13:24:45.944617 31949 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 13:24:45.944629 31949 net.cpp:157] Top shape: 80 (80)
I0520 13:24:45.944641 31949 net.cpp:157] Top shape: 80 (80)
I0520 13:24:45.944650 31949 net.cpp:165] Memory required for data: 2032960
I0520 13:24:45.944658 31949 layer_factory.hpp:77] Creating layer conv1
I0520 13:24:45.944680 31949 net.cpp:106] Creating Layer conv1
I0520 13:24:45.944691 31949 net.cpp:454] conv1 <- data
I0520 13:24:45.944706 31949 net.cpp:411] conv1 -> conv1
I0520 13:24:45.946656 31949 net.cpp:150] Setting up conv1
I0520 13:24:45.946676 31949 net.cpp:157] Top shape: 80 12 120 48 (5529600)
I0520 13:24:45.946686 31949 net.cpp:165] Memory required for data: 24151360
I0520 13:24:45.946707 31949 layer_factory.hpp:77] Creating layer relu1
I0520 13:24:45.946720 31949 net.cpp:106] Creating Layer relu1
I0520 13:24:45.946730 31949 net.cpp:454] relu1 <- conv1
I0520 13:24:45.946743 31949 net.cpp:397] relu1 -> conv1 (in-place)
I0520 13:24:45.947237 31949 net.cpp:150] Setting up relu1
I0520 13:24:45.947252 31949 net.cpp:157] Top shape: 80 12 120 48 (5529600)
I0520 13:24:45.947263 31949 net.cpp:165] Memory required for data: 46269760
I0520 13:24:45.947273 31949 layer_factory.hpp:77] Creating layer pool1
I0520 13:24:45.947288 31949 net.cpp:106] Creating Layer pool1
I0520 13:24:45.947298 31949 net.cpp:454] pool1 <- conv1
I0520 13:24:45.947310 31949 net.cpp:411] pool1 -> pool1
I0520 13:24:45.947386 31949 net.cpp:150] Setting up pool1
I0520 13:24:45.947399 31949 net.cpp:157] Top shape: 80 12 60 48 (2764800)
I0520 13:24:45.947408 31949 net.cpp:165] Memory required for data: 57328960
I0520 13:24:45.947419 31949 layer_factory.hpp:77] Creating layer conv2
I0520 13:24:45.947437 31949 net.cpp:106] Creating Layer conv2
I0520 13:24:45.947446 31949 net.cpp:454] conv2 <- pool1
I0520 13:24:45.947460 31949 net.cpp:411] conv2 -> conv2
I0520 13:24:45.949404 31949 net.cpp:150] Setting up conv2
I0520 13:24:45.949424 31949 net.cpp:157] Top shape: 80 20 54 46 (3974400)
I0520 13:24:45.949440 31949 net.cpp:165] Memory required for data: 73226560
I0520 13:24:45.949456 31949 layer_factory.hpp:77] Creating layer relu2
I0520 13:24:45.949470 31949 net.cpp:106] Creating Layer relu2
I0520 13:24:45.949481 31949 net.cpp:454] relu2 <- conv2
I0520 13:24:45.949492 31949 net.cpp:397] relu2 -> conv2 (in-place)
I0520 13:24:45.949829 31949 net.cpp:150] Setting up relu2
I0520 13:24:45.949843 31949 net.cpp:157] Top shape: 80 20 54 46 (3974400)
I0520 13:24:45.949853 31949 net.cpp:165] Memory required for data: 89124160
I0520 13:24:45.949863 31949 layer_factory.hpp:77] Creating layer pool2
I0520 13:24:45.949877 31949 net.cpp:106] Creating Layer pool2
I0520 13:24:45.949887 31949 net.cpp:454] pool2 <- conv2
I0520 13:24:45.949899 31949 net.cpp:411] pool2 -> pool2
I0520 13:24:45.949970 31949 net.cpp:150] Setting up pool2
I0520 13:24:45.949983 31949 net.cpp:157] Top shape: 80 20 27 46 (1987200)
I0520 13:24:45.949993 31949 net.cpp:165] Memory required for data: 97072960
I0520 13:24:45.950003 31949 layer_factory.hpp:77] Creating layer conv3
I0520 13:24:45.950022 31949 net.cpp:106] Creating Layer conv3
I0520 13:24:45.950033 31949 net.cpp:454] conv3 <- pool2
I0520 13:24:45.950047 31949 net.cpp:411] conv3 -> conv3
I0520 13:24:45.952023 31949 net.cpp:150] Setting up conv3
I0520 13:24:45.952045 31949 net.cpp:157] Top shape: 80 28 22 44 (2168320)
I0520 13:24:45.952057 31949 net.cpp:165] Memory required for data: 105746240
I0520 13:24:45.952090 31949 layer_factory.hpp:77] Creating layer relu3
I0520 13:24:45.952103 31949 net.cpp:106] Creating Layer relu3
I0520 13:24:45.952113 31949 net.cpp:454] relu3 <- conv3
I0520 13:24:45.952126 31949 net.cpp:397] relu3 -> conv3 (in-place)
I0520 13:24:45.952602 31949 net.cpp:150] Setting up relu3
I0520 13:24:45.952618 31949 net.cpp:157] Top shape: 80 28 22 44 (2168320)
I0520 13:24:45.952628 31949 net.cpp:165] Memory required for data: 114419520
I0520 13:24:45.952639 31949 layer_factory.hpp:77] Creating layer pool3
I0520 13:24:45.952653 31949 net.cpp:106] Creating Layer pool3
I0520 13:24:45.952663 31949 net.cpp:454] pool3 <- conv3
I0520 13:24:45.952677 31949 net.cpp:411] pool3 -> pool3
I0520 13:24:45.952747 31949 net.cpp:150] Setting up pool3
I0520 13:24:45.952761 31949 net.cpp:157] Top shape: 80 28 11 44 (1084160)
I0520 13:24:45.952770 31949 net.cpp:165] Memory required for data: 118756160
I0520 13:24:45.952780 31949 layer_factory.hpp:77] Creating layer conv4
I0520 13:24:45.952798 31949 net.cpp:106] Creating Layer conv4
I0520 13:24:45.952808 31949 net.cpp:454] conv4 <- pool3
I0520 13:24:45.952822 31949 net.cpp:411] conv4 -> conv4
I0520 13:24:45.954898 31949 net.cpp:150] Setting up conv4
I0520 13:24:45.954921 31949 net.cpp:157] Top shape: 80 36 6 42 (725760)
I0520 13:24:45.954933 31949 net.cpp:165] Memory required for data: 121659200
I0520 13:24:45.954948 31949 layer_factory.hpp:77] Creating layer relu4
I0520 13:24:45.954962 31949 net.cpp:106] Creating Layer relu4
I0520 13:24:45.954972 31949 net.cpp:454] relu4 <- conv4
I0520 13:24:45.954984 31949 net.cpp:397] relu4 -> conv4 (in-place)
I0520 13:24:45.955456 31949 net.cpp:150] Setting up relu4
I0520 13:24:45.955471 31949 net.cpp:157] Top shape: 80 36 6 42 (725760)
I0520 13:24:45.955482 31949 net.cpp:165] Memory required for data: 124562240
I0520 13:24:45.955492 31949 layer_factory.hpp:77] Creating layer pool4
I0520 13:24:45.955504 31949 net.cpp:106] Creating Layer pool4
I0520 13:24:45.955514 31949 net.cpp:454] pool4 <- conv4
I0520 13:24:45.955528 31949 net.cpp:411] pool4 -> pool4
I0520 13:24:45.955598 31949 net.cpp:150] Setting up pool4
I0520 13:24:45.955611 31949 net.cpp:157] Top shape: 80 36 3 42 (362880)
I0520 13:24:45.955621 31949 net.cpp:165] Memory required for data: 126013760
I0520 13:24:45.955629 31949 layer_factory.hpp:77] Creating layer ip1
I0520 13:24:45.955646 31949 net.cpp:106] Creating Layer ip1
I0520 13:24:45.955657 31949 net.cpp:454] ip1 <- pool4
I0520 13:24:45.955669 31949 net.cpp:411] ip1 -> ip1
I0520 13:24:45.971186 31949 net.cpp:150] Setting up ip1
I0520 13:24:45.971215 31949 net.cpp:157] Top shape: 80 196 (15680)
I0520 13:24:45.971227 31949 net.cpp:165] Memory required for data: 126076480
I0520 13:24:45.971251 31949 layer_factory.hpp:77] Creating layer relu5
I0520 13:24:45.971266 31949 net.cpp:106] Creating Layer relu5
I0520 13:24:45.971276 31949 net.cpp:454] relu5 <- ip1
I0520 13:24:45.971288 31949 net.cpp:397] relu5 -> ip1 (in-place)
I0520 13:24:45.971638 31949 net.cpp:150] Setting up relu5
I0520 13:24:45.971652 31949 net.cpp:157] Top shape: 80 196 (15680)
I0520 13:24:45.971662 31949 net.cpp:165] Memory required for data: 126139200
I0520 13:24:45.971735 31949 layer_factory.hpp:77] Creating layer drop1
I0520 13:24:45.971755 31949 net.cpp:106] Creating Layer drop1
I0520 13:24:45.971766 31949 net.cpp:454] drop1 <- ip1
I0520 13:24:45.971779 31949 net.cpp:397] drop1 -> ip1 (in-place)
I0520 13:24:45.971828 31949 net.cpp:150] Setting up drop1
I0520 13:24:45.971842 31949 net.cpp:157] Top shape: 80 196 (15680)
I0520 13:24:45.971851 31949 net.cpp:165] Memory required for data: 126201920
I0520 13:24:45.971861 31949 layer_factory.hpp:77] Creating layer ip2
I0520 13:24:45.971875 31949 net.cpp:106] Creating Layer ip2
I0520 13:24:45.971885 31949 net.cpp:454] ip2 <- ip1
I0520 13:24:45.971899 31949 net.cpp:411] ip2 -> ip2
I0520 13:24:45.972378 31949 net.cpp:150] Setting up ip2
I0520 13:24:45.972391 31949 net.cpp:157] Top shape: 80 98 (7840)
I0520 13:24:45.972401 31949 net.cpp:165] Memory required for data: 126233280
I0520 13:24:45.972416 31949 layer_factory.hpp:77] Creating layer relu6
I0520 13:24:45.972442 31949 net.cpp:106] Creating Layer relu6
I0520 13:24:45.972452 31949 net.cpp:454] relu6 <- ip2
I0520 13:24:45.972465 31949 net.cpp:397] relu6 -> ip2 (in-place)
I0520 13:24:45.973014 31949 net.cpp:150] Setting up relu6
I0520 13:24:45.973036 31949 net.cpp:157] Top shape: 80 98 (7840)
I0520 13:24:45.973045 31949 net.cpp:165] Memory required for data: 126264640
I0520 13:24:45.973055 31949 layer_factory.hpp:77] Creating layer drop2
I0520 13:24:45.973069 31949 net.cpp:106] Creating Layer drop2
I0520 13:24:45.973079 31949 net.cpp:454] drop2 <- ip2
I0520 13:24:45.973093 31949 net.cpp:397] drop2 -> ip2 (in-place)
I0520 13:24:45.973136 31949 net.cpp:150] Setting up drop2
I0520 13:24:45.973150 31949 net.cpp:157] Top shape: 80 98 (7840)
I0520 13:24:45.973160 31949 net.cpp:165] Memory required for data: 126296000
I0520 13:24:45.973168 31949 layer_factory.hpp:77] Creating layer ip3
I0520 13:24:45.973182 31949 net.cpp:106] Creating Layer ip3
I0520 13:24:45.973192 31949 net.cpp:454] ip3 <- ip2
I0520 13:24:45.973206 31949 net.cpp:411] ip3 -> ip3
I0520 13:24:45.973428 31949 net.cpp:150] Setting up ip3
I0520 13:24:45.973440 31949 net.cpp:157] Top shape: 80 11 (880)
I0520 13:24:45.973449 31949 net.cpp:165] Memory required for data: 126299520
I0520 13:24:45.973464 31949 layer_factory.hpp:77] Creating layer drop3
I0520 13:24:45.973477 31949 net.cpp:106] Creating Layer drop3
I0520 13:24:45.973487 31949 net.cpp:454] drop3 <- ip3
I0520 13:24:45.973500 31949 net.cpp:397] drop3 -> ip3 (in-place)
I0520 13:24:45.973541 31949 net.cpp:150] Setting up drop3
I0520 13:24:45.973553 31949 net.cpp:157] Top shape: 80 11 (880)
I0520 13:24:45.973563 31949 net.cpp:165] Memory required for data: 126303040
I0520 13:24:45.973573 31949 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 13:24:45.973587 31949 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 13:24:45.973595 31949 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 13:24:45.973608 31949 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 13:24:45.973623 31949 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 13:24:45.973697 31949 net.cpp:150] Setting up ip3_drop3_0_split
I0520 13:24:45.973711 31949 net.cpp:157] Top shape: 80 11 (880)
I0520 13:24:45.973722 31949 net.cpp:157] Top shape: 80 11 (880)
I0520 13:24:45.973732 31949 net.cpp:165] Memory required for data: 126310080
I0520 13:24:45.973742 31949 layer_factory.hpp:77] Creating layer accuracy
I0520 13:24:45.973764 31949 net.cpp:106] Creating Layer accuracy
I0520 13:24:45.973773 31949 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 13:24:45.973784 31949 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 13:24:45.973798 31949 net.cpp:411] accuracy -> accuracy
I0520 13:24:45.973822 31949 net.cpp:150] Setting up accuracy
I0520 13:24:45.973834 31949 net.cpp:157] Top shape: (1)
I0520 13:24:45.973844 31949 net.cpp:165] Memory required for data: 126310084
I0520 13:24:45.973853 31949 layer_factory.hpp:77] Creating layer loss
I0520 13:24:45.973867 31949 net.cpp:106] Creating Layer loss
I0520 13:24:45.973877 31949 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 13:24:45.973887 31949 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 13:24:45.973901 31949 net.cpp:411] loss -> loss
I0520 13:24:45.973918 31949 layer_factory.hpp:77] Creating layer loss
I0520 13:24:45.974417 31949 net.cpp:150] Setting up loss
I0520 13:24:45.974431 31949 net.cpp:157] Top shape: (1)
I0520 13:24:45.974441 31949 net.cpp:160]     with loss weight 1
I0520 13:24:45.974459 31949 net.cpp:165] Memory required for data: 126310088
I0520 13:24:45.974469 31949 net.cpp:226] loss needs backward computation.
I0520 13:24:45.974480 31949 net.cpp:228] accuracy does not need backward computation.
I0520 13:24:45.974491 31949 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 13:24:45.974501 31949 net.cpp:226] drop3 needs backward computation.
I0520 13:24:45.974512 31949 net.cpp:226] ip3 needs backward computation.
I0520 13:24:45.974522 31949 net.cpp:226] drop2 needs backward computation.
I0520 13:24:45.974532 31949 net.cpp:226] relu6 needs backward computation.
I0520 13:24:45.974551 31949 net.cpp:226] ip2 needs backward computation.
I0520 13:24:45.974561 31949 net.cpp:226] drop1 needs backward computation.
I0520 13:24:45.974571 31949 net.cpp:226] relu5 needs backward computation.
I0520 13:24:45.974581 31949 net.cpp:226] ip1 needs backward computation.
I0520 13:24:45.974591 31949 net.cpp:226] pool4 needs backward computation.
I0520 13:24:45.974601 31949 net.cpp:226] relu4 needs backward computation.
I0520 13:24:45.974611 31949 net.cpp:226] conv4 needs backward computation.
I0520 13:24:45.974622 31949 net.cpp:226] pool3 needs backward computation.
I0520 13:24:45.974632 31949 net.cpp:226] relu3 needs backward computation.
I0520 13:24:45.974642 31949 net.cpp:226] conv3 needs backward computation.
I0520 13:24:45.974653 31949 net.cpp:226] pool2 needs backward computation.
I0520 13:24:45.974663 31949 net.cpp:226] relu2 needs backward computation.
I0520 13:24:45.974673 31949 net.cpp:226] conv2 needs backward computation.
I0520 13:24:45.974683 31949 net.cpp:226] pool1 needs backward computation.
I0520 13:24:45.974692 31949 net.cpp:226] relu1 needs backward computation.
I0520 13:24:45.974702 31949 net.cpp:226] conv1 needs backward computation.
I0520 13:24:45.974714 31949 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 13:24:45.974725 31949 net.cpp:228] data_hdf5 does not need backward computation.
I0520 13:24:45.974735 31949 net.cpp:270] This network produces output accuracy
I0520 13:24:45.974745 31949 net.cpp:270] This network produces output loss
I0520 13:24:45.974773 31949 net.cpp:283] Network initialization done.
I0520 13:24:45.974905 31949 solver.cpp:60] Solver scaffolding done.
I0520 13:24:45.976042 31949 caffe.cpp:212] Starting Optimization
I0520 13:24:45.976060 31949 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 13:24:45.976073 31949 solver.cpp:289] Learning Rate Policy: fixed
I0520 13:24:45.977149 31949 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 13:25:34.220860 31949 solver.cpp:409]     Test net output #0: accuracy = 0.0496333
I0520 13:25:34.221019 31949 solver.cpp:409]     Test net output #1: loss = 2.39898 (* 1 = 2.39898 loss)
I0520 13:25:34.250434 31949 solver.cpp:237] Iteration 0, loss = 2.39723
I0520 13:25:34.250471 31949 solver.cpp:253]     Train net output #0: loss = 2.39723 (* 1 = 2.39723 loss)
I0520 13:25:34.250489 31949 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 13:25:42.899564 31949 solver.cpp:237] Iteration 187, loss = 2.27596
I0520 13:25:42.899600 31949 solver.cpp:253]     Train net output #0: loss = 2.27596 (* 1 = 2.27596 loss)
I0520 13:25:42.899616 31949 sgd_solver.cpp:106] Iteration 187, lr = 0.0025
I0520 13:25:51.549803 31949 solver.cpp:237] Iteration 374, loss = 2.25976
I0520 13:25:51.549839 31949 solver.cpp:253]     Train net output #0: loss = 2.25976 (* 1 = 2.25976 loss)
I0520 13:25:51.549854 31949 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0520 13:26:00.198335 31949 solver.cpp:237] Iteration 561, loss = 2.03183
I0520 13:26:00.198379 31949 solver.cpp:253]     Train net output #0: loss = 2.03183 (* 1 = 2.03183 loss)
I0520 13:26:00.198397 31949 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0520 13:26:08.844789 31949 solver.cpp:237] Iteration 748, loss = 2.02967
I0520 13:26:08.844938 31949 solver.cpp:253]     Train net output #0: loss = 2.02967 (* 1 = 2.02967 loss)
I0520 13:26:08.844952 31949 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0520 13:26:17.491766 31949 solver.cpp:237] Iteration 935, loss = 1.92246
I0520 13:26:17.491801 31949 solver.cpp:253]     Train net output #0: loss = 1.92246 (* 1 = 1.92246 loss)
I0520 13:26:17.491819 31949 sgd_solver.cpp:106] Iteration 935, lr = 0.0025
I0520 13:26:26.144799 31949 solver.cpp:237] Iteration 1122, loss = 2.02765
I0520 13:26:26.144846 31949 solver.cpp:253]     Train net output #0: loss = 2.02765 (* 1 = 2.02765 loss)
I0520 13:26:26.144861 31949 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0520 13:26:56.948081 31949 solver.cpp:237] Iteration 1309, loss = 1.9229
I0520 13:26:56.948242 31949 solver.cpp:253]     Train net output #0: loss = 1.9229 (* 1 = 1.9229 loss)
I0520 13:26:56.948258 31949 sgd_solver.cpp:106] Iteration 1309, lr = 0.0025
I0520 13:27:05.596915 31949 solver.cpp:237] Iteration 1496, loss = 1.98718
I0520 13:27:05.596946 31949 solver.cpp:253]     Train net output #0: loss = 1.98718 (* 1 = 1.98718 loss)
I0520 13:27:05.596963 31949 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 13:27:14.249599 31949 solver.cpp:237] Iteration 1683, loss = 1.90793
I0520 13:27:14.249644 31949 solver.cpp:253]     Train net output #0: loss = 1.90793 (* 1 = 1.90793 loss)
I0520 13:27:14.249661 31949 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0520 13:27:22.900970 31949 solver.cpp:237] Iteration 1870, loss = 1.8135
I0520 13:27:22.901006 31949 solver.cpp:253]     Train net output #0: loss = 1.8135 (* 1 = 1.8135 loss)
I0520 13:27:22.901022 31949 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0520 13:27:23.085744 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_1875.caffemodel
I0520 13:27:23.158732 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_1875.solverstate
I0520 13:27:31.611968 31949 solver.cpp:237] Iteration 2057, loss = 1.79627
I0520 13:27:31.612121 31949 solver.cpp:253]     Train net output #0: loss = 1.79627 (* 1 = 1.79627 loss)
I0520 13:27:31.612135 31949 sgd_solver.cpp:106] Iteration 2057, lr = 0.0025
I0520 13:27:40.262245 31949 solver.cpp:237] Iteration 2244, loss = 1.8307
I0520 13:27:40.262290 31949 solver.cpp:253]     Train net output #0: loss = 1.8307 (* 1 = 1.8307 loss)
I0520 13:27:40.262307 31949 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0520 13:27:48.910719 31949 solver.cpp:237] Iteration 2431, loss = 1.70447
I0520 13:27:48.910755 31949 solver.cpp:253]     Train net output #0: loss = 1.70447 (* 1 = 1.70447 loss)
I0520 13:27:48.910773 31949 sgd_solver.cpp:106] Iteration 2431, lr = 0.0025
I0520 13:28:19.738400 31949 solver.cpp:237] Iteration 2618, loss = 1.6468
I0520 13:28:19.738556 31949 solver.cpp:253]     Train net output #0: loss = 1.6468 (* 1 = 1.6468 loss)
I0520 13:28:19.738571 31949 sgd_solver.cpp:106] Iteration 2618, lr = 0.0025
I0520 13:28:28.389309 31949 solver.cpp:237] Iteration 2805, loss = 1.65996
I0520 13:28:28.389350 31949 solver.cpp:253]     Train net output #0: loss = 1.65996 (* 1 = 1.65996 loss)
I0520 13:28:28.389370 31949 sgd_solver.cpp:106] Iteration 2805, lr = 0.0025
I0520 13:28:37.036988 31949 solver.cpp:237] Iteration 2992, loss = 1.51991
I0520 13:28:37.037022 31949 solver.cpp:253]     Train net output #0: loss = 1.51991 (* 1 = 1.51991 loss)
I0520 13:28:37.037039 31949 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0520 13:28:45.686169 31949 solver.cpp:237] Iteration 3179, loss = 1.78194
I0520 13:28:45.686204 31949 solver.cpp:253]     Train net output #0: loss = 1.78194 (* 1 = 1.78194 loss)
I0520 13:28:45.686221 31949 sgd_solver.cpp:106] Iteration 3179, lr = 0.0025
I0520 13:28:54.334945 31949 solver.cpp:237] Iteration 3366, loss = 1.69955
I0520 13:28:54.335101 31949 solver.cpp:253]     Train net output #0: loss = 1.69955 (* 1 = 1.69955 loss)
I0520 13:28:54.335115 31949 sgd_solver.cpp:106] Iteration 3366, lr = 0.0025
I0520 13:29:02.989114 31949 solver.cpp:237] Iteration 3553, loss = 1.63123
I0520 13:29:02.989148 31949 solver.cpp:253]     Train net output #0: loss = 1.63123 (* 1 = 1.63123 loss)
I0520 13:29:02.989166 31949 sgd_solver.cpp:106] Iteration 3553, lr = 0.0025
I0520 13:29:11.633160 31949 solver.cpp:237] Iteration 3740, loss = 1.51897
I0520 13:29:11.633195 31949 solver.cpp:253]     Train net output #0: loss = 1.51897 (* 1 = 1.51897 loss)
I0520 13:29:11.633213 31949 sgd_solver.cpp:106] Iteration 3740, lr = 0.0025
I0520 13:29:12.048707 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_3750.caffemodel
I0520 13:29:12.118191 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_3750.solverstate
I0520 13:29:12.143508 31949 solver.cpp:341] Iteration 3750, Testing net (#0)
I0520 13:29:59.464972 31949 solver.cpp:409]     Test net output #0: accuracy = 0.714221
I0520 13:29:59.465129 31949 solver.cpp:409]     Test net output #1: loss = 0.961492 (* 1 = 0.961492 loss)
I0520 13:30:29.830145 31949 solver.cpp:237] Iteration 3927, loss = 1.50803
I0520 13:30:29.830302 31949 solver.cpp:253]     Train net output #0: loss = 1.50803 (* 1 = 1.50803 loss)
I0520 13:30:29.830317 31949 sgd_solver.cpp:106] Iteration 3927, lr = 0.0025
I0520 13:30:38.476799 31949 solver.cpp:237] Iteration 4114, loss = 1.58406
I0520 13:30:38.476840 31949 solver.cpp:253]     Train net output #0: loss = 1.58406 (* 1 = 1.58406 loss)
I0520 13:30:38.476858 31949 sgd_solver.cpp:106] Iteration 4114, lr = 0.0025
I0520 13:30:47.120632 31949 solver.cpp:237] Iteration 4301, loss = 1.53691
I0520 13:30:47.120667 31949 solver.cpp:253]     Train net output #0: loss = 1.53691 (* 1 = 1.53691 loss)
I0520 13:30:47.120684 31949 sgd_solver.cpp:106] Iteration 4301, lr = 0.0025
I0520 13:30:55.762300 31949 solver.cpp:237] Iteration 4488, loss = 1.50053
I0520 13:30:55.762334 31949 solver.cpp:253]     Train net output #0: loss = 1.50053 (* 1 = 1.50053 loss)
I0520 13:30:55.762347 31949 sgd_solver.cpp:106] Iteration 4488, lr = 0.0025
I0520 13:31:04.403095 31949 solver.cpp:237] Iteration 4675, loss = 1.6013
I0520 13:31:04.403241 31949 solver.cpp:253]     Train net output #0: loss = 1.6013 (* 1 = 1.6013 loss)
I0520 13:31:04.403255 31949 sgd_solver.cpp:106] Iteration 4675, lr = 0.0025
I0520 13:31:13.047214 31949 solver.cpp:237] Iteration 4862, loss = 1.4183
I0520 13:31:13.047248 31949 solver.cpp:253]     Train net output #0: loss = 1.4183 (* 1 = 1.4183 loss)
I0520 13:31:13.047265 31949 sgd_solver.cpp:106] Iteration 4862, lr = 0.0025
I0520 13:31:43.822243 31949 solver.cpp:237] Iteration 5049, loss = 1.5108
I0520 13:31:43.822407 31949 solver.cpp:253]     Train net output #0: loss = 1.5108 (* 1 = 1.5108 loss)
I0520 13:31:43.822422 31949 sgd_solver.cpp:106] Iteration 5049, lr = 0.0025
I0520 13:31:52.466317 31949 solver.cpp:237] Iteration 5236, loss = 1.54972
I0520 13:31:52.466362 31949 solver.cpp:253]     Train net output #0: loss = 1.54972 (* 1 = 1.54972 loss)
I0520 13:31:52.466379 31949 sgd_solver.cpp:106] Iteration 5236, lr = 0.0025
I0520 13:32:01.111457 31949 solver.cpp:237] Iteration 5423, loss = 1.71929
I0520 13:32:01.111493 31949 solver.cpp:253]     Train net output #0: loss = 1.71929 (* 1 = 1.71929 loss)
I0520 13:32:01.111510 31949 sgd_solver.cpp:106] Iteration 5423, lr = 0.0025
I0520 13:32:09.757325 31949 solver.cpp:237] Iteration 5610, loss = 1.38611
I0520 13:32:09.757361 31949 solver.cpp:253]     Train net output #0: loss = 1.38611 (* 1 = 1.38611 loss)
I0520 13:32:09.757377 31949 sgd_solver.cpp:106] Iteration 5610, lr = 0.0025
I0520 13:32:10.404373 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_5625.caffemodel
I0520 13:32:10.476160 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_5625.solverstate
I0520 13:32:18.472895 31949 solver.cpp:237] Iteration 5797, loss = 1.30759
I0520 13:32:18.473063 31949 solver.cpp:253]     Train net output #0: loss = 1.30759 (* 1 = 1.30759 loss)
I0520 13:32:18.473078 31949 sgd_solver.cpp:106] Iteration 5797, lr = 0.0025
I0520 13:32:27.113005 31949 solver.cpp:237] Iteration 5984, loss = 1.41961
I0520 13:32:27.113040 31949 solver.cpp:253]     Train net output #0: loss = 1.41961 (* 1 = 1.41961 loss)
I0520 13:32:27.113057 31949 sgd_solver.cpp:106] Iteration 5984, lr = 0.0025
I0520 13:32:35.758584 31949 solver.cpp:237] Iteration 6171, loss = 1.40694
I0520 13:32:35.758620 31949 solver.cpp:253]     Train net output #0: loss = 1.40694 (* 1 = 1.40694 loss)
I0520 13:32:35.758636 31949 sgd_solver.cpp:106] Iteration 6171, lr = 0.0025
I0520 13:33:06.585357 31949 solver.cpp:237] Iteration 6358, loss = 1.34942
I0520 13:33:06.585518 31949 solver.cpp:253]     Train net output #0: loss = 1.34942 (* 1 = 1.34942 loss)
I0520 13:33:06.585535 31949 sgd_solver.cpp:106] Iteration 6358, lr = 0.0025
I0520 13:33:15.230008 31949 solver.cpp:237] Iteration 6545, loss = 1.24456
I0520 13:33:15.230042 31949 solver.cpp:253]     Train net output #0: loss = 1.24456 (* 1 = 1.24456 loss)
I0520 13:33:15.230059 31949 sgd_solver.cpp:106] Iteration 6545, lr = 0.0025
I0520 13:33:23.874155 31949 solver.cpp:237] Iteration 6732, loss = 1.47844
I0520 13:33:23.874189 31949 solver.cpp:253]     Train net output #0: loss = 1.47844 (* 1 = 1.47844 loss)
I0520 13:33:23.874205 31949 sgd_solver.cpp:106] Iteration 6732, lr = 0.0025
I0520 13:33:32.518002 31949 solver.cpp:237] Iteration 6919, loss = 1.45222
I0520 13:33:32.518054 31949 solver.cpp:253]     Train net output #0: loss = 1.45222 (* 1 = 1.45222 loss)
I0520 13:33:32.518069 31949 sgd_solver.cpp:106] Iteration 6919, lr = 0.0025
I0520 13:33:41.161344 31949 solver.cpp:237] Iteration 7106, loss = 1.46416
I0520 13:33:41.161480 31949 solver.cpp:253]     Train net output #0: loss = 1.46416 (* 1 = 1.46416 loss)
I0520 13:33:41.161494 31949 sgd_solver.cpp:106] Iteration 7106, lr = 0.0025
I0520 13:33:49.803635 31949 solver.cpp:237] Iteration 7293, loss = 1.36492
I0520 13:33:49.803669 31949 solver.cpp:253]     Train net output #0: loss = 1.36492 (* 1 = 1.36492 loss)
I0520 13:33:49.803686 31949 sgd_solver.cpp:106] Iteration 7293, lr = 0.0025
I0520 13:33:58.448492 31949 solver.cpp:237] Iteration 7480, loss = 1.14864
I0520 13:33:58.448530 31949 solver.cpp:253]     Train net output #0: loss = 1.14864 (* 1 = 1.14864 loss)
I0520 13:33:58.448552 31949 sgd_solver.cpp:106] Iteration 7480, lr = 0.0025
I0520 13:33:59.327903 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_7500.caffemodel
I0520 13:33:59.399914 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_7500.solverstate
I0520 13:33:59.427474 31949 solver.cpp:341] Iteration 7500, Testing net (#0)
I0520 13:35:07.543236 31949 solver.cpp:409]     Test net output #0: accuracy = 0.8067
I0520 13:35:07.543404 31949 solver.cpp:409]     Test net output #1: loss = 0.700518 (* 1 = 0.700518 loss)
I0520 13:35:37.443950 31949 solver.cpp:237] Iteration 7667, loss = 1.31301
I0520 13:35:37.444000 31949 solver.cpp:253]     Train net output #0: loss = 1.31301 (* 1 = 1.31301 loss)
I0520 13:35:37.444017 31949 sgd_solver.cpp:106] Iteration 7667, lr = 0.0025
I0520 13:35:46.099417 31949 solver.cpp:237] Iteration 7854, loss = 1.32339
I0520 13:35:46.099565 31949 solver.cpp:253]     Train net output #0: loss = 1.32339 (* 1 = 1.32339 loss)
I0520 13:35:46.099581 31949 sgd_solver.cpp:106] Iteration 7854, lr = 0.0025
I0520 13:35:54.754571 31949 solver.cpp:237] Iteration 8041, loss = 1.40398
I0520 13:35:54.754606 31949 solver.cpp:253]     Train net output #0: loss = 1.40398 (* 1 = 1.40398 loss)
I0520 13:35:54.754622 31949 sgd_solver.cpp:106] Iteration 8041, lr = 0.0025
I0520 13:36:03.406769 31949 solver.cpp:237] Iteration 8228, loss = 1.43293
I0520 13:36:03.406807 31949 solver.cpp:253]     Train net output #0: loss = 1.43293 (* 1 = 1.43293 loss)
I0520 13:36:03.406822 31949 sgd_solver.cpp:106] Iteration 8228, lr = 0.0025
I0520 13:36:12.063681 31949 solver.cpp:237] Iteration 8415, loss = 1.37413
I0520 13:36:12.063717 31949 solver.cpp:253]     Train net output #0: loss = 1.37413 (* 1 = 1.37413 loss)
I0520 13:36:12.063732 31949 sgd_solver.cpp:106] Iteration 8415, lr = 0.0025
I0520 13:36:20.712034 31949 solver.cpp:237] Iteration 8602, loss = 1.34369
I0520 13:36:20.712172 31949 solver.cpp:253]     Train net output #0: loss = 1.34369 (* 1 = 1.34369 loss)
I0520 13:36:20.712187 31949 sgd_solver.cpp:106] Iteration 8602, lr = 0.0025
I0520 13:36:51.512044 31949 solver.cpp:237] Iteration 8789, loss = 1.4726
I0520 13:36:51.512217 31949 solver.cpp:253]     Train net output #0: loss = 1.4726 (* 1 = 1.4726 loss)
I0520 13:36:51.512233 31949 sgd_solver.cpp:106] Iteration 8789, lr = 0.0025
I0520 13:37:00.169050 31949 solver.cpp:237] Iteration 8976, loss = 1.29504
I0520 13:37:00.169085 31949 solver.cpp:253]     Train net output #0: loss = 1.29504 (* 1 = 1.29504 loss)
I0520 13:37:00.169102 31949 sgd_solver.cpp:106] Iteration 8976, lr = 0.0025
I0520 13:37:08.827468 31949 solver.cpp:237] Iteration 9163, loss = 1.36045
I0520 13:37:08.827503 31949 solver.cpp:253]     Train net output #0: loss = 1.36045 (* 1 = 1.36045 loss)
I0520 13:37:08.827519 31949 sgd_solver.cpp:106] Iteration 9163, lr = 0.0025
I0520 13:37:17.477560 31949 solver.cpp:237] Iteration 9350, loss = 1.20348
I0520 13:37:17.477605 31949 solver.cpp:253]     Train net output #0: loss = 1.20348 (* 1 = 1.20348 loss)
I0520 13:37:17.477622 31949 sgd_solver.cpp:106] Iteration 9350, lr = 0.0025
I0520 13:37:18.586765 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_9375.caffemodel
I0520 13:37:18.662060 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_9375.solverstate
I0520 13:37:26.199318 31949 solver.cpp:237] Iteration 9537, loss = 1.4482
I0520 13:37:26.199478 31949 solver.cpp:253]     Train net output #0: loss = 1.4482 (* 1 = 1.4482 loss)
I0520 13:37:26.199492 31949 sgd_solver.cpp:106] Iteration 9537, lr = 0.0025
I0520 13:37:34.852383 31949 solver.cpp:237] Iteration 9724, loss = 1.22006
I0520 13:37:34.852417 31949 solver.cpp:253]     Train net output #0: loss = 1.22006 (* 1 = 1.22006 loss)
I0520 13:37:34.852435 31949 sgd_solver.cpp:106] Iteration 9724, lr = 0.0025
I0520 13:37:43.508644 31949 solver.cpp:237] Iteration 9911, loss = 1.38479
I0520 13:37:43.508689 31949 solver.cpp:253]     Train net output #0: loss = 1.38479 (* 1 = 1.38479 loss)
I0520 13:37:43.508705 31949 sgd_solver.cpp:106] Iteration 9911, lr = 0.0025
I0520 13:38:14.308056 31949 solver.cpp:237] Iteration 10098, loss = 1.29528
I0520 13:38:14.308231 31949 solver.cpp:253]     Train net output #0: loss = 1.29528 (* 1 = 1.29528 loss)
I0520 13:38:14.308248 31949 sgd_solver.cpp:106] Iteration 10098, lr = 0.0025
I0520 13:38:22.961390 31949 solver.cpp:237] Iteration 10285, loss = 1.32841
I0520 13:38:22.961424 31949 solver.cpp:253]     Train net output #0: loss = 1.32841 (* 1 = 1.32841 loss)
I0520 13:38:22.961441 31949 sgd_solver.cpp:106] Iteration 10285, lr = 0.0025
I0520 13:38:31.618691 31949 solver.cpp:237] Iteration 10472, loss = 1.48666
I0520 13:38:31.618738 31949 solver.cpp:253]     Train net output #0: loss = 1.48666 (* 1 = 1.48666 loss)
I0520 13:38:31.618755 31949 sgd_solver.cpp:106] Iteration 10472, lr = 0.0025
I0520 13:38:40.270668 31949 solver.cpp:237] Iteration 10659, loss = 1.3207
I0520 13:38:40.270704 31949 solver.cpp:253]     Train net output #0: loss = 1.3207 (* 1 = 1.3207 loss)
I0520 13:38:40.270720 31949 sgd_solver.cpp:106] Iteration 10659, lr = 0.0025
I0520 13:38:48.919885 31949 solver.cpp:237] Iteration 10846, loss = 1.52049
I0520 13:38:48.920029 31949 solver.cpp:253]     Train net output #0: loss = 1.52049 (* 1 = 1.52049 loss)
I0520 13:38:48.920043 31949 sgd_solver.cpp:106] Iteration 10846, lr = 0.0025
I0520 13:38:57.572007 31949 solver.cpp:237] Iteration 11033, loss = 1.4138
I0520 13:38:57.572052 31949 solver.cpp:253]     Train net output #0: loss = 1.4138 (* 1 = 1.4138 loss)
I0520 13:38:57.572069 31949 sgd_solver.cpp:106] Iteration 11033, lr = 0.0025
I0520 13:39:06.231225 31949 solver.cpp:237] Iteration 11220, loss = 1.03313
I0520 13:39:06.231259 31949 solver.cpp:253]     Train net output #0: loss = 1.03313 (* 1 = 1.03313 loss)
I0520 13:39:06.231276 31949 sgd_solver.cpp:106] Iteration 11220, lr = 0.0025
I0520 13:39:07.574313 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_11250.caffemodel
I0520 13:39:07.648658 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_11250.solverstate
I0520 13:39:07.674875 31949 solver.cpp:341] Iteration 11250, Testing net (#0)
I0520 13:39:54.668848 31949 solver.cpp:409]     Test net output #0: accuracy = 0.83262
I0520 13:39:54.669008 31949 solver.cpp:409]     Test net output #1: loss = 0.624162 (* 1 = 0.624162 loss)
I0520 13:40:24.154055 31949 solver.cpp:237] Iteration 11407, loss = 1.48164
I0520 13:40:24.154105 31949 solver.cpp:253]     Train net output #0: loss = 1.48164 (* 1 = 1.48164 loss)
I0520 13:40:24.154121 31949 sgd_solver.cpp:106] Iteration 11407, lr = 0.0025
I0520 13:40:32.802541 31949 solver.cpp:237] Iteration 11594, loss = 1.34072
I0520 13:40:32.802703 31949 solver.cpp:253]     Train net output #0: loss = 1.34072 (* 1 = 1.34072 loss)
I0520 13:40:32.802717 31949 sgd_solver.cpp:106] Iteration 11594, lr = 0.0025
I0520 13:40:41.451395 31949 solver.cpp:237] Iteration 11781, loss = 1.20036
I0520 13:40:41.451429 31949 solver.cpp:253]     Train net output #0: loss = 1.20036 (* 1 = 1.20036 loss)
I0520 13:40:41.451447 31949 sgd_solver.cpp:106] Iteration 11781, lr = 0.0025
I0520 13:40:50.102092 31949 solver.cpp:237] Iteration 11968, loss = 1.26403
I0520 13:40:50.102128 31949 solver.cpp:253]     Train net output #0: loss = 1.26403 (* 1 = 1.26403 loss)
I0520 13:40:50.102144 31949 sgd_solver.cpp:106] Iteration 11968, lr = 0.0025
I0520 13:40:58.755285 31949 solver.cpp:237] Iteration 12155, loss = 1.11651
I0520 13:40:58.755326 31949 solver.cpp:253]     Train net output #0: loss = 1.11651 (* 1 = 1.11651 loss)
I0520 13:40:58.755347 31949 sgd_solver.cpp:106] Iteration 12155, lr = 0.0025
I0520 13:41:07.401845 31949 solver.cpp:237] Iteration 12342, loss = 1.11108
I0520 13:41:07.401985 31949 solver.cpp:253]     Train net output #0: loss = 1.11108 (* 1 = 1.11108 loss)
I0520 13:41:07.401999 31949 sgd_solver.cpp:106] Iteration 12342, lr = 0.0025
I0520 13:41:38.285585 31949 solver.cpp:237] Iteration 12529, loss = 1.12825
I0520 13:41:38.285763 31949 solver.cpp:253]     Train net output #0: loss = 1.12825 (* 1 = 1.12825 loss)
I0520 13:41:38.285778 31949 sgd_solver.cpp:106] Iteration 12529, lr = 0.0025
I0520 13:41:46.932837 31949 solver.cpp:237] Iteration 12716, loss = 1.40013
I0520 13:41:46.932883 31949 solver.cpp:253]     Train net output #0: loss = 1.40013 (* 1 = 1.40013 loss)
I0520 13:41:46.932903 31949 sgd_solver.cpp:106] Iteration 12716, lr = 0.0025
I0520 13:41:55.580723 31949 solver.cpp:237] Iteration 12903, loss = 1.27709
I0520 13:41:55.580759 31949 solver.cpp:253]     Train net output #0: loss = 1.27709 (* 1 = 1.27709 loss)
I0520 13:41:55.580773 31949 sgd_solver.cpp:106] Iteration 12903, lr = 0.0025
I0520 13:42:04.230690 31949 solver.cpp:237] Iteration 13090, loss = 1.20505
I0520 13:42:04.230725 31949 solver.cpp:253]     Train net output #0: loss = 1.20505 (* 1 = 1.20505 loss)
I0520 13:42:04.230741 31949 sgd_solver.cpp:106] Iteration 13090, lr = 0.0025
I0520 13:42:05.804266 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_13125.caffemodel
I0520 13:42:05.873369 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_13125.solverstate
I0520 13:42:12.942975 31949 solver.cpp:237] Iteration 13277, loss = 1.08479
I0520 13:42:12.943138 31949 solver.cpp:253]     Train net output #0: loss = 1.08479 (* 1 = 1.08479 loss)
I0520 13:42:12.943152 31949 sgd_solver.cpp:106] Iteration 13277, lr = 0.0025
I0520 13:42:21.591648 31949 solver.cpp:237] Iteration 13464, loss = 1.30485
I0520 13:42:21.591682 31949 solver.cpp:253]     Train net output #0: loss = 1.30485 (* 1 = 1.30485 loss)
I0520 13:42:21.591699 31949 sgd_solver.cpp:106] Iteration 13464, lr = 0.0025
I0520 13:42:30.242250 31949 solver.cpp:237] Iteration 13651, loss = 1.44644
I0520 13:42:30.242286 31949 solver.cpp:253]     Train net output #0: loss = 1.44644 (* 1 = 1.44644 loss)
I0520 13:42:30.242301 31949 sgd_solver.cpp:106] Iteration 13651, lr = 0.0025
I0520 13:43:01.028574 31949 solver.cpp:237] Iteration 13838, loss = 1.41288
I0520 13:43:01.028743 31949 solver.cpp:253]     Train net output #0: loss = 1.41288 (* 1 = 1.41288 loss)
I0520 13:43:01.028758 31949 sgd_solver.cpp:106] Iteration 13838, lr = 0.0025
I0520 13:43:09.671219 31949 solver.cpp:237] Iteration 14025, loss = 1.23855
I0520 13:43:09.671254 31949 solver.cpp:253]     Train net output #0: loss = 1.23855 (* 1 = 1.23855 loss)
I0520 13:43:09.671273 31949 sgd_solver.cpp:106] Iteration 14025, lr = 0.0025
I0520 13:43:18.317631 31949 solver.cpp:237] Iteration 14212, loss = 1.31552
I0520 13:43:18.317667 31949 solver.cpp:253]     Train net output #0: loss = 1.31552 (* 1 = 1.31552 loss)
I0520 13:43:18.317682 31949 sgd_solver.cpp:106] Iteration 14212, lr = 0.0025
I0520 13:43:26.967715 31949 solver.cpp:237] Iteration 14399, loss = 1.19552
I0520 13:43:26.967754 31949 solver.cpp:253]     Train net output #0: loss = 1.19552 (* 1 = 1.19552 loss)
I0520 13:43:26.967773 31949 sgd_solver.cpp:106] Iteration 14399, lr = 0.0025
I0520 13:43:35.615455 31949 solver.cpp:237] Iteration 14586, loss = 1.30113
I0520 13:43:35.615594 31949 solver.cpp:253]     Train net output #0: loss = 1.30113 (* 1 = 1.30113 loss)
I0520 13:43:35.615608 31949 sgd_solver.cpp:106] Iteration 14586, lr = 0.0025
I0520 13:43:44.264034 31949 solver.cpp:237] Iteration 14773, loss = 1.54112
I0520 13:43:44.264067 31949 solver.cpp:253]     Train net output #0: loss = 1.54112 (* 1 = 1.54112 loss)
I0520 13:43:44.264084 31949 sgd_solver.cpp:106] Iteration 14773, lr = 0.0025
I0520 13:43:52.911950 31949 solver.cpp:237] Iteration 14960, loss = 1.22486
I0520 13:43:52.911988 31949 solver.cpp:253]     Train net output #0: loss = 1.22486 (* 1 = 1.22486 loss)
I0520 13:43:52.912009 31949 sgd_solver.cpp:106] Iteration 14960, lr = 0.0025
I0520 13:43:54.713804 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_15000.caffemodel
I0520 13:43:54.782898 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_15000.solverstate
I0520 13:43:54.809228 31949 solver.cpp:341] Iteration 15000, Testing net (#0)
I0520 13:45:02.933591 31949 solver.cpp:409]     Test net output #0: accuracy = 0.84104
I0520 13:45:02.933765 31949 solver.cpp:409]     Test net output #1: loss = 0.537585 (* 1 = 0.537585 loss)
I0520 13:45:31.896432 31949 solver.cpp:237] Iteration 15147, loss = 1.55538
I0520 13:45:31.896487 31949 solver.cpp:253]     Train net output #0: loss = 1.55538 (* 1 = 1.55538 loss)
I0520 13:45:31.896502 31949 sgd_solver.cpp:106] Iteration 15147, lr = 0.0025
I0520 13:45:40.543954 31949 solver.cpp:237] Iteration 15334, loss = 1.28502
I0520 13:45:40.544105 31949 solver.cpp:253]     Train net output #0: loss = 1.28502 (* 1 = 1.28502 loss)
I0520 13:45:40.544118 31949 sgd_solver.cpp:106] Iteration 15334, lr = 0.0025
I0520 13:45:49.193179 31949 solver.cpp:237] Iteration 15521, loss = 1.18426
I0520 13:45:49.193213 31949 solver.cpp:253]     Train net output #0: loss = 1.18426 (* 1 = 1.18426 loss)
I0520 13:45:49.193229 31949 sgd_solver.cpp:106] Iteration 15521, lr = 0.0025
I0520 13:45:57.840153 31949 solver.cpp:237] Iteration 15708, loss = 1.34441
I0520 13:45:57.840196 31949 solver.cpp:253]     Train net output #0: loss = 1.34441 (* 1 = 1.34441 loss)
I0520 13:45:57.840214 31949 sgd_solver.cpp:106] Iteration 15708, lr = 0.0025
I0520 13:46:06.488040 31949 solver.cpp:237] Iteration 15895, loss = 1.39679
I0520 13:46:06.488075 31949 solver.cpp:253]     Train net output #0: loss = 1.39679 (* 1 = 1.39679 loss)
I0520 13:46:06.488091 31949 sgd_solver.cpp:106] Iteration 15895, lr = 0.0025
I0520 13:46:15.140190 31949 solver.cpp:237] Iteration 16082, loss = 1.26042
I0520 13:46:15.140334 31949 solver.cpp:253]     Train net output #0: loss = 1.26042 (* 1 = 1.26042 loss)
I0520 13:46:15.140347 31949 sgd_solver.cpp:106] Iteration 16082, lr = 0.0025
I0520 13:46:45.996670 31949 solver.cpp:237] Iteration 16269, loss = 1.35263
I0520 13:46:45.996840 31949 solver.cpp:253]     Train net output #0: loss = 1.35263 (* 1 = 1.35263 loss)
I0520 13:46:45.996855 31949 sgd_solver.cpp:106] Iteration 16269, lr = 0.0025
I0520 13:46:54.643221 31949 solver.cpp:237] Iteration 16456, loss = 1.39683
I0520 13:46:54.643260 31949 solver.cpp:253]     Train net output #0: loss = 1.39683 (* 1 = 1.39683 loss)
I0520 13:46:54.643280 31949 sgd_solver.cpp:106] Iteration 16456, lr = 0.0025
I0520 13:47:03.293653 31949 solver.cpp:237] Iteration 16643, loss = 1.4071
I0520 13:47:03.293687 31949 solver.cpp:253]     Train net output #0: loss = 1.4071 (* 1 = 1.4071 loss)
I0520 13:47:03.293702 31949 sgd_solver.cpp:106] Iteration 16643, lr = 0.0025
I0520 13:47:11.941834 31949 solver.cpp:237] Iteration 16830, loss = 1.35712
I0520 13:47:11.941884 31949 solver.cpp:253]     Train net output #0: loss = 1.35712 (* 1 = 1.35712 loss)
I0520 13:47:11.941900 31949 sgd_solver.cpp:106] Iteration 16830, lr = 0.0025
I0520 13:47:13.976222 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_16875.caffemodel
I0520 13:47:14.048305 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_16875.solverstate
I0520 13:47:20.661324 31949 solver.cpp:237] Iteration 17017, loss = 1.22257
I0520 13:47:20.661489 31949 solver.cpp:253]     Train net output #0: loss = 1.22257 (* 1 = 1.22257 loss)
I0520 13:47:20.661502 31949 sgd_solver.cpp:106] Iteration 17017, lr = 0.0025
I0520 13:47:29.309075 31949 solver.cpp:237] Iteration 17204, loss = 1.28591
I0520 13:47:29.309109 31949 solver.cpp:253]     Train net output #0: loss = 1.28591 (* 1 = 1.28591 loss)
I0520 13:47:29.309126 31949 sgd_solver.cpp:106] Iteration 17204, lr = 0.0025
I0520 13:47:37.958904 31949 solver.cpp:237] Iteration 17391, loss = 1.27763
I0520 13:47:37.958955 31949 solver.cpp:253]     Train net output #0: loss = 1.27763 (* 1 = 1.27763 loss)
I0520 13:47:37.958971 31949 sgd_solver.cpp:106] Iteration 17391, lr = 0.0025
I0520 13:48:08.762378 31949 solver.cpp:237] Iteration 17578, loss = 1.17346
I0520 13:48:08.762555 31949 solver.cpp:253]     Train net output #0: loss = 1.17346 (* 1 = 1.17346 loss)
I0520 13:48:08.762570 31949 sgd_solver.cpp:106] Iteration 17578, lr = 0.0025
I0520 13:48:17.410601 31949 solver.cpp:237] Iteration 17765, loss = 1.02848
I0520 13:48:17.410636 31949 solver.cpp:253]     Train net output #0: loss = 1.02848 (* 1 = 1.02848 loss)
I0520 13:48:17.410655 31949 sgd_solver.cpp:106] Iteration 17765, lr = 0.0025
I0520 13:48:26.061751 31949 solver.cpp:237] Iteration 17952, loss = 1.15984
I0520 13:48:26.061786 31949 solver.cpp:253]     Train net output #0: loss = 1.15984 (* 1 = 1.15984 loss)
I0520 13:48:26.061802 31949 sgd_solver.cpp:106] Iteration 17952, lr = 0.0025
I0520 13:48:34.712398 31949 solver.cpp:237] Iteration 18139, loss = 1.28806
I0520 13:48:34.712440 31949 solver.cpp:253]     Train net output #0: loss = 1.28806 (* 1 = 1.28806 loss)
I0520 13:48:34.712460 31949 sgd_solver.cpp:106] Iteration 18139, lr = 0.0025
I0520 13:48:43.358821 31949 solver.cpp:237] Iteration 18326, loss = 1.07615
I0520 13:48:43.358969 31949 solver.cpp:253]     Train net output #0: loss = 1.07615 (* 1 = 1.07615 loss)
I0520 13:48:43.358985 31949 sgd_solver.cpp:106] Iteration 18326, lr = 0.0025
I0520 13:48:52.003859 31949 solver.cpp:237] Iteration 18513, loss = 1.41176
I0520 13:48:52.003901 31949 solver.cpp:253]     Train net output #0: loss = 1.41176 (* 1 = 1.41176 loss)
I0520 13:48:52.003921 31949 sgd_solver.cpp:106] Iteration 18513, lr = 0.0025
I0520 13:49:00.655138 31949 solver.cpp:237] Iteration 18700, loss = 1.40084
I0520 13:49:00.655174 31949 solver.cpp:253]     Train net output #0: loss = 1.40084 (* 1 = 1.40084 loss)
I0520 13:49:00.655189 31949 sgd_solver.cpp:106] Iteration 18700, lr = 0.0025
I0520 13:49:02.924096 31949 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_18750.caffemodel
I0520 13:49:02.996384 31949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834_iter_18750.solverstate
I0520 13:49:03.023542 31949 solver.cpp:341] Iteration 18750, Testing net (#0)
I0520 13:49:50.326119 31949 solver.cpp:409]     Test net output #0: accuracy = 0.853193
I0520 13:49:50.326282 31949 solver.cpp:409]     Test net output #1: loss = 0.478755 (* 1 = 0.478755 loss)
I0520 13:49:50.326297 31949 solver.cpp:326] Optimization Done.
I0520 13:49:50.326309 31949 caffe.cpp:215] Optimization Done.
Application 11232336 resources: utime ~1318s, stime ~232s, Rss ~5333300, inblocks ~3594474, outblocks ~179817
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_80_2016-05-20T11.20.35.603834.solver"
	User time (seconds): 0.57
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:55.84
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15082
	Voluntary context switches: 2981
	Involuntary context switches: 279
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
