2806464
I0521 11:09:43.640305 27022 caffe.cpp:184] Using GPUs 0
I0521 11:09:44.064211 27022 solver.cpp:48] Initializing solver from parameters: 
test_iter: 153
test_interval: 306
base_lr: 0.0025
display: 15
max_iter: 1530
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 153
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619.prototxt"
I0521 11:09:44.066259 27022 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619.prototxt
I0521 11:09:44.082468 27022 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 11:09:44.082533 27022 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 11:09:44.082911 27022 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 980
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 11:09:44.083114 27022 layer_factory.hpp:77] Creating layer data_hdf5
I0521 11:09:44.083150 27022 net.cpp:106] Creating Layer data_hdf5
I0521 11:09:44.083168 27022 net.cpp:411] data_hdf5 -> data
I0521 11:09:44.083201 27022 net.cpp:411] data_hdf5 -> label
I0521 11:09:44.083243 27022 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 11:09:44.084486 27022 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 11:09:44.086736 27022 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 11:10:05.594614 27022 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 11:10:05.599855 27022 net.cpp:150] Setting up data_hdf5
I0521 11:10:05.599896 27022 net.cpp:157] Top shape: 980 1 127 50 (6223000)
I0521 11:10:05.599915 27022 net.cpp:157] Top shape: 980 (980)
I0521 11:10:05.599927 27022 net.cpp:165] Memory required for data: 24895920
I0521 11:10:05.599946 27022 layer_factory.hpp:77] Creating layer conv1
I0521 11:10:05.599992 27022 net.cpp:106] Creating Layer conv1
I0521 11:10:05.600008 27022 net.cpp:454] conv1 <- data
I0521 11:10:05.600031 27022 net.cpp:411] conv1 -> conv1
I0521 11:10:06.638336 27022 net.cpp:150] Setting up conv1
I0521 11:10:06.638391 27022 net.cpp:157] Top shape: 980 12 120 48 (67737600)
I0521 11:10:06.638417 27022 net.cpp:165] Memory required for data: 295846320
I0521 11:10:06.638447 27022 layer_factory.hpp:77] Creating layer relu1
I0521 11:10:06.638468 27022 net.cpp:106] Creating Layer relu1
I0521 11:10:06.638489 27022 net.cpp:454] relu1 <- conv1
I0521 11:10:06.638525 27022 net.cpp:397] relu1 -> conv1 (in-place)
I0521 11:10:06.639056 27022 net.cpp:150] Setting up relu1
I0521 11:10:06.639080 27022 net.cpp:157] Top shape: 980 12 120 48 (67737600)
I0521 11:10:06.639093 27022 net.cpp:165] Memory required for data: 566796720
I0521 11:10:06.639109 27022 layer_factory.hpp:77] Creating layer pool1
I0521 11:10:06.639137 27022 net.cpp:106] Creating Layer pool1
I0521 11:10:06.639149 27022 net.cpp:454] pool1 <- conv1
I0521 11:10:06.639165 27022 net.cpp:411] pool1 -> pool1
I0521 11:10:06.639340 27022 net.cpp:150] Setting up pool1
I0521 11:10:06.639361 27022 net.cpp:157] Top shape: 980 12 60 48 (33868800)
I0521 11:10:06.639374 27022 net.cpp:165] Memory required for data: 702271920
I0521 11:10:06.639390 27022 layer_factory.hpp:77] Creating layer conv2
I0521 11:10:06.639420 27022 net.cpp:106] Creating Layer conv2
I0521 11:10:06.639435 27022 net.cpp:454] conv2 <- pool1
I0521 11:10:06.639451 27022 net.cpp:411] conv2 -> conv2
I0521 11:10:06.642165 27022 net.cpp:150] Setting up conv2
I0521 11:10:06.642196 27022 net.cpp:157] Top shape: 980 20 54 46 (48686400)
I0521 11:10:06.642210 27022 net.cpp:165] Memory required for data: 897017520
I0521 11:10:06.642238 27022 layer_factory.hpp:77] Creating layer relu2
I0521 11:10:06.642266 27022 net.cpp:106] Creating Layer relu2
I0521 11:10:06.642279 27022 net.cpp:454] relu2 <- conv2
I0521 11:10:06.642295 27022 net.cpp:397] relu2 -> conv2 (in-place)
I0521 11:10:06.642678 27022 net.cpp:150] Setting up relu2
I0521 11:10:06.642698 27022 net.cpp:157] Top shape: 980 20 54 46 (48686400)
I0521 11:10:06.642710 27022 net.cpp:165] Memory required for data: 1091763120
I0521 11:10:06.642725 27022 layer_factory.hpp:77] Creating layer pool2
I0521 11:10:06.642748 27022 net.cpp:106] Creating Layer pool2
I0521 11:10:06.642761 27022 net.cpp:454] pool2 <- conv2
I0521 11:10:06.642796 27022 net.cpp:411] pool2 -> pool2
I0521 11:10:06.642879 27022 net.cpp:150] Setting up pool2
I0521 11:10:06.642897 27022 net.cpp:157] Top shape: 980 20 27 46 (24343200)
I0521 11:10:06.642912 27022 net.cpp:165] Memory required for data: 1189135920
I0521 11:10:06.642931 27022 layer_factory.hpp:77] Creating layer conv3
I0521 11:10:06.642952 27022 net.cpp:106] Creating Layer conv3
I0521 11:10:06.642966 27022 net.cpp:454] conv3 <- pool2
I0521 11:10:06.642982 27022 net.cpp:411] conv3 -> conv3
I0521 11:10:06.644979 27022 net.cpp:150] Setting up conv3
I0521 11:10:06.645005 27022 net.cpp:157] Top shape: 980 28 22 44 (26561920)
I0521 11:10:06.645025 27022 net.cpp:165] Memory required for data: 1295383600
I0521 11:10:06.645047 27022 layer_factory.hpp:77] Creating layer relu3
I0521 11:10:06.645069 27022 net.cpp:106] Creating Layer relu3
I0521 11:10:06.645092 27022 net.cpp:454] relu3 <- conv3
I0521 11:10:06.645107 27022 net.cpp:397] relu3 -> conv3 (in-place)
I0521 11:10:06.645604 27022 net.cpp:150] Setting up relu3
I0521 11:10:06.645629 27022 net.cpp:157] Top shape: 980 28 22 44 (26561920)
I0521 11:10:06.645642 27022 net.cpp:165] Memory required for data: 1401631280
I0521 11:10:06.645658 27022 layer_factory.hpp:77] Creating layer pool3
I0521 11:10:06.645683 27022 net.cpp:106] Creating Layer pool3
I0521 11:10:06.645696 27022 net.cpp:454] pool3 <- conv3
I0521 11:10:06.645711 27022 net.cpp:411] pool3 -> pool3
I0521 11:10:06.645794 27022 net.cpp:150] Setting up pool3
I0521 11:10:06.645819 27022 net.cpp:157] Top shape: 980 28 11 44 (13280960)
I0521 11:10:06.645833 27022 net.cpp:165] Memory required for data: 1454755120
I0521 11:10:06.645848 27022 layer_factory.hpp:77] Creating layer conv4
I0521 11:10:06.645866 27022 net.cpp:106] Creating Layer conv4
I0521 11:10:06.645885 27022 net.cpp:454] conv4 <- pool3
I0521 11:10:06.645902 27022 net.cpp:411] conv4 -> conv4
I0521 11:10:06.648670 27022 net.cpp:150] Setting up conv4
I0521 11:10:06.648700 27022 net.cpp:157] Top shape: 980 36 6 42 (8890560)
I0521 11:10:06.648716 27022 net.cpp:165] Memory required for data: 1490317360
I0521 11:10:06.648736 27022 layer_factory.hpp:77] Creating layer relu4
I0521 11:10:06.648756 27022 net.cpp:106] Creating Layer relu4
I0521 11:10:06.648782 27022 net.cpp:454] relu4 <- conv4
I0521 11:10:06.648798 27022 net.cpp:397] relu4 -> conv4 (in-place)
I0521 11:10:06.649288 27022 net.cpp:150] Setting up relu4
I0521 11:10:06.649312 27022 net.cpp:157] Top shape: 980 36 6 42 (8890560)
I0521 11:10:06.649325 27022 net.cpp:165] Memory required for data: 1525879600
I0521 11:10:06.649341 27022 layer_factory.hpp:77] Creating layer pool4
I0521 11:10:06.649365 27022 net.cpp:106] Creating Layer pool4
I0521 11:10:06.649379 27022 net.cpp:454] pool4 <- conv4
I0521 11:10:06.649395 27022 net.cpp:411] pool4 -> pool4
I0521 11:10:06.649478 27022 net.cpp:150] Setting up pool4
I0521 11:10:06.649508 27022 net.cpp:157] Top shape: 980 36 3 42 (4445280)
I0521 11:10:06.649523 27022 net.cpp:165] Memory required for data: 1543660720
I0521 11:10:06.649538 27022 layer_factory.hpp:77] Creating layer ip1
I0521 11:10:06.649566 27022 net.cpp:106] Creating Layer ip1
I0521 11:10:06.649580 27022 net.cpp:454] ip1 <- pool4
I0521 11:10:06.649601 27022 net.cpp:411] ip1 -> ip1
I0521 11:10:06.665019 27022 net.cpp:150] Setting up ip1
I0521 11:10:06.665050 27022 net.cpp:157] Top shape: 980 196 (192080)
I0521 11:10:06.665074 27022 net.cpp:165] Memory required for data: 1544429040
I0521 11:10:06.665101 27022 layer_factory.hpp:77] Creating layer relu5
I0521 11:10:06.665122 27022 net.cpp:106] Creating Layer relu5
I0521 11:10:06.665148 27022 net.cpp:454] relu5 <- ip1
I0521 11:10:06.665165 27022 net.cpp:397] relu5 -> ip1 (in-place)
I0521 11:10:06.665530 27022 net.cpp:150] Setting up relu5
I0521 11:10:06.665551 27022 net.cpp:157] Top shape: 980 196 (192080)
I0521 11:10:06.665565 27022 net.cpp:165] Memory required for data: 1545197360
I0521 11:10:06.665580 27022 layer_factory.hpp:77] Creating layer drop1
I0521 11:10:06.665611 27022 net.cpp:106] Creating Layer drop1
I0521 11:10:06.665624 27022 net.cpp:454] drop1 <- ip1
I0521 11:10:06.665652 27022 net.cpp:397] drop1 -> ip1 (in-place)
I0521 11:10:06.665710 27022 net.cpp:150] Setting up drop1
I0521 11:10:06.665727 27022 net.cpp:157] Top shape: 980 196 (192080)
I0521 11:10:06.665741 27022 net.cpp:165] Memory required for data: 1545965680
I0521 11:10:06.665753 27022 layer_factory.hpp:77] Creating layer ip2
I0521 11:10:06.665776 27022 net.cpp:106] Creating Layer ip2
I0521 11:10:06.665796 27022 net.cpp:454] ip2 <- ip1
I0521 11:10:06.665812 27022 net.cpp:411] ip2 -> ip2
I0521 11:10:06.666306 27022 net.cpp:150] Setting up ip2
I0521 11:10:06.666324 27022 net.cpp:157] Top shape: 980 98 (96040)
I0521 11:10:06.666337 27022 net.cpp:165] Memory required for data: 1546349840
I0521 11:10:06.666358 27022 layer_factory.hpp:77] Creating layer relu6
I0521 11:10:06.666381 27022 net.cpp:106] Creating Layer relu6
I0521 11:10:06.666394 27022 net.cpp:454] relu6 <- ip2
I0521 11:10:06.666409 27022 net.cpp:397] relu6 -> ip2 (in-place)
I0521 11:10:06.666960 27022 net.cpp:150] Setting up relu6
I0521 11:10:06.666982 27022 net.cpp:157] Top shape: 980 98 (96040)
I0521 11:10:06.666996 27022 net.cpp:165] Memory required for data: 1546734000
I0521 11:10:06.667012 27022 layer_factory.hpp:77] Creating layer drop2
I0521 11:10:06.667037 27022 net.cpp:106] Creating Layer drop2
I0521 11:10:06.667050 27022 net.cpp:454] drop2 <- ip2
I0521 11:10:06.667067 27022 net.cpp:397] drop2 -> ip2 (in-place)
I0521 11:10:06.667115 27022 net.cpp:150] Setting up drop2
I0521 11:10:06.667137 27022 net.cpp:157] Top shape: 980 98 (96040)
I0521 11:10:06.667151 27022 net.cpp:165] Memory required for data: 1547118160
I0521 11:10:06.667163 27022 layer_factory.hpp:77] Creating layer ip3
I0521 11:10:06.667179 27022 net.cpp:106] Creating Layer ip3
I0521 11:10:06.667194 27022 net.cpp:454] ip3 <- ip2
I0521 11:10:06.667217 27022 net.cpp:411] ip3 -> ip3
I0521 11:10:06.667443 27022 net.cpp:150] Setting up ip3
I0521 11:10:06.667461 27022 net.cpp:157] Top shape: 980 11 (10780)
I0521 11:10:06.667474 27022 net.cpp:165] Memory required for data: 1547161280
I0521 11:10:06.667495 27022 layer_factory.hpp:77] Creating layer drop3
I0521 11:10:06.667516 27022 net.cpp:106] Creating Layer drop3
I0521 11:10:06.667529 27022 net.cpp:454] drop3 <- ip3
I0521 11:10:06.667546 27022 net.cpp:397] drop3 -> ip3 (in-place)
I0521 11:10:06.667592 27022 net.cpp:150] Setting up drop3
I0521 11:10:06.667614 27022 net.cpp:157] Top shape: 980 11 (10780)
I0521 11:10:06.667628 27022 net.cpp:165] Memory required for data: 1547204400
I0521 11:10:06.667647 27022 layer_factory.hpp:77] Creating layer loss
I0521 11:10:06.667668 27022 net.cpp:106] Creating Layer loss
I0521 11:10:06.667685 27022 net.cpp:454] loss <- ip3
I0521 11:10:06.667703 27022 net.cpp:454] loss <- label
I0521 11:10:06.667719 27022 net.cpp:411] loss -> loss
I0521 11:10:06.667739 27022 layer_factory.hpp:77] Creating layer loss
I0521 11:10:06.668426 27022 net.cpp:150] Setting up loss
I0521 11:10:06.668447 27022 net.cpp:157] Top shape: (1)
I0521 11:10:06.668472 27022 net.cpp:160]     with loss weight 1
I0521 11:10:06.668520 27022 net.cpp:165] Memory required for data: 1547204404
I0521 11:10:06.668542 27022 net.cpp:226] loss needs backward computation.
I0521 11:10:06.668556 27022 net.cpp:226] drop3 needs backward computation.
I0521 11:10:06.668570 27022 net.cpp:226] ip3 needs backward computation.
I0521 11:10:06.668583 27022 net.cpp:226] drop2 needs backward computation.
I0521 11:10:06.668596 27022 net.cpp:226] relu6 needs backward computation.
I0521 11:10:06.668611 27022 net.cpp:226] ip2 needs backward computation.
I0521 11:10:06.668623 27022 net.cpp:226] drop1 needs backward computation.
I0521 11:10:06.668642 27022 net.cpp:226] relu5 needs backward computation.
I0521 11:10:06.668655 27022 net.cpp:226] ip1 needs backward computation.
I0521 11:10:06.668673 27022 net.cpp:226] pool4 needs backward computation.
I0521 11:10:06.668686 27022 net.cpp:226] relu4 needs backward computation.
I0521 11:10:06.668699 27022 net.cpp:226] conv4 needs backward computation.
I0521 11:10:06.668715 27022 net.cpp:226] pool3 needs backward computation.
I0521 11:10:06.668745 27022 net.cpp:226] relu3 needs backward computation.
I0521 11:10:06.668759 27022 net.cpp:226] conv3 needs backward computation.
I0521 11:10:06.668772 27022 net.cpp:226] pool2 needs backward computation.
I0521 11:10:06.668787 27022 net.cpp:226] relu2 needs backward computation.
I0521 11:10:06.668798 27022 net.cpp:226] conv2 needs backward computation.
I0521 11:10:06.668814 27022 net.cpp:226] pool1 needs backward computation.
I0521 11:10:06.668826 27022 net.cpp:226] relu1 needs backward computation.
I0521 11:10:06.668846 27022 net.cpp:226] conv1 needs backward computation.
I0521 11:10:06.668861 27022 net.cpp:228] data_hdf5 does not need backward computation.
I0521 11:10:06.668876 27022 net.cpp:270] This network produces output loss
I0521 11:10:06.668903 27022 net.cpp:283] Network initialization done.
I0521 11:10:06.670524 27022 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619.prototxt
I0521 11:10:06.670603 27022 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 11:10:06.670980 27022 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 980
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 11:10:06.671202 27022 layer_factory.hpp:77] Creating layer data_hdf5
I0521 11:10:06.671222 27022 net.cpp:106] Creating Layer data_hdf5
I0521 11:10:06.671237 27022 net.cpp:411] data_hdf5 -> data
I0521 11:10:06.671259 27022 net.cpp:411] data_hdf5 -> label
I0521 11:10:06.671277 27022 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 11:10:06.672469 27022 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 11:10:27.972915 27022 net.cpp:150] Setting up data_hdf5
I0521 11:10:27.973083 27022 net.cpp:157] Top shape: 980 1 127 50 (6223000)
I0521 11:10:27.973104 27022 net.cpp:157] Top shape: 980 (980)
I0521 11:10:27.973116 27022 net.cpp:165] Memory required for data: 24895920
I0521 11:10:27.973131 27022 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 11:10:27.973166 27022 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 11:10:27.973179 27022 net.cpp:454] label_data_hdf5_1_split <- label
I0521 11:10:27.973196 27022 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 11:10:27.973239 27022 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 11:10:27.973325 27022 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 11:10:27.973343 27022 net.cpp:157] Top shape: 980 (980)
I0521 11:10:27.973358 27022 net.cpp:157] Top shape: 980 (980)
I0521 11:10:27.973381 27022 net.cpp:165] Memory required for data: 24903760
I0521 11:10:27.973393 27022 layer_factory.hpp:77] Creating layer conv1
I0521 11:10:27.973419 27022 net.cpp:106] Creating Layer conv1
I0521 11:10:27.973438 27022 net.cpp:454] conv1 <- data
I0521 11:10:27.973456 27022 net.cpp:411] conv1 -> conv1
I0521 11:10:27.975426 27022 net.cpp:150] Setting up conv1
I0521 11:10:27.975452 27022 net.cpp:157] Top shape: 980 12 120 48 (67737600)
I0521 11:10:27.975472 27022 net.cpp:165] Memory required for data: 295854160
I0521 11:10:27.975497 27022 layer_factory.hpp:77] Creating layer relu1
I0521 11:10:27.975518 27022 net.cpp:106] Creating Layer relu1
I0521 11:10:27.975541 27022 net.cpp:454] relu1 <- conv1
I0521 11:10:27.975558 27022 net.cpp:397] relu1 -> conv1 (in-place)
I0521 11:10:27.976075 27022 net.cpp:150] Setting up relu1
I0521 11:10:27.976099 27022 net.cpp:157] Top shape: 980 12 120 48 (67737600)
I0521 11:10:27.976112 27022 net.cpp:165] Memory required for data: 566804560
I0521 11:10:27.976125 27022 layer_factory.hpp:77] Creating layer pool1
I0521 11:10:27.976155 27022 net.cpp:106] Creating Layer pool1
I0521 11:10:27.976168 27022 net.cpp:454] pool1 <- conv1
I0521 11:10:27.976184 27022 net.cpp:411] pool1 -> pool1
I0521 11:10:27.976274 27022 net.cpp:150] Setting up pool1
I0521 11:10:27.976290 27022 net.cpp:157] Top shape: 980 12 60 48 (33868800)
I0521 11:10:27.976305 27022 net.cpp:165] Memory required for data: 702279760
I0521 11:10:27.976325 27022 layer_factory.hpp:77] Creating layer conv2
I0521 11:10:27.976346 27022 net.cpp:106] Creating Layer conv2
I0521 11:10:27.976367 27022 net.cpp:454] conv2 <- pool1
I0521 11:10:27.976384 27022 net.cpp:411] conv2 -> conv2
I0521 11:10:27.978332 27022 net.cpp:150] Setting up conv2
I0521 11:10:27.978358 27022 net.cpp:157] Top shape: 980 20 54 46 (48686400)
I0521 11:10:27.978379 27022 net.cpp:165] Memory required for data: 897025360
I0521 11:10:27.978399 27022 layer_factory.hpp:77] Creating layer relu2
I0521 11:10:27.978420 27022 net.cpp:106] Creating Layer relu2
I0521 11:10:27.978441 27022 net.cpp:454] relu2 <- conv2
I0521 11:10:27.978458 27022 net.cpp:397] relu2 -> conv2 (in-place)
I0521 11:10:27.978811 27022 net.cpp:150] Setting up relu2
I0521 11:10:27.978831 27022 net.cpp:157] Top shape: 980 20 54 46 (48686400)
I0521 11:10:27.978843 27022 net.cpp:165] Memory required for data: 1091770960
I0521 11:10:27.978858 27022 layer_factory.hpp:77] Creating layer pool2
I0521 11:10:27.978881 27022 net.cpp:106] Creating Layer pool2
I0521 11:10:27.978894 27022 net.cpp:454] pool2 <- conv2
I0521 11:10:27.978910 27022 net.cpp:411] pool2 -> pool2
I0521 11:10:27.979001 27022 net.cpp:150] Setting up pool2
I0521 11:10:27.979017 27022 net.cpp:157] Top shape: 980 20 27 46 (24343200)
I0521 11:10:27.979030 27022 net.cpp:165] Memory required for data: 1189143760
I0521 11:10:27.979043 27022 layer_factory.hpp:77] Creating layer conv3
I0521 11:10:27.979074 27022 net.cpp:106] Creating Layer conv3
I0521 11:10:27.979087 27022 net.cpp:454] conv3 <- pool2
I0521 11:10:27.979104 27022 net.cpp:411] conv3 -> conv3
I0521 11:10:27.981106 27022 net.cpp:150] Setting up conv3
I0521 11:10:27.981130 27022 net.cpp:157] Top shape: 980 28 22 44 (26561920)
I0521 11:10:27.981149 27022 net.cpp:165] Memory required for data: 1295391440
I0521 11:10:27.981189 27022 layer_factory.hpp:77] Creating layer relu3
I0521 11:10:27.981215 27022 net.cpp:106] Creating Layer relu3
I0521 11:10:27.981228 27022 net.cpp:454] relu3 <- conv3
I0521 11:10:27.981245 27022 net.cpp:397] relu3 -> conv3 (in-place)
I0521 11:10:27.981744 27022 net.cpp:150] Setting up relu3
I0521 11:10:27.981766 27022 net.cpp:157] Top shape: 980 28 22 44 (26561920)
I0521 11:10:27.981780 27022 net.cpp:165] Memory required for data: 1401639120
I0521 11:10:27.981796 27022 layer_factory.hpp:77] Creating layer pool3
I0521 11:10:27.981819 27022 net.cpp:106] Creating Layer pool3
I0521 11:10:27.981833 27022 net.cpp:454] pool3 <- conv3
I0521 11:10:27.981849 27022 net.cpp:411] pool3 -> pool3
I0521 11:10:27.981935 27022 net.cpp:150] Setting up pool3
I0521 11:10:27.981953 27022 net.cpp:157] Top shape: 980 28 11 44 (13280960)
I0521 11:10:27.981968 27022 net.cpp:165] Memory required for data: 1454762960
I0521 11:10:27.981981 27022 layer_factory.hpp:77] Creating layer conv4
I0521 11:10:27.982008 27022 net.cpp:106] Creating Layer conv4
I0521 11:10:27.982022 27022 net.cpp:454] conv4 <- pool3
I0521 11:10:27.982038 27022 net.cpp:411] conv4 -> conv4
I0521 11:10:27.984120 27022 net.cpp:150] Setting up conv4
I0521 11:10:27.984144 27022 net.cpp:157] Top shape: 980 36 6 42 (8890560)
I0521 11:10:27.984164 27022 net.cpp:165] Memory required for data: 1490325200
I0521 11:10:27.984184 27022 layer_factory.hpp:77] Creating layer relu4
I0521 11:10:27.984203 27022 net.cpp:106] Creating Layer relu4
I0521 11:10:27.984216 27022 net.cpp:454] relu4 <- conv4
I0521 11:10:27.984241 27022 net.cpp:397] relu4 -> conv4 (in-place)
I0521 11:10:27.984732 27022 net.cpp:150] Setting up relu4
I0521 11:10:27.984755 27022 net.cpp:157] Top shape: 980 36 6 42 (8890560)
I0521 11:10:27.984768 27022 net.cpp:165] Memory required for data: 1525887440
I0521 11:10:27.984784 27022 layer_factory.hpp:77] Creating layer pool4
I0521 11:10:27.984808 27022 net.cpp:106] Creating Layer pool4
I0521 11:10:27.984822 27022 net.cpp:454] pool4 <- conv4
I0521 11:10:27.984838 27022 net.cpp:411] pool4 -> pool4
I0521 11:10:27.984925 27022 net.cpp:150] Setting up pool4
I0521 11:10:27.984942 27022 net.cpp:157] Top shape: 980 36 3 42 (4445280)
I0521 11:10:27.984956 27022 net.cpp:165] Memory required for data: 1543668560
I0521 11:10:27.984968 27022 layer_factory.hpp:77] Creating layer ip1
I0521 11:10:27.984992 27022 net.cpp:106] Creating Layer ip1
I0521 11:10:27.985005 27022 net.cpp:454] ip1 <- pool4
I0521 11:10:27.985028 27022 net.cpp:411] ip1 -> ip1
I0521 11:10:28.000516 27022 net.cpp:150] Setting up ip1
I0521 11:10:28.000548 27022 net.cpp:157] Top shape: 980 196 (192080)
I0521 11:10:28.000569 27022 net.cpp:165] Memory required for data: 1544436880
I0521 11:10:28.000594 27022 layer_factory.hpp:77] Creating layer relu5
I0521 11:10:28.000617 27022 net.cpp:106] Creating Layer relu5
I0521 11:10:28.000641 27022 net.cpp:454] relu5 <- ip1
I0521 11:10:28.000658 27022 net.cpp:397] relu5 -> ip1 (in-place)
I0521 11:10:28.001019 27022 net.cpp:150] Setting up relu5
I0521 11:10:28.001039 27022 net.cpp:157] Top shape: 980 196 (192080)
I0521 11:10:28.001052 27022 net.cpp:165] Memory required for data: 1545205200
I0521 11:10:28.001068 27022 layer_factory.hpp:77] Creating layer drop1
I0521 11:10:28.001098 27022 net.cpp:106] Creating Layer drop1
I0521 11:10:28.001111 27022 net.cpp:454] drop1 <- ip1
I0521 11:10:28.001127 27022 net.cpp:397] drop1 -> ip1 (in-place)
I0521 11:10:28.001184 27022 net.cpp:150] Setting up drop1
I0521 11:10:28.001201 27022 net.cpp:157] Top shape: 980 196 (192080)
I0521 11:10:28.001214 27022 net.cpp:165] Memory required for data: 1545973520
I0521 11:10:28.001226 27022 layer_factory.hpp:77] Creating layer ip2
I0521 11:10:28.001243 27022 net.cpp:106] Creating Layer ip2
I0521 11:10:28.001258 27022 net.cpp:454] ip2 <- ip1
I0521 11:10:28.001281 27022 net.cpp:411] ip2 -> ip2
I0521 11:10:28.001785 27022 net.cpp:150] Setting up ip2
I0521 11:10:28.001804 27022 net.cpp:157] Top shape: 980 98 (96040)
I0521 11:10:28.001817 27022 net.cpp:165] Memory required for data: 1546357680
I0521 11:10:28.001857 27022 layer_factory.hpp:77] Creating layer relu6
I0521 11:10:28.001874 27022 net.cpp:106] Creating Layer relu6
I0521 11:10:28.001888 27022 net.cpp:454] relu6 <- ip2
I0521 11:10:28.001904 27022 net.cpp:397] relu6 -> ip2 (in-place)
I0521 11:10:28.002465 27022 net.cpp:150] Setting up relu6
I0521 11:10:28.002488 27022 net.cpp:157] Top shape: 980 98 (96040)
I0521 11:10:28.002502 27022 net.cpp:165] Memory required for data: 1546741840
I0521 11:10:28.002514 27022 layer_factory.hpp:77] Creating layer drop2
I0521 11:10:28.002534 27022 net.cpp:106] Creating Layer drop2
I0521 11:10:28.002555 27022 net.cpp:454] drop2 <- ip2
I0521 11:10:28.002573 27022 net.cpp:397] drop2 -> ip2 (in-place)
I0521 11:10:28.002629 27022 net.cpp:150] Setting up drop2
I0521 11:10:28.002652 27022 net.cpp:157] Top shape: 980 98 (96040)
I0521 11:10:28.002665 27022 net.cpp:165] Memory required for data: 1547126000
I0521 11:10:28.002678 27022 layer_factory.hpp:77] Creating layer ip3
I0521 11:10:28.002701 27022 net.cpp:106] Creating Layer ip3
I0521 11:10:28.002714 27022 net.cpp:454] ip3 <- ip2
I0521 11:10:28.002733 27022 net.cpp:411] ip3 -> ip3
I0521 11:10:28.002976 27022 net.cpp:150] Setting up ip3
I0521 11:10:28.002995 27022 net.cpp:157] Top shape: 980 11 (10780)
I0521 11:10:28.003008 27022 net.cpp:165] Memory required for data: 1547169120
I0521 11:10:28.003029 27022 layer_factory.hpp:77] Creating layer drop3
I0521 11:10:28.003051 27022 net.cpp:106] Creating Layer drop3
I0521 11:10:28.003064 27022 net.cpp:454] drop3 <- ip3
I0521 11:10:28.003080 27022 net.cpp:397] drop3 -> ip3 (in-place)
I0521 11:10:28.003134 27022 net.cpp:150] Setting up drop3
I0521 11:10:28.003151 27022 net.cpp:157] Top shape: 980 11 (10780)
I0521 11:10:28.003164 27022 net.cpp:165] Memory required for data: 1547212240
I0521 11:10:28.003177 27022 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 11:10:28.003193 27022 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 11:10:28.003208 27022 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 11:10:28.003229 27022 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 11:10:28.003248 27022 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 11:10:28.003337 27022 net.cpp:150] Setting up ip3_drop3_0_split
I0521 11:10:28.003357 27022 net.cpp:157] Top shape: 980 11 (10780)
I0521 11:10:28.003373 27022 net.cpp:157] Top shape: 980 11 (10780)
I0521 11:10:28.003386 27022 net.cpp:165] Memory required for data: 1547298480
I0521 11:10:28.003399 27022 layer_factory.hpp:77] Creating layer accuracy
I0521 11:10:28.003429 27022 net.cpp:106] Creating Layer accuracy
I0521 11:10:28.003443 27022 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 11:10:28.003458 27022 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 11:10:28.003473 27022 net.cpp:411] accuracy -> accuracy
I0521 11:10:28.003509 27022 net.cpp:150] Setting up accuracy
I0521 11:10:28.003525 27022 net.cpp:157] Top shape: (1)
I0521 11:10:28.003541 27022 net.cpp:165] Memory required for data: 1547298484
I0521 11:10:28.003554 27022 layer_factory.hpp:77] Creating layer loss
I0521 11:10:28.003569 27022 net.cpp:106] Creating Layer loss
I0521 11:10:28.003584 27022 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 11:10:28.003604 27022 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 11:10:28.003620 27022 net.cpp:411] loss -> loss
I0521 11:10:28.003641 27022 layer_factory.hpp:77] Creating layer loss
I0521 11:10:28.004168 27022 net.cpp:150] Setting up loss
I0521 11:10:28.004189 27022 net.cpp:157] Top shape: (1)
I0521 11:10:28.004201 27022 net.cpp:160]     with loss weight 1
I0521 11:10:28.004225 27022 net.cpp:165] Memory required for data: 1547298488
I0521 11:10:28.004245 27022 net.cpp:226] loss needs backward computation.
I0521 11:10:28.004258 27022 net.cpp:228] accuracy does not need backward computation.
I0521 11:10:28.004273 27022 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 11:10:28.004287 27022 net.cpp:226] drop3 needs backward computation.
I0521 11:10:28.004298 27022 net.cpp:226] ip3 needs backward computation.
I0521 11:10:28.004323 27022 net.cpp:226] drop2 needs backward computation.
I0521 11:10:28.004341 27022 net.cpp:226] relu6 needs backward computation.
I0521 11:10:28.004354 27022 net.cpp:226] ip2 needs backward computation.
I0521 11:10:28.004370 27022 net.cpp:226] drop1 needs backward computation.
I0521 11:10:28.004384 27022 net.cpp:226] relu5 needs backward computation.
I0521 11:10:28.004395 27022 net.cpp:226] ip1 needs backward computation.
I0521 11:10:28.004410 27022 net.cpp:226] pool4 needs backward computation.
I0521 11:10:28.004422 27022 net.cpp:226] relu4 needs backward computation.
I0521 11:10:28.004441 27022 net.cpp:226] conv4 needs backward computation.
I0521 11:10:28.004456 27022 net.cpp:226] pool3 needs backward computation.
I0521 11:10:28.004472 27022 net.cpp:226] relu3 needs backward computation.
I0521 11:10:28.004484 27022 net.cpp:226] conv3 needs backward computation.
I0521 11:10:28.004498 27022 net.cpp:226] pool2 needs backward computation.
I0521 11:10:28.004513 27022 net.cpp:226] relu2 needs backward computation.
I0521 11:10:28.004524 27022 net.cpp:226] conv2 needs backward computation.
I0521 11:10:28.004544 27022 net.cpp:226] pool1 needs backward computation.
I0521 11:10:28.004559 27022 net.cpp:226] relu1 needs backward computation.
I0521 11:10:28.004575 27022 net.cpp:226] conv1 needs backward computation.
I0521 11:10:28.004590 27022 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 11:10:28.004603 27022 net.cpp:228] data_hdf5 does not need backward computation.
I0521 11:10:28.004614 27022 net.cpp:270] This network produces output accuracy
I0521 11:10:28.004631 27022 net.cpp:270] This network produces output loss
I0521 11:10:28.004662 27022 net.cpp:283] Network initialization done.
I0521 11:10:28.004796 27022 solver.cpp:60] Solver scaffolding done.
I0521 11:10:28.005949 27022 caffe.cpp:212] Starting Optimization
I0521 11:10:28.005966 27022 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 11:10:28.005978 27022 solver.cpp:289] Learning Rate Policy: fixed
I0521 11:10:28.007231 27022 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 11:11:14.033478 27022 solver.cpp:409]     Test net output #0: accuracy = 0.0938243
I0521 11:11:14.033646 27022 solver.cpp:409]     Test net output #1: loss = 2.39742 (* 1 = 2.39742 loss)
I0521 11:11:14.212266 27022 solver.cpp:237] Iteration 0, loss = 2.39791
I0521 11:11:14.212307 27022 solver.cpp:253]     Train net output #0: loss = 2.39791 (* 1 = 2.39791 loss)
I0521 11:11:14.212328 27022 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 11:11:22.114724 27022 solver.cpp:237] Iteration 15, loss = 2.38712
I0521 11:11:22.114779 27022 solver.cpp:253]     Train net output #0: loss = 2.38712 (* 1 = 2.38712 loss)
I0521 11:11:22.114797 27022 sgd_solver.cpp:106] Iteration 15, lr = 0.0025
I0521 11:11:30.007271 27022 solver.cpp:237] Iteration 30, loss = 2.37501
I0521 11:11:30.007305 27022 solver.cpp:253]     Train net output #0: loss = 2.37501 (* 1 = 2.37501 loss)
I0521 11:11:30.007324 27022 sgd_solver.cpp:106] Iteration 30, lr = 0.0025
I0521 11:11:37.900761 27022 solver.cpp:237] Iteration 45, loss = 2.35883
I0521 11:11:37.900794 27022 solver.cpp:253]     Train net output #0: loss = 2.35883 (* 1 = 2.35883 loss)
I0521 11:11:37.900818 27022 sgd_solver.cpp:106] Iteration 45, lr = 0.0025
I0521 11:11:45.793803 27022 solver.cpp:237] Iteration 60, loss = 2.34703
I0521 11:11:45.793949 27022 solver.cpp:253]     Train net output #0: loss = 2.34703 (* 1 = 2.34703 loss)
I0521 11:11:45.793965 27022 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 11:11:53.678731 27022 solver.cpp:237] Iteration 75, loss = 2.32839
I0521 11:11:53.678777 27022 solver.cpp:253]     Train net output #0: loss = 2.32839 (* 1 = 2.32839 loss)
I0521 11:11:53.678807 27022 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 11:12:01.568778 27022 solver.cpp:237] Iteration 90, loss = 2.33143
I0521 11:12:01.568811 27022 solver.cpp:253]     Train net output #0: loss = 2.33143 (* 1 = 2.33143 loss)
I0521 11:12:01.568830 27022 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 11:12:31.608698 27022 solver.cpp:237] Iteration 105, loss = 2.31632
I0521 11:12:31.608873 27022 solver.cpp:253]     Train net output #0: loss = 2.31632 (* 1 = 2.31632 loss)
I0521 11:12:31.608891 27022 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0521 11:12:39.500226 27022 solver.cpp:237] Iteration 120, loss = 2.30827
I0521 11:12:39.500277 27022 solver.cpp:253]     Train net output #0: loss = 2.30827 (* 1 = 2.30827 loss)
I0521 11:12:39.500305 27022 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 11:12:47.389854 27022 solver.cpp:237] Iteration 135, loss = 2.33922
I0521 11:12:47.389889 27022 solver.cpp:253]     Train net output #0: loss = 2.33922 (* 1 = 2.33922 loss)
I0521 11:12:47.389911 27022 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 11:12:55.285708 27022 solver.cpp:237] Iteration 150, loss = 2.3081
I0521 11:12:55.285742 27022 solver.cpp:253]     Train net output #0: loss = 2.3081 (* 1 = 2.3081 loss)
I0521 11:12:55.285759 27022 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 11:12:56.341778 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_153.caffemodel
I0521 11:12:56.751919 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_153.solverstate
I0521 11:13:03.259974 27022 solver.cpp:237] Iteration 165, loss = 2.2933
I0521 11:13:03.260135 27022 solver.cpp:253]     Train net output #0: loss = 2.2933 (* 1 = 2.2933 loss)
I0521 11:13:03.260154 27022 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0521 11:13:11.155153 27022 solver.cpp:237] Iteration 180, loss = 2.29082
I0521 11:13:11.155192 27022 solver.cpp:253]     Train net output #0: loss = 2.29082 (* 1 = 2.29082 loss)
I0521 11:13:11.155210 27022 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 11:13:19.052644 27022 solver.cpp:237] Iteration 195, loss = 2.27653
I0521 11:13:19.052680 27022 solver.cpp:253]     Train net output #0: loss = 2.27653 (* 1 = 2.27653 loss)
I0521 11:13:19.052697 27022 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0521 11:13:49.095787 27022 solver.cpp:237] Iteration 210, loss = 2.24202
I0521 11:13:49.095954 27022 solver.cpp:253]     Train net output #0: loss = 2.24202 (* 1 = 2.24202 loss)
I0521 11:13:49.095971 27022 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 11:13:56.995254 27022 solver.cpp:237] Iteration 225, loss = 2.25747
I0521 11:13:56.995307 27022 solver.cpp:253]     Train net output #0: loss = 2.25747 (* 1 = 2.25747 loss)
I0521 11:13:56.995334 27022 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 11:14:04.889363 27022 solver.cpp:237] Iteration 240, loss = 2.23011
I0521 11:14:04.889396 27022 solver.cpp:253]     Train net output #0: loss = 2.23011 (* 1 = 2.23011 loss)
I0521 11:14:04.889420 27022 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 11:14:12.785291 27022 solver.cpp:237] Iteration 255, loss = 2.20294
I0521 11:14:12.785326 27022 solver.cpp:253]     Train net output #0: loss = 2.20294 (* 1 = 2.20294 loss)
I0521 11:14:12.785349 27022 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 11:14:20.680348 27022 solver.cpp:237] Iteration 270, loss = 2.17364
I0521 11:14:20.680507 27022 solver.cpp:253]     Train net output #0: loss = 2.17364 (* 1 = 2.17364 loss)
I0521 11:14:20.680526 27022 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 11:14:28.573895 27022 solver.cpp:237] Iteration 285, loss = 2.16697
I0521 11:14:28.573930 27022 solver.cpp:253]     Train net output #0: loss = 2.16697 (* 1 = 2.16697 loss)
I0521 11:14:28.573953 27022 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 11:14:36.466248 27022 solver.cpp:237] Iteration 300, loss = 2.14486
I0521 11:14:36.466284 27022 solver.cpp:253]     Train net output #0: loss = 2.14486 (* 1 = 2.14486 loss)
I0521 11:14:36.466306 27022 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 11:14:39.102938 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_306.caffemodel
I0521 11:14:39.511809 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_306.solverstate
I0521 11:14:39.537613 27022 solver.cpp:341] Iteration 306, Testing net (#0)
I0521 11:15:24.659768 27022 solver.cpp:409]     Test net output #0: accuracy = 0.473176
I0521 11:15:24.659926 27022 solver.cpp:409]     Test net output #1: loss = 1.97661 (* 1 = 1.97661 loss)
I0521 11:15:51.697654 27022 solver.cpp:237] Iteration 315, loss = 2.10933
I0521 11:15:51.697708 27022 solver.cpp:253]     Train net output #0: loss = 2.10933 (* 1 = 2.10933 loss)
I0521 11:15:51.697736 27022 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0521 11:15:59.590904 27022 solver.cpp:237] Iteration 330, loss = 2.08359
I0521 11:15:59.591053 27022 solver.cpp:253]     Train net output #0: loss = 2.08359 (* 1 = 2.08359 loss)
I0521 11:15:59.591078 27022 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 11:16:07.486696 27022 solver.cpp:237] Iteration 345, loss = 2.09597
I0521 11:16:07.486731 27022 solver.cpp:253]     Train net output #0: loss = 2.09597 (* 1 = 2.09597 loss)
I0521 11:16:07.486754 27022 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 11:16:15.377584 27022 solver.cpp:237] Iteration 360, loss = 2.0779
I0521 11:16:15.377616 27022 solver.cpp:253]     Train net output #0: loss = 2.0779 (* 1 = 2.0779 loss)
I0521 11:16:15.377640 27022 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 11:16:23.270254 27022 solver.cpp:237] Iteration 375, loss = 2.05812
I0521 11:16:23.270299 27022 solver.cpp:253]     Train net output #0: loss = 2.05812 (* 1 = 2.05812 loss)
I0521 11:16:23.270318 27022 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 11:16:31.167879 27022 solver.cpp:237] Iteration 390, loss = 2.03316
I0521 11:16:31.168020 27022 solver.cpp:253]     Train net output #0: loss = 2.03316 (* 1 = 2.03316 loss)
I0521 11:16:31.168036 27022 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 11:16:39.060133 27022 solver.cpp:237] Iteration 405, loss = 2.00609
I0521 11:16:39.060168 27022 solver.cpp:253]     Train net output #0: loss = 2.00609 (* 1 = 2.00609 loss)
I0521 11:16:39.060192 27022 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 11:17:09.150599 27022 solver.cpp:237] Iteration 420, loss = 1.99354
I0521 11:17:09.150764 27022 solver.cpp:253]     Train net output #0: loss = 1.99354 (* 1 = 1.99354 loss)
I0521 11:17:09.150781 27022 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 11:17:17.040145 27022 solver.cpp:237] Iteration 435, loss = 1.94979
I0521 11:17:17.040184 27022 solver.cpp:253]     Train net output #0: loss = 1.94979 (* 1 = 1.94979 loss)
I0521 11:17:17.040201 27022 sgd_solver.cpp:106] Iteration 435, lr = 0.0025
I0521 11:17:24.933845 27022 solver.cpp:237] Iteration 450, loss = 1.96738
I0521 11:17:24.933876 27022 solver.cpp:253]     Train net output #0: loss = 1.96738 (* 1 = 1.96738 loss)
I0521 11:17:24.933899 27022 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 11:17:29.142900 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_459.caffemodel
I0521 11:17:29.553634 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_459.solverstate
I0521 11:17:32.895135 27022 solver.cpp:237] Iteration 465, loss = 1.97649
I0521 11:17:32.895186 27022 solver.cpp:253]     Train net output #0: loss = 1.97649 (* 1 = 1.97649 loss)
I0521 11:17:32.895215 27022 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0521 11:17:40.790017 27022 solver.cpp:237] Iteration 480, loss = 1.95598
I0521 11:17:40.790189 27022 solver.cpp:253]     Train net output #0: loss = 1.95598 (* 1 = 1.95598 loss)
I0521 11:17:40.790208 27022 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 11:17:48.689860 27022 solver.cpp:237] Iteration 495, loss = 1.96052
I0521 11:17:48.689896 27022 solver.cpp:253]     Train net output #0: loss = 1.96052 (* 1 = 1.96052 loss)
I0521 11:17:48.689914 27022 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0521 11:18:18.783884 27022 solver.cpp:237] Iteration 510, loss = 1.92301
I0521 11:18:18.784054 27022 solver.cpp:253]     Train net output #0: loss = 1.92301 (* 1 = 1.92301 loss)
I0521 11:18:18.784071 27022 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 11:18:26.679375 27022 solver.cpp:237] Iteration 525, loss = 1.91167
I0521 11:18:26.679410 27022 solver.cpp:253]     Train net output #0: loss = 1.91167 (* 1 = 1.91167 loss)
I0521 11:18:26.679433 27022 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 11:18:34.572454 27022 solver.cpp:237] Iteration 540, loss = 1.86571
I0521 11:18:34.572489 27022 solver.cpp:253]     Train net output #0: loss = 1.86571 (* 1 = 1.86571 loss)
I0521 11:18:34.572504 27022 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 11:18:42.466967 27022 solver.cpp:237] Iteration 555, loss = 1.91913
I0521 11:18:42.467000 27022 solver.cpp:253]     Train net output #0: loss = 1.91913 (* 1 = 1.91913 loss)
I0521 11:18:42.467020 27022 sgd_solver.cpp:106] Iteration 555, lr = 0.0025
I0521 11:18:50.360612 27022 solver.cpp:237] Iteration 570, loss = 1.8715
I0521 11:18:50.360750 27022 solver.cpp:253]     Train net output #0: loss = 1.8715 (* 1 = 1.8715 loss)
I0521 11:18:50.360767 27022 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 11:18:58.257711 27022 solver.cpp:237] Iteration 585, loss = 1.82835
I0521 11:18:58.257747 27022 solver.cpp:253]     Train net output #0: loss = 1.82835 (* 1 = 1.82835 loss)
I0521 11:18:58.257763 27022 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0521 11:19:06.149574 27022 solver.cpp:237] Iteration 600, loss = 1.86788
I0521 11:19:06.149612 27022 solver.cpp:253]     Train net output #0: loss = 1.86788 (* 1 = 1.86788 loss)
I0521 11:19:06.149628 27022 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 11:19:11.939976 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_612.caffemodel
I0521 11:19:12.349462 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_612.solverstate
I0521 11:19:12.377115 27022 solver.cpp:341] Iteration 612, Testing net (#0)
I0521 11:20:18.421721 27022 solver.cpp:409]     Test net output #0: accuracy = 0.579659
I0521 11:20:18.421895 27022 solver.cpp:409]     Test net output #1: loss = 1.48961 (* 1 = 1.48961 loss)
I0521 11:20:42.357367 27022 solver.cpp:237] Iteration 615, loss = 1.85127
I0521 11:20:42.357420 27022 solver.cpp:253]     Train net output #0: loss = 1.85127 (* 1 = 1.85127 loss)
I0521 11:20:42.357445 27022 sgd_solver.cpp:106] Iteration 615, lr = 0.0025
I0521 11:20:50.256753 27022 solver.cpp:237] Iteration 630, loss = 1.85698
I0521 11:20:50.256897 27022 solver.cpp:253]     Train net output #0: loss = 1.85698 (* 1 = 1.85698 loss)
I0521 11:20:50.256914 27022 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 11:20:58.159461 27022 solver.cpp:237] Iteration 645, loss = 1.87544
I0521 11:20:58.159508 27022 solver.cpp:253]     Train net output #0: loss = 1.87544 (* 1 = 1.87544 loss)
I0521 11:20:58.159525 27022 sgd_solver.cpp:106] Iteration 645, lr = 0.0025
I0521 11:21:06.054626 27022 solver.cpp:237] Iteration 660, loss = 1.8327
I0521 11:21:06.054659 27022 solver.cpp:253]     Train net output #0: loss = 1.8327 (* 1 = 1.8327 loss)
I0521 11:21:06.054683 27022 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 11:21:13.952625 27022 solver.cpp:237] Iteration 675, loss = 1.88848
I0521 11:21:13.952657 27022 solver.cpp:253]     Train net output #0: loss = 1.88848 (* 1 = 1.88848 loss)
I0521 11:21:13.952680 27022 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 11:21:21.853165 27022 solver.cpp:237] Iteration 690, loss = 1.87933
I0521 11:21:21.853320 27022 solver.cpp:253]     Train net output #0: loss = 1.87933 (* 1 = 1.87933 loss)
I0521 11:21:21.853338 27022 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 11:21:29.747457 27022 solver.cpp:237] Iteration 705, loss = 1.8269
I0521 11:21:29.747491 27022 solver.cpp:253]     Train net output #0: loss = 1.8269 (* 1 = 1.8269 loss)
I0521 11:21:29.747515 27022 sgd_solver.cpp:106] Iteration 705, lr = 0.0025
I0521 11:21:59.822532 27022 solver.cpp:237] Iteration 720, loss = 1.81954
I0521 11:21:59.822702 27022 solver.cpp:253]     Train net output #0: loss = 1.81954 (* 1 = 1.81954 loss)
I0521 11:21:59.822721 27022 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 11:22:07.725282 27022 solver.cpp:237] Iteration 735, loss = 1.83021
I0521 11:22:07.725318 27022 solver.cpp:253]     Train net output #0: loss = 1.83021 (* 1 = 1.83021 loss)
I0521 11:22:07.725335 27022 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0521 11:22:15.623708 27022 solver.cpp:237] Iteration 750, loss = 1.8628
I0521 11:22:15.623760 27022 solver.cpp:253]     Train net output #0: loss = 1.8628 (* 1 = 1.8628 loss)
I0521 11:22:15.623785 27022 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 11:22:22.994345 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_765.caffemodel
I0521 11:22:23.405021 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_765.solverstate
I0521 11:22:23.588973 27022 solver.cpp:237] Iteration 765, loss = 1.83422
I0521 11:22:23.589030 27022 solver.cpp:253]     Train net output #0: loss = 1.83422 (* 1 = 1.83422 loss)
I0521 11:22:23.589049 27022 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 11:22:31.481659 27022 solver.cpp:237] Iteration 780, loss = 1.82187
I0521 11:22:31.481806 27022 solver.cpp:253]     Train net output #0: loss = 1.82187 (* 1 = 1.82187 loss)
I0521 11:22:31.481822 27022 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 11:22:39.379361 27022 solver.cpp:237] Iteration 795, loss = 1.80391
I0521 11:22:39.379415 27022 solver.cpp:253]     Train net output #0: loss = 1.80391 (* 1 = 1.80391 loss)
I0521 11:22:39.379443 27022 sgd_solver.cpp:106] Iteration 795, lr = 0.0025
I0521 11:22:47.282130 27022 solver.cpp:237] Iteration 810, loss = 1.86807
I0521 11:22:47.282163 27022 solver.cpp:253]     Train net output #0: loss = 1.86807 (* 1 = 1.86807 loss)
I0521 11:22:47.282186 27022 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 11:23:17.407325 27022 solver.cpp:237] Iteration 825, loss = 1.82752
I0521 11:23:17.407516 27022 solver.cpp:253]     Train net output #0: loss = 1.82752 (* 1 = 1.82752 loss)
I0521 11:23:17.407533 27022 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 11:23:25.306327 27022 solver.cpp:237] Iteration 840, loss = 1.84458
I0521 11:23:25.306361 27022 solver.cpp:253]     Train net output #0: loss = 1.84458 (* 1 = 1.84458 loss)
I0521 11:23:25.306385 27022 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 11:23:33.205459 27022 solver.cpp:237] Iteration 855, loss = 1.75342
I0521 11:23:33.205514 27022 solver.cpp:253]     Train net output #0: loss = 1.75342 (* 1 = 1.75342 loss)
I0521 11:23:33.205535 27022 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 11:23:41.103005 27022 solver.cpp:237] Iteration 870, loss = 1.79746
I0521 11:23:41.103040 27022 solver.cpp:253]     Train net output #0: loss = 1.79746 (* 1 = 1.79746 loss)
I0521 11:23:41.103065 27022 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 11:23:48.996084 27022 solver.cpp:237] Iteration 885, loss = 1.79793
I0521 11:23:48.996225 27022 solver.cpp:253]     Train net output #0: loss = 1.79793 (* 1 = 1.79793 loss)
I0521 11:23:48.996243 27022 sgd_solver.cpp:106] Iteration 885, lr = 0.0025
I0521 11:23:56.895403 27022 solver.cpp:237] Iteration 900, loss = 1.79993
I0521 11:23:56.895457 27022 solver.cpp:253]     Train net output #0: loss = 1.79993 (* 1 = 1.79993 loss)
I0521 11:23:56.895483 27022 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 11:24:04.786278 27022 solver.cpp:237] Iteration 915, loss = 1.85009
I0521 11:24:04.786312 27022 solver.cpp:253]     Train net output #0: loss = 1.85009 (* 1 = 1.85009 loss)
I0521 11:24:04.786331 27022 sgd_solver.cpp:106] Iteration 915, lr = 0.0025
I0521 11:24:05.837232 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_918.caffemodel
I0521 11:24:06.245180 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_918.solverstate
I0521 11:24:06.270495 27022 solver.cpp:341] Iteration 918, Testing net (#0)
I0521 11:24:51.078953 27022 solver.cpp:409]     Test net output #0: accuracy = 0.57511
I0521 11:24:51.079115 27022 solver.cpp:409]     Test net output #1: loss = 1.4555 (* 1 = 1.4555 loss)
I0521 11:25:19.781491 27022 solver.cpp:237] Iteration 930, loss = 1.75666
I0521 11:25:19.781550 27022 solver.cpp:253]     Train net output #0: loss = 1.75666 (* 1 = 1.75666 loss)
I0521 11:25:19.781575 27022 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 11:25:27.671299 27022 solver.cpp:237] Iteration 945, loss = 1.81642
I0521 11:25:27.671452 27022 solver.cpp:253]     Train net output #0: loss = 1.81642 (* 1 = 1.81642 loss)
I0521 11:25:27.671468 27022 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 11:25:35.570693 27022 solver.cpp:237] Iteration 960, loss = 1.74919
I0521 11:25:35.570740 27022 solver.cpp:253]     Train net output #0: loss = 1.74919 (* 1 = 1.74919 loss)
I0521 11:25:35.570757 27022 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 11:25:43.462926 27022 solver.cpp:237] Iteration 975, loss = 1.83918
I0521 11:25:43.462960 27022 solver.cpp:253]     Train net output #0: loss = 1.83918 (* 1 = 1.83918 loss)
I0521 11:25:43.462985 27022 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 11:25:51.353518 27022 solver.cpp:237] Iteration 990, loss = 1.84018
I0521 11:25:51.353554 27022 solver.cpp:253]     Train net output #0: loss = 1.84018 (* 1 = 1.84018 loss)
I0521 11:25:51.353570 27022 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 11:25:59.241842 27022 solver.cpp:237] Iteration 1005, loss = 1.79029
I0521 11:25:59.242012 27022 solver.cpp:253]     Train net output #0: loss = 1.79029 (* 1 = 1.79029 loss)
I0521 11:25:59.242032 27022 sgd_solver.cpp:106] Iteration 1005, lr = 0.0025
I0521 11:26:29.344270 27022 solver.cpp:237] Iteration 1020, loss = 1.80154
I0521 11:26:29.344445 27022 solver.cpp:253]     Train net output #0: loss = 1.80154 (* 1 = 1.80154 loss)
I0521 11:26:29.344463 27022 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 11:26:37.232642 27022 solver.cpp:237] Iteration 1035, loss = 1.85196
I0521 11:26:37.232676 27022 solver.cpp:253]     Train net output #0: loss = 1.85196 (* 1 = 1.85196 loss)
I0521 11:26:37.232699 27022 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 11:26:45.121773 27022 solver.cpp:237] Iteration 1050, loss = 1.72697
I0521 11:26:45.121805 27022 solver.cpp:253]     Train net output #0: loss = 1.72697 (* 1 = 1.72697 loss)
I0521 11:26:45.121829 27022 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 11:26:53.009249 27022 solver.cpp:237] Iteration 1065, loss = 1.82584
I0521 11:26:53.009295 27022 solver.cpp:253]     Train net output #0: loss = 1.82584 (* 1 = 1.82584 loss)
I0521 11:26:53.009313 27022 sgd_solver.cpp:106] Iteration 1065, lr = 0.0025
I0521 11:26:55.639762 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1071.caffemodel
I0521 11:26:56.047747 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1071.solverstate
I0521 11:27:00.959463 27022 solver.cpp:237] Iteration 1080, loss = 1.73968
I0521 11:27:00.959627 27022 solver.cpp:253]     Train net output #0: loss = 1.73968 (* 1 = 1.73968 loss)
I0521 11:27:00.959645 27022 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 11:27:08.855095 27022 solver.cpp:237] Iteration 1095, loss = 1.76996
I0521 11:27:08.855129 27022 solver.cpp:253]     Train net output #0: loss = 1.76996 (* 1 = 1.76996 loss)
I0521 11:27:08.855152 27022 sgd_solver.cpp:106] Iteration 1095, lr = 0.0025
I0521 11:27:16.743469 27022 solver.cpp:237] Iteration 1110, loss = 1.71362
I0521 11:27:16.743516 27022 solver.cpp:253]     Train net output #0: loss = 1.71362 (* 1 = 1.71362 loss)
I0521 11:27:16.743533 27022 sgd_solver.cpp:106] Iteration 1110, lr = 0.0025
I0521 11:27:46.873478 27022 solver.cpp:237] Iteration 1125, loss = 1.70157
I0521 11:27:46.873652 27022 solver.cpp:253]     Train net output #0: loss = 1.70157 (* 1 = 1.70157 loss)
I0521 11:27:46.873669 27022 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 11:27:54.765509 27022 solver.cpp:237] Iteration 1140, loss = 1.7546
I0521 11:27:54.765545 27022 solver.cpp:253]     Train net output #0: loss = 1.7546 (* 1 = 1.7546 loss)
I0521 11:27:54.765563 27022 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 11:28:02.659986 27022 solver.cpp:237] Iteration 1155, loss = 1.80033
I0521 11:28:02.660038 27022 solver.cpp:253]     Train net output #0: loss = 1.80033 (* 1 = 1.80033 loss)
I0521 11:28:02.660064 27022 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 11:28:10.549012 27022 solver.cpp:237] Iteration 1170, loss = 1.77238
I0521 11:28:10.549046 27022 solver.cpp:253]     Train net output #0: loss = 1.77238 (* 1 = 1.77238 loss)
I0521 11:28:10.549069 27022 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 11:28:18.436415 27022 solver.cpp:237] Iteration 1185, loss = 1.73325
I0521 11:28:18.436560 27022 solver.cpp:253]     Train net output #0: loss = 1.73325 (* 1 = 1.73325 loss)
I0521 11:28:18.436576 27022 sgd_solver.cpp:106] Iteration 1185, lr = 0.0025
I0521 11:28:26.327649 27022 solver.cpp:237] Iteration 1200, loss = 1.69603
I0521 11:28:26.327683 27022 solver.cpp:253]     Train net output #0: loss = 1.69603 (* 1 = 1.69603 loss)
I0521 11:28:26.327702 27022 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 11:28:34.210623 27022 solver.cpp:237] Iteration 1215, loss = 1.7283
I0521 11:28:34.210667 27022 solver.cpp:253]     Train net output #0: loss = 1.7283 (* 1 = 1.7283 loss)
I0521 11:28:34.210687 27022 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 11:28:38.415700 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1224.caffemodel
I0521 11:28:38.822829 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1224.solverstate
I0521 11:28:38.849227 27022 solver.cpp:341] Iteration 1224, Testing net (#0)
I0521 11:29:44.851254 27022 solver.cpp:409]     Test net output #0: accuracy = 0.656669
I0521 11:29:44.851434 27022 solver.cpp:409]     Test net output #1: loss = 1.20502 (* 1 = 1.20502 loss)
I0521 11:30:10.360066 27022 solver.cpp:237] Iteration 1230, loss = 1.62053
I0521 11:30:10.360121 27022 solver.cpp:253]     Train net output #0: loss = 1.62053 (* 1 = 1.62053 loss)
I0521 11:30:10.360148 27022 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0521 11:30:18.252696 27022 solver.cpp:237] Iteration 1245, loss = 1.7588
I0521 11:30:18.252852 27022 solver.cpp:253]     Train net output #0: loss = 1.7588 (* 1 = 1.7588 loss)
I0521 11:30:18.252869 27022 sgd_solver.cpp:106] Iteration 1245, lr = 0.0025
I0521 11:30:26.138804 27022 solver.cpp:237] Iteration 1260, loss = 1.75614
I0521 11:30:26.138836 27022 solver.cpp:253]     Train net output #0: loss = 1.75614 (* 1 = 1.75614 loss)
I0521 11:30:26.138860 27022 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 11:30:34.025362 27022 solver.cpp:237] Iteration 1275, loss = 1.76062
I0521 11:30:34.025411 27022 solver.cpp:253]     Train net output #0: loss = 1.76062 (* 1 = 1.76062 loss)
I0521 11:30:34.025430 27022 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 11:30:41.914958 27022 solver.cpp:237] Iteration 1290, loss = 1.73838
I0521 11:30:41.914991 27022 solver.cpp:253]     Train net output #0: loss = 1.73838 (* 1 = 1.73838 loss)
I0521 11:30:41.915015 27022 sgd_solver.cpp:106] Iteration 1290, lr = 0.0025
I0521 11:30:49.806349 27022 solver.cpp:237] Iteration 1305, loss = 1.76379
I0521 11:30:49.806496 27022 solver.cpp:253]     Train net output #0: loss = 1.76379 (* 1 = 1.76379 loss)
I0521 11:30:49.806514 27022 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0521 11:30:57.692138 27022 solver.cpp:237] Iteration 1320, loss = 1.69014
I0521 11:30:57.692185 27022 solver.cpp:253]     Train net output #0: loss = 1.69014 (* 1 = 1.69014 loss)
I0521 11:30:57.692203 27022 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 11:31:27.784495 27022 solver.cpp:237] Iteration 1335, loss = 1.71549
I0521 11:31:27.784665 27022 solver.cpp:253]     Train net output #0: loss = 1.71549 (* 1 = 1.71549 loss)
I0521 11:31:27.784682 27022 sgd_solver.cpp:106] Iteration 1335, lr = 0.0025
I0521 11:31:35.676849 27022 solver.cpp:237] Iteration 1350, loss = 1.68999
I0521 11:31:35.676883 27022 solver.cpp:253]     Train net output #0: loss = 1.68999 (* 1 = 1.68999 loss)
I0521 11:31:35.676908 27022 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 11:31:43.569931 27022 solver.cpp:237] Iteration 1365, loss = 1.79261
I0521 11:31:43.569964 27022 solver.cpp:253]     Train net output #0: loss = 1.79261 (* 1 = 1.79261 loss)
I0521 11:31:43.569988 27022 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0521 11:31:49.361233 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1377.caffemodel
I0521 11:31:49.769526 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1377.solverstate
I0521 11:31:51.530619 27022 solver.cpp:237] Iteration 1380, loss = 1.71266
I0521 11:31:51.530670 27022 solver.cpp:253]     Train net output #0: loss = 1.71266 (* 1 = 1.71266 loss)
I0521 11:31:51.530695 27022 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 11:31:59.421896 27022 solver.cpp:237] Iteration 1395, loss = 1.77174
I0521 11:31:59.422058 27022 solver.cpp:253]     Train net output #0: loss = 1.77174 (* 1 = 1.77174 loss)
I0521 11:31:59.422075 27022 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0521 11:32:07.317986 27022 solver.cpp:237] Iteration 1410, loss = 1.70799
I0521 11:32:07.318019 27022 solver.cpp:253]     Train net output #0: loss = 1.70799 (* 1 = 1.70799 loss)
I0521 11:32:07.318038 27022 sgd_solver.cpp:106] Iteration 1410, lr = 0.0025
I0521 11:32:15.210005 27022 solver.cpp:237] Iteration 1425, loss = 1.70925
I0521 11:32:15.210057 27022 solver.cpp:253]     Train net output #0: loss = 1.70925 (* 1 = 1.70925 loss)
I0521 11:32:15.210083 27022 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 11:32:45.283462 27022 solver.cpp:237] Iteration 1440, loss = 1.77973
I0521 11:32:45.283632 27022 solver.cpp:253]     Train net output #0: loss = 1.77973 (* 1 = 1.77973 loss)
I0521 11:32:45.283650 27022 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 11:32:53.172718 27022 solver.cpp:237] Iteration 1455, loss = 1.68085
I0521 11:32:53.172751 27022 solver.cpp:253]     Train net output #0: loss = 1.68085 (* 1 = 1.68085 loss)
I0521 11:32:53.172770 27022 sgd_solver.cpp:106] Iteration 1455, lr = 0.0025
I0521 11:33:01.062890 27022 solver.cpp:237] Iteration 1470, loss = 1.70809
I0521 11:33:01.062922 27022 solver.cpp:253]     Train net output #0: loss = 1.70809 (* 1 = 1.70809 loss)
I0521 11:33:01.062942 27022 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 11:33:08.954797 27022 solver.cpp:237] Iteration 1485, loss = 1.72774
I0521 11:33:08.954844 27022 solver.cpp:253]     Train net output #0: loss = 1.72774 (* 1 = 1.72774 loss)
I0521 11:33:08.954872 27022 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 11:33:16.845945 27022 solver.cpp:237] Iteration 1500, loss = 1.73739
I0521 11:33:16.846091 27022 solver.cpp:253]     Train net output #0: loss = 1.73739 (* 1 = 1.73739 loss)
I0521 11:33:16.846108 27022 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 11:33:24.734683 27022 solver.cpp:237] Iteration 1515, loss = 1.70955
I0521 11:33:24.734717 27022 solver.cpp:253]     Train net output #0: loss = 1.70955 (* 1 = 1.70955 loss)
I0521 11:33:24.734736 27022 sgd_solver.cpp:106] Iteration 1515, lr = 0.0025
I0521 11:33:32.098302 27022 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1530.caffemodel
I0521 11:33:32.507541 27022 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619_iter_1530.solverstate
I0521 11:33:53.587208 27022 solver.cpp:321] Iteration 1530, loss = 1.65436
I0521 11:33:53.587376 27022 solver.cpp:341] Iteration 1530, Testing net (#0)
I0521 11:34:38.686728 27022 solver.cpp:409]     Test net output #0: accuracy = 0.667347
I0521 11:34:38.686904 27022 solver.cpp:409]     Test net output #1: loss = 1.16513 (* 1 = 1.16513 loss)
I0521 11:34:38.686923 27022 solver.cpp:326] Optimization Done.
I0521 11:34:38.686934 27022 caffe.cpp:215] Optimization Done.
Application 11237798 resources: utime ~1268s, stime ~228s, Rss ~5329492, inblocks ~3744348, outblocks ~179818
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_980_2016-05-20T11.21.08.421619.solver"
	User time (seconds): 0.59
	System time (seconds): 0.13
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:00.85
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 2
	Minor (reclaiming a frame) page faults: 15082
	Voluntary context switches: 3425
	Involuntary context switches: 200
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
