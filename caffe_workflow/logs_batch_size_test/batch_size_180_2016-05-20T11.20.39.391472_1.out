2805297
I0520 15:34:52.430779  7341 caffe.cpp:184] Using GPUs 0
I0520 15:34:52.852028  7341 solver.cpp:48] Initializing solver from parameters: 
test_iter: 833
test_interval: 1666
base_lr: 0.0025
display: 83
max_iter: 8333
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 833
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472.prototxt"
I0520 15:34:52.853699  7341 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472.prototxt
I0520 15:34:52.864398  7341 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 15:34:52.864459  7341 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 15:34:52.864810  7341 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 180
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:34:52.864989  7341 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:34:52.865011  7341 net.cpp:106] Creating Layer data_hdf5
I0520 15:34:52.865026  7341 net.cpp:411] data_hdf5 -> data
I0520 15:34:52.865059  7341 net.cpp:411] data_hdf5 -> label
I0520 15:34:52.865092  7341 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 15:34:52.866258  7341 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 15:34:52.868405  7341 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 15:35:14.384836  7341 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 15:35:14.389950  7341 net.cpp:150] Setting up data_hdf5
I0520 15:35:14.389989  7341 net.cpp:157] Top shape: 180 1 127 50 (1143000)
I0520 15:35:14.390004  7341 net.cpp:157] Top shape: 180 (180)
I0520 15:35:14.390017  7341 net.cpp:165] Memory required for data: 4572720
I0520 15:35:14.390030  7341 layer_factory.hpp:77] Creating layer conv1
I0520 15:35:14.390064  7341 net.cpp:106] Creating Layer conv1
I0520 15:35:14.390076  7341 net.cpp:454] conv1 <- data
I0520 15:35:14.390095  7341 net.cpp:411] conv1 -> conv1
I0520 15:35:14.967999  7341 net.cpp:150] Setting up conv1
I0520 15:35:14.968044  7341 net.cpp:157] Top shape: 180 12 120 48 (12441600)
I0520 15:35:14.968055  7341 net.cpp:165] Memory required for data: 54339120
I0520 15:35:14.968085  7341 layer_factory.hpp:77] Creating layer relu1
I0520 15:35:14.968106  7341 net.cpp:106] Creating Layer relu1
I0520 15:35:14.968117  7341 net.cpp:454] relu1 <- conv1
I0520 15:35:14.968130  7341 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:35:14.968652  7341 net.cpp:150] Setting up relu1
I0520 15:35:14.968667  7341 net.cpp:157] Top shape: 180 12 120 48 (12441600)
I0520 15:35:14.968678  7341 net.cpp:165] Memory required for data: 104105520
I0520 15:35:14.968688  7341 layer_factory.hpp:77] Creating layer pool1
I0520 15:35:14.968704  7341 net.cpp:106] Creating Layer pool1
I0520 15:35:14.968714  7341 net.cpp:454] pool1 <- conv1
I0520 15:35:14.968727  7341 net.cpp:411] pool1 -> pool1
I0520 15:35:14.968807  7341 net.cpp:150] Setting up pool1
I0520 15:35:14.968822  7341 net.cpp:157] Top shape: 180 12 60 48 (6220800)
I0520 15:35:14.968832  7341 net.cpp:165] Memory required for data: 128988720
I0520 15:35:14.968842  7341 layer_factory.hpp:77] Creating layer conv2
I0520 15:35:14.968864  7341 net.cpp:106] Creating Layer conv2
I0520 15:35:14.968874  7341 net.cpp:454] conv2 <- pool1
I0520 15:35:14.968888  7341 net.cpp:411] conv2 -> conv2
I0520 15:35:14.971654  7341 net.cpp:150] Setting up conv2
I0520 15:35:14.971683  7341 net.cpp:157] Top shape: 180 20 54 46 (8942400)
I0520 15:35:14.971693  7341 net.cpp:165] Memory required for data: 164758320
I0520 15:35:14.971714  7341 layer_factory.hpp:77] Creating layer relu2
I0520 15:35:14.971727  7341 net.cpp:106] Creating Layer relu2
I0520 15:35:14.971737  7341 net.cpp:454] relu2 <- conv2
I0520 15:35:14.971750  7341 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:35:14.972084  7341 net.cpp:150] Setting up relu2
I0520 15:35:14.972097  7341 net.cpp:157] Top shape: 180 20 54 46 (8942400)
I0520 15:35:14.972107  7341 net.cpp:165] Memory required for data: 200527920
I0520 15:35:14.972117  7341 layer_factory.hpp:77] Creating layer pool2
I0520 15:35:14.972129  7341 net.cpp:106] Creating Layer pool2
I0520 15:35:14.972141  7341 net.cpp:454] pool2 <- conv2
I0520 15:35:14.972165  7341 net.cpp:411] pool2 -> pool2
I0520 15:35:14.972234  7341 net.cpp:150] Setting up pool2
I0520 15:35:14.972247  7341 net.cpp:157] Top shape: 180 20 27 46 (4471200)
I0520 15:35:14.972256  7341 net.cpp:165] Memory required for data: 218412720
I0520 15:35:14.972265  7341 layer_factory.hpp:77] Creating layer conv3
I0520 15:35:14.972285  7341 net.cpp:106] Creating Layer conv3
I0520 15:35:14.972295  7341 net.cpp:454] conv3 <- pool2
I0520 15:35:14.972307  7341 net.cpp:411] conv3 -> conv3
I0520 15:35:14.974277  7341 net.cpp:150] Setting up conv3
I0520 15:35:14.974300  7341 net.cpp:157] Top shape: 180 28 22 44 (4878720)
I0520 15:35:14.974310  7341 net.cpp:165] Memory required for data: 237927600
I0520 15:35:14.974329  7341 layer_factory.hpp:77] Creating layer relu3
I0520 15:35:14.974345  7341 net.cpp:106] Creating Layer relu3
I0520 15:35:14.974355  7341 net.cpp:454] relu3 <- conv3
I0520 15:35:14.974369  7341 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:35:14.974838  7341 net.cpp:150] Setting up relu3
I0520 15:35:14.974856  7341 net.cpp:157] Top shape: 180 28 22 44 (4878720)
I0520 15:35:14.974866  7341 net.cpp:165] Memory required for data: 257442480
I0520 15:35:14.974876  7341 layer_factory.hpp:77] Creating layer pool3
I0520 15:35:14.974889  7341 net.cpp:106] Creating Layer pool3
I0520 15:35:14.974900  7341 net.cpp:454] pool3 <- conv3
I0520 15:35:14.974911  7341 net.cpp:411] pool3 -> pool3
I0520 15:35:14.974979  7341 net.cpp:150] Setting up pool3
I0520 15:35:14.974992  7341 net.cpp:157] Top shape: 180 28 11 44 (2439360)
I0520 15:35:14.975003  7341 net.cpp:165] Memory required for data: 267199920
I0520 15:35:14.975013  7341 layer_factory.hpp:77] Creating layer conv4
I0520 15:35:14.975028  7341 net.cpp:106] Creating Layer conv4
I0520 15:35:14.975039  7341 net.cpp:454] conv4 <- pool3
I0520 15:35:14.975054  7341 net.cpp:411] conv4 -> conv4
I0520 15:35:14.977772  7341 net.cpp:150] Setting up conv4
I0520 15:35:14.977800  7341 net.cpp:157] Top shape: 180 36 6 42 (1632960)
I0520 15:35:14.977810  7341 net.cpp:165] Memory required for data: 273731760
I0520 15:35:14.977826  7341 layer_factory.hpp:77] Creating layer relu4
I0520 15:35:14.977840  7341 net.cpp:106] Creating Layer relu4
I0520 15:35:14.977850  7341 net.cpp:454] relu4 <- conv4
I0520 15:35:14.977864  7341 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:35:14.978327  7341 net.cpp:150] Setting up relu4
I0520 15:35:14.978343  7341 net.cpp:157] Top shape: 180 36 6 42 (1632960)
I0520 15:35:14.978354  7341 net.cpp:165] Memory required for data: 280263600
I0520 15:35:14.978364  7341 layer_factory.hpp:77] Creating layer pool4
I0520 15:35:14.978377  7341 net.cpp:106] Creating Layer pool4
I0520 15:35:14.978387  7341 net.cpp:454] pool4 <- conv4
I0520 15:35:14.978400  7341 net.cpp:411] pool4 -> pool4
I0520 15:35:14.978468  7341 net.cpp:150] Setting up pool4
I0520 15:35:14.978482  7341 net.cpp:157] Top shape: 180 36 3 42 (816480)
I0520 15:35:14.978492  7341 net.cpp:165] Memory required for data: 283529520
I0520 15:35:14.978502  7341 layer_factory.hpp:77] Creating layer ip1
I0520 15:35:14.978520  7341 net.cpp:106] Creating Layer ip1
I0520 15:35:14.978531  7341 net.cpp:454] ip1 <- pool4
I0520 15:35:14.978544  7341 net.cpp:411] ip1 -> ip1
I0520 15:35:14.993993  7341 net.cpp:150] Setting up ip1
I0520 15:35:14.994021  7341 net.cpp:157] Top shape: 180 196 (35280)
I0520 15:35:14.994035  7341 net.cpp:165] Memory required for data: 283670640
I0520 15:35:14.994057  7341 layer_factory.hpp:77] Creating layer relu5
I0520 15:35:14.994072  7341 net.cpp:106] Creating Layer relu5
I0520 15:35:14.994082  7341 net.cpp:454] relu5 <- ip1
I0520 15:35:14.994096  7341 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:35:14.994437  7341 net.cpp:150] Setting up relu5
I0520 15:35:14.994451  7341 net.cpp:157] Top shape: 180 196 (35280)
I0520 15:35:14.994462  7341 net.cpp:165] Memory required for data: 283811760
I0520 15:35:14.994472  7341 layer_factory.hpp:77] Creating layer drop1
I0520 15:35:14.994493  7341 net.cpp:106] Creating Layer drop1
I0520 15:35:14.994504  7341 net.cpp:454] drop1 <- ip1
I0520 15:35:14.994529  7341 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:35:14.994575  7341 net.cpp:150] Setting up drop1
I0520 15:35:14.994588  7341 net.cpp:157] Top shape: 180 196 (35280)
I0520 15:35:14.994598  7341 net.cpp:165] Memory required for data: 283952880
I0520 15:35:14.994608  7341 layer_factory.hpp:77] Creating layer ip2
I0520 15:35:14.994627  7341 net.cpp:106] Creating Layer ip2
I0520 15:35:14.994637  7341 net.cpp:454] ip2 <- ip1
I0520 15:35:14.994650  7341 net.cpp:411] ip2 -> ip2
I0520 15:35:14.995111  7341 net.cpp:150] Setting up ip2
I0520 15:35:14.995124  7341 net.cpp:157] Top shape: 180 98 (17640)
I0520 15:35:14.995134  7341 net.cpp:165] Memory required for data: 284023440
I0520 15:35:14.995149  7341 layer_factory.hpp:77] Creating layer relu6
I0520 15:35:14.995162  7341 net.cpp:106] Creating Layer relu6
I0520 15:35:14.995172  7341 net.cpp:454] relu6 <- ip2
I0520 15:35:14.995182  7341 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:35:14.995697  7341 net.cpp:150] Setting up relu6
I0520 15:35:14.995713  7341 net.cpp:157] Top shape: 180 98 (17640)
I0520 15:35:14.995724  7341 net.cpp:165] Memory required for data: 284094000
I0520 15:35:14.995734  7341 layer_factory.hpp:77] Creating layer drop2
I0520 15:35:14.995748  7341 net.cpp:106] Creating Layer drop2
I0520 15:35:14.995757  7341 net.cpp:454] drop2 <- ip2
I0520 15:35:14.995769  7341 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:35:14.995811  7341 net.cpp:150] Setting up drop2
I0520 15:35:14.995825  7341 net.cpp:157] Top shape: 180 98 (17640)
I0520 15:35:14.995836  7341 net.cpp:165] Memory required for data: 284164560
I0520 15:35:14.995844  7341 layer_factory.hpp:77] Creating layer ip3
I0520 15:35:14.995858  7341 net.cpp:106] Creating Layer ip3
I0520 15:35:14.995868  7341 net.cpp:454] ip3 <- ip2
I0520 15:35:14.995880  7341 net.cpp:411] ip3 -> ip3
I0520 15:35:14.996091  7341 net.cpp:150] Setting up ip3
I0520 15:35:14.996104  7341 net.cpp:157] Top shape: 180 11 (1980)
I0520 15:35:14.996114  7341 net.cpp:165] Memory required for data: 284172480
I0520 15:35:14.996129  7341 layer_factory.hpp:77] Creating layer drop3
I0520 15:35:14.996141  7341 net.cpp:106] Creating Layer drop3
I0520 15:35:14.996151  7341 net.cpp:454] drop3 <- ip3
I0520 15:35:14.996163  7341 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:35:14.996202  7341 net.cpp:150] Setting up drop3
I0520 15:35:14.996215  7341 net.cpp:157] Top shape: 180 11 (1980)
I0520 15:35:14.996225  7341 net.cpp:165] Memory required for data: 284180400
I0520 15:35:14.996235  7341 layer_factory.hpp:77] Creating layer loss
I0520 15:35:14.996254  7341 net.cpp:106] Creating Layer loss
I0520 15:35:14.996264  7341 net.cpp:454] loss <- ip3
I0520 15:35:14.996275  7341 net.cpp:454] loss <- label
I0520 15:35:14.996289  7341 net.cpp:411] loss -> loss
I0520 15:35:14.996304  7341 layer_factory.hpp:77] Creating layer loss
I0520 15:35:14.996950  7341 net.cpp:150] Setting up loss
I0520 15:35:14.996970  7341 net.cpp:157] Top shape: (1)
I0520 15:35:14.996984  7341 net.cpp:160]     with loss weight 1
I0520 15:35:14.997027  7341 net.cpp:165] Memory required for data: 284180404
I0520 15:35:14.997038  7341 net.cpp:226] loss needs backward computation.
I0520 15:35:14.997050  7341 net.cpp:226] drop3 needs backward computation.
I0520 15:35:14.997059  7341 net.cpp:226] ip3 needs backward computation.
I0520 15:35:14.997071  7341 net.cpp:226] drop2 needs backward computation.
I0520 15:35:14.997079  7341 net.cpp:226] relu6 needs backward computation.
I0520 15:35:14.997089  7341 net.cpp:226] ip2 needs backward computation.
I0520 15:35:14.997099  7341 net.cpp:226] drop1 needs backward computation.
I0520 15:35:14.997108  7341 net.cpp:226] relu5 needs backward computation.
I0520 15:35:14.997118  7341 net.cpp:226] ip1 needs backward computation.
I0520 15:35:14.997128  7341 net.cpp:226] pool4 needs backward computation.
I0520 15:35:14.997139  7341 net.cpp:226] relu4 needs backward computation.
I0520 15:35:14.997148  7341 net.cpp:226] conv4 needs backward computation.
I0520 15:35:14.997159  7341 net.cpp:226] pool3 needs backward computation.
I0520 15:35:14.997169  7341 net.cpp:226] relu3 needs backward computation.
I0520 15:35:14.997186  7341 net.cpp:226] conv3 needs backward computation.
I0520 15:35:14.997198  7341 net.cpp:226] pool2 needs backward computation.
I0520 15:35:14.997210  7341 net.cpp:226] relu2 needs backward computation.
I0520 15:35:14.997220  7341 net.cpp:226] conv2 needs backward computation.
I0520 15:35:14.997230  7341 net.cpp:226] pool1 needs backward computation.
I0520 15:35:14.997241  7341 net.cpp:226] relu1 needs backward computation.
I0520 15:35:14.997249  7341 net.cpp:226] conv1 needs backward computation.
I0520 15:35:14.997262  7341 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:35:14.997270  7341 net.cpp:270] This network produces output loss
I0520 15:35:14.997294  7341 net.cpp:283] Network initialization done.
I0520 15:35:14.998878  7341 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472.prototxt
I0520 15:35:14.998949  7341 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 15:35:14.999303  7341 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 180
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:35:14.999493  7341 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:35:14.999508  7341 net.cpp:106] Creating Layer data_hdf5
I0520 15:35:14.999521  7341 net.cpp:411] data_hdf5 -> data
I0520 15:35:14.999538  7341 net.cpp:411] data_hdf5 -> label
I0520 15:35:14.999554  7341 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 15:35:15.000795  7341 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 15:35:36.305065  7341 net.cpp:150] Setting up data_hdf5
I0520 15:35:36.305233  7341 net.cpp:157] Top shape: 180 1 127 50 (1143000)
I0520 15:35:36.305248  7341 net.cpp:157] Top shape: 180 (180)
I0520 15:35:36.305258  7341 net.cpp:165] Memory required for data: 4572720
I0520 15:35:36.305271  7341 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 15:35:36.305300  7341 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 15:35:36.305311  7341 net.cpp:454] label_data_hdf5_1_split <- label
I0520 15:35:36.305326  7341 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 15:35:36.305347  7341 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 15:35:36.305419  7341 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 15:35:36.305433  7341 net.cpp:157] Top shape: 180 (180)
I0520 15:35:36.305445  7341 net.cpp:157] Top shape: 180 (180)
I0520 15:35:36.305454  7341 net.cpp:165] Memory required for data: 4574160
I0520 15:35:36.305464  7341 layer_factory.hpp:77] Creating layer conv1
I0520 15:35:36.305486  7341 net.cpp:106] Creating Layer conv1
I0520 15:35:36.305496  7341 net.cpp:454] conv1 <- data
I0520 15:35:36.305511  7341 net.cpp:411] conv1 -> conv1
I0520 15:35:36.307451  7341 net.cpp:150] Setting up conv1
I0520 15:35:36.307476  7341 net.cpp:157] Top shape: 180 12 120 48 (12441600)
I0520 15:35:36.307487  7341 net.cpp:165] Memory required for data: 54340560
I0520 15:35:36.307507  7341 layer_factory.hpp:77] Creating layer relu1
I0520 15:35:36.307523  7341 net.cpp:106] Creating Layer relu1
I0520 15:35:36.307533  7341 net.cpp:454] relu1 <- conv1
I0520 15:35:36.307545  7341 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:35:36.308040  7341 net.cpp:150] Setting up relu1
I0520 15:35:36.308056  7341 net.cpp:157] Top shape: 180 12 120 48 (12441600)
I0520 15:35:36.308066  7341 net.cpp:165] Memory required for data: 104106960
I0520 15:35:36.308076  7341 layer_factory.hpp:77] Creating layer pool1
I0520 15:35:36.308092  7341 net.cpp:106] Creating Layer pool1
I0520 15:35:36.308102  7341 net.cpp:454] pool1 <- conv1
I0520 15:35:36.308115  7341 net.cpp:411] pool1 -> pool1
I0520 15:35:36.308189  7341 net.cpp:150] Setting up pool1
I0520 15:35:36.308202  7341 net.cpp:157] Top shape: 180 12 60 48 (6220800)
I0520 15:35:36.308212  7341 net.cpp:165] Memory required for data: 128990160
I0520 15:35:36.308223  7341 layer_factory.hpp:77] Creating layer conv2
I0520 15:35:36.308238  7341 net.cpp:106] Creating Layer conv2
I0520 15:35:36.308248  7341 net.cpp:454] conv2 <- pool1
I0520 15:35:36.308262  7341 net.cpp:411] conv2 -> conv2
I0520 15:35:36.310185  7341 net.cpp:150] Setting up conv2
I0520 15:35:36.310209  7341 net.cpp:157] Top shape: 180 20 54 46 (8942400)
I0520 15:35:36.310217  7341 net.cpp:165] Memory required for data: 164759760
I0520 15:35:36.310237  7341 layer_factory.hpp:77] Creating layer relu2
I0520 15:35:36.310251  7341 net.cpp:106] Creating Layer relu2
I0520 15:35:36.310261  7341 net.cpp:454] relu2 <- conv2
I0520 15:35:36.310273  7341 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:35:36.310606  7341 net.cpp:150] Setting up relu2
I0520 15:35:36.310621  7341 net.cpp:157] Top shape: 180 20 54 46 (8942400)
I0520 15:35:36.310631  7341 net.cpp:165] Memory required for data: 200529360
I0520 15:35:36.310641  7341 layer_factory.hpp:77] Creating layer pool2
I0520 15:35:36.310654  7341 net.cpp:106] Creating Layer pool2
I0520 15:35:36.310663  7341 net.cpp:454] pool2 <- conv2
I0520 15:35:36.310677  7341 net.cpp:411] pool2 -> pool2
I0520 15:35:36.310747  7341 net.cpp:150] Setting up pool2
I0520 15:35:36.310760  7341 net.cpp:157] Top shape: 180 20 27 46 (4471200)
I0520 15:35:36.310770  7341 net.cpp:165] Memory required for data: 218414160
I0520 15:35:36.310781  7341 layer_factory.hpp:77] Creating layer conv3
I0520 15:35:36.310798  7341 net.cpp:106] Creating Layer conv3
I0520 15:35:36.310808  7341 net.cpp:454] conv3 <- pool2
I0520 15:35:36.310823  7341 net.cpp:411] conv3 -> conv3
I0520 15:35:36.312793  7341 net.cpp:150] Setting up conv3
I0520 15:35:36.312816  7341 net.cpp:157] Top shape: 180 28 22 44 (4878720)
I0520 15:35:36.312829  7341 net.cpp:165] Memory required for data: 237929040
I0520 15:35:36.312861  7341 layer_factory.hpp:77] Creating layer relu3
I0520 15:35:36.312875  7341 net.cpp:106] Creating Layer relu3
I0520 15:35:36.312885  7341 net.cpp:454] relu3 <- conv3
I0520 15:35:36.312898  7341 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:35:36.313372  7341 net.cpp:150] Setting up relu3
I0520 15:35:36.313388  7341 net.cpp:157] Top shape: 180 28 22 44 (4878720)
I0520 15:35:36.313398  7341 net.cpp:165] Memory required for data: 257443920
I0520 15:35:36.313408  7341 layer_factory.hpp:77] Creating layer pool3
I0520 15:35:36.313422  7341 net.cpp:106] Creating Layer pool3
I0520 15:35:36.313432  7341 net.cpp:454] pool3 <- conv3
I0520 15:35:36.313446  7341 net.cpp:411] pool3 -> pool3
I0520 15:35:36.313516  7341 net.cpp:150] Setting up pool3
I0520 15:35:36.313530  7341 net.cpp:157] Top shape: 180 28 11 44 (2439360)
I0520 15:35:36.313540  7341 net.cpp:165] Memory required for data: 267201360
I0520 15:35:36.313550  7341 layer_factory.hpp:77] Creating layer conv4
I0520 15:35:36.313568  7341 net.cpp:106] Creating Layer conv4
I0520 15:35:36.313578  7341 net.cpp:454] conv4 <- pool3
I0520 15:35:36.313593  7341 net.cpp:411] conv4 -> conv4
I0520 15:35:36.315637  7341 net.cpp:150] Setting up conv4
I0520 15:35:36.315660  7341 net.cpp:157] Top shape: 180 36 6 42 (1632960)
I0520 15:35:36.315672  7341 net.cpp:165] Memory required for data: 273733200
I0520 15:35:36.315687  7341 layer_factory.hpp:77] Creating layer relu4
I0520 15:35:36.315701  7341 net.cpp:106] Creating Layer relu4
I0520 15:35:36.315711  7341 net.cpp:454] relu4 <- conv4
I0520 15:35:36.315724  7341 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:35:36.316191  7341 net.cpp:150] Setting up relu4
I0520 15:35:36.316207  7341 net.cpp:157] Top shape: 180 36 6 42 (1632960)
I0520 15:35:36.316217  7341 net.cpp:165] Memory required for data: 280265040
I0520 15:35:36.316228  7341 layer_factory.hpp:77] Creating layer pool4
I0520 15:35:36.316241  7341 net.cpp:106] Creating Layer pool4
I0520 15:35:36.316251  7341 net.cpp:454] pool4 <- conv4
I0520 15:35:36.316263  7341 net.cpp:411] pool4 -> pool4
I0520 15:35:36.316334  7341 net.cpp:150] Setting up pool4
I0520 15:35:36.316349  7341 net.cpp:157] Top shape: 180 36 3 42 (816480)
I0520 15:35:36.316357  7341 net.cpp:165] Memory required for data: 283530960
I0520 15:35:36.316367  7341 layer_factory.hpp:77] Creating layer ip1
I0520 15:35:36.316383  7341 net.cpp:106] Creating Layer ip1
I0520 15:35:36.316393  7341 net.cpp:454] ip1 <- pool4
I0520 15:35:36.316407  7341 net.cpp:411] ip1 -> ip1
I0520 15:35:36.331881  7341 net.cpp:150] Setting up ip1
I0520 15:35:36.331909  7341 net.cpp:157] Top shape: 180 196 (35280)
I0520 15:35:36.331920  7341 net.cpp:165] Memory required for data: 283672080
I0520 15:35:36.331943  7341 layer_factory.hpp:77] Creating layer relu5
I0520 15:35:36.331957  7341 net.cpp:106] Creating Layer relu5
I0520 15:35:36.331967  7341 net.cpp:454] relu5 <- ip1
I0520 15:35:36.331981  7341 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:35:36.332329  7341 net.cpp:150] Setting up relu5
I0520 15:35:36.332342  7341 net.cpp:157] Top shape: 180 196 (35280)
I0520 15:35:36.332352  7341 net.cpp:165] Memory required for data: 283813200
I0520 15:35:36.332362  7341 layer_factory.hpp:77] Creating layer drop1
I0520 15:35:36.332382  7341 net.cpp:106] Creating Layer drop1
I0520 15:35:36.332392  7341 net.cpp:454] drop1 <- ip1
I0520 15:35:36.332406  7341 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:35:36.332451  7341 net.cpp:150] Setting up drop1
I0520 15:35:36.332464  7341 net.cpp:157] Top shape: 180 196 (35280)
I0520 15:35:36.332473  7341 net.cpp:165] Memory required for data: 283954320
I0520 15:35:36.332484  7341 layer_factory.hpp:77] Creating layer ip2
I0520 15:35:36.332499  7341 net.cpp:106] Creating Layer ip2
I0520 15:35:36.332509  7341 net.cpp:454] ip2 <- ip1
I0520 15:35:36.332520  7341 net.cpp:411] ip2 -> ip2
I0520 15:35:36.333003  7341 net.cpp:150] Setting up ip2
I0520 15:35:36.333017  7341 net.cpp:157] Top shape: 180 98 (17640)
I0520 15:35:36.333027  7341 net.cpp:165] Memory required for data: 284024880
I0520 15:35:36.333055  7341 layer_factory.hpp:77] Creating layer relu6
I0520 15:35:36.333068  7341 net.cpp:106] Creating Layer relu6
I0520 15:35:36.333078  7341 net.cpp:454] relu6 <- ip2
I0520 15:35:36.333091  7341 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:35:36.333622  7341 net.cpp:150] Setting up relu6
I0520 15:35:36.333638  7341 net.cpp:157] Top shape: 180 98 (17640)
I0520 15:35:36.333648  7341 net.cpp:165] Memory required for data: 284095440
I0520 15:35:36.333658  7341 layer_factory.hpp:77] Creating layer drop2
I0520 15:35:36.333673  7341 net.cpp:106] Creating Layer drop2
I0520 15:35:36.333683  7341 net.cpp:454] drop2 <- ip2
I0520 15:35:36.333695  7341 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:35:36.333739  7341 net.cpp:150] Setting up drop2
I0520 15:35:36.333751  7341 net.cpp:157] Top shape: 180 98 (17640)
I0520 15:35:36.333760  7341 net.cpp:165] Memory required for data: 284166000
I0520 15:35:36.333770  7341 layer_factory.hpp:77] Creating layer ip3
I0520 15:35:36.333784  7341 net.cpp:106] Creating Layer ip3
I0520 15:35:36.333794  7341 net.cpp:454] ip3 <- ip2
I0520 15:35:36.333808  7341 net.cpp:411] ip3 -> ip3
I0520 15:35:36.334028  7341 net.cpp:150] Setting up ip3
I0520 15:35:36.334041  7341 net.cpp:157] Top shape: 180 11 (1980)
I0520 15:35:36.334050  7341 net.cpp:165] Memory required for data: 284173920
I0520 15:35:36.334066  7341 layer_factory.hpp:77] Creating layer drop3
I0520 15:35:36.334079  7341 net.cpp:106] Creating Layer drop3
I0520 15:35:36.334089  7341 net.cpp:454] drop3 <- ip3
I0520 15:35:36.334101  7341 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:35:36.334142  7341 net.cpp:150] Setting up drop3
I0520 15:35:36.334156  7341 net.cpp:157] Top shape: 180 11 (1980)
I0520 15:35:36.334166  7341 net.cpp:165] Memory required for data: 284181840
I0520 15:35:36.334174  7341 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 15:35:36.334187  7341 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 15:35:36.334197  7341 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 15:35:36.334209  7341 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 15:35:36.334224  7341 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 15:35:36.334297  7341 net.cpp:150] Setting up ip3_drop3_0_split
I0520 15:35:36.334311  7341 net.cpp:157] Top shape: 180 11 (1980)
I0520 15:35:36.334322  7341 net.cpp:157] Top shape: 180 11 (1980)
I0520 15:35:36.334332  7341 net.cpp:165] Memory required for data: 284197680
I0520 15:35:36.334343  7341 layer_factory.hpp:77] Creating layer accuracy
I0520 15:35:36.334364  7341 net.cpp:106] Creating Layer accuracy
I0520 15:35:36.334374  7341 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 15:35:36.334385  7341 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 15:35:36.334398  7341 net.cpp:411] accuracy -> accuracy
I0520 15:35:36.334424  7341 net.cpp:150] Setting up accuracy
I0520 15:35:36.334435  7341 net.cpp:157] Top shape: (1)
I0520 15:35:36.334445  7341 net.cpp:165] Memory required for data: 284197684
I0520 15:35:36.334455  7341 layer_factory.hpp:77] Creating layer loss
I0520 15:35:36.334470  7341 net.cpp:106] Creating Layer loss
I0520 15:35:36.334480  7341 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 15:35:36.334491  7341 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 15:35:36.334503  7341 net.cpp:411] loss -> loss
I0520 15:35:36.334522  7341 layer_factory.hpp:77] Creating layer loss
I0520 15:35:36.335002  7341 net.cpp:150] Setting up loss
I0520 15:35:36.335016  7341 net.cpp:157] Top shape: (1)
I0520 15:35:36.335026  7341 net.cpp:160]     with loss weight 1
I0520 15:35:36.335044  7341 net.cpp:165] Memory required for data: 284197688
I0520 15:35:36.335054  7341 net.cpp:226] loss needs backward computation.
I0520 15:35:36.335065  7341 net.cpp:228] accuracy does not need backward computation.
I0520 15:35:36.335077  7341 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 15:35:36.335086  7341 net.cpp:226] drop3 needs backward computation.
I0520 15:35:36.335094  7341 net.cpp:226] ip3 needs backward computation.
I0520 15:35:36.335105  7341 net.cpp:226] drop2 needs backward computation.
I0520 15:35:36.335124  7341 net.cpp:226] relu6 needs backward computation.
I0520 15:35:36.335134  7341 net.cpp:226] ip2 needs backward computation.
I0520 15:35:36.335144  7341 net.cpp:226] drop1 needs backward computation.
I0520 15:35:36.335152  7341 net.cpp:226] relu5 needs backward computation.
I0520 15:35:36.335162  7341 net.cpp:226] ip1 needs backward computation.
I0520 15:35:36.335172  7341 net.cpp:226] pool4 needs backward computation.
I0520 15:35:36.335181  7341 net.cpp:226] relu4 needs backward computation.
I0520 15:35:36.335191  7341 net.cpp:226] conv4 needs backward computation.
I0520 15:35:36.335202  7341 net.cpp:226] pool3 needs backward computation.
I0520 15:35:36.335212  7341 net.cpp:226] relu3 needs backward computation.
I0520 15:35:36.335221  7341 net.cpp:226] conv3 needs backward computation.
I0520 15:35:36.335232  7341 net.cpp:226] pool2 needs backward computation.
I0520 15:35:36.335242  7341 net.cpp:226] relu2 needs backward computation.
I0520 15:35:36.335253  7341 net.cpp:226] conv2 needs backward computation.
I0520 15:35:36.335261  7341 net.cpp:226] pool1 needs backward computation.
I0520 15:35:36.335273  7341 net.cpp:226] relu1 needs backward computation.
I0520 15:35:36.335281  7341 net.cpp:226] conv1 needs backward computation.
I0520 15:35:36.335292  7341 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 15:35:36.335304  7341 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:35:36.335314  7341 net.cpp:270] This network produces output accuracy
I0520 15:35:36.335325  7341 net.cpp:270] This network produces output loss
I0520 15:35:36.335352  7341 net.cpp:283] Network initialization done.
I0520 15:35:36.335485  7341 solver.cpp:60] Solver scaffolding done.
I0520 15:35:36.336632  7341 caffe.cpp:212] Starting Optimization
I0520 15:35:36.336652  7341 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 15:35:36.336664  7341 solver.cpp:289] Learning Rate Policy: fixed
I0520 15:35:36.337875  7341 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 15:36:23.151264  7341 solver.cpp:409]     Test net output #0: accuracy = 0.0587369
I0520 15:36:23.151424  7341 solver.cpp:409]     Test net output #1: loss = 2.39783 (* 1 = 2.39783 loss)
I0520 15:36:23.197540  7341 solver.cpp:237] Iteration 0, loss = 2.39716
I0520 15:36:23.197578  7341 solver.cpp:253]     Train net output #0: loss = 2.39716 (* 1 = 2.39716 loss)
I0520 15:36:23.197597  7341 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 15:36:31.457239  7341 solver.cpp:237] Iteration 83, loss = 2.33908
I0520 15:36:31.457275  7341 solver.cpp:253]     Train net output #0: loss = 2.33908 (* 1 = 2.33908 loss)
I0520 15:36:31.457289  7341 sgd_solver.cpp:106] Iteration 83, lr = 0.0025
I0520 15:36:39.714433  7341 solver.cpp:237] Iteration 166, loss = 2.30261
I0520 15:36:39.714468  7341 solver.cpp:253]     Train net output #0: loss = 2.30261 (* 1 = 2.30261 loss)
I0520 15:36:39.714483  7341 sgd_solver.cpp:106] Iteration 166, lr = 0.0025
I0520 15:36:47.966629  7341 solver.cpp:237] Iteration 249, loss = 2.35298
I0520 15:36:47.966670  7341 solver.cpp:253]     Train net output #0: loss = 2.35298 (* 1 = 2.35298 loss)
I0520 15:36:47.966687  7341 sgd_solver.cpp:106] Iteration 249, lr = 0.0025
I0520 15:36:56.221231  7341 solver.cpp:237] Iteration 332, loss = 2.29901
I0520 15:36:56.221374  7341 solver.cpp:253]     Train net output #0: loss = 2.29901 (* 1 = 2.29901 loss)
I0520 15:36:56.221386  7341 sgd_solver.cpp:106] Iteration 332, lr = 0.0025
I0520 15:37:04.487397  7341 solver.cpp:237] Iteration 415, loss = 2.33004
I0520 15:37:04.487432  7341 solver.cpp:253]     Train net output #0: loss = 2.33004 (* 1 = 2.33004 loss)
I0520 15:37:04.487447  7341 sgd_solver.cpp:106] Iteration 415, lr = 0.0025
I0520 15:37:12.746081  7341 solver.cpp:237] Iteration 498, loss = 2.21921
I0520 15:37:12.746124  7341 solver.cpp:253]     Train net output #0: loss = 2.21921 (* 1 = 2.21921 loss)
I0520 15:37:12.746141  7341 sgd_solver.cpp:106] Iteration 498, lr = 0.0025
I0520 15:37:43.069208  7341 solver.cpp:237] Iteration 581, loss = 2.11588
I0520 15:37:43.069370  7341 solver.cpp:253]     Train net output #0: loss = 2.11588 (* 1 = 2.11588 loss)
I0520 15:37:43.069385  7341 sgd_solver.cpp:106] Iteration 581, lr = 0.0025
I0520 15:37:51.333081  7341 solver.cpp:237] Iteration 664, loss = 2.01486
I0520 15:37:51.333115  7341 solver.cpp:253]     Train net output #0: loss = 2.01486 (* 1 = 2.01486 loss)
I0520 15:37:51.333132  7341 sgd_solver.cpp:106] Iteration 664, lr = 0.0025
I0520 15:37:59.606657  7341 solver.cpp:237] Iteration 747, loss = 2.02139
I0520 15:37:59.606700  7341 solver.cpp:253]     Train net output #0: loss = 2.02139 (* 1 = 2.02139 loss)
I0520 15:37:59.606716  7341 sgd_solver.cpp:106] Iteration 747, lr = 0.0025
I0520 15:38:07.870944  7341 solver.cpp:237] Iteration 830, loss = 2.01248
I0520 15:38:07.870980  7341 solver.cpp:253]     Train net output #0: loss = 2.01248 (* 1 = 2.01248 loss)
I0520 15:38:07.870995  7341 sgd_solver.cpp:106] Iteration 830, lr = 0.0025
I0520 15:38:08.069952  7341 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_833.caffemodel
I0520 15:38:08.181020  7341 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_833.solverstate
I0520 15:38:16.202088  7341 solver.cpp:237] Iteration 913, loss = 1.721
I0520 15:38:16.202244  7341 solver.cpp:253]     Train net output #0: loss = 1.721 (* 1 = 1.721 loss)
I0520 15:38:16.202257  7341 sgd_solver.cpp:106] Iteration 913, lr = 0.0025
I0520 15:38:24.464093  7341 solver.cpp:237] Iteration 996, loss = 1.79416
I0520 15:38:24.464133  7341 solver.cpp:253]     Train net output #0: loss = 1.79416 (* 1 = 1.79416 loss)
I0520 15:38:24.464153  7341 sgd_solver.cpp:106] Iteration 996, lr = 0.0025
I0520 15:38:32.725587  7341 solver.cpp:237] Iteration 1079, loss = 1.82978
I0520 15:38:32.725621  7341 solver.cpp:253]     Train net output #0: loss = 1.82978 (* 1 = 1.82978 loss)
I0520 15:38:32.725637  7341 sgd_solver.cpp:106] Iteration 1079, lr = 0.0025
I0520 15:39:03.069988  7341 solver.cpp:237] Iteration 1162, loss = 1.8833
I0520 15:39:03.070143  7341 solver.cpp:253]     Train net output #0: loss = 1.8833 (* 1 = 1.8833 loss)
I0520 15:39:03.070160  7341 sgd_solver.cpp:106] Iteration 1162, lr = 0.0025
I0520 15:39:11.331606  7341 solver.cpp:237] Iteration 1245, loss = 1.71118
I0520 15:39:11.331640  7341 solver.cpp:253]     Train net output #0: loss = 1.71118 (* 1 = 1.71118 loss)
I0520 15:39:11.331656  7341 sgd_solver.cpp:106] Iteration 1245, lr = 0.0025
I0520 15:39:19.595793  7341 solver.cpp:237] Iteration 1328, loss = 1.71827
I0520 15:39:19.595830  7341 solver.cpp:253]     Train net output #0: loss = 1.71827 (* 1 = 1.71827 loss)
I0520 15:39:19.595851  7341 sgd_solver.cpp:106] Iteration 1328, lr = 0.0025
I0520 15:39:27.864241  7341 solver.cpp:237] Iteration 1411, loss = 1.74266
I0520 15:39:27.864276  7341 solver.cpp:253]     Train net output #0: loss = 1.74266 (* 1 = 1.74266 loss)
I0520 15:39:27.864290  7341 sgd_solver.cpp:106] Iteration 1411, lr = 0.0025
I0520 15:39:36.128417  7341 solver.cpp:237] Iteration 1494, loss = 1.70344
I0520 15:39:36.128566  7341 solver.cpp:253]     Train net output #0: loss = 1.70344 (* 1 = 1.70344 loss)
I0520 15:39:36.128579  7341 sgd_solver.cpp:106] Iteration 1494, lr = 0.0025
I0520 15:39:44.396615  7341 solver.cpp:237] Iteration 1577, loss = 1.69031
I0520 15:39:44.396659  7341 solver.cpp:253]     Train net output #0: loss = 1.69031 (* 1 = 1.69031 loss)
I0520 15:39:44.396674  7341 sgd_solver.cpp:106] Iteration 1577, lr = 0.0025
I0520 15:39:52.668692  7341 solver.cpp:237] Iteration 1660, loss = 1.81555
I0520 15:39:52.668726  7341 solver.cpp:253]     Train net output #0: loss = 1.81555 (* 1 = 1.81555 loss)
I0520 15:39:52.668741  7341 sgd_solver.cpp:106] Iteration 1660, lr = 0.0025
I0520 15:39:53.166793  7341 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_1666.caffemodel
I0520 15:39:53.274562  7341 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_1666.solverstate
I0520 15:39:53.301707  7341 solver.cpp:341] Iteration 1666, Testing net (#0)
I0520 15:40:39.063709  7341 solver.cpp:409]     Test net output #0: accuracy = 0.641223
I0520 15:40:39.063927  7341 solver.cpp:409]     Test net output #1: loss = 1.22732 (* 1 = 1.22732 loss)
I0520 15:41:08.921977  7341 solver.cpp:237] Iteration 1743, loss = 1.74464
I0520 15:41:08.922026  7341 solver.cpp:253]     Train net output #0: loss = 1.74464 (* 1 = 1.74464 loss)
I0520 15:41:08.922040  7341 sgd_solver.cpp:106] Iteration 1743, lr = 0.0025
I0520 15:41:17.179787  7341 solver.cpp:237] Iteration 1826, loss = 1.65844
I0520 15:41:17.179936  7341 solver.cpp:253]     Train net output #0: loss = 1.65844 (* 1 = 1.65844 loss)
I0520 15:41:17.179950  7341 sgd_solver.cpp:106] Iteration 1826, lr = 0.0025
I0520 15:41:25.447034  7341 solver.cpp:237] Iteration 1909, loss = 1.77464
I0520 15:41:25.447067  7341 solver.cpp:253]     Train net output #0: loss = 1.77464 (* 1 = 1.77464 loss)
I0520 15:41:25.447082  7341 sgd_solver.cpp:106] Iteration 1909, lr = 0.0025
I0520 15:41:33.706943  7341 solver.cpp:237] Iteration 1992, loss = 1.73071
I0520 15:41:33.706977  7341 solver.cpp:253]     Train net output #0: loss = 1.73071 (* 1 = 1.73071 loss)
I0520 15:41:33.706993  7341 sgd_solver.cpp:106] Iteration 1992, lr = 0.0025
I0520 15:41:41.969135  7341 solver.cpp:237] Iteration 2075, loss = 1.68287
I0520 15:41:41.969179  7341 solver.cpp:253]     Train net output #0: loss = 1.68287 (* 1 = 1.68287 loss)
I0520 15:41:41.969193  7341 sgd_solver.cpp:106] Iteration 2075, lr = 0.0025
I0520 15:41:50.228042  7341 solver.cpp:237] Iteration 2158, loss = 1.55648
I0520 15:41:50.228178  7341 solver.cpp:253]     Train net output #0: loss = 1.55648 (* 1 = 1.55648 loss)
I0520 15:41:50.228191  7341 sgd_solver.cpp:106] Iteration 2158, lr = 0.0025
I0520 15:42:20.680724  7341 solver.cpp:237] Iteration 2241, loss = 1.76943
I0520 15:42:20.680887  7341 solver.cpp:253]     Train net output #0: loss = 1.76943 (* 1 = 1.76943 loss)
I0520 15:42:20.680902  7341 sgd_solver.cpp:106] Iteration 2241, lr = 0.0025
I0520 15:42:28.943929  7341 solver.cpp:237] Iteration 2324, loss = 1.67706
I0520 15:42:28.943964  7341 solver.cpp:253]     Train net output #0: loss = 1.67706 (* 1 = 1.67706 loss)
I0520 15:42:28.943979  7341 sgd_solver.cpp:106] Iteration 2324, lr = 0.0025
I0520 15:42:37.206848  7341 solver.cpp:237] Iteration 2407, loss = 1.70533
I0520 15:42:37.206887  7341 solver.cpp:253]     Train net output #0: loss = 1.70533 (* 1 = 1.70533 loss)
I0520 15:42:37.206905  7341 sgd_solver.cpp:106] Iteration 2407, lr = 0.0025
I0520 15:42:45.469017  7341 solver.cpp:237] Iteration 2490, loss = 1.50713
I0520 15:42:45.469051  7341 solver.cpp:253]     Train net output #0: loss = 1.50713 (* 1 = 1.50713 loss)
I0520 15:42:45.469066  7341 sgd_solver.cpp:106] Iteration 2490, lr = 0.0025
I0520 15:42:46.264536  7341 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_2499.caffemodel
I0520 15:42:46.373591  7341 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_2499.solverstate
I0520 15:42:53.797691  7341 solver.cpp:237] Iteration 2573, loss = 1.80412
I0520 15:42:53.797859  7341 solver.cpp:253]     Train net output #0: loss = 1.80412 (* 1 = 1.80412 loss)
I0520 15:42:53.797873  7341 sgd_solver.cpp:106] Iteration 2573, lr = 0.0025
I0520 15:43:02.062059  7341 solver.cpp:237] Iteration 2656, loss = 1.56745
I0520 15:43:02.062106  7341 solver.cpp:253]     Train net output #0: loss = 1.56745 (* 1 = 1.56745 loss)
I0520 15:43:02.062121  7341 sgd_solver.cpp:106] Iteration 2656, lr = 0.0025
I0520 15:43:10.320416  7341 solver.cpp:237] Iteration 2739, loss = 1.71547
I0520 15:43:10.320451  7341 solver.cpp:253]     Train net output #0: loss = 1.71547 (* 1 = 1.71547 loss)
I0520 15:43:10.320466  7341 sgd_solver.cpp:106] Iteration 2739, lr = 0.0025
I0520 15:43:40.672091  7341 solver.cpp:237] Iteration 2822, loss = 1.64446
I0520 15:43:40.672245  7341 solver.cpp:253]     Train net output #0: loss = 1.64446 (* 1 = 1.64446 loss)
I0520 15:43:40.672260  7341 sgd_solver.cpp:106] Iteration 2822, lr = 0.0025
I0520 15:43:48.932636  7341 solver.cpp:237] Iteration 2905, loss = 1.60913
I0520 15:43:48.932682  7341 solver.cpp:253]     Train net output #0: loss = 1.60913 (* 1 = 1.60913 loss)
I0520 15:43:48.932698  7341 sgd_solver.cpp:106] Iteration 2905, lr = 0.0025
I0520 15:43:57.192219  7341 solver.cpp:237] Iteration 2988, loss = 1.65272
I0520 15:43:57.192253  7341 solver.cpp:253]     Train net output #0: loss = 1.65272 (* 1 = 1.65272 loss)
I0520 15:43:57.192267  7341 sgd_solver.cpp:106] Iteration 2988, lr = 0.0025
I0520 15:44:05.464103  7341 solver.cpp:237] Iteration 3071, loss = 1.63201
I0520 15:44:05.464138  7341 solver.cpp:253]     Train net output #0: loss = 1.63201 (* 1 = 1.63201 loss)
I0520 15:44:05.464153  7341 sgd_solver.cpp:106] Iteration 3071, lr = 0.0025
I0520 15:44:13.738934  7341 solver.cpp:237] Iteration 3154, loss = 1.8708
I0520 15:44:13.739081  7341 solver.cpp:253]     Train net output #0: loss = 1.8708 (* 1 = 1.8708 loss)
I0520 15:44:13.739095  7341 sgd_solver.cpp:106] Iteration 3154, lr = 0.0025
I0520 15:44:22.006683  7341 solver.cpp:237] Iteration 3237, loss = 1.64438
I0520 15:44:22.006716  7341 solver.cpp:253]     Train net output #0: loss = 1.64438 (* 1 = 1.64438 loss)
I0520 15:44:22.006731  7341 sgd_solver.cpp:106] Iteration 3237, lr = 0.0025
I0520 15:44:30.272567  7341 solver.cpp:237] Iteration 3320, loss = 1.63635
I0520 15:44:30.272603  7341 solver.cpp:253]     Train net output #0: loss = 1.63635 (* 1 = 1.63635 loss)
I0520 15:44:30.272615  7341 sgd_solver.cpp:106] Iteration 3320, lr = 0.0025
I0520 15:44:31.366891  7341 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_3332.caffemodel
I0520 15:44:31.476972  7341 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_180_2016-05-20T11.20.39.391472_iter_3332.solverstate
I0520 15:44:31.504990  7341 solver.cpp:341] Iteration 3332, Testing net (#0)
I0520 15:45:38.149613  7341 solver.cpp:409]     Test net output #0: accuracy = 0.70249
I0520 15:45:38.149780  7341 solver.cpp:409]     Test net output #1: loss = 1.04619 (* 1 = 1.04619 loss)
I0520 15:46:07.342834  7341 solver.cpp:237] Iteration 3403, loss = 1.56705
I0520 15:46:07.342885  7341 solver.cpp:253]     Train net output #0: loss = 1.56705 (* 1 = 1.56705 loss)
I0520 15:46:07.342900  7341 sgd_solver.cpp:106] Iteration 3403, lr = 0.0025
I0520 15:46:15.596570  7341 solver.cpp:237] Iteration 3486, loss = 1.59607
I0520 15:46:15.596712  7341 solver.cpp:253]     Train net output #0: loss = 1.59607 (* 1 = 1.59607 loss)
I0520 15:46:15.596726  7341 sgd_solver.cpp:106] Iteration 3486, lr = 0.0025
I0520 15:46:23.862350  7341 solver.cpp:237] Iteration 3569, loss = 1.69045
I0520 15:46:23.862391  7341 solver.cpp:253]     Train net output #0: loss = 1.69045 (* 1 = 1.69045 loss)
I0520 15:46:23.862408  7341 sgd_solver.cpp:106] Iteration 3569, lr = 0.0025
I0520 15:46:32.131042  7341 solver.cpp:237] Iteration 3652, loss = 1.45423
I0520 15:46:32.131075  7341 solver.cpp:253]     Train net output #0: loss = 1.45423 (* 1 = 1.45423 loss)
I0520 15:46:32.131088  7341 sgd_solver.cpp:106] Iteration 3652, lr = 0.0025
aprun: Apid 11232984: Caught signal Terminated, sending to application
*** Aborted at 1463773598 (unix time) try "date -d @1463773598" if you are using GNU date ***
PC: @     0x2aaaaaaca834 ([vdso]+0x833)
aprun: Apid 11232984: Caught signal Terminated, sending to application
*** SIGTERM (@0x1caa) received by PID 7341 (TID 0x2aaac746f900) from PID 7338; stack trace: ***
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaab7c78850 (unknown)
    @     0x2aaaaaaca834 ([vdso]+0x833)
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaab82072d0 maybe_syscall_gettime_cpu
    @     0x2aaab82074b0 __GI_clock_gettime
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaab9898f3e (unknown)
    @     0x2aaab928ec5b (unknown)
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaab926d723 (unknown)
    @     0x2aaab92655e1 (unknown)
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaab9266356 (unknown)
    @     0x2aaab91d5562 (unknown)
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaab91d56ba (unknown)
    @     0x2aaab91b8715 cuMemcpy
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaaaacf9e92 (unknown)
    @     0x2aaaaacde306 (unknown)
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @     0x2aaaaad00328 cudaMemcpy
    @           0x4d6a10 caffe::caffe_copy<>()
    @           0x626ea9 caffe::HDF5DataLayer<>::Forward_gpu()
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
    @           0x5ca109 caffe::Solver<>::Step()
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
aprun: Apid 11232984: Caught signal Terminated, sending to application
    @           0x438669 (unknown)
aprun: Apid 11232984: Caught signal Terminated, sending to application
aprun: Apid 11232984: Caught signal Terminated, sending to application
aprun: Apid 11232984: Caught signal Terminated, sending to application
aprun: Apid 11232984: Caught signal Terminated, sending to application
aprun: Apid 11232984: Caught signal Terminated, sending to application
aprun: Apid 11232984: Caught signal Terminated, sending to application
aprun: Apid 11232984: Caught signal Terminated, sending to application
