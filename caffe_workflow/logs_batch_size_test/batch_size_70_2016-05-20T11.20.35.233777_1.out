2805098
I0520 13:17:13.734906 12390 caffe.cpp:184] Using GPUs 0
I0520 13:17:14.156056 12390 solver.cpp:48] Initializing solver from parameters: 
test_iter: 2142
test_interval: 4285
base_lr: 0.0025
display: 214
max_iter: 21428
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 2142
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777.prototxt"
I0520 13:17:14.157805 12390 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777.prototxt
I0520 13:17:14.161177 12390 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 13:17:14.161237 12390 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 13:17:14.161582 12390 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 70
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 13:17:14.161761 12390 layer_factory.hpp:77] Creating layer data_hdf5
I0520 13:17:14.161785 12390 net.cpp:106] Creating Layer data_hdf5
I0520 13:17:14.161800 12390 net.cpp:411] data_hdf5 -> data
I0520 13:17:14.161834 12390 net.cpp:411] data_hdf5 -> label
I0520 13:17:14.161866 12390 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 13:17:14.163048 12390 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 13:17:14.165192 12390 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 13:17:35.731528 12390 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 13:17:35.736625 12390 net.cpp:150] Setting up data_hdf5
I0520 13:17:35.736670 12390 net.cpp:157] Top shape: 70 1 127 50 (444500)
I0520 13:17:35.736685 12390 net.cpp:157] Top shape: 70 (70)
I0520 13:17:35.736695 12390 net.cpp:165] Memory required for data: 1778280
I0520 13:17:35.736708 12390 layer_factory.hpp:77] Creating layer conv1
I0520 13:17:35.736742 12390 net.cpp:106] Creating Layer conv1
I0520 13:17:35.736753 12390 net.cpp:454] conv1 <- data
I0520 13:17:35.736778 12390 net.cpp:411] conv1 -> conv1
I0520 13:17:36.098496 12390 net.cpp:150] Setting up conv1
I0520 13:17:36.098543 12390 net.cpp:157] Top shape: 70 12 120 48 (4838400)
I0520 13:17:36.098554 12390 net.cpp:165] Memory required for data: 21131880
I0520 13:17:36.098585 12390 layer_factory.hpp:77] Creating layer relu1
I0520 13:17:36.098606 12390 net.cpp:106] Creating Layer relu1
I0520 13:17:36.098618 12390 net.cpp:454] relu1 <- conv1
I0520 13:17:36.098630 12390 net.cpp:397] relu1 -> conv1 (in-place)
I0520 13:17:36.099148 12390 net.cpp:150] Setting up relu1
I0520 13:17:36.099164 12390 net.cpp:157] Top shape: 70 12 120 48 (4838400)
I0520 13:17:36.099175 12390 net.cpp:165] Memory required for data: 40485480
I0520 13:17:36.099185 12390 layer_factory.hpp:77] Creating layer pool1
I0520 13:17:36.099201 12390 net.cpp:106] Creating Layer pool1
I0520 13:17:36.099211 12390 net.cpp:454] pool1 <- conv1
I0520 13:17:36.099225 12390 net.cpp:411] pool1 -> pool1
I0520 13:17:36.099304 12390 net.cpp:150] Setting up pool1
I0520 13:17:36.099319 12390 net.cpp:157] Top shape: 70 12 60 48 (2419200)
I0520 13:17:36.099329 12390 net.cpp:165] Memory required for data: 50162280
I0520 13:17:36.099339 12390 layer_factory.hpp:77] Creating layer conv2
I0520 13:17:36.099370 12390 net.cpp:106] Creating Layer conv2
I0520 13:17:36.099380 12390 net.cpp:454] conv2 <- pool1
I0520 13:17:36.099393 12390 net.cpp:411] conv2 -> conv2
I0520 13:17:36.102141 12390 net.cpp:150] Setting up conv2
I0520 13:17:36.102169 12390 net.cpp:157] Top shape: 70 20 54 46 (3477600)
I0520 13:17:36.102180 12390 net.cpp:165] Memory required for data: 64072680
I0520 13:17:36.102200 12390 layer_factory.hpp:77] Creating layer relu2
I0520 13:17:36.102213 12390 net.cpp:106] Creating Layer relu2
I0520 13:17:36.102223 12390 net.cpp:454] relu2 <- conv2
I0520 13:17:36.102236 12390 net.cpp:397] relu2 -> conv2 (in-place)
I0520 13:17:36.102568 12390 net.cpp:150] Setting up relu2
I0520 13:17:36.102582 12390 net.cpp:157] Top shape: 70 20 54 46 (3477600)
I0520 13:17:36.102592 12390 net.cpp:165] Memory required for data: 77983080
I0520 13:17:36.102603 12390 layer_factory.hpp:77] Creating layer pool2
I0520 13:17:36.102615 12390 net.cpp:106] Creating Layer pool2
I0520 13:17:36.102625 12390 net.cpp:454] pool2 <- conv2
I0520 13:17:36.102650 12390 net.cpp:411] pool2 -> pool2
I0520 13:17:36.102720 12390 net.cpp:150] Setting up pool2
I0520 13:17:36.102733 12390 net.cpp:157] Top shape: 70 20 27 46 (1738800)
I0520 13:17:36.102743 12390 net.cpp:165] Memory required for data: 84938280
I0520 13:17:36.102751 12390 layer_factory.hpp:77] Creating layer conv3
I0520 13:17:36.102769 12390 net.cpp:106] Creating Layer conv3
I0520 13:17:36.102779 12390 net.cpp:454] conv3 <- pool2
I0520 13:17:36.102793 12390 net.cpp:411] conv3 -> conv3
I0520 13:17:36.104722 12390 net.cpp:150] Setting up conv3
I0520 13:17:36.104745 12390 net.cpp:157] Top shape: 70 28 22 44 (1897280)
I0520 13:17:36.104758 12390 net.cpp:165] Memory required for data: 92527400
I0520 13:17:36.104776 12390 layer_factory.hpp:77] Creating layer relu3
I0520 13:17:36.104794 12390 net.cpp:106] Creating Layer relu3
I0520 13:17:36.104804 12390 net.cpp:454] relu3 <- conv3
I0520 13:17:36.104816 12390 net.cpp:397] relu3 -> conv3 (in-place)
I0520 13:17:36.105283 12390 net.cpp:150] Setting up relu3
I0520 13:17:36.105300 12390 net.cpp:157] Top shape: 70 28 22 44 (1897280)
I0520 13:17:36.105310 12390 net.cpp:165] Memory required for data: 100116520
I0520 13:17:36.105320 12390 layer_factory.hpp:77] Creating layer pool3
I0520 13:17:36.105334 12390 net.cpp:106] Creating Layer pool3
I0520 13:17:36.105343 12390 net.cpp:454] pool3 <- conv3
I0520 13:17:36.105355 12390 net.cpp:411] pool3 -> pool3
I0520 13:17:36.105423 12390 net.cpp:150] Setting up pool3
I0520 13:17:36.105437 12390 net.cpp:157] Top shape: 70 28 11 44 (948640)
I0520 13:17:36.105446 12390 net.cpp:165] Memory required for data: 103911080
I0520 13:17:36.105454 12390 layer_factory.hpp:77] Creating layer conv4
I0520 13:17:36.105471 12390 net.cpp:106] Creating Layer conv4
I0520 13:17:36.105481 12390 net.cpp:454] conv4 <- pool3
I0520 13:17:36.105495 12390 net.cpp:411] conv4 -> conv4
I0520 13:17:36.108292 12390 net.cpp:150] Setting up conv4
I0520 13:17:36.108321 12390 net.cpp:157] Top shape: 70 36 6 42 (635040)
I0520 13:17:36.108331 12390 net.cpp:165] Memory required for data: 106451240
I0520 13:17:36.108347 12390 layer_factory.hpp:77] Creating layer relu4
I0520 13:17:36.108361 12390 net.cpp:106] Creating Layer relu4
I0520 13:17:36.108371 12390 net.cpp:454] relu4 <- conv4
I0520 13:17:36.108384 12390 net.cpp:397] relu4 -> conv4 (in-place)
I0520 13:17:36.108855 12390 net.cpp:150] Setting up relu4
I0520 13:17:36.108871 12390 net.cpp:157] Top shape: 70 36 6 42 (635040)
I0520 13:17:36.108881 12390 net.cpp:165] Memory required for data: 108991400
I0520 13:17:36.108892 12390 layer_factory.hpp:77] Creating layer pool4
I0520 13:17:36.108906 12390 net.cpp:106] Creating Layer pool4
I0520 13:17:36.108916 12390 net.cpp:454] pool4 <- conv4
I0520 13:17:36.108928 12390 net.cpp:411] pool4 -> pool4
I0520 13:17:36.108996 12390 net.cpp:150] Setting up pool4
I0520 13:17:36.109009 12390 net.cpp:157] Top shape: 70 36 3 42 (317520)
I0520 13:17:36.109020 12390 net.cpp:165] Memory required for data: 110261480
I0520 13:17:36.109030 12390 layer_factory.hpp:77] Creating layer ip1
I0520 13:17:36.109050 12390 net.cpp:106] Creating Layer ip1
I0520 13:17:36.109061 12390 net.cpp:454] ip1 <- pool4
I0520 13:17:36.109073 12390 net.cpp:411] ip1 -> ip1
I0520 13:17:36.124531 12390 net.cpp:150] Setting up ip1
I0520 13:17:36.124560 12390 net.cpp:157] Top shape: 70 196 (13720)
I0520 13:17:36.124573 12390 net.cpp:165] Memory required for data: 110316360
I0520 13:17:36.124595 12390 layer_factory.hpp:77] Creating layer relu5
I0520 13:17:36.124610 12390 net.cpp:106] Creating Layer relu5
I0520 13:17:36.124620 12390 net.cpp:454] relu5 <- ip1
I0520 13:17:36.124634 12390 net.cpp:397] relu5 -> ip1 (in-place)
I0520 13:17:36.124979 12390 net.cpp:150] Setting up relu5
I0520 13:17:36.124994 12390 net.cpp:157] Top shape: 70 196 (13720)
I0520 13:17:36.125005 12390 net.cpp:165] Memory required for data: 110371240
I0520 13:17:36.125015 12390 layer_factory.hpp:77] Creating layer drop1
I0520 13:17:36.125036 12390 net.cpp:106] Creating Layer drop1
I0520 13:17:36.125046 12390 net.cpp:454] drop1 <- ip1
I0520 13:17:36.125058 12390 net.cpp:397] drop1 -> ip1 (in-place)
I0520 13:17:36.125119 12390 net.cpp:150] Setting up drop1
I0520 13:17:36.125133 12390 net.cpp:157] Top shape: 70 196 (13720)
I0520 13:17:36.125144 12390 net.cpp:165] Memory required for data: 110426120
I0520 13:17:36.125154 12390 layer_factory.hpp:77] Creating layer ip2
I0520 13:17:36.125172 12390 net.cpp:106] Creating Layer ip2
I0520 13:17:36.125182 12390 net.cpp:454] ip2 <- ip1
I0520 13:17:36.125195 12390 net.cpp:411] ip2 -> ip2
I0520 13:17:36.125658 12390 net.cpp:150] Setting up ip2
I0520 13:17:36.125671 12390 net.cpp:157] Top shape: 70 98 (6860)
I0520 13:17:36.125680 12390 net.cpp:165] Memory required for data: 110453560
I0520 13:17:36.125696 12390 layer_factory.hpp:77] Creating layer relu6
I0520 13:17:36.125710 12390 net.cpp:106] Creating Layer relu6
I0520 13:17:36.125718 12390 net.cpp:454] relu6 <- ip2
I0520 13:17:36.125730 12390 net.cpp:397] relu6 -> ip2 (in-place)
I0520 13:17:36.126245 12390 net.cpp:150] Setting up relu6
I0520 13:17:36.126261 12390 net.cpp:157] Top shape: 70 98 (6860)
I0520 13:17:36.126271 12390 net.cpp:165] Memory required for data: 110481000
I0520 13:17:36.126281 12390 layer_factory.hpp:77] Creating layer drop2
I0520 13:17:36.126296 12390 net.cpp:106] Creating Layer drop2
I0520 13:17:36.126304 12390 net.cpp:454] drop2 <- ip2
I0520 13:17:36.126317 12390 net.cpp:397] drop2 -> ip2 (in-place)
I0520 13:17:36.126360 12390 net.cpp:150] Setting up drop2
I0520 13:17:36.126374 12390 net.cpp:157] Top shape: 70 98 (6860)
I0520 13:17:36.126384 12390 net.cpp:165] Memory required for data: 110508440
I0520 13:17:36.126394 12390 layer_factory.hpp:77] Creating layer ip3
I0520 13:17:36.126406 12390 net.cpp:106] Creating Layer ip3
I0520 13:17:36.126415 12390 net.cpp:454] ip3 <- ip2
I0520 13:17:36.126428 12390 net.cpp:411] ip3 -> ip3
I0520 13:17:36.126637 12390 net.cpp:150] Setting up ip3
I0520 13:17:36.126651 12390 net.cpp:157] Top shape: 70 11 (770)
I0520 13:17:36.126660 12390 net.cpp:165] Memory required for data: 110511520
I0520 13:17:36.126675 12390 layer_factory.hpp:77] Creating layer drop3
I0520 13:17:36.126688 12390 net.cpp:106] Creating Layer drop3
I0520 13:17:36.126698 12390 net.cpp:454] drop3 <- ip3
I0520 13:17:36.126709 12390 net.cpp:397] drop3 -> ip3 (in-place)
I0520 13:17:36.126749 12390 net.cpp:150] Setting up drop3
I0520 13:17:36.126762 12390 net.cpp:157] Top shape: 70 11 (770)
I0520 13:17:36.126771 12390 net.cpp:165] Memory required for data: 110514600
I0520 13:17:36.126781 12390 layer_factory.hpp:77] Creating layer loss
I0520 13:17:36.126801 12390 net.cpp:106] Creating Layer loss
I0520 13:17:36.126811 12390 net.cpp:454] loss <- ip3
I0520 13:17:36.126821 12390 net.cpp:454] loss <- label
I0520 13:17:36.126834 12390 net.cpp:411] loss -> loss
I0520 13:17:36.126850 12390 layer_factory.hpp:77] Creating layer loss
I0520 13:17:36.127498 12390 net.cpp:150] Setting up loss
I0520 13:17:36.127519 12390 net.cpp:157] Top shape: (1)
I0520 13:17:36.127533 12390 net.cpp:160]     with loss weight 1
I0520 13:17:36.127575 12390 net.cpp:165] Memory required for data: 110514604
I0520 13:17:36.127586 12390 net.cpp:226] loss needs backward computation.
I0520 13:17:36.127598 12390 net.cpp:226] drop3 needs backward computation.
I0520 13:17:36.127606 12390 net.cpp:226] ip3 needs backward computation.
I0520 13:17:36.127615 12390 net.cpp:226] drop2 needs backward computation.
I0520 13:17:36.127625 12390 net.cpp:226] relu6 needs backward computation.
I0520 13:17:36.127635 12390 net.cpp:226] ip2 needs backward computation.
I0520 13:17:36.127645 12390 net.cpp:226] drop1 needs backward computation.
I0520 13:17:36.127655 12390 net.cpp:226] relu5 needs backward computation.
I0520 13:17:36.127665 12390 net.cpp:226] ip1 needs backward computation.
I0520 13:17:36.127676 12390 net.cpp:226] pool4 needs backward computation.
I0520 13:17:36.127686 12390 net.cpp:226] relu4 needs backward computation.
I0520 13:17:36.127696 12390 net.cpp:226] conv4 needs backward computation.
I0520 13:17:36.127707 12390 net.cpp:226] pool3 needs backward computation.
I0520 13:17:36.127717 12390 net.cpp:226] relu3 needs backward computation.
I0520 13:17:36.127735 12390 net.cpp:226] conv3 needs backward computation.
I0520 13:17:36.127746 12390 net.cpp:226] pool2 needs backward computation.
I0520 13:17:36.127758 12390 net.cpp:226] relu2 needs backward computation.
I0520 13:17:36.127768 12390 net.cpp:226] conv2 needs backward computation.
I0520 13:17:36.127779 12390 net.cpp:226] pool1 needs backward computation.
I0520 13:17:36.127789 12390 net.cpp:226] relu1 needs backward computation.
I0520 13:17:36.127799 12390 net.cpp:226] conv1 needs backward computation.
I0520 13:17:36.127810 12390 net.cpp:228] data_hdf5 does not need backward computation.
I0520 13:17:36.127820 12390 net.cpp:270] This network produces output loss
I0520 13:17:36.127845 12390 net.cpp:283] Network initialization done.
I0520 13:17:36.129405 12390 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777.prototxt
I0520 13:17:36.129475 12390 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 13:17:36.129832 12390 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 70
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 13:17:36.130022 12390 layer_factory.hpp:77] Creating layer data_hdf5
I0520 13:17:36.130036 12390 net.cpp:106] Creating Layer data_hdf5
I0520 13:17:36.130048 12390 net.cpp:411] data_hdf5 -> data
I0520 13:17:36.130064 12390 net.cpp:411] data_hdf5 -> label
I0520 13:17:36.130080 12390 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 13:17:36.149190 12390 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 13:17:57.553493 12390 net.cpp:150] Setting up data_hdf5
I0520 13:17:57.553658 12390 net.cpp:157] Top shape: 70 1 127 50 (444500)
I0520 13:17:57.553671 12390 net.cpp:157] Top shape: 70 (70)
I0520 13:17:57.553683 12390 net.cpp:165] Memory required for data: 1778280
I0520 13:17:57.553696 12390 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 13:17:57.553725 12390 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 13:17:57.553735 12390 net.cpp:454] label_data_hdf5_1_split <- label
I0520 13:17:57.553751 12390 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 13:17:57.553772 12390 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 13:17:57.553844 12390 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 13:17:57.553858 12390 net.cpp:157] Top shape: 70 (70)
I0520 13:17:57.553870 12390 net.cpp:157] Top shape: 70 (70)
I0520 13:17:57.553879 12390 net.cpp:165] Memory required for data: 1778840
I0520 13:17:57.553891 12390 layer_factory.hpp:77] Creating layer conv1
I0520 13:17:57.553911 12390 net.cpp:106] Creating Layer conv1
I0520 13:17:57.553922 12390 net.cpp:454] conv1 <- data
I0520 13:17:57.553936 12390 net.cpp:411] conv1 -> conv1
I0520 13:17:57.555865 12390 net.cpp:150] Setting up conv1
I0520 13:17:57.555888 12390 net.cpp:157] Top shape: 70 12 120 48 (4838400)
I0520 13:17:57.555902 12390 net.cpp:165] Memory required for data: 21132440
I0520 13:17:57.555922 12390 layer_factory.hpp:77] Creating layer relu1
I0520 13:17:57.555937 12390 net.cpp:106] Creating Layer relu1
I0520 13:17:57.555946 12390 net.cpp:454] relu1 <- conv1
I0520 13:17:57.555959 12390 net.cpp:397] relu1 -> conv1 (in-place)
I0520 13:17:57.556458 12390 net.cpp:150] Setting up relu1
I0520 13:17:57.556474 12390 net.cpp:157] Top shape: 70 12 120 48 (4838400)
I0520 13:17:57.556485 12390 net.cpp:165] Memory required for data: 40486040
I0520 13:17:57.556495 12390 layer_factory.hpp:77] Creating layer pool1
I0520 13:17:57.556512 12390 net.cpp:106] Creating Layer pool1
I0520 13:17:57.556521 12390 net.cpp:454] pool1 <- conv1
I0520 13:17:57.556535 12390 net.cpp:411] pool1 -> pool1
I0520 13:17:57.556609 12390 net.cpp:150] Setting up pool1
I0520 13:17:57.556623 12390 net.cpp:157] Top shape: 70 12 60 48 (2419200)
I0520 13:17:57.556632 12390 net.cpp:165] Memory required for data: 50162840
I0520 13:17:57.556643 12390 layer_factory.hpp:77] Creating layer conv2
I0520 13:17:57.556660 12390 net.cpp:106] Creating Layer conv2
I0520 13:17:57.556670 12390 net.cpp:454] conv2 <- pool1
I0520 13:17:57.556684 12390 net.cpp:411] conv2 -> conv2
I0520 13:17:57.558593 12390 net.cpp:150] Setting up conv2
I0520 13:17:57.558614 12390 net.cpp:157] Top shape: 70 20 54 46 (3477600)
I0520 13:17:57.558627 12390 net.cpp:165] Memory required for data: 64073240
I0520 13:17:57.558645 12390 layer_factory.hpp:77] Creating layer relu2
I0520 13:17:57.558658 12390 net.cpp:106] Creating Layer relu2
I0520 13:17:57.558668 12390 net.cpp:454] relu2 <- conv2
I0520 13:17:57.558681 12390 net.cpp:397] relu2 -> conv2 (in-place)
I0520 13:17:57.559012 12390 net.cpp:150] Setting up relu2
I0520 13:17:57.559026 12390 net.cpp:157] Top shape: 70 20 54 46 (3477600)
I0520 13:17:57.559036 12390 net.cpp:165] Memory required for data: 77983640
I0520 13:17:57.559047 12390 layer_factory.hpp:77] Creating layer pool2
I0520 13:17:57.559061 12390 net.cpp:106] Creating Layer pool2
I0520 13:17:57.559070 12390 net.cpp:454] pool2 <- conv2
I0520 13:17:57.559082 12390 net.cpp:411] pool2 -> pool2
I0520 13:17:57.559154 12390 net.cpp:150] Setting up pool2
I0520 13:17:57.559168 12390 net.cpp:157] Top shape: 70 20 27 46 (1738800)
I0520 13:17:57.559178 12390 net.cpp:165] Memory required for data: 84938840
I0520 13:17:57.559188 12390 layer_factory.hpp:77] Creating layer conv3
I0520 13:17:57.559206 12390 net.cpp:106] Creating Layer conv3
I0520 13:17:57.559216 12390 net.cpp:454] conv3 <- pool2
I0520 13:17:57.559231 12390 net.cpp:411] conv3 -> conv3
I0520 13:17:57.561209 12390 net.cpp:150] Setting up conv3
I0520 13:17:57.561233 12390 net.cpp:157] Top shape: 70 28 22 44 (1897280)
I0520 13:17:57.561242 12390 net.cpp:165] Memory required for data: 92527960
I0520 13:17:57.561275 12390 layer_factory.hpp:77] Creating layer relu3
I0520 13:17:57.561288 12390 net.cpp:106] Creating Layer relu3
I0520 13:17:57.561298 12390 net.cpp:454] relu3 <- conv3
I0520 13:17:57.561311 12390 net.cpp:397] relu3 -> conv3 (in-place)
I0520 13:17:57.561784 12390 net.cpp:150] Setting up relu3
I0520 13:17:57.561800 12390 net.cpp:157] Top shape: 70 28 22 44 (1897280)
I0520 13:17:57.561810 12390 net.cpp:165] Memory required for data: 100117080
I0520 13:17:57.561820 12390 layer_factory.hpp:77] Creating layer pool3
I0520 13:17:57.561833 12390 net.cpp:106] Creating Layer pool3
I0520 13:17:57.561843 12390 net.cpp:454] pool3 <- conv3
I0520 13:17:57.561856 12390 net.cpp:411] pool3 -> pool3
I0520 13:17:57.561928 12390 net.cpp:150] Setting up pool3
I0520 13:17:57.561942 12390 net.cpp:157] Top shape: 70 28 11 44 (948640)
I0520 13:17:57.561951 12390 net.cpp:165] Memory required for data: 103911640
I0520 13:17:57.561962 12390 layer_factory.hpp:77] Creating layer conv4
I0520 13:17:57.561978 12390 net.cpp:106] Creating Layer conv4
I0520 13:17:57.561988 12390 net.cpp:454] conv4 <- pool3
I0520 13:17:57.562003 12390 net.cpp:411] conv4 -> conv4
I0520 13:17:57.564069 12390 net.cpp:150] Setting up conv4
I0520 13:17:57.564090 12390 net.cpp:157] Top shape: 70 36 6 42 (635040)
I0520 13:17:57.564105 12390 net.cpp:165] Memory required for data: 106451800
I0520 13:17:57.564119 12390 layer_factory.hpp:77] Creating layer relu4
I0520 13:17:57.564133 12390 net.cpp:106] Creating Layer relu4
I0520 13:17:57.564143 12390 net.cpp:454] relu4 <- conv4
I0520 13:17:57.564155 12390 net.cpp:397] relu4 -> conv4 (in-place)
I0520 13:17:57.564630 12390 net.cpp:150] Setting up relu4
I0520 13:17:57.564646 12390 net.cpp:157] Top shape: 70 36 6 42 (635040)
I0520 13:17:57.564656 12390 net.cpp:165] Memory required for data: 108991960
I0520 13:17:57.564666 12390 layer_factory.hpp:77] Creating layer pool4
I0520 13:17:57.564679 12390 net.cpp:106] Creating Layer pool4
I0520 13:17:57.564689 12390 net.cpp:454] pool4 <- conv4
I0520 13:17:57.564703 12390 net.cpp:411] pool4 -> pool4
I0520 13:17:57.564774 12390 net.cpp:150] Setting up pool4
I0520 13:17:57.564787 12390 net.cpp:157] Top shape: 70 36 3 42 (317520)
I0520 13:17:57.564797 12390 net.cpp:165] Memory required for data: 110262040
I0520 13:17:57.564807 12390 layer_factory.hpp:77] Creating layer ip1
I0520 13:17:57.564823 12390 net.cpp:106] Creating Layer ip1
I0520 13:17:57.564834 12390 net.cpp:454] ip1 <- pool4
I0520 13:17:57.564846 12390 net.cpp:411] ip1 -> ip1
I0520 13:17:57.580387 12390 net.cpp:150] Setting up ip1
I0520 13:17:57.580415 12390 net.cpp:157] Top shape: 70 196 (13720)
I0520 13:17:57.580433 12390 net.cpp:165] Memory required for data: 110316920
I0520 13:17:57.580454 12390 layer_factory.hpp:77] Creating layer relu5
I0520 13:17:57.580471 12390 net.cpp:106] Creating Layer relu5
I0520 13:17:57.580481 12390 net.cpp:454] relu5 <- ip1
I0520 13:17:57.580494 12390 net.cpp:397] relu5 -> ip1 (in-place)
I0520 13:17:57.580839 12390 net.cpp:150] Setting up relu5
I0520 13:17:57.580853 12390 net.cpp:157] Top shape: 70 196 (13720)
I0520 13:17:57.580863 12390 net.cpp:165] Memory required for data: 110371800
I0520 13:17:57.580873 12390 layer_factory.hpp:77] Creating layer drop1
I0520 13:17:57.580893 12390 net.cpp:106] Creating Layer drop1
I0520 13:17:57.580902 12390 net.cpp:454] drop1 <- ip1
I0520 13:17:57.580915 12390 net.cpp:397] drop1 -> ip1 (in-place)
I0520 13:17:57.580963 12390 net.cpp:150] Setting up drop1
I0520 13:17:57.580976 12390 net.cpp:157] Top shape: 70 196 (13720)
I0520 13:17:57.580986 12390 net.cpp:165] Memory required for data: 110426680
I0520 13:17:57.580996 12390 layer_factory.hpp:77] Creating layer ip2
I0520 13:17:57.581012 12390 net.cpp:106] Creating Layer ip2
I0520 13:17:57.581022 12390 net.cpp:454] ip2 <- ip1
I0520 13:17:57.581034 12390 net.cpp:411] ip2 -> ip2
I0520 13:17:57.581513 12390 net.cpp:150] Setting up ip2
I0520 13:17:57.581526 12390 net.cpp:157] Top shape: 70 98 (6860)
I0520 13:17:57.581537 12390 net.cpp:165] Memory required for data: 110454120
I0520 13:17:57.581552 12390 layer_factory.hpp:77] Creating layer relu6
I0520 13:17:57.581578 12390 net.cpp:106] Creating Layer relu6
I0520 13:17:57.581586 12390 net.cpp:454] relu6 <- ip2
I0520 13:17:57.581599 12390 net.cpp:397] relu6 -> ip2 (in-place)
I0520 13:17:57.582129 12390 net.cpp:150] Setting up relu6
I0520 13:17:57.582145 12390 net.cpp:157] Top shape: 70 98 (6860)
I0520 13:17:57.582159 12390 net.cpp:165] Memory required for data: 110481560
I0520 13:17:57.582170 12390 layer_factory.hpp:77] Creating layer drop2
I0520 13:17:57.582182 12390 net.cpp:106] Creating Layer drop2
I0520 13:17:57.582192 12390 net.cpp:454] drop2 <- ip2
I0520 13:17:57.582206 12390 net.cpp:397] drop2 -> ip2 (in-place)
I0520 13:17:57.582250 12390 net.cpp:150] Setting up drop2
I0520 13:17:57.582263 12390 net.cpp:157] Top shape: 70 98 (6860)
I0520 13:17:57.582273 12390 net.cpp:165] Memory required for data: 110509000
I0520 13:17:57.582283 12390 layer_factory.hpp:77] Creating layer ip3
I0520 13:17:57.582298 12390 net.cpp:106] Creating Layer ip3
I0520 13:17:57.582307 12390 net.cpp:454] ip3 <- ip2
I0520 13:17:57.582321 12390 net.cpp:411] ip3 -> ip3
I0520 13:17:57.582545 12390 net.cpp:150] Setting up ip3
I0520 13:17:57.582557 12390 net.cpp:157] Top shape: 70 11 (770)
I0520 13:17:57.582567 12390 net.cpp:165] Memory required for data: 110512080
I0520 13:17:57.582582 12390 layer_factory.hpp:77] Creating layer drop3
I0520 13:17:57.582597 12390 net.cpp:106] Creating Layer drop3
I0520 13:17:57.582605 12390 net.cpp:454] drop3 <- ip3
I0520 13:17:57.582618 12390 net.cpp:397] drop3 -> ip3 (in-place)
I0520 13:17:57.582660 12390 net.cpp:150] Setting up drop3
I0520 13:17:57.582674 12390 net.cpp:157] Top shape: 70 11 (770)
I0520 13:17:57.582682 12390 net.cpp:165] Memory required for data: 110515160
I0520 13:17:57.582692 12390 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 13:17:57.582705 12390 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 13:17:57.582715 12390 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 13:17:57.582728 12390 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 13:17:57.582743 12390 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 13:17:57.582818 12390 net.cpp:150] Setting up ip3_drop3_0_split
I0520 13:17:57.582830 12390 net.cpp:157] Top shape: 70 11 (770)
I0520 13:17:57.582844 12390 net.cpp:157] Top shape: 70 11 (770)
I0520 13:17:57.582854 12390 net.cpp:165] Memory required for data: 110521320
I0520 13:17:57.582864 12390 layer_factory.hpp:77] Creating layer accuracy
I0520 13:17:57.582885 12390 net.cpp:106] Creating Layer accuracy
I0520 13:17:57.582895 12390 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 13:17:57.582906 12390 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 13:17:57.582921 12390 net.cpp:411] accuracy -> accuracy
I0520 13:17:57.582943 12390 net.cpp:150] Setting up accuracy
I0520 13:17:57.582955 12390 net.cpp:157] Top shape: (1)
I0520 13:17:57.582965 12390 net.cpp:165] Memory required for data: 110521324
I0520 13:17:57.582975 12390 layer_factory.hpp:77] Creating layer loss
I0520 13:17:57.582988 12390 net.cpp:106] Creating Layer loss
I0520 13:17:57.582998 12390 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 13:17:57.583009 12390 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 13:17:57.583022 12390 net.cpp:411] loss -> loss
I0520 13:17:57.583039 12390 layer_factory.hpp:77] Creating layer loss
I0520 13:17:57.583536 12390 net.cpp:150] Setting up loss
I0520 13:17:57.583550 12390 net.cpp:157] Top shape: (1)
I0520 13:17:57.583559 12390 net.cpp:160]     with loss weight 1
I0520 13:17:57.583578 12390 net.cpp:165] Memory required for data: 110521328
I0520 13:17:57.583588 12390 net.cpp:226] loss needs backward computation.
I0520 13:17:57.583600 12390 net.cpp:228] accuracy does not need backward computation.
I0520 13:17:57.583611 12390 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 13:17:57.583621 12390 net.cpp:226] drop3 needs backward computation.
I0520 13:17:57.583631 12390 net.cpp:226] ip3 needs backward computation.
I0520 13:17:57.583642 12390 net.cpp:226] drop2 needs backward computation.
I0520 13:17:57.583652 12390 net.cpp:226] relu6 needs backward computation.
I0520 13:17:57.583670 12390 net.cpp:226] ip2 needs backward computation.
I0520 13:17:57.583681 12390 net.cpp:226] drop1 needs backward computation.
I0520 13:17:57.583690 12390 net.cpp:226] relu5 needs backward computation.
I0520 13:17:57.583700 12390 net.cpp:226] ip1 needs backward computation.
I0520 13:17:57.583710 12390 net.cpp:226] pool4 needs backward computation.
I0520 13:17:57.583720 12390 net.cpp:226] relu4 needs backward computation.
I0520 13:17:57.583730 12390 net.cpp:226] conv4 needs backward computation.
I0520 13:17:57.583739 12390 net.cpp:226] pool3 needs backward computation.
I0520 13:17:57.583750 12390 net.cpp:226] relu3 needs backward computation.
I0520 13:17:57.583760 12390 net.cpp:226] conv3 needs backward computation.
I0520 13:17:57.583771 12390 net.cpp:226] pool2 needs backward computation.
I0520 13:17:57.583781 12390 net.cpp:226] relu2 needs backward computation.
I0520 13:17:57.583791 12390 net.cpp:226] conv2 needs backward computation.
I0520 13:17:57.583801 12390 net.cpp:226] pool1 needs backward computation.
I0520 13:17:57.583811 12390 net.cpp:226] relu1 needs backward computation.
I0520 13:17:57.583822 12390 net.cpp:226] conv1 needs backward computation.
I0520 13:17:57.583832 12390 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 13:17:57.583844 12390 net.cpp:228] data_hdf5 does not need backward computation.
I0520 13:17:57.583854 12390 net.cpp:270] This network produces output accuracy
I0520 13:17:57.583865 12390 net.cpp:270] This network produces output loss
I0520 13:17:57.583895 12390 net.cpp:283] Network initialization done.
I0520 13:17:57.584028 12390 solver.cpp:60] Solver scaffolding done.
I0520 13:17:57.585176 12390 caffe.cpp:212] Starting Optimization
I0520 13:17:57.585194 12390 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 13:17:57.585204 12390 solver.cpp:289] Learning Rate Policy: fixed
I0520 13:17:57.586423 12390 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 13:18:46.401695 12390 solver.cpp:409]     Test net output #0: accuracy = 0.118274
I0520 13:18:46.401865 12390 solver.cpp:409]     Test net output #1: loss = 2.39871 (* 1 = 2.39871 loss)
I0520 13:18:46.429863 12390 solver.cpp:237] Iteration 0, loss = 2.39672
I0520 13:18:46.429900 12390 solver.cpp:253]     Train net output #0: loss = 2.39672 (* 1 = 2.39672 loss)
I0520 13:18:46.429919 12390 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 13:18:55.301765 12390 solver.cpp:237] Iteration 214, loss = 2.27256
I0520 13:18:55.301800 12390 solver.cpp:253]     Train net output #0: loss = 2.27256 (* 1 = 2.27256 loss)
I0520 13:18:55.301815 12390 sgd_solver.cpp:106] Iteration 214, lr = 0.0025
I0520 13:19:04.172592 12390 solver.cpp:237] Iteration 428, loss = 2.01621
I0520 13:19:04.172626 12390 solver.cpp:253]     Train net output #0: loss = 2.01621 (* 1 = 2.01621 loss)
I0520 13:19:04.172643 12390 sgd_solver.cpp:106] Iteration 428, lr = 0.0025
I0520 13:19:13.043488 12390 solver.cpp:237] Iteration 642, loss = 2.28553
I0520 13:19:13.043532 12390 solver.cpp:253]     Train net output #0: loss = 2.28553 (* 1 = 2.28553 loss)
I0520 13:19:13.043546 12390 sgd_solver.cpp:106] Iteration 642, lr = 0.0025
I0520 13:19:21.919827 12390 solver.cpp:237] Iteration 856, loss = 1.79888
I0520 13:19:21.919973 12390 solver.cpp:253]     Train net output #0: loss = 1.79888 (* 1 = 1.79888 loss)
I0520 13:19:21.919987 12390 sgd_solver.cpp:106] Iteration 856, lr = 0.0025
I0520 13:19:30.786921 12390 solver.cpp:237] Iteration 1070, loss = 1.88427
I0520 13:19:30.786954 12390 solver.cpp:253]     Train net output #0: loss = 1.88427 (* 1 = 1.88427 loss)
I0520 13:19:30.786972 12390 sgd_solver.cpp:106] Iteration 1070, lr = 0.0025
I0520 13:19:39.661154 12390 solver.cpp:237] Iteration 1284, loss = 2.01896
I0520 13:19:39.661195 12390 solver.cpp:253]     Train net output #0: loss = 2.01896 (* 1 = 2.01896 loss)
I0520 13:19:39.661208 12390 sgd_solver.cpp:106] Iteration 1284, lr = 0.0025
I0520 13:20:10.747808 12390 solver.cpp:237] Iteration 1498, loss = 1.71993
I0520 13:20:10.747972 12390 solver.cpp:253]     Train net output #0: loss = 1.71993 (* 1 = 1.71993 loss)
I0520 13:20:10.747987 12390 sgd_solver.cpp:106] Iteration 1498, lr = 0.0025
I0520 13:20:19.623373 12390 solver.cpp:237] Iteration 1712, loss = 1.53649
I0520 13:20:19.623409 12390 solver.cpp:253]     Train net output #0: loss = 1.53649 (* 1 = 1.53649 loss)
I0520 13:20:19.623425 12390 sgd_solver.cpp:106] Iteration 1712, lr = 0.0025
I0520 13:20:28.502907 12390 solver.cpp:237] Iteration 1926, loss = 1.79351
I0520 13:20:28.502951 12390 solver.cpp:253]     Train net output #0: loss = 1.79351 (* 1 = 1.79351 loss)
I0520 13:20:28.502964 12390 sgd_solver.cpp:106] Iteration 1926, lr = 0.0025
I0520 13:20:37.375015 12390 solver.cpp:237] Iteration 2140, loss = 1.70524
I0520 13:20:37.375049 12390 solver.cpp:253]     Train net output #0: loss = 1.70524 (* 1 = 1.70524 loss)
I0520 13:20:37.375066 12390 sgd_solver.cpp:106] Iteration 2140, lr = 0.0025
I0520 13:20:37.417352 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_2142.caffemodel
I0520 13:20:37.487463 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_2142.solverstate
I0520 13:20:46.316478 12390 solver.cpp:237] Iteration 2354, loss = 1.89103
I0520 13:20:46.316645 12390 solver.cpp:253]     Train net output #0: loss = 1.89103 (* 1 = 1.89103 loss)
I0520 13:20:46.316659 12390 sgd_solver.cpp:106] Iteration 2354, lr = 0.0025
I0520 13:20:55.192875 12390 solver.cpp:237] Iteration 2568, loss = 1.58594
I0520 13:20:55.192910 12390 solver.cpp:253]     Train net output #0: loss = 1.58594 (* 1 = 1.58594 loss)
I0520 13:20:55.192931 12390 sgd_solver.cpp:106] Iteration 2568, lr = 0.0025
I0520 13:21:04.067873 12390 solver.cpp:237] Iteration 2782, loss = 1.54263
I0520 13:21:04.067909 12390 solver.cpp:253]     Train net output #0: loss = 1.54263 (* 1 = 1.54263 loss)
I0520 13:21:04.067925 12390 sgd_solver.cpp:106] Iteration 2782, lr = 0.0025
I0520 13:21:35.082542 12390 solver.cpp:237] Iteration 2996, loss = 1.81205
I0520 13:21:35.082698 12390 solver.cpp:253]     Train net output #0: loss = 1.81205 (* 1 = 1.81205 loss)
I0520 13:21:35.082713 12390 sgd_solver.cpp:106] Iteration 2996, lr = 0.0025
I0520 13:21:43.958729 12390 solver.cpp:237] Iteration 3210, loss = 1.68216
I0520 13:21:43.958766 12390 solver.cpp:253]     Train net output #0: loss = 1.68216 (* 1 = 1.68216 loss)
I0520 13:21:43.958787 12390 sgd_solver.cpp:106] Iteration 3210, lr = 0.0025
I0520 13:21:52.833853 12390 solver.cpp:237] Iteration 3424, loss = 1.33672
I0520 13:21:52.833889 12390 solver.cpp:253]     Train net output #0: loss = 1.33672 (* 1 = 1.33672 loss)
I0520 13:21:52.833905 12390 sgd_solver.cpp:106] Iteration 3424, lr = 0.0025
I0520 13:22:01.708370 12390 solver.cpp:237] Iteration 3638, loss = 1.67273
I0520 13:22:01.708405 12390 solver.cpp:253]     Train net output #0: loss = 1.67273 (* 1 = 1.67273 loss)
I0520 13:22:01.708422 12390 sgd_solver.cpp:106] Iteration 3638, lr = 0.0025
I0520 13:22:10.582320 12390 solver.cpp:237] Iteration 3852, loss = 1.36698
I0520 13:22:10.582484 12390 solver.cpp:253]     Train net output #0: loss = 1.36698 (* 1 = 1.36698 loss)
I0520 13:22:10.582499 12390 sgd_solver.cpp:106] Iteration 3852, lr = 0.0025
I0520 13:22:19.466150 12390 solver.cpp:237] Iteration 4066, loss = 1.60857
I0520 13:22:19.466184 12390 solver.cpp:253]     Train net output #0: loss = 1.60857 (* 1 = 1.60857 loss)
I0520 13:22:19.466202 12390 sgd_solver.cpp:106] Iteration 4066, lr = 0.0025
I0520 13:22:28.340266 12390 solver.cpp:237] Iteration 4280, loss = 1.4856
I0520 13:22:28.340314 12390 solver.cpp:253]     Train net output #0: loss = 1.4856 (* 1 = 1.4856 loss)
I0520 13:22:28.340329 12390 sgd_solver.cpp:106] Iteration 4280, lr = 0.0025
I0520 13:22:28.465097 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_4284.caffemodel
I0520 13:22:28.531975 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_4284.solverstate
I0520 13:22:28.571409 12390 solver.cpp:341] Iteration 4285, Testing net (#0)
I0520 13:23:16.472443 12390 solver.cpp:409]     Test net output #0: accuracy = 0.757398
I0520 13:23:16.472602 12390 solver.cpp:409]     Test net output #1: loss = 0.81889 (* 1 = 0.81889 loss)
I0520 13:23:47.341992 12390 solver.cpp:237] Iteration 4494, loss = 1.42955
I0520 13:23:47.342150 12390 solver.cpp:253]     Train net output #0: loss = 1.42955 (* 1 = 1.42955 loss)
I0520 13:23:47.342166 12390 sgd_solver.cpp:106] Iteration 4494, lr = 0.0025
I0520 13:23:56.202049 12390 solver.cpp:237] Iteration 4708, loss = 1.36921
I0520 13:23:56.202083 12390 solver.cpp:253]     Train net output #0: loss = 1.36921 (* 1 = 1.36921 loss)
I0520 13:23:56.202100 12390 sgd_solver.cpp:106] Iteration 4708, lr = 0.0025
I0520 13:24:05.062297 12390 solver.cpp:237] Iteration 4922, loss = 1.50292
I0520 13:24:05.062331 12390 solver.cpp:253]     Train net output #0: loss = 1.50292 (* 1 = 1.50292 loss)
I0520 13:24:05.062350 12390 sgd_solver.cpp:106] Iteration 4922, lr = 0.0025
I0520 13:24:13.927233 12390 solver.cpp:237] Iteration 5136, loss = 1.30104
I0520 13:24:13.927279 12390 solver.cpp:253]     Train net output #0: loss = 1.30104 (* 1 = 1.30104 loss)
I0520 13:24:13.927295 12390 sgd_solver.cpp:106] Iteration 5136, lr = 0.0025
I0520 13:24:22.789695 12390 solver.cpp:237] Iteration 5350, loss = 1.34714
I0520 13:24:22.789834 12390 solver.cpp:253]     Train net output #0: loss = 1.34714 (* 1 = 1.34714 loss)
I0520 13:24:22.789847 12390 sgd_solver.cpp:106] Iteration 5350, lr = 0.0025
I0520 13:24:31.654463 12390 solver.cpp:237] Iteration 5564, loss = 1.53694
I0520 13:24:31.654496 12390 solver.cpp:253]     Train net output #0: loss = 1.53694 (* 1 = 1.53694 loss)
I0520 13:24:31.654513 12390 sgd_solver.cpp:106] Iteration 5564, lr = 0.0025
I0520 13:25:02.755352 12390 solver.cpp:237] Iteration 5778, loss = 1.31947
I0520 13:25:02.755517 12390 solver.cpp:253]     Train net output #0: loss = 1.31947 (* 1 = 1.31947 loss)
I0520 13:25:02.755532 12390 sgd_solver.cpp:106] Iteration 5778, lr = 0.0025
I0520 13:25:11.620261 12390 solver.cpp:237] Iteration 5992, loss = 1.20238
I0520 13:25:11.620297 12390 solver.cpp:253]     Train net output #0: loss = 1.20238 (* 1 = 1.20238 loss)
I0520 13:25:11.620314 12390 sgd_solver.cpp:106] Iteration 5992, lr = 0.0025
I0520 13:25:20.482708 12390 solver.cpp:237] Iteration 6206, loss = 1.46872
I0520 13:25:20.482744 12390 solver.cpp:253]     Train net output #0: loss = 1.46872 (* 1 = 1.46872 loss)
I0520 13:25:20.482758 12390 sgd_solver.cpp:106] Iteration 6206, lr = 0.0025
I0520 13:25:29.344519 12390 solver.cpp:237] Iteration 6420, loss = 1.44369
I0520 13:25:29.344559 12390 solver.cpp:253]     Train net output #0: loss = 1.44369 (* 1 = 1.44369 loss)
I0520 13:25:29.344576 12390 sgd_solver.cpp:106] Iteration 6420, lr = 0.0025
I0520 13:25:29.551515 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_6426.caffemodel
I0520 13:25:29.618957 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_6426.solverstate
I0520 13:25:38.279506 12390 solver.cpp:237] Iteration 6634, loss = 1.40599
I0520 13:25:38.279675 12390 solver.cpp:253]     Train net output #0: loss = 1.40599 (* 1 = 1.40599 loss)
I0520 13:25:38.279690 12390 sgd_solver.cpp:106] Iteration 6634, lr = 0.0025
I0520 13:25:47.139505 12390 solver.cpp:237] Iteration 6848, loss = 1.20743
I0520 13:25:47.139556 12390 solver.cpp:253]     Train net output #0: loss = 1.20743 (* 1 = 1.20743 loss)
I0520 13:25:47.139570 12390 sgd_solver.cpp:106] Iteration 6848, lr = 0.0025
I0520 13:25:56.007138 12390 solver.cpp:237] Iteration 7062, loss = 1.50744
I0520 13:25:56.007174 12390 solver.cpp:253]     Train net output #0: loss = 1.50744 (* 1 = 1.50744 loss)
I0520 13:25:56.007191 12390 sgd_solver.cpp:106] Iteration 7062, lr = 0.0025
I0520 13:26:27.147864 12390 solver.cpp:237] Iteration 7276, loss = 1.52075
I0520 13:26:27.148030 12390 solver.cpp:253]     Train net output #0: loss = 1.52075 (* 1 = 1.52075 loss)
I0520 13:26:27.148046 12390 sgd_solver.cpp:106] Iteration 7276, lr = 0.0025
I0520 13:26:36.012609 12390 solver.cpp:237] Iteration 7490, loss = 1.41573
I0520 13:26:36.012645 12390 solver.cpp:253]     Train net output #0: loss = 1.41573 (* 1 = 1.41573 loss)
I0520 13:26:36.012663 12390 sgd_solver.cpp:106] Iteration 7490, lr = 0.0025
I0520 13:26:44.878120 12390 solver.cpp:237] Iteration 7704, loss = 1.56548
I0520 13:26:44.878166 12390 solver.cpp:253]     Train net output #0: loss = 1.56548 (* 1 = 1.56548 loss)
I0520 13:26:44.878180 12390 sgd_solver.cpp:106] Iteration 7704, lr = 0.0025
I0520 13:26:53.740762 12390 solver.cpp:237] Iteration 7918, loss = 1.61509
I0520 13:26:53.740798 12390 solver.cpp:253]     Train net output #0: loss = 1.61509 (* 1 = 1.61509 loss)
I0520 13:26:53.740814 12390 sgd_solver.cpp:106] Iteration 7918, lr = 0.0025
I0520 13:27:02.603068 12390 solver.cpp:237] Iteration 8132, loss = 1.21016
I0520 13:27:02.603214 12390 solver.cpp:253]     Train net output #0: loss = 1.21016 (* 1 = 1.21016 loss)
I0520 13:27:02.603229 12390 sgd_solver.cpp:106] Iteration 8132, lr = 0.0025
I0520 13:27:11.463420 12390 solver.cpp:237] Iteration 8346, loss = 1.16366
I0520 13:27:11.463455 12390 solver.cpp:253]     Train net output #0: loss = 1.16366 (* 1 = 1.16366 loss)
I0520 13:27:11.463472 12390 sgd_solver.cpp:106] Iteration 8346, lr = 0.0025
I0520 13:27:20.321919 12390 solver.cpp:237] Iteration 8560, loss = 1.41508
I0520 13:27:20.321955 12390 solver.cpp:253]     Train net output #0: loss = 1.41508 (* 1 = 1.41508 loss)
I0520 13:27:20.321971 12390 sgd_solver.cpp:106] Iteration 8560, lr = 0.0025
I0520 13:27:20.612787 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_8568.caffemodel
I0520 13:27:20.681566 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_8568.solverstate
I0520 13:27:20.764938 12390 solver.cpp:341] Iteration 8570, Testing net (#0)
I0520 13:28:29.552500 12390 solver.cpp:409]     Test net output #0: accuracy = 0.818252
I0520 13:28:29.552670 12390 solver.cpp:409]     Test net output #1: loss = 0.666278 (* 1 = 0.666278 loss)
I0520 13:29:00.214900 12390 solver.cpp:237] Iteration 8774, loss = 1.49994
I0520 13:29:00.215075 12390 solver.cpp:253]     Train net output #0: loss = 1.49994 (* 1 = 1.49994 loss)
I0520 13:29:00.215091 12390 sgd_solver.cpp:106] Iteration 8774, lr = 0.0025
I0520 13:29:09.100289 12390 solver.cpp:237] Iteration 8988, loss = 1.43344
I0520 13:29:09.100334 12390 solver.cpp:253]     Train net output #0: loss = 1.43344 (* 1 = 1.43344 loss)
I0520 13:29:09.100352 12390 sgd_solver.cpp:106] Iteration 8988, lr = 0.0025
I0520 13:29:17.988646 12390 solver.cpp:237] Iteration 9202, loss = 1.04558
I0520 13:29:17.988682 12390 solver.cpp:253]     Train net output #0: loss = 1.04558 (* 1 = 1.04558 loss)
I0520 13:29:17.988698 12390 sgd_solver.cpp:106] Iteration 9202, lr = 0.0025
I0520 13:29:26.876900 12390 solver.cpp:237] Iteration 9416, loss = 1.33222
I0520 13:29:26.876936 12390 solver.cpp:253]     Train net output #0: loss = 1.33222 (* 1 = 1.33222 loss)
I0520 13:29:26.876952 12390 sgd_solver.cpp:106] Iteration 9416, lr = 0.0025
I0520 13:29:35.768962 12390 solver.cpp:237] Iteration 9630, loss = 1.30596
I0520 13:29:35.769116 12390 solver.cpp:253]     Train net output #0: loss = 1.30596 (* 1 = 1.30596 loss)
I0520 13:29:35.769130 12390 sgd_solver.cpp:106] Iteration 9630, lr = 0.0025
I0520 13:29:44.665127 12390 solver.cpp:237] Iteration 9844, loss = 1.40502
I0520 13:29:44.665160 12390 solver.cpp:253]     Train net output #0: loss = 1.40502 (* 1 = 1.40502 loss)
I0520 13:29:44.665176 12390 sgd_solver.cpp:106] Iteration 9844, lr = 0.0025
I0520 13:30:15.711082 12390 solver.cpp:237] Iteration 10058, loss = 1.49352
I0520 13:30:15.711242 12390 solver.cpp:253]     Train net output #0: loss = 1.49352 (* 1 = 1.49352 loss)
I0520 13:30:15.711258 12390 sgd_solver.cpp:106] Iteration 10058, lr = 0.0025
I0520 13:30:24.603886 12390 solver.cpp:237] Iteration 10272, loss = 1.49446
I0520 13:30:24.603930 12390 solver.cpp:253]     Train net output #0: loss = 1.49446 (* 1 = 1.49446 loss)
I0520 13:30:24.603947 12390 sgd_solver.cpp:106] Iteration 10272, lr = 0.0025
I0520 13:30:33.494818 12390 solver.cpp:237] Iteration 10486, loss = 1.23735
I0520 13:30:33.494855 12390 solver.cpp:253]     Train net output #0: loss = 1.23735 (* 1 = 1.23735 loss)
I0520 13:30:33.494871 12390 sgd_solver.cpp:106] Iteration 10486, lr = 0.0025
I0520 13:30:42.383265 12390 solver.cpp:237] Iteration 10700, loss = 1.23918
I0520 13:30:42.383301 12390 solver.cpp:253]     Train net output #0: loss = 1.23918 (* 1 = 1.23918 loss)
I0520 13:30:42.383316 12390 sgd_solver.cpp:106] Iteration 10700, lr = 0.0025
I0520 13:30:42.756044 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_10710.caffemodel
I0520 13:30:42.824525 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_10710.solverstate
I0520 13:30:51.336413 12390 solver.cpp:237] Iteration 10914, loss = 1.45917
I0520 13:30:51.336575 12390 solver.cpp:253]     Train net output #0: loss = 1.45917 (* 1 = 1.45917 loss)
I0520 13:30:51.336590 12390 sgd_solver.cpp:106] Iteration 10914, lr = 0.0025
I0520 13:31:00.224366 12390 solver.cpp:237] Iteration 11128, loss = 1.21052
I0520 13:31:00.224400 12390 solver.cpp:253]     Train net output #0: loss = 1.21052 (* 1 = 1.21052 loss)
I0520 13:31:00.224417 12390 sgd_solver.cpp:106] Iteration 11128, lr = 0.0025
I0520 13:31:09.116127 12390 solver.cpp:237] Iteration 11342, loss = 1.23704
I0520 13:31:09.116163 12390 solver.cpp:253]     Train net output #0: loss = 1.23704 (* 1 = 1.23704 loss)
I0520 13:31:09.116179 12390 sgd_solver.cpp:106] Iteration 11342, lr = 0.0025
I0520 13:31:40.167101 12390 solver.cpp:237] Iteration 11556, loss = 1.4216
I0520 13:31:40.167280 12390 solver.cpp:253]     Train net output #0: loss = 1.4216 (* 1 = 1.4216 loss)
I0520 13:31:40.167295 12390 sgd_solver.cpp:106] Iteration 11556, lr = 0.0025
I0520 13:31:49.061906 12390 solver.cpp:237] Iteration 11770, loss = 1.21211
I0520 13:31:49.061942 12390 solver.cpp:253]     Train net output #0: loss = 1.21211 (* 1 = 1.21211 loss)
I0520 13:31:49.061960 12390 sgd_solver.cpp:106] Iteration 11770, lr = 0.0025
I0520 13:31:57.956866 12390 solver.cpp:237] Iteration 11984, loss = 1.35959
I0520 13:31:57.956902 12390 solver.cpp:253]     Train net output #0: loss = 1.35959 (* 1 = 1.35959 loss)
I0520 13:31:57.956918 12390 sgd_solver.cpp:106] Iteration 11984, lr = 0.0025
I0520 13:32:06.849040 12390 solver.cpp:237] Iteration 12198, loss = 1.20926
I0520 13:32:06.849165 12390 solver.cpp:253]     Train net output #0: loss = 1.20926 (* 1 = 1.20926 loss)
I0520 13:32:06.849179 12390 sgd_solver.cpp:106] Iteration 12198, lr = 0.0025
I0520 13:32:15.737921 12390 solver.cpp:237] Iteration 12412, loss = 1.53968
I0520 13:32:15.738065 12390 solver.cpp:253]     Train net output #0: loss = 1.53968 (* 1 = 1.53968 loss)
I0520 13:32:15.738078 12390 sgd_solver.cpp:106] Iteration 12412, lr = 0.0025
I0520 13:32:24.627594 12390 solver.cpp:237] Iteration 12626, loss = 1.42456
I0520 13:32:24.627629 12390 solver.cpp:253]     Train net output #0: loss = 1.42456 (* 1 = 1.42456 loss)
I0520 13:32:24.627645 12390 sgd_solver.cpp:106] Iteration 12626, lr = 0.0025
I0520 13:32:33.514688 12390 solver.cpp:237] Iteration 12840, loss = 1.23786
I0520 13:32:33.514736 12390 solver.cpp:253]     Train net output #0: loss = 1.23786 (* 1 = 1.23786 loss)
I0520 13:32:33.514750 12390 sgd_solver.cpp:106] Iteration 12840, lr = 0.0025
I0520 13:32:33.971501 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_12852.caffemodel
I0520 13:32:34.037513 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_12852.solverstate
I0520 13:32:34.160233 12390 solver.cpp:341] Iteration 12855, Testing net (#0)
I0520 13:33:21.668778 12390 solver.cpp:409]     Test net output #0: accuracy = 0.83622
I0520 13:33:21.668936 12390 solver.cpp:409]     Test net output #1: loss = 0.545661 (* 1 = 0.545661 loss)
I0520 13:33:52.109112 12390 solver.cpp:237] Iteration 13054, loss = 1.34506
I0520 13:33:52.109272 12390 solver.cpp:253]     Train net output #0: loss = 1.34506 (* 1 = 1.34506 loss)
I0520 13:33:52.109287 12390 sgd_solver.cpp:106] Iteration 13054, lr = 0.0025
I0520 13:34:00.980033 12390 solver.cpp:237] Iteration 13268, loss = 1.05203
I0520 13:34:00.980068 12390 solver.cpp:253]     Train net output #0: loss = 1.05203 (* 1 = 1.05203 loss)
I0520 13:34:00.980087 12390 sgd_solver.cpp:106] Iteration 13268, lr = 0.0025
I0520 13:34:09.848953 12390 solver.cpp:237] Iteration 13482, loss = 1.0926
I0520 13:34:09.848991 12390 solver.cpp:253]     Train net output #0: loss = 1.0926 (* 1 = 1.0926 loss)
I0520 13:34:09.849014 12390 sgd_solver.cpp:106] Iteration 13482, lr = 0.0025
I0520 13:34:18.716691 12390 solver.cpp:237] Iteration 13696, loss = 1.27159
I0520 13:34:18.716727 12390 solver.cpp:253]     Train net output #0: loss = 1.27159 (* 1 = 1.27159 loss)
I0520 13:34:18.716743 12390 sgd_solver.cpp:106] Iteration 13696, lr = 0.0025
I0520 13:34:27.585880 12390 solver.cpp:237] Iteration 13910, loss = 1.32872
I0520 13:34:27.586020 12390 solver.cpp:253]     Train net output #0: loss = 1.32872 (* 1 = 1.32872 loss)
I0520 13:34:27.586033 12390 sgd_solver.cpp:106] Iteration 13910, lr = 0.0025
I0520 13:34:36.468998 12390 solver.cpp:237] Iteration 14124, loss = 1.4245
I0520 13:34:36.469041 12390 solver.cpp:253]     Train net output #0: loss = 1.4245 (* 1 = 1.4245 loss)
I0520 13:34:36.469060 12390 sgd_solver.cpp:106] Iteration 14124, lr = 0.0025
I0520 13:35:07.580130 12390 solver.cpp:237] Iteration 14338, loss = 1.39387
I0520 13:35:07.580307 12390 solver.cpp:253]     Train net output #0: loss = 1.39387 (* 1 = 1.39387 loss)
I0520 13:35:07.580323 12390 sgd_solver.cpp:106] Iteration 14338, lr = 0.0025
I0520 13:35:16.452792 12390 solver.cpp:237] Iteration 14552, loss = 1.2403
I0520 13:35:16.452827 12390 solver.cpp:253]     Train net output #0: loss = 1.2403 (* 1 = 1.2403 loss)
I0520 13:35:16.452844 12390 sgd_solver.cpp:106] Iteration 14552, lr = 0.0025
I0520 13:35:25.324095 12390 solver.cpp:237] Iteration 14766, loss = 1.43104
I0520 13:35:25.324141 12390 solver.cpp:253]     Train net output #0: loss = 1.43104 (* 1 = 1.43104 loss)
I0520 13:35:25.324156 12390 sgd_solver.cpp:106] Iteration 14766, lr = 0.0025
I0520 13:35:34.195080 12390 solver.cpp:237] Iteration 14980, loss = 1.03472
I0520 13:35:34.195116 12390 solver.cpp:253]     Train net output #0: loss = 1.03472 (* 1 = 1.03472 loss)
I0520 13:35:34.195132 12390 sgd_solver.cpp:106] Iteration 14980, lr = 0.0025
I0520 13:35:34.734112 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_14994.caffemodel
I0520 13:35:34.800586 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_14994.solverstate
I0520 13:35:43.134160 12390 solver.cpp:237] Iteration 15194, loss = 1.31909
I0520 13:35:43.134320 12390 solver.cpp:253]     Train net output #0: loss = 1.31909 (* 1 = 1.31909 loss)
I0520 13:35:43.134333 12390 sgd_solver.cpp:106] Iteration 15194, lr = 0.0025
I0520 13:35:51.999611 12390 solver.cpp:237] Iteration 15408, loss = 1.29875
I0520 13:35:51.999657 12390 solver.cpp:253]     Train net output #0: loss = 1.29875 (* 1 = 1.29875 loss)
I0520 13:35:51.999673 12390 sgd_solver.cpp:106] Iteration 15408, lr = 0.0025
I0520 13:36:00.861709 12390 solver.cpp:237] Iteration 15622, loss = 1.17117
I0520 13:36:00.861745 12390 solver.cpp:253]     Train net output #0: loss = 1.17117 (* 1 = 1.17117 loss)
I0520 13:36:00.861763 12390 sgd_solver.cpp:106] Iteration 15622, lr = 0.0025
I0520 13:36:31.964035 12390 solver.cpp:237] Iteration 15836, loss = 1.1837
I0520 13:36:31.964203 12390 solver.cpp:253]     Train net output #0: loss = 1.1837 (* 1 = 1.1837 loss)
I0520 13:36:31.964217 12390 sgd_solver.cpp:106] Iteration 15836, lr = 0.0025
I0520 13:36:40.835642 12390 solver.cpp:237] Iteration 16050, loss = 1.4664
I0520 13:36:40.835685 12390 solver.cpp:253]     Train net output #0: loss = 1.4664 (* 1 = 1.4664 loss)
I0520 13:36:40.835705 12390 sgd_solver.cpp:106] Iteration 16050, lr = 0.0025
I0520 13:36:49.701933 12390 solver.cpp:237] Iteration 16264, loss = 1.18698
I0520 13:36:49.701968 12390 solver.cpp:253]     Train net output #0: loss = 1.18698 (* 1 = 1.18698 loss)
I0520 13:36:49.701984 12390 sgd_solver.cpp:106] Iteration 16264, lr = 0.0025
I0520 13:36:58.571530 12390 solver.cpp:237] Iteration 16478, loss = 1.26497
I0520 13:36:58.571560 12390 solver.cpp:253]     Train net output #0: loss = 1.26497 (* 1 = 1.26497 loss)
I0520 13:36:58.571574 12390 sgd_solver.cpp:106] Iteration 16478, lr = 0.0025
I0520 13:37:07.445158 12390 solver.cpp:237] Iteration 16692, loss = 1.21853
I0520 13:37:07.445315 12390 solver.cpp:253]     Train net output #0: loss = 1.21853 (* 1 = 1.21853 loss)
I0520 13:37:07.445329 12390 sgd_solver.cpp:106] Iteration 16692, lr = 0.0025
I0520 13:37:16.317008 12390 solver.cpp:237] Iteration 16906, loss = 1.64382
I0520 13:37:16.317042 12390 solver.cpp:253]     Train net output #0: loss = 1.64382 (* 1 = 1.64382 loss)
I0520 13:37:16.317060 12390 sgd_solver.cpp:106] Iteration 16906, lr = 0.0025
I0520 13:37:25.189760 12390 solver.cpp:237] Iteration 17120, loss = 1.25267
I0520 13:37:25.189795 12390 solver.cpp:253]     Train net output #0: loss = 1.25267 (* 1 = 1.25267 loss)
I0520 13:37:25.189811 12390 sgd_solver.cpp:106] Iteration 17120, lr = 0.0025
I0520 13:37:25.811983 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_17136.caffemodel
I0520 13:37:25.878509 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_17136.solverstate
I0520 13:37:26.042614 12390 solver.cpp:341] Iteration 17140, Testing net (#0)
I0520 13:38:34.744052 12390 solver.cpp:409]     Test net output #0: accuracy = 0.854375
I0520 13:38:34.744228 12390 solver.cpp:409]     Test net output #1: loss = 0.498552 (* 1 = 0.498552 loss)
I0520 13:39:04.950418 12390 solver.cpp:237] Iteration 17334, loss = 1.46983
I0520 13:39:04.950583 12390 solver.cpp:253]     Train net output #0: loss = 1.46983 (* 1 = 1.46983 loss)
I0520 13:39:04.950598 12390 sgd_solver.cpp:106] Iteration 17334, lr = 0.0025
I0520 13:39:13.826477 12390 solver.cpp:237] Iteration 17548, loss = 1.15812
I0520 13:39:13.826519 12390 solver.cpp:253]     Train net output #0: loss = 1.15812 (* 1 = 1.15812 loss)
I0520 13:39:13.826539 12390 sgd_solver.cpp:106] Iteration 17548, lr = 0.0025
I0520 13:39:22.699362 12390 solver.cpp:237] Iteration 17762, loss = 1.24982
I0520 13:39:22.699396 12390 solver.cpp:253]     Train net output #0: loss = 1.24982 (* 1 = 1.24982 loss)
I0520 13:39:22.699412 12390 sgd_solver.cpp:106] Iteration 17762, lr = 0.0025
I0520 13:39:31.571676 12390 solver.cpp:237] Iteration 17976, loss = 1.31666
I0520 13:39:31.571712 12390 solver.cpp:253]     Train net output #0: loss = 1.31666 (* 1 = 1.31666 loss)
I0520 13:39:31.571728 12390 sgd_solver.cpp:106] Iteration 17976, lr = 0.0025
I0520 13:39:40.445637 12390 solver.cpp:237] Iteration 18190, loss = 1.1767
I0520 13:39:40.445793 12390 solver.cpp:253]     Train net output #0: loss = 1.1767 (* 1 = 1.1767 loss)
I0520 13:39:40.445807 12390 sgd_solver.cpp:106] Iteration 18190, lr = 0.0025
I0520 13:39:49.319903 12390 solver.cpp:237] Iteration 18404, loss = 1.34249
I0520 13:39:49.319937 12390 solver.cpp:253]     Train net output #0: loss = 1.34249 (* 1 = 1.34249 loss)
I0520 13:39:49.319954 12390 sgd_solver.cpp:106] Iteration 18404, lr = 0.0025
I0520 13:40:20.451388 12390 solver.cpp:237] Iteration 18618, loss = 0.999906
I0520 13:40:20.451565 12390 solver.cpp:253]     Train net output #0: loss = 0.999906 (* 1 = 0.999906 loss)
I0520 13:40:20.451581 12390 sgd_solver.cpp:106] Iteration 18618, lr = 0.0025
I0520 13:40:29.314414 12390 solver.cpp:237] Iteration 18832, loss = 1.31045
I0520 13:40:29.314450 12390 solver.cpp:253]     Train net output #0: loss = 1.31045 (* 1 = 1.31045 loss)
I0520 13:40:29.314471 12390 sgd_solver.cpp:106] Iteration 18832, lr = 0.0025
I0520 13:40:38.188544 12390 solver.cpp:237] Iteration 19046, loss = 1.0913
I0520 13:40:38.188580 12390 solver.cpp:253]     Train net output #0: loss = 1.0913 (* 1 = 1.0913 loss)
I0520 13:40:38.188596 12390 sgd_solver.cpp:106] Iteration 19046, lr = 0.0025
I0520 13:40:47.056066 12390 solver.cpp:237] Iteration 19260, loss = 1.12512
I0520 13:40:47.056113 12390 solver.cpp:253]     Train net output #0: loss = 1.12512 (* 1 = 1.12512 loss)
I0520 13:40:47.056133 12390 sgd_solver.cpp:106] Iteration 19260, lr = 0.0025
I0520 13:40:47.762513 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_19278.caffemodel
I0520 13:40:47.831379 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_19278.solverstate
I0520 13:40:56.001801 12390 solver.cpp:237] Iteration 19474, loss = 1.34229
I0520 13:40:56.001962 12390 solver.cpp:253]     Train net output #0: loss = 1.34229 (* 1 = 1.34229 loss)
I0520 13:40:56.001977 12390 sgd_solver.cpp:106] Iteration 19474, lr = 0.0025
I0520 13:41:04.866871 12390 solver.cpp:237] Iteration 19688, loss = 0.915771
I0520 13:41:04.866907 12390 solver.cpp:253]     Train net output #0: loss = 0.915771 (* 1 = 0.915771 loss)
I0520 13:41:04.866925 12390 sgd_solver.cpp:106] Iteration 19688, lr = 0.0025
I0520 13:41:13.733191 12390 solver.cpp:237] Iteration 19902, loss = 1.11927
I0520 13:41:13.733232 12390 solver.cpp:253]     Train net output #0: loss = 1.11927 (* 1 = 1.11927 loss)
I0520 13:41:13.733253 12390 sgd_solver.cpp:106] Iteration 19902, lr = 0.0025
I0520 13:41:44.794031 12390 solver.cpp:237] Iteration 20116, loss = 1.01761
I0520 13:41:44.794214 12390 solver.cpp:253]     Train net output #0: loss = 1.01761 (* 1 = 1.01761 loss)
I0520 13:41:44.794230 12390 sgd_solver.cpp:106] Iteration 20116, lr = 0.0025
I0520 13:41:53.669503 12390 solver.cpp:237] Iteration 20330, loss = 1.35016
I0520 13:41:53.669538 12390 solver.cpp:253]     Train net output #0: loss = 1.35016 (* 1 = 1.35016 loss)
I0520 13:41:53.669551 12390 sgd_solver.cpp:106] Iteration 20330, lr = 0.0025
I0520 13:42:02.538246 12390 solver.cpp:237] Iteration 20544, loss = 1.47288
I0520 13:42:02.538293 12390 solver.cpp:253]     Train net output #0: loss = 1.47288 (* 1 = 1.47288 loss)
I0520 13:42:02.538310 12390 sgd_solver.cpp:106] Iteration 20544, lr = 0.0025
I0520 13:42:11.404685 12390 solver.cpp:237] Iteration 20758, loss = 1.11225
I0520 13:42:11.404721 12390 solver.cpp:253]     Train net output #0: loss = 1.11225 (* 1 = 1.11225 loss)
I0520 13:42:11.404737 12390 sgd_solver.cpp:106] Iteration 20758, lr = 0.0025
I0520 13:42:20.275377 12390 solver.cpp:237] Iteration 20972, loss = 1.29623
I0520 13:42:20.275527 12390 solver.cpp:253]     Train net output #0: loss = 1.29623 (* 1 = 1.29623 loss)
I0520 13:42:20.275540 12390 sgd_solver.cpp:106] Iteration 20972, lr = 0.0025
I0520 13:42:29.145954 12390 solver.cpp:237] Iteration 21186, loss = 1.1908
I0520 13:42:29.145998 12390 solver.cpp:253]     Train net output #0: loss = 1.1908 (* 1 = 1.1908 loss)
I0520 13:42:29.146013 12390 sgd_solver.cpp:106] Iteration 21186, lr = 0.0025
I0520 13:42:38.016208 12390 solver.cpp:237] Iteration 21400, loss = 1.33039
I0520 13:42:38.016243 12390 solver.cpp:253]     Train net output #0: loss = 1.33039 (* 1 = 1.33039 loss)
I0520 13:42:38.016261 12390 sgd_solver.cpp:106] Iteration 21400, lr = 0.0025
I0520 13:42:38.804864 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_21420.caffemodel
I0520 13:42:38.872957 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_21420.solverstate
I0520 13:42:39.079411 12390 solver.cpp:341] Iteration 21425, Testing net (#0)
I0520 13:43:26.946563 12390 solver.cpp:409]     Test net output #0: accuracy = 0.860758
I0520 13:43:26.946725 12390 solver.cpp:409]     Test net output #1: loss = 0.537076 (* 1 = 0.537076 loss)
I0520 13:43:27.043397 12390 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_21428.caffemodel
I0520 13:43:27.111683 12390 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777_iter_21428.solverstate
I0520 13:43:27.138203 12390 solver.cpp:326] Optimization Done.
I0520 13:43:27.138231 12390 caffe.cpp:215] Optimization Done.
Application 11232299 resources: utime ~1343s, stime ~233s, Rss ~5329180, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_70_2016-05-20T11.20.35.233777.solver"
	User time (seconds): 0.58
	System time (seconds): 0.13
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 26:19.21
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15112
	Voluntary context switches: 2827
	Involuntary context switches: 129
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
