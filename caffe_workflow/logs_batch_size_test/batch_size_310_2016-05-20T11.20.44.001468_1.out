2805936
I0520 21:05:02.755450 31010 caffe.cpp:184] Using GPUs 0
I0520 21:05:03.183486 31010 solver.cpp:48] Initializing solver from parameters: 
test_iter: 483
test_interval: 967
base_lr: 0.0025
display: 48
max_iter: 4838
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 483
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468.prototxt"
I0520 21:05:03.185333 31010 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468.prototxt
I0520 21:05:03.201935 31010 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 21:05:03.201994 31010 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 21:05:03.202340 31010 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 310
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:05:03.202522 31010 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:05:03.202546 31010 net.cpp:106] Creating Layer data_hdf5
I0520 21:05:03.202561 31010 net.cpp:411] data_hdf5 -> data
I0520 21:05:03.202594 31010 net.cpp:411] data_hdf5 -> label
I0520 21:05:03.202625 31010 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 21:05:03.212779 31010 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 21:05:03.238582 31010 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 21:05:24.876137 31010 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 21:05:24.881285 31010 net.cpp:150] Setting up data_hdf5
I0520 21:05:24.881326 31010 net.cpp:157] Top shape: 310 1 127 50 (1968500)
I0520 21:05:24.881340 31010 net.cpp:157] Top shape: 310 (310)
I0520 21:05:24.881350 31010 net.cpp:165] Memory required for data: 7875240
I0520 21:05:24.881364 31010 layer_factory.hpp:77] Creating layer conv1
I0520 21:05:24.881398 31010 net.cpp:106] Creating Layer conv1
I0520 21:05:24.881409 31010 net.cpp:454] conv1 <- data
I0520 21:05:24.881433 31010 net.cpp:411] conv1 -> conv1
I0520 21:05:26.052783 31010 net.cpp:150] Setting up conv1
I0520 21:05:26.052830 31010 net.cpp:157] Top shape: 310 12 120 48 (21427200)
I0520 21:05:26.052841 31010 net.cpp:165] Memory required for data: 93584040
I0520 21:05:26.052870 31010 layer_factory.hpp:77] Creating layer relu1
I0520 21:05:26.052891 31010 net.cpp:106] Creating Layer relu1
I0520 21:05:26.052901 31010 net.cpp:454] relu1 <- conv1
I0520 21:05:26.052916 31010 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:05:26.053437 31010 net.cpp:150] Setting up relu1
I0520 21:05:26.053454 31010 net.cpp:157] Top shape: 310 12 120 48 (21427200)
I0520 21:05:26.053465 31010 net.cpp:165] Memory required for data: 179292840
I0520 21:05:26.053477 31010 layer_factory.hpp:77] Creating layer pool1
I0520 21:05:26.053493 31010 net.cpp:106] Creating Layer pool1
I0520 21:05:26.053503 31010 net.cpp:454] pool1 <- conv1
I0520 21:05:26.053516 31010 net.cpp:411] pool1 -> pool1
I0520 21:05:26.053597 31010 net.cpp:150] Setting up pool1
I0520 21:05:26.053611 31010 net.cpp:157] Top shape: 310 12 60 48 (10713600)
I0520 21:05:26.053622 31010 net.cpp:165] Memory required for data: 222147240
I0520 21:05:26.053630 31010 layer_factory.hpp:77] Creating layer conv2
I0520 21:05:26.053652 31010 net.cpp:106] Creating Layer conv2
I0520 21:05:26.053663 31010 net.cpp:454] conv2 <- pool1
I0520 21:05:26.053675 31010 net.cpp:411] conv2 -> conv2
I0520 21:05:26.056408 31010 net.cpp:150] Setting up conv2
I0520 21:05:26.056437 31010 net.cpp:157] Top shape: 310 20 54 46 (15400800)
I0520 21:05:26.056447 31010 net.cpp:165] Memory required for data: 283750440
I0520 21:05:26.056466 31010 layer_factory.hpp:77] Creating layer relu2
I0520 21:05:26.056480 31010 net.cpp:106] Creating Layer relu2
I0520 21:05:26.056490 31010 net.cpp:454] relu2 <- conv2
I0520 21:05:26.056504 31010 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:05:26.056833 31010 net.cpp:150] Setting up relu2
I0520 21:05:26.056848 31010 net.cpp:157] Top shape: 310 20 54 46 (15400800)
I0520 21:05:26.056857 31010 net.cpp:165] Memory required for data: 345353640
I0520 21:05:26.056867 31010 layer_factory.hpp:77] Creating layer pool2
I0520 21:05:26.056880 31010 net.cpp:106] Creating Layer pool2
I0520 21:05:26.056890 31010 net.cpp:454] pool2 <- conv2
I0520 21:05:26.056915 31010 net.cpp:411] pool2 -> pool2
I0520 21:05:26.056983 31010 net.cpp:150] Setting up pool2
I0520 21:05:26.056996 31010 net.cpp:157] Top shape: 310 20 27 46 (7700400)
I0520 21:05:26.057006 31010 net.cpp:165] Memory required for data: 376155240
I0520 21:05:26.057015 31010 layer_factory.hpp:77] Creating layer conv3
I0520 21:05:26.057034 31010 net.cpp:106] Creating Layer conv3
I0520 21:05:26.057044 31010 net.cpp:454] conv3 <- pool2
I0520 21:05:26.057057 31010 net.cpp:411] conv3 -> conv3
I0520 21:05:26.058972 31010 net.cpp:150] Setting up conv3
I0520 21:05:26.058995 31010 net.cpp:157] Top shape: 310 28 22 44 (8402240)
I0520 21:05:26.059008 31010 net.cpp:165] Memory required for data: 409764200
I0520 21:05:26.059026 31010 layer_factory.hpp:77] Creating layer relu3
I0520 21:05:26.059042 31010 net.cpp:106] Creating Layer relu3
I0520 21:05:26.059052 31010 net.cpp:454] relu3 <- conv3
I0520 21:05:26.059064 31010 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:05:26.059538 31010 net.cpp:150] Setting up relu3
I0520 21:05:26.059556 31010 net.cpp:157] Top shape: 310 28 22 44 (8402240)
I0520 21:05:26.059566 31010 net.cpp:165] Memory required for data: 443373160
I0520 21:05:26.059576 31010 layer_factory.hpp:77] Creating layer pool3
I0520 21:05:26.059589 31010 net.cpp:106] Creating Layer pool3
I0520 21:05:26.059599 31010 net.cpp:454] pool3 <- conv3
I0520 21:05:26.059612 31010 net.cpp:411] pool3 -> pool3
I0520 21:05:26.059691 31010 net.cpp:150] Setting up pool3
I0520 21:05:26.059705 31010 net.cpp:157] Top shape: 310 28 11 44 (4201120)
I0520 21:05:26.059715 31010 net.cpp:165] Memory required for data: 460177640
I0520 21:05:26.059727 31010 layer_factory.hpp:77] Creating layer conv4
I0520 21:05:26.059744 31010 net.cpp:106] Creating Layer conv4
I0520 21:05:26.059756 31010 net.cpp:454] conv4 <- pool3
I0520 21:05:26.059769 31010 net.cpp:411] conv4 -> conv4
I0520 21:05:26.062541 31010 net.cpp:150] Setting up conv4
I0520 21:05:26.062571 31010 net.cpp:157] Top shape: 310 36 6 42 (2812320)
I0520 21:05:26.062580 31010 net.cpp:165] Memory required for data: 471426920
I0520 21:05:26.062597 31010 layer_factory.hpp:77] Creating layer relu4
I0520 21:05:26.062610 31010 net.cpp:106] Creating Layer relu4
I0520 21:05:26.062620 31010 net.cpp:454] relu4 <- conv4
I0520 21:05:26.062633 31010 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:05:26.063108 31010 net.cpp:150] Setting up relu4
I0520 21:05:26.063124 31010 net.cpp:157] Top shape: 310 36 6 42 (2812320)
I0520 21:05:26.063135 31010 net.cpp:165] Memory required for data: 482676200
I0520 21:05:26.063145 31010 layer_factory.hpp:77] Creating layer pool4
I0520 21:05:26.063158 31010 net.cpp:106] Creating Layer pool4
I0520 21:05:26.063169 31010 net.cpp:454] pool4 <- conv4
I0520 21:05:26.063181 31010 net.cpp:411] pool4 -> pool4
I0520 21:05:26.063249 31010 net.cpp:150] Setting up pool4
I0520 21:05:26.063262 31010 net.cpp:157] Top shape: 310 36 3 42 (1406160)
I0520 21:05:26.063273 31010 net.cpp:165] Memory required for data: 488300840
I0520 21:05:26.063283 31010 layer_factory.hpp:77] Creating layer ip1
I0520 21:05:26.063302 31010 net.cpp:106] Creating Layer ip1
I0520 21:05:26.063313 31010 net.cpp:454] ip1 <- pool4
I0520 21:05:26.063325 31010 net.cpp:411] ip1 -> ip1
I0520 21:05:26.078778 31010 net.cpp:150] Setting up ip1
I0520 21:05:26.078809 31010 net.cpp:157] Top shape: 310 196 (60760)
I0520 21:05:26.078820 31010 net.cpp:165] Memory required for data: 488543880
I0520 21:05:26.078843 31010 layer_factory.hpp:77] Creating layer relu5
I0520 21:05:26.078858 31010 net.cpp:106] Creating Layer relu5
I0520 21:05:26.078868 31010 net.cpp:454] relu5 <- ip1
I0520 21:05:26.078881 31010 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:05:26.079226 31010 net.cpp:150] Setting up relu5
I0520 21:05:26.079239 31010 net.cpp:157] Top shape: 310 196 (60760)
I0520 21:05:26.079249 31010 net.cpp:165] Memory required for data: 488786920
I0520 21:05:26.079259 31010 layer_factory.hpp:77] Creating layer drop1
I0520 21:05:26.079282 31010 net.cpp:106] Creating Layer drop1
I0520 21:05:26.079291 31010 net.cpp:454] drop1 <- ip1
I0520 21:05:26.079316 31010 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:05:26.079363 31010 net.cpp:150] Setting up drop1
I0520 21:05:26.079376 31010 net.cpp:157] Top shape: 310 196 (60760)
I0520 21:05:26.079386 31010 net.cpp:165] Memory required for data: 489029960
I0520 21:05:26.079396 31010 layer_factory.hpp:77] Creating layer ip2
I0520 21:05:26.079414 31010 net.cpp:106] Creating Layer ip2
I0520 21:05:26.079424 31010 net.cpp:454] ip2 <- ip1
I0520 21:05:26.079437 31010 net.cpp:411] ip2 -> ip2
I0520 21:05:26.079906 31010 net.cpp:150] Setting up ip2
I0520 21:05:26.079921 31010 net.cpp:157] Top shape: 310 98 (30380)
I0520 21:05:26.079931 31010 net.cpp:165] Memory required for data: 489151480
I0520 21:05:26.079946 31010 layer_factory.hpp:77] Creating layer relu6
I0520 21:05:26.079957 31010 net.cpp:106] Creating Layer relu6
I0520 21:05:26.079967 31010 net.cpp:454] relu6 <- ip2
I0520 21:05:26.079979 31010 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:05:26.080499 31010 net.cpp:150] Setting up relu6
I0520 21:05:26.080516 31010 net.cpp:157] Top shape: 310 98 (30380)
I0520 21:05:26.080526 31010 net.cpp:165] Memory required for data: 489273000
I0520 21:05:26.080536 31010 layer_factory.hpp:77] Creating layer drop2
I0520 21:05:26.080549 31010 net.cpp:106] Creating Layer drop2
I0520 21:05:26.080559 31010 net.cpp:454] drop2 <- ip2
I0520 21:05:26.080571 31010 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:05:26.080613 31010 net.cpp:150] Setting up drop2
I0520 21:05:26.080626 31010 net.cpp:157] Top shape: 310 98 (30380)
I0520 21:05:26.080636 31010 net.cpp:165] Memory required for data: 489394520
I0520 21:05:26.080646 31010 layer_factory.hpp:77] Creating layer ip3
I0520 21:05:26.080659 31010 net.cpp:106] Creating Layer ip3
I0520 21:05:26.080669 31010 net.cpp:454] ip3 <- ip2
I0520 21:05:26.080682 31010 net.cpp:411] ip3 -> ip3
I0520 21:05:26.080893 31010 net.cpp:150] Setting up ip3
I0520 21:05:26.080906 31010 net.cpp:157] Top shape: 310 11 (3410)
I0520 21:05:26.080916 31010 net.cpp:165] Memory required for data: 489408160
I0520 21:05:26.080931 31010 layer_factory.hpp:77] Creating layer drop3
I0520 21:05:26.080943 31010 net.cpp:106] Creating Layer drop3
I0520 21:05:26.080953 31010 net.cpp:454] drop3 <- ip3
I0520 21:05:26.080965 31010 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:05:26.081004 31010 net.cpp:150] Setting up drop3
I0520 21:05:26.081017 31010 net.cpp:157] Top shape: 310 11 (3410)
I0520 21:05:26.081027 31010 net.cpp:165] Memory required for data: 489421800
I0520 21:05:26.081037 31010 layer_factory.hpp:77] Creating layer loss
I0520 21:05:26.081056 31010 net.cpp:106] Creating Layer loss
I0520 21:05:26.081066 31010 net.cpp:454] loss <- ip3
I0520 21:05:26.081078 31010 net.cpp:454] loss <- label
I0520 21:05:26.081089 31010 net.cpp:411] loss -> loss
I0520 21:05:26.081107 31010 layer_factory.hpp:77] Creating layer loss
I0520 21:05:26.081755 31010 net.cpp:150] Setting up loss
I0520 21:05:26.081770 31010 net.cpp:157] Top shape: (1)
I0520 21:05:26.081782 31010 net.cpp:160]     with loss weight 1
I0520 21:05:26.081825 31010 net.cpp:165] Memory required for data: 489421804
I0520 21:05:26.081836 31010 net.cpp:226] loss needs backward computation.
I0520 21:05:26.081847 31010 net.cpp:226] drop3 needs backward computation.
I0520 21:05:26.081857 31010 net.cpp:226] ip3 needs backward computation.
I0520 21:05:26.081867 31010 net.cpp:226] drop2 needs backward computation.
I0520 21:05:26.081877 31010 net.cpp:226] relu6 needs backward computation.
I0520 21:05:26.081887 31010 net.cpp:226] ip2 needs backward computation.
I0520 21:05:26.081897 31010 net.cpp:226] drop1 needs backward computation.
I0520 21:05:26.081907 31010 net.cpp:226] relu5 needs backward computation.
I0520 21:05:26.081917 31010 net.cpp:226] ip1 needs backward computation.
I0520 21:05:26.081926 31010 net.cpp:226] pool4 needs backward computation.
I0520 21:05:26.081938 31010 net.cpp:226] relu4 needs backward computation.
I0520 21:05:26.081946 31010 net.cpp:226] conv4 needs backward computation.
I0520 21:05:26.081957 31010 net.cpp:226] pool3 needs backward computation.
I0520 21:05:26.081975 31010 net.cpp:226] relu3 needs backward computation.
I0520 21:05:26.081985 31010 net.cpp:226] conv3 needs backward computation.
I0520 21:05:26.081995 31010 net.cpp:226] pool2 needs backward computation.
I0520 21:05:26.082005 31010 net.cpp:226] relu2 needs backward computation.
I0520 21:05:26.082015 31010 net.cpp:226] conv2 needs backward computation.
I0520 21:05:26.082026 31010 net.cpp:226] pool1 needs backward computation.
I0520 21:05:26.082036 31010 net.cpp:226] relu1 needs backward computation.
I0520 21:05:26.082046 31010 net.cpp:226] conv1 needs backward computation.
I0520 21:05:26.082057 31010 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:05:26.082067 31010 net.cpp:270] This network produces output loss
I0520 21:05:26.082090 31010 net.cpp:283] Network initialization done.
I0520 21:05:26.083691 31010 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468.prototxt
I0520 21:05:26.083763 31010 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 21:05:26.084120 31010 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 310
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:05:26.084309 31010 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:05:26.084324 31010 net.cpp:106] Creating Layer data_hdf5
I0520 21:05:26.084337 31010 net.cpp:411] data_hdf5 -> data
I0520 21:05:26.084353 31010 net.cpp:411] data_hdf5 -> label
I0520 21:05:26.084369 31010 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 21:05:26.085541 31010 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 21:05:47.420066 31010 net.cpp:150] Setting up data_hdf5
I0520 21:05:47.420234 31010 net.cpp:157] Top shape: 310 1 127 50 (1968500)
I0520 21:05:47.420248 31010 net.cpp:157] Top shape: 310 (310)
I0520 21:05:47.420259 31010 net.cpp:165] Memory required for data: 7875240
I0520 21:05:47.420298 31010 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 21:05:47.420327 31010 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 21:05:47.420337 31010 net.cpp:454] label_data_hdf5_1_split <- label
I0520 21:05:47.420351 31010 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 21:05:47.420372 31010 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 21:05:47.420444 31010 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 21:05:47.420457 31010 net.cpp:157] Top shape: 310 (310)
I0520 21:05:47.420469 31010 net.cpp:157] Top shape: 310 (310)
I0520 21:05:47.420478 31010 net.cpp:165] Memory required for data: 7877720
I0520 21:05:47.420487 31010 layer_factory.hpp:77] Creating layer conv1
I0520 21:05:47.420507 31010 net.cpp:106] Creating Layer conv1
I0520 21:05:47.420518 31010 net.cpp:454] conv1 <- data
I0520 21:05:47.420532 31010 net.cpp:411] conv1 -> conv1
I0520 21:05:47.422466 31010 net.cpp:150] Setting up conv1
I0520 21:05:47.422490 31010 net.cpp:157] Top shape: 310 12 120 48 (21427200)
I0520 21:05:47.422502 31010 net.cpp:165] Memory required for data: 93586520
I0520 21:05:47.422523 31010 layer_factory.hpp:77] Creating layer relu1
I0520 21:05:47.422538 31010 net.cpp:106] Creating Layer relu1
I0520 21:05:47.422547 31010 net.cpp:454] relu1 <- conv1
I0520 21:05:47.422560 31010 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:05:47.423058 31010 net.cpp:150] Setting up relu1
I0520 21:05:47.423074 31010 net.cpp:157] Top shape: 310 12 120 48 (21427200)
I0520 21:05:47.423084 31010 net.cpp:165] Memory required for data: 179295320
I0520 21:05:47.423094 31010 layer_factory.hpp:77] Creating layer pool1
I0520 21:05:47.423111 31010 net.cpp:106] Creating Layer pool1
I0520 21:05:47.423120 31010 net.cpp:454] pool1 <- conv1
I0520 21:05:47.423135 31010 net.cpp:411] pool1 -> pool1
I0520 21:05:47.423209 31010 net.cpp:150] Setting up pool1
I0520 21:05:47.423223 31010 net.cpp:157] Top shape: 310 12 60 48 (10713600)
I0520 21:05:47.423233 31010 net.cpp:165] Memory required for data: 222149720
I0520 21:05:47.423241 31010 layer_factory.hpp:77] Creating layer conv2
I0520 21:05:47.423259 31010 net.cpp:106] Creating Layer conv2
I0520 21:05:47.423269 31010 net.cpp:454] conv2 <- pool1
I0520 21:05:47.423282 31010 net.cpp:411] conv2 -> conv2
I0520 21:05:47.425215 31010 net.cpp:150] Setting up conv2
I0520 21:05:47.425237 31010 net.cpp:157] Top shape: 310 20 54 46 (15400800)
I0520 21:05:47.425247 31010 net.cpp:165] Memory required for data: 283752920
I0520 21:05:47.425266 31010 layer_factory.hpp:77] Creating layer relu2
I0520 21:05:47.425279 31010 net.cpp:106] Creating Layer relu2
I0520 21:05:47.425290 31010 net.cpp:454] relu2 <- conv2
I0520 21:05:47.425302 31010 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:05:47.425635 31010 net.cpp:150] Setting up relu2
I0520 21:05:47.425649 31010 net.cpp:157] Top shape: 310 20 54 46 (15400800)
I0520 21:05:47.425659 31010 net.cpp:165] Memory required for data: 345356120
I0520 21:05:47.425669 31010 layer_factory.hpp:77] Creating layer pool2
I0520 21:05:47.425683 31010 net.cpp:106] Creating Layer pool2
I0520 21:05:47.425693 31010 net.cpp:454] pool2 <- conv2
I0520 21:05:47.425704 31010 net.cpp:411] pool2 -> pool2
I0520 21:05:47.425776 31010 net.cpp:150] Setting up pool2
I0520 21:05:47.425791 31010 net.cpp:157] Top shape: 310 20 27 46 (7700400)
I0520 21:05:47.425799 31010 net.cpp:165] Memory required for data: 376157720
I0520 21:05:47.425809 31010 layer_factory.hpp:77] Creating layer conv3
I0520 21:05:47.425828 31010 net.cpp:106] Creating Layer conv3
I0520 21:05:47.425839 31010 net.cpp:454] conv3 <- pool2
I0520 21:05:47.425853 31010 net.cpp:411] conv3 -> conv3
I0520 21:05:47.427832 31010 net.cpp:150] Setting up conv3
I0520 21:05:47.427855 31010 net.cpp:157] Top shape: 310 28 22 44 (8402240)
I0520 21:05:47.427867 31010 net.cpp:165] Memory required for data: 409766680
I0520 21:05:47.427901 31010 layer_factory.hpp:77] Creating layer relu3
I0520 21:05:47.427913 31010 net.cpp:106] Creating Layer relu3
I0520 21:05:47.427923 31010 net.cpp:454] relu3 <- conv3
I0520 21:05:47.427937 31010 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:05:47.428411 31010 net.cpp:150] Setting up relu3
I0520 21:05:47.428426 31010 net.cpp:157] Top shape: 310 28 22 44 (8402240)
I0520 21:05:47.428437 31010 net.cpp:165] Memory required for data: 443375640
I0520 21:05:47.428447 31010 layer_factory.hpp:77] Creating layer pool3
I0520 21:05:47.428460 31010 net.cpp:106] Creating Layer pool3
I0520 21:05:47.428470 31010 net.cpp:454] pool3 <- conv3
I0520 21:05:47.428483 31010 net.cpp:411] pool3 -> pool3
I0520 21:05:47.428555 31010 net.cpp:150] Setting up pool3
I0520 21:05:47.428568 31010 net.cpp:157] Top shape: 310 28 11 44 (4201120)
I0520 21:05:47.428578 31010 net.cpp:165] Memory required for data: 460180120
I0520 21:05:47.428588 31010 layer_factory.hpp:77] Creating layer conv4
I0520 21:05:47.428603 31010 net.cpp:106] Creating Layer conv4
I0520 21:05:47.428614 31010 net.cpp:454] conv4 <- pool3
I0520 21:05:47.428628 31010 net.cpp:411] conv4 -> conv4
I0520 21:05:47.430682 31010 net.cpp:150] Setting up conv4
I0520 21:05:47.430704 31010 net.cpp:157] Top shape: 310 36 6 42 (2812320)
I0520 21:05:47.430717 31010 net.cpp:165] Memory required for data: 471429400
I0520 21:05:47.430732 31010 layer_factory.hpp:77] Creating layer relu4
I0520 21:05:47.430747 31010 net.cpp:106] Creating Layer relu4
I0520 21:05:47.430757 31010 net.cpp:454] relu4 <- conv4
I0520 21:05:47.430768 31010 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:05:47.431241 31010 net.cpp:150] Setting up relu4
I0520 21:05:47.431255 31010 net.cpp:157] Top shape: 310 36 6 42 (2812320)
I0520 21:05:47.431267 31010 net.cpp:165] Memory required for data: 482678680
I0520 21:05:47.431277 31010 layer_factory.hpp:77] Creating layer pool4
I0520 21:05:47.431289 31010 net.cpp:106] Creating Layer pool4
I0520 21:05:47.431299 31010 net.cpp:454] pool4 <- conv4
I0520 21:05:47.431313 31010 net.cpp:411] pool4 -> pool4
I0520 21:05:47.431383 31010 net.cpp:150] Setting up pool4
I0520 21:05:47.431396 31010 net.cpp:157] Top shape: 310 36 3 42 (1406160)
I0520 21:05:47.431406 31010 net.cpp:165] Memory required for data: 488303320
I0520 21:05:47.431416 31010 layer_factory.hpp:77] Creating layer ip1
I0520 21:05:47.431432 31010 net.cpp:106] Creating Layer ip1
I0520 21:05:47.431442 31010 net.cpp:454] ip1 <- pool4
I0520 21:05:47.431457 31010 net.cpp:411] ip1 -> ip1
I0520 21:05:47.446915 31010 net.cpp:150] Setting up ip1
I0520 21:05:47.446945 31010 net.cpp:157] Top shape: 310 196 (60760)
I0520 21:05:47.446957 31010 net.cpp:165] Memory required for data: 488546360
I0520 21:05:47.446979 31010 layer_factory.hpp:77] Creating layer relu5
I0520 21:05:47.446995 31010 net.cpp:106] Creating Layer relu5
I0520 21:05:47.447005 31010 net.cpp:454] relu5 <- ip1
I0520 21:05:47.447018 31010 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:05:47.447365 31010 net.cpp:150] Setting up relu5
I0520 21:05:47.447379 31010 net.cpp:157] Top shape: 310 196 (60760)
I0520 21:05:47.447389 31010 net.cpp:165] Memory required for data: 488789400
I0520 21:05:47.447399 31010 layer_factory.hpp:77] Creating layer drop1
I0520 21:05:47.447418 31010 net.cpp:106] Creating Layer drop1
I0520 21:05:47.447428 31010 net.cpp:454] drop1 <- ip1
I0520 21:05:47.447441 31010 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:05:47.447485 31010 net.cpp:150] Setting up drop1
I0520 21:05:47.447499 31010 net.cpp:157] Top shape: 310 196 (60760)
I0520 21:05:47.447510 31010 net.cpp:165] Memory required for data: 489032440
I0520 21:05:47.447520 31010 layer_factory.hpp:77] Creating layer ip2
I0520 21:05:47.447535 31010 net.cpp:106] Creating Layer ip2
I0520 21:05:47.447543 31010 net.cpp:454] ip2 <- ip1
I0520 21:05:47.447557 31010 net.cpp:411] ip2 -> ip2
I0520 21:05:47.448043 31010 net.cpp:150] Setting up ip2
I0520 21:05:47.448057 31010 net.cpp:157] Top shape: 310 98 (30380)
I0520 21:05:47.448066 31010 net.cpp:165] Memory required for data: 489153960
I0520 21:05:47.448096 31010 layer_factory.hpp:77] Creating layer relu6
I0520 21:05:47.448108 31010 net.cpp:106] Creating Layer relu6
I0520 21:05:47.448118 31010 net.cpp:454] relu6 <- ip2
I0520 21:05:47.448130 31010 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:05:47.448669 31010 net.cpp:150] Setting up relu6
I0520 21:05:47.448690 31010 net.cpp:157] Top shape: 310 98 (30380)
I0520 21:05:47.448700 31010 net.cpp:165] Memory required for data: 489275480
I0520 21:05:47.448710 31010 layer_factory.hpp:77] Creating layer drop2
I0520 21:05:47.448724 31010 net.cpp:106] Creating Layer drop2
I0520 21:05:47.448734 31010 net.cpp:454] drop2 <- ip2
I0520 21:05:47.448747 31010 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:05:47.448791 31010 net.cpp:150] Setting up drop2
I0520 21:05:47.448804 31010 net.cpp:157] Top shape: 310 98 (30380)
I0520 21:05:47.448814 31010 net.cpp:165] Memory required for data: 489397000
I0520 21:05:47.448824 31010 layer_factory.hpp:77] Creating layer ip3
I0520 21:05:47.448838 31010 net.cpp:106] Creating Layer ip3
I0520 21:05:47.448848 31010 net.cpp:454] ip3 <- ip2
I0520 21:05:47.448863 31010 net.cpp:411] ip3 -> ip3
I0520 21:05:47.449086 31010 net.cpp:150] Setting up ip3
I0520 21:05:47.449100 31010 net.cpp:157] Top shape: 310 11 (3410)
I0520 21:05:47.449110 31010 net.cpp:165] Memory required for data: 489410640
I0520 21:05:47.449126 31010 layer_factory.hpp:77] Creating layer drop3
I0520 21:05:47.449138 31010 net.cpp:106] Creating Layer drop3
I0520 21:05:47.449148 31010 net.cpp:454] drop3 <- ip3
I0520 21:05:47.449162 31010 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:05:47.449203 31010 net.cpp:150] Setting up drop3
I0520 21:05:47.449214 31010 net.cpp:157] Top shape: 310 11 (3410)
I0520 21:05:47.449224 31010 net.cpp:165] Memory required for data: 489424280
I0520 21:05:47.449234 31010 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 21:05:47.449247 31010 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 21:05:47.449257 31010 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 21:05:47.449270 31010 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 21:05:47.449285 31010 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 21:05:47.449358 31010 net.cpp:150] Setting up ip3_drop3_0_split
I0520 21:05:47.449371 31010 net.cpp:157] Top shape: 310 11 (3410)
I0520 21:05:47.449383 31010 net.cpp:157] Top shape: 310 11 (3410)
I0520 21:05:47.449393 31010 net.cpp:165] Memory required for data: 489451560
I0520 21:05:47.449405 31010 layer_factory.hpp:77] Creating layer accuracy
I0520 21:05:47.449427 31010 net.cpp:106] Creating Layer accuracy
I0520 21:05:47.449437 31010 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 21:05:47.449448 31010 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 21:05:47.449462 31010 net.cpp:411] accuracy -> accuracy
I0520 21:05:47.449486 31010 net.cpp:150] Setting up accuracy
I0520 21:05:47.449498 31010 net.cpp:157] Top shape: (1)
I0520 21:05:47.449508 31010 net.cpp:165] Memory required for data: 489451564
I0520 21:05:47.449517 31010 layer_factory.hpp:77] Creating layer loss
I0520 21:05:47.449529 31010 net.cpp:106] Creating Layer loss
I0520 21:05:47.449540 31010 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 21:05:47.449551 31010 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 21:05:47.449564 31010 net.cpp:411] loss -> loss
I0520 21:05:47.449582 31010 layer_factory.hpp:77] Creating layer loss
I0520 21:05:47.450067 31010 net.cpp:150] Setting up loss
I0520 21:05:47.450081 31010 net.cpp:157] Top shape: (1)
I0520 21:05:47.450090 31010 net.cpp:160]     with loss weight 1
I0520 21:05:47.450109 31010 net.cpp:165] Memory required for data: 489451568
I0520 21:05:47.450119 31010 net.cpp:226] loss needs backward computation.
I0520 21:05:47.450130 31010 net.cpp:228] accuracy does not need backward computation.
I0520 21:05:47.450141 31010 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 21:05:47.450152 31010 net.cpp:226] drop3 needs backward computation.
I0520 21:05:47.450162 31010 net.cpp:226] ip3 needs backward computation.
I0520 21:05:47.450173 31010 net.cpp:226] drop2 needs backward computation.
I0520 21:05:47.450191 31010 net.cpp:226] relu6 needs backward computation.
I0520 21:05:47.450201 31010 net.cpp:226] ip2 needs backward computation.
I0520 21:05:47.450212 31010 net.cpp:226] drop1 needs backward computation.
I0520 21:05:47.450220 31010 net.cpp:226] relu5 needs backward computation.
I0520 21:05:47.450230 31010 net.cpp:226] ip1 needs backward computation.
I0520 21:05:47.450239 31010 net.cpp:226] pool4 needs backward computation.
I0520 21:05:47.450250 31010 net.cpp:226] relu4 needs backward computation.
I0520 21:05:47.450259 31010 net.cpp:226] conv4 needs backward computation.
I0520 21:05:47.450270 31010 net.cpp:226] pool3 needs backward computation.
I0520 21:05:47.450281 31010 net.cpp:226] relu3 needs backward computation.
I0520 21:05:47.450291 31010 net.cpp:226] conv3 needs backward computation.
I0520 21:05:47.450301 31010 net.cpp:226] pool2 needs backward computation.
I0520 21:05:47.450311 31010 net.cpp:226] relu2 needs backward computation.
I0520 21:05:47.450321 31010 net.cpp:226] conv2 needs backward computation.
I0520 21:05:47.450332 31010 net.cpp:226] pool1 needs backward computation.
I0520 21:05:47.450342 31010 net.cpp:226] relu1 needs backward computation.
I0520 21:05:47.450351 31010 net.cpp:226] conv1 needs backward computation.
I0520 21:05:47.450362 31010 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 21:05:47.450374 31010 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:05:47.450384 31010 net.cpp:270] This network produces output accuracy
I0520 21:05:47.450394 31010 net.cpp:270] This network produces output loss
I0520 21:05:47.450420 31010 net.cpp:283] Network initialization done.
I0520 21:05:47.450553 31010 solver.cpp:60] Solver scaffolding done.
I0520 21:05:47.451694 31010 caffe.cpp:212] Starting Optimization
I0520 21:05:47.451711 31010 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 21:05:47.451725 31010 solver.cpp:289] Learning Rate Policy: fixed
I0520 21:05:47.452958 31010 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 21:06:33.794278 31010 solver.cpp:409]     Test net output #0: accuracy = 0.0867897
I0520 21:06:33.794438 31010 solver.cpp:409]     Test net output #1: loss = 2.39778 (* 1 = 2.39778 loss)
I0520 21:06:33.861865 31010 solver.cpp:237] Iteration 0, loss = 2.39754
I0520 21:06:33.861902 31010 solver.cpp:253]     Train net output #0: loss = 2.39754 (* 1 = 2.39754 loss)
I0520 21:06:33.861923 31010 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 21:06:42.014482 31010 solver.cpp:237] Iteration 48, loss = 2.36002
I0520 21:06:42.014530 31010 solver.cpp:253]     Train net output #0: loss = 2.36002 (* 1 = 2.36002 loss)
I0520 21:06:42.014545 31010 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0520 21:06:50.165923 31010 solver.cpp:237] Iteration 96, loss = 2.3471
I0520 21:06:50.165956 31010 solver.cpp:253]     Train net output #0: loss = 2.3471 (* 1 = 2.3471 loss)
I0520 21:06:50.165972 31010 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0520 21:06:58.322743 31010 solver.cpp:237] Iteration 144, loss = 2.34344
I0520 21:06:58.322777 31010 solver.cpp:253]     Train net output #0: loss = 2.34344 (* 1 = 2.34344 loss)
I0520 21:06:58.322793 31010 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0520 21:07:06.478142 31010 solver.cpp:237] Iteration 192, loss = 2.35604
I0520 21:07:06.478288 31010 solver.cpp:253]     Train net output #0: loss = 2.35604 (* 1 = 2.35604 loss)
I0520 21:07:06.478302 31010 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0520 21:07:14.629807 31010 solver.cpp:237] Iteration 240, loss = 2.31378
I0520 21:07:14.629847 31010 solver.cpp:253]     Train net output #0: loss = 2.31378 (* 1 = 2.31378 loss)
I0520 21:07:14.629866 31010 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0520 21:07:22.783545 31010 solver.cpp:237] Iteration 288, loss = 2.27148
I0520 21:07:22.783577 31010 solver.cpp:253]     Train net output #0: loss = 2.27148 (* 1 = 2.27148 loss)
I0520 21:07:22.783591 31010 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0520 21:07:53.266095 31010 solver.cpp:237] Iteration 336, loss = 2.29477
I0520 21:07:53.266259 31010 solver.cpp:253]     Train net output #0: loss = 2.29477 (* 1 = 2.29477 loss)
I0520 21:07:53.266273 31010 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0520 21:08:01.421252 31010 solver.cpp:237] Iteration 384, loss = 2.20972
I0520 21:08:01.421298 31010 solver.cpp:253]     Train net output #0: loss = 2.20972 (* 1 = 2.20972 loss)
I0520 21:08:01.421315 31010 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0520 21:08:09.575909 31010 solver.cpp:237] Iteration 432, loss = 2.13181
I0520 21:08:09.575944 31010 solver.cpp:253]     Train net output #0: loss = 2.13181 (* 1 = 2.13181 loss)
I0520 21:08:09.575960 31010 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0520 21:08:17.730592 31010 solver.cpp:237] Iteration 480, loss = 2.05864
I0520 21:08:17.730628 31010 solver.cpp:253]     Train net output #0: loss = 2.05864 (* 1 = 2.05864 loss)
I0520 21:08:17.730644 31010 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0520 21:08:18.071486 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_483.caffemodel
I0520 21:08:18.231997 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_483.solverstate
I0520 21:08:25.956351 31010 solver.cpp:237] Iteration 528, loss = 2.03338
I0520 21:08:25.956511 31010 solver.cpp:253]     Train net output #0: loss = 2.03338 (* 1 = 2.03338 loss)
I0520 21:08:25.956526 31010 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0520 21:08:34.111461 31010 solver.cpp:237] Iteration 576, loss = 1.94569
I0520 21:08:34.111495 31010 solver.cpp:253]     Train net output #0: loss = 1.94569 (* 1 = 1.94569 loss)
I0520 21:08:34.111511 31010 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0520 21:08:42.265535 31010 solver.cpp:237] Iteration 624, loss = 2.01286
I0520 21:08:42.265568 31010 solver.cpp:253]     Train net output #0: loss = 2.01286 (* 1 = 2.01286 loss)
I0520 21:08:42.265581 31010 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0520 21:09:12.655946 31010 solver.cpp:237] Iteration 672, loss = 1.97781
I0520 21:09:12.656114 31010 solver.cpp:253]     Train net output #0: loss = 1.97781 (* 1 = 1.97781 loss)
I0520 21:09:12.656129 31010 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0520 21:09:20.810647 31010 solver.cpp:237] Iteration 720, loss = 2.0356
I0520 21:09:20.810681 31010 solver.cpp:253]     Train net output #0: loss = 2.0356 (* 1 = 2.0356 loss)
I0520 21:09:20.810696 31010 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0520 21:09:28.965703 31010 solver.cpp:237] Iteration 768, loss = 1.86507
I0520 21:09:28.965737 31010 solver.cpp:253]     Train net output #0: loss = 1.86507 (* 1 = 1.86507 loss)
I0520 21:09:28.965754 31010 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0520 21:09:37.117785 31010 solver.cpp:237] Iteration 816, loss = 1.93504
I0520 21:09:37.117835 31010 solver.cpp:253]     Train net output #0: loss = 1.93504 (* 1 = 1.93504 loss)
I0520 21:09:37.117851 31010 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0520 21:09:45.272447 31010 solver.cpp:237] Iteration 864, loss = 1.95637
I0520 21:09:45.272596 31010 solver.cpp:253]     Train net output #0: loss = 1.95637 (* 1 = 1.95637 loss)
I0520 21:09:45.272610 31010 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0520 21:09:53.424233 31010 solver.cpp:237] Iteration 912, loss = 1.82172
I0520 21:09:53.424266 31010 solver.cpp:253]     Train net output #0: loss = 1.82172 (* 1 = 1.82172 loss)
I0520 21:09:53.424283 31010 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0520 21:10:01.572123 31010 solver.cpp:237] Iteration 960, loss = 1.99333
I0520 21:10:01.572156 31010 solver.cpp:253]     Train net output #0: loss = 1.99333 (* 1 = 1.99333 loss)
I0520 21:10:01.572173 31010 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0520 21:10:02.422982 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_966.caffemodel
I0520 21:10:02.579799 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_966.solverstate
I0520 21:10:02.656122 31010 solver.cpp:341] Iteration 967, Testing net (#0)
I0520 21:10:48.217898 31010 solver.cpp:409]     Test net output #0: accuracy = 0.58659
I0520 21:10:48.218075 31010 solver.cpp:409]     Test net output #1: loss = 1.60297 (* 1 = 1.60297 loss)
I0520 21:11:17.406126 31010 solver.cpp:237] Iteration 1008, loss = 1.73753
I0520 21:11:17.406175 31010 solver.cpp:253]     Train net output #0: loss = 1.73753 (* 1 = 1.73753 loss)
I0520 21:11:17.406191 31010 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0520 21:11:25.561359 31010 solver.cpp:237] Iteration 1056, loss = 1.71379
I0520 21:11:25.561504 31010 solver.cpp:253]     Train net output #0: loss = 1.71379 (* 1 = 1.71379 loss)
I0520 21:11:25.561518 31010 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0520 21:11:33.715086 31010 solver.cpp:237] Iteration 1104, loss = 1.80074
I0520 21:11:33.715118 31010 solver.cpp:253]     Train net output #0: loss = 1.80074 (* 1 = 1.80074 loss)
I0520 21:11:33.715136 31010 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0520 21:11:41.872386 31010 solver.cpp:237] Iteration 1152, loss = 1.85467
I0520 21:11:41.872432 31010 solver.cpp:253]     Train net output #0: loss = 1.85467 (* 1 = 1.85467 loss)
I0520 21:11:41.872449 31010 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0520 21:11:50.033917 31010 solver.cpp:237] Iteration 1200, loss = 1.79528
I0520 21:11:50.033952 31010 solver.cpp:253]     Train net output #0: loss = 1.79528 (* 1 = 1.79528 loss)
I0520 21:11:50.033967 31010 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0520 21:11:58.188141 31010 solver.cpp:237] Iteration 1248, loss = 1.84075
I0520 21:11:58.188293 31010 solver.cpp:253]     Train net output #0: loss = 1.84075 (* 1 = 1.84075 loss)
I0520 21:11:58.188308 31010 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0520 21:12:28.569594 31010 solver.cpp:237] Iteration 1296, loss = 1.82821
I0520 21:12:28.569759 31010 solver.cpp:253]     Train net output #0: loss = 1.82821 (* 1 = 1.82821 loss)
I0520 21:12:28.569775 31010 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0520 21:12:36.729804 31010 solver.cpp:237] Iteration 1344, loss = 1.70987
I0520 21:12:36.729841 31010 solver.cpp:253]     Train net output #0: loss = 1.70987 (* 1 = 1.70987 loss)
I0520 21:12:36.729861 31010 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0520 21:12:44.887825 31010 solver.cpp:237] Iteration 1392, loss = 1.79427
I0520 21:12:44.887855 31010 solver.cpp:253]     Train net output #0: loss = 1.79427 (* 1 = 1.79427 loss)
I0520 21:12:44.887867 31010 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0520 21:12:53.047989 31010 solver.cpp:237] Iteration 1440, loss = 1.92664
I0520 21:12:53.048023 31010 solver.cpp:253]     Train net output #0: loss = 1.92664 (* 1 = 1.92664 loss)
I0520 21:12:53.048039 31010 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0520 21:12:54.409077 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_1449.caffemodel
I0520 21:12:54.569263 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_1449.solverstate
I0520 21:13:01.278255 31010 solver.cpp:237] Iteration 1488, loss = 1.74218
I0520 21:13:01.278421 31010 solver.cpp:253]     Train net output #0: loss = 1.74218 (* 1 = 1.74218 loss)
I0520 21:13:01.278435 31010 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0520 21:13:09.434371 31010 solver.cpp:237] Iteration 1536, loss = 1.62467
I0520 21:13:09.434406 31010 solver.cpp:253]     Train net output #0: loss = 1.62467 (* 1 = 1.62467 loss)
I0520 21:13:09.434422 31010 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0520 21:13:17.588064 31010 solver.cpp:237] Iteration 1584, loss = 1.87623
I0520 21:13:17.588093 31010 solver.cpp:253]     Train net output #0: loss = 1.87623 (* 1 = 1.87623 loss)
I0520 21:13:17.588107 31010 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0520 21:13:47.886188 31010 solver.cpp:237] Iteration 1632, loss = 1.72385
I0520 21:13:47.886346 31010 solver.cpp:253]     Train net output #0: loss = 1.72385 (* 1 = 1.72385 loss)
I0520 21:13:47.886361 31010 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0520 21:13:56.042026 31010 solver.cpp:237] Iteration 1680, loss = 1.80396
I0520 21:13:56.042058 31010 solver.cpp:253]     Train net output #0: loss = 1.80396 (* 1 = 1.80396 loss)
I0520 21:13:56.042073 31010 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0520 21:14:04.193536 31010 solver.cpp:237] Iteration 1728, loss = 1.78374
I0520 21:14:04.193572 31010 solver.cpp:253]     Train net output #0: loss = 1.78374 (* 1 = 1.78374 loss)
I0520 21:14:04.193588 31010 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0520 21:14:12.354439 31010 solver.cpp:237] Iteration 1776, loss = 1.76571
I0520 21:14:12.354477 31010 solver.cpp:253]     Train net output #0: loss = 1.76571 (* 1 = 1.76571 loss)
I0520 21:14:12.354495 31010 sgd_solver.cpp:106] Iteration 1776, lr = 0.0025
I0520 21:14:20.513929 31010 solver.cpp:237] Iteration 1824, loss = 1.80421
I0520 21:14:20.514065 31010 solver.cpp:253]     Train net output #0: loss = 1.80421 (* 1 = 1.80421 loss)
I0520 21:14:20.514078 31010 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0520 21:14:28.669850 31010 solver.cpp:237] Iteration 1872, loss = 1.79553
I0520 21:14:28.669883 31010 solver.cpp:253]     Train net output #0: loss = 1.79553 (* 1 = 1.79553 loss)
I0520 21:14:28.669899 31010 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0520 21:14:36.828806 31010 solver.cpp:237] Iteration 1920, loss = 1.70477
I0520 21:14:36.828850 31010 solver.cpp:253]     Train net output #0: loss = 1.70477 (* 1 = 1.70477 loss)
I0520 21:14:36.828866 31010 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0520 21:14:38.697240 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_1932.caffemodel
I0520 21:14:38.868883 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_1932.solverstate
I0520 21:14:39.119174 31010 solver.cpp:341] Iteration 1934, Testing net (#0)
I0520 21:15:45.601707 31010 solver.cpp:409]     Test net output #0: accuracy = 0.662005
I0520 21:15:45.601876 31010 solver.cpp:409]     Test net output #1: loss = 1.16739 (* 1 = 1.16739 loss)
I0520 21:16:13.655455 31010 solver.cpp:237] Iteration 1968, loss = 1.65662
I0520 21:16:13.655504 31010 solver.cpp:253]     Train net output #0: loss = 1.65662 (* 1 = 1.65662 loss)
I0520 21:16:13.655520 31010 sgd_solver.cpp:106] Iteration 1968, lr = 0.0025
I0520 21:16:21.823006 31010 solver.cpp:237] Iteration 2016, loss = 1.70512
I0520 21:16:21.823166 31010 solver.cpp:253]     Train net output #0: loss = 1.70512 (* 1 = 1.70512 loss)
I0520 21:16:21.823180 31010 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0520 21:16:29.993227 31010 solver.cpp:237] Iteration 2064, loss = 1.77102
I0520 21:16:29.993260 31010 solver.cpp:253]     Train net output #0: loss = 1.77102 (* 1 = 1.77102 loss)
I0520 21:16:29.993276 31010 sgd_solver.cpp:106] Iteration 2064, lr = 0.0025
I0520 21:16:38.163027 31010 solver.cpp:237] Iteration 2112, loss = 1.58378
I0520 21:16:38.163060 31010 solver.cpp:253]     Train net output #0: loss = 1.58378 (* 1 = 1.58378 loss)
I0520 21:16:38.163075 31010 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0520 21:16:46.336027 31010 solver.cpp:237] Iteration 2160, loss = 1.75811
I0520 21:16:46.336057 31010 solver.cpp:253]     Train net output #0: loss = 1.75811 (* 1 = 1.75811 loss)
I0520 21:16:46.336081 31010 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0520 21:16:54.513748 31010 solver.cpp:237] Iteration 2208, loss = 1.71498
I0520 21:16:54.513882 31010 solver.cpp:253]     Train net output #0: loss = 1.71498 (* 1 = 1.71498 loss)
I0520 21:16:54.513896 31010 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0520 21:17:02.688931 31010 solver.cpp:237] Iteration 2256, loss = 1.66001
I0520 21:17:02.688964 31010 solver.cpp:253]     Train net output #0: loss = 1.66001 (* 1 = 1.66001 loss)
I0520 21:17:02.688982 31010 sgd_solver.cpp:106] Iteration 2256, lr = 0.0025
I0520 21:17:33.079499 31010 solver.cpp:237] Iteration 2304, loss = 1.68021
I0520 21:17:33.079669 31010 solver.cpp:253]     Train net output #0: loss = 1.68021 (* 1 = 1.68021 loss)
I0520 21:17:33.079684 31010 sgd_solver.cpp:106] Iteration 2304, lr = 0.0025
I0520 21:17:41.249250 31010 solver.cpp:237] Iteration 2352, loss = 1.6461
I0520 21:17:41.249284 31010 solver.cpp:253]     Train net output #0: loss = 1.6461 (* 1 = 1.6461 loss)
I0520 21:17:41.249300 31010 sgd_solver.cpp:106] Iteration 2352, lr = 0.0025
I0520 21:17:49.422267 31010 solver.cpp:237] Iteration 2400, loss = 1.61533
I0520 21:17:49.422302 31010 solver.cpp:253]     Train net output #0: loss = 1.61533 (* 1 = 1.61533 loss)
I0520 21:17:49.422317 31010 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0520 21:17:51.805480 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_2415.caffemodel
I0520 21:17:51.964988 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_2415.solverstate
I0520 21:17:57.657393 31010 solver.cpp:237] Iteration 2448, loss = 1.61553
I0520 21:17:57.657443 31010 solver.cpp:253]     Train net output #0: loss = 1.61553 (* 1 = 1.61553 loss)
I0520 21:17:57.657459 31010 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0520 21:18:05.827970 31010 solver.cpp:237] Iteration 2496, loss = 1.57045
I0520 21:18:05.828111 31010 solver.cpp:253]     Train net output #0: loss = 1.57045 (* 1 = 1.57045 loss)
I0520 21:18:05.828125 31010 sgd_solver.cpp:106] Iteration 2496, lr = 0.0025
I0520 21:18:13.999464 31010 solver.cpp:237] Iteration 2544, loss = 1.66021
I0520 21:18:13.999495 31010 solver.cpp:253]     Train net output #0: loss = 1.66021 (* 1 = 1.66021 loss)
I0520 21:18:13.999512 31010 sgd_solver.cpp:106] Iteration 2544, lr = 0.0025
I0520 21:18:44.422410 31010 solver.cpp:237] Iteration 2592, loss = 1.63782
I0520 21:18:44.422581 31010 solver.cpp:253]     Train net output #0: loss = 1.63782 (* 1 = 1.63782 loss)
I0520 21:18:44.422596 31010 sgd_solver.cpp:106] Iteration 2592, lr = 0.0025
I0520 21:18:52.600879 31010 solver.cpp:237] Iteration 2640, loss = 1.68685
I0520 21:18:52.600924 31010 solver.cpp:253]     Train net output #0: loss = 1.68685 (* 1 = 1.68685 loss)
I0520 21:18:52.600940 31010 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0520 21:19:00.770603 31010 solver.cpp:237] Iteration 2688, loss = 1.63368
I0520 21:19:00.770637 31010 solver.cpp:253]     Train net output #0: loss = 1.63368 (* 1 = 1.63368 loss)
I0520 21:19:00.770653 31010 sgd_solver.cpp:106] Iteration 2688, lr = 0.0025
I0520 21:19:08.941987 31010 solver.cpp:237] Iteration 2736, loss = 1.52158
I0520 21:19:08.942023 31010 solver.cpp:253]     Train net output #0: loss = 1.52158 (* 1 = 1.52158 loss)
I0520 21:19:08.942039 31010 sgd_solver.cpp:106] Iteration 2736, lr = 0.0025
I0520 21:19:17.112687 31010 solver.cpp:237] Iteration 2784, loss = 1.64718
I0520 21:19:17.112844 31010 solver.cpp:253]     Train net output #0: loss = 1.64718 (* 1 = 1.64718 loss)
I0520 21:19:17.112859 31010 sgd_solver.cpp:106] Iteration 2784, lr = 0.0025
I0520 21:19:25.282486 31010 solver.cpp:237] Iteration 2832, loss = 1.63153
I0520 21:19:25.282519 31010 solver.cpp:253]     Train net output #0: loss = 1.63153 (* 1 = 1.63153 loss)
I0520 21:19:25.282536 31010 sgd_solver.cpp:106] Iteration 2832, lr = 0.0025
I0520 21:19:33.451508 31010 solver.cpp:237] Iteration 2880, loss = 1.60596
I0520 21:19:33.451541 31010 solver.cpp:253]     Train net output #0: loss = 1.60596 (* 1 = 1.60596 loss)
I0520 21:19:33.451557 31010 sgd_solver.cpp:106] Iteration 2880, lr = 0.0025
I0520 21:19:36.339864 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_2898.caffemodel
I0520 21:19:36.497303 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_2898.solverstate
I0520 21:19:36.915630 31010 solver.cpp:341] Iteration 2901, Testing net (#0)
I0520 21:20:22.122396 31010 solver.cpp:409]     Test net output #0: accuracy = 0.691525
I0520 21:20:22.122557 31010 solver.cpp:409]     Test net output #1: loss = 1.07096 (* 1 = 1.07096 loss)
I0520 21:20:49.033344 31010 solver.cpp:237] Iteration 2928, loss = 1.51395
I0520 21:20:49.033396 31010 solver.cpp:253]     Train net output #0: loss = 1.51395 (* 1 = 1.51395 loss)
I0520 21:20:49.033409 31010 sgd_solver.cpp:106] Iteration 2928, lr = 0.0025
I0520 21:20:57.200822 31010 solver.cpp:237] Iteration 2976, loss = 1.64549
I0520 21:20:57.200981 31010 solver.cpp:253]     Train net output #0: loss = 1.64549 (* 1 = 1.64549 loss)
I0520 21:20:57.200995 31010 sgd_solver.cpp:106] Iteration 2976, lr = 0.0025
I0520 21:21:05.371687 31010 solver.cpp:237] Iteration 3024, loss = 1.60306
I0520 21:21:05.371721 31010 solver.cpp:253]     Train net output #0: loss = 1.60306 (* 1 = 1.60306 loss)
I0520 21:21:05.371737 31010 sgd_solver.cpp:106] Iteration 3024, lr = 0.0025
I0520 21:21:13.544536 31010 solver.cpp:237] Iteration 3072, loss = 1.67025
I0520 21:21:13.544570 31010 solver.cpp:253]     Train net output #0: loss = 1.67025 (* 1 = 1.67025 loss)
I0520 21:21:13.544586 31010 sgd_solver.cpp:106] Iteration 3072, lr = 0.0025
I0520 21:21:21.708942 31010 solver.cpp:237] Iteration 3120, loss = 1.56706
I0520 21:21:21.708987 31010 solver.cpp:253]     Train net output #0: loss = 1.56706 (* 1 = 1.56706 loss)
I0520 21:21:21.709005 31010 sgd_solver.cpp:106] Iteration 3120, lr = 0.0025
I0520 21:21:29.865066 31010 solver.cpp:237] Iteration 3168, loss = 1.60244
I0520 21:21:29.865216 31010 solver.cpp:253]     Train net output #0: loss = 1.60244 (* 1 = 1.60244 loss)
I0520 21:21:29.865229 31010 sgd_solver.cpp:106] Iteration 3168, lr = 0.0025
I0520 21:21:38.024368 31010 solver.cpp:237] Iteration 3216, loss = 1.56144
I0520 21:21:38.024401 31010 solver.cpp:253]     Train net output #0: loss = 1.56144 (* 1 = 1.56144 loss)
I0520 21:21:38.024415 31010 sgd_solver.cpp:106] Iteration 3216, lr = 0.0025
I0520 21:22:08.444579 31010 solver.cpp:237] Iteration 3264, loss = 1.63287
I0520 21:22:08.444742 31010 solver.cpp:253]     Train net output #0: loss = 1.63287 (* 1 = 1.63287 loss)
I0520 21:22:08.444757 31010 sgd_solver.cpp:106] Iteration 3264, lr = 0.0025
I0520 21:22:16.600458 31010 solver.cpp:237] Iteration 3312, loss = 1.58189
I0520 21:22:16.600492 31010 solver.cpp:253]     Train net output #0: loss = 1.58189 (* 1 = 1.58189 loss)
I0520 21:22:16.600510 31010 sgd_solver.cpp:106] Iteration 3312, lr = 0.0025
I0520 21:22:24.763175 31010 solver.cpp:237] Iteration 3360, loss = 1.65331
I0520 21:22:24.763209 31010 solver.cpp:253]     Train net output #0: loss = 1.65331 (* 1 = 1.65331 loss)
I0520 21:22:24.763224 31010 sgd_solver.cpp:106] Iteration 3360, lr = 0.0025
I0520 21:22:28.158349 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_3381.caffemodel
I0520 21:22:28.333703 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_3381.solverstate
I0520 21:22:33.018990 31010 solver.cpp:237] Iteration 3408, loss = 1.56492
I0520 21:22:33.019038 31010 solver.cpp:253]     Train net output #0: loss = 1.56492 (* 1 = 1.56492 loss)
I0520 21:22:33.019054 31010 sgd_solver.cpp:106] Iteration 3408, lr = 0.0025
I0520 21:22:41.174458 31010 solver.cpp:237] Iteration 3456, loss = 1.7912
I0520 21:22:41.174603 31010 solver.cpp:253]     Train net output #0: loss = 1.7912 (* 1 = 1.7912 loss)
I0520 21:22:41.174618 31010 sgd_solver.cpp:106] Iteration 3456, lr = 0.0025
I0520 21:22:49.331852 31010 solver.cpp:237] Iteration 3504, loss = 1.68778
I0520 21:22:49.331884 31010 solver.cpp:253]     Train net output #0: loss = 1.68778 (* 1 = 1.68778 loss)
I0520 21:22:49.331902 31010 sgd_solver.cpp:106] Iteration 3504, lr = 0.0025
I0520 21:23:19.664058 31010 solver.cpp:237] Iteration 3552, loss = 1.65093
I0520 21:23:19.664228 31010 solver.cpp:253]     Train net output #0: loss = 1.65093 (* 1 = 1.65093 loss)
I0520 21:23:19.664243 31010 sgd_solver.cpp:106] Iteration 3552, lr = 0.0025
I0520 21:23:27.825608 31010 solver.cpp:237] Iteration 3600, loss = 1.55523
I0520 21:23:27.825654 31010 solver.cpp:253]     Train net output #0: loss = 1.55523 (* 1 = 1.55523 loss)
I0520 21:23:27.825670 31010 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0520 21:23:35.982527 31010 solver.cpp:237] Iteration 3648, loss = 1.59963
I0520 21:23:35.982563 31010 solver.cpp:253]     Train net output #0: loss = 1.59963 (* 1 = 1.59963 loss)
I0520 21:23:35.982578 31010 sgd_solver.cpp:106] Iteration 3648, lr = 0.0025
I0520 21:23:44.131467 31010 solver.cpp:237] Iteration 3696, loss = 1.61845
I0520 21:23:44.131500 31010 solver.cpp:253]     Train net output #0: loss = 1.61845 (* 1 = 1.61845 loss)
I0520 21:23:44.131516 31010 sgd_solver.cpp:106] Iteration 3696, lr = 0.0025
I0520 21:23:52.295178 31010 solver.cpp:237] Iteration 3744, loss = 1.49002
I0520 21:23:52.295334 31010 solver.cpp:253]     Train net output #0: loss = 1.49002 (* 1 = 1.49002 loss)
I0520 21:23:52.295348 31010 sgd_solver.cpp:106] Iteration 3744, lr = 0.0025
I0520 21:24:00.452255 31010 solver.cpp:237] Iteration 3792, loss = 1.5289
I0520 21:24:00.452289 31010 solver.cpp:253]     Train net output #0: loss = 1.5289 (* 1 = 1.5289 loss)
I0520 21:24:00.452304 31010 sgd_solver.cpp:106] Iteration 3792, lr = 0.0025
I0520 21:24:08.609042 31010 solver.cpp:237] Iteration 3840, loss = 1.63817
I0520 21:24:08.609076 31010 solver.cpp:253]     Train net output #0: loss = 1.63817 (* 1 = 1.63817 loss)
I0520 21:24:08.609093 31010 sgd_solver.cpp:106] Iteration 3840, lr = 0.0025
I0520 21:24:12.521054 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_3864.caffemodel
I0520 21:24:12.678015 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_3864.solverstate
I0520 21:24:13.265161 31010 solver.cpp:341] Iteration 3868, Testing net (#0)
I0520 21:25:19.670641 31010 solver.cpp:409]     Test net output #0: accuracy = 0.719315
I0520 21:25:19.670825 31010 solver.cpp:409]     Test net output #1: loss = 0.971669 (* 1 = 0.971669 loss)
I0520 21:25:45.320237 31010 solver.cpp:237] Iteration 3888, loss = 1.4654
I0520 21:25:45.320288 31010 solver.cpp:253]     Train net output #0: loss = 1.4654 (* 1 = 1.4654 loss)
I0520 21:25:45.320304 31010 sgd_solver.cpp:106] Iteration 3888, lr = 0.0025
I0520 21:25:53.480063 31010 solver.cpp:237] Iteration 3936, loss = 1.61699
I0520 21:25:53.480227 31010 solver.cpp:253]     Train net output #0: loss = 1.61699 (* 1 = 1.61699 loss)
I0520 21:25:53.480242 31010 sgd_solver.cpp:106] Iteration 3936, lr = 0.0025
I0520 21:26:01.644199 31010 solver.cpp:237] Iteration 3984, loss = 1.45234
I0520 21:26:01.644233 31010 solver.cpp:253]     Train net output #0: loss = 1.45234 (* 1 = 1.45234 loss)
I0520 21:26:01.644250 31010 sgd_solver.cpp:106] Iteration 3984, lr = 0.0025
I0520 21:26:09.809653 31010 solver.cpp:237] Iteration 4032, loss = 1.54456
I0520 21:26:09.809686 31010 solver.cpp:253]     Train net output #0: loss = 1.54456 (* 1 = 1.54456 loss)
I0520 21:26:09.809702 31010 sgd_solver.cpp:106] Iteration 4032, lr = 0.0025
I0520 21:26:17.977157 31010 solver.cpp:237] Iteration 4080, loss = 1.57366
I0520 21:26:17.977195 31010 solver.cpp:253]     Train net output #0: loss = 1.57366 (* 1 = 1.57366 loss)
I0520 21:26:17.977210 31010 sgd_solver.cpp:106] Iteration 4080, lr = 0.0025
I0520 21:26:26.142797 31010 solver.cpp:237] Iteration 4128, loss = 1.58943
I0520 21:26:26.142940 31010 solver.cpp:253]     Train net output #0: loss = 1.58943 (* 1 = 1.58943 loss)
I0520 21:26:26.142953 31010 sgd_solver.cpp:106] Iteration 4128, lr = 0.0025
I0520 21:26:34.311750 31010 solver.cpp:237] Iteration 4176, loss = 1.53602
I0520 21:26:34.311784 31010 solver.cpp:253]     Train net output #0: loss = 1.53602 (* 1 = 1.53602 loss)
I0520 21:26:34.311800 31010 sgd_solver.cpp:106] Iteration 4176, lr = 0.0025
I0520 21:27:04.648241 31010 solver.cpp:237] Iteration 4224, loss = 1.5826
I0520 21:27:04.648414 31010 solver.cpp:253]     Train net output #0: loss = 1.5826 (* 1 = 1.5826 loss)
I0520 21:27:04.648429 31010 sgd_solver.cpp:106] Iteration 4224, lr = 0.0025
I0520 21:27:12.815466 31010 solver.cpp:237] Iteration 4272, loss = 1.42144
I0520 21:27:12.815508 31010 solver.cpp:253]     Train net output #0: loss = 1.42144 (* 1 = 1.42144 loss)
I0520 21:27:12.815521 31010 sgd_solver.cpp:106] Iteration 4272, lr = 0.0025
I0520 21:27:20.981379 31010 solver.cpp:237] Iteration 4320, loss = 1.46119
I0520 21:27:20.981413 31010 solver.cpp:253]     Train net output #0: loss = 1.46119 (* 1 = 1.46119 loss)
I0520 21:27:20.981428 31010 sgd_solver.cpp:106] Iteration 4320, lr = 0.0025
I0520 21:27:25.404991 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_4347.caffemodel
I0520 21:27:25.570948 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_4347.solverstate
I0520 21:27:29.227349 31010 solver.cpp:237] Iteration 4368, loss = 1.53481
I0520 21:27:29.227398 31010 solver.cpp:253]     Train net output #0: loss = 1.53481 (* 1 = 1.53481 loss)
I0520 21:27:29.227414 31010 sgd_solver.cpp:106] Iteration 4368, lr = 0.0025
I0520 21:27:37.389536 31010 solver.cpp:237] Iteration 4416, loss = 1.48379
I0520 21:27:37.389698 31010 solver.cpp:253]     Train net output #0: loss = 1.48379 (* 1 = 1.48379 loss)
I0520 21:27:37.389711 31010 sgd_solver.cpp:106] Iteration 4416, lr = 0.0025
I0520 21:27:45.553591 31010 solver.cpp:237] Iteration 4464, loss = 1.51692
I0520 21:27:45.553625 31010 solver.cpp:253]     Train net output #0: loss = 1.51692 (* 1 = 1.51692 loss)
I0520 21:27:45.553640 31010 sgd_solver.cpp:106] Iteration 4464, lr = 0.0025
I0520 21:27:53.720625 31010 solver.cpp:237] Iteration 4512, loss = 1.57186
I0520 21:27:53.720659 31010 solver.cpp:253]     Train net output #0: loss = 1.57186 (* 1 = 1.57186 loss)
I0520 21:27:53.720674 31010 sgd_solver.cpp:106] Iteration 4512, lr = 0.0025
I0520 21:28:24.127638 31010 solver.cpp:237] Iteration 4560, loss = 1.59513
I0520 21:28:24.127815 31010 solver.cpp:253]     Train net output #0: loss = 1.59513 (* 1 = 1.59513 loss)
I0520 21:28:24.127830 31010 sgd_solver.cpp:106] Iteration 4560, lr = 0.0025
I0520 21:28:32.293856 31010 solver.cpp:237] Iteration 4608, loss = 1.50411
I0520 21:28:32.293890 31010 solver.cpp:253]     Train net output #0: loss = 1.50411 (* 1 = 1.50411 loss)
I0520 21:28:32.293906 31010 sgd_solver.cpp:106] Iteration 4608, lr = 0.0025
I0520 21:28:40.455930 31010 solver.cpp:237] Iteration 4656, loss = 1.40598
I0520 21:28:40.455965 31010 solver.cpp:253]     Train net output #0: loss = 1.40598 (* 1 = 1.40598 loss)
I0520 21:28:40.455981 31010 sgd_solver.cpp:106] Iteration 4656, lr = 0.0025
I0520 21:28:48.624068 31010 solver.cpp:237] Iteration 4704, loss = 1.54681
I0520 21:28:48.624112 31010 solver.cpp:253]     Train net output #0: loss = 1.54681 (* 1 = 1.54681 loss)
I0520 21:28:48.624125 31010 sgd_solver.cpp:106] Iteration 4704, lr = 0.0025
I0520 21:28:56.788439 31010 solver.cpp:237] Iteration 4752, loss = 1.49568
I0520 21:28:56.788599 31010 solver.cpp:253]     Train net output #0: loss = 1.49568 (* 1 = 1.49568 loss)
I0520 21:28:56.788611 31010 sgd_solver.cpp:106] Iteration 4752, lr = 0.0025
I0520 21:29:04.955617 31010 solver.cpp:237] Iteration 4800, loss = 1.48645
I0520 21:29:04.955657 31010 solver.cpp:253]     Train net output #0: loss = 1.48645 (* 1 = 1.48645 loss)
I0520 21:29:04.955670 31010 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0520 21:29:09.888643 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_4830.caffemodel
I0520 21:29:10.049679 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_4830.solverstate
I0520 21:29:10.809576 31010 solver.cpp:341] Iteration 4835, Testing net (#0)
I0520 21:29:56.365456 31010 solver.cpp:409]     Test net output #0: accuracy = 0.76119
I0520 21:29:56.365625 31010 solver.cpp:409]     Test net output #1: loss = 0.863458 (* 1 = 0.863458 loss)
I0520 21:29:56.756463 31010 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_4838.caffemodel
I0520 21:29:56.918306 31010 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468_iter_4838.solverstate
I0520 21:29:56.947939 31010 solver.cpp:326] Optimization Done.
I0520 21:29:56.947968 31010 caffe.cpp:215] Optimization Done.
Application 11235400 resources: utime ~1266s, stime ~228s, Rss ~5329432, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_310_2016-05-20T11.20.44.001468.solver"
	User time (seconds): 0.54
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:01.21
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15081
	Voluntary context switches: 2759
	Involuntary context switches: 77
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
