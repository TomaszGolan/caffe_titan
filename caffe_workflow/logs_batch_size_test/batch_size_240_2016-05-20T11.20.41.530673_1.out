2805887
I0520 19:30:26.114814 20884 caffe.cpp:184] Using GPUs 0
I0520 19:30:26.539003 20884 solver.cpp:48] Initializing solver from parameters: 
test_iter: 625
test_interval: 1250
base_lr: 0.0025
display: 62
max_iter: 6250
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 625
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673.prototxt"
I0520 19:30:26.540555 20884 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673.prototxt
I0520 19:30:26.552126 20884 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 19:30:26.552186 20884 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 19:30:26.552532 20884 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 240
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 19:30:26.552712 20884 layer_factory.hpp:77] Creating layer data_hdf5
I0520 19:30:26.552736 20884 net.cpp:106] Creating Layer data_hdf5
I0520 19:30:26.552752 20884 net.cpp:411] data_hdf5 -> data
I0520 19:30:26.552783 20884 net.cpp:411] data_hdf5 -> label
I0520 19:30:26.552815 20884 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 19:30:26.553939 20884 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 19:30:26.556138 20884 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 19:30:48.098613 20884 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 19:30:48.103699 20884 net.cpp:150] Setting up data_hdf5
I0520 19:30:48.103744 20884 net.cpp:157] Top shape: 240 1 127 50 (1524000)
I0520 19:30:48.103759 20884 net.cpp:157] Top shape: 240 (240)
I0520 19:30:48.103768 20884 net.cpp:165] Memory required for data: 6096960
I0520 19:30:48.103782 20884 layer_factory.hpp:77] Creating layer conv1
I0520 19:30:48.103816 20884 net.cpp:106] Creating Layer conv1
I0520 19:30:48.103827 20884 net.cpp:454] conv1 <- data
I0520 19:30:48.103849 20884 net.cpp:411] conv1 -> conv1
I0520 19:30:48.469182 20884 net.cpp:150] Setting up conv1
I0520 19:30:48.469228 20884 net.cpp:157] Top shape: 240 12 120 48 (16588800)
I0520 19:30:48.469239 20884 net.cpp:165] Memory required for data: 72452160
I0520 19:30:48.469270 20884 layer_factory.hpp:77] Creating layer relu1
I0520 19:30:48.469290 20884 net.cpp:106] Creating Layer relu1
I0520 19:30:48.469301 20884 net.cpp:454] relu1 <- conv1
I0520 19:30:48.469316 20884 net.cpp:397] relu1 -> conv1 (in-place)
I0520 19:30:48.469842 20884 net.cpp:150] Setting up relu1
I0520 19:30:48.469859 20884 net.cpp:157] Top shape: 240 12 120 48 (16588800)
I0520 19:30:48.469869 20884 net.cpp:165] Memory required for data: 138807360
I0520 19:30:48.469879 20884 layer_factory.hpp:77] Creating layer pool1
I0520 19:30:48.469897 20884 net.cpp:106] Creating Layer pool1
I0520 19:30:48.469907 20884 net.cpp:454] pool1 <- conv1
I0520 19:30:48.469920 20884 net.cpp:411] pool1 -> pool1
I0520 19:30:48.470000 20884 net.cpp:150] Setting up pool1
I0520 19:30:48.470015 20884 net.cpp:157] Top shape: 240 12 60 48 (8294400)
I0520 19:30:48.470023 20884 net.cpp:165] Memory required for data: 171984960
I0520 19:30:48.470034 20884 layer_factory.hpp:77] Creating layer conv2
I0520 19:30:48.470057 20884 net.cpp:106] Creating Layer conv2
I0520 19:30:48.470067 20884 net.cpp:454] conv2 <- pool1
I0520 19:30:48.470082 20884 net.cpp:411] conv2 -> conv2
I0520 19:30:48.472757 20884 net.cpp:150] Setting up conv2
I0520 19:30:48.472784 20884 net.cpp:157] Top shape: 240 20 54 46 (11923200)
I0520 19:30:48.472795 20884 net.cpp:165] Memory required for data: 219677760
I0520 19:30:48.472815 20884 layer_factory.hpp:77] Creating layer relu2
I0520 19:30:48.472829 20884 net.cpp:106] Creating Layer relu2
I0520 19:30:48.472839 20884 net.cpp:454] relu2 <- conv2
I0520 19:30:48.472851 20884 net.cpp:397] relu2 -> conv2 (in-place)
I0520 19:30:48.473181 20884 net.cpp:150] Setting up relu2
I0520 19:30:48.473196 20884 net.cpp:157] Top shape: 240 20 54 46 (11923200)
I0520 19:30:48.473206 20884 net.cpp:165] Memory required for data: 267370560
I0520 19:30:48.473215 20884 layer_factory.hpp:77] Creating layer pool2
I0520 19:30:48.473228 20884 net.cpp:106] Creating Layer pool2
I0520 19:30:48.473238 20884 net.cpp:454] pool2 <- conv2
I0520 19:30:48.473263 20884 net.cpp:411] pool2 -> pool2
I0520 19:30:48.473333 20884 net.cpp:150] Setting up pool2
I0520 19:30:48.473346 20884 net.cpp:157] Top shape: 240 20 27 46 (5961600)
I0520 19:30:48.473356 20884 net.cpp:165] Memory required for data: 291216960
I0520 19:30:48.473364 20884 layer_factory.hpp:77] Creating layer conv3
I0520 19:30:48.473382 20884 net.cpp:106] Creating Layer conv3
I0520 19:30:48.473392 20884 net.cpp:454] conv3 <- pool2
I0520 19:30:48.473407 20884 net.cpp:411] conv3 -> conv3
I0520 19:30:48.475358 20884 net.cpp:150] Setting up conv3
I0520 19:30:48.475383 20884 net.cpp:157] Top shape: 240 28 22 44 (6504960)
I0520 19:30:48.475392 20884 net.cpp:165] Memory required for data: 317236800
I0520 19:30:48.475412 20884 layer_factory.hpp:77] Creating layer relu3
I0520 19:30:48.475428 20884 net.cpp:106] Creating Layer relu3
I0520 19:30:48.475438 20884 net.cpp:454] relu3 <- conv3
I0520 19:30:48.475450 20884 net.cpp:397] relu3 -> conv3 (in-place)
I0520 19:30:48.475919 20884 net.cpp:150] Setting up relu3
I0520 19:30:48.475936 20884 net.cpp:157] Top shape: 240 28 22 44 (6504960)
I0520 19:30:48.475946 20884 net.cpp:165] Memory required for data: 343256640
I0520 19:30:48.475957 20884 layer_factory.hpp:77] Creating layer pool3
I0520 19:30:48.475970 20884 net.cpp:106] Creating Layer pool3
I0520 19:30:48.475980 20884 net.cpp:454] pool3 <- conv3
I0520 19:30:48.475992 20884 net.cpp:411] pool3 -> pool3
I0520 19:30:48.476061 20884 net.cpp:150] Setting up pool3
I0520 19:30:48.476074 20884 net.cpp:157] Top shape: 240 28 11 44 (3252480)
I0520 19:30:48.476084 20884 net.cpp:165] Memory required for data: 356266560
I0520 19:30:48.476092 20884 layer_factory.hpp:77] Creating layer conv4
I0520 19:30:48.476110 20884 net.cpp:106] Creating Layer conv4
I0520 19:30:48.476120 20884 net.cpp:454] conv4 <- pool3
I0520 19:30:48.476135 20884 net.cpp:411] conv4 -> conv4
I0520 19:30:48.478885 20884 net.cpp:150] Setting up conv4
I0520 19:30:48.478914 20884 net.cpp:157] Top shape: 240 36 6 42 (2177280)
I0520 19:30:48.478924 20884 net.cpp:165] Memory required for data: 364975680
I0520 19:30:48.478938 20884 layer_factory.hpp:77] Creating layer relu4
I0520 19:30:48.478952 20884 net.cpp:106] Creating Layer relu4
I0520 19:30:48.478963 20884 net.cpp:454] relu4 <- conv4
I0520 19:30:48.478976 20884 net.cpp:397] relu4 -> conv4 (in-place)
I0520 19:30:48.479439 20884 net.cpp:150] Setting up relu4
I0520 19:30:48.479455 20884 net.cpp:157] Top shape: 240 36 6 42 (2177280)
I0520 19:30:48.479465 20884 net.cpp:165] Memory required for data: 373684800
I0520 19:30:48.479475 20884 layer_factory.hpp:77] Creating layer pool4
I0520 19:30:48.479488 20884 net.cpp:106] Creating Layer pool4
I0520 19:30:48.479498 20884 net.cpp:454] pool4 <- conv4
I0520 19:30:48.479511 20884 net.cpp:411] pool4 -> pool4
I0520 19:30:48.479579 20884 net.cpp:150] Setting up pool4
I0520 19:30:48.479593 20884 net.cpp:157] Top shape: 240 36 3 42 (1088640)
I0520 19:30:48.479604 20884 net.cpp:165] Memory required for data: 378039360
I0520 19:30:48.479614 20884 layer_factory.hpp:77] Creating layer ip1
I0520 19:30:48.479635 20884 net.cpp:106] Creating Layer ip1
I0520 19:30:48.479645 20884 net.cpp:454] ip1 <- pool4
I0520 19:30:48.479657 20884 net.cpp:411] ip1 -> ip1
I0520 19:30:48.495057 20884 net.cpp:150] Setting up ip1
I0520 19:30:48.495085 20884 net.cpp:157] Top shape: 240 196 (47040)
I0520 19:30:48.495098 20884 net.cpp:165] Memory required for data: 378227520
I0520 19:30:48.495120 20884 layer_factory.hpp:77] Creating layer relu5
I0520 19:30:48.495134 20884 net.cpp:106] Creating Layer relu5
I0520 19:30:48.495144 20884 net.cpp:454] relu5 <- ip1
I0520 19:30:48.495157 20884 net.cpp:397] relu5 -> ip1 (in-place)
I0520 19:30:48.495499 20884 net.cpp:150] Setting up relu5
I0520 19:30:48.495513 20884 net.cpp:157] Top shape: 240 196 (47040)
I0520 19:30:48.495524 20884 net.cpp:165] Memory required for data: 378415680
I0520 19:30:48.495534 20884 layer_factory.hpp:77] Creating layer drop1
I0520 19:30:48.495556 20884 net.cpp:106] Creating Layer drop1
I0520 19:30:48.495568 20884 net.cpp:454] drop1 <- ip1
I0520 19:30:48.495591 20884 net.cpp:397] drop1 -> ip1 (in-place)
I0520 19:30:48.495641 20884 net.cpp:150] Setting up drop1
I0520 19:30:48.495654 20884 net.cpp:157] Top shape: 240 196 (47040)
I0520 19:30:48.495666 20884 net.cpp:165] Memory required for data: 378603840
I0520 19:30:48.495674 20884 layer_factory.hpp:77] Creating layer ip2
I0520 19:30:48.495692 20884 net.cpp:106] Creating Layer ip2
I0520 19:30:48.495702 20884 net.cpp:454] ip2 <- ip1
I0520 19:30:48.495714 20884 net.cpp:411] ip2 -> ip2
I0520 19:30:48.496177 20884 net.cpp:150] Setting up ip2
I0520 19:30:48.496191 20884 net.cpp:157] Top shape: 240 98 (23520)
I0520 19:30:48.496201 20884 net.cpp:165] Memory required for data: 378697920
I0520 19:30:48.496215 20884 layer_factory.hpp:77] Creating layer relu6
I0520 19:30:48.496227 20884 net.cpp:106] Creating Layer relu6
I0520 19:30:48.496237 20884 net.cpp:454] relu6 <- ip2
I0520 19:30:48.496249 20884 net.cpp:397] relu6 -> ip2 (in-place)
I0520 19:30:48.496770 20884 net.cpp:150] Setting up relu6
I0520 19:30:48.496786 20884 net.cpp:157] Top shape: 240 98 (23520)
I0520 19:30:48.496796 20884 net.cpp:165] Memory required for data: 378792000
I0520 19:30:48.496808 20884 layer_factory.hpp:77] Creating layer drop2
I0520 19:30:48.496819 20884 net.cpp:106] Creating Layer drop2
I0520 19:30:48.496829 20884 net.cpp:454] drop2 <- ip2
I0520 19:30:48.496841 20884 net.cpp:397] drop2 -> ip2 (in-place)
I0520 19:30:48.496884 20884 net.cpp:150] Setting up drop2
I0520 19:30:48.496897 20884 net.cpp:157] Top shape: 240 98 (23520)
I0520 19:30:48.496909 20884 net.cpp:165] Memory required for data: 378886080
I0520 19:30:48.496919 20884 layer_factory.hpp:77] Creating layer ip3
I0520 19:30:48.496932 20884 net.cpp:106] Creating Layer ip3
I0520 19:30:48.496942 20884 net.cpp:454] ip3 <- ip2
I0520 19:30:48.496954 20884 net.cpp:411] ip3 -> ip3
I0520 19:30:48.497165 20884 net.cpp:150] Setting up ip3
I0520 19:30:48.497179 20884 net.cpp:157] Top shape: 240 11 (2640)
I0520 19:30:48.497189 20884 net.cpp:165] Memory required for data: 378896640
I0520 19:30:48.497203 20884 layer_factory.hpp:77] Creating layer drop3
I0520 19:30:48.497215 20884 net.cpp:106] Creating Layer drop3
I0520 19:30:48.497225 20884 net.cpp:454] drop3 <- ip3
I0520 19:30:48.497236 20884 net.cpp:397] drop3 -> ip3 (in-place)
I0520 19:30:48.497277 20884 net.cpp:150] Setting up drop3
I0520 19:30:48.497289 20884 net.cpp:157] Top shape: 240 11 (2640)
I0520 19:30:48.497300 20884 net.cpp:165] Memory required for data: 378907200
I0520 19:30:48.497310 20884 layer_factory.hpp:77] Creating layer loss
I0520 19:30:48.497329 20884 net.cpp:106] Creating Layer loss
I0520 19:30:48.497339 20884 net.cpp:454] loss <- ip3
I0520 19:30:48.497349 20884 net.cpp:454] loss <- label
I0520 19:30:48.497362 20884 net.cpp:411] loss -> loss
I0520 19:30:48.497380 20884 layer_factory.hpp:77] Creating layer loss
I0520 19:30:48.498034 20884 net.cpp:150] Setting up loss
I0520 19:30:48.498055 20884 net.cpp:157] Top shape: (1)
I0520 19:30:48.498069 20884 net.cpp:160]     with loss weight 1
I0520 19:30:48.498111 20884 net.cpp:165] Memory required for data: 378907204
I0520 19:30:48.498121 20884 net.cpp:226] loss needs backward computation.
I0520 19:30:48.498132 20884 net.cpp:226] drop3 needs backward computation.
I0520 19:30:48.498139 20884 net.cpp:226] ip3 needs backward computation.
I0520 19:30:48.498150 20884 net.cpp:226] drop2 needs backward computation.
I0520 19:30:48.498160 20884 net.cpp:226] relu6 needs backward computation.
I0520 19:30:48.498170 20884 net.cpp:226] ip2 needs backward computation.
I0520 19:30:48.498180 20884 net.cpp:226] drop1 needs backward computation.
I0520 19:30:48.498190 20884 net.cpp:226] relu5 needs backward computation.
I0520 19:30:48.498198 20884 net.cpp:226] ip1 needs backward computation.
I0520 19:30:48.498209 20884 net.cpp:226] pool4 needs backward computation.
I0520 19:30:48.498219 20884 net.cpp:226] relu4 needs backward computation.
I0520 19:30:48.498229 20884 net.cpp:226] conv4 needs backward computation.
I0520 19:30:48.498239 20884 net.cpp:226] pool3 needs backward computation.
I0520 19:30:48.498260 20884 net.cpp:226] relu3 needs backward computation.
I0520 19:30:48.498270 20884 net.cpp:226] conv3 needs backward computation.
I0520 19:30:48.498283 20884 net.cpp:226] pool2 needs backward computation.
I0520 19:30:48.498293 20884 net.cpp:226] relu2 needs backward computation.
I0520 19:30:48.498303 20884 net.cpp:226] conv2 needs backward computation.
I0520 19:30:48.498314 20884 net.cpp:226] pool1 needs backward computation.
I0520 19:30:48.498324 20884 net.cpp:226] relu1 needs backward computation.
I0520 19:30:48.498334 20884 net.cpp:226] conv1 needs backward computation.
I0520 19:30:48.498345 20884 net.cpp:228] data_hdf5 does not need backward computation.
I0520 19:30:48.498355 20884 net.cpp:270] This network produces output loss
I0520 19:30:48.498378 20884 net.cpp:283] Network initialization done.
I0520 19:30:48.499932 20884 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673.prototxt
I0520 19:30:48.500003 20884 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 19:30:48.500360 20884 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 240
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 19:30:48.500547 20884 layer_factory.hpp:77] Creating layer data_hdf5
I0520 19:30:48.500562 20884 net.cpp:106] Creating Layer data_hdf5
I0520 19:30:48.500574 20884 net.cpp:411] data_hdf5 -> data
I0520 19:30:48.500591 20884 net.cpp:411] data_hdf5 -> label
I0520 19:30:48.500607 20884 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 19:30:48.501785 20884 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 19:31:09.791151 20884 net.cpp:150] Setting up data_hdf5
I0520 19:31:09.791326 20884 net.cpp:157] Top shape: 240 1 127 50 (1524000)
I0520 19:31:09.791340 20884 net.cpp:157] Top shape: 240 (240)
I0520 19:31:09.791350 20884 net.cpp:165] Memory required for data: 6096960
I0520 19:31:09.791364 20884 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 19:31:09.791393 20884 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 19:31:09.791404 20884 net.cpp:454] label_data_hdf5_1_split <- label
I0520 19:31:09.791419 20884 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 19:31:09.791440 20884 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 19:31:09.791512 20884 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 19:31:09.791525 20884 net.cpp:157] Top shape: 240 (240)
I0520 19:31:09.791537 20884 net.cpp:157] Top shape: 240 (240)
I0520 19:31:09.791546 20884 net.cpp:165] Memory required for data: 6098880
I0520 19:31:09.791556 20884 layer_factory.hpp:77] Creating layer conv1
I0520 19:31:09.791576 20884 net.cpp:106] Creating Layer conv1
I0520 19:31:09.791587 20884 net.cpp:454] conv1 <- data
I0520 19:31:09.791602 20884 net.cpp:411] conv1 -> conv1
I0520 19:31:09.793550 20884 net.cpp:150] Setting up conv1
I0520 19:31:09.793575 20884 net.cpp:157] Top shape: 240 12 120 48 (16588800)
I0520 19:31:09.793586 20884 net.cpp:165] Memory required for data: 72454080
I0520 19:31:09.793606 20884 layer_factory.hpp:77] Creating layer relu1
I0520 19:31:09.793622 20884 net.cpp:106] Creating Layer relu1
I0520 19:31:09.793632 20884 net.cpp:454] relu1 <- conv1
I0520 19:31:09.793645 20884 net.cpp:397] relu1 -> conv1 (in-place)
I0520 19:31:09.794145 20884 net.cpp:150] Setting up relu1
I0520 19:31:09.794162 20884 net.cpp:157] Top shape: 240 12 120 48 (16588800)
I0520 19:31:09.794172 20884 net.cpp:165] Memory required for data: 138809280
I0520 19:31:09.794181 20884 layer_factory.hpp:77] Creating layer pool1
I0520 19:31:09.794198 20884 net.cpp:106] Creating Layer pool1
I0520 19:31:09.794209 20884 net.cpp:454] pool1 <- conv1
I0520 19:31:09.794221 20884 net.cpp:411] pool1 -> pool1
I0520 19:31:09.794296 20884 net.cpp:150] Setting up pool1
I0520 19:31:09.794311 20884 net.cpp:157] Top shape: 240 12 60 48 (8294400)
I0520 19:31:09.794322 20884 net.cpp:165] Memory required for data: 171986880
I0520 19:31:09.794332 20884 layer_factory.hpp:77] Creating layer conv2
I0520 19:31:09.794349 20884 net.cpp:106] Creating Layer conv2
I0520 19:31:09.794361 20884 net.cpp:454] conv2 <- pool1
I0520 19:31:09.794375 20884 net.cpp:411] conv2 -> conv2
I0520 19:31:09.796290 20884 net.cpp:150] Setting up conv2
I0520 19:31:09.796314 20884 net.cpp:157] Top shape: 240 20 54 46 (11923200)
I0520 19:31:09.796325 20884 net.cpp:165] Memory required for data: 219679680
I0520 19:31:09.796342 20884 layer_factory.hpp:77] Creating layer relu2
I0520 19:31:09.796356 20884 net.cpp:106] Creating Layer relu2
I0520 19:31:09.796366 20884 net.cpp:454] relu2 <- conv2
I0520 19:31:09.796380 20884 net.cpp:397] relu2 -> conv2 (in-place)
I0520 19:31:09.796712 20884 net.cpp:150] Setting up relu2
I0520 19:31:09.796725 20884 net.cpp:157] Top shape: 240 20 54 46 (11923200)
I0520 19:31:09.796736 20884 net.cpp:165] Memory required for data: 267372480
I0520 19:31:09.796746 20884 layer_factory.hpp:77] Creating layer pool2
I0520 19:31:09.796758 20884 net.cpp:106] Creating Layer pool2
I0520 19:31:09.796768 20884 net.cpp:454] pool2 <- conv2
I0520 19:31:09.796780 20884 net.cpp:411] pool2 -> pool2
I0520 19:31:09.796854 20884 net.cpp:150] Setting up pool2
I0520 19:31:09.796866 20884 net.cpp:157] Top shape: 240 20 27 46 (5961600)
I0520 19:31:09.796876 20884 net.cpp:165] Memory required for data: 291218880
I0520 19:31:09.796886 20884 layer_factory.hpp:77] Creating layer conv3
I0520 19:31:09.796903 20884 net.cpp:106] Creating Layer conv3
I0520 19:31:09.796914 20884 net.cpp:454] conv3 <- pool2
I0520 19:31:09.796928 20884 net.cpp:411] conv3 -> conv3
I0520 19:31:09.798898 20884 net.cpp:150] Setting up conv3
I0520 19:31:09.798921 20884 net.cpp:157] Top shape: 240 28 22 44 (6504960)
I0520 19:31:09.798934 20884 net.cpp:165] Memory required for data: 317238720
I0520 19:31:09.798967 20884 layer_factory.hpp:77] Creating layer relu3
I0520 19:31:09.798980 20884 net.cpp:106] Creating Layer relu3
I0520 19:31:09.798991 20884 net.cpp:454] relu3 <- conv3
I0520 19:31:09.799005 20884 net.cpp:397] relu3 -> conv3 (in-place)
I0520 19:31:09.799474 20884 net.cpp:150] Setting up relu3
I0520 19:31:09.799490 20884 net.cpp:157] Top shape: 240 28 22 44 (6504960)
I0520 19:31:09.799500 20884 net.cpp:165] Memory required for data: 343258560
I0520 19:31:09.799511 20884 layer_factory.hpp:77] Creating layer pool3
I0520 19:31:09.799525 20884 net.cpp:106] Creating Layer pool3
I0520 19:31:09.799535 20884 net.cpp:454] pool3 <- conv3
I0520 19:31:09.799546 20884 net.cpp:411] pool3 -> pool3
I0520 19:31:09.799618 20884 net.cpp:150] Setting up pool3
I0520 19:31:09.799631 20884 net.cpp:157] Top shape: 240 28 11 44 (3252480)
I0520 19:31:09.799641 20884 net.cpp:165] Memory required for data: 356268480
I0520 19:31:09.799649 20884 layer_factory.hpp:77] Creating layer conv4
I0520 19:31:09.799666 20884 net.cpp:106] Creating Layer conv4
I0520 19:31:09.799677 20884 net.cpp:454] conv4 <- pool3
I0520 19:31:09.799691 20884 net.cpp:411] conv4 -> conv4
I0520 19:31:09.801754 20884 net.cpp:150] Setting up conv4
I0520 19:31:09.801776 20884 net.cpp:157] Top shape: 240 36 6 42 (2177280)
I0520 19:31:09.801789 20884 net.cpp:165] Memory required for data: 364977600
I0520 19:31:09.801803 20884 layer_factory.hpp:77] Creating layer relu4
I0520 19:31:09.801817 20884 net.cpp:106] Creating Layer relu4
I0520 19:31:09.801827 20884 net.cpp:454] relu4 <- conv4
I0520 19:31:09.801841 20884 net.cpp:397] relu4 -> conv4 (in-place)
I0520 19:31:09.802320 20884 net.cpp:150] Setting up relu4
I0520 19:31:09.802336 20884 net.cpp:157] Top shape: 240 36 6 42 (2177280)
I0520 19:31:09.802346 20884 net.cpp:165] Memory required for data: 373686720
I0520 19:31:09.802356 20884 layer_factory.hpp:77] Creating layer pool4
I0520 19:31:09.802369 20884 net.cpp:106] Creating Layer pool4
I0520 19:31:09.802381 20884 net.cpp:454] pool4 <- conv4
I0520 19:31:09.802393 20884 net.cpp:411] pool4 -> pool4
I0520 19:31:09.802464 20884 net.cpp:150] Setting up pool4
I0520 19:31:09.802479 20884 net.cpp:157] Top shape: 240 36 3 42 (1088640)
I0520 19:31:09.802487 20884 net.cpp:165] Memory required for data: 378041280
I0520 19:31:09.802496 20884 layer_factory.hpp:77] Creating layer ip1
I0520 19:31:09.802512 20884 net.cpp:106] Creating Layer ip1
I0520 19:31:09.802522 20884 net.cpp:454] ip1 <- pool4
I0520 19:31:09.802536 20884 net.cpp:411] ip1 -> ip1
I0520 19:31:09.817930 20884 net.cpp:150] Setting up ip1
I0520 19:31:09.817958 20884 net.cpp:157] Top shape: 240 196 (47040)
I0520 19:31:09.817970 20884 net.cpp:165] Memory required for data: 378229440
I0520 19:31:09.817992 20884 layer_factory.hpp:77] Creating layer relu5
I0520 19:31:09.818007 20884 net.cpp:106] Creating Layer relu5
I0520 19:31:09.818017 20884 net.cpp:454] relu5 <- ip1
I0520 19:31:09.818032 20884 net.cpp:397] relu5 -> ip1 (in-place)
I0520 19:31:09.818377 20884 net.cpp:150] Setting up relu5
I0520 19:31:09.818392 20884 net.cpp:157] Top shape: 240 196 (47040)
I0520 19:31:09.818400 20884 net.cpp:165] Memory required for data: 378417600
I0520 19:31:09.818410 20884 layer_factory.hpp:77] Creating layer drop1
I0520 19:31:09.818429 20884 net.cpp:106] Creating Layer drop1
I0520 19:31:09.818439 20884 net.cpp:454] drop1 <- ip1
I0520 19:31:09.818452 20884 net.cpp:397] drop1 -> ip1 (in-place)
I0520 19:31:09.818500 20884 net.cpp:150] Setting up drop1
I0520 19:31:09.818512 20884 net.cpp:157] Top shape: 240 196 (47040)
I0520 19:31:09.818522 20884 net.cpp:165] Memory required for data: 378605760
I0520 19:31:09.818532 20884 layer_factory.hpp:77] Creating layer ip2
I0520 19:31:09.818547 20884 net.cpp:106] Creating Layer ip2
I0520 19:31:09.818557 20884 net.cpp:454] ip2 <- ip1
I0520 19:31:09.818569 20884 net.cpp:411] ip2 -> ip2
I0520 19:31:09.819049 20884 net.cpp:150] Setting up ip2
I0520 19:31:09.819062 20884 net.cpp:157] Top shape: 240 98 (23520)
I0520 19:31:09.819072 20884 net.cpp:165] Memory required for data: 378699840
I0520 19:31:09.819099 20884 layer_factory.hpp:77] Creating layer relu6
I0520 19:31:09.819113 20884 net.cpp:106] Creating Layer relu6
I0520 19:31:09.819123 20884 net.cpp:454] relu6 <- ip2
I0520 19:31:09.819135 20884 net.cpp:397] relu6 -> ip2 (in-place)
I0520 19:31:09.819666 20884 net.cpp:150] Setting up relu6
I0520 19:31:09.819682 20884 net.cpp:157] Top shape: 240 98 (23520)
I0520 19:31:09.819692 20884 net.cpp:165] Memory required for data: 378793920
I0520 19:31:09.819702 20884 layer_factory.hpp:77] Creating layer drop2
I0520 19:31:09.819716 20884 net.cpp:106] Creating Layer drop2
I0520 19:31:09.819726 20884 net.cpp:454] drop2 <- ip2
I0520 19:31:09.819739 20884 net.cpp:397] drop2 -> ip2 (in-place)
I0520 19:31:09.819784 20884 net.cpp:150] Setting up drop2
I0520 19:31:09.819797 20884 net.cpp:157] Top shape: 240 98 (23520)
I0520 19:31:09.819808 20884 net.cpp:165] Memory required for data: 378888000
I0520 19:31:09.819818 20884 layer_factory.hpp:77] Creating layer ip3
I0520 19:31:09.819831 20884 net.cpp:106] Creating Layer ip3
I0520 19:31:09.819841 20884 net.cpp:454] ip3 <- ip2
I0520 19:31:09.819855 20884 net.cpp:411] ip3 -> ip3
I0520 19:31:09.820080 20884 net.cpp:150] Setting up ip3
I0520 19:31:09.820093 20884 net.cpp:157] Top shape: 240 11 (2640)
I0520 19:31:09.820103 20884 net.cpp:165] Memory required for data: 378898560
I0520 19:31:09.820118 20884 layer_factory.hpp:77] Creating layer drop3
I0520 19:31:09.820132 20884 net.cpp:106] Creating Layer drop3
I0520 19:31:09.820142 20884 net.cpp:454] drop3 <- ip3
I0520 19:31:09.820155 20884 net.cpp:397] drop3 -> ip3 (in-place)
I0520 19:31:09.820196 20884 net.cpp:150] Setting up drop3
I0520 19:31:09.820209 20884 net.cpp:157] Top shape: 240 11 (2640)
I0520 19:31:09.820219 20884 net.cpp:165] Memory required for data: 378909120
I0520 19:31:09.820228 20884 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 19:31:09.820241 20884 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 19:31:09.820251 20884 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 19:31:09.820264 20884 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 19:31:09.820279 20884 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 19:31:09.820353 20884 net.cpp:150] Setting up ip3_drop3_0_split
I0520 19:31:09.820365 20884 net.cpp:157] Top shape: 240 11 (2640)
I0520 19:31:09.820379 20884 net.cpp:157] Top shape: 240 11 (2640)
I0520 19:31:09.820389 20884 net.cpp:165] Memory required for data: 378930240
I0520 19:31:09.820399 20884 layer_factory.hpp:77] Creating layer accuracy
I0520 19:31:09.820420 20884 net.cpp:106] Creating Layer accuracy
I0520 19:31:09.820430 20884 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 19:31:09.820441 20884 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 19:31:09.820456 20884 net.cpp:411] accuracy -> accuracy
I0520 19:31:09.820478 20884 net.cpp:150] Setting up accuracy
I0520 19:31:09.820492 20884 net.cpp:157] Top shape: (1)
I0520 19:31:09.820502 20884 net.cpp:165] Memory required for data: 378930244
I0520 19:31:09.820511 20884 layer_factory.hpp:77] Creating layer loss
I0520 19:31:09.820524 20884 net.cpp:106] Creating Layer loss
I0520 19:31:09.820534 20884 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 19:31:09.820545 20884 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 19:31:09.820559 20884 net.cpp:411] loss -> loss
I0520 19:31:09.820575 20884 layer_factory.hpp:77] Creating layer loss
I0520 19:31:09.821058 20884 net.cpp:150] Setting up loss
I0520 19:31:09.821072 20884 net.cpp:157] Top shape: (1)
I0520 19:31:09.821082 20884 net.cpp:160]     with loss weight 1
I0520 19:31:09.821101 20884 net.cpp:165] Memory required for data: 378930248
I0520 19:31:09.821111 20884 net.cpp:226] loss needs backward computation.
I0520 19:31:09.821122 20884 net.cpp:228] accuracy does not need backward computation.
I0520 19:31:09.821135 20884 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 19:31:09.821144 20884 net.cpp:226] drop3 needs backward computation.
I0520 19:31:09.821156 20884 net.cpp:226] ip3 needs backward computation.
I0520 19:31:09.821166 20884 net.cpp:226] drop2 needs backward computation.
I0520 19:31:09.821184 20884 net.cpp:226] relu6 needs backward computation.
I0520 19:31:09.821193 20884 net.cpp:226] ip2 needs backward computation.
I0520 19:31:09.821204 20884 net.cpp:226] drop1 needs backward computation.
I0520 19:31:09.821213 20884 net.cpp:226] relu5 needs backward computation.
I0520 19:31:09.821223 20884 net.cpp:226] ip1 needs backward computation.
I0520 19:31:09.821233 20884 net.cpp:226] pool4 needs backward computation.
I0520 19:31:09.821244 20884 net.cpp:226] relu4 needs backward computation.
I0520 19:31:09.821252 20884 net.cpp:226] conv4 needs backward computation.
I0520 19:31:09.821262 20884 net.cpp:226] pool3 needs backward computation.
I0520 19:31:09.821274 20884 net.cpp:226] relu3 needs backward computation.
I0520 19:31:09.821283 20884 net.cpp:226] conv3 needs backward computation.
I0520 19:31:09.821293 20884 net.cpp:226] pool2 needs backward computation.
I0520 19:31:09.821305 20884 net.cpp:226] relu2 needs backward computation.
I0520 19:31:09.821313 20884 net.cpp:226] conv2 needs backward computation.
I0520 19:31:09.821323 20884 net.cpp:226] pool1 needs backward computation.
I0520 19:31:09.821334 20884 net.cpp:226] relu1 needs backward computation.
I0520 19:31:09.821344 20884 net.cpp:226] conv1 needs backward computation.
I0520 19:31:09.821355 20884 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 19:31:09.821367 20884 net.cpp:228] data_hdf5 does not need backward computation.
I0520 19:31:09.821378 20884 net.cpp:270] This network produces output accuracy
I0520 19:31:09.821388 20884 net.cpp:270] This network produces output loss
I0520 19:31:09.821418 20884 net.cpp:283] Network initialization done.
I0520 19:31:09.821558 20884 solver.cpp:60] Solver scaffolding done.
I0520 19:31:09.822692 20884 caffe.cpp:212] Starting Optimization
I0520 19:31:09.822710 20884 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 19:31:09.822724 20884 solver.cpp:289] Learning Rate Policy: fixed
I0520 19:31:09.823940 20884 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 19:31:56.241022 20884 solver.cpp:409]     Test net output #0: accuracy = 0.0736867
I0520 19:31:56.241183 20884 solver.cpp:409]     Test net output #1: loss = 2.3992 (* 1 = 2.3992 loss)
I0520 19:31:56.297000 20884 solver.cpp:237] Iteration 0, loss = 2.40027
I0520 19:31:56.297037 20884 solver.cpp:253]     Train net output #0: loss = 2.40027 (* 1 = 2.40027 loss)
I0520 19:31:56.297055 20884 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 19:32:04.434736 20884 solver.cpp:237] Iteration 62, loss = 2.35543
I0520 19:32:04.434770 20884 solver.cpp:253]     Train net output #0: loss = 2.35543 (* 1 = 2.35543 loss)
I0520 19:32:04.434787 20884 sgd_solver.cpp:106] Iteration 62, lr = 0.0025
I0520 19:32:12.570950 20884 solver.cpp:237] Iteration 124, loss = 2.30769
I0520 19:32:12.570981 20884 solver.cpp:253]     Train net output #0: loss = 2.30769 (* 1 = 2.30769 loss)
I0520 19:32:12.570998 20884 sgd_solver.cpp:106] Iteration 124, lr = 0.0025
I0520 19:32:20.707070 20884 solver.cpp:237] Iteration 186, loss = 2.3075
I0520 19:32:20.707111 20884 solver.cpp:253]     Train net output #0: loss = 2.3075 (* 1 = 2.3075 loss)
I0520 19:32:20.707131 20884 sgd_solver.cpp:106] Iteration 186, lr = 0.0025
I0520 19:32:28.841946 20884 solver.cpp:237] Iteration 248, loss = 2.30749
I0520 19:32:28.842092 20884 solver.cpp:253]     Train net output #0: loss = 2.30749 (* 1 = 2.30749 loss)
I0520 19:32:28.842104 20884 sgd_solver.cpp:106] Iteration 248, lr = 0.0025
I0520 19:32:36.979825 20884 solver.cpp:237] Iteration 310, loss = 2.14315
I0520 19:32:36.979857 20884 solver.cpp:253]     Train net output #0: loss = 2.14315 (* 1 = 2.14315 loss)
I0520 19:32:36.979876 20884 sgd_solver.cpp:106] Iteration 310, lr = 0.0025
I0520 19:32:45.119706 20884 solver.cpp:237] Iteration 372, loss = 2.13243
I0520 19:32:45.119746 20884 solver.cpp:253]     Train net output #0: loss = 2.13243 (* 1 = 2.13243 loss)
I0520 19:32:45.119767 20884 sgd_solver.cpp:106] Iteration 372, lr = 0.0025
I0520 19:33:15.400429 20884 solver.cpp:237] Iteration 434, loss = 2.1147
I0520 19:33:15.400589 20884 solver.cpp:253]     Train net output #0: loss = 2.1147 (* 1 = 2.1147 loss)
I0520 19:33:15.400604 20884 sgd_solver.cpp:106] Iteration 434, lr = 0.0025
I0520 19:33:23.541754 20884 solver.cpp:237] Iteration 496, loss = 2.06311
I0520 19:33:23.541785 20884 solver.cpp:253]     Train net output #0: loss = 2.06311 (* 1 = 2.06311 loss)
I0520 19:33:23.541797 20884 sgd_solver.cpp:106] Iteration 496, lr = 0.0025
I0520 19:33:31.682312 20884 solver.cpp:237] Iteration 558, loss = 1.88112
I0520 19:33:31.682356 20884 solver.cpp:253]     Train net output #0: loss = 1.88112 (* 1 = 1.88112 loss)
I0520 19:33:31.682374 20884 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0520 19:33:39.822890 20884 solver.cpp:237] Iteration 620, loss = 1.85764
I0520 19:33:39.822923 20884 solver.cpp:253]     Train net output #0: loss = 1.85764 (* 1 = 1.85764 loss)
I0520 19:33:39.822940 20884 sgd_solver.cpp:106] Iteration 620, lr = 0.0025
I0520 19:33:40.348428 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_625.caffemodel
I0520 19:33:40.481969 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_625.solverstate
I0520 19:33:48.037349 20884 solver.cpp:237] Iteration 682, loss = 2.00535
I0520 19:33:48.037500 20884 solver.cpp:253]     Train net output #0: loss = 2.00535 (* 1 = 2.00535 loss)
I0520 19:33:48.037514 20884 sgd_solver.cpp:106] Iteration 682, lr = 0.0025
I0520 19:33:56.194437 20884 solver.cpp:237] Iteration 744, loss = 1.89511
I0520 19:33:56.194480 20884 solver.cpp:253]     Train net output #0: loss = 1.89511 (* 1 = 1.89511 loss)
I0520 19:33:56.194497 20884 sgd_solver.cpp:106] Iteration 744, lr = 0.0025
I0520 19:34:04.338881 20884 solver.cpp:237] Iteration 806, loss = 1.90999
I0520 19:34:04.338917 20884 solver.cpp:253]     Train net output #0: loss = 1.90999 (* 1 = 1.90999 loss)
I0520 19:34:04.338932 20884 sgd_solver.cpp:106] Iteration 806, lr = 0.0025
I0520 19:34:34.629827 20884 solver.cpp:237] Iteration 868, loss = 1.80422
I0520 19:34:34.629981 20884 solver.cpp:253]     Train net output #0: loss = 1.80422 (* 1 = 1.80422 loss)
I0520 19:34:34.629997 20884 sgd_solver.cpp:106] Iteration 868, lr = 0.0025
I0520 19:34:42.779189 20884 solver.cpp:237] Iteration 930, loss = 1.78644
I0520 19:34:42.779223 20884 solver.cpp:253]     Train net output #0: loss = 1.78644 (* 1 = 1.78644 loss)
I0520 19:34:42.779240 20884 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0520 19:34:50.928825 20884 solver.cpp:237] Iteration 992, loss = 1.76999
I0520 19:34:50.928869 20884 solver.cpp:253]     Train net output #0: loss = 1.76999 (* 1 = 1.76999 loss)
I0520 19:34:50.928886 20884 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0520 19:34:59.084157 20884 solver.cpp:237] Iteration 1054, loss = 1.90627
I0520 19:34:59.084192 20884 solver.cpp:253]     Train net output #0: loss = 1.90627 (* 1 = 1.90627 loss)
I0520 19:34:59.084208 20884 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0520 19:35:07.238652 20884 solver.cpp:237] Iteration 1116, loss = 1.69012
I0520 19:35:07.238814 20884 solver.cpp:253]     Train net output #0: loss = 1.69012 (* 1 = 1.69012 loss)
I0520 19:35:07.238828 20884 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0520 19:35:15.384167 20884 solver.cpp:237] Iteration 1178, loss = 1.74151
I0520 19:35:15.384212 20884 solver.cpp:253]     Train net output #0: loss = 1.74151 (* 1 = 1.74151 loss)
I0520 19:35:15.384227 20884 sgd_solver.cpp:106] Iteration 1178, lr = 0.0025
I0520 19:35:23.528223 20884 solver.cpp:237] Iteration 1240, loss = 1.78091
I0520 19:35:23.528257 20884 solver.cpp:253]     Train net output #0: loss = 1.78091 (* 1 = 1.78091 loss)
I0520 19:35:23.528273 20884 sgd_solver.cpp:106] Iteration 1240, lr = 0.0025
I0520 19:35:24.711627 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_1250.caffemodel
I0520 19:35:24.841094 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_1250.solverstate
I0520 19:35:24.867135 20884 solver.cpp:341] Iteration 1250, Testing net (#0)
I0520 19:36:10.277686 20884 solver.cpp:409]     Test net output #0: accuracy = 0.62298
I0520 19:36:10.277845 20884 solver.cpp:409]     Test net output #1: loss = 1.27889 (* 1 = 1.27889 loss)
I0520 19:36:39.301851 20884 solver.cpp:237] Iteration 1302, loss = 1.65772
I0520 19:36:39.301900 20884 solver.cpp:253]     Train net output #0: loss = 1.65772 (* 1 = 1.65772 loss)
I0520 19:36:39.301914 20884 sgd_solver.cpp:106] Iteration 1302, lr = 0.0025
I0520 19:36:47.444437 20884 solver.cpp:237] Iteration 1364, loss = 1.70482
I0520 19:36:47.444576 20884 solver.cpp:253]     Train net output #0: loss = 1.70482 (* 1 = 1.70482 loss)
I0520 19:36:47.444589 20884 sgd_solver.cpp:106] Iteration 1364, lr = 0.0025
I0520 19:36:55.588397 20884 solver.cpp:237] Iteration 1426, loss = 1.80731
I0520 19:36:55.588438 20884 solver.cpp:253]     Train net output #0: loss = 1.80731 (* 1 = 1.80731 loss)
I0520 19:36:55.588456 20884 sgd_solver.cpp:106] Iteration 1426, lr = 0.0025
I0520 19:37:03.733135 20884 solver.cpp:237] Iteration 1488, loss = 1.77381
I0520 19:37:03.733170 20884 solver.cpp:253]     Train net output #0: loss = 1.77381 (* 1 = 1.77381 loss)
I0520 19:37:03.733185 20884 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0520 19:37:11.878741 20884 solver.cpp:237] Iteration 1550, loss = 1.80967
I0520 19:37:11.878774 20884 solver.cpp:253]     Train net output #0: loss = 1.80967 (* 1 = 1.80967 loss)
I0520 19:37:11.878790 20884 sgd_solver.cpp:106] Iteration 1550, lr = 0.0025
I0520 19:37:20.024026 20884 solver.cpp:237] Iteration 1612, loss = 1.66137
I0520 19:37:20.024168 20884 solver.cpp:253]     Train net output #0: loss = 1.66137 (* 1 = 1.66137 loss)
I0520 19:37:20.024183 20884 sgd_solver.cpp:106] Iteration 1612, lr = 0.0025
I0520 19:37:50.341681 20884 solver.cpp:237] Iteration 1674, loss = 1.69706
I0520 19:37:50.341847 20884 solver.cpp:253]     Train net output #0: loss = 1.69706 (* 1 = 1.69706 loss)
I0520 19:37:50.341863 20884 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0520 19:37:58.485716 20884 solver.cpp:237] Iteration 1736, loss = 1.74541
I0520 19:37:58.485749 20884 solver.cpp:253]     Train net output #0: loss = 1.74541 (* 1 = 1.74541 loss)
I0520 19:37:58.485767 20884 sgd_solver.cpp:106] Iteration 1736, lr = 0.0025
I0520 19:38:06.631532 20884 solver.cpp:237] Iteration 1798, loss = 1.8041
I0520 19:38:06.631584 20884 solver.cpp:253]     Train net output #0: loss = 1.8041 (* 1 = 1.8041 loss)
I0520 19:38:06.631598 20884 sgd_solver.cpp:106] Iteration 1798, lr = 0.0025
I0520 19:38:14.778159 20884 solver.cpp:237] Iteration 1860, loss = 1.77831
I0520 19:38:14.778192 20884 solver.cpp:253]     Train net output #0: loss = 1.77831 (* 1 = 1.77831 loss)
I0520 19:38:14.778209 20884 sgd_solver.cpp:106] Iteration 1860, lr = 0.0025
I0520 19:38:16.617532 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_1875.caffemodel
I0520 19:38:16.751421 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_1875.solverstate
I0520 19:38:22.993067 20884 solver.cpp:237] Iteration 1922, loss = 1.53131
I0520 19:38:22.993240 20884 solver.cpp:253]     Train net output #0: loss = 1.53131 (* 1 = 1.53131 loss)
I0520 19:38:22.993254 20884 sgd_solver.cpp:106] Iteration 1922, lr = 0.0025
I0520 19:38:31.131428 20884 solver.cpp:237] Iteration 1984, loss = 1.56
I0520 19:38:31.131467 20884 solver.cpp:253]     Train net output #0: loss = 1.56 (* 1 = 1.56 loss)
I0520 19:38:31.131487 20884 sgd_solver.cpp:106] Iteration 1984, lr = 0.0025
I0520 19:38:39.266705 20884 solver.cpp:237] Iteration 2046, loss = 1.59423
I0520 19:38:39.266739 20884 solver.cpp:253]     Train net output #0: loss = 1.59423 (* 1 = 1.59423 loss)
I0520 19:38:39.266755 20884 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0520 19:39:09.599417 20884 solver.cpp:237] Iteration 2108, loss = 1.69085
I0520 19:39:09.599576 20884 solver.cpp:253]     Train net output #0: loss = 1.69085 (* 1 = 1.69085 loss)
I0520 19:39:09.599591 20884 sgd_solver.cpp:106] Iteration 2108, lr = 0.0025
I0520 19:39:17.741296 20884 solver.cpp:237] Iteration 2170, loss = 1.66612
I0520 19:39:17.741328 20884 solver.cpp:253]     Train net output #0: loss = 1.66612 (* 1 = 1.66612 loss)
I0520 19:39:17.741346 20884 sgd_solver.cpp:106] Iteration 2170, lr = 0.0025
I0520 19:39:25.888335 20884 solver.cpp:237] Iteration 2232, loss = 1.64447
I0520 19:39:25.888380 20884 solver.cpp:253]     Train net output #0: loss = 1.64447 (* 1 = 1.64447 loss)
I0520 19:39:25.888396 20884 sgd_solver.cpp:106] Iteration 2232, lr = 0.0025
I0520 19:39:34.036399 20884 solver.cpp:237] Iteration 2294, loss = 1.62065
I0520 19:39:34.036433 20884 solver.cpp:253]     Train net output #0: loss = 1.62065 (* 1 = 1.62065 loss)
I0520 19:39:34.036450 20884 sgd_solver.cpp:106] Iteration 2294, lr = 0.0025
I0520 19:39:42.185446 20884 solver.cpp:237] Iteration 2356, loss = 1.69205
I0520 19:39:42.185600 20884 solver.cpp:253]     Train net output #0: loss = 1.69205 (* 1 = 1.69205 loss)
I0520 19:39:42.185613 20884 sgd_solver.cpp:106] Iteration 2356, lr = 0.0025
I0520 19:39:50.332554 20884 solver.cpp:237] Iteration 2418, loss = 1.55164
I0520 19:39:50.332602 20884 solver.cpp:253]     Train net output #0: loss = 1.55164 (* 1 = 1.55164 loss)
I0520 19:39:50.332618 20884 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0520 19:39:58.468703 20884 solver.cpp:237] Iteration 2480, loss = 1.63879
I0520 19:39:58.468736 20884 solver.cpp:253]     Train net output #0: loss = 1.63879 (* 1 = 1.63879 loss)
I0520 19:39:58.468752 20884 sgd_solver.cpp:106] Iteration 2480, lr = 0.0025
I0520 19:40:00.963366 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_2500.caffemodel
I0520 19:40:01.095712 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_2500.solverstate
I0520 19:40:01.123999 20884 solver.cpp:341] Iteration 2500, Testing net (#0)
I0520 19:41:07.318434 20884 solver.cpp:409]     Test net output #0: accuracy = 0.684767
I0520 19:41:07.318605 20884 solver.cpp:409]     Test net output #1: loss = 1.09139 (* 1 = 1.09139 loss)
I0520 19:41:35.075141 20884 solver.cpp:237] Iteration 2542, loss = 1.79198
I0520 19:41:35.075192 20884 solver.cpp:253]     Train net output #0: loss = 1.79198 (* 1 = 1.79198 loss)
I0520 19:41:35.075207 20884 sgd_solver.cpp:106] Iteration 2542, lr = 0.0025
I0520 19:41:43.225878 20884 solver.cpp:237] Iteration 2604, loss = 1.68775
I0520 19:41:43.226022 20884 solver.cpp:253]     Train net output #0: loss = 1.68775 (* 1 = 1.68775 loss)
I0520 19:41:43.226035 20884 sgd_solver.cpp:106] Iteration 2604, lr = 0.0025
I0520 19:41:51.375824 20884 solver.cpp:237] Iteration 2666, loss = 1.72048
I0520 19:41:51.375859 20884 solver.cpp:253]     Train net output #0: loss = 1.72048 (* 1 = 1.72048 loss)
I0520 19:41:51.375877 20884 sgd_solver.cpp:106] Iteration 2666, lr = 0.0025
I0520 19:41:59.520812 20884 solver.cpp:237] Iteration 2728, loss = 1.56984
I0520 19:41:59.520843 20884 solver.cpp:253]     Train net output #0: loss = 1.56984 (* 1 = 1.56984 loss)
I0520 19:41:59.520861 20884 sgd_solver.cpp:106] Iteration 2728, lr = 0.0025
I0520 19:42:07.655429 20884 solver.cpp:237] Iteration 2790, loss = 1.6106
I0520 19:42:07.655463 20884 solver.cpp:253]     Train net output #0: loss = 1.6106 (* 1 = 1.6106 loss)
I0520 19:42:07.655480 20884 sgd_solver.cpp:106] Iteration 2790, lr = 0.0025
I0520 19:42:15.788090 20884 solver.cpp:237] Iteration 2852, loss = 1.61853
I0520 19:42:15.788230 20884 solver.cpp:253]     Train net output #0: loss = 1.61853 (* 1 = 1.61853 loss)
I0520 19:42:15.788244 20884 sgd_solver.cpp:106] Iteration 2852, lr = 0.0025
I0520 19:42:23.934826 20884 solver.cpp:237] Iteration 2914, loss = 1.651
I0520 19:42:23.934859 20884 solver.cpp:253]     Train net output #0: loss = 1.651 (* 1 = 1.651 loss)
I0520 19:42:23.934876 20884 sgd_solver.cpp:106] Iteration 2914, lr = 0.0025
I0520 19:42:54.233402 20884 solver.cpp:237] Iteration 2976, loss = 1.60906
I0520 19:42:54.233566 20884 solver.cpp:253]     Train net output #0: loss = 1.60906 (* 1 = 1.60906 loss)
I0520 19:42:54.233582 20884 sgd_solver.cpp:106] Iteration 2976, lr = 0.0025
I0520 19:43:02.377445 20884 solver.cpp:237] Iteration 3038, loss = 1.49938
I0520 19:43:02.377478 20884 solver.cpp:253]     Train net output #0: loss = 1.49938 (* 1 = 1.49938 loss)
I0520 19:43:02.377496 20884 sgd_solver.cpp:106] Iteration 3038, lr = 0.0025
I0520 19:43:10.524158 20884 solver.cpp:237] Iteration 3100, loss = 1.6198
I0520 19:43:10.524194 20884 solver.cpp:253]     Train net output #0: loss = 1.6198 (* 1 = 1.6198 loss)
I0520 19:43:10.524211 20884 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0520 19:43:13.677587 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_3125.caffemodel
I0520 19:43:13.809324 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_3125.solverstate
I0520 19:43:18.742712 20884 solver.cpp:237] Iteration 3162, loss = 1.63844
I0520 19:43:18.742761 20884 solver.cpp:253]     Train net output #0: loss = 1.63844 (* 1 = 1.63844 loss)
I0520 19:43:18.742774 20884 sgd_solver.cpp:106] Iteration 3162, lr = 0.0025
I0520 19:43:26.894870 20884 solver.cpp:237] Iteration 3224, loss = 1.54078
I0520 19:43:26.895010 20884 solver.cpp:253]     Train net output #0: loss = 1.54078 (* 1 = 1.54078 loss)
I0520 19:43:26.895023 20884 sgd_solver.cpp:106] Iteration 3224, lr = 0.0025
I0520 19:43:35.046705 20884 solver.cpp:237] Iteration 3286, loss = 1.59874
I0520 19:43:35.046741 20884 solver.cpp:253]     Train net output #0: loss = 1.59874 (* 1 = 1.59874 loss)
I0520 19:43:35.046761 20884 sgd_solver.cpp:106] Iteration 3286, lr = 0.0025
I0520 19:44:05.342417 20884 solver.cpp:237] Iteration 3348, loss = 1.64976
I0520 19:44:05.342595 20884 solver.cpp:253]     Train net output #0: loss = 1.64976 (* 1 = 1.64976 loss)
I0520 19:44:05.342609 20884 sgd_solver.cpp:106] Iteration 3348, lr = 0.0025
I0520 19:44:13.485117 20884 solver.cpp:237] Iteration 3410, loss = 1.61193
I0520 19:44:13.485149 20884 solver.cpp:253]     Train net output #0: loss = 1.61193 (* 1 = 1.61193 loss)
I0520 19:44:13.485167 20884 sgd_solver.cpp:106] Iteration 3410, lr = 0.0025
I0520 19:44:21.624033 20884 solver.cpp:237] Iteration 3472, loss = 1.58687
I0520 19:44:21.624073 20884 solver.cpp:253]     Train net output #0: loss = 1.58687 (* 1 = 1.58687 loss)
I0520 19:44:21.624090 20884 sgd_solver.cpp:106] Iteration 3472, lr = 0.0025
I0520 19:44:29.763919 20884 solver.cpp:237] Iteration 3534, loss = 1.4702
I0520 19:44:29.763952 20884 solver.cpp:253]     Train net output #0: loss = 1.4702 (* 1 = 1.4702 loss)
I0520 19:44:29.763968 20884 sgd_solver.cpp:106] Iteration 3534, lr = 0.0025
I0520 19:44:37.908073 20884 solver.cpp:237] Iteration 3596, loss = 1.64466
I0520 19:44:37.908205 20884 solver.cpp:253]     Train net output #0: loss = 1.64466 (* 1 = 1.64466 loss)
I0520 19:44:37.908218 20884 sgd_solver.cpp:106] Iteration 3596, lr = 0.0025
I0520 19:44:46.050678 20884 solver.cpp:237] Iteration 3658, loss = 1.64649
I0520 19:44:46.050724 20884 solver.cpp:253]     Train net output #0: loss = 1.64649 (* 1 = 1.64649 loss)
I0520 19:44:46.050741 20884 sgd_solver.cpp:106] Iteration 3658, lr = 0.0025
I0520 19:44:54.203145 20884 solver.cpp:237] Iteration 3720, loss = 1.47979
I0520 19:44:54.203179 20884 solver.cpp:253]     Train net output #0: loss = 1.47979 (* 1 = 1.47979 loss)
I0520 19:44:54.203196 20884 sgd_solver.cpp:106] Iteration 3720, lr = 0.0025
I0520 19:44:58.015036 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_3750.caffemodel
I0520 19:44:58.144749 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_3750.solverstate
I0520 19:44:58.170759 20884 solver.cpp:341] Iteration 3750, Testing net (#0)
I0520 19:45:43.264333 20884 solver.cpp:409]     Test net output #0: accuracy = 0.72354
I0520 19:45:43.264490 20884 solver.cpp:409]     Test net output #1: loss = 1.03359 (* 1 = 1.03359 loss)
I0520 19:46:09.623404 20884 solver.cpp:237] Iteration 3782, loss = 1.55532
I0520 19:46:09.623452 20884 solver.cpp:253]     Train net output #0: loss = 1.55532 (* 1 = 1.55532 loss)
I0520 19:46:09.623468 20884 sgd_solver.cpp:106] Iteration 3782, lr = 0.0025
I0520 19:46:17.769155 20884 solver.cpp:237] Iteration 3844, loss = 1.63725
I0520 19:46:17.769305 20884 solver.cpp:253]     Train net output #0: loss = 1.63725 (* 1 = 1.63725 loss)
I0520 19:46:17.769318 20884 sgd_solver.cpp:106] Iteration 3844, lr = 0.0025
I0520 19:46:25.912868 20884 solver.cpp:237] Iteration 3906, loss = 1.53445
I0520 19:46:25.912911 20884 solver.cpp:253]     Train net output #0: loss = 1.53445 (* 1 = 1.53445 loss)
I0520 19:46:25.912927 20884 sgd_solver.cpp:106] Iteration 3906, lr = 0.0025
I0520 19:46:34.054697 20884 solver.cpp:237] Iteration 3968, loss = 1.65358
I0520 19:46:34.054730 20884 solver.cpp:253]     Train net output #0: loss = 1.65358 (* 1 = 1.65358 loss)
I0520 19:46:34.054749 20884 sgd_solver.cpp:106] Iteration 3968, lr = 0.0025
I0520 19:46:42.192122 20884 solver.cpp:237] Iteration 4030, loss = 1.48185
I0520 19:46:42.192155 20884 solver.cpp:253]     Train net output #0: loss = 1.48185 (* 1 = 1.48185 loss)
I0520 19:46:42.192169 20884 sgd_solver.cpp:106] Iteration 4030, lr = 0.0025
I0520 19:46:50.345027 20884 solver.cpp:237] Iteration 4092, loss = 1.56893
I0520 19:46:50.345194 20884 solver.cpp:253]     Train net output #0: loss = 1.56893 (* 1 = 1.56893 loss)
I0520 19:46:50.345209 20884 sgd_solver.cpp:106] Iteration 4092, lr = 0.0025
I0520 19:46:58.490375 20884 solver.cpp:237] Iteration 4154, loss = 1.42174
I0520 19:46:58.490406 20884 solver.cpp:253]     Train net output #0: loss = 1.42174 (* 1 = 1.42174 loss)
I0520 19:46:58.490424 20884 sgd_solver.cpp:106] Iteration 4154, lr = 0.0025
I0520 19:47:28.864707 20884 solver.cpp:237] Iteration 4216, loss = 1.55269
I0520 19:47:28.864871 20884 solver.cpp:253]     Train net output #0: loss = 1.55269 (* 1 = 1.55269 loss)
I0520 19:47:28.864886 20884 sgd_solver.cpp:106] Iteration 4216, lr = 0.0025
I0520 19:47:37.011188 20884 solver.cpp:237] Iteration 4278, loss = 1.48848
I0520 19:47:37.011222 20884 solver.cpp:253]     Train net output #0: loss = 1.48848 (* 1 = 1.48848 loss)
I0520 19:47:37.011240 20884 sgd_solver.cpp:106] Iteration 4278, lr = 0.0025
I0520 19:47:45.151759 20884 solver.cpp:237] Iteration 4340, loss = 1.59317
I0520 19:47:45.151793 20884 solver.cpp:253]     Train net output #0: loss = 1.59317 (* 1 = 1.59317 loss)
I0520 19:47:45.151813 20884 sgd_solver.cpp:106] Iteration 4340, lr = 0.0025
I0520 19:47:49.616802 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_4375.caffemodel
I0520 19:47:49.745901 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_4375.solverstate
I0520 19:47:53.356256 20884 solver.cpp:237] Iteration 4402, loss = 1.59606
I0520 19:47:53.356298 20884 solver.cpp:253]     Train net output #0: loss = 1.59606 (* 1 = 1.59606 loss)
I0520 19:47:53.356318 20884 sgd_solver.cpp:106] Iteration 4402, lr = 0.0025
I0520 19:48:01.496958 20884 solver.cpp:237] Iteration 4464, loss = 1.4969
I0520 19:48:01.497103 20884 solver.cpp:253]     Train net output #0: loss = 1.4969 (* 1 = 1.4969 loss)
I0520 19:48:01.497117 20884 sgd_solver.cpp:106] Iteration 4464, lr = 0.0025
I0520 19:48:09.631539 20884 solver.cpp:237] Iteration 4526, loss = 1.49026
I0520 19:48:09.631577 20884 solver.cpp:253]     Train net output #0: loss = 1.49026 (* 1 = 1.49026 loss)
I0520 19:48:09.631594 20884 sgd_solver.cpp:106] Iteration 4526, lr = 0.0025
I0520 19:48:39.954025 20884 solver.cpp:237] Iteration 4588, loss = 1.481
I0520 19:48:39.954198 20884 solver.cpp:253]     Train net output #0: loss = 1.481 (* 1 = 1.481 loss)
I0520 19:48:39.954215 20884 sgd_solver.cpp:106] Iteration 4588, lr = 0.0025
I0520 19:48:48.089697 20884 solver.cpp:237] Iteration 4650, loss = 1.45046
I0520 19:48:48.089730 20884 solver.cpp:253]     Train net output #0: loss = 1.45046 (* 1 = 1.45046 loss)
I0520 19:48:48.089748 20884 sgd_solver.cpp:106] Iteration 4650, lr = 0.0025
I0520 19:48:56.231540 20884 solver.cpp:237] Iteration 4712, loss = 1.34809
I0520 19:48:56.231580 20884 solver.cpp:253]     Train net output #0: loss = 1.34809 (* 1 = 1.34809 loss)
I0520 19:48:56.231600 20884 sgd_solver.cpp:106] Iteration 4712, lr = 0.0025
I0520 19:49:04.378403 20884 solver.cpp:237] Iteration 4774, loss = 1.44522
I0520 19:49:04.378437 20884 solver.cpp:253]     Train net output #0: loss = 1.44522 (* 1 = 1.44522 loss)
I0520 19:49:04.378454 20884 sgd_solver.cpp:106] Iteration 4774, lr = 0.0025
I0520 19:49:12.524055 20884 solver.cpp:237] Iteration 4836, loss = 1.42246
I0520 19:49:12.524189 20884 solver.cpp:253]     Train net output #0: loss = 1.42246 (* 1 = 1.42246 loss)
I0520 19:49:12.524204 20884 sgd_solver.cpp:106] Iteration 4836, lr = 0.0025
I0520 19:49:20.664345 20884 solver.cpp:237] Iteration 4898, loss = 1.47242
I0520 19:49:20.664391 20884 solver.cpp:253]     Train net output #0: loss = 1.47242 (* 1 = 1.47242 loss)
I0520 19:49:20.664408 20884 sgd_solver.cpp:106] Iteration 4898, lr = 0.0025
I0520 19:49:28.807855 20884 solver.cpp:237] Iteration 4960, loss = 1.56465
I0520 19:49:28.807889 20884 solver.cpp:253]     Train net output #0: loss = 1.56465 (* 1 = 1.56465 loss)
I0520 19:49:28.807905 20884 sgd_solver.cpp:106] Iteration 4960, lr = 0.0025
I0520 19:49:33.933845 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_5000.caffemodel
I0520 19:49:34.064009 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_5000.solverstate
I0520 19:49:34.090102 20884 solver.cpp:341] Iteration 5000, Testing net (#0)
I0520 19:50:40.309988 20884 solver.cpp:409]     Test net output #0: accuracy = 0.770293
I0520 19:50:40.310159 20884 solver.cpp:409]     Test net output #1: loss = 0.787205 (* 1 = 0.787205 loss)
I0520 19:51:05.431903 20884 solver.cpp:237] Iteration 5022, loss = 1.4352
I0520 19:51:05.431954 20884 solver.cpp:253]     Train net output #0: loss = 1.4352 (* 1 = 1.4352 loss)
I0520 19:51:05.431968 20884 sgd_solver.cpp:106] Iteration 5022, lr = 0.0025
I0520 19:51:13.590791 20884 solver.cpp:237] Iteration 5084, loss = 1.4931
I0520 19:51:13.590940 20884 solver.cpp:253]     Train net output #0: loss = 1.4931 (* 1 = 1.4931 loss)
I0520 19:51:13.590955 20884 sgd_solver.cpp:106] Iteration 5084, lr = 0.0025
I0520 19:51:21.737030 20884 solver.cpp:237] Iteration 5146, loss = 1.41286
I0520 19:51:21.737063 20884 solver.cpp:253]     Train net output #0: loss = 1.41286 (* 1 = 1.41286 loss)
I0520 19:51:21.737079 20884 sgd_solver.cpp:106] Iteration 5146, lr = 0.0025
I0520 19:51:29.884655 20884 solver.cpp:237] Iteration 5208, loss = 1.31449
I0520 19:51:29.884704 20884 solver.cpp:253]     Train net output #0: loss = 1.31449 (* 1 = 1.31449 loss)
I0520 19:51:29.884718 20884 sgd_solver.cpp:106] Iteration 5208, lr = 0.0025
I0520 19:51:38.034885 20884 solver.cpp:237] Iteration 5270, loss = 1.52095
I0520 19:51:38.034919 20884 solver.cpp:253]     Train net output #0: loss = 1.52095 (* 1 = 1.52095 loss)
I0520 19:51:38.034935 20884 sgd_solver.cpp:106] Iteration 5270, lr = 0.0025
I0520 19:51:46.181622 20884 solver.cpp:237] Iteration 5332, loss = 1.50812
I0520 19:51:46.181766 20884 solver.cpp:253]     Train net output #0: loss = 1.50812 (* 1 = 1.50812 loss)
I0520 19:51:46.181778 20884 sgd_solver.cpp:106] Iteration 5332, lr = 0.0025
I0520 19:51:54.333470 20884 solver.cpp:237] Iteration 5394, loss = 1.36918
I0520 19:51:54.333501 20884 solver.cpp:253]     Train net output #0: loss = 1.36918 (* 1 = 1.36918 loss)
I0520 19:51:54.333523 20884 sgd_solver.cpp:106] Iteration 5394, lr = 0.0025
I0520 19:52:24.623039 20884 solver.cpp:237] Iteration 5456, loss = 1.72079
I0520 19:52:24.623206 20884 solver.cpp:253]     Train net output #0: loss = 1.72079 (* 1 = 1.72079 loss)
I0520 19:52:24.623222 20884 sgd_solver.cpp:106] Iteration 5456, lr = 0.0025
I0520 19:52:32.767125 20884 solver.cpp:237] Iteration 5518, loss = 1.50991
I0520 19:52:32.767159 20884 solver.cpp:253]     Train net output #0: loss = 1.50991 (* 1 = 1.50991 loss)
I0520 19:52:32.767175 20884 sgd_solver.cpp:106] Iteration 5518, lr = 0.0025
I0520 19:52:40.910517 20884 solver.cpp:237] Iteration 5580, loss = 1.43196
I0520 19:52:40.910559 20884 solver.cpp:253]     Train net output #0: loss = 1.43196 (* 1 = 1.43196 loss)
I0520 19:52:40.910575 20884 sgd_solver.cpp:106] Iteration 5580, lr = 0.0025
I0520 19:52:46.693861 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_5625.caffemodel
I0520 19:52:46.824317 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_5625.solverstate
I0520 19:52:49.123297 20884 solver.cpp:237] Iteration 5642, loss = 1.50264
I0520 19:52:49.123344 20884 solver.cpp:253]     Train net output #0: loss = 1.50264 (* 1 = 1.50264 loss)
I0520 19:52:49.123358 20884 sgd_solver.cpp:106] Iteration 5642, lr = 0.0025
I0520 19:52:57.261042 20884 solver.cpp:237] Iteration 5704, loss = 1.51248
I0520 19:52:57.261200 20884 solver.cpp:253]     Train net output #0: loss = 1.51248 (* 1 = 1.51248 loss)
I0520 19:52:57.261216 20884 sgd_solver.cpp:106] Iteration 5704, lr = 0.0025
I0520 19:53:05.405637 20884 solver.cpp:237] Iteration 5766, loss = 1.57932
I0520 19:53:05.405686 20884 solver.cpp:253]     Train net output #0: loss = 1.57932 (* 1 = 1.57932 loss)
I0520 19:53:05.405702 20884 sgd_solver.cpp:106] Iteration 5766, lr = 0.0025
I0520 19:53:13.549597 20884 solver.cpp:237] Iteration 5828, loss = 1.49887
I0520 19:53:13.549631 20884 solver.cpp:253]     Train net output #0: loss = 1.49887 (* 1 = 1.49887 loss)
I0520 19:53:13.549648 20884 sgd_solver.cpp:106] Iteration 5828, lr = 0.0025
I0520 19:53:43.873843 20884 solver.cpp:237] Iteration 5890, loss = 1.46725
I0520 19:53:43.874014 20884 solver.cpp:253]     Train net output #0: loss = 1.46725 (* 1 = 1.46725 loss)
I0520 19:53:43.874030 20884 sgd_solver.cpp:106] Iteration 5890, lr = 0.0025
I0520 19:53:52.021064 20884 solver.cpp:237] Iteration 5952, loss = 1.32481
I0520 19:53:52.021098 20884 solver.cpp:253]     Train net output #0: loss = 1.32481 (* 1 = 1.32481 loss)
I0520 19:53:52.021116 20884 sgd_solver.cpp:106] Iteration 5952, lr = 0.0025
I0520 19:54:00.163463 20884 solver.cpp:237] Iteration 6014, loss = 1.45859
I0520 19:54:00.163499 20884 solver.cpp:253]     Train net output #0: loss = 1.45859 (* 1 = 1.45859 loss)
I0520 19:54:00.163516 20884 sgd_solver.cpp:106] Iteration 6014, lr = 0.0025
I0520 19:54:08.303716 20884 solver.cpp:237] Iteration 6076, loss = 1.37614
I0520 19:54:08.303751 20884 solver.cpp:253]     Train net output #0: loss = 1.37614 (* 1 = 1.37614 loss)
I0520 19:54:08.303766 20884 sgd_solver.cpp:106] Iteration 6076, lr = 0.0025
I0520 19:54:16.444923 20884 solver.cpp:237] Iteration 6138, loss = 1.58473
I0520 19:54:16.445066 20884 solver.cpp:253]     Train net output #0: loss = 1.58473 (* 1 = 1.58473 loss)
I0520 19:54:16.445080 20884 sgd_solver.cpp:106] Iteration 6138, lr = 0.0025
I0520 19:54:24.590936 20884 solver.cpp:237] Iteration 6200, loss = 1.43327
I0520 19:54:24.590973 20884 solver.cpp:253]     Train net output #0: loss = 1.43327 (* 1 = 1.43327 loss)
I0520 19:54:24.590989 20884 sgd_solver.cpp:106] Iteration 6200, lr = 0.0025
I0520 19:54:31.028156 20884 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_6250.caffemodel
I0520 19:54:31.159931 20884 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673_iter_6250.solverstate
I0520 19:54:31.189432 20884 solver.cpp:341] Iteration 6250, Testing net (#0)
I0520 19:55:16.681213 20884 solver.cpp:409]     Test net output #0: accuracy = 0.79406
I0520 19:55:16.681378 20884 solver.cpp:409]     Test net output #1: loss = 0.770949 (* 1 = 0.770949 loss)
I0520 19:55:16.681392 20884 solver.cpp:326] Optimization Done.
I0520 19:55:16.681404 20884 caffe.cpp:215] Optimization Done.
Application 11234852 resources: utime ~1266s, stime ~226s, Rss ~5329404, inblocks ~3594475, outblocks ~179817
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_240_2016-05-20T11.20.41.530673.solver"
	User time (seconds): 0.55
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:56.81
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15116
	Voluntary context switches: 2752
	Involuntary context switches: 74
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
