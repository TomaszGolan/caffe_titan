2806209
I0521 03:40:13.823588 22284 caffe.cpp:184] Using GPUs 0
I0521 03:40:14.255043 22284 solver.cpp:48] Initializing solver from parameters: 
test_iter: 245
test_interval: 491
base_lr: 0.0025
display: 24
max_iter: 2459
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 245
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767.prototxt"
I0521 03:40:14.257051 22284 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767.prototxt
I0521 03:40:14.272783 22284 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 03:40:14.272843 22284 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 03:40:14.273190 22284 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 610
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 03:40:14.273370 22284 layer_factory.hpp:77] Creating layer data_hdf5
I0521 03:40:14.273394 22284 net.cpp:106] Creating Layer data_hdf5
I0521 03:40:14.273408 22284 net.cpp:411] data_hdf5 -> data
I0521 03:40:14.273442 22284 net.cpp:411] data_hdf5 -> label
I0521 03:40:14.273475 22284 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 03:40:14.274832 22284 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 03:40:14.276995 22284 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 03:40:35.765497 22284 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 03:40:35.770730 22284 net.cpp:150] Setting up data_hdf5
I0521 03:40:35.770769 22284 net.cpp:157] Top shape: 610 1 127 50 (3873500)
I0521 03:40:35.770784 22284 net.cpp:157] Top shape: 610 (610)
I0521 03:40:35.770797 22284 net.cpp:165] Memory required for data: 15496440
I0521 03:40:35.770809 22284 layer_factory.hpp:77] Creating layer conv1
I0521 03:40:35.770844 22284 net.cpp:106] Creating Layer conv1
I0521 03:40:35.770855 22284 net.cpp:454] conv1 <- data
I0521 03:40:35.770877 22284 net.cpp:411] conv1 -> conv1
I0521 03:40:36.136970 22284 net.cpp:150] Setting up conv1
I0521 03:40:36.137017 22284 net.cpp:157] Top shape: 610 12 120 48 (42163200)
I0521 03:40:36.137028 22284 net.cpp:165] Memory required for data: 184149240
I0521 03:40:36.137058 22284 layer_factory.hpp:77] Creating layer relu1
I0521 03:40:36.137079 22284 net.cpp:106] Creating Layer relu1
I0521 03:40:36.137090 22284 net.cpp:454] relu1 <- conv1
I0521 03:40:36.137104 22284 net.cpp:397] relu1 -> conv1 (in-place)
I0521 03:40:36.137619 22284 net.cpp:150] Setting up relu1
I0521 03:40:36.137634 22284 net.cpp:157] Top shape: 610 12 120 48 (42163200)
I0521 03:40:36.137645 22284 net.cpp:165] Memory required for data: 352802040
I0521 03:40:36.137655 22284 layer_factory.hpp:77] Creating layer pool1
I0521 03:40:36.137672 22284 net.cpp:106] Creating Layer pool1
I0521 03:40:36.137682 22284 net.cpp:454] pool1 <- conv1
I0521 03:40:36.137696 22284 net.cpp:411] pool1 -> pool1
I0521 03:40:36.137775 22284 net.cpp:150] Setting up pool1
I0521 03:40:36.137789 22284 net.cpp:157] Top shape: 610 12 60 48 (21081600)
I0521 03:40:36.137799 22284 net.cpp:165] Memory required for data: 437128440
I0521 03:40:36.137809 22284 layer_factory.hpp:77] Creating layer conv2
I0521 03:40:36.137830 22284 net.cpp:106] Creating Layer conv2
I0521 03:40:36.137841 22284 net.cpp:454] conv2 <- pool1
I0521 03:40:36.137856 22284 net.cpp:411] conv2 -> conv2
I0521 03:40:36.140534 22284 net.cpp:150] Setting up conv2
I0521 03:40:36.140563 22284 net.cpp:157] Top shape: 610 20 54 46 (30304800)
I0521 03:40:36.140573 22284 net.cpp:165] Memory required for data: 558347640
I0521 03:40:36.140593 22284 layer_factory.hpp:77] Creating layer relu2
I0521 03:40:36.140606 22284 net.cpp:106] Creating Layer relu2
I0521 03:40:36.140616 22284 net.cpp:454] relu2 <- conv2
I0521 03:40:36.140628 22284 net.cpp:397] relu2 -> conv2 (in-place)
I0521 03:40:36.140959 22284 net.cpp:150] Setting up relu2
I0521 03:40:36.140974 22284 net.cpp:157] Top shape: 610 20 54 46 (30304800)
I0521 03:40:36.140985 22284 net.cpp:165] Memory required for data: 679566840
I0521 03:40:36.140995 22284 layer_factory.hpp:77] Creating layer pool2
I0521 03:40:36.141007 22284 net.cpp:106] Creating Layer pool2
I0521 03:40:36.141017 22284 net.cpp:454] pool2 <- conv2
I0521 03:40:36.141042 22284 net.cpp:411] pool2 -> pool2
I0521 03:40:36.141110 22284 net.cpp:150] Setting up pool2
I0521 03:40:36.141124 22284 net.cpp:157] Top shape: 610 20 27 46 (15152400)
I0521 03:40:36.141134 22284 net.cpp:165] Memory required for data: 740176440
I0521 03:40:36.141144 22284 layer_factory.hpp:77] Creating layer conv3
I0521 03:40:36.141163 22284 net.cpp:106] Creating Layer conv3
I0521 03:40:36.141175 22284 net.cpp:454] conv3 <- pool2
I0521 03:40:36.141187 22284 net.cpp:411] conv3 -> conv3
I0521 03:40:36.143107 22284 net.cpp:150] Setting up conv3
I0521 03:40:36.143131 22284 net.cpp:157] Top shape: 610 28 22 44 (16533440)
I0521 03:40:36.143143 22284 net.cpp:165] Memory required for data: 806310200
I0521 03:40:36.143162 22284 layer_factory.hpp:77] Creating layer relu3
I0521 03:40:36.143177 22284 net.cpp:106] Creating Layer relu3
I0521 03:40:36.143187 22284 net.cpp:454] relu3 <- conv3
I0521 03:40:36.143199 22284 net.cpp:397] relu3 -> conv3 (in-place)
I0521 03:40:36.143667 22284 net.cpp:150] Setting up relu3
I0521 03:40:36.143690 22284 net.cpp:157] Top shape: 610 28 22 44 (16533440)
I0521 03:40:36.143702 22284 net.cpp:165] Memory required for data: 872443960
I0521 03:40:36.143712 22284 layer_factory.hpp:77] Creating layer pool3
I0521 03:40:36.143723 22284 net.cpp:106] Creating Layer pool3
I0521 03:40:36.143733 22284 net.cpp:454] pool3 <- conv3
I0521 03:40:36.143746 22284 net.cpp:411] pool3 -> pool3
I0521 03:40:36.143813 22284 net.cpp:150] Setting up pool3
I0521 03:40:36.143826 22284 net.cpp:157] Top shape: 610 28 11 44 (8266720)
I0521 03:40:36.143834 22284 net.cpp:165] Memory required for data: 905510840
I0521 03:40:36.143846 22284 layer_factory.hpp:77] Creating layer conv4
I0521 03:40:36.143860 22284 net.cpp:106] Creating Layer conv4
I0521 03:40:36.143872 22284 net.cpp:454] conv4 <- pool3
I0521 03:40:36.143887 22284 net.cpp:411] conv4 -> conv4
I0521 03:40:36.146677 22284 net.cpp:150] Setting up conv4
I0521 03:40:36.146705 22284 net.cpp:157] Top shape: 610 36 6 42 (5533920)
I0521 03:40:36.146716 22284 net.cpp:165] Memory required for data: 927646520
I0521 03:40:36.146731 22284 layer_factory.hpp:77] Creating layer relu4
I0521 03:40:36.146746 22284 net.cpp:106] Creating Layer relu4
I0521 03:40:36.146756 22284 net.cpp:454] relu4 <- conv4
I0521 03:40:36.146770 22284 net.cpp:397] relu4 -> conv4 (in-place)
I0521 03:40:36.147241 22284 net.cpp:150] Setting up relu4
I0521 03:40:36.147258 22284 net.cpp:157] Top shape: 610 36 6 42 (5533920)
I0521 03:40:36.147267 22284 net.cpp:165] Memory required for data: 949782200
I0521 03:40:36.147277 22284 layer_factory.hpp:77] Creating layer pool4
I0521 03:40:36.147290 22284 net.cpp:106] Creating Layer pool4
I0521 03:40:36.147300 22284 net.cpp:454] pool4 <- conv4
I0521 03:40:36.147313 22284 net.cpp:411] pool4 -> pool4
I0521 03:40:36.147382 22284 net.cpp:150] Setting up pool4
I0521 03:40:36.147395 22284 net.cpp:157] Top shape: 610 36 3 42 (2766960)
I0521 03:40:36.147405 22284 net.cpp:165] Memory required for data: 960850040
I0521 03:40:36.147415 22284 layer_factory.hpp:77] Creating layer ip1
I0521 03:40:36.147435 22284 net.cpp:106] Creating Layer ip1
I0521 03:40:36.147445 22284 net.cpp:454] ip1 <- pool4
I0521 03:40:36.147459 22284 net.cpp:411] ip1 -> ip1
I0521 03:40:36.162967 22284 net.cpp:150] Setting up ip1
I0521 03:40:36.162995 22284 net.cpp:157] Top shape: 610 196 (119560)
I0521 03:40:36.163012 22284 net.cpp:165] Memory required for data: 961328280
I0521 03:40:36.163038 22284 layer_factory.hpp:77] Creating layer relu5
I0521 03:40:36.163053 22284 net.cpp:106] Creating Layer relu5
I0521 03:40:36.163063 22284 net.cpp:454] relu5 <- ip1
I0521 03:40:36.163075 22284 net.cpp:397] relu5 -> ip1 (in-place)
I0521 03:40:36.163421 22284 net.cpp:150] Setting up relu5
I0521 03:40:36.163435 22284 net.cpp:157] Top shape: 610 196 (119560)
I0521 03:40:36.163445 22284 net.cpp:165] Memory required for data: 961806520
I0521 03:40:36.163455 22284 layer_factory.hpp:77] Creating layer drop1
I0521 03:40:36.163477 22284 net.cpp:106] Creating Layer drop1
I0521 03:40:36.163488 22284 net.cpp:454] drop1 <- ip1
I0521 03:40:36.163513 22284 net.cpp:397] drop1 -> ip1 (in-place)
I0521 03:40:36.163559 22284 net.cpp:150] Setting up drop1
I0521 03:40:36.163573 22284 net.cpp:157] Top shape: 610 196 (119560)
I0521 03:40:36.163583 22284 net.cpp:165] Memory required for data: 962284760
I0521 03:40:36.163592 22284 layer_factory.hpp:77] Creating layer ip2
I0521 03:40:36.163611 22284 net.cpp:106] Creating Layer ip2
I0521 03:40:36.163621 22284 net.cpp:454] ip2 <- ip1
I0521 03:40:36.163635 22284 net.cpp:411] ip2 -> ip2
I0521 03:40:36.164098 22284 net.cpp:150] Setting up ip2
I0521 03:40:36.164111 22284 net.cpp:157] Top shape: 610 98 (59780)
I0521 03:40:36.164121 22284 net.cpp:165] Memory required for data: 962523880
I0521 03:40:36.164136 22284 layer_factory.hpp:77] Creating layer relu6
I0521 03:40:36.164157 22284 net.cpp:106] Creating Layer relu6
I0521 03:40:36.164167 22284 net.cpp:454] relu6 <- ip2
I0521 03:40:36.164178 22284 net.cpp:397] relu6 -> ip2 (in-place)
I0521 03:40:36.164700 22284 net.cpp:150] Setting up relu6
I0521 03:40:36.164716 22284 net.cpp:157] Top shape: 610 98 (59780)
I0521 03:40:36.164726 22284 net.cpp:165] Memory required for data: 962763000
I0521 03:40:36.164736 22284 layer_factory.hpp:77] Creating layer drop2
I0521 03:40:36.164749 22284 net.cpp:106] Creating Layer drop2
I0521 03:40:36.164758 22284 net.cpp:454] drop2 <- ip2
I0521 03:40:36.164770 22284 net.cpp:397] drop2 -> ip2 (in-place)
I0521 03:40:36.164813 22284 net.cpp:150] Setting up drop2
I0521 03:40:36.164826 22284 net.cpp:157] Top shape: 610 98 (59780)
I0521 03:40:36.164837 22284 net.cpp:165] Memory required for data: 963002120
I0521 03:40:36.164847 22284 layer_factory.hpp:77] Creating layer ip3
I0521 03:40:36.164860 22284 net.cpp:106] Creating Layer ip3
I0521 03:40:36.164870 22284 net.cpp:454] ip3 <- ip2
I0521 03:40:36.164883 22284 net.cpp:411] ip3 -> ip3
I0521 03:40:36.165094 22284 net.cpp:150] Setting up ip3
I0521 03:40:36.165107 22284 net.cpp:157] Top shape: 610 11 (6710)
I0521 03:40:36.165117 22284 net.cpp:165] Memory required for data: 963028960
I0521 03:40:36.165132 22284 layer_factory.hpp:77] Creating layer drop3
I0521 03:40:36.165145 22284 net.cpp:106] Creating Layer drop3
I0521 03:40:36.165154 22284 net.cpp:454] drop3 <- ip3
I0521 03:40:36.165166 22284 net.cpp:397] drop3 -> ip3 (in-place)
I0521 03:40:36.165205 22284 net.cpp:150] Setting up drop3
I0521 03:40:36.165218 22284 net.cpp:157] Top shape: 610 11 (6710)
I0521 03:40:36.165228 22284 net.cpp:165] Memory required for data: 963055800
I0521 03:40:36.165238 22284 layer_factory.hpp:77] Creating layer loss
I0521 03:40:36.165257 22284 net.cpp:106] Creating Layer loss
I0521 03:40:36.165267 22284 net.cpp:454] loss <- ip3
I0521 03:40:36.165277 22284 net.cpp:454] loss <- label
I0521 03:40:36.165290 22284 net.cpp:411] loss -> loss
I0521 03:40:36.165307 22284 layer_factory.hpp:77] Creating layer loss
I0521 03:40:36.165954 22284 net.cpp:150] Setting up loss
I0521 03:40:36.165976 22284 net.cpp:157] Top shape: (1)
I0521 03:40:36.165985 22284 net.cpp:160]     with loss weight 1
I0521 03:40:36.166028 22284 net.cpp:165] Memory required for data: 963055804
I0521 03:40:36.166036 22284 net.cpp:226] loss needs backward computation.
I0521 03:40:36.166048 22284 net.cpp:226] drop3 needs backward computation.
I0521 03:40:36.166056 22284 net.cpp:226] ip3 needs backward computation.
I0521 03:40:36.166066 22284 net.cpp:226] drop2 needs backward computation.
I0521 03:40:36.166076 22284 net.cpp:226] relu6 needs backward computation.
I0521 03:40:36.166086 22284 net.cpp:226] ip2 needs backward computation.
I0521 03:40:36.166096 22284 net.cpp:226] drop1 needs backward computation.
I0521 03:40:36.166105 22284 net.cpp:226] relu5 needs backward computation.
I0521 03:40:36.166115 22284 net.cpp:226] ip1 needs backward computation.
I0521 03:40:36.166124 22284 net.cpp:226] pool4 needs backward computation.
I0521 03:40:36.166134 22284 net.cpp:226] relu4 needs backward computation.
I0521 03:40:36.166144 22284 net.cpp:226] conv4 needs backward computation.
I0521 03:40:36.166154 22284 net.cpp:226] pool3 needs backward computation.
I0521 03:40:36.166174 22284 net.cpp:226] relu3 needs backward computation.
I0521 03:40:36.166184 22284 net.cpp:226] conv3 needs backward computation.
I0521 03:40:36.166195 22284 net.cpp:226] pool2 needs backward computation.
I0521 03:40:36.166205 22284 net.cpp:226] relu2 needs backward computation.
I0521 03:40:36.166215 22284 net.cpp:226] conv2 needs backward computation.
I0521 03:40:36.166225 22284 net.cpp:226] pool1 needs backward computation.
I0521 03:40:36.166236 22284 net.cpp:226] relu1 needs backward computation.
I0521 03:40:36.166245 22284 net.cpp:226] conv1 needs backward computation.
I0521 03:40:36.166256 22284 net.cpp:228] data_hdf5 does not need backward computation.
I0521 03:40:36.166266 22284 net.cpp:270] This network produces output loss
I0521 03:40:36.166290 22284 net.cpp:283] Network initialization done.
I0521 03:40:36.167843 22284 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767.prototxt
I0521 03:40:36.167914 22284 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 03:40:36.168277 22284 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 610
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 03:40:36.168472 22284 layer_factory.hpp:77] Creating layer data_hdf5
I0521 03:40:36.168486 22284 net.cpp:106] Creating Layer data_hdf5
I0521 03:40:36.168498 22284 net.cpp:411] data_hdf5 -> data
I0521 03:40:36.168515 22284 net.cpp:411] data_hdf5 -> label
I0521 03:40:36.168531 22284 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 03:40:36.169942 22284 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 03:40:57.434290 22284 net.cpp:150] Setting up data_hdf5
I0521 03:40:57.434458 22284 net.cpp:157] Top shape: 610 1 127 50 (3873500)
I0521 03:40:57.434471 22284 net.cpp:157] Top shape: 610 (610)
I0521 03:40:57.434483 22284 net.cpp:165] Memory required for data: 15496440
I0521 03:40:57.434495 22284 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 03:40:57.434525 22284 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 03:40:57.434535 22284 net.cpp:454] label_data_hdf5_1_split <- label
I0521 03:40:57.434550 22284 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 03:40:57.434571 22284 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 03:40:57.434644 22284 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 03:40:57.434659 22284 net.cpp:157] Top shape: 610 (610)
I0521 03:40:57.434669 22284 net.cpp:157] Top shape: 610 (610)
I0521 03:40:57.434679 22284 net.cpp:165] Memory required for data: 15501320
I0521 03:40:57.434689 22284 layer_factory.hpp:77] Creating layer conv1
I0521 03:40:57.434711 22284 net.cpp:106] Creating Layer conv1
I0521 03:40:57.434722 22284 net.cpp:454] conv1 <- data
I0521 03:40:57.434737 22284 net.cpp:411] conv1 -> conv1
I0521 03:40:57.436669 22284 net.cpp:150] Setting up conv1
I0521 03:40:57.436694 22284 net.cpp:157] Top shape: 610 12 120 48 (42163200)
I0521 03:40:57.436705 22284 net.cpp:165] Memory required for data: 184154120
I0521 03:40:57.436727 22284 layer_factory.hpp:77] Creating layer relu1
I0521 03:40:57.436741 22284 net.cpp:106] Creating Layer relu1
I0521 03:40:57.436751 22284 net.cpp:454] relu1 <- conv1
I0521 03:40:57.436764 22284 net.cpp:397] relu1 -> conv1 (in-place)
I0521 03:40:57.437259 22284 net.cpp:150] Setting up relu1
I0521 03:40:57.437276 22284 net.cpp:157] Top shape: 610 12 120 48 (42163200)
I0521 03:40:57.437286 22284 net.cpp:165] Memory required for data: 352806920
I0521 03:40:57.437296 22284 layer_factory.hpp:77] Creating layer pool1
I0521 03:40:57.437312 22284 net.cpp:106] Creating Layer pool1
I0521 03:40:57.437322 22284 net.cpp:454] pool1 <- conv1
I0521 03:40:57.437335 22284 net.cpp:411] pool1 -> pool1
I0521 03:40:57.437409 22284 net.cpp:150] Setting up pool1
I0521 03:40:57.437423 22284 net.cpp:157] Top shape: 610 12 60 48 (21081600)
I0521 03:40:57.437433 22284 net.cpp:165] Memory required for data: 437133320
I0521 03:40:57.437443 22284 layer_factory.hpp:77] Creating layer conv2
I0521 03:40:57.437458 22284 net.cpp:106] Creating Layer conv2
I0521 03:40:57.437469 22284 net.cpp:454] conv2 <- pool1
I0521 03:40:57.437484 22284 net.cpp:411] conv2 -> conv2
I0521 03:40:57.439395 22284 net.cpp:150] Setting up conv2
I0521 03:40:57.439417 22284 net.cpp:157] Top shape: 610 20 54 46 (30304800)
I0521 03:40:57.439429 22284 net.cpp:165] Memory required for data: 558352520
I0521 03:40:57.439446 22284 layer_factory.hpp:77] Creating layer relu2
I0521 03:40:57.439460 22284 net.cpp:106] Creating Layer relu2
I0521 03:40:57.439471 22284 net.cpp:454] relu2 <- conv2
I0521 03:40:57.439483 22284 net.cpp:397] relu2 -> conv2 (in-place)
I0521 03:40:57.439815 22284 net.cpp:150] Setting up relu2
I0521 03:40:57.439829 22284 net.cpp:157] Top shape: 610 20 54 46 (30304800)
I0521 03:40:57.439839 22284 net.cpp:165] Memory required for data: 679571720
I0521 03:40:57.439849 22284 layer_factory.hpp:77] Creating layer pool2
I0521 03:40:57.439862 22284 net.cpp:106] Creating Layer pool2
I0521 03:40:57.439872 22284 net.cpp:454] pool2 <- conv2
I0521 03:40:57.439884 22284 net.cpp:411] pool2 -> pool2
I0521 03:40:57.439954 22284 net.cpp:150] Setting up pool2
I0521 03:40:57.439967 22284 net.cpp:157] Top shape: 610 20 27 46 (15152400)
I0521 03:40:57.439977 22284 net.cpp:165] Memory required for data: 740181320
I0521 03:40:57.439987 22284 layer_factory.hpp:77] Creating layer conv3
I0521 03:40:57.440006 22284 net.cpp:106] Creating Layer conv3
I0521 03:40:57.440016 22284 net.cpp:454] conv3 <- pool2
I0521 03:40:57.440031 22284 net.cpp:411] conv3 -> conv3
I0521 03:40:57.442005 22284 net.cpp:150] Setting up conv3
I0521 03:40:57.442028 22284 net.cpp:157] Top shape: 610 28 22 44 (16533440)
I0521 03:40:57.442041 22284 net.cpp:165] Memory required for data: 806315080
I0521 03:40:57.442073 22284 layer_factory.hpp:77] Creating layer relu3
I0521 03:40:57.442086 22284 net.cpp:106] Creating Layer relu3
I0521 03:40:57.442097 22284 net.cpp:454] relu3 <- conv3
I0521 03:40:57.442111 22284 net.cpp:397] relu3 -> conv3 (in-place)
I0521 03:40:57.442589 22284 net.cpp:150] Setting up relu3
I0521 03:40:57.442605 22284 net.cpp:157] Top shape: 610 28 22 44 (16533440)
I0521 03:40:57.442615 22284 net.cpp:165] Memory required for data: 872448840
I0521 03:40:57.442625 22284 layer_factory.hpp:77] Creating layer pool3
I0521 03:40:57.442637 22284 net.cpp:106] Creating Layer pool3
I0521 03:40:57.442647 22284 net.cpp:454] pool3 <- conv3
I0521 03:40:57.442661 22284 net.cpp:411] pool3 -> pool3
I0521 03:40:57.442731 22284 net.cpp:150] Setting up pool3
I0521 03:40:57.442745 22284 net.cpp:157] Top shape: 610 28 11 44 (8266720)
I0521 03:40:57.442755 22284 net.cpp:165] Memory required for data: 905515720
I0521 03:40:57.442764 22284 layer_factory.hpp:77] Creating layer conv4
I0521 03:40:57.442781 22284 net.cpp:106] Creating Layer conv4
I0521 03:40:57.442792 22284 net.cpp:454] conv4 <- pool3
I0521 03:40:57.442807 22284 net.cpp:411] conv4 -> conv4
I0521 03:40:57.444865 22284 net.cpp:150] Setting up conv4
I0521 03:40:57.444887 22284 net.cpp:157] Top shape: 610 36 6 42 (5533920)
I0521 03:40:57.444900 22284 net.cpp:165] Memory required for data: 927651400
I0521 03:40:57.444916 22284 layer_factory.hpp:77] Creating layer relu4
I0521 03:40:57.444928 22284 net.cpp:106] Creating Layer relu4
I0521 03:40:57.444938 22284 net.cpp:454] relu4 <- conv4
I0521 03:40:57.444952 22284 net.cpp:397] relu4 -> conv4 (in-place)
I0521 03:40:57.445426 22284 net.cpp:150] Setting up relu4
I0521 03:40:57.445442 22284 net.cpp:157] Top shape: 610 36 6 42 (5533920)
I0521 03:40:57.445451 22284 net.cpp:165] Memory required for data: 949787080
I0521 03:40:57.445461 22284 layer_factory.hpp:77] Creating layer pool4
I0521 03:40:57.445474 22284 net.cpp:106] Creating Layer pool4
I0521 03:40:57.445484 22284 net.cpp:454] pool4 <- conv4
I0521 03:40:57.445498 22284 net.cpp:411] pool4 -> pool4
I0521 03:40:57.445569 22284 net.cpp:150] Setting up pool4
I0521 03:40:57.445583 22284 net.cpp:157] Top shape: 610 36 3 42 (2766960)
I0521 03:40:57.445592 22284 net.cpp:165] Memory required for data: 960854920
I0521 03:40:57.445602 22284 layer_factory.hpp:77] Creating layer ip1
I0521 03:40:57.445617 22284 net.cpp:106] Creating Layer ip1
I0521 03:40:57.445627 22284 net.cpp:454] ip1 <- pool4
I0521 03:40:57.445642 22284 net.cpp:411] ip1 -> ip1
I0521 03:40:57.461110 22284 net.cpp:150] Setting up ip1
I0521 03:40:57.461138 22284 net.cpp:157] Top shape: 610 196 (119560)
I0521 03:40:57.461153 22284 net.cpp:165] Memory required for data: 961333160
I0521 03:40:57.461175 22284 layer_factory.hpp:77] Creating layer relu5
I0521 03:40:57.461189 22284 net.cpp:106] Creating Layer relu5
I0521 03:40:57.461200 22284 net.cpp:454] relu5 <- ip1
I0521 03:40:57.461213 22284 net.cpp:397] relu5 -> ip1 (in-place)
I0521 03:40:57.461560 22284 net.cpp:150] Setting up relu5
I0521 03:40:57.461575 22284 net.cpp:157] Top shape: 610 196 (119560)
I0521 03:40:57.461585 22284 net.cpp:165] Memory required for data: 961811400
I0521 03:40:57.461594 22284 layer_factory.hpp:77] Creating layer drop1
I0521 03:40:57.461613 22284 net.cpp:106] Creating Layer drop1
I0521 03:40:57.461623 22284 net.cpp:454] drop1 <- ip1
I0521 03:40:57.461637 22284 net.cpp:397] drop1 -> ip1 (in-place)
I0521 03:40:57.461680 22284 net.cpp:150] Setting up drop1
I0521 03:40:57.461694 22284 net.cpp:157] Top shape: 610 196 (119560)
I0521 03:40:57.461702 22284 net.cpp:165] Memory required for data: 962289640
I0521 03:40:57.461711 22284 layer_factory.hpp:77] Creating layer ip2
I0521 03:40:57.461726 22284 net.cpp:106] Creating Layer ip2
I0521 03:40:57.461736 22284 net.cpp:454] ip2 <- ip1
I0521 03:40:57.461750 22284 net.cpp:411] ip2 -> ip2
I0521 03:40:57.462229 22284 net.cpp:150] Setting up ip2
I0521 03:40:57.462242 22284 net.cpp:157] Top shape: 610 98 (59780)
I0521 03:40:57.462252 22284 net.cpp:165] Memory required for data: 962528760
I0521 03:40:57.462280 22284 layer_factory.hpp:77] Creating layer relu6
I0521 03:40:57.462293 22284 net.cpp:106] Creating Layer relu6
I0521 03:40:57.462304 22284 net.cpp:454] relu6 <- ip2
I0521 03:40:57.462317 22284 net.cpp:397] relu6 -> ip2 (in-place)
I0521 03:40:57.462846 22284 net.cpp:150] Setting up relu6
I0521 03:40:57.462862 22284 net.cpp:157] Top shape: 610 98 (59780)
I0521 03:40:57.462872 22284 net.cpp:165] Memory required for data: 962767880
I0521 03:40:57.462882 22284 layer_factory.hpp:77] Creating layer drop2
I0521 03:40:57.462895 22284 net.cpp:106] Creating Layer drop2
I0521 03:40:57.462905 22284 net.cpp:454] drop2 <- ip2
I0521 03:40:57.462918 22284 net.cpp:397] drop2 -> ip2 (in-place)
I0521 03:40:57.462962 22284 net.cpp:150] Setting up drop2
I0521 03:40:57.462975 22284 net.cpp:157] Top shape: 610 98 (59780)
I0521 03:40:57.462985 22284 net.cpp:165] Memory required for data: 963007000
I0521 03:40:57.462996 22284 layer_factory.hpp:77] Creating layer ip3
I0521 03:40:57.463009 22284 net.cpp:106] Creating Layer ip3
I0521 03:40:57.463019 22284 net.cpp:454] ip3 <- ip2
I0521 03:40:57.463032 22284 net.cpp:411] ip3 -> ip3
I0521 03:40:57.463256 22284 net.cpp:150] Setting up ip3
I0521 03:40:57.463269 22284 net.cpp:157] Top shape: 610 11 (6710)
I0521 03:40:57.463279 22284 net.cpp:165] Memory required for data: 963033840
I0521 03:40:57.463295 22284 layer_factory.hpp:77] Creating layer drop3
I0521 03:40:57.463309 22284 net.cpp:106] Creating Layer drop3
I0521 03:40:57.463318 22284 net.cpp:454] drop3 <- ip3
I0521 03:40:57.463331 22284 net.cpp:397] drop3 -> ip3 (in-place)
I0521 03:40:57.463372 22284 net.cpp:150] Setting up drop3
I0521 03:40:57.463385 22284 net.cpp:157] Top shape: 610 11 (6710)
I0521 03:40:57.463395 22284 net.cpp:165] Memory required for data: 963060680
I0521 03:40:57.463405 22284 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 03:40:57.463418 22284 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 03:40:57.463428 22284 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 03:40:57.463440 22284 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 03:40:57.463455 22284 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 03:40:57.463529 22284 net.cpp:150] Setting up ip3_drop3_0_split
I0521 03:40:57.463542 22284 net.cpp:157] Top shape: 610 11 (6710)
I0521 03:40:57.463554 22284 net.cpp:157] Top shape: 610 11 (6710)
I0521 03:40:57.463564 22284 net.cpp:165] Memory required for data: 963114360
I0521 03:40:57.463574 22284 layer_factory.hpp:77] Creating layer accuracy
I0521 03:40:57.463595 22284 net.cpp:106] Creating Layer accuracy
I0521 03:40:57.463605 22284 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 03:40:57.463618 22284 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 03:40:57.463630 22284 net.cpp:411] accuracy -> accuracy
I0521 03:40:57.463654 22284 net.cpp:150] Setting up accuracy
I0521 03:40:57.463666 22284 net.cpp:157] Top shape: (1)
I0521 03:40:57.463676 22284 net.cpp:165] Memory required for data: 963114364
I0521 03:40:57.463686 22284 layer_factory.hpp:77] Creating layer loss
I0521 03:40:57.463698 22284 net.cpp:106] Creating Layer loss
I0521 03:40:57.463708 22284 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 03:40:57.463719 22284 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 03:40:57.463732 22284 net.cpp:411] loss -> loss
I0521 03:40:57.463750 22284 layer_factory.hpp:77] Creating layer loss
I0521 03:40:57.464274 22284 net.cpp:150] Setting up loss
I0521 03:40:57.464289 22284 net.cpp:157] Top shape: (1)
I0521 03:40:57.464298 22284 net.cpp:160]     with loss weight 1
I0521 03:40:57.464316 22284 net.cpp:165] Memory required for data: 963114368
I0521 03:40:57.464328 22284 net.cpp:226] loss needs backward computation.
I0521 03:40:57.464339 22284 net.cpp:228] accuracy does not need backward computation.
I0521 03:40:57.464349 22284 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 03:40:57.464359 22284 net.cpp:226] drop3 needs backward computation.
I0521 03:40:57.464370 22284 net.cpp:226] ip3 needs backward computation.
I0521 03:40:57.464380 22284 net.cpp:226] drop2 needs backward computation.
I0521 03:40:57.464399 22284 net.cpp:226] relu6 needs backward computation.
I0521 03:40:57.464411 22284 net.cpp:226] ip2 needs backward computation.
I0521 03:40:57.464421 22284 net.cpp:226] drop1 needs backward computation.
I0521 03:40:57.464429 22284 net.cpp:226] relu5 needs backward computation.
I0521 03:40:57.464439 22284 net.cpp:226] ip1 needs backward computation.
I0521 03:40:57.464448 22284 net.cpp:226] pool4 needs backward computation.
I0521 03:40:57.464459 22284 net.cpp:226] relu4 needs backward computation.
I0521 03:40:57.464469 22284 net.cpp:226] conv4 needs backward computation.
I0521 03:40:57.464479 22284 net.cpp:226] pool3 needs backward computation.
I0521 03:40:57.464489 22284 net.cpp:226] relu3 needs backward computation.
I0521 03:40:57.464499 22284 net.cpp:226] conv3 needs backward computation.
I0521 03:40:57.464510 22284 net.cpp:226] pool2 needs backward computation.
I0521 03:40:57.464520 22284 net.cpp:226] relu2 needs backward computation.
I0521 03:40:57.464530 22284 net.cpp:226] conv2 needs backward computation.
I0521 03:40:57.464540 22284 net.cpp:226] pool1 needs backward computation.
I0521 03:40:57.464550 22284 net.cpp:226] relu1 needs backward computation.
I0521 03:40:57.464560 22284 net.cpp:226] conv1 needs backward computation.
I0521 03:40:57.464571 22284 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 03:40:57.464581 22284 net.cpp:228] data_hdf5 does not need backward computation.
I0521 03:40:57.464591 22284 net.cpp:270] This network produces output accuracy
I0521 03:40:57.464601 22284 net.cpp:270] This network produces output loss
I0521 03:40:57.464630 22284 net.cpp:283] Network initialization done.
I0521 03:40:57.464762 22284 solver.cpp:60] Solver scaffolding done.
I0521 03:40:57.465891 22284 caffe.cpp:212] Starting Optimization
I0521 03:40:57.465910 22284 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 03:40:57.465919 22284 solver.cpp:289] Learning Rate Policy: fixed
I0521 03:40:57.467140 22284 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 03:41:43.377763 22284 solver.cpp:409]     Test net output #0: accuracy = 0.0993844
I0521 03:41:43.377938 22284 solver.cpp:409]     Test net output #1: loss = 2.39834 (* 1 = 2.39834 loss)
I0521 03:41:43.494613 22284 solver.cpp:237] Iteration 0, loss = 2.39944
I0521 03:41:43.494650 22284 solver.cpp:253]     Train net output #0: loss = 2.39944 (* 1 = 2.39944 loss)
I0521 03:41:43.494669 22284 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 03:41:51.385591 22284 solver.cpp:237] Iteration 24, loss = 2.38115
I0521 03:41:51.385627 22284 solver.cpp:253]     Train net output #0: loss = 2.38115 (* 1 = 2.38115 loss)
I0521 03:41:51.385648 22284 sgd_solver.cpp:106] Iteration 24, lr = 0.0025
I0521 03:41:59.275584 22284 solver.cpp:237] Iteration 48, loss = 2.36888
I0521 03:41:59.275616 22284 solver.cpp:253]     Train net output #0: loss = 2.36888 (* 1 = 2.36888 loss)
I0521 03:41:59.275635 22284 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0521 03:42:07.165575 22284 solver.cpp:237] Iteration 72, loss = 2.34942
I0521 03:42:07.165606 22284 solver.cpp:253]     Train net output #0: loss = 2.34942 (* 1 = 2.34942 loss)
I0521 03:42:07.165621 22284 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0521 03:42:15.059870 22284 solver.cpp:237] Iteration 96, loss = 2.33607
I0521 03:42:15.060014 22284 solver.cpp:253]     Train net output #0: loss = 2.33607 (* 1 = 2.33607 loss)
I0521 03:42:15.060029 22284 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 03:42:22.947440 22284 solver.cpp:237] Iteration 120, loss = 2.33278
I0521 03:42:22.947470 22284 solver.cpp:253]     Train net output #0: loss = 2.33278 (* 1 = 2.33278 loss)
I0521 03:42:22.947489 22284 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 03:42:30.833161 22284 solver.cpp:237] Iteration 144, loss = 2.31609
I0521 03:42:30.833192 22284 solver.cpp:253]     Train net output #0: loss = 2.31609 (* 1 = 2.31609 loss)
I0521 03:42:30.833209 22284 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 03:43:00.827469 22284 solver.cpp:237] Iteration 168, loss = 2.31271
I0521 03:43:00.827630 22284 solver.cpp:253]     Train net output #0: loss = 2.31271 (* 1 = 2.31271 loss)
I0521 03:43:00.827644 22284 sgd_solver.cpp:106] Iteration 168, lr = 0.0025
I0521 03:43:08.719207 22284 solver.cpp:237] Iteration 192, loss = 2.30328
I0521 03:43:08.719239 22284 solver.cpp:253]     Train net output #0: loss = 2.30328 (* 1 = 2.30328 loss)
I0521 03:43:08.719254 22284 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 03:43:16.612015 22284 solver.cpp:237] Iteration 216, loss = 2.3025
I0521 03:43:16.612046 22284 solver.cpp:253]     Train net output #0: loss = 2.3025 (* 1 = 2.3025 loss)
I0521 03:43:16.612064 22284 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 03:43:24.501513 22284 solver.cpp:237] Iteration 240, loss = 2.30561
I0521 03:43:24.501544 22284 solver.cpp:253]     Train net output #0: loss = 2.30561 (* 1 = 2.30561 loss)
I0521 03:43:24.501561 22284 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 03:43:25.820322 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_245.caffemodel
I0521 03:43:26.092435 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_245.solverstate
I0521 03:43:32.465535 22284 solver.cpp:237] Iteration 264, loss = 2.30397
I0521 03:43:32.465687 22284 solver.cpp:253]     Train net output #0: loss = 2.30397 (* 1 = 2.30397 loss)
I0521 03:43:32.465700 22284 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0521 03:43:40.354843 22284 solver.cpp:237] Iteration 288, loss = 2.26343
I0521 03:43:40.354876 22284 solver.cpp:253]     Train net output #0: loss = 2.26343 (* 1 = 2.26343 loss)
I0521 03:43:40.354890 22284 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 03:43:48.253417 22284 solver.cpp:237] Iteration 312, loss = 2.21645
I0521 03:43:48.253448 22284 solver.cpp:253]     Train net output #0: loss = 2.21645 (* 1 = 2.21645 loss)
I0521 03:43:48.253463 22284 sgd_solver.cpp:106] Iteration 312, lr = 0.0025
I0521 03:44:18.254344 22284 solver.cpp:237] Iteration 336, loss = 2.14554
I0521 03:44:18.254499 22284 solver.cpp:253]     Train net output #0: loss = 2.14554 (* 1 = 2.14554 loss)
I0521 03:44:18.254513 22284 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 03:44:26.144593 22284 solver.cpp:237] Iteration 360, loss = 2.17455
I0521 03:44:26.144625 22284 solver.cpp:253]     Train net output #0: loss = 2.17455 (* 1 = 2.17455 loss)
I0521 03:44:26.144642 22284 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 03:44:34.033689 22284 solver.cpp:237] Iteration 384, loss = 2.08985
I0521 03:44:34.033722 22284 solver.cpp:253]     Train net output #0: loss = 2.08985 (* 1 = 2.08985 loss)
I0521 03:44:34.033738 22284 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 03:44:41.926867 22284 solver.cpp:237] Iteration 408, loss = 2.06629
I0521 03:44:41.926903 22284 solver.cpp:253]     Train net output #0: loss = 2.06629 (* 1 = 2.06629 loss)
I0521 03:44:41.926928 22284 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0521 03:44:49.817441 22284 solver.cpp:237] Iteration 432, loss = 2.0498
I0521 03:44:49.817584 22284 solver.cpp:253]     Train net output #0: loss = 2.0498 (* 1 = 2.0498 loss)
I0521 03:44:49.817597 22284 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 03:44:57.712342 22284 solver.cpp:237] Iteration 456, loss = 2.00046
I0521 03:44:57.712374 22284 solver.cpp:253]     Train net output #0: loss = 2.00046 (* 1 = 2.00046 loss)
I0521 03:44:57.712393 22284 sgd_solver.cpp:106] Iteration 456, lr = 0.0025
I0521 03:45:05.608510 22284 solver.cpp:237] Iteration 480, loss = 1.97671
I0521 03:45:05.608552 22284 solver.cpp:253]     Train net output #0: loss = 1.97671 (* 1 = 1.97671 loss)
I0521 03:45:05.608569 22284 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 03:45:08.566918 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_490.caffemodel
I0521 03:45:08.835538 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_490.solverstate
I0521 03:45:08.959677 22284 solver.cpp:341] Iteration 491, Testing net (#0)
I0521 03:45:54.193424 22284 solver.cpp:409]     Test net output #0: accuracy = 0.536046
I0521 03:45:54.193589 22284 solver.cpp:409]     Test net output #1: loss = 1.73127 (* 1 = 1.73127 loss)
I0521 03:46:20.678519 22284 solver.cpp:237] Iteration 504, loss = 2.01211
I0521 03:46:20.678570 22284 solver.cpp:253]     Train net output #0: loss = 2.01211 (* 1 = 2.01211 loss)
I0521 03:46:20.678582 22284 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 03:46:28.570241 22284 solver.cpp:237] Iteration 528, loss = 1.99797
I0521 03:46:28.570396 22284 solver.cpp:253]     Train net output #0: loss = 1.99797 (* 1 = 1.99797 loss)
I0521 03:46:28.570411 22284 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 03:46:36.455209 22284 solver.cpp:237] Iteration 552, loss = 1.97242
I0521 03:46:36.455242 22284 solver.cpp:253]     Train net output #0: loss = 1.97242 (* 1 = 1.97242 loss)
I0521 03:46:36.455257 22284 sgd_solver.cpp:106] Iteration 552, lr = 0.0025
I0521 03:46:44.342715 22284 solver.cpp:237] Iteration 576, loss = 1.90519
I0521 03:46:44.342747 22284 solver.cpp:253]     Train net output #0: loss = 1.90519 (* 1 = 1.90519 loss)
I0521 03:46:44.342763 22284 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 03:46:52.228895 22284 solver.cpp:237] Iteration 600, loss = 1.91266
I0521 03:46:52.228938 22284 solver.cpp:253]     Train net output #0: loss = 1.91266 (* 1 = 1.91266 loss)
I0521 03:46:52.228955 22284 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 03:47:00.115756 22284 solver.cpp:237] Iteration 624, loss = 1.92231
I0521 03:47:00.115890 22284 solver.cpp:253]     Train net output #0: loss = 1.92231 (* 1 = 1.92231 loss)
I0521 03:47:00.115903 22284 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 03:47:08.001626 22284 solver.cpp:237] Iteration 648, loss = 1.86234
I0521 03:47:08.001657 22284 solver.cpp:253]     Train net output #0: loss = 1.86234 (* 1 = 1.86234 loss)
I0521 03:47:08.001673 22284 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 03:47:38.052386 22284 solver.cpp:237] Iteration 672, loss = 1.88488
I0521 03:47:38.052559 22284 solver.cpp:253]     Train net output #0: loss = 1.88488 (* 1 = 1.88488 loss)
I0521 03:47:38.052574 22284 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 03:47:45.934768 22284 solver.cpp:237] Iteration 696, loss = 1.91534
I0521 03:47:45.934801 22284 solver.cpp:253]     Train net output #0: loss = 1.91534 (* 1 = 1.91534 loss)
I0521 03:47:45.934818 22284 sgd_solver.cpp:106] Iteration 696, lr = 0.0025
I0521 03:47:53.816344 22284 solver.cpp:237] Iteration 720, loss = 1.86644
I0521 03:47:53.816377 22284 solver.cpp:253]     Train net output #0: loss = 1.86644 (* 1 = 1.86644 loss)
I0521 03:47:53.816393 22284 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 03:47:58.415949 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_735.caffemodel
I0521 03:47:58.686524 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_735.solverstate
I0521 03:48:01.767037 22284 solver.cpp:237] Iteration 744, loss = 1.84919
I0521 03:48:01.767087 22284 solver.cpp:253]     Train net output #0: loss = 1.84919 (* 1 = 1.84919 loss)
I0521 03:48:01.767103 22284 sgd_solver.cpp:106] Iteration 744, lr = 0.0025
I0521 03:48:09.650467 22284 solver.cpp:237] Iteration 768, loss = 1.82668
I0521 03:48:09.650609 22284 solver.cpp:253]     Train net output #0: loss = 1.82668 (* 1 = 1.82668 loss)
I0521 03:48:09.650621 22284 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 03:48:17.546042 22284 solver.cpp:237] Iteration 792, loss = 1.89269
I0521 03:48:17.546075 22284 solver.cpp:253]     Train net output #0: loss = 1.89269 (* 1 = 1.89269 loss)
I0521 03:48:17.546092 22284 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 03:48:25.429749 22284 solver.cpp:237] Iteration 816, loss = 1.76203
I0521 03:48:25.429782 22284 solver.cpp:253]     Train net output #0: loss = 1.76203 (* 1 = 1.76203 loss)
I0521 03:48:25.429800 22284 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 03:48:55.456939 22284 solver.cpp:237] Iteration 840, loss = 1.83665
I0521 03:48:55.457098 22284 solver.cpp:253]     Train net output #0: loss = 1.83665 (* 1 = 1.83665 loss)
I0521 03:48:55.457111 22284 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 03:49:03.341574 22284 solver.cpp:237] Iteration 864, loss = 1.82033
I0521 03:49:03.341608 22284 solver.cpp:253]     Train net output #0: loss = 1.82033 (* 1 = 1.82033 loss)
I0521 03:49:03.341624 22284 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 03:49:11.228700 22284 solver.cpp:237] Iteration 888, loss = 1.85799
I0521 03:49:11.228734 22284 solver.cpp:253]     Train net output #0: loss = 1.85799 (* 1 = 1.85799 loss)
I0521 03:49:11.228750 22284 sgd_solver.cpp:106] Iteration 888, lr = 0.0025
I0521 03:49:19.109777 22284 solver.cpp:237] Iteration 912, loss = 1.80708
I0521 03:49:19.109817 22284 solver.cpp:253]     Train net output #0: loss = 1.80708 (* 1 = 1.80708 loss)
I0521 03:49:19.109836 22284 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 03:49:26.999744 22284 solver.cpp:237] Iteration 936, loss = 1.79287
I0521 03:49:26.999879 22284 solver.cpp:253]     Train net output #0: loss = 1.79287 (* 1 = 1.79287 loss)
I0521 03:49:26.999892 22284 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 03:49:34.884872 22284 solver.cpp:237] Iteration 960, loss = 1.80509
I0521 03:49:34.884905 22284 solver.cpp:253]     Train net output #0: loss = 1.80509 (* 1 = 1.80509 loss)
I0521 03:49:34.884922 22284 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 03:49:41.129498 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_980.caffemodel
I0521 03:49:41.399835 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_980.solverstate
I0521 03:49:41.853605 22284 solver.cpp:341] Iteration 982, Testing net (#0)
I0521 03:50:47.937239 22284 solver.cpp:409]     Test net output #0: accuracy = 0.630258
I0521 03:50:47.937419 22284 solver.cpp:409]     Test net output #1: loss = 1.28495 (* 1 = 1.28495 loss)
I0521 03:51:10.843150 22284 solver.cpp:237] Iteration 984, loss = 1.79252
I0521 03:51:10.843201 22284 solver.cpp:253]     Train net output #0: loss = 1.79252 (* 1 = 1.79252 loss)
I0521 03:51:10.843216 22284 sgd_solver.cpp:106] Iteration 984, lr = 0.0025
I0521 03:51:18.728020 22284 solver.cpp:237] Iteration 1008, loss = 1.79999
I0521 03:51:18.728175 22284 solver.cpp:253]     Train net output #0: loss = 1.79999 (* 1 = 1.79999 loss)
I0521 03:51:18.728189 22284 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 03:51:26.611042 22284 solver.cpp:237] Iteration 1032, loss = 1.78132
I0521 03:51:26.611085 22284 solver.cpp:253]     Train net output #0: loss = 1.78132 (* 1 = 1.78132 loss)
I0521 03:51:26.611104 22284 sgd_solver.cpp:106] Iteration 1032, lr = 0.0025
I0521 03:51:34.494379 22284 solver.cpp:237] Iteration 1056, loss = 1.79602
I0521 03:51:34.494412 22284 solver.cpp:253]     Train net output #0: loss = 1.79602 (* 1 = 1.79602 loss)
I0521 03:51:34.494429 22284 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 03:51:42.379346 22284 solver.cpp:237] Iteration 1080, loss = 1.778
I0521 03:51:42.379379 22284 solver.cpp:253]     Train net output #0: loss = 1.778 (* 1 = 1.778 loss)
I0521 03:51:42.379395 22284 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 03:51:50.259017 22284 solver.cpp:237] Iteration 1104, loss = 1.83598
I0521 03:51:50.259167 22284 solver.cpp:253]     Train net output #0: loss = 1.83598 (* 1 = 1.83598 loss)
I0521 03:51:50.259181 22284 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 03:51:58.142205 22284 solver.cpp:237] Iteration 1128, loss = 1.75298
I0521 03:51:58.142238 22284 solver.cpp:253]     Train net output #0: loss = 1.75298 (* 1 = 1.75298 loss)
I0521 03:51:58.142256 22284 sgd_solver.cpp:106] Iteration 1128, lr = 0.0025
I0521 03:52:28.140220 22284 solver.cpp:237] Iteration 1152, loss = 1.86682
I0521 03:52:28.140386 22284 solver.cpp:253]     Train net output #0: loss = 1.86682 (* 1 = 1.86682 loss)
I0521 03:52:28.140400 22284 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 03:52:36.022361 22284 solver.cpp:237] Iteration 1176, loss = 1.77714
I0521 03:52:36.022405 22284 solver.cpp:253]     Train net output #0: loss = 1.77714 (* 1 = 1.77714 loss)
I0521 03:52:36.022418 22284 sgd_solver.cpp:106] Iteration 1176, lr = 0.0025
I0521 03:52:43.903956 22284 solver.cpp:237] Iteration 1200, loss = 1.68527
I0521 03:52:43.903991 22284 solver.cpp:253]     Train net output #0: loss = 1.68527 (* 1 = 1.68527 loss)
I0521 03:52:43.904007 22284 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 03:52:51.785389 22284 solver.cpp:237] Iteration 1224, loss = 1.73016
I0521 03:52:51.785421 22284 solver.cpp:253]     Train net output #0: loss = 1.73016 (* 1 = 1.73016 loss)
I0521 03:52:51.785439 22284 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 03:52:51.785815 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1225.caffemodel
I0521 03:52:52.055985 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1225.solverstate
I0521 03:52:59.740366 22284 solver.cpp:237] Iteration 1248, loss = 1.73384
I0521 03:52:59.740526 22284 solver.cpp:253]     Train net output #0: loss = 1.73384 (* 1 = 1.73384 loss)
I0521 03:52:59.740540 22284 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 03:53:07.620312 22284 solver.cpp:237] Iteration 1272, loss = 1.81867
I0521 03:53:07.620362 22284 solver.cpp:253]     Train net output #0: loss = 1.81867 (* 1 = 1.81867 loss)
I0521 03:53:07.620378 22284 sgd_solver.cpp:106] Iteration 1272, lr = 0.0025
I0521 03:53:15.502764 22284 solver.cpp:237] Iteration 1296, loss = 1.76555
I0521 03:53:15.502796 22284 solver.cpp:253]     Train net output #0: loss = 1.76555 (* 1 = 1.76555 loss)
I0521 03:53:15.502813 22284 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 03:53:45.551234 22284 solver.cpp:237] Iteration 1320, loss = 1.75618
I0521 03:53:45.551398 22284 solver.cpp:253]     Train net output #0: loss = 1.75618 (* 1 = 1.75618 loss)
I0521 03:53:45.551414 22284 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 03:53:53.433357 22284 solver.cpp:237] Iteration 1344, loss = 1.76515
I0521 03:53:53.433408 22284 solver.cpp:253]     Train net output #0: loss = 1.76515 (* 1 = 1.76515 loss)
I0521 03:53:53.433421 22284 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 03:54:01.317020 22284 solver.cpp:237] Iteration 1368, loss = 1.73587
I0521 03:54:01.317054 22284 solver.cpp:253]     Train net output #0: loss = 1.73587 (* 1 = 1.73587 loss)
I0521 03:54:01.317068 22284 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 03:54:09.196565 22284 solver.cpp:237] Iteration 1392, loss = 1.73524
I0521 03:54:09.196597 22284 solver.cpp:253]     Train net output #0: loss = 1.73524 (* 1 = 1.73524 loss)
I0521 03:54:09.196614 22284 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 03:54:17.083386 22284 solver.cpp:237] Iteration 1416, loss = 1.75574
I0521 03:54:17.083542 22284 solver.cpp:253]     Train net output #0: loss = 1.75574 (* 1 = 1.75574 loss)
I0521 03:54:17.083556 22284 sgd_solver.cpp:106] Iteration 1416, lr = 0.0025
I0521 03:54:24.965561 22284 solver.cpp:237] Iteration 1440, loss = 1.81285
I0521 03:54:24.965592 22284 solver.cpp:253]     Train net output #0: loss = 1.81285 (* 1 = 1.81285 loss)
I0521 03:54:24.965610 22284 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 03:54:32.852659 22284 solver.cpp:237] Iteration 1464, loss = 1.76004
I0521 03:54:32.852691 22284 solver.cpp:253]     Train net output #0: loss = 1.76004 (* 1 = 1.76004 loss)
I0521 03:54:32.852705 22284 sgd_solver.cpp:106] Iteration 1464, lr = 0.0025
I0521 03:54:34.493530 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1470.caffemodel
I0521 03:54:34.761910 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1470.solverstate
I0521 03:54:35.542896 22284 solver.cpp:341] Iteration 1473, Testing net (#0)
I0521 03:55:20.468961 22284 solver.cpp:409]     Test net output #0: accuracy = 0.649722
I0521 03:55:20.469120 22284 solver.cpp:409]     Test net output #1: loss = 1.2365 (* 1 = 1.2365 loss)
I0521 03:55:47.644553 22284 solver.cpp:237] Iteration 1488, loss = 1.65534
I0521 03:55:47.644603 22284 solver.cpp:253]     Train net output #0: loss = 1.65534 (* 1 = 1.65534 loss)
I0521 03:55:47.644619 22284 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 03:55:55.526937 22284 solver.cpp:237] Iteration 1512, loss = 1.71709
I0521 03:55:55.527076 22284 solver.cpp:253]     Train net output #0: loss = 1.71709 (* 1 = 1.71709 loss)
I0521 03:55:55.527089 22284 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 03:56:03.410565 22284 solver.cpp:237] Iteration 1536, loss = 1.73073
I0521 03:56:03.410593 22284 solver.cpp:253]     Train net output #0: loss = 1.73073 (* 1 = 1.73073 loss)
I0521 03:56:03.410619 22284 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 03:56:11.298751 22284 solver.cpp:237] Iteration 1560, loss = 1.72292
I0521 03:56:11.298784 22284 solver.cpp:253]     Train net output #0: loss = 1.72292 (* 1 = 1.72292 loss)
I0521 03:56:11.298802 22284 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 03:56:19.182922 22284 solver.cpp:237] Iteration 1584, loss = 1.77052
I0521 03:56:19.182955 22284 solver.cpp:253]     Train net output #0: loss = 1.77052 (* 1 = 1.77052 loss)
I0521 03:56:19.182971 22284 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 03:56:27.067693 22284 solver.cpp:237] Iteration 1608, loss = 1.70631
I0521 03:56:27.067838 22284 solver.cpp:253]     Train net output #0: loss = 1.70631 (* 1 = 1.70631 loss)
I0521 03:56:27.067852 22284 sgd_solver.cpp:106] Iteration 1608, lr = 0.0025
I0521 03:56:34.952395 22284 solver.cpp:237] Iteration 1632, loss = 1.69928
I0521 03:56:34.952427 22284 solver.cpp:253]     Train net output #0: loss = 1.69928 (* 1 = 1.69928 loss)
I0521 03:56:34.952445 22284 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 03:57:05.012455 22284 solver.cpp:237] Iteration 1656, loss = 1.7194
I0521 03:57:05.012625 22284 solver.cpp:253]     Train net output #0: loss = 1.7194 (* 1 = 1.7194 loss)
I0521 03:57:05.012641 22284 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 03:57:12.896713 22284 solver.cpp:237] Iteration 1680, loss = 1.71557
I0521 03:57:12.896751 22284 solver.cpp:253]     Train net output #0: loss = 1.71557 (* 1 = 1.71557 loss)
I0521 03:57:12.896767 22284 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 03:57:20.780395 22284 solver.cpp:237] Iteration 1704, loss = 1.58203
I0521 03:57:20.780428 22284 solver.cpp:253]     Train net output #0: loss = 1.58203 (* 1 = 1.58203 loss)
I0521 03:57:20.780446 22284 sgd_solver.cpp:106] Iteration 1704, lr = 0.0025
I0521 03:57:24.066618 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1715.caffemodel
I0521 03:57:24.335830 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1715.solverstate
I0521 03:57:28.732168 22284 solver.cpp:237] Iteration 1728, loss = 1.71745
I0521 03:57:28.732213 22284 solver.cpp:253]     Train net output #0: loss = 1.71745 (* 1 = 1.71745 loss)
I0521 03:57:28.732229 22284 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 03:57:36.612143 22284 solver.cpp:237] Iteration 1752, loss = 1.71455
I0521 03:57:36.612313 22284 solver.cpp:253]     Train net output #0: loss = 1.71455 (* 1 = 1.71455 loss)
I0521 03:57:36.612326 22284 sgd_solver.cpp:106] Iteration 1752, lr = 0.0025
I0521 03:57:44.494642 22284 solver.cpp:237] Iteration 1776, loss = 1.7237
I0521 03:57:44.494673 22284 solver.cpp:253]     Train net output #0: loss = 1.7237 (* 1 = 1.7237 loss)
I0521 03:57:44.494691 22284 sgd_solver.cpp:106] Iteration 1776, lr = 0.0025
I0521 03:57:52.374626 22284 solver.cpp:237] Iteration 1800, loss = 1.70659
I0521 03:57:52.374660 22284 solver.cpp:253]     Train net output #0: loss = 1.70659 (* 1 = 1.70659 loss)
I0521 03:57:52.374676 22284 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 03:58:22.402954 22284 solver.cpp:237] Iteration 1824, loss = 1.78428
I0521 03:58:22.403125 22284 solver.cpp:253]     Train net output #0: loss = 1.78428 (* 1 = 1.78428 loss)
I0521 03:58:22.403138 22284 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0521 03:58:30.288594 22284 solver.cpp:237] Iteration 1848, loss = 1.67989
I0521 03:58:30.288638 22284 solver.cpp:253]     Train net output #0: loss = 1.67989 (* 1 = 1.67989 loss)
I0521 03:58:30.288656 22284 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 03:58:38.172061 22284 solver.cpp:237] Iteration 1872, loss = 1.65343
I0521 03:58:38.172094 22284 solver.cpp:253]     Train net output #0: loss = 1.65343 (* 1 = 1.65343 loss)
I0521 03:58:38.172108 22284 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0521 03:58:46.059201 22284 solver.cpp:237] Iteration 1896, loss = 1.70532
I0521 03:58:46.059234 22284 solver.cpp:253]     Train net output #0: loss = 1.70532 (* 1 = 1.70532 loss)
I0521 03:58:46.059252 22284 sgd_solver.cpp:106] Iteration 1896, lr = 0.0025
I0521 03:58:53.944705 22284 solver.cpp:237] Iteration 1920, loss = 1.57455
I0521 03:58:53.944854 22284 solver.cpp:253]     Train net output #0: loss = 1.57455 (* 1 = 1.57455 loss)
I0521 03:58:53.944867 22284 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0521 03:59:01.830227 22284 solver.cpp:237] Iteration 1944, loss = 1.64939
I0521 03:59:01.830258 22284 solver.cpp:253]     Train net output #0: loss = 1.64939 (* 1 = 1.64939 loss)
I0521 03:59:01.830276 22284 sgd_solver.cpp:106] Iteration 1944, lr = 0.0025
I0521 03:59:06.760907 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1960.caffemodel
I0521 03:59:07.030364 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_1960.solverstate
I0521 03:59:08.140369 22284 solver.cpp:341] Iteration 1964, Testing net (#0)
I0521 04:00:14.154815 22284 solver.cpp:409]     Test net output #0: accuracy = 0.672245
I0521 04:00:14.154989 22284 solver.cpp:409]     Test net output #1: loss = 1.12053 (* 1 = 1.12053 loss)
I0521 04:00:37.714740 22284 solver.cpp:237] Iteration 1968, loss = 1.66658
I0521 04:00:37.714792 22284 solver.cpp:253]     Train net output #0: loss = 1.66658 (* 1 = 1.66658 loss)
I0521 04:00:37.714808 22284 sgd_solver.cpp:106] Iteration 1968, lr = 0.0025
I0521 04:00:45.607410 22284 solver.cpp:237] Iteration 1992, loss = 1.69241
I0521 04:00:45.607563 22284 solver.cpp:253]     Train net output #0: loss = 1.69241 (* 1 = 1.69241 loss)
I0521 04:00:45.607576 22284 sgd_solver.cpp:106] Iteration 1992, lr = 0.0025
I0521 04:00:53.499843 22284 solver.cpp:237] Iteration 2016, loss = 1.60469
I0521 04:00:53.499876 22284 solver.cpp:253]     Train net output #0: loss = 1.60469 (* 1 = 1.60469 loss)
I0521 04:00:53.499894 22284 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0521 04:01:01.388371 22284 solver.cpp:237] Iteration 2040, loss = 1.62414
I0521 04:01:01.388406 22284 solver.cpp:253]     Train net output #0: loss = 1.62414 (* 1 = 1.62414 loss)
I0521 04:01:01.388424 22284 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0521 04:01:09.276548 22284 solver.cpp:237] Iteration 2064, loss = 1.62268
I0521 04:01:09.276582 22284 solver.cpp:253]     Train net output #0: loss = 1.62268 (* 1 = 1.62268 loss)
I0521 04:01:09.276598 22284 sgd_solver.cpp:106] Iteration 2064, lr = 0.0025
I0521 04:01:17.162132 22284 solver.cpp:237] Iteration 2088, loss = 1.74199
I0521 04:01:17.162269 22284 solver.cpp:253]     Train net output #0: loss = 1.74199 (* 1 = 1.74199 loss)
I0521 04:01:17.162283 22284 sgd_solver.cpp:106] Iteration 2088, lr = 0.0025
I0521 04:01:25.053450 22284 solver.cpp:237] Iteration 2112, loss = 1.68903
I0521 04:01:25.053488 22284 solver.cpp:253]     Train net output #0: loss = 1.68903 (* 1 = 1.68903 loss)
I0521 04:01:25.053504 22284 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 04:01:55.107350 22284 solver.cpp:237] Iteration 2136, loss = 1.6451
I0521 04:01:55.107522 22284 solver.cpp:253]     Train net output #0: loss = 1.6451 (* 1 = 1.6451 loss)
I0521 04:01:55.107537 22284 sgd_solver.cpp:106] Iteration 2136, lr = 0.0025
I0521 04:02:02.997305 22284 solver.cpp:237] Iteration 2160, loss = 1.63661
I0521 04:02:02.997337 22284 solver.cpp:253]     Train net output #0: loss = 1.63661 (* 1 = 1.63661 loss)
I0521 04:02:02.997354 22284 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0521 04:02:10.890074 22284 solver.cpp:237] Iteration 2184, loss = 1.69603
I0521 04:02:10.890113 22284 solver.cpp:253]     Train net output #0: loss = 1.69603 (* 1 = 1.69603 loss)
I0521 04:02:10.890127 22284 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0521 04:02:17.465365 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_2205.caffemodel
I0521 04:02:17.735731 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_2205.solverstate
I0521 04:02:18.846879 22284 solver.cpp:237] Iteration 2208, loss = 1.65924
I0521 04:02:18.846926 22284 solver.cpp:253]     Train net output #0: loss = 1.65924 (* 1 = 1.65924 loss)
I0521 04:02:18.846942 22284 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0521 04:02:26.732453 22284 solver.cpp:237] Iteration 2232, loss = 1.61982
I0521 04:02:26.732607 22284 solver.cpp:253]     Train net output #0: loss = 1.61982 (* 1 = 1.61982 loss)
I0521 04:02:26.732620 22284 sgd_solver.cpp:106] Iteration 2232, lr = 0.0025
I0521 04:02:34.618180 22284 solver.cpp:237] Iteration 2256, loss = 1.58758
I0521 04:02:34.618211 22284 solver.cpp:253]     Train net output #0: loss = 1.58758 (* 1 = 1.58758 loss)
I0521 04:02:34.618229 22284 sgd_solver.cpp:106] Iteration 2256, lr = 0.0025
I0521 04:02:42.496969 22284 solver.cpp:237] Iteration 2280, loss = 1.59142
I0521 04:02:42.497011 22284 solver.cpp:253]     Train net output #0: loss = 1.59142 (* 1 = 1.59142 loss)
I0521 04:02:42.497032 22284 sgd_solver.cpp:106] Iteration 2280, lr = 0.0025
I0521 04:03:12.574924 22284 solver.cpp:237] Iteration 2304, loss = 1.63647
I0521 04:03:12.575098 22284 solver.cpp:253]     Train net output #0: loss = 1.63647 (* 1 = 1.63647 loss)
I0521 04:03:12.575114 22284 sgd_solver.cpp:106] Iteration 2304, lr = 0.0025
I0521 04:03:20.460995 22284 solver.cpp:237] Iteration 2328, loss = 1.70672
I0521 04:03:20.461026 22284 solver.cpp:253]     Train net output #0: loss = 1.70672 (* 1 = 1.70672 loss)
I0521 04:03:20.461045 22284 sgd_solver.cpp:106] Iteration 2328, lr = 0.0025
I0521 04:03:28.349441 22284 solver.cpp:237] Iteration 2352, loss = 1.66432
I0521 04:03:28.349486 22284 solver.cpp:253]     Train net output #0: loss = 1.66432 (* 1 = 1.66432 loss)
I0521 04:03:28.349503 22284 sgd_solver.cpp:106] Iteration 2352, lr = 0.0025
I0521 04:03:36.236462 22284 solver.cpp:237] Iteration 2376, loss = 1.60359
I0521 04:03:36.236495 22284 solver.cpp:253]     Train net output #0: loss = 1.60359 (* 1 = 1.60359 loss)
I0521 04:03:36.236511 22284 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0521 04:03:44.126112 22284 solver.cpp:237] Iteration 2400, loss = 1.64079
I0521 04:03:44.126256 22284 solver.cpp:253]     Train net output #0: loss = 1.64079 (* 1 = 1.64079 loss)
I0521 04:03:44.126269 22284 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0521 04:03:52.014631 22284 solver.cpp:237] Iteration 2424, loss = 1.64676
I0521 04:03:52.014668 22284 solver.cpp:253]     Train net output #0: loss = 1.64676 (* 1 = 1.64676 loss)
I0521 04:03:52.014689 22284 sgd_solver.cpp:106] Iteration 2424, lr = 0.0025
I0521 04:03:59.899245 22284 solver.cpp:237] Iteration 2448, loss = 1.5631
I0521 04:03:59.899276 22284 solver.cpp:253]     Train net output #0: loss = 1.5631 (* 1 = 1.5631 loss)
I0521 04:03:59.899293 22284 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0521 04:04:00.229033 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_2450.caffemodel
I0521 04:04:00.500958 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_2450.solverstate
I0521 04:04:01.941005 22284 solver.cpp:341] Iteration 2455, Testing net (#0)
I0521 04:04:47.172297 22284 solver.cpp:409]     Test net output #0: accuracy = 0.689595
I0521 04:04:47.172459 22284 solver.cpp:409]     Test net output #1: loss = 1.07453 (* 1 = 1.07453 loss)
I0521 04:04:48.256783 22284 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_2459.caffemodel
I0521 04:04:48.526926 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767_iter_2459.solverstate
I0521 04:04:48.555235 22284 solver.cpp:326] Optimization Done.
I0521 04:04:48.555263 22284 caffe.cpp:215] Optimization Done.
Application 11236730 resources: utime ~1251s, stime ~226s, Rss ~5329440, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_610_2016-05-20T11.20.54.782767.solver"
	User time (seconds): 0.55
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:40.67
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15070
	Voluntary context switches: 2700
	Involuntary context switches: 116
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
