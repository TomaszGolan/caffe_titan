2806406
I0521 09:55:06.188604 32608 caffe.cpp:184] Using GPUs 0
I0521 09:55:06.628854 32608 solver.cpp:48] Initializing solver from parameters: 
test_iter: 163
test_interval: 326
base_lr: 0.0025
display: 16
max_iter: 1630
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 163
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795.prototxt"
I0521 09:55:06.630518 32608 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795.prototxt
I0521 09:55:06.644165 32608 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 09:55:06.644230 32608 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 09:55:06.644608 32608 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 920
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:55:06.644814 32608 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:55:06.644850 32608 net.cpp:106] Creating Layer data_hdf5
I0521 09:55:06.644870 32608 net.cpp:411] data_hdf5 -> data
I0521 09:55:06.644909 32608 net.cpp:411] data_hdf5 -> label
I0521 09:55:06.644948 32608 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 09:55:06.646153 32608 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 09:55:06.648401 32608 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 09:55:28.197346 32608 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 09:55:28.202527 32608 net.cpp:150] Setting up data_hdf5
I0521 09:55:28.202569 32608 net.cpp:157] Top shape: 920 1 127 50 (5842000)
I0521 09:55:28.202585 32608 net.cpp:157] Top shape: 920 (920)
I0521 09:55:28.202599 32608 net.cpp:165] Memory required for data: 23371680
I0521 09:55:28.202617 32608 layer_factory.hpp:77] Creating layer conv1
I0521 09:55:28.202664 32608 net.cpp:106] Creating Layer conv1
I0521 09:55:28.202678 32608 net.cpp:454] conv1 <- data
I0521 09:55:28.202702 32608 net.cpp:411] conv1 -> conv1
I0521 09:55:28.569376 32608 net.cpp:150] Setting up conv1
I0521 09:55:28.569430 32608 net.cpp:157] Top shape: 920 12 120 48 (63590400)
I0521 09:55:28.569445 32608 net.cpp:165] Memory required for data: 277733280
I0521 09:55:28.569475 32608 layer_factory.hpp:77] Creating layer relu1
I0521 09:55:28.569504 32608 net.cpp:106] Creating Layer relu1
I0521 09:55:28.569538 32608 net.cpp:454] relu1 <- conv1
I0521 09:55:28.569555 32608 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:55:28.570088 32608 net.cpp:150] Setting up relu1
I0521 09:55:28.570112 32608 net.cpp:157] Top shape: 920 12 120 48 (63590400)
I0521 09:55:28.570127 32608 net.cpp:165] Memory required for data: 532094880
I0521 09:55:28.570138 32608 layer_factory.hpp:77] Creating layer pool1
I0521 09:55:28.570169 32608 net.cpp:106] Creating Layer pool1
I0521 09:55:28.570183 32608 net.cpp:454] pool1 <- conv1
I0521 09:55:28.570199 32608 net.cpp:411] pool1 -> pool1
I0521 09:55:28.570291 32608 net.cpp:150] Setting up pool1
I0521 09:55:28.570309 32608 net.cpp:157] Top shape: 920 12 60 48 (31795200)
I0521 09:55:28.570324 32608 net.cpp:165] Memory required for data: 659275680
I0521 09:55:28.570348 32608 layer_factory.hpp:77] Creating layer conv2
I0521 09:55:28.570372 32608 net.cpp:106] Creating Layer conv2
I0521 09:55:28.570394 32608 net.cpp:454] conv2 <- pool1
I0521 09:55:28.570410 32608 net.cpp:411] conv2 -> conv2
I0521 09:55:28.573104 32608 net.cpp:150] Setting up conv2
I0521 09:55:28.573134 32608 net.cpp:157] Top shape: 920 20 54 46 (45705600)
I0521 09:55:28.573148 32608 net.cpp:165] Memory required for data: 842098080
I0521 09:55:28.573176 32608 layer_factory.hpp:77] Creating layer relu2
I0521 09:55:28.573204 32608 net.cpp:106] Creating Layer relu2
I0521 09:55:28.573220 32608 net.cpp:454] relu2 <- conv2
I0521 09:55:28.573235 32608 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:55:28.573590 32608 net.cpp:150] Setting up relu2
I0521 09:55:28.573611 32608 net.cpp:157] Top shape: 920 20 54 46 (45705600)
I0521 09:55:28.573623 32608 net.cpp:165] Memory required for data: 1024920480
I0521 09:55:28.573637 32608 layer_factory.hpp:77] Creating layer pool2
I0521 09:55:28.573662 32608 net.cpp:106] Creating Layer pool2
I0521 09:55:28.573675 32608 net.cpp:454] pool2 <- conv2
I0521 09:55:28.573710 32608 net.cpp:411] pool2 -> pool2
I0521 09:55:28.573794 32608 net.cpp:150] Setting up pool2
I0521 09:55:28.573812 32608 net.cpp:157] Top shape: 920 20 27 46 (22852800)
I0521 09:55:28.573824 32608 net.cpp:165] Memory required for data: 1116331680
I0521 09:55:28.573845 32608 layer_factory.hpp:77] Creating layer conv3
I0521 09:55:28.573868 32608 net.cpp:106] Creating Layer conv3
I0521 09:55:28.573880 32608 net.cpp:454] conv3 <- pool2
I0521 09:55:28.573897 32608 net.cpp:411] conv3 -> conv3
I0521 09:55:28.575841 32608 net.cpp:150] Setting up conv3
I0521 09:55:28.575866 32608 net.cpp:157] Top shape: 920 28 22 44 (24935680)
I0521 09:55:28.575886 32608 net.cpp:165] Memory required for data: 1216074400
I0521 09:55:28.575909 32608 layer_factory.hpp:77] Creating layer relu3
I0521 09:55:28.575932 32608 net.cpp:106] Creating Layer relu3
I0521 09:55:28.575953 32608 net.cpp:454] relu3 <- conv3
I0521 09:55:28.575969 32608 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:55:28.576453 32608 net.cpp:150] Setting up relu3
I0521 09:55:28.576478 32608 net.cpp:157] Top shape: 920 28 22 44 (24935680)
I0521 09:55:28.576490 32608 net.cpp:165] Memory required for data: 1315817120
I0521 09:55:28.576506 32608 layer_factory.hpp:77] Creating layer pool3
I0521 09:55:28.576530 32608 net.cpp:106] Creating Layer pool3
I0521 09:55:28.576545 32608 net.cpp:454] pool3 <- conv3
I0521 09:55:28.576560 32608 net.cpp:411] pool3 -> pool3
I0521 09:55:28.576640 32608 net.cpp:150] Setting up pool3
I0521 09:55:28.576663 32608 net.cpp:157] Top shape: 920 28 11 44 (12467840)
I0521 09:55:28.576675 32608 net.cpp:165] Memory required for data: 1365688480
I0521 09:55:28.576690 32608 layer_factory.hpp:77] Creating layer conv4
I0521 09:55:28.576716 32608 net.cpp:106] Creating Layer conv4
I0521 09:55:28.576730 32608 net.cpp:454] conv4 <- pool3
I0521 09:55:28.576747 32608 net.cpp:411] conv4 -> conv4
I0521 09:55:28.579493 32608 net.cpp:150] Setting up conv4
I0521 09:55:28.579524 32608 net.cpp:157] Top shape: 920 36 6 42 (8346240)
I0521 09:55:28.579540 32608 net.cpp:165] Memory required for data: 1399073440
I0521 09:55:28.579560 32608 layer_factory.hpp:77] Creating layer relu4
I0521 09:55:28.579581 32608 net.cpp:106] Creating Layer relu4
I0521 09:55:28.579607 32608 net.cpp:454] relu4 <- conv4
I0521 09:55:28.579623 32608 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:55:28.580111 32608 net.cpp:150] Setting up relu4
I0521 09:55:28.580134 32608 net.cpp:157] Top shape: 920 36 6 42 (8346240)
I0521 09:55:28.580148 32608 net.cpp:165] Memory required for data: 1432458400
I0521 09:55:28.580164 32608 layer_factory.hpp:77] Creating layer pool4
I0521 09:55:28.580179 32608 net.cpp:106] Creating Layer pool4
I0521 09:55:28.580201 32608 net.cpp:454] pool4 <- conv4
I0521 09:55:28.580217 32608 net.cpp:411] pool4 -> pool4
I0521 09:55:28.580298 32608 net.cpp:150] Setting up pool4
I0521 09:55:28.580322 32608 net.cpp:157] Top shape: 920 36 3 42 (4173120)
I0521 09:55:28.580335 32608 net.cpp:165] Memory required for data: 1449150880
I0521 09:55:28.580350 32608 layer_factory.hpp:77] Creating layer ip1
I0521 09:55:28.580379 32608 net.cpp:106] Creating Layer ip1
I0521 09:55:28.580392 32608 net.cpp:454] ip1 <- pool4
I0521 09:55:28.580409 32608 net.cpp:411] ip1 -> ip1
I0521 09:55:28.595829 32608 net.cpp:150] Setting up ip1
I0521 09:55:28.595860 32608 net.cpp:157] Top shape: 920 196 (180320)
I0521 09:55:28.595882 32608 net.cpp:165] Memory required for data: 1449872160
I0521 09:55:28.595908 32608 layer_factory.hpp:77] Creating layer relu5
I0521 09:55:28.595930 32608 net.cpp:106] Creating Layer relu5
I0521 09:55:28.595957 32608 net.cpp:454] relu5 <- ip1
I0521 09:55:28.595973 32608 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:55:28.596328 32608 net.cpp:150] Setting up relu5
I0521 09:55:28.596349 32608 net.cpp:157] Top shape: 920 196 (180320)
I0521 09:55:28.596361 32608 net.cpp:165] Memory required for data: 1450593440
I0521 09:55:28.596377 32608 layer_factory.hpp:77] Creating layer drop1
I0521 09:55:28.596407 32608 net.cpp:106] Creating Layer drop1
I0521 09:55:28.596421 32608 net.cpp:454] drop1 <- ip1
I0521 09:55:28.596449 32608 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:55:28.596508 32608 net.cpp:150] Setting up drop1
I0521 09:55:28.596534 32608 net.cpp:157] Top shape: 920 196 (180320)
I0521 09:55:28.596546 32608 net.cpp:165] Memory required for data: 1451314720
I0521 09:55:28.596559 32608 layer_factory.hpp:77] Creating layer ip2
I0521 09:55:28.596588 32608 net.cpp:106] Creating Layer ip2
I0521 09:55:28.596602 32608 net.cpp:454] ip2 <- ip1
I0521 09:55:28.596619 32608 net.cpp:411] ip2 -> ip2
I0521 09:55:28.597112 32608 net.cpp:150] Setting up ip2
I0521 09:55:28.597131 32608 net.cpp:157] Top shape: 920 98 (90160)
I0521 09:55:28.597144 32608 net.cpp:165] Memory required for data: 1451675360
I0521 09:55:28.597164 32608 layer_factory.hpp:77] Creating layer relu6
I0521 09:55:28.597180 32608 net.cpp:106] Creating Layer relu6
I0521 09:55:28.597201 32608 net.cpp:454] relu6 <- ip2
I0521 09:55:28.597216 32608 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:55:28.597767 32608 net.cpp:150] Setting up relu6
I0521 09:55:28.597790 32608 net.cpp:157] Top shape: 920 98 (90160)
I0521 09:55:28.597805 32608 net.cpp:165] Memory required for data: 1452036000
I0521 09:55:28.597820 32608 layer_factory.hpp:77] Creating layer drop2
I0521 09:55:28.597842 32608 net.cpp:106] Creating Layer drop2
I0521 09:55:28.597856 32608 net.cpp:454] drop2 <- ip2
I0521 09:55:28.597872 32608 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:55:28.597920 32608 net.cpp:150] Setting up drop2
I0521 09:55:28.597944 32608 net.cpp:157] Top shape: 920 98 (90160)
I0521 09:55:28.597956 32608 net.cpp:165] Memory required for data: 1452396640
I0521 09:55:28.597975 32608 layer_factory.hpp:77] Creating layer ip3
I0521 09:55:28.597991 32608 net.cpp:106] Creating Layer ip3
I0521 09:55:28.598006 32608 net.cpp:454] ip3 <- ip2
I0521 09:55:28.598022 32608 net.cpp:411] ip3 -> ip3
I0521 09:55:28.598253 32608 net.cpp:150] Setting up ip3
I0521 09:55:28.598271 32608 net.cpp:157] Top shape: 920 11 (10120)
I0521 09:55:28.598284 32608 net.cpp:165] Memory required for data: 1452437120
I0521 09:55:28.598305 32608 layer_factory.hpp:77] Creating layer drop3
I0521 09:55:28.598326 32608 net.cpp:106] Creating Layer drop3
I0521 09:55:28.598346 32608 net.cpp:454] drop3 <- ip3
I0521 09:55:28.598361 32608 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:55:28.598407 32608 net.cpp:150] Setting up drop3
I0521 09:55:28.598431 32608 net.cpp:157] Top shape: 920 11 (10120)
I0521 09:55:28.598444 32608 net.cpp:165] Memory required for data: 1452477600
I0521 09:55:28.598456 32608 layer_factory.hpp:77] Creating layer loss
I0521 09:55:28.598479 32608 net.cpp:106] Creating Layer loss
I0521 09:55:28.598492 32608 net.cpp:454] loss <- ip3
I0521 09:55:28.598513 32608 net.cpp:454] loss <- label
I0521 09:55:28.598528 32608 net.cpp:411] loss -> loss
I0521 09:55:28.598548 32608 layer_factory.hpp:77] Creating layer loss
I0521 09:55:28.599226 32608 net.cpp:150] Setting up loss
I0521 09:55:28.599247 32608 net.cpp:157] Top shape: (1)
I0521 09:55:28.599264 32608 net.cpp:160]     with loss weight 1
I0521 09:55:28.599315 32608 net.cpp:165] Memory required for data: 1452477604
I0521 09:55:28.599339 32608 net.cpp:226] loss needs backward computation.
I0521 09:55:28.599354 32608 net.cpp:226] drop3 needs backward computation.
I0521 09:55:28.599366 32608 net.cpp:226] ip3 needs backward computation.
I0521 09:55:28.599380 32608 net.cpp:226] drop2 needs backward computation.
I0521 09:55:28.599391 32608 net.cpp:226] relu6 needs backward computation.
I0521 09:55:28.599407 32608 net.cpp:226] ip2 needs backward computation.
I0521 09:55:28.599427 32608 net.cpp:226] drop1 needs backward computation.
I0521 09:55:28.599441 32608 net.cpp:226] relu5 needs backward computation.
I0521 09:55:28.599453 32608 net.cpp:226] ip1 needs backward computation.
I0521 09:55:28.599467 32608 net.cpp:226] pool4 needs backward computation.
I0521 09:55:28.599480 32608 net.cpp:226] relu4 needs backward computation.
I0521 09:55:28.599493 32608 net.cpp:226] conv4 needs backward computation.
I0521 09:55:28.599509 32608 net.cpp:226] pool3 needs backward computation.
I0521 09:55:28.599537 32608 net.cpp:226] relu3 needs backward computation.
I0521 09:55:28.599550 32608 net.cpp:226] conv3 needs backward computation.
I0521 09:55:28.599565 32608 net.cpp:226] pool2 needs backward computation.
I0521 09:55:28.599577 32608 net.cpp:226] relu2 needs backward computation.
I0521 09:55:28.599589 32608 net.cpp:226] conv2 needs backward computation.
I0521 09:55:28.599606 32608 net.cpp:226] pool1 needs backward computation.
I0521 09:55:28.599624 32608 net.cpp:226] relu1 needs backward computation.
I0521 09:55:28.599637 32608 net.cpp:226] conv1 needs backward computation.
I0521 09:55:28.599650 32608 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:55:28.599663 32608 net.cpp:270] This network produces output loss
I0521 09:55:28.599689 32608 net.cpp:283] Network initialization done.
I0521 09:55:28.601284 32608 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795.prototxt
I0521 09:55:28.601362 32608 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 09:55:28.601744 32608 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 920
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:55:28.601966 32608 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:55:28.601986 32608 net.cpp:106] Creating Layer data_hdf5
I0521 09:55:28.602001 32608 net.cpp:411] data_hdf5 -> data
I0521 09:55:28.602020 32608 net.cpp:411] data_hdf5 -> label
I0521 09:55:28.602041 32608 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 09:55:28.603189 32608 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 09:55:49.968348 32608 net.cpp:150] Setting up data_hdf5
I0521 09:55:49.968518 32608 net.cpp:157] Top shape: 920 1 127 50 (5842000)
I0521 09:55:49.968535 32608 net.cpp:157] Top shape: 920 (920)
I0521 09:55:49.968549 32608 net.cpp:165] Memory required for data: 23371680
I0521 09:55:49.968564 32608 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 09:55:49.968597 32608 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 09:55:49.968611 32608 net.cpp:454] label_data_hdf5_1_split <- label
I0521 09:55:49.968646 32608 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 09:55:49.968668 32608 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 09:55:49.968750 32608 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 09:55:49.968775 32608 net.cpp:157] Top shape: 920 (920)
I0521 09:55:49.968789 32608 net.cpp:157] Top shape: 920 (920)
I0521 09:55:49.968813 32608 net.cpp:165] Memory required for data: 23379040
I0521 09:55:49.968827 32608 layer_factory.hpp:77] Creating layer conv1
I0521 09:55:49.968866 32608 net.cpp:106] Creating Layer conv1
I0521 09:55:49.968881 32608 net.cpp:454] conv1 <- data
I0521 09:55:49.968899 32608 net.cpp:411] conv1 -> conv1
I0521 09:55:49.970861 32608 net.cpp:150] Setting up conv1
I0521 09:55:49.970886 32608 net.cpp:157] Top shape: 920 12 120 48 (63590400)
I0521 09:55:49.970906 32608 net.cpp:165] Memory required for data: 277740640
I0521 09:55:49.970929 32608 layer_factory.hpp:77] Creating layer relu1
I0521 09:55:49.970952 32608 net.cpp:106] Creating Layer relu1
I0521 09:55:49.970974 32608 net.cpp:454] relu1 <- conv1
I0521 09:55:49.970990 32608 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:55:49.971503 32608 net.cpp:150] Setting up relu1
I0521 09:55:49.971526 32608 net.cpp:157] Top shape: 920 12 120 48 (63590400)
I0521 09:55:49.971540 32608 net.cpp:165] Memory required for data: 532102240
I0521 09:55:49.971554 32608 layer_factory.hpp:77] Creating layer pool1
I0521 09:55:49.971583 32608 net.cpp:106] Creating Layer pool1
I0521 09:55:49.971597 32608 net.cpp:454] pool1 <- conv1
I0521 09:55:49.971613 32608 net.cpp:411] pool1 -> pool1
I0521 09:55:49.971703 32608 net.cpp:150] Setting up pool1
I0521 09:55:49.971719 32608 net.cpp:157] Top shape: 920 12 60 48 (31795200)
I0521 09:55:49.971735 32608 net.cpp:165] Memory required for data: 659283040
I0521 09:55:49.971752 32608 layer_factory.hpp:77] Creating layer conv2
I0521 09:55:49.971773 32608 net.cpp:106] Creating Layer conv2
I0521 09:55:49.971786 32608 net.cpp:454] conv2 <- pool1
I0521 09:55:49.971803 32608 net.cpp:411] conv2 -> conv2
I0521 09:55:49.973762 32608 net.cpp:150] Setting up conv2
I0521 09:55:49.973785 32608 net.cpp:157] Top shape: 920 20 54 46 (45705600)
I0521 09:55:49.973806 32608 net.cpp:165] Memory required for data: 842105440
I0521 09:55:49.973829 32608 layer_factory.hpp:77] Creating layer relu2
I0521 09:55:49.973848 32608 net.cpp:106] Creating Layer relu2
I0521 09:55:49.973870 32608 net.cpp:454] relu2 <- conv2
I0521 09:55:49.973886 32608 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:55:49.974236 32608 net.cpp:150] Setting up relu2
I0521 09:55:49.974256 32608 net.cpp:157] Top shape: 920 20 54 46 (45705600)
I0521 09:55:49.974268 32608 net.cpp:165] Memory required for data: 1024927840
I0521 09:55:49.974284 32608 layer_factory.hpp:77] Creating layer pool2
I0521 09:55:49.974306 32608 net.cpp:106] Creating Layer pool2
I0521 09:55:49.974320 32608 net.cpp:454] pool2 <- conv2
I0521 09:55:49.974335 32608 net.cpp:411] pool2 -> pool2
I0521 09:55:49.974422 32608 net.cpp:150] Setting up pool2
I0521 09:55:49.974441 32608 net.cpp:157] Top shape: 920 20 27 46 (22852800)
I0521 09:55:49.974455 32608 net.cpp:165] Memory required for data: 1116339040
I0521 09:55:49.974473 32608 layer_factory.hpp:77] Creating layer conv3
I0521 09:55:49.974496 32608 net.cpp:106] Creating Layer conv3
I0521 09:55:49.974509 32608 net.cpp:454] conv3 <- pool2
I0521 09:55:49.974526 32608 net.cpp:411] conv3 -> conv3
I0521 09:55:49.976533 32608 net.cpp:150] Setting up conv3
I0521 09:55:49.976558 32608 net.cpp:157] Top shape: 920 28 22 44 (24935680)
I0521 09:55:49.976577 32608 net.cpp:165] Memory required for data: 1216081760
I0521 09:55:49.976618 32608 layer_factory.hpp:77] Creating layer relu3
I0521 09:55:49.976642 32608 net.cpp:106] Creating Layer relu3
I0521 09:55:49.976656 32608 net.cpp:454] relu3 <- conv3
I0521 09:55:49.976671 32608 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:55:49.977176 32608 net.cpp:150] Setting up relu3
I0521 09:55:49.977200 32608 net.cpp:157] Top shape: 920 28 22 44 (24935680)
I0521 09:55:49.977215 32608 net.cpp:165] Memory required for data: 1315824480
I0521 09:55:49.977231 32608 layer_factory.hpp:77] Creating layer pool3
I0521 09:55:49.977255 32608 net.cpp:106] Creating Layer pool3
I0521 09:55:49.977269 32608 net.cpp:454] pool3 <- conv3
I0521 09:55:49.977285 32608 net.cpp:411] pool3 -> pool3
I0521 09:55:49.977370 32608 net.cpp:150] Setting up pool3
I0521 09:55:49.977388 32608 net.cpp:157] Top shape: 920 28 11 44 (12467840)
I0521 09:55:49.977402 32608 net.cpp:165] Memory required for data: 1365695840
I0521 09:55:49.977414 32608 layer_factory.hpp:77] Creating layer conv4
I0521 09:55:49.977444 32608 net.cpp:106] Creating Layer conv4
I0521 09:55:49.977458 32608 net.cpp:454] conv4 <- pool3
I0521 09:55:49.977475 32608 net.cpp:411] conv4 -> conv4
I0521 09:55:49.979559 32608 net.cpp:150] Setting up conv4
I0521 09:55:49.979583 32608 net.cpp:157] Top shape: 920 36 6 42 (8346240)
I0521 09:55:49.979604 32608 net.cpp:165] Memory required for data: 1399080800
I0521 09:55:49.979624 32608 layer_factory.hpp:77] Creating layer relu4
I0521 09:55:49.979642 32608 net.cpp:106] Creating Layer relu4
I0521 09:55:49.979655 32608 net.cpp:454] relu4 <- conv4
I0521 09:55:49.979681 32608 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:55:49.980168 32608 net.cpp:150] Setting up relu4
I0521 09:55:49.980191 32608 net.cpp:157] Top shape: 920 36 6 42 (8346240)
I0521 09:55:49.980204 32608 net.cpp:165] Memory required for data: 1432465760
I0521 09:55:49.980221 32608 layer_factory.hpp:77] Creating layer pool4
I0521 09:55:49.980245 32608 net.cpp:106] Creating Layer pool4
I0521 09:55:49.980258 32608 net.cpp:454] pool4 <- conv4
I0521 09:55:49.980275 32608 net.cpp:411] pool4 -> pool4
I0521 09:55:49.980361 32608 net.cpp:150] Setting up pool4
I0521 09:55:49.980378 32608 net.cpp:157] Top shape: 920 36 3 42 (4173120)
I0521 09:55:49.980393 32608 net.cpp:165] Memory required for data: 1449158240
I0521 09:55:49.980406 32608 layer_factory.hpp:77] Creating layer ip1
I0521 09:55:49.980430 32608 net.cpp:106] Creating Layer ip1
I0521 09:55:49.980443 32608 net.cpp:454] ip1 <- pool4
I0521 09:55:49.980459 32608 net.cpp:411] ip1 -> ip1
I0521 09:55:49.995928 32608 net.cpp:150] Setting up ip1
I0521 09:55:49.995960 32608 net.cpp:157] Top shape: 920 196 (180320)
I0521 09:55:49.995981 32608 net.cpp:165] Memory required for data: 1449879520
I0521 09:55:49.996007 32608 layer_factory.hpp:77] Creating layer relu5
I0521 09:55:49.996029 32608 net.cpp:106] Creating Layer relu5
I0521 09:55:49.996054 32608 net.cpp:454] relu5 <- ip1
I0521 09:55:49.996071 32608 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:55:49.996434 32608 net.cpp:150] Setting up relu5
I0521 09:55:49.996454 32608 net.cpp:157] Top shape: 920 196 (180320)
I0521 09:55:49.996467 32608 net.cpp:165] Memory required for data: 1450600800
I0521 09:55:49.996482 32608 layer_factory.hpp:77] Creating layer drop1
I0521 09:55:49.996511 32608 net.cpp:106] Creating Layer drop1
I0521 09:55:49.996526 32608 net.cpp:454] drop1 <- ip1
I0521 09:55:49.996541 32608 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:55:49.996599 32608 net.cpp:150] Setting up drop1
I0521 09:55:49.996615 32608 net.cpp:157] Top shape: 920 196 (180320)
I0521 09:55:49.996628 32608 net.cpp:165] Memory required for data: 1451322080
I0521 09:55:49.996641 32608 layer_factory.hpp:77] Creating layer ip2
I0521 09:55:49.996662 32608 net.cpp:106] Creating Layer ip2
I0521 09:55:49.996675 32608 net.cpp:454] ip2 <- ip1
I0521 09:55:49.996698 32608 net.cpp:411] ip2 -> ip2
I0521 09:55:49.997198 32608 net.cpp:150] Setting up ip2
I0521 09:55:49.997217 32608 net.cpp:157] Top shape: 920 98 (90160)
I0521 09:55:49.997231 32608 net.cpp:165] Memory required for data: 1451682720
I0521 09:55:49.997263 32608 layer_factory.hpp:77] Creating layer relu6
I0521 09:55:49.997287 32608 net.cpp:106] Creating Layer relu6
I0521 09:55:49.997300 32608 net.cpp:454] relu6 <- ip2
I0521 09:55:49.997316 32608 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:55:49.997881 32608 net.cpp:150] Setting up relu6
I0521 09:55:49.997905 32608 net.cpp:157] Top shape: 920 98 (90160)
I0521 09:55:49.997917 32608 net.cpp:165] Memory required for data: 1452043360
I0521 09:55:49.997930 32608 layer_factory.hpp:77] Creating layer drop2
I0521 09:55:49.997949 32608 net.cpp:106] Creating Layer drop2
I0521 09:55:49.997970 32608 net.cpp:454] drop2 <- ip2
I0521 09:55:49.997987 32608 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:55:49.998037 32608 net.cpp:150] Setting up drop2
I0521 09:55:49.998060 32608 net.cpp:157] Top shape: 920 98 (90160)
I0521 09:55:49.998073 32608 net.cpp:165] Memory required for data: 1452404000
I0521 09:55:49.998087 32608 layer_factory.hpp:77] Creating layer ip3
I0521 09:55:49.998103 32608 net.cpp:106] Creating Layer ip3
I0521 09:55:49.998118 32608 net.cpp:454] ip3 <- ip2
I0521 09:55:49.998142 32608 net.cpp:411] ip3 -> ip3
I0521 09:55:49.998380 32608 net.cpp:150] Setting up ip3
I0521 09:55:49.998399 32608 net.cpp:157] Top shape: 920 11 (10120)
I0521 09:55:49.998412 32608 net.cpp:165] Memory required for data: 1452444480
I0521 09:55:49.998433 32608 layer_factory.hpp:77] Creating layer drop3
I0521 09:55:49.998456 32608 net.cpp:106] Creating Layer drop3
I0521 09:55:49.998469 32608 net.cpp:454] drop3 <- ip3
I0521 09:55:49.998484 32608 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:55:49.998539 32608 net.cpp:150] Setting up drop3
I0521 09:55:49.998555 32608 net.cpp:157] Top shape: 920 11 (10120)
I0521 09:55:49.998569 32608 net.cpp:165] Memory required for data: 1452484960
I0521 09:55:49.998580 32608 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 09:55:49.998595 32608 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 09:55:49.998611 32608 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 09:55:49.998633 32608 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 09:55:49.998651 32608 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 09:55:49.998740 32608 net.cpp:150] Setting up ip3_drop3_0_split
I0521 09:55:49.998757 32608 net.cpp:157] Top shape: 920 11 (10120)
I0521 09:55:49.998772 32608 net.cpp:157] Top shape: 920 11 (10120)
I0521 09:55:49.998787 32608 net.cpp:165] Memory required for data: 1452565920
I0521 09:55:49.998800 32608 layer_factory.hpp:77] Creating layer accuracy
I0521 09:55:49.998828 32608 net.cpp:106] Creating Layer accuracy
I0521 09:55:49.998842 32608 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 09:55:49.998855 32608 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 09:55:49.998874 32608 net.cpp:411] accuracy -> accuracy
I0521 09:55:49.998900 32608 net.cpp:150] Setting up accuracy
I0521 09:55:49.998924 32608 net.cpp:157] Top shape: (1)
I0521 09:55:49.998941 32608 net.cpp:165] Memory required for data: 1452565924
I0521 09:55:49.998955 32608 layer_factory.hpp:77] Creating layer loss
I0521 09:55:49.998970 32608 net.cpp:106] Creating Layer loss
I0521 09:55:49.998985 32608 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 09:55:49.998998 32608 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 09:55:49.999022 32608 net.cpp:411] loss -> loss
I0521 09:55:49.999042 32608 layer_factory.hpp:77] Creating layer loss
I0521 09:55:49.999564 32608 net.cpp:150] Setting up loss
I0521 09:55:49.999584 32608 net.cpp:157] Top shape: (1)
I0521 09:55:49.999596 32608 net.cpp:160]     with loss weight 1
I0521 09:55:49.999622 32608 net.cpp:165] Memory required for data: 1452565928
I0521 09:55:49.999644 32608 net.cpp:226] loss needs backward computation.
I0521 09:55:49.999657 32608 net.cpp:228] accuracy does not need backward computation.
I0521 09:55:49.999680 32608 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 09:55:49.999693 32608 net.cpp:226] drop3 needs backward computation.
I0521 09:55:49.999706 32608 net.cpp:226] ip3 needs backward computation.
I0521 09:55:49.999729 32608 net.cpp:226] drop2 needs backward computation.
I0521 09:55:49.999749 32608 net.cpp:226] relu6 needs backward computation.
I0521 09:55:49.999763 32608 net.cpp:226] ip2 needs backward computation.
I0521 09:55:49.999778 32608 net.cpp:226] drop1 needs backward computation.
I0521 09:55:49.999790 32608 net.cpp:226] relu5 needs backward computation.
I0521 09:55:49.999802 32608 net.cpp:226] ip1 needs backward computation.
I0521 09:55:49.999814 32608 net.cpp:226] pool4 needs backward computation.
I0521 09:55:49.999830 32608 net.cpp:226] relu4 needs backward computation.
I0521 09:55:49.999847 32608 net.cpp:226] conv4 needs backward computation.
I0521 09:55:49.999861 32608 net.cpp:226] pool3 needs backward computation.
I0521 09:55:49.999874 32608 net.cpp:226] relu3 needs backward computation.
I0521 09:55:49.999886 32608 net.cpp:226] conv3 needs backward computation.
I0521 09:55:49.999899 32608 net.cpp:226] pool2 needs backward computation.
I0521 09:55:49.999912 32608 net.cpp:226] relu2 needs backward computation.
I0521 09:55:49.999927 32608 net.cpp:226] conv2 needs backward computation.
I0521 09:55:49.999939 32608 net.cpp:226] pool1 needs backward computation.
I0521 09:55:49.999959 32608 net.cpp:226] relu1 needs backward computation.
I0521 09:55:49.999972 32608 net.cpp:226] conv1 needs backward computation.
I0521 09:55:49.999987 32608 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 09:55:50.000001 32608 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:55:50.000013 32608 net.cpp:270] This network produces output accuracy
I0521 09:55:50.000028 32608 net.cpp:270] This network produces output loss
I0521 09:55:50.000059 32608 net.cpp:283] Network initialization done.
I0521 09:55:50.000195 32608 solver.cpp:60] Solver scaffolding done.
I0521 09:55:50.001345 32608 caffe.cpp:212] Starting Optimization
I0521 09:55:50.001363 32608 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 09:55:50.001376 32608 solver.cpp:289] Learning Rate Policy: fixed
I0521 09:55:50.002596 32608 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 09:56:36.029994 32608 solver.cpp:409]     Test net output #0: accuracy = 0.164464
I0521 09:56:36.030163 32608 solver.cpp:409]     Test net output #1: loss = 2.39593 (* 1 = 2.39593 loss)
I0521 09:56:36.198626 32608 solver.cpp:237] Iteration 0, loss = 2.39545
I0521 09:56:36.198667 32608 solver.cpp:253]     Train net output #0: loss = 2.39545 (* 1 = 2.39545 loss)
I0521 09:56:36.198688 32608 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 09:56:44.090862 32608 solver.cpp:237] Iteration 16, loss = 2.38623
I0521 09:56:44.090898 32608 solver.cpp:253]     Train net output #0: loss = 2.38623 (* 1 = 2.38623 loss)
I0521 09:56:44.090920 32608 sgd_solver.cpp:106] Iteration 16, lr = 0.0025
I0521 09:56:51.984303 32608 solver.cpp:237] Iteration 32, loss = 2.37421
I0521 09:56:51.984352 32608 solver.cpp:253]     Train net output #0: loss = 2.37421 (* 1 = 2.37421 loss)
I0521 09:56:51.984369 32608 sgd_solver.cpp:106] Iteration 32, lr = 0.0025
I0521 09:56:59.877398 32608 solver.cpp:237] Iteration 48, loss = 2.34851
I0521 09:56:59.877430 32608 solver.cpp:253]     Train net output #0: loss = 2.34851 (* 1 = 2.34851 loss)
I0521 09:56:59.877454 32608 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0521 09:57:07.765547 32608 solver.cpp:237] Iteration 64, loss = 2.34555
I0521 09:57:07.765697 32608 solver.cpp:253]     Train net output #0: loss = 2.34555 (* 1 = 2.34555 loss)
I0521 09:57:07.765713 32608 sgd_solver.cpp:106] Iteration 64, lr = 0.0025
I0521 09:57:15.659140 32608 solver.cpp:237] Iteration 80, loss = 2.32831
I0521 09:57:15.659194 32608 solver.cpp:253]     Train net output #0: loss = 2.32831 (* 1 = 2.32831 loss)
I0521 09:57:15.659219 32608 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0521 09:57:23.551586 32608 solver.cpp:237] Iteration 96, loss = 2.33358
I0521 09:57:23.551620 32608 solver.cpp:253]     Train net output #0: loss = 2.33358 (* 1 = 2.33358 loss)
I0521 09:57:23.551641 32608 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 09:57:53.647609 32608 solver.cpp:237] Iteration 112, loss = 2.33318
I0521 09:57:53.647778 32608 solver.cpp:253]     Train net output #0: loss = 2.33318 (* 1 = 2.33318 loss)
I0521 09:57:53.647794 32608 sgd_solver.cpp:106] Iteration 112, lr = 0.0025
I0521 09:58:01.539788 32608 solver.cpp:237] Iteration 128, loss = 2.3301
I0521 09:58:01.539856 32608 solver.cpp:253]     Train net output #0: loss = 2.3301 (* 1 = 2.3301 loss)
I0521 09:58:01.539885 32608 sgd_solver.cpp:106] Iteration 128, lr = 0.0025
I0521 09:58:09.436265 32608 solver.cpp:237] Iteration 144, loss = 2.31674
I0521 09:58:09.436302 32608 solver.cpp:253]     Train net output #0: loss = 2.31674 (* 1 = 2.31674 loss)
I0521 09:58:09.436329 32608 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 09:58:17.336158 32608 solver.cpp:237] Iteration 160, loss = 2.32787
I0521 09:58:17.336192 32608 solver.cpp:253]     Train net output #0: loss = 2.32787 (* 1 = 2.32787 loss)
I0521 09:58:17.336216 32608 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 09:58:18.322054 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_163.caffemodel
I0521 09:58:18.710762 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_163.solverstate
I0521 09:58:25.300582 32608 solver.cpp:237] Iteration 176, loss = 2.28695
I0521 09:58:25.300748 32608 solver.cpp:253]     Train net output #0: loss = 2.28695 (* 1 = 2.28695 loss)
I0521 09:58:25.300765 32608 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 09:58:33.192319 32608 solver.cpp:237] Iteration 192, loss = 2.28919
I0521 09:58:33.192373 32608 solver.cpp:253]     Train net output #0: loss = 2.28919 (* 1 = 2.28919 loss)
I0521 09:58:33.192399 32608 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 09:58:41.084383 32608 solver.cpp:237] Iteration 208, loss = 2.2922
I0521 09:58:41.084417 32608 solver.cpp:253]     Train net output #0: loss = 2.2922 (* 1 = 2.2922 loss)
I0521 09:58:41.084440 32608 sgd_solver.cpp:106] Iteration 208, lr = 0.0025
I0521 09:59:11.151579 32608 solver.cpp:237] Iteration 224, loss = 2.26488
I0521 09:59:11.151749 32608 solver.cpp:253]     Train net output #0: loss = 2.26488 (* 1 = 2.26488 loss)
I0521 09:59:11.151767 32608 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 09:59:19.044311 32608 solver.cpp:237] Iteration 240, loss = 2.26286
I0521 09:59:19.044364 32608 solver.cpp:253]     Train net output #0: loss = 2.26286 (* 1 = 2.26286 loss)
I0521 09:59:19.044381 32608 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 09:59:26.937403 32608 solver.cpp:237] Iteration 256, loss = 2.23006
I0521 09:59:26.937438 32608 solver.cpp:253]     Train net output #0: loss = 2.23006 (* 1 = 2.23006 loss)
I0521 09:59:26.937456 32608 sgd_solver.cpp:106] Iteration 256, lr = 0.0025
I0521 09:59:34.828265 32608 solver.cpp:237] Iteration 272, loss = 2.20032
I0521 09:59:34.828300 32608 solver.cpp:253]     Train net output #0: loss = 2.20032 (* 1 = 2.20032 loss)
I0521 09:59:34.828320 32608 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 09:59:42.724629 32608 solver.cpp:237] Iteration 288, loss = 2.20151
I0521 09:59:42.724789 32608 solver.cpp:253]     Train net output #0: loss = 2.20151 (* 1 = 2.20151 loss)
I0521 09:59:42.724807 32608 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 09:59:50.618340 32608 solver.cpp:237] Iteration 304, loss = 2.16015
I0521 09:59:50.618376 32608 solver.cpp:253]     Train net output #0: loss = 2.16015 (* 1 = 2.16015 loss)
I0521 09:59:50.618394 32608 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0521 09:59:58.510443 32608 solver.cpp:237] Iteration 320, loss = 2.12806
I0521 09:59:58.510478 32608 solver.cpp:253]     Train net output #0: loss = 2.12806 (* 1 = 2.12806 loss)
I0521 09:59:58.510500 32608 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 10:00:00.978281 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_326.caffemodel
I0521 10:00:01.363253 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_326.solverstate
I0521 10:00:01.389497 32608 solver.cpp:341] Iteration 326, Testing net (#0)
I0521 10:00:46.523308 32608 solver.cpp:409]     Test net output #0: accuracy = 0.48675
I0521 10:00:46.523469 32608 solver.cpp:409]     Test net output #1: loss = 1.94589 (* 1 = 1.94589 loss)
I0521 10:01:13.761167 32608 solver.cpp:237] Iteration 336, loss = 2.07026
I0521 10:01:13.761224 32608 solver.cpp:253]     Train net output #0: loss = 2.07026 (* 1 = 2.07026 loss)
I0521 10:01:13.761250 32608 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 10:01:21.653957 32608 solver.cpp:237] Iteration 352, loss = 2.05921
I0521 10:01:21.654116 32608 solver.cpp:253]     Train net output #0: loss = 2.05921 (* 1 = 2.05921 loss)
I0521 10:01:21.654135 32608 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 10:01:29.546546 32608 solver.cpp:237] Iteration 368, loss = 2.08225
I0521 10:01:29.546581 32608 solver.cpp:253]     Train net output #0: loss = 2.08225 (* 1 = 2.08225 loss)
I0521 10:01:29.546603 32608 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0521 10:01:37.447563 32608 solver.cpp:237] Iteration 384, loss = 2.04869
I0521 10:01:37.447598 32608 solver.cpp:253]     Train net output #0: loss = 2.04869 (* 1 = 2.04869 loss)
I0521 10:01:37.447615 32608 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 10:01:45.341119 32608 solver.cpp:237] Iteration 400, loss = 2.0319
I0521 10:01:45.341150 32608 solver.cpp:253]     Train net output #0: loss = 2.0319 (* 1 = 2.0319 loss)
I0521 10:01:45.341174 32608 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 10:01:53.238288 32608 solver.cpp:237] Iteration 416, loss = 2.02962
I0521 10:01:53.238448 32608 solver.cpp:253]     Train net output #0: loss = 2.02962 (* 1 = 2.02962 loss)
I0521 10:01:53.238466 32608 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 10:02:01.136809 32608 solver.cpp:237] Iteration 432, loss = 1.94027
I0521 10:02:01.136848 32608 solver.cpp:253]     Train net output #0: loss = 1.94027 (* 1 = 1.94027 loss)
I0521 10:02:01.136865 32608 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 10:02:31.226724 32608 solver.cpp:237] Iteration 448, loss = 1.94813
I0521 10:02:31.226896 32608 solver.cpp:253]     Train net output #0: loss = 1.94813 (* 1 = 1.94813 loss)
I0521 10:02:31.226913 32608 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 10:02:39.127295 32608 solver.cpp:237] Iteration 464, loss = 1.98245
I0521 10:02:39.127348 32608 solver.cpp:253]     Train net output #0: loss = 1.98245 (* 1 = 1.98245 loss)
I0521 10:02:39.127365 32608 sgd_solver.cpp:106] Iteration 464, lr = 0.0025
I0521 10:02:47.020568 32608 solver.cpp:237] Iteration 480, loss = 1.95654
I0521 10:02:47.020602 32608 solver.cpp:253]     Train net output #0: loss = 1.95654 (* 1 = 1.95654 loss)
I0521 10:02:47.020627 32608 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 10:02:50.971266 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_489.caffemodel
I0521 10:02:51.358582 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_489.solverstate
I0521 10:02:54.986867 32608 solver.cpp:237] Iteration 496, loss = 1.96202
I0521 10:02:54.986920 32608 solver.cpp:253]     Train net output #0: loss = 1.96202 (* 1 = 1.96202 loss)
I0521 10:02:54.986937 32608 sgd_solver.cpp:106] Iteration 496, lr = 0.0025
I0521 10:03:02.882482 32608 solver.cpp:237] Iteration 512, loss = 1.95094
I0521 10:03:02.882648 32608 solver.cpp:253]     Train net output #0: loss = 1.95094 (* 1 = 1.95094 loss)
I0521 10:03:02.882666 32608 sgd_solver.cpp:106] Iteration 512, lr = 0.0025
I0521 10:03:10.779855 32608 solver.cpp:237] Iteration 528, loss = 1.96585
I0521 10:03:10.779891 32608 solver.cpp:253]     Train net output #0: loss = 1.96585 (* 1 = 1.96585 loss)
I0521 10:03:10.779908 32608 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 10:03:40.836149 32608 solver.cpp:237] Iteration 544, loss = 1.96231
I0521 10:03:40.836320 32608 solver.cpp:253]     Train net output #0: loss = 1.96231 (* 1 = 1.96231 loss)
I0521 10:03:40.836338 32608 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 10:03:48.729071 32608 solver.cpp:237] Iteration 560, loss = 1.86149
I0521 10:03:48.729105 32608 solver.cpp:253]     Train net output #0: loss = 1.86149 (* 1 = 1.86149 loss)
I0521 10:03:48.729130 32608 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 10:03:56.627301 32608 solver.cpp:237] Iteration 576, loss = 1.94286
I0521 10:03:56.627351 32608 solver.cpp:253]     Train net output #0: loss = 1.94286 (* 1 = 1.94286 loss)
I0521 10:03:56.627368 32608 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 10:04:04.518784 32608 solver.cpp:237] Iteration 592, loss = 1.85802
I0521 10:04:04.518816 32608 solver.cpp:253]     Train net output #0: loss = 1.85802 (* 1 = 1.85802 loss)
I0521 10:04:04.518839 32608 sgd_solver.cpp:106] Iteration 592, lr = 0.0025
I0521 10:04:12.411303 32608 solver.cpp:237] Iteration 608, loss = 1.92266
I0521 10:04:12.411453 32608 solver.cpp:253]     Train net output #0: loss = 1.92266 (* 1 = 1.92266 loss)
I0521 10:04:12.411468 32608 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 10:04:20.305420 32608 solver.cpp:237] Iteration 624, loss = 1.8483
I0521 10:04:20.305474 32608 solver.cpp:253]     Train net output #0: loss = 1.8483 (* 1 = 1.8483 loss)
I0521 10:04:20.305492 32608 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 10:04:28.206254 32608 solver.cpp:237] Iteration 640, loss = 1.87047
I0521 10:04:28.206286 32608 solver.cpp:253]     Train net output #0: loss = 1.87047 (* 1 = 1.87047 loss)
I0521 10:04:28.206310 32608 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 10:04:33.633219 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_652.caffemodel
I0521 10:04:34.021241 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_652.solverstate
I0521 10:04:34.048467 32608 solver.cpp:341] Iteration 652, Testing net (#0)
I0521 10:05:40.004704 32608 solver.cpp:409]     Test net output #0: accuracy = 0.597006
I0521 10:05:40.004899 32608 solver.cpp:409]     Test net output #1: loss = 1.4651 (* 1 = 1.4651 loss)
I0521 10:06:04.325062 32608 solver.cpp:237] Iteration 656, loss = 1.88574
I0521 10:06:04.325122 32608 solver.cpp:253]     Train net output #0: loss = 1.88574 (* 1 = 1.88574 loss)
I0521 10:06:04.325147 32608 sgd_solver.cpp:106] Iteration 656, lr = 0.0025
I0521 10:06:12.210947 32608 solver.cpp:237] Iteration 672, loss = 1.86665
I0521 10:06:12.211098 32608 solver.cpp:253]     Train net output #0: loss = 1.86665 (* 1 = 1.86665 loss)
I0521 10:06:12.211114 32608 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 10:06:20.091220 32608 solver.cpp:237] Iteration 688, loss = 1.85047
I0521 10:06:20.091254 32608 solver.cpp:253]     Train net output #0: loss = 1.85047 (* 1 = 1.85047 loss)
I0521 10:06:20.091277 32608 sgd_solver.cpp:106] Iteration 688, lr = 0.0025
I0521 10:06:27.969952 32608 solver.cpp:237] Iteration 704, loss = 1.83579
I0521 10:06:27.970005 32608 solver.cpp:253]     Train net output #0: loss = 1.83579 (* 1 = 1.83579 loss)
I0521 10:06:27.970029 32608 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 10:06:35.852291 32608 solver.cpp:237] Iteration 720, loss = 1.83948
I0521 10:06:35.852325 32608 solver.cpp:253]     Train net output #0: loss = 1.83948 (* 1 = 1.83948 loss)
I0521 10:06:35.852349 32608 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 10:06:43.740109 32608 solver.cpp:237] Iteration 736, loss = 1.89172
I0521 10:06:43.740252 32608 solver.cpp:253]     Train net output #0: loss = 1.89172 (* 1 = 1.89172 loss)
I0521 10:06:43.740268 32608 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 10:06:51.624722 32608 solver.cpp:237] Iteration 752, loss = 1.83464
I0521 10:06:51.624768 32608 solver.cpp:253]     Train net output #0: loss = 1.83464 (* 1 = 1.83464 loss)
I0521 10:06:51.624785 32608 sgd_solver.cpp:106] Iteration 752, lr = 0.0025
I0521 10:07:21.756670 32608 solver.cpp:237] Iteration 768, loss = 1.83122
I0521 10:07:21.756840 32608 solver.cpp:253]     Train net output #0: loss = 1.83122 (* 1 = 1.83122 loss)
I0521 10:07:21.756858 32608 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 10:07:29.643219 32608 solver.cpp:237] Iteration 784, loss = 1.79532
I0521 10:07:29.643254 32608 solver.cpp:253]     Train net output #0: loss = 1.79532 (* 1 = 1.79532 loss)
I0521 10:07:29.643278 32608 sgd_solver.cpp:106] Iteration 784, lr = 0.0025
I0521 10:07:37.525295 32608 solver.cpp:237] Iteration 800, loss = 1.81208
I0521 10:07:37.525352 32608 solver.cpp:253]     Train net output #0: loss = 1.81208 (* 1 = 1.81208 loss)
I0521 10:07:37.525368 32608 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 10:07:44.425170 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_815.caffemodel
I0521 10:07:44.812650 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_815.solverstate
I0521 10:07:45.479846 32608 solver.cpp:237] Iteration 816, loss = 1.79006
I0521 10:07:45.479902 32608 solver.cpp:253]     Train net output #0: loss = 1.79006 (* 1 = 1.79006 loss)
I0521 10:07:45.479920 32608 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 10:07:53.367784 32608 solver.cpp:237] Iteration 832, loss = 1.75178
I0521 10:07:53.367940 32608 solver.cpp:253]     Train net output #0: loss = 1.75178 (* 1 = 1.75178 loss)
I0521 10:07:53.367956 32608 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 10:08:01.257475 32608 solver.cpp:237] Iteration 848, loss = 1.83894
I0521 10:08:01.257509 32608 solver.cpp:253]     Train net output #0: loss = 1.83894 (* 1 = 1.83894 loss)
I0521 10:08:01.257527 32608 sgd_solver.cpp:106] Iteration 848, lr = 0.0025
I0521 10:08:09.145004 32608 solver.cpp:237] Iteration 864, loss = 1.81997
I0521 10:08:09.145056 32608 solver.cpp:253]     Train net output #0: loss = 1.81997 (* 1 = 1.81997 loss)
I0521 10:08:09.145081 32608 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 10:08:39.212121 32608 solver.cpp:237] Iteration 880, loss = 1.83532
I0521 10:08:39.212302 32608 solver.cpp:253]     Train net output #0: loss = 1.83532 (* 1 = 1.83532 loss)
I0521 10:08:39.212319 32608 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 10:08:47.099503 32608 solver.cpp:237] Iteration 896, loss = 1.74147
I0521 10:08:47.099537 32608 solver.cpp:253]     Train net output #0: loss = 1.74147 (* 1 = 1.74147 loss)
I0521 10:08:47.099560 32608 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 10:08:54.984242 32608 solver.cpp:237] Iteration 912, loss = 1.75737
I0521 10:08:54.984297 32608 solver.cpp:253]     Train net output #0: loss = 1.75737 (* 1 = 1.75737 loss)
I0521 10:08:54.984323 32608 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 10:09:02.870332 32608 solver.cpp:237] Iteration 928, loss = 1.8051
I0521 10:09:02.870367 32608 solver.cpp:253]     Train net output #0: loss = 1.8051 (* 1 = 1.8051 loss)
I0521 10:09:02.870389 32608 sgd_solver.cpp:106] Iteration 928, lr = 0.0025
I0521 10:09:10.754757 32608 solver.cpp:237] Iteration 944, loss = 1.80473
I0521 10:09:10.754902 32608 solver.cpp:253]     Train net output #0: loss = 1.80473 (* 1 = 1.80473 loss)
I0521 10:09:10.754919 32608 sgd_solver.cpp:106] Iteration 944, lr = 0.0025
I0521 10:09:18.645280 32608 solver.cpp:237] Iteration 960, loss = 1.72954
I0521 10:09:18.645335 32608 solver.cpp:253]     Train net output #0: loss = 1.72954 (* 1 = 1.72954 loss)
I0521 10:09:18.645360 32608 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 10:09:26.529842 32608 solver.cpp:237] Iteration 976, loss = 1.76749
I0521 10:09:26.529875 32608 solver.cpp:253]     Train net output #0: loss = 1.76749 (* 1 = 1.76749 loss)
I0521 10:09:26.529898 32608 sgd_solver.cpp:106] Iteration 976, lr = 0.0025
I0521 10:09:27.024318 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_978.caffemodel
I0521 10:09:27.409991 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_978.solverstate
I0521 10:09:27.435338 32608 solver.cpp:341] Iteration 978, Testing net (#0)
I0521 10:10:12.213397 32608 solver.cpp:409]     Test net output #0: accuracy = 0.649166
I0521 10:10:12.213564 32608 solver.cpp:409]     Test net output #1: loss = 1.27715 (* 1 = 1.27715 loss)
I0521 10:10:41.433585 32608 solver.cpp:237] Iteration 992, loss = 1.76904
I0521 10:10:41.433641 32608 solver.cpp:253]     Train net output #0: loss = 1.76904 (* 1 = 1.76904 loss)
I0521 10:10:41.433666 32608 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0521 10:10:49.326829 32608 solver.cpp:237] Iteration 1008, loss = 1.76177
I0521 10:10:49.326980 32608 solver.cpp:253]     Train net output #0: loss = 1.76177 (* 1 = 1.76177 loss)
I0521 10:10:49.326997 32608 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 10:10:57.209554 32608 solver.cpp:237] Iteration 1024, loss = 1.77606
I0521 10:10:57.209604 32608 solver.cpp:253]     Train net output #0: loss = 1.77606 (* 1 = 1.77606 loss)
I0521 10:10:57.209622 32608 sgd_solver.cpp:106] Iteration 1024, lr = 0.0025
I0521 10:11:05.102231 32608 solver.cpp:237] Iteration 1040, loss = 1.79373
I0521 10:11:05.102264 32608 solver.cpp:253]     Train net output #0: loss = 1.79373 (* 1 = 1.79373 loss)
I0521 10:11:05.102289 32608 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 10:11:12.983438 32608 solver.cpp:237] Iteration 1056, loss = 1.82902
I0521 10:11:12.983471 32608 solver.cpp:253]     Train net output #0: loss = 1.82902 (* 1 = 1.82902 loss)
I0521 10:11:12.983494 32608 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 10:11:20.868532 32608 solver.cpp:237] Iteration 1072, loss = 1.73147
I0521 10:11:20.868685 32608 solver.cpp:253]     Train net output #0: loss = 1.73147 (* 1 = 1.73147 loss)
I0521 10:11:20.868700 32608 sgd_solver.cpp:106] Iteration 1072, lr = 0.0025
I0521 10:11:51.016933 32608 solver.cpp:237] Iteration 1088, loss = 1.83578
I0521 10:11:51.017102 32608 solver.cpp:253]     Train net output #0: loss = 1.83578 (* 1 = 1.83578 loss)
I0521 10:11:51.017119 32608 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 10:11:58.907308 32608 solver.cpp:237] Iteration 1104, loss = 1.77621
I0521 10:11:58.907342 32608 solver.cpp:253]     Train net output #0: loss = 1.77621 (* 1 = 1.77621 loss)
I0521 10:11:58.907366 32608 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 10:12:06.797714 32608 solver.cpp:237] Iteration 1120, loss = 1.7076
I0521 10:12:06.797749 32608 solver.cpp:253]     Train net output #0: loss = 1.7076 (* 1 = 1.7076 loss)
I0521 10:12:06.797767 32608 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 10:12:14.683284 32608 solver.cpp:237] Iteration 1136, loss = 1.73155
I0521 10:12:14.683334 32608 solver.cpp:253]     Train net output #0: loss = 1.73155 (* 1 = 1.73155 loss)
I0521 10:12:14.683351 32608 sgd_solver.cpp:106] Iteration 1136, lr = 0.0025
I0521 10:12:16.657023 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1141.caffemodel
I0521 10:12:17.041601 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1141.solverstate
I0521 10:12:22.634728 32608 solver.cpp:237] Iteration 1152, loss = 1.71912
I0521 10:12:22.634896 32608 solver.cpp:253]     Train net output #0: loss = 1.71912 (* 1 = 1.71912 loss)
I0521 10:12:22.634913 32608 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 10:12:30.524773 32608 solver.cpp:237] Iteration 1168, loss = 1.76531
I0521 10:12:30.524807 32608 solver.cpp:253]     Train net output #0: loss = 1.76531 (* 1 = 1.76531 loss)
I0521 10:12:30.524826 32608 sgd_solver.cpp:106] Iteration 1168, lr = 0.0025
I0521 10:12:38.419706 32608 solver.cpp:237] Iteration 1184, loss = 1.73875
I0521 10:12:38.419756 32608 solver.cpp:253]     Train net output #0: loss = 1.73875 (* 1 = 1.73875 loss)
I0521 10:12:38.419773 32608 sgd_solver.cpp:106] Iteration 1184, lr = 0.0025
I0521 10:13:08.489689 32608 solver.cpp:237] Iteration 1200, loss = 1.69767
I0521 10:13:08.489864 32608 solver.cpp:253]     Train net output #0: loss = 1.69767 (* 1 = 1.69767 loss)
I0521 10:13:08.489881 32608 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 10:13:16.379191 32608 solver.cpp:237] Iteration 1216, loss = 1.71672
I0521 10:13:16.379225 32608 solver.cpp:253]     Train net output #0: loss = 1.71672 (* 1 = 1.71672 loss)
I0521 10:13:16.379245 32608 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 10:13:24.270421 32608 solver.cpp:237] Iteration 1232, loss = 1.73379
I0521 10:13:24.270455 32608 solver.cpp:253]     Train net output #0: loss = 1.73379 (* 1 = 1.73379 loss)
I0521 10:13:24.270473 32608 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 10:13:32.158296 32608 solver.cpp:237] Iteration 1248, loss = 1.71366
I0521 10:13:32.158346 32608 solver.cpp:253]     Train net output #0: loss = 1.71366 (* 1 = 1.71366 loss)
I0521 10:13:32.158363 32608 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 10:13:40.047503 32608 solver.cpp:237] Iteration 1264, loss = 1.67688
I0521 10:13:40.047662 32608 solver.cpp:253]     Train net output #0: loss = 1.67688 (* 1 = 1.67688 loss)
I0521 10:13:40.047677 32608 sgd_solver.cpp:106] Iteration 1264, lr = 0.0025
I0521 10:13:47.933192 32608 solver.cpp:237] Iteration 1280, loss = 1.75573
I0521 10:13:47.933224 32608 solver.cpp:253]     Train net output #0: loss = 1.75573 (* 1 = 1.75573 loss)
I0521 10:13:47.933248 32608 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 10:13:55.822815 32608 solver.cpp:237] Iteration 1296, loss = 1.64923
I0521 10:13:55.822873 32608 solver.cpp:253]     Train net output #0: loss = 1.64923 (* 1 = 1.64923 loss)
I0521 10:13:55.822897 32608 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 10:13:59.271116 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1304.caffemodel
I0521 10:13:59.655133 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1304.solverstate
I0521 10:13:59.681953 32608 solver.cpp:341] Iteration 1304, Testing net (#0)
I0521 10:15:05.682646 32608 solver.cpp:409]     Test net output #0: accuracy = 0.661363
I0521 10:15:05.682828 32608 solver.cpp:409]     Test net output #1: loss = 1.18407 (* 1 = 1.18407 loss)
I0521 10:15:31.942242 32608 solver.cpp:237] Iteration 1312, loss = 1.74679
I0521 10:15:31.942301 32608 solver.cpp:253]     Train net output #0: loss = 1.74679 (* 1 = 1.74679 loss)
I0521 10:15:31.942318 32608 sgd_solver.cpp:106] Iteration 1312, lr = 0.0025
I0521 10:15:39.830729 32608 solver.cpp:237] Iteration 1328, loss = 1.77212
I0521 10:15:39.830878 32608 solver.cpp:253]     Train net output #0: loss = 1.77212 (* 1 = 1.77212 loss)
I0521 10:15:39.830894 32608 sgd_solver.cpp:106] Iteration 1328, lr = 0.0025
I0521 10:15:47.717914 32608 solver.cpp:237] Iteration 1344, loss = 1.76652
I0521 10:15:47.717947 32608 solver.cpp:253]     Train net output #0: loss = 1.76652 (* 1 = 1.76652 loss)
I0521 10:15:47.717972 32608 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 10:15:55.606577 32608 solver.cpp:237] Iteration 1360, loss = 1.88786
I0521 10:15:55.606611 32608 solver.cpp:253]     Train net output #0: loss = 1.88786 (* 1 = 1.88786 loss)
I0521 10:15:55.606628 32608 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 10:16:03.490332 32608 solver.cpp:237] Iteration 1376, loss = 1.66278
I0521 10:16:03.490381 32608 solver.cpp:253]     Train net output #0: loss = 1.66278 (* 1 = 1.66278 loss)
I0521 10:16:03.490398 32608 sgd_solver.cpp:106] Iteration 1376, lr = 0.0025
I0521 10:16:11.381490 32608 solver.cpp:237] Iteration 1392, loss = 1.75862
I0521 10:16:11.381638 32608 solver.cpp:253]     Train net output #0: loss = 1.75862 (* 1 = 1.75862 loss)
I0521 10:16:11.381654 32608 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 10:16:19.268028 32608 solver.cpp:237] Iteration 1408, loss = 1.69825
I0521 10:16:19.268061 32608 solver.cpp:253]     Train net output #0: loss = 1.69825 (* 1 = 1.69825 loss)
I0521 10:16:19.268085 32608 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 10:16:49.334383 32608 solver.cpp:237] Iteration 1424, loss = 1.7923
I0521 10:16:49.334556 32608 solver.cpp:253]     Train net output #0: loss = 1.7923 (* 1 = 1.7923 loss)
I0521 10:16:49.334573 32608 sgd_solver.cpp:106] Iteration 1424, lr = 0.0025
I0521 10:16:57.227056 32608 solver.cpp:237] Iteration 1440, loss = 1.64732
I0521 10:16:57.227090 32608 solver.cpp:253]     Train net output #0: loss = 1.64732 (* 1 = 1.64732 loss)
I0521 10:16:57.227108 32608 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 10:17:05.115051 32608 solver.cpp:237] Iteration 1456, loss = 1.6996
I0521 10:17:05.115083 32608 solver.cpp:253]     Train net output #0: loss = 1.6996 (* 1 = 1.6996 loss)
I0521 10:17:05.115102 32608 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 10:17:10.043978 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1467.caffemodel
I0521 10:17:10.429730 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1467.solverstate
I0521 10:17:13.067958 32608 solver.cpp:237] Iteration 1472, loss = 1.72937
I0521 10:17:13.068017 32608 solver.cpp:253]     Train net output #0: loss = 1.72937 (* 1 = 1.72937 loss)
I0521 10:17:13.068037 32608 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 10:17:20.949468 32608 solver.cpp:237] Iteration 1488, loss = 1.69394
I0521 10:17:20.949631 32608 solver.cpp:253]     Train net output #0: loss = 1.69394 (* 1 = 1.69394 loss)
I0521 10:17:20.949647 32608 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 10:17:28.836757 32608 solver.cpp:237] Iteration 1504, loss = 1.63854
I0521 10:17:28.836791 32608 solver.cpp:253]     Train net output #0: loss = 1.63854 (* 1 = 1.63854 loss)
I0521 10:17:28.836813 32608 sgd_solver.cpp:106] Iteration 1504, lr = 0.0025
I0521 10:17:36.726544 32608 solver.cpp:237] Iteration 1520, loss = 1.67645
I0521 10:17:36.726588 32608 solver.cpp:253]     Train net output #0: loss = 1.67645 (* 1 = 1.67645 loss)
I0521 10:17:36.726613 32608 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 10:18:06.807744 32608 solver.cpp:237] Iteration 1536, loss = 1.65539
I0521 10:18:06.807920 32608 solver.cpp:253]     Train net output #0: loss = 1.65539 (* 1 = 1.65539 loss)
I0521 10:18:06.807937 32608 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 10:18:14.700109 32608 solver.cpp:237] Iteration 1552, loss = 1.68955
I0521 10:18:14.700144 32608 solver.cpp:253]     Train net output #0: loss = 1.68955 (* 1 = 1.68955 loss)
I0521 10:18:14.700166 32608 sgd_solver.cpp:106] Iteration 1552, lr = 0.0025
I0521 10:18:22.588394 32608 solver.cpp:237] Iteration 1568, loss = 1.65097
I0521 10:18:22.588428 32608 solver.cpp:253]     Train net output #0: loss = 1.65097 (* 1 = 1.65097 loss)
I0521 10:18:22.588450 32608 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 10:18:30.482118 32608 solver.cpp:237] Iteration 1584, loss = 1.70266
I0521 10:18:30.482167 32608 solver.cpp:253]     Train net output #0: loss = 1.70266 (* 1 = 1.70266 loss)
I0521 10:18:30.482194 32608 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 10:18:38.368779 32608 solver.cpp:237] Iteration 1600, loss = 1.67822
I0521 10:18:38.368932 32608 solver.cpp:253]     Train net output #0: loss = 1.67822 (* 1 = 1.67822 loss)
I0521 10:18:38.368948 32608 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 10:18:46.252688 32608 solver.cpp:237] Iteration 1616, loss = 1.70184
I0521 10:18:46.252722 32608 solver.cpp:253]     Train net output #0: loss = 1.70184 (* 1 = 1.70184 loss)
I0521 10:18:46.252745 32608 sgd_solver.cpp:106] Iteration 1616, lr = 0.0025
I0521 10:18:52.663951 32608 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1630.caffemodel
I0521 10:18:53.049409 32608 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795_iter_1630.solverstate
I0521 10:18:53.078435 32608 solver.cpp:341] Iteration 1630, Testing net (#0)
I0521 10:19:38.164248 32608 solver.cpp:409]     Test net output #0: accuracy = 0.664457
I0521 10:19:38.164424 32608 solver.cpp:409]     Test net output #1: loss = 1.18428 (* 1 = 1.18428 loss)
I0521 10:19:38.164441 32608 solver.cpp:326] Optimization Done.
I0521 10:19:38.164453 32608 caffe.cpp:215] Optimization Done.
Application 11237564 resources: utime ~1248s, stime ~226s, Rss ~5328984, inblocks ~3594475, outblocks ~179818
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_920_2016-05-20T11.21.06.248795.solver"
	User time (seconds): 0.53
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:37.97
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15080
	Voluntary context switches: 2717
	Involuntary context switches: 83
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
