2805914
I0520 20:14:13.729491 27968 caffe.cpp:184] Using GPUs 0
I0520 20:14:14.154134 27968 solver.cpp:48] Initializing solver from parameters: 
test_iter: 576
test_interval: 1153
base_lr: 0.0025
display: 57
max_iter: 5769
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 576
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158.prototxt"
I0520 20:14:14.155897 27968 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158.prototxt
I0520 20:14:14.173012 27968 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 20:14:14.173071 27968 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 20:14:14.173419 27968 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 260
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 20:14:14.173596 27968 layer_factory.hpp:77] Creating layer data_hdf5
I0520 20:14:14.173620 27968 net.cpp:106] Creating Layer data_hdf5
I0520 20:14:14.173635 27968 net.cpp:411] data_hdf5 -> data
I0520 20:14:14.173667 27968 net.cpp:411] data_hdf5 -> label
I0520 20:14:14.173699 27968 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 20:14:14.174954 27968 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 20:14:14.177145 27968 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 20:14:35.723687 27968 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 20:14:35.728801 27968 net.cpp:150] Setting up data_hdf5
I0520 20:14:35.728845 27968 net.cpp:157] Top shape: 260 1 127 50 (1651000)
I0520 20:14:35.728868 27968 net.cpp:157] Top shape: 260 (260)
I0520 20:14:35.728878 27968 net.cpp:165] Memory required for data: 6605040
I0520 20:14:35.728893 27968 layer_factory.hpp:77] Creating layer conv1
I0520 20:14:35.728926 27968 net.cpp:106] Creating Layer conv1
I0520 20:14:35.728937 27968 net.cpp:454] conv1 <- data
I0520 20:14:35.728960 27968 net.cpp:411] conv1 -> conv1
I0520 20:14:36.094319 27968 net.cpp:150] Setting up conv1
I0520 20:14:36.094367 27968 net.cpp:157] Top shape: 260 12 120 48 (17971200)
I0520 20:14:36.094378 27968 net.cpp:165] Memory required for data: 78489840
I0520 20:14:36.094409 27968 layer_factory.hpp:77] Creating layer relu1
I0520 20:14:36.094430 27968 net.cpp:106] Creating Layer relu1
I0520 20:14:36.094441 27968 net.cpp:454] relu1 <- conv1
I0520 20:14:36.094455 27968 net.cpp:397] relu1 -> conv1 (in-place)
I0520 20:14:36.094977 27968 net.cpp:150] Setting up relu1
I0520 20:14:36.094995 27968 net.cpp:157] Top shape: 260 12 120 48 (17971200)
I0520 20:14:36.095005 27968 net.cpp:165] Memory required for data: 150374640
I0520 20:14:36.095015 27968 layer_factory.hpp:77] Creating layer pool1
I0520 20:14:36.095031 27968 net.cpp:106] Creating Layer pool1
I0520 20:14:36.095041 27968 net.cpp:454] pool1 <- conv1
I0520 20:14:36.095053 27968 net.cpp:411] pool1 -> pool1
I0520 20:14:36.095134 27968 net.cpp:150] Setting up pool1
I0520 20:14:36.095149 27968 net.cpp:157] Top shape: 260 12 60 48 (8985600)
I0520 20:14:36.095158 27968 net.cpp:165] Memory required for data: 186317040
I0520 20:14:36.095170 27968 layer_factory.hpp:77] Creating layer conv2
I0520 20:14:36.095191 27968 net.cpp:106] Creating Layer conv2
I0520 20:14:36.095202 27968 net.cpp:454] conv2 <- pool1
I0520 20:14:36.095216 27968 net.cpp:411] conv2 -> conv2
I0520 20:14:36.097892 27968 net.cpp:150] Setting up conv2
I0520 20:14:36.097920 27968 net.cpp:157] Top shape: 260 20 54 46 (12916800)
I0520 20:14:36.097930 27968 net.cpp:165] Memory required for data: 237984240
I0520 20:14:36.097950 27968 layer_factory.hpp:77] Creating layer relu2
I0520 20:14:36.097965 27968 net.cpp:106] Creating Layer relu2
I0520 20:14:36.097975 27968 net.cpp:454] relu2 <- conv2
I0520 20:14:36.097986 27968 net.cpp:397] relu2 -> conv2 (in-place)
I0520 20:14:36.098318 27968 net.cpp:150] Setting up relu2
I0520 20:14:36.098332 27968 net.cpp:157] Top shape: 260 20 54 46 (12916800)
I0520 20:14:36.098342 27968 net.cpp:165] Memory required for data: 289651440
I0520 20:14:36.098352 27968 layer_factory.hpp:77] Creating layer pool2
I0520 20:14:36.098366 27968 net.cpp:106] Creating Layer pool2
I0520 20:14:36.098376 27968 net.cpp:454] pool2 <- conv2
I0520 20:14:36.098400 27968 net.cpp:411] pool2 -> pool2
I0520 20:14:36.098469 27968 net.cpp:150] Setting up pool2
I0520 20:14:36.098484 27968 net.cpp:157] Top shape: 260 20 27 46 (6458400)
I0520 20:14:36.098492 27968 net.cpp:165] Memory required for data: 315485040
I0520 20:14:36.098502 27968 layer_factory.hpp:77] Creating layer conv3
I0520 20:14:36.098520 27968 net.cpp:106] Creating Layer conv3
I0520 20:14:36.098531 27968 net.cpp:454] conv3 <- pool2
I0520 20:14:36.098546 27968 net.cpp:411] conv3 -> conv3
I0520 20:14:36.100484 27968 net.cpp:150] Setting up conv3
I0520 20:14:36.100507 27968 net.cpp:157] Top shape: 260 28 22 44 (7047040)
I0520 20:14:36.100519 27968 net.cpp:165] Memory required for data: 343673200
I0520 20:14:36.100538 27968 layer_factory.hpp:77] Creating layer relu3
I0520 20:14:36.100554 27968 net.cpp:106] Creating Layer relu3
I0520 20:14:36.100564 27968 net.cpp:454] relu3 <- conv3
I0520 20:14:36.100576 27968 net.cpp:397] relu3 -> conv3 (in-place)
I0520 20:14:36.101059 27968 net.cpp:150] Setting up relu3
I0520 20:14:36.101076 27968 net.cpp:157] Top shape: 260 28 22 44 (7047040)
I0520 20:14:36.101088 27968 net.cpp:165] Memory required for data: 371861360
I0520 20:14:36.101097 27968 layer_factory.hpp:77] Creating layer pool3
I0520 20:14:36.101110 27968 net.cpp:106] Creating Layer pool3
I0520 20:14:36.101120 27968 net.cpp:454] pool3 <- conv3
I0520 20:14:36.101133 27968 net.cpp:411] pool3 -> pool3
I0520 20:14:36.101200 27968 net.cpp:150] Setting up pool3
I0520 20:14:36.101214 27968 net.cpp:157] Top shape: 260 28 11 44 (3523520)
I0520 20:14:36.101224 27968 net.cpp:165] Memory required for data: 385955440
I0520 20:14:36.101233 27968 layer_factory.hpp:77] Creating layer conv4
I0520 20:14:36.101249 27968 net.cpp:106] Creating Layer conv4
I0520 20:14:36.101259 27968 net.cpp:454] conv4 <- pool3
I0520 20:14:36.101274 27968 net.cpp:411] conv4 -> conv4
I0520 20:14:36.103996 27968 net.cpp:150] Setting up conv4
I0520 20:14:36.104024 27968 net.cpp:157] Top shape: 260 36 6 42 (2358720)
I0520 20:14:36.104034 27968 net.cpp:165] Memory required for data: 395390320
I0520 20:14:36.104049 27968 layer_factory.hpp:77] Creating layer relu4
I0520 20:14:36.104064 27968 net.cpp:106] Creating Layer relu4
I0520 20:14:36.104074 27968 net.cpp:454] relu4 <- conv4
I0520 20:14:36.104087 27968 net.cpp:397] relu4 -> conv4 (in-place)
I0520 20:14:36.104553 27968 net.cpp:150] Setting up relu4
I0520 20:14:36.104569 27968 net.cpp:157] Top shape: 260 36 6 42 (2358720)
I0520 20:14:36.104580 27968 net.cpp:165] Memory required for data: 404825200
I0520 20:14:36.104590 27968 layer_factory.hpp:77] Creating layer pool4
I0520 20:14:36.104604 27968 net.cpp:106] Creating Layer pool4
I0520 20:14:36.104614 27968 net.cpp:454] pool4 <- conv4
I0520 20:14:36.104626 27968 net.cpp:411] pool4 -> pool4
I0520 20:14:36.104693 27968 net.cpp:150] Setting up pool4
I0520 20:14:36.104707 27968 net.cpp:157] Top shape: 260 36 3 42 (1179360)
I0520 20:14:36.104717 27968 net.cpp:165] Memory required for data: 409542640
I0520 20:14:36.104727 27968 layer_factory.hpp:77] Creating layer ip1
I0520 20:14:36.104748 27968 net.cpp:106] Creating Layer ip1
I0520 20:14:36.104758 27968 net.cpp:454] ip1 <- pool4
I0520 20:14:36.104770 27968 net.cpp:411] ip1 -> ip1
I0520 20:14:36.120213 27968 net.cpp:150] Setting up ip1
I0520 20:14:36.120242 27968 net.cpp:157] Top shape: 260 196 (50960)
I0520 20:14:36.120254 27968 net.cpp:165] Memory required for data: 409746480
I0520 20:14:36.120276 27968 layer_factory.hpp:77] Creating layer relu5
I0520 20:14:36.120291 27968 net.cpp:106] Creating Layer relu5
I0520 20:14:36.120301 27968 net.cpp:454] relu5 <- ip1
I0520 20:14:36.120314 27968 net.cpp:397] relu5 -> ip1 (in-place)
I0520 20:14:36.120656 27968 net.cpp:150] Setting up relu5
I0520 20:14:36.120671 27968 net.cpp:157] Top shape: 260 196 (50960)
I0520 20:14:36.120682 27968 net.cpp:165] Memory required for data: 409950320
I0520 20:14:36.120692 27968 layer_factory.hpp:77] Creating layer drop1
I0520 20:14:36.120712 27968 net.cpp:106] Creating Layer drop1
I0520 20:14:36.120723 27968 net.cpp:454] drop1 <- ip1
I0520 20:14:36.120746 27968 net.cpp:397] drop1 -> ip1 (in-place)
I0520 20:14:36.120792 27968 net.cpp:150] Setting up drop1
I0520 20:14:36.120806 27968 net.cpp:157] Top shape: 260 196 (50960)
I0520 20:14:36.120816 27968 net.cpp:165] Memory required for data: 410154160
I0520 20:14:36.120826 27968 layer_factory.hpp:77] Creating layer ip2
I0520 20:14:36.120844 27968 net.cpp:106] Creating Layer ip2
I0520 20:14:36.120862 27968 net.cpp:454] ip2 <- ip1
I0520 20:14:36.120874 27968 net.cpp:411] ip2 -> ip2
I0520 20:14:36.121340 27968 net.cpp:150] Setting up ip2
I0520 20:14:36.121353 27968 net.cpp:157] Top shape: 260 98 (25480)
I0520 20:14:36.121363 27968 net.cpp:165] Memory required for data: 410256080
I0520 20:14:36.121378 27968 layer_factory.hpp:77] Creating layer relu6
I0520 20:14:36.121392 27968 net.cpp:106] Creating Layer relu6
I0520 20:14:36.121402 27968 net.cpp:454] relu6 <- ip2
I0520 20:14:36.121413 27968 net.cpp:397] relu6 -> ip2 (in-place)
I0520 20:14:36.121934 27968 net.cpp:150] Setting up relu6
I0520 20:14:36.121951 27968 net.cpp:157] Top shape: 260 98 (25480)
I0520 20:14:36.121963 27968 net.cpp:165] Memory required for data: 410358000
I0520 20:14:36.121973 27968 layer_factory.hpp:77] Creating layer drop2
I0520 20:14:36.121984 27968 net.cpp:106] Creating Layer drop2
I0520 20:14:36.121994 27968 net.cpp:454] drop2 <- ip2
I0520 20:14:36.122006 27968 net.cpp:397] drop2 -> ip2 (in-place)
I0520 20:14:36.122050 27968 net.cpp:150] Setting up drop2
I0520 20:14:36.122062 27968 net.cpp:157] Top shape: 260 98 (25480)
I0520 20:14:36.122072 27968 net.cpp:165] Memory required for data: 410459920
I0520 20:14:36.122082 27968 layer_factory.hpp:77] Creating layer ip3
I0520 20:14:36.122095 27968 net.cpp:106] Creating Layer ip3
I0520 20:14:36.122104 27968 net.cpp:454] ip3 <- ip2
I0520 20:14:36.122117 27968 net.cpp:411] ip3 -> ip3
I0520 20:14:36.122328 27968 net.cpp:150] Setting up ip3
I0520 20:14:36.122341 27968 net.cpp:157] Top shape: 260 11 (2860)
I0520 20:14:36.122351 27968 net.cpp:165] Memory required for data: 410471360
I0520 20:14:36.122366 27968 layer_factory.hpp:77] Creating layer drop3
I0520 20:14:36.122378 27968 net.cpp:106] Creating Layer drop3
I0520 20:14:36.122388 27968 net.cpp:454] drop3 <- ip3
I0520 20:14:36.122400 27968 net.cpp:397] drop3 -> ip3 (in-place)
I0520 20:14:36.122440 27968 net.cpp:150] Setting up drop3
I0520 20:14:36.122452 27968 net.cpp:157] Top shape: 260 11 (2860)
I0520 20:14:36.122462 27968 net.cpp:165] Memory required for data: 410482800
I0520 20:14:36.122473 27968 layer_factory.hpp:77] Creating layer loss
I0520 20:14:36.122491 27968 net.cpp:106] Creating Layer loss
I0520 20:14:36.122501 27968 net.cpp:454] loss <- ip3
I0520 20:14:36.122512 27968 net.cpp:454] loss <- label
I0520 20:14:36.122524 27968 net.cpp:411] loss -> loss
I0520 20:14:36.122540 27968 layer_factory.hpp:77] Creating layer loss
I0520 20:14:36.123179 27968 net.cpp:150] Setting up loss
I0520 20:14:36.123200 27968 net.cpp:157] Top shape: (1)
I0520 20:14:36.123213 27968 net.cpp:160]     with loss weight 1
I0520 20:14:36.123255 27968 net.cpp:165] Memory required for data: 410482804
I0520 20:14:36.123265 27968 net.cpp:226] loss needs backward computation.
I0520 20:14:36.123276 27968 net.cpp:226] drop3 needs backward computation.
I0520 20:14:36.123286 27968 net.cpp:226] ip3 needs backward computation.
I0520 20:14:36.123294 27968 net.cpp:226] drop2 needs backward computation.
I0520 20:14:36.123304 27968 net.cpp:226] relu6 needs backward computation.
I0520 20:14:36.123314 27968 net.cpp:226] ip2 needs backward computation.
I0520 20:14:36.123324 27968 net.cpp:226] drop1 needs backward computation.
I0520 20:14:36.123333 27968 net.cpp:226] relu5 needs backward computation.
I0520 20:14:36.123344 27968 net.cpp:226] ip1 needs backward computation.
I0520 20:14:36.123354 27968 net.cpp:226] pool4 needs backward computation.
I0520 20:14:36.123364 27968 net.cpp:226] relu4 needs backward computation.
I0520 20:14:36.123374 27968 net.cpp:226] conv4 needs backward computation.
I0520 20:14:36.123384 27968 net.cpp:226] pool3 needs backward computation.
I0520 20:14:36.123404 27968 net.cpp:226] relu3 needs backward computation.
I0520 20:14:36.123414 27968 net.cpp:226] conv3 needs backward computation.
I0520 20:14:36.123422 27968 net.cpp:226] pool2 needs backward computation.
I0520 20:14:36.123433 27968 net.cpp:226] relu2 needs backward computation.
I0520 20:14:36.123443 27968 net.cpp:226] conv2 needs backward computation.
I0520 20:14:36.123453 27968 net.cpp:226] pool1 needs backward computation.
I0520 20:14:36.123464 27968 net.cpp:226] relu1 needs backward computation.
I0520 20:14:36.123474 27968 net.cpp:226] conv1 needs backward computation.
I0520 20:14:36.123486 27968 net.cpp:228] data_hdf5 does not need backward computation.
I0520 20:14:36.123495 27968 net.cpp:270] This network produces output loss
I0520 20:14:36.123518 27968 net.cpp:283] Network initialization done.
I0520 20:14:36.125120 27968 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158.prototxt
I0520 20:14:36.125192 27968 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 20:14:36.125546 27968 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 260
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 20:14:36.125736 27968 layer_factory.hpp:77] Creating layer data_hdf5
I0520 20:14:36.125751 27968 net.cpp:106] Creating Layer data_hdf5
I0520 20:14:36.125763 27968 net.cpp:411] data_hdf5 -> data
I0520 20:14:36.125780 27968 net.cpp:411] data_hdf5 -> label
I0520 20:14:36.125797 27968 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 20:14:36.127035 27968 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 20:14:57.415611 27968 net.cpp:150] Setting up data_hdf5
I0520 20:14:57.415778 27968 net.cpp:157] Top shape: 260 1 127 50 (1651000)
I0520 20:14:57.415793 27968 net.cpp:157] Top shape: 260 (260)
I0520 20:14:57.415803 27968 net.cpp:165] Memory required for data: 6605040
I0520 20:14:57.415817 27968 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 20:14:57.415845 27968 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 20:14:57.415856 27968 net.cpp:454] label_data_hdf5_1_split <- label
I0520 20:14:57.415870 27968 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 20:14:57.415892 27968 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 20:14:57.415964 27968 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 20:14:57.415978 27968 net.cpp:157] Top shape: 260 (260)
I0520 20:14:57.415990 27968 net.cpp:157] Top shape: 260 (260)
I0520 20:14:57.415999 27968 net.cpp:165] Memory required for data: 6607120
I0520 20:14:57.416009 27968 layer_factory.hpp:77] Creating layer conv1
I0520 20:14:57.416031 27968 net.cpp:106] Creating Layer conv1
I0520 20:14:57.416043 27968 net.cpp:454] conv1 <- data
I0520 20:14:57.416056 27968 net.cpp:411] conv1 -> conv1
I0520 20:14:57.417994 27968 net.cpp:150] Setting up conv1
I0520 20:14:57.418016 27968 net.cpp:157] Top shape: 260 12 120 48 (17971200)
I0520 20:14:57.418028 27968 net.cpp:165] Memory required for data: 78491920
I0520 20:14:57.418050 27968 layer_factory.hpp:77] Creating layer relu1
I0520 20:14:57.418064 27968 net.cpp:106] Creating Layer relu1
I0520 20:14:57.418074 27968 net.cpp:454] relu1 <- conv1
I0520 20:14:57.418087 27968 net.cpp:397] relu1 -> conv1 (in-place)
I0520 20:14:57.418658 27968 net.cpp:150] Setting up relu1
I0520 20:14:57.418680 27968 net.cpp:157] Top shape: 260 12 120 48 (17971200)
I0520 20:14:57.418690 27968 net.cpp:165] Memory required for data: 150376720
I0520 20:14:57.418700 27968 layer_factory.hpp:77] Creating layer pool1
I0520 20:14:57.418716 27968 net.cpp:106] Creating Layer pool1
I0520 20:14:57.418726 27968 net.cpp:454] pool1 <- conv1
I0520 20:14:57.418740 27968 net.cpp:411] pool1 -> pool1
I0520 20:14:57.418813 27968 net.cpp:150] Setting up pool1
I0520 20:14:57.418826 27968 net.cpp:157] Top shape: 260 12 60 48 (8985600)
I0520 20:14:57.418836 27968 net.cpp:165] Memory required for data: 186319120
I0520 20:14:57.418845 27968 layer_factory.hpp:77] Creating layer conv2
I0520 20:14:57.418860 27968 net.cpp:106] Creating Layer conv2
I0520 20:14:57.418871 27968 net.cpp:454] conv2 <- pool1
I0520 20:14:57.418885 27968 net.cpp:411] conv2 -> conv2
I0520 20:14:57.420799 27968 net.cpp:150] Setting up conv2
I0520 20:14:57.420821 27968 net.cpp:157] Top shape: 260 20 54 46 (12916800)
I0520 20:14:57.420833 27968 net.cpp:165] Memory required for data: 237986320
I0520 20:14:57.420850 27968 layer_factory.hpp:77] Creating layer relu2
I0520 20:14:57.420872 27968 net.cpp:106] Creating Layer relu2
I0520 20:14:57.420882 27968 net.cpp:454] relu2 <- conv2
I0520 20:14:57.420894 27968 net.cpp:397] relu2 -> conv2 (in-place)
I0520 20:14:57.421227 27968 net.cpp:150] Setting up relu2
I0520 20:14:57.421241 27968 net.cpp:157] Top shape: 260 20 54 46 (12916800)
I0520 20:14:57.421252 27968 net.cpp:165] Memory required for data: 289653520
I0520 20:14:57.421262 27968 layer_factory.hpp:77] Creating layer pool2
I0520 20:14:57.421275 27968 net.cpp:106] Creating Layer pool2
I0520 20:14:57.421285 27968 net.cpp:454] pool2 <- conv2
I0520 20:14:57.421298 27968 net.cpp:411] pool2 -> pool2
I0520 20:14:57.421370 27968 net.cpp:150] Setting up pool2
I0520 20:14:57.421383 27968 net.cpp:157] Top shape: 260 20 27 46 (6458400)
I0520 20:14:57.421393 27968 net.cpp:165] Memory required for data: 315487120
I0520 20:14:57.421402 27968 layer_factory.hpp:77] Creating layer conv3
I0520 20:14:57.421421 27968 net.cpp:106] Creating Layer conv3
I0520 20:14:57.421430 27968 net.cpp:454] conv3 <- pool2
I0520 20:14:57.421444 27968 net.cpp:411] conv3 -> conv3
I0520 20:14:57.423413 27968 net.cpp:150] Setting up conv3
I0520 20:14:57.423436 27968 net.cpp:157] Top shape: 260 28 22 44 (7047040)
I0520 20:14:57.423449 27968 net.cpp:165] Memory required for data: 343675280
I0520 20:14:57.423481 27968 layer_factory.hpp:77] Creating layer relu3
I0520 20:14:57.423494 27968 net.cpp:106] Creating Layer relu3
I0520 20:14:57.423504 27968 net.cpp:454] relu3 <- conv3
I0520 20:14:57.423517 27968 net.cpp:397] relu3 -> conv3 (in-place)
I0520 20:14:57.423992 27968 net.cpp:150] Setting up relu3
I0520 20:14:57.424007 27968 net.cpp:157] Top shape: 260 28 22 44 (7047040)
I0520 20:14:57.424018 27968 net.cpp:165] Memory required for data: 371863440
I0520 20:14:57.424028 27968 layer_factory.hpp:77] Creating layer pool3
I0520 20:14:57.424041 27968 net.cpp:106] Creating Layer pool3
I0520 20:14:57.424051 27968 net.cpp:454] pool3 <- conv3
I0520 20:14:57.424063 27968 net.cpp:411] pool3 -> pool3
I0520 20:14:57.424134 27968 net.cpp:150] Setting up pool3
I0520 20:14:57.424147 27968 net.cpp:157] Top shape: 260 28 11 44 (3523520)
I0520 20:14:57.424157 27968 net.cpp:165] Memory required for data: 385957520
I0520 20:14:57.424167 27968 layer_factory.hpp:77] Creating layer conv4
I0520 20:14:57.424185 27968 net.cpp:106] Creating Layer conv4
I0520 20:14:57.424196 27968 net.cpp:454] conv4 <- pool3
I0520 20:14:57.424209 27968 net.cpp:411] conv4 -> conv4
I0520 20:14:57.426267 27968 net.cpp:150] Setting up conv4
I0520 20:14:57.426290 27968 net.cpp:157] Top shape: 260 36 6 42 (2358720)
I0520 20:14:57.426302 27968 net.cpp:165] Memory required for data: 395392400
I0520 20:14:57.426316 27968 layer_factory.hpp:77] Creating layer relu4
I0520 20:14:57.426331 27968 net.cpp:106] Creating Layer relu4
I0520 20:14:57.426340 27968 net.cpp:454] relu4 <- conv4
I0520 20:14:57.426353 27968 net.cpp:397] relu4 -> conv4 (in-place)
I0520 20:14:57.426822 27968 net.cpp:150] Setting up relu4
I0520 20:14:57.426838 27968 net.cpp:157] Top shape: 260 36 6 42 (2358720)
I0520 20:14:57.426849 27968 net.cpp:165] Memory required for data: 404827280
I0520 20:14:57.426859 27968 layer_factory.hpp:77] Creating layer pool4
I0520 20:14:57.426872 27968 net.cpp:106] Creating Layer pool4
I0520 20:14:57.426882 27968 net.cpp:454] pool4 <- conv4
I0520 20:14:57.426895 27968 net.cpp:411] pool4 -> pool4
I0520 20:14:57.426965 27968 net.cpp:150] Setting up pool4
I0520 20:14:57.426978 27968 net.cpp:157] Top shape: 260 36 3 42 (1179360)
I0520 20:14:57.426987 27968 net.cpp:165] Memory required for data: 409544720
I0520 20:14:57.426997 27968 layer_factory.hpp:77] Creating layer ip1
I0520 20:14:57.427012 27968 net.cpp:106] Creating Layer ip1
I0520 20:14:57.427022 27968 net.cpp:454] ip1 <- pool4
I0520 20:14:57.427037 27968 net.cpp:411] ip1 -> ip1
I0520 20:14:57.442498 27968 net.cpp:150] Setting up ip1
I0520 20:14:57.442525 27968 net.cpp:157] Top shape: 260 196 (50960)
I0520 20:14:57.442536 27968 net.cpp:165] Memory required for data: 409748560
I0520 20:14:57.442562 27968 layer_factory.hpp:77] Creating layer relu5
I0520 20:14:57.442577 27968 net.cpp:106] Creating Layer relu5
I0520 20:14:57.442587 27968 net.cpp:454] relu5 <- ip1
I0520 20:14:57.442601 27968 net.cpp:397] relu5 -> ip1 (in-place)
I0520 20:14:57.442946 27968 net.cpp:150] Setting up relu5
I0520 20:14:57.442960 27968 net.cpp:157] Top shape: 260 196 (50960)
I0520 20:14:57.442970 27968 net.cpp:165] Memory required for data: 409952400
I0520 20:14:57.442981 27968 layer_factory.hpp:77] Creating layer drop1
I0520 20:14:57.442999 27968 net.cpp:106] Creating Layer drop1
I0520 20:14:57.443009 27968 net.cpp:454] drop1 <- ip1
I0520 20:14:57.443023 27968 net.cpp:397] drop1 -> ip1 (in-place)
I0520 20:14:57.443066 27968 net.cpp:150] Setting up drop1
I0520 20:14:57.443079 27968 net.cpp:157] Top shape: 260 196 (50960)
I0520 20:14:57.443089 27968 net.cpp:165] Memory required for data: 410156240
I0520 20:14:57.443099 27968 layer_factory.hpp:77] Creating layer ip2
I0520 20:14:57.443114 27968 net.cpp:106] Creating Layer ip2
I0520 20:14:57.443123 27968 net.cpp:454] ip2 <- ip1
I0520 20:14:57.443136 27968 net.cpp:411] ip2 -> ip2
I0520 20:14:57.443614 27968 net.cpp:150] Setting up ip2
I0520 20:14:57.443627 27968 net.cpp:157] Top shape: 260 98 (25480)
I0520 20:14:57.443637 27968 net.cpp:165] Memory required for data: 410258160
I0520 20:14:57.443665 27968 layer_factory.hpp:77] Creating layer relu6
I0520 20:14:57.443678 27968 net.cpp:106] Creating Layer relu6
I0520 20:14:57.443688 27968 net.cpp:454] relu6 <- ip2
I0520 20:14:57.443701 27968 net.cpp:397] relu6 -> ip2 (in-place)
I0520 20:14:57.444234 27968 net.cpp:150] Setting up relu6
I0520 20:14:57.444255 27968 net.cpp:157] Top shape: 260 98 (25480)
I0520 20:14:57.444265 27968 net.cpp:165] Memory required for data: 410360080
I0520 20:14:57.444274 27968 layer_factory.hpp:77] Creating layer drop2
I0520 20:14:57.444289 27968 net.cpp:106] Creating Layer drop2
I0520 20:14:57.444299 27968 net.cpp:454] drop2 <- ip2
I0520 20:14:57.444313 27968 net.cpp:397] drop2 -> ip2 (in-place)
I0520 20:14:57.444355 27968 net.cpp:150] Setting up drop2
I0520 20:14:57.444368 27968 net.cpp:157] Top shape: 260 98 (25480)
I0520 20:14:57.444378 27968 net.cpp:165] Memory required for data: 410462000
I0520 20:14:57.444389 27968 layer_factory.hpp:77] Creating layer ip3
I0520 20:14:57.444402 27968 net.cpp:106] Creating Layer ip3
I0520 20:14:57.444412 27968 net.cpp:454] ip3 <- ip2
I0520 20:14:57.444427 27968 net.cpp:411] ip3 -> ip3
I0520 20:14:57.444650 27968 net.cpp:150] Setting up ip3
I0520 20:14:57.444664 27968 net.cpp:157] Top shape: 260 11 (2860)
I0520 20:14:57.444674 27968 net.cpp:165] Memory required for data: 410473440
I0520 20:14:57.444689 27968 layer_factory.hpp:77] Creating layer drop3
I0520 20:14:57.444701 27968 net.cpp:106] Creating Layer drop3
I0520 20:14:57.444711 27968 net.cpp:454] drop3 <- ip3
I0520 20:14:57.444723 27968 net.cpp:397] drop3 -> ip3 (in-place)
I0520 20:14:57.444766 27968 net.cpp:150] Setting up drop3
I0520 20:14:57.444777 27968 net.cpp:157] Top shape: 260 11 (2860)
I0520 20:14:57.444787 27968 net.cpp:165] Memory required for data: 410484880
I0520 20:14:57.444797 27968 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 20:14:57.444809 27968 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 20:14:57.444819 27968 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 20:14:57.444833 27968 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 20:14:57.444847 27968 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 20:14:57.444928 27968 net.cpp:150] Setting up ip3_drop3_0_split
I0520 20:14:57.444941 27968 net.cpp:157] Top shape: 260 11 (2860)
I0520 20:14:57.444954 27968 net.cpp:157] Top shape: 260 11 (2860)
I0520 20:14:57.444964 27968 net.cpp:165] Memory required for data: 410507760
I0520 20:14:57.444975 27968 layer_factory.hpp:77] Creating layer accuracy
I0520 20:14:57.444996 27968 net.cpp:106] Creating Layer accuracy
I0520 20:14:57.445006 27968 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 20:14:57.445017 27968 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 20:14:57.445031 27968 net.cpp:411] accuracy -> accuracy
I0520 20:14:57.445055 27968 net.cpp:150] Setting up accuracy
I0520 20:14:57.445066 27968 net.cpp:157] Top shape: (1)
I0520 20:14:57.445075 27968 net.cpp:165] Memory required for data: 410507764
I0520 20:14:57.445086 27968 layer_factory.hpp:77] Creating layer loss
I0520 20:14:57.445099 27968 net.cpp:106] Creating Layer loss
I0520 20:14:57.445108 27968 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 20:14:57.445119 27968 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 20:14:57.445132 27968 net.cpp:411] loss -> loss
I0520 20:14:57.445150 27968 layer_factory.hpp:77] Creating layer loss
I0520 20:14:57.445636 27968 net.cpp:150] Setting up loss
I0520 20:14:57.445650 27968 net.cpp:157] Top shape: (1)
I0520 20:14:57.445659 27968 net.cpp:160]     with loss weight 1
I0520 20:14:57.445678 27968 net.cpp:165] Memory required for data: 410507768
I0520 20:14:57.445688 27968 net.cpp:226] loss needs backward computation.
I0520 20:14:57.445699 27968 net.cpp:228] accuracy does not need backward computation.
I0520 20:14:57.445710 27968 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 20:14:57.445720 27968 net.cpp:226] drop3 needs backward computation.
I0520 20:14:57.445729 27968 net.cpp:226] ip3 needs backward computation.
I0520 20:14:57.445739 27968 net.cpp:226] drop2 needs backward computation.
I0520 20:14:57.445757 27968 net.cpp:226] relu6 needs backward computation.
I0520 20:14:57.445767 27968 net.cpp:226] ip2 needs backward computation.
I0520 20:14:57.445777 27968 net.cpp:226] drop1 needs backward computation.
I0520 20:14:57.445786 27968 net.cpp:226] relu5 needs backward computation.
I0520 20:14:57.445796 27968 net.cpp:226] ip1 needs backward computation.
I0520 20:14:57.445806 27968 net.cpp:226] pool4 needs backward computation.
I0520 20:14:57.445816 27968 net.cpp:226] relu4 needs backward computation.
I0520 20:14:57.445827 27968 net.cpp:226] conv4 needs backward computation.
I0520 20:14:57.445837 27968 net.cpp:226] pool3 needs backward computation.
I0520 20:14:57.445847 27968 net.cpp:226] relu3 needs backward computation.
I0520 20:14:57.445854 27968 net.cpp:226] conv3 needs backward computation.
I0520 20:14:57.445865 27968 net.cpp:226] pool2 needs backward computation.
I0520 20:14:57.445875 27968 net.cpp:226] relu2 needs backward computation.
I0520 20:14:57.445886 27968 net.cpp:226] conv2 needs backward computation.
I0520 20:14:57.445897 27968 net.cpp:226] pool1 needs backward computation.
I0520 20:14:57.445907 27968 net.cpp:226] relu1 needs backward computation.
I0520 20:14:57.445917 27968 net.cpp:226] conv1 needs backward computation.
I0520 20:14:57.445929 27968 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 20:14:57.445940 27968 net.cpp:228] data_hdf5 does not need backward computation.
I0520 20:14:57.445950 27968 net.cpp:270] This network produces output accuracy
I0520 20:14:57.445958 27968 net.cpp:270] This network produces output loss
I0520 20:14:57.445986 27968 net.cpp:283] Network initialization done.
I0520 20:14:57.446120 27968 solver.cpp:60] Solver scaffolding done.
I0520 20:14:57.447262 27968 caffe.cpp:212] Starting Optimization
I0520 20:14:57.447280 27968 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 20:14:57.447293 27968 solver.cpp:289] Learning Rate Policy: fixed
I0520 20:14:57.448513 27968 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 20:15:43.878919 27968 solver.cpp:409]     Test net output #0: accuracy = 0.0508546
I0520 20:15:43.879081 27968 solver.cpp:409]     Test net output #1: loss = 2.39916 (* 1 = 2.39916 loss)
I0520 20:15:43.938314 27968 solver.cpp:237] Iteration 0, loss = 2.40021
I0520 20:15:43.938349 27968 solver.cpp:253]     Train net output #0: loss = 2.40021 (* 1 = 2.40021 loss)
I0520 20:15:43.938369 27968 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 20:15:52.038646 27968 solver.cpp:237] Iteration 57, loss = 2.3373
I0520 20:15:52.038689 27968 solver.cpp:253]     Train net output #0: loss = 2.3373 (* 1 = 2.3373 loss)
I0520 20:15:52.038707 27968 sgd_solver.cpp:106] Iteration 57, lr = 0.0025
I0520 20:16:00.135568 27968 solver.cpp:237] Iteration 114, loss = 2.35312
I0520 20:16:00.135601 27968 solver.cpp:253]     Train net output #0: loss = 2.35312 (* 1 = 2.35312 loss)
I0520 20:16:00.135618 27968 sgd_solver.cpp:106] Iteration 114, lr = 0.0025
I0520 20:16:08.233422 27968 solver.cpp:237] Iteration 171, loss = 2.32287
I0520 20:16:08.233455 27968 solver.cpp:253]     Train net output #0: loss = 2.32287 (* 1 = 2.32287 loss)
I0520 20:16:08.233474 27968 sgd_solver.cpp:106] Iteration 171, lr = 0.0025
I0520 20:16:16.334084 27968 solver.cpp:237] Iteration 228, loss = 2.23621
I0520 20:16:16.334239 27968 solver.cpp:253]     Train net output #0: loss = 2.23621 (* 1 = 2.23621 loss)
I0520 20:16:16.334252 27968 sgd_solver.cpp:106] Iteration 228, lr = 0.0025
I0520 20:16:24.438058 27968 solver.cpp:237] Iteration 285, loss = 2.15195
I0520 20:16:24.438091 27968 solver.cpp:253]     Train net output #0: loss = 2.15195 (* 1 = 2.15195 loss)
I0520 20:16:24.438108 27968 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0520 20:16:32.537226 27968 solver.cpp:237] Iteration 342, loss = 2.1041
I0520 20:16:32.537261 27968 solver.cpp:253]     Train net output #0: loss = 2.1041 (* 1 = 2.1041 loss)
I0520 20:16:32.537278 27968 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0520 20:17:02.713562 27968 solver.cpp:237] Iteration 399, loss = 2.0124
I0520 20:17:02.713723 27968 solver.cpp:253]     Train net output #0: loss = 2.0124 (* 1 = 2.0124 loss)
I0520 20:17:02.713737 27968 sgd_solver.cpp:106] Iteration 399, lr = 0.0025
I0520 20:17:10.809897 27968 solver.cpp:237] Iteration 456, loss = 2.07844
I0520 20:17:10.809929 27968 solver.cpp:253]     Train net output #0: loss = 2.07844 (* 1 = 2.07844 loss)
I0520 20:17:10.809947 27968 sgd_solver.cpp:106] Iteration 456, lr = 0.0025
I0520 20:17:18.908977 27968 solver.cpp:237] Iteration 513, loss = 1.96664
I0520 20:17:18.909011 27968 solver.cpp:253]     Train net output #0: loss = 1.96664 (* 1 = 1.96664 loss)
I0520 20:17:18.909025 27968 sgd_solver.cpp:106] Iteration 513, lr = 0.0025
I0520 20:17:27.014715 27968 solver.cpp:237] Iteration 570, loss = 1.95494
I0520 20:17:27.014753 27968 solver.cpp:253]     Train net output #0: loss = 1.95494 (* 1 = 1.95494 loss)
I0520 20:17:27.014770 27968 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0520 20:17:27.726071 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_576.caffemodel
I0520 20:17:27.866328 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_576.solverstate
I0520 20:17:35.186285 27968 solver.cpp:237] Iteration 627, loss = 1.99521
I0520 20:17:35.186436 27968 solver.cpp:253]     Train net output #0: loss = 1.99521 (* 1 = 1.99521 loss)
I0520 20:17:35.186450 27968 sgd_solver.cpp:106] Iteration 627, lr = 0.0025
I0520 20:17:43.288022 27968 solver.cpp:237] Iteration 684, loss = 1.85427
I0520 20:17:43.288054 27968 solver.cpp:253]     Train net output #0: loss = 1.85427 (* 1 = 1.85427 loss)
I0520 20:17:43.288069 27968 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0520 20:17:51.390492 27968 solver.cpp:237] Iteration 741, loss = 1.89319
I0520 20:17:51.390534 27968 solver.cpp:253]     Train net output #0: loss = 1.89319 (* 1 = 1.89319 loss)
I0520 20:17:51.390548 27968 sgd_solver.cpp:106] Iteration 741, lr = 0.0025
I0520 20:18:21.597183 27968 solver.cpp:237] Iteration 798, loss = 1.7788
I0520 20:18:21.597339 27968 solver.cpp:253]     Train net output #0: loss = 1.7788 (* 1 = 1.7788 loss)
I0520 20:18:21.597353 27968 sgd_solver.cpp:106] Iteration 798, lr = 0.0025
I0520 20:18:29.700997 27968 solver.cpp:237] Iteration 855, loss = 1.81274
I0520 20:18:29.701031 27968 solver.cpp:253]     Train net output #0: loss = 1.81274 (* 1 = 1.81274 loss)
I0520 20:18:29.701048 27968 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0520 20:18:37.804569 27968 solver.cpp:237] Iteration 912, loss = 1.90562
I0520 20:18:37.804603 27968 solver.cpp:253]     Train net output #0: loss = 1.90562 (* 1 = 1.90562 loss)
I0520 20:18:37.804620 27968 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0520 20:18:45.921507 27968 solver.cpp:237] Iteration 969, loss = 1.80485
I0520 20:18:45.921543 27968 solver.cpp:253]     Train net output #0: loss = 1.80485 (* 1 = 1.80485 loss)
I0520 20:18:45.921566 27968 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0520 20:18:54.040529 27968 solver.cpp:237] Iteration 1026, loss = 1.79088
I0520 20:18:54.040673 27968 solver.cpp:253]     Train net output #0: loss = 1.79088 (* 1 = 1.79088 loss)
I0520 20:18:54.040688 27968 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0520 20:19:02.160794 27968 solver.cpp:237] Iteration 1083, loss = 1.84306
I0520 20:19:02.160828 27968 solver.cpp:253]     Train net output #0: loss = 1.84306 (* 1 = 1.84306 loss)
I0520 20:19:02.160845 27968 sgd_solver.cpp:106] Iteration 1083, lr = 0.0025
I0520 20:19:10.272815 27968 solver.cpp:237] Iteration 1140, loss = 1.7299
I0520 20:19:10.272851 27968 solver.cpp:253]     Train net output #0: loss = 1.7299 (* 1 = 1.7299 loss)
I0520 20:19:10.272877 27968 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0520 20:19:11.837316 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_1152.caffemodel
I0520 20:19:11.974238 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_1152.solverstate
I0520 20:19:12.043201 27968 solver.cpp:341] Iteration 1153, Testing net (#0)
I0520 20:19:57.671533 27968 solver.cpp:409]     Test net output #0: accuracy = 0.633039
I0520 20:19:57.671689 27968 solver.cpp:409]     Test net output #1: loss = 1.30421 (* 1 = 1.30421 loss)
I0520 20:20:26.121865 27968 solver.cpp:237] Iteration 1197, loss = 1.79195
I0520 20:20:26.121913 27968 solver.cpp:253]     Train net output #0: loss = 1.79195 (* 1 = 1.79195 loss)
I0520 20:20:26.121932 27968 sgd_solver.cpp:106] Iteration 1197, lr = 0.0025
I0520 20:20:34.215354 27968 solver.cpp:237] Iteration 1254, loss = 1.7928
I0520 20:20:34.215492 27968 solver.cpp:253]     Train net output #0: loss = 1.7928 (* 1 = 1.7928 loss)
I0520 20:20:34.215507 27968 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0520 20:20:42.315215 27968 solver.cpp:237] Iteration 1311, loss = 1.80745
I0520 20:20:42.315248 27968 solver.cpp:253]     Train net output #0: loss = 1.80745 (* 1 = 1.80745 loss)
I0520 20:20:42.315263 27968 sgd_solver.cpp:106] Iteration 1311, lr = 0.0025
I0520 20:20:50.426445 27968 solver.cpp:237] Iteration 1368, loss = 1.80429
I0520 20:20:50.426478 27968 solver.cpp:253]     Train net output #0: loss = 1.80429 (* 1 = 1.80429 loss)
I0520 20:20:50.426499 27968 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0520 20:20:58.531188 27968 solver.cpp:237] Iteration 1425, loss = 1.66153
I0520 20:20:58.531221 27968 solver.cpp:253]     Train net output #0: loss = 1.66153 (* 1 = 1.66153 loss)
I0520 20:20:58.531239 27968 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0520 20:21:06.643085 27968 solver.cpp:237] Iteration 1482, loss = 1.69881
I0520 20:21:06.643220 27968 solver.cpp:253]     Train net output #0: loss = 1.69881 (* 1 = 1.69881 loss)
I0520 20:21:06.643234 27968 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0520 20:21:36.888417 27968 solver.cpp:237] Iteration 1539, loss = 1.73829
I0520 20:21:36.888581 27968 solver.cpp:253]     Train net output #0: loss = 1.73829 (* 1 = 1.73829 loss)
I0520 20:21:36.888597 27968 sgd_solver.cpp:106] Iteration 1539, lr = 0.0025
I0520 20:21:44.993296 27968 solver.cpp:237] Iteration 1596, loss = 1.7456
I0520 20:21:44.993330 27968 solver.cpp:253]     Train net output #0: loss = 1.7456 (* 1 = 1.7456 loss)
I0520 20:21:44.993348 27968 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0520 20:21:53.100390 27968 solver.cpp:237] Iteration 1653, loss = 1.68995
I0520 20:21:53.100425 27968 solver.cpp:253]     Train net output #0: loss = 1.68995 (* 1 = 1.68995 loss)
I0520 20:21:53.100440 27968 sgd_solver.cpp:106] Iteration 1653, lr = 0.0025
I0520 20:22:01.207795 27968 solver.cpp:237] Iteration 1710, loss = 1.71989
I0520 20:22:01.207844 27968 solver.cpp:253]     Train net output #0: loss = 1.71989 (* 1 = 1.71989 loss)
I0520 20:22:01.207859 27968 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0520 20:22:03.621788 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_1728.caffemodel
I0520 20:22:03.761261 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_1728.solverstate
I0520 20:22:09.370028 27968 solver.cpp:237] Iteration 1767, loss = 1.75208
I0520 20:22:09.370195 27968 solver.cpp:253]     Train net output #0: loss = 1.75208 (* 1 = 1.75208 loss)
I0520 20:22:09.370209 27968 sgd_solver.cpp:106] Iteration 1767, lr = 0.0025
I0520 20:22:17.471703 27968 solver.cpp:237] Iteration 1824, loss = 1.71333
I0520 20:22:17.471736 27968 solver.cpp:253]     Train net output #0: loss = 1.71333 (* 1 = 1.71333 loss)
I0520 20:22:17.471753 27968 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0520 20:22:25.574214 27968 solver.cpp:237] Iteration 1881, loss = 1.70754
I0520 20:22:25.574266 27968 solver.cpp:253]     Train net output #0: loss = 1.70754 (* 1 = 1.70754 loss)
I0520 20:22:25.574282 27968 sgd_solver.cpp:106] Iteration 1881, lr = 0.0025
I0520 20:22:55.795805 27968 solver.cpp:237] Iteration 1938, loss = 1.72872
I0520 20:22:55.795970 27968 solver.cpp:253]     Train net output #0: loss = 1.72872 (* 1 = 1.72872 loss)
I0520 20:22:55.795986 27968 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0520 20:23:03.899405 27968 solver.cpp:237] Iteration 1995, loss = 1.69157
I0520 20:23:03.899438 27968 solver.cpp:253]     Train net output #0: loss = 1.69157 (* 1 = 1.69157 loss)
I0520 20:23:03.899456 27968 sgd_solver.cpp:106] Iteration 1995, lr = 0.0025
I0520 20:23:12.005121 27968 solver.cpp:237] Iteration 2052, loss = 1.72545
I0520 20:23:12.005154 27968 solver.cpp:253]     Train net output #0: loss = 1.72545 (* 1 = 1.72545 loss)
I0520 20:23:12.005170 27968 sgd_solver.cpp:106] Iteration 2052, lr = 0.0025
I0520 20:23:20.114292 27968 solver.cpp:237] Iteration 2109, loss = 1.77804
I0520 20:23:20.114325 27968 solver.cpp:253]     Train net output #0: loss = 1.77804 (* 1 = 1.77804 loss)
I0520 20:23:20.114346 27968 sgd_solver.cpp:106] Iteration 2109, lr = 0.0025
I0520 20:23:28.220314 27968 solver.cpp:237] Iteration 2166, loss = 1.61726
I0520 20:23:28.220451 27968 solver.cpp:253]     Train net output #0: loss = 1.61726 (* 1 = 1.61726 loss)
I0520 20:23:28.220464 27968 sgd_solver.cpp:106] Iteration 2166, lr = 0.0025
I0520 20:23:36.331743 27968 solver.cpp:237] Iteration 2223, loss = 1.66043
I0520 20:23:36.331775 27968 solver.cpp:253]     Train net output #0: loss = 1.66043 (* 1 = 1.66043 loss)
I0520 20:23:36.331792 27968 sgd_solver.cpp:106] Iteration 2223, lr = 0.0025
I0520 20:23:44.445235 27968 solver.cpp:237] Iteration 2280, loss = 1.54677
I0520 20:23:44.445266 27968 solver.cpp:253]     Train net output #0: loss = 1.54677 (* 1 = 1.54677 loss)
I0520 20:23:44.445283 27968 sgd_solver.cpp:106] Iteration 2280, lr = 0.0025
I0520 20:23:47.721607 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_2304.caffemodel
I0520 20:23:47.861778 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_2304.solverstate
I0520 20:23:48.075527 27968 solver.cpp:341] Iteration 2306, Testing net (#0)
I0520 20:24:54.574199 27968 solver.cpp:409]     Test net output #0: accuracy = 0.673097
I0520 20:24:54.574357 27968 solver.cpp:409]     Test net output #1: loss = 1.12489 (* 1 = 1.12489 loss)
I0520 20:25:21.173729 27968 solver.cpp:237] Iteration 2337, loss = 1.55371
I0520 20:25:21.173779 27968 solver.cpp:253]     Train net output #0: loss = 1.55371 (* 1 = 1.55371 loss)
I0520 20:25:21.173794 27968 sgd_solver.cpp:106] Iteration 2337, lr = 0.0025
I0520 20:25:29.281533 27968 solver.cpp:237] Iteration 2394, loss = 1.67263
I0520 20:25:29.281682 27968 solver.cpp:253]     Train net output #0: loss = 1.67263 (* 1 = 1.67263 loss)
I0520 20:25:29.281695 27968 sgd_solver.cpp:106] Iteration 2394, lr = 0.0025
I0520 20:25:37.386005 27968 solver.cpp:237] Iteration 2451, loss = 1.62905
I0520 20:25:37.386039 27968 solver.cpp:253]     Train net output #0: loss = 1.62905 (* 1 = 1.62905 loss)
I0520 20:25:37.386055 27968 sgd_solver.cpp:106] Iteration 2451, lr = 0.0025
I0520 20:25:45.492296 27968 solver.cpp:237] Iteration 2508, loss = 1.64016
I0520 20:25:45.492341 27968 solver.cpp:253]     Train net output #0: loss = 1.64016 (* 1 = 1.64016 loss)
I0520 20:25:45.492357 27968 sgd_solver.cpp:106] Iteration 2508, lr = 0.0025
I0520 20:25:53.598433 27968 solver.cpp:237] Iteration 2565, loss = 1.75462
I0520 20:25:53.598467 27968 solver.cpp:253]     Train net output #0: loss = 1.75462 (* 1 = 1.75462 loss)
I0520 20:25:53.598481 27968 sgd_solver.cpp:106] Iteration 2565, lr = 0.0025
I0520 20:26:01.705191 27968 solver.cpp:237] Iteration 2622, loss = 1.60522
I0520 20:26:01.705328 27968 solver.cpp:253]     Train net output #0: loss = 1.60522 (* 1 = 1.60522 loss)
I0520 20:26:01.705343 27968 sgd_solver.cpp:106] Iteration 2622, lr = 0.0025
I0520 20:26:09.805660 27968 solver.cpp:237] Iteration 2679, loss = 1.52835
I0520 20:26:09.805709 27968 solver.cpp:253]     Train net output #0: loss = 1.52835 (* 1 = 1.52835 loss)
I0520 20:26:09.805726 27968 sgd_solver.cpp:106] Iteration 2679, lr = 0.0025
I0520 20:26:40.036978 27968 solver.cpp:237] Iteration 2736, loss = 1.44992
I0520 20:26:40.037140 27968 solver.cpp:253]     Train net output #0: loss = 1.44992 (* 1 = 1.44992 loss)
I0520 20:26:40.037155 27968 sgd_solver.cpp:106] Iteration 2736, lr = 0.0025
I0520 20:26:48.141705 27968 solver.cpp:237] Iteration 2793, loss = 1.64778
I0520 20:26:48.141738 27968 solver.cpp:253]     Train net output #0: loss = 1.64778 (* 1 = 1.64778 loss)
I0520 20:26:48.141754 27968 sgd_solver.cpp:106] Iteration 2793, lr = 0.0025
I0520 20:26:56.257328 27968 solver.cpp:237] Iteration 2850, loss = 1.59025
I0520 20:26:56.257362 27968 solver.cpp:253]     Train net output #0: loss = 1.59025 (* 1 = 1.59025 loss)
I0520 20:26:56.257380 27968 sgd_solver.cpp:106] Iteration 2850, lr = 0.0025
I0520 20:27:00.382242 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_2880.caffemodel
I0520 20:27:00.521868 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_2880.solverstate
I0520 20:27:04.434584 27968 solver.cpp:237] Iteration 2907, loss = 1.57229
I0520 20:27:04.434633 27968 solver.cpp:253]     Train net output #0: loss = 1.57229 (* 1 = 1.57229 loss)
I0520 20:27:04.434645 27968 sgd_solver.cpp:106] Iteration 2907, lr = 0.0025
I0520 20:27:12.544920 27968 solver.cpp:237] Iteration 2964, loss = 1.55566
I0520 20:27:12.545066 27968 solver.cpp:253]     Train net output #0: loss = 1.55566 (* 1 = 1.55566 loss)
I0520 20:27:12.545080 27968 sgd_solver.cpp:106] Iteration 2964, lr = 0.0025
I0520 20:27:20.653117 27968 solver.cpp:237] Iteration 3021, loss = 1.6418
I0520 20:27:20.653151 27968 solver.cpp:253]     Train net output #0: loss = 1.6418 (* 1 = 1.6418 loss)
I0520 20:27:20.653168 27968 sgd_solver.cpp:106] Iteration 3021, lr = 0.0025
I0520 20:27:50.906355 27968 solver.cpp:237] Iteration 3078, loss = 1.68942
I0520 20:27:50.906533 27968 solver.cpp:253]     Train net output #0: loss = 1.68942 (* 1 = 1.68942 loss)
I0520 20:27:50.906548 27968 sgd_solver.cpp:106] Iteration 3078, lr = 0.0025
I0520 20:27:59.025414 27968 solver.cpp:237] Iteration 3135, loss = 1.53141
I0520 20:27:59.025447 27968 solver.cpp:253]     Train net output #0: loss = 1.53141 (* 1 = 1.53141 loss)
I0520 20:27:59.025465 27968 sgd_solver.cpp:106] Iteration 3135, lr = 0.0025
I0520 20:28:07.135624 27968 solver.cpp:237] Iteration 3192, loss = 1.64069
I0520 20:28:07.135659 27968 solver.cpp:253]     Train net output #0: loss = 1.64069 (* 1 = 1.64069 loss)
I0520 20:28:07.135676 27968 sgd_solver.cpp:106] Iteration 3192, lr = 0.0025
I0520 20:28:15.251085 27968 solver.cpp:237] Iteration 3249, loss = 1.6402
I0520 20:28:15.251132 27968 solver.cpp:253]     Train net output #0: loss = 1.6402 (* 1 = 1.6402 loss)
I0520 20:28:15.251148 27968 sgd_solver.cpp:106] Iteration 3249, lr = 0.0025
I0520 20:28:23.362666 27968 solver.cpp:237] Iteration 3306, loss = 1.60664
I0520 20:28:23.362807 27968 solver.cpp:253]     Train net output #0: loss = 1.60664 (* 1 = 1.60664 loss)
I0520 20:28:23.362820 27968 sgd_solver.cpp:106] Iteration 3306, lr = 0.0025
I0520 20:28:31.470762 27968 solver.cpp:237] Iteration 3363, loss = 1.60662
I0520 20:28:31.470794 27968 solver.cpp:253]     Train net output #0: loss = 1.60662 (* 1 = 1.60662 loss)
I0520 20:28:31.470813 27968 sgd_solver.cpp:106] Iteration 3363, lr = 0.0025
I0520 20:28:39.586496 27968 solver.cpp:237] Iteration 3420, loss = 1.58999
I0520 20:28:39.586546 27968 solver.cpp:253]     Train net output #0: loss = 1.58999 (* 1 = 1.58999 loss)
I0520 20:28:39.586561 27968 sgd_solver.cpp:106] Iteration 3420, lr = 0.0025
I0520 20:28:44.569383 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_3456.caffemodel
I0520 20:28:44.707679 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_3456.solverstate
I0520 20:28:45.062609 27968 solver.cpp:341] Iteration 3459, Testing net (#0)
I0520 20:29:30.388759 27968 solver.cpp:409]     Test net output #0: accuracy = 0.715974
I0520 20:29:30.388924 27968 solver.cpp:409]     Test net output #1: loss = 0.986887 (* 1 = 0.986887 loss)
I0520 20:29:55.155442 27968 solver.cpp:237] Iteration 3477, loss = 1.56056
I0520 20:29:55.155493 27968 solver.cpp:253]     Train net output #0: loss = 1.56056 (* 1 = 1.56056 loss)
I0520 20:29:55.155508 27968 sgd_solver.cpp:106] Iteration 3477, lr = 0.0025
I0520 20:30:03.251011 27968 solver.cpp:237] Iteration 3534, loss = 1.61559
I0520 20:30:03.251163 27968 solver.cpp:253]     Train net output #0: loss = 1.61559 (* 1 = 1.61559 loss)
I0520 20:30:03.251176 27968 sgd_solver.cpp:106] Iteration 3534, lr = 0.0025
I0520 20:30:11.350958 27968 solver.cpp:237] Iteration 3591, loss = 1.43459
I0520 20:30:11.350991 27968 solver.cpp:253]     Train net output #0: loss = 1.43459 (* 1 = 1.43459 loss)
I0520 20:30:11.351006 27968 sgd_solver.cpp:106] Iteration 3591, lr = 0.0025
I0520 20:30:19.458756 27968 solver.cpp:237] Iteration 3648, loss = 1.53426
I0520 20:30:19.458791 27968 solver.cpp:253]     Train net output #0: loss = 1.53426 (* 1 = 1.53426 loss)
I0520 20:30:19.458804 27968 sgd_solver.cpp:106] Iteration 3648, lr = 0.0025
I0520 20:30:27.564888 27968 solver.cpp:237] Iteration 3705, loss = 1.56344
I0520 20:30:27.564921 27968 solver.cpp:253]     Train net output #0: loss = 1.56344 (* 1 = 1.56344 loss)
I0520 20:30:27.564939 27968 sgd_solver.cpp:106] Iteration 3705, lr = 0.0025
I0520 20:30:35.680402 27968 solver.cpp:237] Iteration 3762, loss = 1.60933
I0520 20:30:35.680548 27968 solver.cpp:253]     Train net output #0: loss = 1.60933 (* 1 = 1.60933 loss)
I0520 20:30:35.680562 27968 sgd_solver.cpp:106] Iteration 3762, lr = 0.0025
I0520 20:30:43.787006 27968 solver.cpp:237] Iteration 3819, loss = 1.43449
I0520 20:30:43.787039 27968 solver.cpp:253]     Train net output #0: loss = 1.43449 (* 1 = 1.43449 loss)
I0520 20:30:43.787055 27968 sgd_solver.cpp:106] Iteration 3819, lr = 0.0025
I0520 20:31:14.058985 27968 solver.cpp:237] Iteration 3876, loss = 1.56449
I0520 20:31:14.059154 27968 solver.cpp:253]     Train net output #0: loss = 1.56449 (* 1 = 1.56449 loss)
I0520 20:31:14.059170 27968 sgd_solver.cpp:106] Iteration 3876, lr = 0.0025
I0520 20:31:22.169780 27968 solver.cpp:237] Iteration 3933, loss = 1.46206
I0520 20:31:22.169814 27968 solver.cpp:253]     Train net output #0: loss = 1.46206 (* 1 = 1.46206 loss)
I0520 20:31:22.169831 27968 sgd_solver.cpp:106] Iteration 3933, lr = 0.0025
I0520 20:31:30.282583 27968 solver.cpp:237] Iteration 3990, loss = 1.62758
I0520 20:31:30.282618 27968 solver.cpp:253]     Train net output #0: loss = 1.62758 (* 1 = 1.62758 loss)
I0520 20:31:30.282634 27968 sgd_solver.cpp:106] Iteration 3990, lr = 0.0025
I0520 20:31:36.113334 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_4032.caffemodel
I0520 20:31:36.251117 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_4032.solverstate
I0520 20:31:38.454380 27968 solver.cpp:237] Iteration 4047, loss = 1.58981
I0520 20:31:38.454421 27968 solver.cpp:253]     Train net output #0: loss = 1.58981 (* 1 = 1.58981 loss)
I0520 20:31:38.454442 27968 sgd_solver.cpp:106] Iteration 4047, lr = 0.0025
I0520 20:31:46.562723 27968 solver.cpp:237] Iteration 4104, loss = 1.45576
I0520 20:31:46.562868 27968 solver.cpp:253]     Train net output #0: loss = 1.45576 (* 1 = 1.45576 loss)
I0520 20:31:46.562881 27968 sgd_solver.cpp:106] Iteration 4104, lr = 0.0025
I0520 20:31:54.672488 27968 solver.cpp:237] Iteration 4161, loss = 1.47408
I0520 20:31:54.672523 27968 solver.cpp:253]     Train net output #0: loss = 1.47408 (* 1 = 1.47408 loss)
I0520 20:31:54.672536 27968 sgd_solver.cpp:106] Iteration 4161, lr = 0.0025
I0520 20:32:02.785382 27968 solver.cpp:237] Iteration 4218, loss = 1.46306
I0520 20:32:02.785425 27968 solver.cpp:253]     Train net output #0: loss = 1.46306 (* 1 = 1.46306 loss)
I0520 20:32:02.785440 27968 sgd_solver.cpp:106] Iteration 4218, lr = 0.0025
I0520 20:32:33.063575 27968 solver.cpp:237] Iteration 4275, loss = 1.48488
I0520 20:32:33.063745 27968 solver.cpp:253]     Train net output #0: loss = 1.48488 (* 1 = 1.48488 loss)
I0520 20:32:33.063760 27968 sgd_solver.cpp:106] Iteration 4275, lr = 0.0025
I0520 20:32:41.173974 27968 solver.cpp:237] Iteration 4332, loss = 1.518
I0520 20:32:41.174006 27968 solver.cpp:253]     Train net output #0: loss = 1.518 (* 1 = 1.518 loss)
I0520 20:32:41.174024 27968 sgd_solver.cpp:106] Iteration 4332, lr = 0.0025
I0520 20:32:49.284150 27968 solver.cpp:237] Iteration 4389, loss = 1.41057
I0520 20:32:49.284185 27968 solver.cpp:253]     Train net output #0: loss = 1.41057 (* 1 = 1.41057 loss)
I0520 20:32:49.284203 27968 sgd_solver.cpp:106] Iteration 4389, lr = 0.0025
I0520 20:32:57.393172 27968 solver.cpp:237] Iteration 4446, loss = 1.48924
I0520 20:32:57.393211 27968 solver.cpp:253]     Train net output #0: loss = 1.48924 (* 1 = 1.48924 loss)
I0520 20:32:57.393225 27968 sgd_solver.cpp:106] Iteration 4446, lr = 0.0025
I0520 20:33:05.499980 27968 solver.cpp:237] Iteration 4503, loss = 1.39069
I0520 20:33:05.500121 27968 solver.cpp:253]     Train net output #0: loss = 1.39069 (* 1 = 1.39069 loss)
I0520 20:33:05.500134 27968 sgd_solver.cpp:106] Iteration 4503, lr = 0.0025
I0520 20:33:13.604578 27968 solver.cpp:237] Iteration 4560, loss = 1.43024
I0520 20:33:13.604611 27968 solver.cpp:253]     Train net output #0: loss = 1.43024 (* 1 = 1.43024 loss)
I0520 20:33:13.604629 27968 sgd_solver.cpp:106] Iteration 4560, lr = 0.0025
I0520 20:33:20.294512 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_4608.caffemodel
I0520 20:33:20.431668 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_4608.solverstate
I0520 20:33:20.926576 27968 solver.cpp:341] Iteration 4612, Testing net (#0)
I0520 20:34:27.400676 27968 solver.cpp:409]     Test net output #0: accuracy = 0.766238
I0520 20:34:27.400848 27968 solver.cpp:409]     Test net output #1: loss = 0.806537 (* 1 = 0.806537 loss)
I0520 20:34:50.325485 27968 solver.cpp:237] Iteration 4617, loss = 1.44257
I0520 20:34:50.325536 27968 solver.cpp:253]     Train net output #0: loss = 1.44257 (* 1 = 1.44257 loss)
I0520 20:34:50.325552 27968 sgd_solver.cpp:106] Iteration 4617, lr = 0.0025
I0520 20:34:58.426157 27968 solver.cpp:237] Iteration 4674, loss = 1.43393
I0520 20:34:58.426314 27968 solver.cpp:253]     Train net output #0: loss = 1.43393 (* 1 = 1.43393 loss)
I0520 20:34:58.426328 27968 sgd_solver.cpp:106] Iteration 4674, lr = 0.0025
I0520 20:35:06.535384 27968 solver.cpp:237] Iteration 4731, loss = 1.39672
I0520 20:35:06.535418 27968 solver.cpp:253]     Train net output #0: loss = 1.39672 (* 1 = 1.39672 loss)
I0520 20:35:06.535435 27968 sgd_solver.cpp:106] Iteration 4731, lr = 0.0025
I0520 20:35:14.655779 27968 solver.cpp:237] Iteration 4788, loss = 1.43382
I0520 20:35:14.655813 27968 solver.cpp:253]     Train net output #0: loss = 1.43382 (* 1 = 1.43382 loss)
I0520 20:35:14.655832 27968 sgd_solver.cpp:106] Iteration 4788, lr = 0.0025
I0520 20:35:22.772006 27968 solver.cpp:237] Iteration 4845, loss = 1.64679
I0520 20:35:22.772048 27968 solver.cpp:253]     Train net output #0: loss = 1.64679 (* 1 = 1.64679 loss)
I0520 20:35:22.772070 27968 sgd_solver.cpp:106] Iteration 4845, lr = 0.0025
I0520 20:35:30.891499 27968 solver.cpp:237] Iteration 4902, loss = 1.4315
I0520 20:35:30.891659 27968 solver.cpp:253]     Train net output #0: loss = 1.4315 (* 1 = 1.4315 loss)
I0520 20:35:30.891672 27968 sgd_solver.cpp:106] Iteration 4902, lr = 0.0025
I0520 20:35:39.010465 27968 solver.cpp:237] Iteration 4959, loss = 1.48235
I0520 20:35:39.010498 27968 solver.cpp:253]     Train net output #0: loss = 1.48235 (* 1 = 1.48235 loss)
I0520 20:35:39.010515 27968 sgd_solver.cpp:106] Iteration 4959, lr = 0.0025
I0520 20:36:09.320032 27968 solver.cpp:237] Iteration 5016, loss = 1.37433
I0520 20:36:09.320202 27968 solver.cpp:253]     Train net output #0: loss = 1.37433 (* 1 = 1.37433 loss)
I0520 20:36:09.320219 27968 sgd_solver.cpp:106] Iteration 5016, lr = 0.0025
I0520 20:36:17.437434 27968 solver.cpp:237] Iteration 5073, loss = 1.43573
I0520 20:36:17.437474 27968 solver.cpp:253]     Train net output #0: loss = 1.43573 (* 1 = 1.43573 loss)
I0520 20:36:17.437490 27968 sgd_solver.cpp:106] Iteration 5073, lr = 0.0025
I0520 20:36:25.551348 27968 solver.cpp:237] Iteration 5130, loss = 1.46247
I0520 20:36:25.551383 27968 solver.cpp:253]     Train net output #0: loss = 1.46247 (* 1 = 1.46247 loss)
I0520 20:36:25.551399 27968 sgd_solver.cpp:106] Iteration 5130, lr = 0.0025
I0520 20:36:33.098376 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_5184.caffemodel
I0520 20:36:33.238554 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_5184.solverstate
I0520 20:36:33.737488 27968 solver.cpp:237] Iteration 5187, loss = 1.34449
I0520 20:36:33.737535 27968 solver.cpp:253]     Train net output #0: loss = 1.34449 (* 1 = 1.34449 loss)
I0520 20:36:33.737551 27968 sgd_solver.cpp:106] Iteration 5187, lr = 0.0025
I0520 20:36:41.854991 27968 solver.cpp:237] Iteration 5244, loss = 1.44701
I0520 20:36:41.855172 27968 solver.cpp:253]     Train net output #0: loss = 1.44701 (* 1 = 1.44701 loss)
I0520 20:36:41.855186 27968 sgd_solver.cpp:106] Iteration 5244, lr = 0.0025
I0520 20:36:49.974262 27968 solver.cpp:237] Iteration 5301, loss = 1.4471
I0520 20:36:49.974297 27968 solver.cpp:253]     Train net output #0: loss = 1.4471 (* 1 = 1.4471 loss)
I0520 20:36:49.974313 27968 sgd_solver.cpp:106] Iteration 5301, lr = 0.0025
I0520 20:36:58.089838 27968 solver.cpp:237] Iteration 5358, loss = 1.41953
I0520 20:36:58.089874 27968 solver.cpp:253]     Train net output #0: loss = 1.41953 (* 1 = 1.41953 loss)
I0520 20:36:58.089891 27968 sgd_solver.cpp:106] Iteration 5358, lr = 0.0025
I0520 20:37:28.406155 27968 solver.cpp:237] Iteration 5415, loss = 1.42824
I0520 20:37:28.406322 27968 solver.cpp:253]     Train net output #0: loss = 1.42824 (* 1 = 1.42824 loss)
I0520 20:37:28.406337 27968 sgd_solver.cpp:106] Iteration 5415, lr = 0.0025
I0520 20:37:36.514046 27968 solver.cpp:237] Iteration 5472, loss = 1.43782
I0520 20:37:36.514080 27968 solver.cpp:253]     Train net output #0: loss = 1.43782 (* 1 = 1.43782 loss)
I0520 20:37:36.514097 27968 sgd_solver.cpp:106] Iteration 5472, lr = 0.0025
I0520 20:37:44.621971 27968 solver.cpp:237] Iteration 5529, loss = 1.42897
I0520 20:37:44.622007 27968 solver.cpp:253]     Train net output #0: loss = 1.42897 (* 1 = 1.42897 loss)
I0520 20:37:44.622023 27968 sgd_solver.cpp:106] Iteration 5529, lr = 0.0025
I0520 20:37:52.727756 27968 solver.cpp:237] Iteration 5586, loss = 1.34365
I0520 20:37:52.727794 27968 solver.cpp:253]     Train net output #0: loss = 1.34365 (* 1 = 1.34365 loss)
I0520 20:37:52.727815 27968 sgd_solver.cpp:106] Iteration 5586, lr = 0.0025
I0520 20:38:00.829779 27968 solver.cpp:237] Iteration 5643, loss = 1.43882
I0520 20:38:00.829932 27968 solver.cpp:253]     Train net output #0: loss = 1.43882 (* 1 = 1.43882 loss)
I0520 20:38:00.829946 27968 sgd_solver.cpp:106] Iteration 5643, lr = 0.0025
I0520 20:38:08.928148 27968 solver.cpp:237] Iteration 5700, loss = 1.3209
I0520 20:38:08.928184 27968 solver.cpp:253]     Train net output #0: loss = 1.3209 (* 1 = 1.3209 loss)
I0520 20:38:08.928200 27968 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0520 20:38:17.025135 27968 solver.cpp:237] Iteration 5757, loss = 1.32619
I0520 20:38:17.025177 27968 solver.cpp:253]     Train net output #0: loss = 1.32619 (* 1 = 1.32619 loss)
I0520 20:38:17.025190 27968 sgd_solver.cpp:106] Iteration 5757, lr = 0.0025
I0520 20:38:17.310080 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_5760.caffemodel
I0520 20:38:17.454569 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_5760.solverstate
I0520 20:38:18.094058 27968 solver.cpp:341] Iteration 5765, Testing net (#0)
I0520 20:39:03.753319 27968 solver.cpp:409]     Test net output #0: accuracy = 0.798556
I0520 20:39:03.753492 27968 solver.cpp:409]     Test net output #1: loss = 0.765145 (* 1 = 0.765145 loss)
I0520 20:39:04.223122 27968 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_5769.caffemodel
I0520 20:39:04.361907 27968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158_iter_5769.solverstate
I0520 20:39:04.408606 27968 solver.cpp:326] Optimization Done.
I0520 20:39:04.408634 27968 caffe.cpp:215] Optimization Done.
Application 11235145 resources: utime ~1266s, stime ~227s, Rss ~5329560, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_260_2016-05-20T11.20.42.230158.solver"
	User time (seconds): 0.56
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:56.62
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15108
	Voluntary context switches: 2970
	Involuntary context switches: 138
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
