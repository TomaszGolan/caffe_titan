2805999
I0520 22:21:18.757637 25338 caffe.cpp:184] Using GPUs 0
I0520 22:21:19.181140 25338 solver.cpp:48] Initializing solver from parameters: 
test_iter: 405
test_interval: 810
base_lr: 0.0025
display: 40
max_iter: 4054
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 405
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403.prototxt"
I0520 22:21:19.205411 25338 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403.prototxt
I0520 22:21:19.229820 25338 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 22:21:19.229888 25338 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 22:21:19.230248 25338 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 370
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:21:19.230429 25338 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:21:19.230453 25338 net.cpp:106] Creating Layer data_hdf5
I0520 22:21:19.230466 25338 net.cpp:411] data_hdf5 -> data
I0520 22:21:19.230500 25338 net.cpp:411] data_hdf5 -> label
I0520 22:21:19.230532 25338 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 22:21:19.239418 25338 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 22:21:19.252276 25338 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 22:21:40.807377 25338 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 22:21:40.821544 25338 net.cpp:150] Setting up data_hdf5
I0520 22:21:40.821590 25338 net.cpp:157] Top shape: 370 1 127 50 (2349500)
I0520 22:21:40.821605 25338 net.cpp:157] Top shape: 370 (370)
I0520 22:21:40.821615 25338 net.cpp:165] Memory required for data: 9399480
I0520 22:21:40.821630 25338 layer_factory.hpp:77] Creating layer conv1
I0520 22:21:40.821663 25338 net.cpp:106] Creating Layer conv1
I0520 22:21:40.821674 25338 net.cpp:454] conv1 <- data
I0520 22:21:40.821698 25338 net.cpp:411] conv1 -> conv1
I0520 22:21:41.542287 25338 net.cpp:150] Setting up conv1
I0520 22:21:41.542335 25338 net.cpp:157] Top shape: 370 12 120 48 (25574400)
I0520 22:21:41.542346 25338 net.cpp:165] Memory required for data: 111697080
I0520 22:21:41.542374 25338 layer_factory.hpp:77] Creating layer relu1
I0520 22:21:41.542395 25338 net.cpp:106] Creating Layer relu1
I0520 22:21:41.542407 25338 net.cpp:454] relu1 <- conv1
I0520 22:21:41.542420 25338 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:21:41.542937 25338 net.cpp:150] Setting up relu1
I0520 22:21:41.542953 25338 net.cpp:157] Top shape: 370 12 120 48 (25574400)
I0520 22:21:41.542964 25338 net.cpp:165] Memory required for data: 213994680
I0520 22:21:41.542974 25338 layer_factory.hpp:77] Creating layer pool1
I0520 22:21:41.542991 25338 net.cpp:106] Creating Layer pool1
I0520 22:21:41.543001 25338 net.cpp:454] pool1 <- conv1
I0520 22:21:41.543015 25338 net.cpp:411] pool1 -> pool1
I0520 22:21:41.543093 25338 net.cpp:150] Setting up pool1
I0520 22:21:41.543108 25338 net.cpp:157] Top shape: 370 12 60 48 (12787200)
I0520 22:21:41.543118 25338 net.cpp:165] Memory required for data: 265143480
I0520 22:21:41.543128 25338 layer_factory.hpp:77] Creating layer conv2
I0520 22:21:41.543150 25338 net.cpp:106] Creating Layer conv2
I0520 22:21:41.543161 25338 net.cpp:454] conv2 <- pool1
I0520 22:21:41.543174 25338 net.cpp:411] conv2 -> conv2
I0520 22:21:41.545821 25338 net.cpp:150] Setting up conv2
I0520 22:21:41.545848 25338 net.cpp:157] Top shape: 370 20 54 46 (18381600)
I0520 22:21:41.545860 25338 net.cpp:165] Memory required for data: 338669880
I0520 22:21:41.545891 25338 layer_factory.hpp:77] Creating layer relu2
I0520 22:21:41.545905 25338 net.cpp:106] Creating Layer relu2
I0520 22:21:41.545915 25338 net.cpp:454] relu2 <- conv2
I0520 22:21:41.545928 25338 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:21:41.546258 25338 net.cpp:150] Setting up relu2
I0520 22:21:41.546272 25338 net.cpp:157] Top shape: 370 20 54 46 (18381600)
I0520 22:21:41.546283 25338 net.cpp:165] Memory required for data: 412196280
I0520 22:21:41.546293 25338 layer_factory.hpp:77] Creating layer pool2
I0520 22:21:41.546305 25338 net.cpp:106] Creating Layer pool2
I0520 22:21:41.546314 25338 net.cpp:454] pool2 <- conv2
I0520 22:21:41.546339 25338 net.cpp:411] pool2 -> pool2
I0520 22:21:41.546408 25338 net.cpp:150] Setting up pool2
I0520 22:21:41.546422 25338 net.cpp:157] Top shape: 370 20 27 46 (9190800)
I0520 22:21:41.546432 25338 net.cpp:165] Memory required for data: 448959480
I0520 22:21:41.546439 25338 layer_factory.hpp:77] Creating layer conv3
I0520 22:21:41.546458 25338 net.cpp:106] Creating Layer conv3
I0520 22:21:41.546469 25338 net.cpp:454] conv3 <- pool2
I0520 22:21:41.546483 25338 net.cpp:411] conv3 -> conv3
I0520 22:21:41.548396 25338 net.cpp:150] Setting up conv3
I0520 22:21:41.548419 25338 net.cpp:157] Top shape: 370 28 22 44 (10028480)
I0520 22:21:41.548431 25338 net.cpp:165] Memory required for data: 489073400
I0520 22:21:41.548450 25338 layer_factory.hpp:77] Creating layer relu3
I0520 22:21:41.548466 25338 net.cpp:106] Creating Layer relu3
I0520 22:21:41.548477 25338 net.cpp:454] relu3 <- conv3
I0520 22:21:41.548490 25338 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:21:41.548956 25338 net.cpp:150] Setting up relu3
I0520 22:21:41.548974 25338 net.cpp:157] Top shape: 370 28 22 44 (10028480)
I0520 22:21:41.548985 25338 net.cpp:165] Memory required for data: 529187320
I0520 22:21:41.548993 25338 layer_factory.hpp:77] Creating layer pool3
I0520 22:21:41.549007 25338 net.cpp:106] Creating Layer pool3
I0520 22:21:41.549016 25338 net.cpp:454] pool3 <- conv3
I0520 22:21:41.549029 25338 net.cpp:411] pool3 -> pool3
I0520 22:21:41.549095 25338 net.cpp:150] Setting up pool3
I0520 22:21:41.549108 25338 net.cpp:157] Top shape: 370 28 11 44 (5014240)
I0520 22:21:41.549118 25338 net.cpp:165] Memory required for data: 549244280
I0520 22:21:41.549129 25338 layer_factory.hpp:77] Creating layer conv4
I0520 22:21:41.549145 25338 net.cpp:106] Creating Layer conv4
I0520 22:21:41.549156 25338 net.cpp:454] conv4 <- pool3
I0520 22:21:41.549170 25338 net.cpp:411] conv4 -> conv4
I0520 22:21:41.551920 25338 net.cpp:150] Setting up conv4
I0520 22:21:41.551944 25338 net.cpp:157] Top shape: 370 36 6 42 (3356640)
I0520 22:21:41.551954 25338 net.cpp:165] Memory required for data: 562670840
I0520 22:21:41.551970 25338 layer_factory.hpp:77] Creating layer relu4
I0520 22:21:41.551983 25338 net.cpp:106] Creating Layer relu4
I0520 22:21:41.551993 25338 net.cpp:454] relu4 <- conv4
I0520 22:21:41.552006 25338 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:21:41.552475 25338 net.cpp:150] Setting up relu4
I0520 22:21:41.552491 25338 net.cpp:157] Top shape: 370 36 6 42 (3356640)
I0520 22:21:41.552502 25338 net.cpp:165] Memory required for data: 576097400
I0520 22:21:41.552511 25338 layer_factory.hpp:77] Creating layer pool4
I0520 22:21:41.552525 25338 net.cpp:106] Creating Layer pool4
I0520 22:21:41.552534 25338 net.cpp:454] pool4 <- conv4
I0520 22:21:41.552547 25338 net.cpp:411] pool4 -> pool4
I0520 22:21:41.552614 25338 net.cpp:150] Setting up pool4
I0520 22:21:41.552628 25338 net.cpp:157] Top shape: 370 36 3 42 (1678320)
I0520 22:21:41.552639 25338 net.cpp:165] Memory required for data: 582810680
I0520 22:21:41.552649 25338 layer_factory.hpp:77] Creating layer ip1
I0520 22:21:41.552666 25338 net.cpp:106] Creating Layer ip1
I0520 22:21:41.552677 25338 net.cpp:454] ip1 <- pool4
I0520 22:21:41.552691 25338 net.cpp:411] ip1 -> ip1
I0520 22:21:41.568069 25338 net.cpp:150] Setting up ip1
I0520 22:21:41.568094 25338 net.cpp:157] Top shape: 370 196 (72520)
I0520 22:21:41.568106 25338 net.cpp:165] Memory required for data: 583100760
I0520 22:21:41.568132 25338 layer_factory.hpp:77] Creating layer relu5
I0520 22:21:41.568147 25338 net.cpp:106] Creating Layer relu5
I0520 22:21:41.568158 25338 net.cpp:454] relu5 <- ip1
I0520 22:21:41.568171 25338 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:21:41.568513 25338 net.cpp:150] Setting up relu5
I0520 22:21:41.568527 25338 net.cpp:157] Top shape: 370 196 (72520)
I0520 22:21:41.568538 25338 net.cpp:165] Memory required for data: 583390840
I0520 22:21:41.568548 25338 layer_factory.hpp:77] Creating layer drop1
I0520 22:21:41.568568 25338 net.cpp:106] Creating Layer drop1
I0520 22:21:41.568579 25338 net.cpp:454] drop1 <- ip1
I0520 22:21:41.568604 25338 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:21:41.568651 25338 net.cpp:150] Setting up drop1
I0520 22:21:41.568665 25338 net.cpp:157] Top shape: 370 196 (72520)
I0520 22:21:41.568675 25338 net.cpp:165] Memory required for data: 583680920
I0520 22:21:41.568684 25338 layer_factory.hpp:77] Creating layer ip2
I0520 22:21:41.568702 25338 net.cpp:106] Creating Layer ip2
I0520 22:21:41.568712 25338 net.cpp:454] ip2 <- ip1
I0520 22:21:41.568724 25338 net.cpp:411] ip2 -> ip2
I0520 22:21:41.569190 25338 net.cpp:150] Setting up ip2
I0520 22:21:41.569205 25338 net.cpp:157] Top shape: 370 98 (36260)
I0520 22:21:41.569213 25338 net.cpp:165] Memory required for data: 583825960
I0520 22:21:41.569228 25338 layer_factory.hpp:77] Creating layer relu6
I0520 22:21:41.569241 25338 net.cpp:106] Creating Layer relu6
I0520 22:21:41.569250 25338 net.cpp:454] relu6 <- ip2
I0520 22:21:41.569262 25338 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:21:41.569782 25338 net.cpp:150] Setting up relu6
I0520 22:21:41.569798 25338 net.cpp:157] Top shape: 370 98 (36260)
I0520 22:21:41.569809 25338 net.cpp:165] Memory required for data: 583971000
I0520 22:21:41.569819 25338 layer_factory.hpp:77] Creating layer drop2
I0520 22:21:41.569833 25338 net.cpp:106] Creating Layer drop2
I0520 22:21:41.569842 25338 net.cpp:454] drop2 <- ip2
I0520 22:21:41.569854 25338 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:21:41.569905 25338 net.cpp:150] Setting up drop2
I0520 22:21:41.569917 25338 net.cpp:157] Top shape: 370 98 (36260)
I0520 22:21:41.569927 25338 net.cpp:165] Memory required for data: 584116040
I0520 22:21:41.569937 25338 layer_factory.hpp:77] Creating layer ip3
I0520 22:21:41.569950 25338 net.cpp:106] Creating Layer ip3
I0520 22:21:41.569960 25338 net.cpp:454] ip3 <- ip2
I0520 22:21:41.569973 25338 net.cpp:411] ip3 -> ip3
I0520 22:21:41.570181 25338 net.cpp:150] Setting up ip3
I0520 22:21:41.570195 25338 net.cpp:157] Top shape: 370 11 (4070)
I0520 22:21:41.570205 25338 net.cpp:165] Memory required for data: 584132320
I0520 22:21:41.570220 25338 layer_factory.hpp:77] Creating layer drop3
I0520 22:21:41.570232 25338 net.cpp:106] Creating Layer drop3
I0520 22:21:41.570242 25338 net.cpp:454] drop3 <- ip3
I0520 22:21:41.570255 25338 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:21:41.570291 25338 net.cpp:150] Setting up drop3
I0520 22:21:41.570304 25338 net.cpp:157] Top shape: 370 11 (4070)
I0520 22:21:41.570313 25338 net.cpp:165] Memory required for data: 584148600
I0520 22:21:41.570323 25338 layer_factory.hpp:77] Creating layer loss
I0520 22:21:41.570343 25338 net.cpp:106] Creating Layer loss
I0520 22:21:41.570353 25338 net.cpp:454] loss <- ip3
I0520 22:21:41.570363 25338 net.cpp:454] loss <- label
I0520 22:21:41.570375 25338 net.cpp:411] loss -> loss
I0520 22:21:41.570392 25338 layer_factory.hpp:77] Creating layer loss
I0520 22:21:41.571038 25338 net.cpp:150] Setting up loss
I0520 22:21:41.571054 25338 net.cpp:157] Top shape: (1)
I0520 22:21:41.571065 25338 net.cpp:160]     with loss weight 1
I0520 22:21:41.571107 25338 net.cpp:165] Memory required for data: 584148604
I0520 22:21:41.571118 25338 net.cpp:226] loss needs backward computation.
I0520 22:21:41.571128 25338 net.cpp:226] drop3 needs backward computation.
I0520 22:21:41.571138 25338 net.cpp:226] ip3 needs backward computation.
I0520 22:21:41.571147 25338 net.cpp:226] drop2 needs backward computation.
I0520 22:21:41.571157 25338 net.cpp:226] relu6 needs backward computation.
I0520 22:21:41.571167 25338 net.cpp:226] ip2 needs backward computation.
I0520 22:21:41.571177 25338 net.cpp:226] drop1 needs backward computation.
I0520 22:21:41.571187 25338 net.cpp:226] relu5 needs backward computation.
I0520 22:21:41.571197 25338 net.cpp:226] ip1 needs backward computation.
I0520 22:21:41.571208 25338 net.cpp:226] pool4 needs backward computation.
I0520 22:21:41.571218 25338 net.cpp:226] relu4 needs backward computation.
I0520 22:21:41.571228 25338 net.cpp:226] conv4 needs backward computation.
I0520 22:21:41.571238 25338 net.cpp:226] pool3 needs backward computation.
I0520 22:21:41.571256 25338 net.cpp:226] relu3 needs backward computation.
I0520 22:21:41.571266 25338 net.cpp:226] conv3 needs backward computation.
I0520 22:21:41.571277 25338 net.cpp:226] pool2 needs backward computation.
I0520 22:21:41.571288 25338 net.cpp:226] relu2 needs backward computation.
I0520 22:21:41.571298 25338 net.cpp:226] conv2 needs backward computation.
I0520 22:21:41.571308 25338 net.cpp:226] pool1 needs backward computation.
I0520 22:21:41.571319 25338 net.cpp:226] relu1 needs backward computation.
I0520 22:21:41.571328 25338 net.cpp:226] conv1 needs backward computation.
I0520 22:21:41.571337 25338 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:21:41.571347 25338 net.cpp:270] This network produces output loss
I0520 22:21:41.571372 25338 net.cpp:283] Network initialization done.
I0520 22:21:41.580018 25338 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403.prototxt
I0520 22:21:41.580096 25338 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 22:21:41.580454 25338 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 370
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:21:41.580647 25338 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:21:41.580663 25338 net.cpp:106] Creating Layer data_hdf5
I0520 22:21:41.580677 25338 net.cpp:411] data_hdf5 -> data
I0520 22:21:41.580693 25338 net.cpp:411] data_hdf5 -> label
I0520 22:21:41.580709 25338 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 22:21:41.611199 25338 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 22:22:02.913332 25338 net.cpp:150] Setting up data_hdf5
I0520 22:22:02.913497 25338 net.cpp:157] Top shape: 370 1 127 50 (2349500)
I0520 22:22:02.913512 25338 net.cpp:157] Top shape: 370 (370)
I0520 22:22:02.913523 25338 net.cpp:165] Memory required for data: 9399480
I0520 22:22:02.913537 25338 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 22:22:02.913564 25338 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 22:22:02.913575 25338 net.cpp:454] label_data_hdf5_1_split <- label
I0520 22:22:02.913590 25338 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 22:22:02.913612 25338 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 22:22:02.913684 25338 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 22:22:02.913698 25338 net.cpp:157] Top shape: 370 (370)
I0520 22:22:02.913710 25338 net.cpp:157] Top shape: 370 (370)
I0520 22:22:02.913720 25338 net.cpp:165] Memory required for data: 9402440
I0520 22:22:02.913730 25338 layer_factory.hpp:77] Creating layer conv1
I0520 22:22:02.913753 25338 net.cpp:106] Creating Layer conv1
I0520 22:22:02.913763 25338 net.cpp:454] conv1 <- data
I0520 22:22:02.913777 25338 net.cpp:411] conv1 -> conv1
I0520 22:22:02.915726 25338 net.cpp:150] Setting up conv1
I0520 22:22:02.915750 25338 net.cpp:157] Top shape: 370 12 120 48 (25574400)
I0520 22:22:02.915761 25338 net.cpp:165] Memory required for data: 111700040
I0520 22:22:02.915783 25338 layer_factory.hpp:77] Creating layer relu1
I0520 22:22:02.915798 25338 net.cpp:106] Creating Layer relu1
I0520 22:22:02.915808 25338 net.cpp:454] relu1 <- conv1
I0520 22:22:02.915822 25338 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:22:02.916316 25338 net.cpp:150] Setting up relu1
I0520 22:22:02.916332 25338 net.cpp:157] Top shape: 370 12 120 48 (25574400)
I0520 22:22:02.916342 25338 net.cpp:165] Memory required for data: 213997640
I0520 22:22:02.916352 25338 layer_factory.hpp:77] Creating layer pool1
I0520 22:22:02.916368 25338 net.cpp:106] Creating Layer pool1
I0520 22:22:02.916378 25338 net.cpp:454] pool1 <- conv1
I0520 22:22:02.916391 25338 net.cpp:411] pool1 -> pool1
I0520 22:22:02.916466 25338 net.cpp:150] Setting up pool1
I0520 22:22:02.916479 25338 net.cpp:157] Top shape: 370 12 60 48 (12787200)
I0520 22:22:02.916489 25338 net.cpp:165] Memory required for data: 265146440
I0520 22:22:02.916498 25338 layer_factory.hpp:77] Creating layer conv2
I0520 22:22:02.916518 25338 net.cpp:106] Creating Layer conv2
I0520 22:22:02.916528 25338 net.cpp:454] conv2 <- pool1
I0520 22:22:02.916543 25338 net.cpp:411] conv2 -> conv2
I0520 22:22:02.918503 25338 net.cpp:150] Setting up conv2
I0520 22:22:02.918526 25338 net.cpp:157] Top shape: 370 20 54 46 (18381600)
I0520 22:22:02.918539 25338 net.cpp:165] Memory required for data: 338672840
I0520 22:22:02.918556 25338 layer_factory.hpp:77] Creating layer relu2
I0520 22:22:02.918570 25338 net.cpp:106] Creating Layer relu2
I0520 22:22:02.918579 25338 net.cpp:454] relu2 <- conv2
I0520 22:22:02.918591 25338 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:22:02.918925 25338 net.cpp:150] Setting up relu2
I0520 22:22:02.918938 25338 net.cpp:157] Top shape: 370 20 54 46 (18381600)
I0520 22:22:02.918947 25338 net.cpp:165] Memory required for data: 412199240
I0520 22:22:02.918957 25338 layer_factory.hpp:77] Creating layer pool2
I0520 22:22:02.918970 25338 net.cpp:106] Creating Layer pool2
I0520 22:22:02.918979 25338 net.cpp:454] pool2 <- conv2
I0520 22:22:02.918992 25338 net.cpp:411] pool2 -> pool2
I0520 22:22:02.919064 25338 net.cpp:150] Setting up pool2
I0520 22:22:02.919076 25338 net.cpp:157] Top shape: 370 20 27 46 (9190800)
I0520 22:22:02.919086 25338 net.cpp:165] Memory required for data: 448962440
I0520 22:22:02.919095 25338 layer_factory.hpp:77] Creating layer conv3
I0520 22:22:02.919114 25338 net.cpp:106] Creating Layer conv3
I0520 22:22:02.919124 25338 net.cpp:454] conv3 <- pool2
I0520 22:22:02.919137 25338 net.cpp:411] conv3 -> conv3
I0520 22:22:02.921113 25338 net.cpp:150] Setting up conv3
I0520 22:22:02.921136 25338 net.cpp:157] Top shape: 370 28 22 44 (10028480)
I0520 22:22:02.921146 25338 net.cpp:165] Memory required for data: 489076360
I0520 22:22:02.921178 25338 layer_factory.hpp:77] Creating layer relu3
I0520 22:22:02.921192 25338 net.cpp:106] Creating Layer relu3
I0520 22:22:02.921202 25338 net.cpp:454] relu3 <- conv3
I0520 22:22:02.921216 25338 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:22:02.921687 25338 net.cpp:150] Setting up relu3
I0520 22:22:02.921703 25338 net.cpp:157] Top shape: 370 28 22 44 (10028480)
I0520 22:22:02.921713 25338 net.cpp:165] Memory required for data: 529190280
I0520 22:22:02.921722 25338 layer_factory.hpp:77] Creating layer pool3
I0520 22:22:02.921736 25338 net.cpp:106] Creating Layer pool3
I0520 22:22:02.921746 25338 net.cpp:454] pool3 <- conv3
I0520 22:22:02.921757 25338 net.cpp:411] pool3 -> pool3
I0520 22:22:02.921829 25338 net.cpp:150] Setting up pool3
I0520 22:22:02.921843 25338 net.cpp:157] Top shape: 370 28 11 44 (5014240)
I0520 22:22:02.921852 25338 net.cpp:165] Memory required for data: 549247240
I0520 22:22:02.921869 25338 layer_factory.hpp:77] Creating layer conv4
I0520 22:22:02.921887 25338 net.cpp:106] Creating Layer conv4
I0520 22:22:02.921897 25338 net.cpp:454] conv4 <- pool3
I0520 22:22:02.921912 25338 net.cpp:411] conv4 -> conv4
I0520 22:22:02.923964 25338 net.cpp:150] Setting up conv4
I0520 22:22:02.923982 25338 net.cpp:157] Top shape: 370 36 6 42 (3356640)
I0520 22:22:02.923992 25338 net.cpp:165] Memory required for data: 562673800
I0520 22:22:02.924007 25338 layer_factory.hpp:77] Creating layer relu4
I0520 22:22:02.924021 25338 net.cpp:106] Creating Layer relu4
I0520 22:22:02.924031 25338 net.cpp:454] relu4 <- conv4
I0520 22:22:02.924043 25338 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:22:02.924511 25338 net.cpp:150] Setting up relu4
I0520 22:22:02.924527 25338 net.cpp:157] Top shape: 370 36 6 42 (3356640)
I0520 22:22:02.924537 25338 net.cpp:165] Memory required for data: 576100360
I0520 22:22:02.924546 25338 layer_factory.hpp:77] Creating layer pool4
I0520 22:22:02.924559 25338 net.cpp:106] Creating Layer pool4
I0520 22:22:02.924568 25338 net.cpp:454] pool4 <- conv4
I0520 22:22:02.924582 25338 net.cpp:411] pool4 -> pool4
I0520 22:22:02.924654 25338 net.cpp:150] Setting up pool4
I0520 22:22:02.924666 25338 net.cpp:157] Top shape: 370 36 3 42 (1678320)
I0520 22:22:02.924676 25338 net.cpp:165] Memory required for data: 582813640
I0520 22:22:02.924686 25338 layer_factory.hpp:77] Creating layer ip1
I0520 22:22:02.924702 25338 net.cpp:106] Creating Layer ip1
I0520 22:22:02.924713 25338 net.cpp:454] ip1 <- pool4
I0520 22:22:02.924724 25338 net.cpp:411] ip1 -> ip1
I0520 22:22:02.940281 25338 net.cpp:150] Setting up ip1
I0520 22:22:02.940310 25338 net.cpp:157] Top shape: 370 196 (72520)
I0520 22:22:02.940325 25338 net.cpp:165] Memory required for data: 583103720
I0520 22:22:02.940348 25338 layer_factory.hpp:77] Creating layer relu5
I0520 22:22:02.940363 25338 net.cpp:106] Creating Layer relu5
I0520 22:22:02.940373 25338 net.cpp:454] relu5 <- ip1
I0520 22:22:02.940387 25338 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:22:02.940734 25338 net.cpp:150] Setting up relu5
I0520 22:22:02.940748 25338 net.cpp:157] Top shape: 370 196 (72520)
I0520 22:22:02.940758 25338 net.cpp:165] Memory required for data: 583393800
I0520 22:22:02.940768 25338 layer_factory.hpp:77] Creating layer drop1
I0520 22:22:02.940786 25338 net.cpp:106] Creating Layer drop1
I0520 22:22:02.940796 25338 net.cpp:454] drop1 <- ip1
I0520 22:22:02.940809 25338 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:22:02.940853 25338 net.cpp:150] Setting up drop1
I0520 22:22:02.940866 25338 net.cpp:157] Top shape: 370 196 (72520)
I0520 22:22:02.940876 25338 net.cpp:165] Memory required for data: 583683880
I0520 22:22:02.940886 25338 layer_factory.hpp:77] Creating layer ip2
I0520 22:22:02.940901 25338 net.cpp:106] Creating Layer ip2
I0520 22:22:02.940910 25338 net.cpp:454] ip2 <- ip1
I0520 22:22:02.940923 25338 net.cpp:411] ip2 -> ip2
I0520 22:22:02.941406 25338 net.cpp:150] Setting up ip2
I0520 22:22:02.941419 25338 net.cpp:157] Top shape: 370 98 (36260)
I0520 22:22:02.941429 25338 net.cpp:165] Memory required for data: 583828920
I0520 22:22:02.941457 25338 layer_factory.hpp:77] Creating layer relu6
I0520 22:22:02.941469 25338 net.cpp:106] Creating Layer relu6
I0520 22:22:02.941479 25338 net.cpp:454] relu6 <- ip2
I0520 22:22:02.941493 25338 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:22:02.942039 25338 net.cpp:150] Setting up relu6
I0520 22:22:02.942055 25338 net.cpp:157] Top shape: 370 98 (36260)
I0520 22:22:02.942066 25338 net.cpp:165] Memory required for data: 583973960
I0520 22:22:02.942076 25338 layer_factory.hpp:77] Creating layer drop2
I0520 22:22:02.942090 25338 net.cpp:106] Creating Layer drop2
I0520 22:22:02.942100 25338 net.cpp:454] drop2 <- ip2
I0520 22:22:02.942112 25338 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:22:02.942157 25338 net.cpp:150] Setting up drop2
I0520 22:22:02.942169 25338 net.cpp:157] Top shape: 370 98 (36260)
I0520 22:22:02.942179 25338 net.cpp:165] Memory required for data: 584119000
I0520 22:22:02.942188 25338 layer_factory.hpp:77] Creating layer ip3
I0520 22:22:02.942201 25338 net.cpp:106] Creating Layer ip3
I0520 22:22:02.942211 25338 net.cpp:454] ip3 <- ip2
I0520 22:22:02.942225 25338 net.cpp:411] ip3 -> ip3
I0520 22:22:02.942451 25338 net.cpp:150] Setting up ip3
I0520 22:22:02.942463 25338 net.cpp:157] Top shape: 370 11 (4070)
I0520 22:22:02.942472 25338 net.cpp:165] Memory required for data: 584135280
I0520 22:22:02.942489 25338 layer_factory.hpp:77] Creating layer drop3
I0520 22:22:02.942502 25338 net.cpp:106] Creating Layer drop3
I0520 22:22:02.942512 25338 net.cpp:454] drop3 <- ip3
I0520 22:22:02.942525 25338 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:22:02.942566 25338 net.cpp:150] Setting up drop3
I0520 22:22:02.942579 25338 net.cpp:157] Top shape: 370 11 (4070)
I0520 22:22:02.942589 25338 net.cpp:165] Memory required for data: 584151560
I0520 22:22:02.942598 25338 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 22:22:02.942611 25338 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 22:22:02.942622 25338 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 22:22:02.942636 25338 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 22:22:02.942651 25338 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 22:22:02.942723 25338 net.cpp:150] Setting up ip3_drop3_0_split
I0520 22:22:02.942737 25338 net.cpp:157] Top shape: 370 11 (4070)
I0520 22:22:02.942751 25338 net.cpp:157] Top shape: 370 11 (4070)
I0520 22:22:02.942761 25338 net.cpp:165] Memory required for data: 584184120
I0520 22:22:02.942770 25338 layer_factory.hpp:77] Creating layer accuracy
I0520 22:22:02.942791 25338 net.cpp:106] Creating Layer accuracy
I0520 22:22:02.942801 25338 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 22:22:02.942812 25338 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 22:22:02.942824 25338 net.cpp:411] accuracy -> accuracy
I0520 22:22:02.942847 25338 net.cpp:150] Setting up accuracy
I0520 22:22:02.942860 25338 net.cpp:157] Top shape: (1)
I0520 22:22:02.942870 25338 net.cpp:165] Memory required for data: 584184124
I0520 22:22:02.942881 25338 layer_factory.hpp:77] Creating layer loss
I0520 22:22:02.942894 25338 net.cpp:106] Creating Layer loss
I0520 22:22:02.942904 25338 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 22:22:02.942915 25338 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 22:22:02.942927 25338 net.cpp:411] loss -> loss
I0520 22:22:02.942945 25338 layer_factory.hpp:77] Creating layer loss
I0520 22:22:02.943431 25338 net.cpp:150] Setting up loss
I0520 22:22:02.943445 25338 net.cpp:157] Top shape: (1)
I0520 22:22:02.943455 25338 net.cpp:160]     with loss weight 1
I0520 22:22:02.943473 25338 net.cpp:165] Memory required for data: 584184128
I0520 22:22:02.943483 25338 net.cpp:226] loss needs backward computation.
I0520 22:22:02.943495 25338 net.cpp:228] accuracy does not need backward computation.
I0520 22:22:02.943506 25338 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 22:22:02.943516 25338 net.cpp:226] drop3 needs backward computation.
I0520 22:22:02.943526 25338 net.cpp:226] ip3 needs backward computation.
I0520 22:22:02.943534 25338 net.cpp:226] drop2 needs backward computation.
I0520 22:22:02.943554 25338 net.cpp:226] relu6 needs backward computation.
I0520 22:22:02.943565 25338 net.cpp:226] ip2 needs backward computation.
I0520 22:22:02.943575 25338 net.cpp:226] drop1 needs backward computation.
I0520 22:22:02.943585 25338 net.cpp:226] relu5 needs backward computation.
I0520 22:22:02.943594 25338 net.cpp:226] ip1 needs backward computation.
I0520 22:22:02.943604 25338 net.cpp:226] pool4 needs backward computation.
I0520 22:22:02.943614 25338 net.cpp:226] relu4 needs backward computation.
I0520 22:22:02.943624 25338 net.cpp:226] conv4 needs backward computation.
I0520 22:22:02.943634 25338 net.cpp:226] pool3 needs backward computation.
I0520 22:22:02.943645 25338 net.cpp:226] relu3 needs backward computation.
I0520 22:22:02.943655 25338 net.cpp:226] conv3 needs backward computation.
I0520 22:22:02.943665 25338 net.cpp:226] pool2 needs backward computation.
I0520 22:22:02.943675 25338 net.cpp:226] relu2 needs backward computation.
I0520 22:22:02.943684 25338 net.cpp:226] conv2 needs backward computation.
I0520 22:22:02.943696 25338 net.cpp:226] pool1 needs backward computation.
I0520 22:22:02.943706 25338 net.cpp:226] relu1 needs backward computation.
I0520 22:22:02.943717 25338 net.cpp:226] conv1 needs backward computation.
I0520 22:22:02.943727 25338 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 22:22:02.943739 25338 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:22:02.943753 25338 net.cpp:270] This network produces output accuracy
I0520 22:22:02.943764 25338 net.cpp:270] This network produces output loss
I0520 22:22:02.943797 25338 net.cpp:283] Network initialization done.
I0520 22:22:02.943929 25338 solver.cpp:60] Solver scaffolding done.
I0520 22:22:02.945055 25338 caffe.cpp:212] Starting Optimization
I0520 22:22:02.945073 25338 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 22:22:02.945086 25338 solver.cpp:289] Learning Rate Policy: fixed
I0520 22:22:02.946316 25338 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 22:22:49.125263 25338 solver.cpp:409]     Test net output #0: accuracy = 0.0696098
I0520 22:22:49.125422 25338 solver.cpp:409]     Test net output #1: loss = 2.39846 (* 1 = 2.39846 loss)
I0520 22:22:49.202841 25338 solver.cpp:237] Iteration 0, loss = 2.39957
I0520 22:22:49.202877 25338 solver.cpp:253]     Train net output #0: loss = 2.39957 (* 1 = 2.39957 loss)
I0520 22:22:49.202898 25338 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 22:22:57.210459 25338 solver.cpp:237] Iteration 40, loss = 2.36358
I0520 22:22:57.210495 25338 solver.cpp:253]     Train net output #0: loss = 2.36358 (* 1 = 2.36358 loss)
I0520 22:22:57.210510 25338 sgd_solver.cpp:106] Iteration 40, lr = 0.0025
I0520 22:23:05.216013 25338 solver.cpp:237] Iteration 80, loss = 2.34533
I0520 22:23:05.216055 25338 solver.cpp:253]     Train net output #0: loss = 2.34533 (* 1 = 2.34533 loss)
I0520 22:23:05.216073 25338 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0520 22:23:13.225311 25338 solver.cpp:237] Iteration 120, loss = 2.30765
I0520 22:23:13.225344 25338 solver.cpp:253]     Train net output #0: loss = 2.30765 (* 1 = 2.30765 loss)
I0520 22:23:13.225359 25338 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0520 22:23:21.230803 25338 solver.cpp:237] Iteration 160, loss = 2.32249
I0520 22:23:21.230949 25338 solver.cpp:253]     Train net output #0: loss = 2.32249 (* 1 = 2.32249 loss)
I0520 22:23:21.230962 25338 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0520 22:23:29.232610 25338 solver.cpp:237] Iteration 200, loss = 2.28851
I0520 22:23:29.232655 25338 solver.cpp:253]     Train net output #0: loss = 2.28851 (* 1 = 2.28851 loss)
I0520 22:23:29.232671 25338 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0520 22:23:37.239475 25338 solver.cpp:237] Iteration 240, loss = 2.2437
I0520 22:23:37.239509 25338 solver.cpp:253]     Train net output #0: loss = 2.2437 (* 1 = 2.2437 loss)
I0520 22:23:37.239524 25338 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0520 22:24:07.358072 25338 solver.cpp:237] Iteration 280, loss = 2.16911
I0520 22:24:07.358232 25338 solver.cpp:253]     Train net output #0: loss = 2.16911 (* 1 = 2.16911 loss)
I0520 22:24:07.358247 25338 sgd_solver.cpp:106] Iteration 280, lr = 0.0025
I0520 22:24:15.367239 25338 solver.cpp:237] Iteration 320, loss = 2.16105
I0520 22:24:15.367274 25338 solver.cpp:253]     Train net output #0: loss = 2.16105 (* 1 = 2.16105 loss)
I0520 22:24:15.367290 25338 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0520 22:24:23.371601 25338 solver.cpp:237] Iteration 360, loss = 2.10474
I0520 22:24:23.371645 25338 solver.cpp:253]     Train net output #0: loss = 2.10474 (* 1 = 2.10474 loss)
I0520 22:24:23.371661 25338 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0520 22:24:31.379876 25338 solver.cpp:237] Iteration 400, loss = 2.06271
I0520 22:24:31.379911 25338 solver.cpp:253]     Train net output #0: loss = 2.06271 (* 1 = 2.06271 loss)
I0520 22:24:31.379925 25338 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0520 22:24:32.181489 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_405.caffemodel
I0520 22:24:32.373919 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_405.solverstate
I0520 22:24:39.465147 25338 solver.cpp:237] Iteration 440, loss = 1.94874
I0520 22:24:39.465299 25338 solver.cpp:253]     Train net output #0: loss = 1.94874 (* 1 = 1.94874 loss)
I0520 22:24:39.465313 25338 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0520 22:24:47.473320 25338 solver.cpp:237] Iteration 480, loss = 1.98138
I0520 22:24:47.473369 25338 solver.cpp:253]     Train net output #0: loss = 1.98138 (* 1 = 1.98138 loss)
I0520 22:24:47.473382 25338 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0520 22:24:55.477448 25338 solver.cpp:237] Iteration 520, loss = 1.92332
I0520 22:24:55.477483 25338 solver.cpp:253]     Train net output #0: loss = 1.92332 (* 1 = 1.92332 loss)
I0520 22:24:55.477497 25338 sgd_solver.cpp:106] Iteration 520, lr = 0.0025
I0520 22:25:25.646328 25338 solver.cpp:237] Iteration 560, loss = 1.89241
I0520 22:25:25.646482 25338 solver.cpp:253]     Train net output #0: loss = 1.89241 (* 1 = 1.89241 loss)
I0520 22:25:25.646497 25338 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0520 22:25:33.655444 25338 solver.cpp:237] Iteration 600, loss = 1.89864
I0520 22:25:33.655493 25338 solver.cpp:253]     Train net output #0: loss = 1.89864 (* 1 = 1.89864 loss)
I0520 22:25:33.655506 25338 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0520 22:25:41.666059 25338 solver.cpp:237] Iteration 640, loss = 2.05249
I0520 22:25:41.666092 25338 solver.cpp:253]     Train net output #0: loss = 2.05249 (* 1 = 2.05249 loss)
I0520 22:25:41.666107 25338 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0520 22:25:49.674304 25338 solver.cpp:237] Iteration 680, loss = 1.82522
I0520 22:25:49.674337 25338 solver.cpp:253]     Train net output #0: loss = 1.82522 (* 1 = 1.82522 loss)
I0520 22:25:49.674351 25338 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0520 22:25:57.678447 25338 solver.cpp:237] Iteration 720, loss = 1.78474
I0520 22:25:57.678596 25338 solver.cpp:253]     Train net output #0: loss = 1.78474 (* 1 = 1.78474 loss)
I0520 22:25:57.678608 25338 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0520 22:26:05.681810 25338 solver.cpp:237] Iteration 760, loss = 1.97599
I0520 22:26:05.681844 25338 solver.cpp:253]     Train net output #0: loss = 1.97599 (* 1 = 1.97599 loss)
I0520 22:26:05.681856 25338 sgd_solver.cpp:106] Iteration 760, lr = 0.0025
I0520 22:26:13.688607 25338 solver.cpp:237] Iteration 800, loss = 1.71344
I0520 22:26:13.688642 25338 solver.cpp:253]     Train net output #0: loss = 1.71344 (* 1 = 1.71344 loss)
I0520 22:26:13.688657 25338 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0520 22:26:15.492207 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_810.caffemodel
I0520 22:26:15.677881 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_810.solverstate
I0520 22:26:15.708619 25338 solver.cpp:341] Iteration 810, Testing net (#0)
I0520 22:27:00.993252 25338 solver.cpp:409]     Test net output #0: accuracy = 0.572365
I0520 22:27:00.993410 25338 solver.cpp:409]     Test net output #1: loss = 1.48626 (* 1 = 1.48626 loss)
I0520 22:27:29.171861 25338 solver.cpp:237] Iteration 840, loss = 1.84567
I0520 22:27:29.171908 25338 solver.cpp:253]     Train net output #0: loss = 1.84567 (* 1 = 1.84567 loss)
I0520 22:27:29.171922 25338 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0520 22:27:37.167044 25338 solver.cpp:237] Iteration 880, loss = 1.85465
I0520 22:27:37.167191 25338 solver.cpp:253]     Train net output #0: loss = 1.85465 (* 1 = 1.85465 loss)
I0520 22:27:37.167204 25338 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0520 22:27:45.170138 25338 solver.cpp:237] Iteration 920, loss = 1.88319
I0520 22:27:45.170174 25338 solver.cpp:253]     Train net output #0: loss = 1.88319 (* 1 = 1.88319 loss)
I0520 22:27:45.170195 25338 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0520 22:27:53.170817 25338 solver.cpp:237] Iteration 960, loss = 1.80948
I0520 22:27:53.170851 25338 solver.cpp:253]     Train net output #0: loss = 1.80948 (* 1 = 1.80948 loss)
I0520 22:27:53.170866 25338 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0520 22:28:01.171736 25338 solver.cpp:237] Iteration 1000, loss = 1.79071
I0520 22:28:01.171772 25338 solver.cpp:253]     Train net output #0: loss = 1.79071 (* 1 = 1.79071 loss)
I0520 22:28:01.171785 25338 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0520 22:28:09.170359 25338 solver.cpp:237] Iteration 1040, loss = 1.86981
I0520 22:28:09.170506 25338 solver.cpp:253]     Train net output #0: loss = 1.86981 (* 1 = 1.86981 loss)
I0520 22:28:09.170521 25338 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0520 22:28:17.171901 25338 solver.cpp:237] Iteration 1080, loss = 1.78884
I0520 22:28:17.171936 25338 solver.cpp:253]     Train net output #0: loss = 1.78884 (* 1 = 1.78884 loss)
I0520 22:28:17.171949 25338 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0520 22:28:47.322152 25338 solver.cpp:237] Iteration 1120, loss = 1.74206
I0520 22:28:47.322324 25338 solver.cpp:253]     Train net output #0: loss = 1.74206 (* 1 = 1.74206 loss)
I0520 22:28:47.322340 25338 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0520 22:28:55.321560 25338 solver.cpp:237] Iteration 1160, loss = 1.75446
I0520 22:28:55.321602 25338 solver.cpp:253]     Train net output #0: loss = 1.75446 (* 1 = 1.75446 loss)
I0520 22:28:55.321620 25338 sgd_solver.cpp:106] Iteration 1160, lr = 0.0025
I0520 22:29:03.314316 25338 solver.cpp:237] Iteration 1200, loss = 1.8246
I0520 22:29:03.314350 25338 solver.cpp:253]     Train net output #0: loss = 1.8246 (* 1 = 1.8246 loss)
I0520 22:29:03.314365 25338 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0520 22:29:06.114132 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_1215.caffemodel
I0520 22:29:06.294525 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_1215.solverstate
I0520 22:29:11.395438 25338 solver.cpp:237] Iteration 1240, loss = 1.85222
I0520 22:29:11.395488 25338 solver.cpp:253]     Train net output #0: loss = 1.85222 (* 1 = 1.85222 loss)
I0520 22:29:11.395500 25338 sgd_solver.cpp:106] Iteration 1240, lr = 0.0025
I0520 22:29:19.393352 25338 solver.cpp:237] Iteration 1280, loss = 1.6932
I0520 22:29:19.393512 25338 solver.cpp:253]     Train net output #0: loss = 1.6932 (* 1 = 1.6932 loss)
I0520 22:29:19.393525 25338 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0520 22:29:27.401478 25338 solver.cpp:237] Iteration 1320, loss = 1.73289
I0520 22:29:27.401511 25338 solver.cpp:253]     Train net output #0: loss = 1.73289 (* 1 = 1.73289 loss)
I0520 22:29:27.401526 25338 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0520 22:29:57.623179 25338 solver.cpp:237] Iteration 1360, loss = 1.75101
I0520 22:29:57.623339 25338 solver.cpp:253]     Train net output #0: loss = 1.75101 (* 1 = 1.75101 loss)
I0520 22:29:57.623353 25338 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0520 22:30:05.624451 25338 solver.cpp:237] Iteration 1400, loss = 1.69031
I0520 22:30:05.624485 25338 solver.cpp:253]     Train net output #0: loss = 1.69031 (* 1 = 1.69031 loss)
I0520 22:30:05.624500 25338 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0520 22:30:13.625586 25338 solver.cpp:237] Iteration 1440, loss = 1.69895
I0520 22:30:13.625628 25338 solver.cpp:253]     Train net output #0: loss = 1.69895 (* 1 = 1.69895 loss)
I0520 22:30:13.625644 25338 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0520 22:30:21.623812 25338 solver.cpp:237] Iteration 1480, loss = 1.78686
I0520 22:30:21.623847 25338 solver.cpp:253]     Train net output #0: loss = 1.78686 (* 1 = 1.78686 loss)
I0520 22:30:21.623862 25338 sgd_solver.cpp:106] Iteration 1480, lr = 0.0025
I0520 22:30:29.624346 25338 solver.cpp:237] Iteration 1520, loss = 1.76306
I0520 22:30:29.624498 25338 solver.cpp:253]     Train net output #0: loss = 1.76306 (* 1 = 1.76306 loss)
I0520 22:30:29.624511 25338 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0520 22:30:37.623543 25338 solver.cpp:237] Iteration 1560, loss = 1.71438
I0520 22:30:37.623589 25338 solver.cpp:253]     Train net output #0: loss = 1.71438 (* 1 = 1.71438 loss)
I0520 22:30:37.623603 25338 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0520 22:30:45.628864 25338 solver.cpp:237] Iteration 1600, loss = 1.58175
I0520 22:30:45.628897 25338 solver.cpp:253]     Train net output #0: loss = 1.58175 (* 1 = 1.58175 loss)
I0520 22:30:45.628912 25338 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0520 22:30:49.428583 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_1620.caffemodel
I0520 22:30:49.617698 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_1620.solverstate
I0520 22:30:49.654917 25338 solver.cpp:341] Iteration 1620, Testing net (#0)
I0520 22:31:55.719041 25338 solver.cpp:409]     Test net output #0: accuracy = 0.645058
I0520 22:31:55.719210 25338 solver.cpp:409]     Test net output #1: loss = 1.23298 (* 1 = 1.23298 loss)
I0520 22:32:21.882242 25338 solver.cpp:237] Iteration 1640, loss = 1.71027
I0520 22:32:21.882292 25338 solver.cpp:253]     Train net output #0: loss = 1.71027 (* 1 = 1.71027 loss)
I0520 22:32:21.882307 25338 sgd_solver.cpp:106] Iteration 1640, lr = 0.0025
I0520 22:32:29.887126 25338 solver.cpp:237] Iteration 1680, loss = 1.64349
I0520 22:32:29.887293 25338 solver.cpp:253]     Train net output #0: loss = 1.64349 (* 1 = 1.64349 loss)
I0520 22:32:29.887307 25338 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0520 22:32:37.891330 25338 solver.cpp:237] Iteration 1720, loss = 1.68301
I0520 22:32:37.891367 25338 solver.cpp:253]     Train net output #0: loss = 1.68301 (* 1 = 1.68301 loss)
I0520 22:32:37.891381 25338 sgd_solver.cpp:106] Iteration 1720, lr = 0.0025
I0520 22:32:45.899143 25338 solver.cpp:237] Iteration 1760, loss = 1.73061
I0520 22:32:45.899178 25338 solver.cpp:253]     Train net output #0: loss = 1.73061 (* 1 = 1.73061 loss)
I0520 22:32:45.899191 25338 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0520 22:32:53.907497 25338 solver.cpp:237] Iteration 1800, loss = 1.79981
I0520 22:32:53.907531 25338 solver.cpp:253]     Train net output #0: loss = 1.79981 (* 1 = 1.79981 loss)
I0520 22:32:53.907544 25338 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0520 22:33:01.912024 25338 solver.cpp:237] Iteration 1840, loss = 1.75843
I0520 22:33:01.912174 25338 solver.cpp:253]     Train net output #0: loss = 1.75843 (* 1 = 1.75843 loss)
I0520 22:33:01.912189 25338 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0520 22:33:09.915623 25338 solver.cpp:237] Iteration 1880, loss = 1.65658
I0520 22:33:09.915660 25338 solver.cpp:253]     Train net output #0: loss = 1.65658 (* 1 = 1.65658 loss)
I0520 22:33:09.915680 25338 sgd_solver.cpp:106] Iteration 1880, lr = 0.0025
I0520 22:33:40.105192 25338 solver.cpp:237] Iteration 1920, loss = 1.73608
I0520 22:33:40.105350 25338 solver.cpp:253]     Train net output #0: loss = 1.73608 (* 1 = 1.73608 loss)
I0520 22:33:40.105367 25338 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0520 22:33:48.110743 25338 solver.cpp:237] Iteration 1960, loss = 1.57963
I0520 22:33:48.110775 25338 solver.cpp:253]     Train net output #0: loss = 1.57963 (* 1 = 1.57963 loss)
I0520 22:33:48.110791 25338 sgd_solver.cpp:106] Iteration 1960, lr = 0.0025
I0520 22:33:56.111631 25338 solver.cpp:237] Iteration 2000, loss = 1.63005
I0520 22:33:56.111673 25338 solver.cpp:253]     Train net output #0: loss = 1.63005 (* 1 = 1.63005 loss)
I0520 22:33:56.111690 25338 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0520 22:34:00.914038 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_2025.caffemodel
I0520 22:34:01.103153 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_2025.solverstate
I0520 22:34:04.203058 25338 solver.cpp:237] Iteration 2040, loss = 1.6618
I0520 22:34:04.203105 25338 solver.cpp:253]     Train net output #0: loss = 1.6618 (* 1 = 1.6618 loss)
I0520 22:34:04.203119 25338 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0520 22:34:12.207445 25338 solver.cpp:237] Iteration 2080, loss = 1.68326
I0520 22:34:12.207588 25338 solver.cpp:253]     Train net output #0: loss = 1.68326 (* 1 = 1.68326 loss)
I0520 22:34:12.207602 25338 sgd_solver.cpp:106] Iteration 2080, lr = 0.0025
I0520 22:34:20.211875 25338 solver.cpp:237] Iteration 2120, loss = 1.63316
I0520 22:34:20.211920 25338 solver.cpp:253]     Train net output #0: loss = 1.63316 (* 1 = 1.63316 loss)
I0520 22:34:20.211936 25338 sgd_solver.cpp:106] Iteration 2120, lr = 0.0025
I0520 22:34:28.212616 25338 solver.cpp:237] Iteration 2160, loss = 1.69871
I0520 22:34:28.212651 25338 solver.cpp:253]     Train net output #0: loss = 1.69871 (* 1 = 1.69871 loss)
I0520 22:34:28.212666 25338 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0520 22:34:58.381139 25338 solver.cpp:237] Iteration 2200, loss = 1.62027
I0520 22:34:58.381319 25338 solver.cpp:253]     Train net output #0: loss = 1.62027 (* 1 = 1.62027 loss)
I0520 22:34:58.381333 25338 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0520 22:35:06.382161 25338 solver.cpp:237] Iteration 2240, loss = 1.73309
I0520 22:35:06.382195 25338 solver.cpp:253]     Train net output #0: loss = 1.73309 (* 1 = 1.73309 loss)
I0520 22:35:06.382210 25338 sgd_solver.cpp:106] Iteration 2240, lr = 0.0025
I0520 22:35:14.389719 25338 solver.cpp:237] Iteration 2280, loss = 1.65865
I0520 22:35:14.389757 25338 solver.cpp:253]     Train net output #0: loss = 1.65865 (* 1 = 1.65865 loss)
I0520 22:35:14.389773 25338 sgd_solver.cpp:106] Iteration 2280, lr = 0.0025
I0520 22:35:22.387856 25338 solver.cpp:237] Iteration 2320, loss = 1.60209
I0520 22:35:22.387890 25338 solver.cpp:253]     Train net output #0: loss = 1.60209 (* 1 = 1.60209 loss)
I0520 22:35:22.387904 25338 sgd_solver.cpp:106] Iteration 2320, lr = 0.0025
I0520 22:35:30.387953 25338 solver.cpp:237] Iteration 2360, loss = 1.57776
I0520 22:35:30.388092 25338 solver.cpp:253]     Train net output #0: loss = 1.57776 (* 1 = 1.57776 loss)
I0520 22:35:30.388105 25338 sgd_solver.cpp:106] Iteration 2360, lr = 0.0025
I0520 22:35:38.396777 25338 solver.cpp:237] Iteration 2400, loss = 1.61589
I0520 22:35:38.396821 25338 solver.cpp:253]     Train net output #0: loss = 1.61589 (* 1 = 1.61589 loss)
I0520 22:35:38.396837 25338 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0520 22:35:44.202122 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_2430.caffemodel
I0520 22:35:44.393750 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_2430.solverstate
I0520 22:35:44.430665 25338 solver.cpp:341] Iteration 2430, Testing net (#0)
I0520 22:36:29.308367 25338 solver.cpp:409]     Test net output #0: accuracy = 0.702743
I0520 22:36:29.308526 25338 solver.cpp:409]     Test net output #1: loss = 1.03549 (* 1 = 1.03549 loss)
I0520 22:36:53.568549 25338 solver.cpp:237] Iteration 2440, loss = 1.69144
I0520 22:36:53.568598 25338 solver.cpp:253]     Train net output #0: loss = 1.69144 (* 1 = 1.69144 loss)
I0520 22:36:53.568614 25338 sgd_solver.cpp:106] Iteration 2440, lr = 0.0025
I0520 22:37:01.571974 25338 solver.cpp:237] Iteration 2480, loss = 1.48837
I0520 22:37:01.572124 25338 solver.cpp:253]     Train net output #0: loss = 1.48837 (* 1 = 1.48837 loss)
I0520 22:37:01.572137 25338 sgd_solver.cpp:106] Iteration 2480, lr = 0.0025
I0520 22:37:09.578761 25338 solver.cpp:237] Iteration 2520, loss = 1.61407
I0520 22:37:09.578794 25338 solver.cpp:253]     Train net output #0: loss = 1.61407 (* 1 = 1.61407 loss)
I0520 22:37:09.578809 25338 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0520 22:37:17.579545 25338 solver.cpp:237] Iteration 2560, loss = 1.64963
I0520 22:37:17.579591 25338 solver.cpp:253]     Train net output #0: loss = 1.64963 (* 1 = 1.64963 loss)
I0520 22:37:17.579605 25338 sgd_solver.cpp:106] Iteration 2560, lr = 0.0025
I0520 22:37:25.582726 25338 solver.cpp:237] Iteration 2600, loss = 1.65147
I0520 22:37:25.582759 25338 solver.cpp:253]     Train net output #0: loss = 1.65147 (* 1 = 1.65147 loss)
I0520 22:37:25.582773 25338 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0520 22:37:33.585672 25338 solver.cpp:237] Iteration 2640, loss = 1.60604
I0520 22:37:33.585824 25338 solver.cpp:253]     Train net output #0: loss = 1.60604 (* 1 = 1.60604 loss)
I0520 22:37:33.585836 25338 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0520 22:37:41.585774 25338 solver.cpp:237] Iteration 2680, loss = 1.69419
I0520 22:37:41.585820 25338 solver.cpp:253]     Train net output #0: loss = 1.69419 (* 1 = 1.69419 loss)
I0520 22:37:41.585834 25338 sgd_solver.cpp:106] Iteration 2680, lr = 0.0025
I0520 22:38:11.739327 25338 solver.cpp:237] Iteration 2720, loss = 1.59237
I0520 22:38:11.739495 25338 solver.cpp:253]     Train net output #0: loss = 1.59237 (* 1 = 1.59237 loss)
I0520 22:38:11.739512 25338 sgd_solver.cpp:106] Iteration 2720, lr = 0.0025
I0520 22:38:19.743679 25338 solver.cpp:237] Iteration 2760, loss = 1.64158
I0520 22:38:19.743712 25338 solver.cpp:253]     Train net output #0: loss = 1.64158 (* 1 = 1.64158 loss)
I0520 22:38:19.743727 25338 sgd_solver.cpp:106] Iteration 2760, lr = 0.0025
I0520 22:38:27.749317 25338 solver.cpp:237] Iteration 2800, loss = 1.60484
I0520 22:38:27.749352 25338 solver.cpp:253]     Train net output #0: loss = 1.60484 (* 1 = 1.60484 loss)
I0520 22:38:27.749366 25338 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0520 22:38:34.555944 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_2835.caffemodel
I0520 22:38:34.756842 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_2835.solverstate
I0520 22:38:35.854557 25338 solver.cpp:237] Iteration 2840, loss = 1.66265
I0520 22:38:35.854603 25338 solver.cpp:253]     Train net output #0: loss = 1.66265 (* 1 = 1.66265 loss)
I0520 22:38:35.854616 25338 sgd_solver.cpp:106] Iteration 2840, lr = 0.0025
I0520 22:38:43.860143 25338 solver.cpp:237] Iteration 2880, loss = 1.59148
I0520 22:38:43.860285 25338 solver.cpp:253]     Train net output #0: loss = 1.59148 (* 1 = 1.59148 loss)
I0520 22:38:43.860297 25338 sgd_solver.cpp:106] Iteration 2880, lr = 0.0025
I0520 22:38:51.863786 25338 solver.cpp:237] Iteration 2920, loss = 1.48728
I0520 22:38:51.863819 25338 solver.cpp:253]     Train net output #0: loss = 1.48728 (* 1 = 1.48728 loss)
I0520 22:38:51.863837 25338 sgd_solver.cpp:106] Iteration 2920, lr = 0.0025
I0520 22:38:59.867741 25338 solver.cpp:237] Iteration 2960, loss = 1.63482
I0520 22:38:59.867789 25338 solver.cpp:253]     Train net output #0: loss = 1.63482 (* 1 = 1.63482 loss)
I0520 22:38:59.867802 25338 sgd_solver.cpp:106] Iteration 2960, lr = 0.0025
I0520 22:39:30.036559 25338 solver.cpp:237] Iteration 3000, loss = 1.4392
I0520 22:39:30.036727 25338 solver.cpp:253]     Train net output #0: loss = 1.4392 (* 1 = 1.4392 loss)
I0520 22:39:30.036742 25338 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0520 22:39:38.043560 25338 solver.cpp:237] Iteration 3040, loss = 1.53053
I0520 22:39:38.043596 25338 solver.cpp:253]     Train net output #0: loss = 1.53053 (* 1 = 1.53053 loss)
I0520 22:39:38.043611 25338 sgd_solver.cpp:106] Iteration 3040, lr = 0.0025
I0520 22:39:46.056025 25338 solver.cpp:237] Iteration 3080, loss = 1.56152
I0520 22:39:46.056068 25338 solver.cpp:253]     Train net output #0: loss = 1.56152 (* 1 = 1.56152 loss)
I0520 22:39:46.056083 25338 sgd_solver.cpp:106] Iteration 3080, lr = 0.0025
I0520 22:39:54.061853 25338 solver.cpp:237] Iteration 3120, loss = 1.59127
I0520 22:39:54.061892 25338 solver.cpp:253]     Train net output #0: loss = 1.59127 (* 1 = 1.59127 loss)
I0520 22:39:54.061908 25338 sgd_solver.cpp:106] Iteration 3120, lr = 0.0025
I0520 22:40:02.065160 25338 solver.cpp:237] Iteration 3160, loss = 1.59968
I0520 22:40:02.065299 25338 solver.cpp:253]     Train net output #0: loss = 1.59968 (* 1 = 1.59968 loss)
I0520 22:40:02.065313 25338 sgd_solver.cpp:106] Iteration 3160, lr = 0.0025
I0520 22:40:10.069893 25338 solver.cpp:237] Iteration 3200, loss = 1.91291
I0520 22:40:10.069931 25338 solver.cpp:253]     Train net output #0: loss = 1.91291 (* 1 = 1.91291 loss)
I0520 22:40:10.069943 25338 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0520 22:40:17.874500 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_3240.caffemodel
I0520 22:40:18.052224 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_3240.solverstate
I0520 22:40:18.078277 25338 solver.cpp:341] Iteration 3240, Testing net (#0)
I0520 22:41:24.104626 25338 solver.cpp:409]     Test net output #0: accuracy = 0.733587
I0520 22:41:24.104799 25338 solver.cpp:409]     Test net output #1: loss = 0.938281 (* 1 = 0.938281 loss)
I0520 22:41:24.164754 25338 solver.cpp:237] Iteration 3240, loss = 1.57033
I0520 22:41:24.164783 25338 solver.cpp:253]     Train net output #0: loss = 1.57033 (* 1 = 1.57033 loss)
I0520 22:41:24.164801 25338 sgd_solver.cpp:106] Iteration 3240, lr = 0.0025
I0520 22:41:54.303994 25338 solver.cpp:237] Iteration 3280, loss = 1.54376
I0520 22:41:54.304162 25338 solver.cpp:253]     Train net output #0: loss = 1.54376 (* 1 = 1.54376 loss)
I0520 22:41:54.304179 25338 sgd_solver.cpp:106] Iteration 3280, lr = 0.0025
I0520 22:42:02.314993 25338 solver.cpp:237] Iteration 3320, loss = 1.56315
I0520 22:42:02.315026 25338 solver.cpp:253]     Train net output #0: loss = 1.56315 (* 1 = 1.56315 loss)
I0520 22:42:02.315042 25338 sgd_solver.cpp:106] Iteration 3320, lr = 0.0025
I0520 22:42:10.324947 25338 solver.cpp:237] Iteration 3360, loss = 1.52846
I0520 22:42:10.324981 25338 solver.cpp:253]     Train net output #0: loss = 1.52846 (* 1 = 1.52846 loss)
I0520 22:42:10.324996 25338 sgd_solver.cpp:106] Iteration 3360, lr = 0.0025
I0520 22:42:18.335862 25338 solver.cpp:237] Iteration 3400, loss = 1.5611
I0520 22:42:18.335908 25338 solver.cpp:253]     Train net output #0: loss = 1.5611 (* 1 = 1.5611 loss)
I0520 22:42:18.335922 25338 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0520 22:42:26.345118 25338 solver.cpp:237] Iteration 3440, loss = 1.4111
I0520 22:42:26.345262 25338 solver.cpp:253]     Train net output #0: loss = 1.4111 (* 1 = 1.4111 loss)
I0520 22:42:26.345275 25338 sgd_solver.cpp:106] Iteration 3440, lr = 0.0025
I0520 22:42:34.355592 25338 solver.cpp:237] Iteration 3480, loss = 1.4229
I0520 22:42:34.355625 25338 solver.cpp:253]     Train net output #0: loss = 1.4229 (* 1 = 1.4229 loss)
I0520 22:42:34.355641 25338 sgd_solver.cpp:106] Iteration 3480, lr = 0.0025
I0520 22:43:04.499768 25338 solver.cpp:237] Iteration 3520, loss = 1.50513
I0520 22:43:04.499938 25338 solver.cpp:253]     Train net output #0: loss = 1.50513 (* 1 = 1.50513 loss)
I0520 22:43:04.499953 25338 sgd_solver.cpp:106] Iteration 3520, lr = 0.0025
I0520 22:43:12.509260 25338 solver.cpp:237] Iteration 3560, loss = 1.46472
I0520 22:43:12.509292 25338 solver.cpp:253]     Train net output #0: loss = 1.46472 (* 1 = 1.46472 loss)
I0520 22:43:12.509308 25338 sgd_solver.cpp:106] Iteration 3560, lr = 0.0025
I0520 22:43:20.519908 25338 solver.cpp:237] Iteration 3600, loss = 1.58605
I0520 22:43:20.519942 25338 solver.cpp:253]     Train net output #0: loss = 1.58605 (* 1 = 1.58605 loss)
I0520 22:43:20.519956 25338 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0520 22:43:28.527072 25338 solver.cpp:237] Iteration 3640, loss = 1.54245
I0520 22:43:28.527108 25338 solver.cpp:253]     Train net output #0: loss = 1.54245 (* 1 = 1.54245 loss)
I0520 22:43:28.527127 25338 sgd_solver.cpp:106] Iteration 3640, lr = 0.0025
I0520 22:43:29.327347 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_3645.caffemodel
I0520 22:43:29.521742 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_3645.solverstate
I0520 22:43:36.627235 25338 solver.cpp:237] Iteration 3680, loss = 1.42899
I0520 22:43:36.627411 25338 solver.cpp:253]     Train net output #0: loss = 1.42899 (* 1 = 1.42899 loss)
I0520 22:43:36.627426 25338 sgd_solver.cpp:106] Iteration 3680, lr = 0.0025
I0520 22:43:44.636327 25338 solver.cpp:237] Iteration 3720, loss = 1.50874
I0520 22:43:44.636360 25338 solver.cpp:253]     Train net output #0: loss = 1.50874 (* 1 = 1.50874 loss)
I0520 22:43:44.636376 25338 sgd_solver.cpp:106] Iteration 3720, lr = 0.0025
I0520 22:43:52.649513 25338 solver.cpp:237] Iteration 3760, loss = 1.38601
I0520 22:43:52.649546 25338 solver.cpp:253]     Train net output #0: loss = 1.38601 (* 1 = 1.38601 loss)
I0520 22:43:52.649560 25338 sgd_solver.cpp:106] Iteration 3760, lr = 0.0025
I0520 22:44:22.820554 25338 solver.cpp:237] Iteration 3800, loss = 1.51217
I0520 22:44:22.820725 25338 solver.cpp:253]     Train net output #0: loss = 1.51217 (* 1 = 1.51217 loss)
I0520 22:44:22.820739 25338 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0520 22:44:30.826586 25338 solver.cpp:237] Iteration 3840, loss = 1.40212
I0520 22:44:30.826618 25338 solver.cpp:253]     Train net output #0: loss = 1.40212 (* 1 = 1.40212 loss)
I0520 22:44:30.826634 25338 sgd_solver.cpp:106] Iteration 3840, lr = 0.0025
I0520 22:44:38.834508 25338 solver.cpp:237] Iteration 3880, loss = 1.51443
I0520 22:44:38.834542 25338 solver.cpp:253]     Train net output #0: loss = 1.51443 (* 1 = 1.51443 loss)
I0520 22:44:38.834558 25338 sgd_solver.cpp:106] Iteration 3880, lr = 0.0025
I0520 22:44:46.840883 25338 solver.cpp:237] Iteration 3920, loss = 1.39652
I0520 22:44:46.840919 25338 solver.cpp:253]     Train net output #0: loss = 1.39652 (* 1 = 1.39652 loss)
I0520 22:44:46.840935 25338 sgd_solver.cpp:106] Iteration 3920, lr = 0.0025
I0520 22:44:54.855293 25338 solver.cpp:237] Iteration 3960, loss = 1.52469
I0520 22:44:54.855438 25338 solver.cpp:253]     Train net output #0: loss = 1.52469 (* 1 = 1.52469 loss)
I0520 22:44:54.855451 25338 sgd_solver.cpp:106] Iteration 3960, lr = 0.0025
I0520 22:45:02.862439 25338 solver.cpp:237] Iteration 4000, loss = 1.46812
I0520 22:45:02.862471 25338 solver.cpp:253]     Train net output #0: loss = 1.46812 (* 1 = 1.46812 loss)
I0520 22:45:02.862486 25338 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0520 22:45:10.871507 25338 solver.cpp:237] Iteration 4040, loss = 1.42918
I0520 22:45:10.871541 25338 solver.cpp:253]     Train net output #0: loss = 1.42918 (* 1 = 1.42918 loss)
I0520 22:45:10.871551 25338 sgd_solver.cpp:106] Iteration 4040, lr = 0.0025
I0520 22:45:12.672117 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_4050.caffemodel
I0520 22:45:12.852459 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_4050.solverstate
I0520 22:45:12.880985 25338 solver.cpp:341] Iteration 4050, Testing net (#0)
I0520 22:45:58.099117 25338 solver.cpp:409]     Test net output #0: accuracy = 0.774575
I0520 22:45:58.099287 25338 solver.cpp:409]     Test net output #1: loss = 0.827497 (* 1 = 0.827497 loss)
I0520 22:45:58.758358 25338 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_4054.caffemodel
I0520 22:45:58.938437 25338 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403_iter_4054.solverstate
I0520 22:45:58.966617 25338 solver.cpp:326] Optimization Done.
I0520 22:45:58.966645 25338 caffe.cpp:215] Optimization Done.
Application 11235655 resources: utime ~1254s, stime ~227s, Rss ~5333196, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_370_2016-05-20T11.20.46.176403.solver"
	User time (seconds): 0.57
	System time (seconds): 0.10
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:47.62
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15073
	Voluntary context switches: 2732
	Involuntary context switches: 85
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
