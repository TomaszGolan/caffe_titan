2806134
I0521 01:23:39.797382  6984 caffe.cpp:184] Using GPUs 0
I0521 01:23:40.218029  6984 solver.cpp:48] Initializing solver from parameters: 
test_iter: 288
test_interval: 576
base_lr: 0.0025
display: 28
max_iter: 2884
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 288
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699.prototxt"
I0521 01:23:40.219663  6984 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699.prototxt
I0521 01:23:40.238715  6984 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 01:23:40.238775  6984 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 01:23:40.239120  6984 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 520
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:23:40.239308  6984 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:23:40.239332  6984 net.cpp:106] Creating Layer data_hdf5
I0521 01:23:40.239347  6984 net.cpp:411] data_hdf5 -> data
I0521 01:23:40.239382  6984 net.cpp:411] data_hdf5 -> label
I0521 01:23:40.239413  6984 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 01:23:40.252706  6984 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 01:23:40.276767  6984 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 01:24:01.803424  6984 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 01:24:01.808506  6984 net.cpp:150] Setting up data_hdf5
I0521 01:24:01.808550  6984 net.cpp:157] Top shape: 520 1 127 50 (3302000)
I0521 01:24:01.808564  6984 net.cpp:157] Top shape: 520 (520)
I0521 01:24:01.808579  6984 net.cpp:165] Memory required for data: 13210080
I0521 01:24:01.808593  6984 layer_factory.hpp:77] Creating layer conv1
I0521 01:24:01.808629  6984 net.cpp:106] Creating Layer conv1
I0521 01:24:01.808640  6984 net.cpp:454] conv1 <- data
I0521 01:24:01.808662  6984 net.cpp:411] conv1 -> conv1
I0521 01:24:02.677018  6984 net.cpp:150] Setting up conv1
I0521 01:24:02.677067  6984 net.cpp:157] Top shape: 520 12 120 48 (35942400)
I0521 01:24:02.677078  6984 net.cpp:165] Memory required for data: 156979680
I0521 01:24:02.677106  6984 layer_factory.hpp:77] Creating layer relu1
I0521 01:24:02.677127  6984 net.cpp:106] Creating Layer relu1
I0521 01:24:02.677139  6984 net.cpp:454] relu1 <- conv1
I0521 01:24:02.677152  6984 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:24:02.677671  6984 net.cpp:150] Setting up relu1
I0521 01:24:02.677688  6984 net.cpp:157] Top shape: 520 12 120 48 (35942400)
I0521 01:24:02.677698  6984 net.cpp:165] Memory required for data: 300749280
I0521 01:24:02.677709  6984 layer_factory.hpp:77] Creating layer pool1
I0521 01:24:02.677726  6984 net.cpp:106] Creating Layer pool1
I0521 01:24:02.677736  6984 net.cpp:454] pool1 <- conv1
I0521 01:24:02.677748  6984 net.cpp:411] pool1 -> pool1
I0521 01:24:02.677829  6984 net.cpp:150] Setting up pool1
I0521 01:24:02.677842  6984 net.cpp:157] Top shape: 520 12 60 48 (17971200)
I0521 01:24:02.677851  6984 net.cpp:165] Memory required for data: 372634080
I0521 01:24:02.677861  6984 layer_factory.hpp:77] Creating layer conv2
I0521 01:24:02.677886  6984 net.cpp:106] Creating Layer conv2
I0521 01:24:02.677897  6984 net.cpp:454] conv2 <- pool1
I0521 01:24:02.677911  6984 net.cpp:411] conv2 -> conv2
I0521 01:24:02.680596  6984 net.cpp:150] Setting up conv2
I0521 01:24:02.680624  6984 net.cpp:157] Top shape: 520 20 54 46 (25833600)
I0521 01:24:02.680634  6984 net.cpp:165] Memory required for data: 475968480
I0521 01:24:02.680654  6984 layer_factory.hpp:77] Creating layer relu2
I0521 01:24:02.680667  6984 net.cpp:106] Creating Layer relu2
I0521 01:24:02.680678  6984 net.cpp:454] relu2 <- conv2
I0521 01:24:02.680691  6984 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:24:02.681020  6984 net.cpp:150] Setting up relu2
I0521 01:24:02.681035  6984 net.cpp:157] Top shape: 520 20 54 46 (25833600)
I0521 01:24:02.681046  6984 net.cpp:165] Memory required for data: 579302880
I0521 01:24:02.681056  6984 layer_factory.hpp:77] Creating layer pool2
I0521 01:24:02.681068  6984 net.cpp:106] Creating Layer pool2
I0521 01:24:02.681079  6984 net.cpp:454] pool2 <- conv2
I0521 01:24:02.681104  6984 net.cpp:411] pool2 -> pool2
I0521 01:24:02.681174  6984 net.cpp:150] Setting up pool2
I0521 01:24:02.681187  6984 net.cpp:157] Top shape: 520 20 27 46 (12916800)
I0521 01:24:02.681197  6984 net.cpp:165] Memory required for data: 630970080
I0521 01:24:02.681205  6984 layer_factory.hpp:77] Creating layer conv3
I0521 01:24:02.681224  6984 net.cpp:106] Creating Layer conv3
I0521 01:24:02.681234  6984 net.cpp:454] conv3 <- pool2
I0521 01:24:02.681248  6984 net.cpp:411] conv3 -> conv3
I0521 01:24:02.683163  6984 net.cpp:150] Setting up conv3
I0521 01:24:02.683187  6984 net.cpp:157] Top shape: 520 28 22 44 (14094080)
I0521 01:24:02.683199  6984 net.cpp:165] Memory required for data: 687346400
I0521 01:24:02.683218  6984 layer_factory.hpp:77] Creating layer relu3
I0521 01:24:02.683234  6984 net.cpp:106] Creating Layer relu3
I0521 01:24:02.683244  6984 net.cpp:454] relu3 <- conv3
I0521 01:24:02.683257  6984 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:24:02.683742  6984 net.cpp:150] Setting up relu3
I0521 01:24:02.683758  6984 net.cpp:157] Top shape: 520 28 22 44 (14094080)
I0521 01:24:02.683769  6984 net.cpp:165] Memory required for data: 743722720
I0521 01:24:02.683779  6984 layer_factory.hpp:77] Creating layer pool3
I0521 01:24:02.683792  6984 net.cpp:106] Creating Layer pool3
I0521 01:24:02.683802  6984 net.cpp:454] pool3 <- conv3
I0521 01:24:02.683815  6984 net.cpp:411] pool3 -> pool3
I0521 01:24:02.683883  6984 net.cpp:150] Setting up pool3
I0521 01:24:02.683897  6984 net.cpp:157] Top shape: 520 28 11 44 (7047040)
I0521 01:24:02.683907  6984 net.cpp:165] Memory required for data: 771910880
I0521 01:24:02.683913  6984 layer_factory.hpp:77] Creating layer conv4
I0521 01:24:02.683931  6984 net.cpp:106] Creating Layer conv4
I0521 01:24:02.683943  6984 net.cpp:454] conv4 <- pool3
I0521 01:24:02.683956  6984 net.cpp:411] conv4 -> conv4
I0521 01:24:02.686734  6984 net.cpp:150] Setting up conv4
I0521 01:24:02.686764  6984 net.cpp:157] Top shape: 520 36 6 42 (4717440)
I0521 01:24:02.686774  6984 net.cpp:165] Memory required for data: 790780640
I0521 01:24:02.686789  6984 layer_factory.hpp:77] Creating layer relu4
I0521 01:24:02.686803  6984 net.cpp:106] Creating Layer relu4
I0521 01:24:02.686813  6984 net.cpp:454] relu4 <- conv4
I0521 01:24:02.686826  6984 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:24:02.687305  6984 net.cpp:150] Setting up relu4
I0521 01:24:02.687321  6984 net.cpp:157] Top shape: 520 36 6 42 (4717440)
I0521 01:24:02.687332  6984 net.cpp:165] Memory required for data: 809650400
I0521 01:24:02.687342  6984 layer_factory.hpp:77] Creating layer pool4
I0521 01:24:02.687356  6984 net.cpp:106] Creating Layer pool4
I0521 01:24:02.687366  6984 net.cpp:454] pool4 <- conv4
I0521 01:24:02.687379  6984 net.cpp:411] pool4 -> pool4
I0521 01:24:02.687446  6984 net.cpp:150] Setting up pool4
I0521 01:24:02.687460  6984 net.cpp:157] Top shape: 520 36 3 42 (2358720)
I0521 01:24:02.687470  6984 net.cpp:165] Memory required for data: 819085280
I0521 01:24:02.687480  6984 layer_factory.hpp:77] Creating layer ip1
I0521 01:24:02.687500  6984 net.cpp:106] Creating Layer ip1
I0521 01:24:02.687510  6984 net.cpp:454] ip1 <- pool4
I0521 01:24:02.687525  6984 net.cpp:411] ip1 -> ip1
I0521 01:24:02.702957  6984 net.cpp:150] Setting up ip1
I0521 01:24:02.702986  6984 net.cpp:157] Top shape: 520 196 (101920)
I0521 01:24:02.703001  6984 net.cpp:165] Memory required for data: 819492960
I0521 01:24:02.703022  6984 layer_factory.hpp:77] Creating layer relu5
I0521 01:24:02.703037  6984 net.cpp:106] Creating Layer relu5
I0521 01:24:02.703048  6984 net.cpp:454] relu5 <- ip1
I0521 01:24:02.703061  6984 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:24:02.703408  6984 net.cpp:150] Setting up relu5
I0521 01:24:02.703423  6984 net.cpp:157] Top shape: 520 196 (101920)
I0521 01:24:02.703433  6984 net.cpp:165] Memory required for data: 819900640
I0521 01:24:02.703443  6984 layer_factory.hpp:77] Creating layer drop1
I0521 01:24:02.703464  6984 net.cpp:106] Creating Layer drop1
I0521 01:24:02.703474  6984 net.cpp:454] drop1 <- ip1
I0521 01:24:02.703500  6984 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:24:02.703546  6984 net.cpp:150] Setting up drop1
I0521 01:24:02.703558  6984 net.cpp:157] Top shape: 520 196 (101920)
I0521 01:24:02.703568  6984 net.cpp:165] Memory required for data: 820308320
I0521 01:24:02.703578  6984 layer_factory.hpp:77] Creating layer ip2
I0521 01:24:02.703596  6984 net.cpp:106] Creating Layer ip2
I0521 01:24:02.703606  6984 net.cpp:454] ip2 <- ip1
I0521 01:24:02.703619  6984 net.cpp:411] ip2 -> ip2
I0521 01:24:02.704083  6984 net.cpp:150] Setting up ip2
I0521 01:24:02.704097  6984 net.cpp:157] Top shape: 520 98 (50960)
I0521 01:24:02.704107  6984 net.cpp:165] Memory required for data: 820512160
I0521 01:24:02.704123  6984 layer_factory.hpp:77] Creating layer relu6
I0521 01:24:02.704135  6984 net.cpp:106] Creating Layer relu6
I0521 01:24:02.704145  6984 net.cpp:454] relu6 <- ip2
I0521 01:24:02.704156  6984 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:24:02.704675  6984 net.cpp:150] Setting up relu6
I0521 01:24:02.704691  6984 net.cpp:157] Top shape: 520 98 (50960)
I0521 01:24:02.704702  6984 net.cpp:165] Memory required for data: 820716000
I0521 01:24:02.704715  6984 layer_factory.hpp:77] Creating layer drop2
I0521 01:24:02.704726  6984 net.cpp:106] Creating Layer drop2
I0521 01:24:02.704736  6984 net.cpp:454] drop2 <- ip2
I0521 01:24:02.704751  6984 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:24:02.704792  6984 net.cpp:150] Setting up drop2
I0521 01:24:02.704808  6984 net.cpp:157] Top shape: 520 98 (50960)
I0521 01:24:02.704821  6984 net.cpp:165] Memory required for data: 820919840
I0521 01:24:02.704833  6984 layer_factory.hpp:77] Creating layer ip3
I0521 01:24:02.704849  6984 net.cpp:106] Creating Layer ip3
I0521 01:24:02.704859  6984 net.cpp:454] ip3 <- ip2
I0521 01:24:02.704871  6984 net.cpp:411] ip3 -> ip3
I0521 01:24:02.705080  6984 net.cpp:150] Setting up ip3
I0521 01:24:02.705093  6984 net.cpp:157] Top shape: 520 11 (5720)
I0521 01:24:02.705103  6984 net.cpp:165] Memory required for data: 820942720
I0521 01:24:02.705119  6984 layer_factory.hpp:77] Creating layer drop3
I0521 01:24:02.705132  6984 net.cpp:106] Creating Layer drop3
I0521 01:24:02.705142  6984 net.cpp:454] drop3 <- ip3
I0521 01:24:02.705153  6984 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:24:02.705193  6984 net.cpp:150] Setting up drop3
I0521 01:24:02.705205  6984 net.cpp:157] Top shape: 520 11 (5720)
I0521 01:24:02.705215  6984 net.cpp:165] Memory required for data: 820965600
I0521 01:24:02.705225  6984 layer_factory.hpp:77] Creating layer loss
I0521 01:24:02.705245  6984 net.cpp:106] Creating Layer loss
I0521 01:24:02.705255  6984 net.cpp:454] loss <- ip3
I0521 01:24:02.705265  6984 net.cpp:454] loss <- label
I0521 01:24:02.705278  6984 net.cpp:411] loss -> loss
I0521 01:24:02.705296  6984 layer_factory.hpp:77] Creating layer loss
I0521 01:24:02.705945  6984 net.cpp:150] Setting up loss
I0521 01:24:02.705966  6984 net.cpp:157] Top shape: (1)
I0521 01:24:02.705979  6984 net.cpp:160]     with loss weight 1
I0521 01:24:02.706022  6984 net.cpp:165] Memory required for data: 820965604
I0521 01:24:02.706033  6984 net.cpp:226] loss needs backward computation.
I0521 01:24:02.706044  6984 net.cpp:226] drop3 needs backward computation.
I0521 01:24:02.706051  6984 net.cpp:226] ip3 needs backward computation.
I0521 01:24:02.706063  6984 net.cpp:226] drop2 needs backward computation.
I0521 01:24:02.706073  6984 net.cpp:226] relu6 needs backward computation.
I0521 01:24:02.706082  6984 net.cpp:226] ip2 needs backward computation.
I0521 01:24:02.706094  6984 net.cpp:226] drop1 needs backward computation.
I0521 01:24:02.706102  6984 net.cpp:226] relu5 needs backward computation.
I0521 01:24:02.706112  6984 net.cpp:226] ip1 needs backward computation.
I0521 01:24:02.706122  6984 net.cpp:226] pool4 needs backward computation.
I0521 01:24:02.706132  6984 net.cpp:226] relu4 needs backward computation.
I0521 01:24:02.706142  6984 net.cpp:226] conv4 needs backward computation.
I0521 01:24:02.706153  6984 net.cpp:226] pool3 needs backward computation.
I0521 01:24:02.706172  6984 net.cpp:226] relu3 needs backward computation.
I0521 01:24:02.706181  6984 net.cpp:226] conv3 needs backward computation.
I0521 01:24:02.706192  6984 net.cpp:226] pool2 needs backward computation.
I0521 01:24:02.706202  6984 net.cpp:226] relu2 needs backward computation.
I0521 01:24:02.706212  6984 net.cpp:226] conv2 needs backward computation.
I0521 01:24:02.706223  6984 net.cpp:226] pool1 needs backward computation.
I0521 01:24:02.706233  6984 net.cpp:226] relu1 needs backward computation.
I0521 01:24:02.706243  6984 net.cpp:226] conv1 needs backward computation.
I0521 01:24:02.706254  6984 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:24:02.706264  6984 net.cpp:270] This network produces output loss
I0521 01:24:02.706289  6984 net.cpp:283] Network initialization done.
I0521 01:24:02.707849  6984 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699.prototxt
I0521 01:24:02.707919  6984 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 01:24:02.708273  6984 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 520
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 01:24:02.708462  6984 layer_factory.hpp:77] Creating layer data_hdf5
I0521 01:24:02.708477  6984 net.cpp:106] Creating Layer data_hdf5
I0521 01:24:02.708489  6984 net.cpp:411] data_hdf5 -> data
I0521 01:24:02.708506  6984 net.cpp:411] data_hdf5 -> label
I0521 01:24:02.708523  6984 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 01:24:02.709767  6984 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 01:24:24.017379  6984 net.cpp:150] Setting up data_hdf5
I0521 01:24:24.017541  6984 net.cpp:157] Top shape: 520 1 127 50 (3302000)
I0521 01:24:24.017556  6984 net.cpp:157] Top shape: 520 (520)
I0521 01:24:24.017568  6984 net.cpp:165] Memory required for data: 13210080
I0521 01:24:24.017582  6984 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 01:24:24.017611  6984 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 01:24:24.017621  6984 net.cpp:454] label_data_hdf5_1_split <- label
I0521 01:24:24.017637  6984 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 01:24:24.017657  6984 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 01:24:24.017731  6984 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 01:24:24.017745  6984 net.cpp:157] Top shape: 520 (520)
I0521 01:24:24.017756  6984 net.cpp:157] Top shape: 520 (520)
I0521 01:24:24.017766  6984 net.cpp:165] Memory required for data: 13214240
I0521 01:24:24.017776  6984 layer_factory.hpp:77] Creating layer conv1
I0521 01:24:24.017799  6984 net.cpp:106] Creating Layer conv1
I0521 01:24:24.017809  6984 net.cpp:454] conv1 <- data
I0521 01:24:24.017824  6984 net.cpp:411] conv1 -> conv1
I0521 01:24:24.019773  6984 net.cpp:150] Setting up conv1
I0521 01:24:24.019796  6984 net.cpp:157] Top shape: 520 12 120 48 (35942400)
I0521 01:24:24.019809  6984 net.cpp:165] Memory required for data: 156983840
I0521 01:24:24.019830  6984 layer_factory.hpp:77] Creating layer relu1
I0521 01:24:24.019845  6984 net.cpp:106] Creating Layer relu1
I0521 01:24:24.019855  6984 net.cpp:454] relu1 <- conv1
I0521 01:24:24.019867  6984 net.cpp:397] relu1 -> conv1 (in-place)
I0521 01:24:24.020364  6984 net.cpp:150] Setting up relu1
I0521 01:24:24.020380  6984 net.cpp:157] Top shape: 520 12 120 48 (35942400)
I0521 01:24:24.020391  6984 net.cpp:165] Memory required for data: 300753440
I0521 01:24:24.020401  6984 layer_factory.hpp:77] Creating layer pool1
I0521 01:24:24.020417  6984 net.cpp:106] Creating Layer pool1
I0521 01:24:24.020427  6984 net.cpp:454] pool1 <- conv1
I0521 01:24:24.020442  6984 net.cpp:411] pool1 -> pool1
I0521 01:24:24.020516  6984 net.cpp:150] Setting up pool1
I0521 01:24:24.020529  6984 net.cpp:157] Top shape: 520 12 60 48 (17971200)
I0521 01:24:24.020539  6984 net.cpp:165] Memory required for data: 372638240
I0521 01:24:24.020550  6984 layer_factory.hpp:77] Creating layer conv2
I0521 01:24:24.020567  6984 net.cpp:106] Creating Layer conv2
I0521 01:24:24.020578  6984 net.cpp:454] conv2 <- pool1
I0521 01:24:24.020593  6984 net.cpp:411] conv2 -> conv2
I0521 01:24:24.022503  6984 net.cpp:150] Setting up conv2
I0521 01:24:24.022526  6984 net.cpp:157] Top shape: 520 20 54 46 (25833600)
I0521 01:24:24.022538  6984 net.cpp:165] Memory required for data: 475972640
I0521 01:24:24.022555  6984 layer_factory.hpp:77] Creating layer relu2
I0521 01:24:24.022569  6984 net.cpp:106] Creating Layer relu2
I0521 01:24:24.022579  6984 net.cpp:454] relu2 <- conv2
I0521 01:24:24.022593  6984 net.cpp:397] relu2 -> conv2 (in-place)
I0521 01:24:24.022923  6984 net.cpp:150] Setting up relu2
I0521 01:24:24.022938  6984 net.cpp:157] Top shape: 520 20 54 46 (25833600)
I0521 01:24:24.022948  6984 net.cpp:165] Memory required for data: 579307040
I0521 01:24:24.022959  6984 layer_factory.hpp:77] Creating layer pool2
I0521 01:24:24.022972  6984 net.cpp:106] Creating Layer pool2
I0521 01:24:24.022981  6984 net.cpp:454] pool2 <- conv2
I0521 01:24:24.022994  6984 net.cpp:411] pool2 -> pool2
I0521 01:24:24.023066  6984 net.cpp:150] Setting up pool2
I0521 01:24:24.023078  6984 net.cpp:157] Top shape: 520 20 27 46 (12916800)
I0521 01:24:24.023088  6984 net.cpp:165] Memory required for data: 630974240
I0521 01:24:24.023097  6984 layer_factory.hpp:77] Creating layer conv3
I0521 01:24:24.023116  6984 net.cpp:106] Creating Layer conv3
I0521 01:24:24.023126  6984 net.cpp:454] conv3 <- pool2
I0521 01:24:24.023140  6984 net.cpp:411] conv3 -> conv3
I0521 01:24:24.025112  6984 net.cpp:150] Setting up conv3
I0521 01:24:24.025136  6984 net.cpp:157] Top shape: 520 28 22 44 (14094080)
I0521 01:24:24.025146  6984 net.cpp:165] Memory required for data: 687350560
I0521 01:24:24.025178  6984 layer_factory.hpp:77] Creating layer relu3
I0521 01:24:24.025192  6984 net.cpp:106] Creating Layer relu3
I0521 01:24:24.025202  6984 net.cpp:454] relu3 <- conv3
I0521 01:24:24.025215  6984 net.cpp:397] relu3 -> conv3 (in-place)
I0521 01:24:24.025687  6984 net.cpp:150] Setting up relu3
I0521 01:24:24.025703  6984 net.cpp:157] Top shape: 520 28 22 44 (14094080)
I0521 01:24:24.025713  6984 net.cpp:165] Memory required for data: 743726880
I0521 01:24:24.025723  6984 layer_factory.hpp:77] Creating layer pool3
I0521 01:24:24.025737  6984 net.cpp:106] Creating Layer pool3
I0521 01:24:24.025746  6984 net.cpp:454] pool3 <- conv3
I0521 01:24:24.025759  6984 net.cpp:411] pool3 -> pool3
I0521 01:24:24.025831  6984 net.cpp:150] Setting up pool3
I0521 01:24:24.025845  6984 net.cpp:157] Top shape: 520 28 11 44 (7047040)
I0521 01:24:24.025854  6984 net.cpp:165] Memory required for data: 771915040
I0521 01:24:24.025862  6984 layer_factory.hpp:77] Creating layer conv4
I0521 01:24:24.025879  6984 net.cpp:106] Creating Layer conv4
I0521 01:24:24.025890  6984 net.cpp:454] conv4 <- pool3
I0521 01:24:24.025904  6984 net.cpp:411] conv4 -> conv4
I0521 01:24:24.027963  6984 net.cpp:150] Setting up conv4
I0521 01:24:24.027987  6984 net.cpp:157] Top shape: 520 36 6 42 (4717440)
I0521 01:24:24.027998  6984 net.cpp:165] Memory required for data: 790784800
I0521 01:24:24.028013  6984 layer_factory.hpp:77] Creating layer relu4
I0521 01:24:24.028028  6984 net.cpp:106] Creating Layer relu4
I0521 01:24:24.028038  6984 net.cpp:454] relu4 <- conv4
I0521 01:24:24.028049  6984 net.cpp:397] relu4 -> conv4 (in-place)
I0521 01:24:24.028520  6984 net.cpp:150] Setting up relu4
I0521 01:24:24.028535  6984 net.cpp:157] Top shape: 520 36 6 42 (4717440)
I0521 01:24:24.028545  6984 net.cpp:165] Memory required for data: 809654560
I0521 01:24:24.028556  6984 layer_factory.hpp:77] Creating layer pool4
I0521 01:24:24.028569  6984 net.cpp:106] Creating Layer pool4
I0521 01:24:24.028579  6984 net.cpp:454] pool4 <- conv4
I0521 01:24:24.028594  6984 net.cpp:411] pool4 -> pool4
I0521 01:24:24.028664  6984 net.cpp:150] Setting up pool4
I0521 01:24:24.028678  6984 net.cpp:157] Top shape: 520 36 3 42 (2358720)
I0521 01:24:24.028687  6984 net.cpp:165] Memory required for data: 819089440
I0521 01:24:24.028697  6984 layer_factory.hpp:77] Creating layer ip1
I0521 01:24:24.028712  6984 net.cpp:106] Creating Layer ip1
I0521 01:24:24.028722  6984 net.cpp:454] ip1 <- pool4
I0521 01:24:24.028735  6984 net.cpp:411] ip1 -> ip1
I0521 01:24:24.044205  6984 net.cpp:150] Setting up ip1
I0521 01:24:24.044234  6984 net.cpp:157] Top shape: 520 196 (101920)
I0521 01:24:24.044250  6984 net.cpp:165] Memory required for data: 819497120
I0521 01:24:24.044272  6984 layer_factory.hpp:77] Creating layer relu5
I0521 01:24:24.044287  6984 net.cpp:106] Creating Layer relu5
I0521 01:24:24.044298  6984 net.cpp:454] relu5 <- ip1
I0521 01:24:24.044315  6984 net.cpp:397] relu5 -> ip1 (in-place)
I0521 01:24:24.044662  6984 net.cpp:150] Setting up relu5
I0521 01:24:24.044677  6984 net.cpp:157] Top shape: 520 196 (101920)
I0521 01:24:24.044687  6984 net.cpp:165] Memory required for data: 819904800
I0521 01:24:24.044697  6984 layer_factory.hpp:77] Creating layer drop1
I0521 01:24:24.044714  6984 net.cpp:106] Creating Layer drop1
I0521 01:24:24.044724  6984 net.cpp:454] drop1 <- ip1
I0521 01:24:24.044739  6984 net.cpp:397] drop1 -> ip1 (in-place)
I0521 01:24:24.044782  6984 net.cpp:150] Setting up drop1
I0521 01:24:24.044795  6984 net.cpp:157] Top shape: 520 196 (101920)
I0521 01:24:24.044809  6984 net.cpp:165] Memory required for data: 820312480
I0521 01:24:24.044818  6984 layer_factory.hpp:77] Creating layer ip2
I0521 01:24:24.044833  6984 net.cpp:106] Creating Layer ip2
I0521 01:24:24.044843  6984 net.cpp:454] ip2 <- ip1
I0521 01:24:24.044857  6984 net.cpp:411] ip2 -> ip2
I0521 01:24:24.045336  6984 net.cpp:150] Setting up ip2
I0521 01:24:24.045351  6984 net.cpp:157] Top shape: 520 98 (50960)
I0521 01:24:24.045359  6984 net.cpp:165] Memory required for data: 820516320
I0521 01:24:24.045388  6984 layer_factory.hpp:77] Creating layer relu6
I0521 01:24:24.045402  6984 net.cpp:106] Creating Layer relu6
I0521 01:24:24.045411  6984 net.cpp:454] relu6 <- ip2
I0521 01:24:24.045423  6984 net.cpp:397] relu6 -> ip2 (in-place)
I0521 01:24:24.045955  6984 net.cpp:150] Setting up relu6
I0521 01:24:24.045977  6984 net.cpp:157] Top shape: 520 98 (50960)
I0521 01:24:24.045987  6984 net.cpp:165] Memory required for data: 820720160
I0521 01:24:24.045999  6984 layer_factory.hpp:77] Creating layer drop2
I0521 01:24:24.046012  6984 net.cpp:106] Creating Layer drop2
I0521 01:24:24.046022  6984 net.cpp:454] drop2 <- ip2
I0521 01:24:24.046036  6984 net.cpp:397] drop2 -> ip2 (in-place)
I0521 01:24:24.046079  6984 net.cpp:150] Setting up drop2
I0521 01:24:24.046092  6984 net.cpp:157] Top shape: 520 98 (50960)
I0521 01:24:24.046103  6984 net.cpp:165] Memory required for data: 820924000
I0521 01:24:24.046113  6984 layer_factory.hpp:77] Creating layer ip3
I0521 01:24:24.046126  6984 net.cpp:106] Creating Layer ip3
I0521 01:24:24.046136  6984 net.cpp:454] ip3 <- ip2
I0521 01:24:24.046150  6984 net.cpp:411] ip3 -> ip3
I0521 01:24:24.046372  6984 net.cpp:150] Setting up ip3
I0521 01:24:24.046386  6984 net.cpp:157] Top shape: 520 11 (5720)
I0521 01:24:24.046396  6984 net.cpp:165] Memory required for data: 820946880
I0521 01:24:24.046411  6984 layer_factory.hpp:77] Creating layer drop3
I0521 01:24:24.046423  6984 net.cpp:106] Creating Layer drop3
I0521 01:24:24.046432  6984 net.cpp:454] drop3 <- ip3
I0521 01:24:24.046445  6984 net.cpp:397] drop3 -> ip3 (in-place)
I0521 01:24:24.046486  6984 net.cpp:150] Setting up drop3
I0521 01:24:24.046499  6984 net.cpp:157] Top shape: 520 11 (5720)
I0521 01:24:24.046509  6984 net.cpp:165] Memory required for data: 820969760
I0521 01:24:24.046519  6984 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 01:24:24.046531  6984 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 01:24:24.046541  6984 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 01:24:24.046555  6984 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 01:24:24.046569  6984 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 01:24:24.046643  6984 net.cpp:150] Setting up ip3_drop3_0_split
I0521 01:24:24.046654  6984 net.cpp:157] Top shape: 520 11 (5720)
I0521 01:24:24.046668  6984 net.cpp:157] Top shape: 520 11 (5720)
I0521 01:24:24.046677  6984 net.cpp:165] Memory required for data: 821015520
I0521 01:24:24.046685  6984 layer_factory.hpp:77] Creating layer accuracy
I0521 01:24:24.046707  6984 net.cpp:106] Creating Layer accuracy
I0521 01:24:24.046717  6984 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 01:24:24.046728  6984 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 01:24:24.046742  6984 net.cpp:411] accuracy -> accuracy
I0521 01:24:24.046766  6984 net.cpp:150] Setting up accuracy
I0521 01:24:24.046779  6984 net.cpp:157] Top shape: (1)
I0521 01:24:24.046788  6984 net.cpp:165] Memory required for data: 821015524
I0521 01:24:24.046799  6984 layer_factory.hpp:77] Creating layer loss
I0521 01:24:24.046813  6984 net.cpp:106] Creating Layer loss
I0521 01:24:24.046823  6984 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 01:24:24.046834  6984 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 01:24:24.046847  6984 net.cpp:411] loss -> loss
I0521 01:24:24.046865  6984 layer_factory.hpp:77] Creating layer loss
I0521 01:24:24.047363  6984 net.cpp:150] Setting up loss
I0521 01:24:24.047377  6984 net.cpp:157] Top shape: (1)
I0521 01:24:24.047386  6984 net.cpp:160]     with loss weight 1
I0521 01:24:24.047405  6984 net.cpp:165] Memory required for data: 821015528
I0521 01:24:24.047415  6984 net.cpp:226] loss needs backward computation.
I0521 01:24:24.047426  6984 net.cpp:228] accuracy does not need backward computation.
I0521 01:24:24.047437  6984 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 01:24:24.047447  6984 net.cpp:226] drop3 needs backward computation.
I0521 01:24:24.047458  6984 net.cpp:226] ip3 needs backward computation.
I0521 01:24:24.047469  6984 net.cpp:226] drop2 needs backward computation.
I0521 01:24:24.047487  6984 net.cpp:226] relu6 needs backward computation.
I0521 01:24:24.047497  6984 net.cpp:226] ip2 needs backward computation.
I0521 01:24:24.047508  6984 net.cpp:226] drop1 needs backward computation.
I0521 01:24:24.047516  6984 net.cpp:226] relu5 needs backward computation.
I0521 01:24:24.047526  6984 net.cpp:226] ip1 needs backward computation.
I0521 01:24:24.047536  6984 net.cpp:226] pool4 needs backward computation.
I0521 01:24:24.047547  6984 net.cpp:226] relu4 needs backward computation.
I0521 01:24:24.047557  6984 net.cpp:226] conv4 needs backward computation.
I0521 01:24:24.047569  6984 net.cpp:226] pool3 needs backward computation.
I0521 01:24:24.047579  6984 net.cpp:226] relu3 needs backward computation.
I0521 01:24:24.047588  6984 net.cpp:226] conv3 needs backward computation.
I0521 01:24:24.047598  6984 net.cpp:226] pool2 needs backward computation.
I0521 01:24:24.047608  6984 net.cpp:226] relu2 needs backward computation.
I0521 01:24:24.047618  6984 net.cpp:226] conv2 needs backward computation.
I0521 01:24:24.047628  6984 net.cpp:226] pool1 needs backward computation.
I0521 01:24:24.047639  6984 net.cpp:226] relu1 needs backward computation.
I0521 01:24:24.047649  6984 net.cpp:226] conv1 needs backward computation.
I0521 01:24:24.047660  6984 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 01:24:24.047672  6984 net.cpp:228] data_hdf5 does not need backward computation.
I0521 01:24:24.047683  6984 net.cpp:270] This network produces output accuracy
I0521 01:24:24.047693  6984 net.cpp:270] This network produces output loss
I0521 01:24:24.047720  6984 net.cpp:283] Network initialization done.
I0521 01:24:24.047853  6984 solver.cpp:60] Solver scaffolding done.
I0521 01:24:24.048982  6984 caffe.cpp:212] Starting Optimization
I0521 01:24:24.049000  6984 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 01:24:24.049010  6984 solver.cpp:289] Learning Rate Policy: fixed
I0521 01:24:24.050225  6984 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 01:25:10.034021  6984 solver.cpp:409]     Test net output #0: accuracy = 0.105823
I0521 01:25:10.034188  6984 solver.cpp:409]     Test net output #1: loss = 2.39697 (* 1 = 2.39697 loss)
I0521 01:25:10.136071  6984 solver.cpp:237] Iteration 0, loss = 2.39586
I0521 01:25:10.136106  6984 solver.cpp:253]     Train net output #0: loss = 2.39586 (* 1 = 2.39586 loss)
I0521 01:25:10.136124  6984 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 01:25:17.989279  6984 solver.cpp:237] Iteration 28, loss = 2.37619
I0521 01:25:17.989327  6984 solver.cpp:253]     Train net output #0: loss = 2.37619 (* 1 = 2.37619 loss)
I0521 01:25:17.989342  6984 sgd_solver.cpp:106] Iteration 28, lr = 0.0025
I0521 01:25:25.836895  6984 solver.cpp:237] Iteration 56, loss = 2.36652
I0521 01:25:25.836927  6984 solver.cpp:253]     Train net output #0: loss = 2.36652 (* 1 = 2.36652 loss)
I0521 01:25:25.836943  6984 sgd_solver.cpp:106] Iteration 56, lr = 0.0025
I0521 01:25:33.686975  6984 solver.cpp:237] Iteration 84, loss = 2.35493
I0521 01:25:33.687007  6984 solver.cpp:253]     Train net output #0: loss = 2.35493 (* 1 = 2.35493 loss)
I0521 01:25:33.687023  6984 sgd_solver.cpp:106] Iteration 84, lr = 0.0025
I0521 01:25:41.544239  6984 solver.cpp:237] Iteration 112, loss = 2.33075
I0521 01:25:41.544374  6984 solver.cpp:253]     Train net output #0: loss = 2.33075 (* 1 = 2.33075 loss)
I0521 01:25:41.544389  6984 sgd_solver.cpp:106] Iteration 112, lr = 0.0025
I0521 01:25:49.399004  6984 solver.cpp:237] Iteration 140, loss = 2.32426
I0521 01:25:49.399036  6984 solver.cpp:253]     Train net output #0: loss = 2.32426 (* 1 = 2.32426 loss)
I0521 01:25:49.399054  6984 sgd_solver.cpp:106] Iteration 140, lr = 0.0025
I0521 01:25:57.254539  6984 solver.cpp:237] Iteration 168, loss = 2.33074
I0521 01:25:57.254572  6984 solver.cpp:253]     Train net output #0: loss = 2.33074 (* 1 = 2.33074 loss)
I0521 01:25:57.254590  6984 sgd_solver.cpp:106] Iteration 168, lr = 0.0025
I0521 01:26:27.232347  6984 solver.cpp:237] Iteration 196, loss = 2.30247
I0521 01:26:27.232507  6984 solver.cpp:253]     Train net output #0: loss = 2.30247 (* 1 = 2.30247 loss)
I0521 01:26:27.232522  6984 sgd_solver.cpp:106] Iteration 196, lr = 0.0025
I0521 01:26:35.092492  6984 solver.cpp:237] Iteration 224, loss = 2.29934
I0521 01:26:35.092527  6984 solver.cpp:253]     Train net output #0: loss = 2.29934 (* 1 = 2.29934 loss)
I0521 01:26:35.092548  6984 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 01:26:42.950973  6984 solver.cpp:237] Iteration 252, loss = 2.27777
I0521 01:26:42.951007  6984 solver.cpp:253]     Train net output #0: loss = 2.27777 (* 1 = 2.27777 loss)
I0521 01:26:42.951025  6984 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 01:26:50.807595  6984 solver.cpp:237] Iteration 280, loss = 2.25903
I0521 01:26:50.807627  6984 solver.cpp:253]     Train net output #0: loss = 2.25903 (* 1 = 2.25903 loss)
I0521 01:26:50.807643  6984 sgd_solver.cpp:106] Iteration 280, lr = 0.0025
I0521 01:26:52.770671  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_288.caffemodel
I0521 01:26:53.008983  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_288.solverstate
I0521 01:26:58.729120  6984 solver.cpp:237] Iteration 308, loss = 2.18932
I0521 01:26:58.729276  6984 solver.cpp:253]     Train net output #0: loss = 2.18932 (* 1 = 2.18932 loss)
I0521 01:26:58.729290  6984 sgd_solver.cpp:106] Iteration 308, lr = 0.0025
I0521 01:27:06.579887  6984 solver.cpp:237] Iteration 336, loss = 2.19461
I0521 01:27:06.579919  6984 solver.cpp:253]     Train net output #0: loss = 2.19461 (* 1 = 2.19461 loss)
I0521 01:27:06.579937  6984 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 01:27:14.433997  6984 solver.cpp:237] Iteration 364, loss = 2.18252
I0521 01:27:14.434031  6984 solver.cpp:253]     Train net output #0: loss = 2.18252 (* 1 = 2.18252 loss)
I0521 01:27:14.434046  6984 sgd_solver.cpp:106] Iteration 364, lr = 0.0025
I0521 01:27:44.401232  6984 solver.cpp:237] Iteration 392, loss = 2.08923
I0521 01:27:44.401387  6984 solver.cpp:253]     Train net output #0: loss = 2.08923 (* 1 = 2.08923 loss)
I0521 01:27:44.401401  6984 sgd_solver.cpp:106] Iteration 392, lr = 0.0025
I0521 01:27:52.257167  6984 solver.cpp:237] Iteration 420, loss = 2.04539
I0521 01:27:52.257205  6984 solver.cpp:253]     Train net output #0: loss = 2.04539 (* 1 = 2.04539 loss)
I0521 01:27:52.257222  6984 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 01:28:00.111215  6984 solver.cpp:237] Iteration 448, loss = 2.04706
I0521 01:28:00.111249  6984 solver.cpp:253]     Train net output #0: loss = 2.04706 (* 1 = 2.04706 loss)
I0521 01:28:00.111266  6984 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 01:28:07.973858  6984 solver.cpp:237] Iteration 476, loss = 2.00999
I0521 01:28:07.973891  6984 solver.cpp:253]     Train net output #0: loss = 2.00999 (* 1 = 2.00999 loss)
I0521 01:28:07.973907  6984 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0521 01:28:15.829969  6984 solver.cpp:237] Iteration 504, loss = 1.94614
I0521 01:28:15.830116  6984 solver.cpp:253]     Train net output #0: loss = 1.94614 (* 1 = 1.94614 loss)
I0521 01:28:15.830129  6984 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 01:28:23.688019  6984 solver.cpp:237] Iteration 532, loss = 1.94937
I0521 01:28:23.688050  6984 solver.cpp:253]     Train net output #0: loss = 1.94937 (* 1 = 1.94937 loss)
I0521 01:28:23.688069  6984 sgd_solver.cpp:106] Iteration 532, lr = 0.0025
I0521 01:28:31.544857  6984 solver.cpp:237] Iteration 560, loss = 1.92026
I0521 01:28:31.544890  6984 solver.cpp:253]     Train net output #0: loss = 1.92026 (* 1 = 1.92026 loss)
I0521 01:28:31.544906  6984 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 01:28:35.754884  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_576.caffemodel
I0521 01:28:35.990411  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_576.solverstate
I0521 01:28:36.015341  6984 solver.cpp:341] Iteration 576, Testing net (#0)
I0521 01:29:21.083714  6984 solver.cpp:409]     Test net output #0: accuracy = 0.566493
I0521 01:29:21.083873  6984 solver.cpp:409]     Test net output #1: loss = 1.63122 (* 1 = 1.63122 loss)
I0521 01:29:46.693426  6984 solver.cpp:237] Iteration 588, loss = 1.94826
I0521 01:29:46.693476  6984 solver.cpp:253]     Train net output #0: loss = 1.94826 (* 1 = 1.94826 loss)
I0521 01:29:46.693492  6984 sgd_solver.cpp:106] Iteration 588, lr = 0.0025
I0521 01:29:54.537760  6984 solver.cpp:237] Iteration 616, loss = 1.91913
I0521 01:29:54.537911  6984 solver.cpp:253]     Train net output #0: loss = 1.91913 (* 1 = 1.91913 loss)
I0521 01:29:54.537925  6984 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0521 01:30:02.385135  6984 solver.cpp:237] Iteration 644, loss = 1.88046
I0521 01:30:02.385166  6984 solver.cpp:253]     Train net output #0: loss = 1.88046 (* 1 = 1.88046 loss)
I0521 01:30:02.385184  6984 sgd_solver.cpp:106] Iteration 644, lr = 0.0025
I0521 01:30:10.234211  6984 solver.cpp:237] Iteration 672, loss = 1.96921
I0521 01:30:10.234244  6984 solver.cpp:253]     Train net output #0: loss = 1.96921 (* 1 = 1.96921 loss)
I0521 01:30:10.234261  6984 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 01:30:18.082398  6984 solver.cpp:237] Iteration 700, loss = 1.82196
I0521 01:30:18.082442  6984 solver.cpp:253]     Train net output #0: loss = 1.82196 (* 1 = 1.82196 loss)
I0521 01:30:18.082458  6984 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0521 01:30:25.931187  6984 solver.cpp:237] Iteration 728, loss = 1.81467
I0521 01:30:25.931326  6984 solver.cpp:253]     Train net output #0: loss = 1.81467 (* 1 = 1.81467 loss)
I0521 01:30:25.931340  6984 sgd_solver.cpp:106] Iteration 728, lr = 0.0025
I0521 01:30:33.778550  6984 solver.cpp:237] Iteration 756, loss = 1.82774
I0521 01:30:33.778583  6984 solver.cpp:253]     Train net output #0: loss = 1.82774 (* 1 = 1.82774 loss)
I0521 01:30:33.778599  6984 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 01:31:03.755487  6984 solver.cpp:237] Iteration 784, loss = 1.80818
I0521 01:31:03.755656  6984 solver.cpp:253]     Train net output #0: loss = 1.80818 (* 1 = 1.80818 loss)
I0521 01:31:03.755671  6984 sgd_solver.cpp:106] Iteration 784, lr = 0.0025
I0521 01:31:11.603266  6984 solver.cpp:237] Iteration 812, loss = 1.79594
I0521 01:31:11.603317  6984 solver.cpp:253]     Train net output #0: loss = 1.79594 (* 1 = 1.79594 loss)
I0521 01:31:11.603330  6984 sgd_solver.cpp:106] Iteration 812, lr = 0.0025
I0521 01:31:19.453166  6984 solver.cpp:237] Iteration 840, loss = 1.79159
I0521 01:31:19.453200  6984 solver.cpp:253]     Train net output #0: loss = 1.79159 (* 1 = 1.79159 loss)
I0521 01:31:19.453217  6984 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 01:31:25.898079  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_864.caffemodel
I0521 01:31:26.134210  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_864.solverstate
I0521 01:31:27.366808  6984 solver.cpp:237] Iteration 868, loss = 1.94675
I0521 01:31:27.366857  6984 solver.cpp:253]     Train net output #0: loss = 1.94675 (* 1 = 1.94675 loss)
I0521 01:31:27.366871  6984 sgd_solver.cpp:106] Iteration 868, lr = 0.0025
I0521 01:31:35.216572  6984 solver.cpp:237] Iteration 896, loss = 1.76829
I0521 01:31:35.216719  6984 solver.cpp:253]     Train net output #0: loss = 1.76829 (* 1 = 1.76829 loss)
I0521 01:31:35.216733  6984 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 01:31:43.064697  6984 solver.cpp:237] Iteration 924, loss = 1.83594
I0521 01:31:43.064729  6984 solver.cpp:253]     Train net output #0: loss = 1.83594 (* 1 = 1.83594 loss)
I0521 01:31:43.064746  6984 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 01:31:50.912958  6984 solver.cpp:237] Iteration 952, loss = 1.89269
I0521 01:31:50.912992  6984 solver.cpp:253]     Train net output #0: loss = 1.89269 (* 1 = 1.89269 loss)
I0521 01:31:50.913008  6984 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0521 01:32:20.895623  6984 solver.cpp:237] Iteration 980, loss = 1.75048
I0521 01:32:20.895787  6984 solver.cpp:253]     Train net output #0: loss = 1.75048 (* 1 = 1.75048 loss)
I0521 01:32:20.895800  6984 sgd_solver.cpp:106] Iteration 980, lr = 0.0025
I0521 01:32:28.743641  6984 solver.cpp:237] Iteration 1008, loss = 1.85333
I0521 01:32:28.743681  6984 solver.cpp:253]     Train net output #0: loss = 1.85333 (* 1 = 1.85333 loss)
I0521 01:32:28.743702  6984 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 01:32:36.592253  6984 solver.cpp:237] Iteration 1036, loss = 1.78657
I0521 01:32:36.592286  6984 solver.cpp:253]     Train net output #0: loss = 1.78657 (* 1 = 1.78657 loss)
I0521 01:32:36.592304  6984 sgd_solver.cpp:106] Iteration 1036, lr = 0.0025
I0521 01:32:44.441772  6984 solver.cpp:237] Iteration 1064, loss = 1.87645
I0521 01:32:44.441805  6984 solver.cpp:253]     Train net output #0: loss = 1.87645 (* 1 = 1.87645 loss)
I0521 01:32:44.441822  6984 sgd_solver.cpp:106] Iteration 1064, lr = 0.0025
I0521 01:32:52.289412  6984 solver.cpp:237] Iteration 1092, loss = 1.77952
I0521 01:32:52.289556  6984 solver.cpp:253]     Train net output #0: loss = 1.77952 (* 1 = 1.77952 loss)
I0521 01:32:52.289569  6984 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0521 01:33:00.136910  6984 solver.cpp:237] Iteration 1120, loss = 1.73386
I0521 01:33:00.136945  6984 solver.cpp:253]     Train net output #0: loss = 1.73386 (* 1 = 1.73386 loss)
I0521 01:33:00.136960  6984 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 01:33:07.986045  6984 solver.cpp:237] Iteration 1148, loss = 1.79176
I0521 01:33:07.986078  6984 solver.cpp:253]     Train net output #0: loss = 1.79176 (* 1 = 1.79176 loss)
I0521 01:33:07.986094  6984 sgd_solver.cpp:106] Iteration 1148, lr = 0.0025
I0521 01:33:08.825163  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_1152.caffemodel
I0521 01:33:09.062039  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_1152.solverstate
I0521 01:33:09.090152  6984 solver.cpp:341] Iteration 1152, Testing net (#0)
I0521 01:34:15.093932  6984 solver.cpp:409]     Test net output #0: accuracy = 0.623004
I0521 01:34:15.094102  6984 solver.cpp:409]     Test net output #1: loss = 1.32426 (* 1 = 1.32426 loss)
I0521 01:34:44.029855  6984 solver.cpp:237] Iteration 1176, loss = 1.81124
I0521 01:34:44.029906  6984 solver.cpp:253]     Train net output #0: loss = 1.81124 (* 1 = 1.81124 loss)
I0521 01:34:44.029920  6984 sgd_solver.cpp:106] Iteration 1176, lr = 0.0025
I0521 01:34:51.875113  6984 solver.cpp:237] Iteration 1204, loss = 1.77604
I0521 01:34:51.875267  6984 solver.cpp:253]     Train net output #0: loss = 1.77604 (* 1 = 1.77604 loss)
I0521 01:34:51.875283  6984 sgd_solver.cpp:106] Iteration 1204, lr = 0.0025
I0521 01:34:59.724149  6984 solver.cpp:237] Iteration 1232, loss = 1.66252
I0521 01:34:59.724181  6984 solver.cpp:253]     Train net output #0: loss = 1.66252 (* 1 = 1.66252 loss)
I0521 01:34:59.724198  6984 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 01:35:07.571072  6984 solver.cpp:237] Iteration 1260, loss = 1.71243
I0521 01:35:07.571107  6984 solver.cpp:253]     Train net output #0: loss = 1.71243 (* 1 = 1.71243 loss)
I0521 01:35:07.571122  6984 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 01:35:15.419978  6984 solver.cpp:237] Iteration 1288, loss = 1.67038
I0521 01:35:15.420011  6984 solver.cpp:253]     Train net output #0: loss = 1.67038 (* 1 = 1.67038 loss)
I0521 01:35:15.420027  6984 sgd_solver.cpp:106] Iteration 1288, lr = 0.0025
I0521 01:35:23.267997  6984 solver.cpp:237] Iteration 1316, loss = 1.80339
I0521 01:35:23.268147  6984 solver.cpp:253]     Train net output #0: loss = 1.80339 (* 1 = 1.80339 loss)
I0521 01:35:23.268162  6984 sgd_solver.cpp:106] Iteration 1316, lr = 0.0025
I0521 01:35:31.113499  6984 solver.cpp:237] Iteration 1344, loss = 1.78215
I0521 01:35:31.113533  6984 solver.cpp:253]     Train net output #0: loss = 1.78215 (* 1 = 1.78215 loss)
I0521 01:35:31.113550  6984 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 01:36:01.080538  6984 solver.cpp:237] Iteration 1372, loss = 1.66062
I0521 01:36:01.080698  6984 solver.cpp:253]     Train net output #0: loss = 1.66062 (* 1 = 1.66062 loss)
I0521 01:36:01.080711  6984 sgd_solver.cpp:106] Iteration 1372, lr = 0.0025
I0521 01:36:08.928854  6984 solver.cpp:237] Iteration 1400, loss = 1.74357
I0521 01:36:08.928894  6984 solver.cpp:253]     Train net output #0: loss = 1.74357 (* 1 = 1.74357 loss)
I0521 01:36:08.928915  6984 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0521 01:36:16.774405  6984 solver.cpp:237] Iteration 1428, loss = 1.72413
I0521 01:36:16.774437  6984 solver.cpp:253]     Train net output #0: loss = 1.72413 (* 1 = 1.72413 loss)
I0521 01:36:16.774454  6984 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 01:36:19.858901  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_1440.caffemodel
I0521 01:36:20.095633  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_1440.solverstate
I0521 01:36:24.694067  6984 solver.cpp:237] Iteration 1456, loss = 1.80759
I0521 01:36:24.694115  6984 solver.cpp:253]     Train net output #0: loss = 1.80759 (* 1 = 1.80759 loss)
I0521 01:36:24.694133  6984 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 01:36:32.539676  6984 solver.cpp:237] Iteration 1484, loss = 1.7178
I0521 01:36:32.539847  6984 solver.cpp:253]     Train net output #0: loss = 1.7178 (* 1 = 1.7178 loss)
I0521 01:36:32.539861  6984 sgd_solver.cpp:106] Iteration 1484, lr = 0.0025
I0521 01:36:40.387768  6984 solver.cpp:237] Iteration 1512, loss = 1.65537
I0521 01:36:40.387801  6984 solver.cpp:253]     Train net output #0: loss = 1.65537 (* 1 = 1.65537 loss)
I0521 01:36:40.387819  6984 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 01:37:10.356056  6984 solver.cpp:237] Iteration 1540, loss = 1.70935
I0521 01:37:10.356217  6984 solver.cpp:253]     Train net output #0: loss = 1.70935 (* 1 = 1.70935 loss)
I0521 01:37:10.356231  6984 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 01:37:18.200181  6984 solver.cpp:237] Iteration 1568, loss = 1.69509
I0521 01:37:18.200213  6984 solver.cpp:253]     Train net output #0: loss = 1.69509 (* 1 = 1.69509 loss)
I0521 01:37:18.200232  6984 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 01:37:26.045485  6984 solver.cpp:237] Iteration 1596, loss = 1.67736
I0521 01:37:26.045531  6984 solver.cpp:253]     Train net output #0: loss = 1.67736 (* 1 = 1.67736 loss)
I0521 01:37:26.045545  6984 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0521 01:37:33.893046  6984 solver.cpp:237] Iteration 1624, loss = 1.68359
I0521 01:37:33.893080  6984 solver.cpp:253]     Train net output #0: loss = 1.68359 (* 1 = 1.68359 loss)
I0521 01:37:33.893096  6984 sgd_solver.cpp:106] Iteration 1624, lr = 0.0025
I0521 01:37:41.740353  6984 solver.cpp:237] Iteration 1652, loss = 1.71601
I0521 01:37:41.740494  6984 solver.cpp:253]     Train net output #0: loss = 1.71601 (* 1 = 1.71601 loss)
I0521 01:37:41.740506  6984 sgd_solver.cpp:106] Iteration 1652, lr = 0.0025
I0521 01:37:49.588517  6984 solver.cpp:237] Iteration 1680, loss = 1.68753
I0521 01:37:49.588551  6984 solver.cpp:253]     Train net output #0: loss = 1.68753 (* 1 = 1.68753 loss)
I0521 01:37:49.588572  6984 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 01:37:57.434664  6984 solver.cpp:237] Iteration 1708, loss = 1.76855
I0521 01:37:57.434698  6984 solver.cpp:253]     Train net output #0: loss = 1.76855 (* 1 = 1.76855 loss)
I0521 01:37:57.434715  6984 sgd_solver.cpp:106] Iteration 1708, lr = 0.0025
I0521 01:38:02.759999  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_1728.caffemodel
I0521 01:38:02.995061  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_1728.solverstate
I0521 01:38:03.021338  6984 solver.cpp:341] Iteration 1728, Testing net (#0)
I0521 01:38:47.777017  6984 solver.cpp:409]     Test net output #0: accuracy = 0.664396
I0521 01:38:47.777186  6984 solver.cpp:409]     Test net output #1: loss = 1.15408 (* 1 = 1.15408 loss)
I0521 01:39:12.201151  6984 solver.cpp:237] Iteration 1736, loss = 1.76511
I0521 01:39:12.201201  6984 solver.cpp:253]     Train net output #0: loss = 1.76511 (* 1 = 1.76511 loss)
I0521 01:39:12.201215  6984 sgd_solver.cpp:106] Iteration 1736, lr = 0.0025
I0521 01:39:20.048130  6984 solver.cpp:237] Iteration 1764, loss = 1.70602
I0521 01:39:20.048288  6984 solver.cpp:253]     Train net output #0: loss = 1.70602 (* 1 = 1.70602 loss)
I0521 01:39:20.048301  6984 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 01:39:27.894095  6984 solver.cpp:237] Iteration 1792, loss = 1.65879
I0521 01:39:27.894136  6984 solver.cpp:253]     Train net output #0: loss = 1.65879 (* 1 = 1.65879 loss)
I0521 01:39:27.894157  6984 sgd_solver.cpp:106] Iteration 1792, lr = 0.0025
I0521 01:39:35.740558  6984 solver.cpp:237] Iteration 1820, loss = 1.66738
I0521 01:39:35.740592  6984 solver.cpp:253]     Train net output #0: loss = 1.66738 (* 1 = 1.66738 loss)
I0521 01:39:35.740607  6984 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0521 01:39:43.587878  6984 solver.cpp:237] Iteration 1848, loss = 1.60919
I0521 01:39:43.587913  6984 solver.cpp:253]     Train net output #0: loss = 1.60919 (* 1 = 1.60919 loss)
I0521 01:39:43.587929  6984 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 01:39:51.435444  6984 solver.cpp:237] Iteration 1876, loss = 1.74633
I0521 01:39:51.435603  6984 solver.cpp:253]     Train net output #0: loss = 1.74633 (* 1 = 1.74633 loss)
I0521 01:39:51.435617  6984 sgd_solver.cpp:106] Iteration 1876, lr = 0.0025
I0521 01:39:59.280655  6984 solver.cpp:237] Iteration 1904, loss = 1.72631
I0521 01:39:59.280688  6984 solver.cpp:253]     Train net output #0: loss = 1.72631 (* 1 = 1.72631 loss)
I0521 01:39:59.280704  6984 sgd_solver.cpp:106] Iteration 1904, lr = 0.0025
I0521 01:40:29.269206  6984 solver.cpp:237] Iteration 1932, loss = 1.7617
I0521 01:40:29.269369  6984 solver.cpp:253]     Train net output #0: loss = 1.7617 (* 1 = 1.7617 loss)
I0521 01:40:29.269383  6984 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0521 01:40:37.120704  6984 solver.cpp:237] Iteration 1960, loss = 1.7573
I0521 01:40:37.120738  6984 solver.cpp:253]     Train net output #0: loss = 1.7573 (* 1 = 1.7573 loss)
I0521 01:40:37.120754  6984 sgd_solver.cpp:106] Iteration 1960, lr = 0.0025
I0521 01:40:44.968900  6984 solver.cpp:237] Iteration 1988, loss = 1.67827
I0521 01:40:44.968940  6984 solver.cpp:253]     Train net output #0: loss = 1.67827 (* 1 = 1.67827 loss)
I0521 01:40:44.968962  6984 sgd_solver.cpp:106] Iteration 1988, lr = 0.0025
I0521 01:40:52.535557  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2016.caffemodel
I0521 01:40:52.788907  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2016.solverstate
I0521 01:40:52.898471  6984 solver.cpp:237] Iteration 2016, loss = 1.65819
I0521 01:40:52.898514  6984 solver.cpp:253]     Train net output #0: loss = 1.65819 (* 1 = 1.65819 loss)
I0521 01:40:52.898529  6984 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0521 01:41:00.750079  6984 solver.cpp:237] Iteration 2044, loss = 1.71908
I0521 01:41:00.750222  6984 solver.cpp:253]     Train net output #0: loss = 1.71908 (* 1 = 1.71908 loss)
I0521 01:41:00.750236  6984 sgd_solver.cpp:106] Iteration 2044, lr = 0.0025
I0521 01:41:08.590219  6984 solver.cpp:237] Iteration 2072, loss = 1.59754
I0521 01:41:08.590257  6984 solver.cpp:253]     Train net output #0: loss = 1.59754 (* 1 = 1.59754 loss)
I0521 01:41:08.590277  6984 sgd_solver.cpp:106] Iteration 2072, lr = 0.0025
I0521 01:41:16.438900  6984 solver.cpp:237] Iteration 2100, loss = 1.7002
I0521 01:41:16.438935  6984 solver.cpp:253]     Train net output #0: loss = 1.7002 (* 1 = 1.7002 loss)
I0521 01:41:16.438951  6984 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0521 01:41:46.533619  6984 solver.cpp:237] Iteration 2128, loss = 1.73279
I0521 01:41:46.533785  6984 solver.cpp:253]     Train net output #0: loss = 1.73279 (* 1 = 1.73279 loss)
I0521 01:41:46.533800  6984 sgd_solver.cpp:106] Iteration 2128, lr = 0.0025
I0521 01:41:54.379181  6984 solver.cpp:237] Iteration 2156, loss = 1.64985
I0521 01:41:54.379215  6984 solver.cpp:253]     Train net output #0: loss = 1.64985 (* 1 = 1.64985 loss)
I0521 01:41:54.379231  6984 sgd_solver.cpp:106] Iteration 2156, lr = 0.0025
I0521 01:42:02.229122  6984 solver.cpp:237] Iteration 2184, loss = 1.62686
I0521 01:42:02.229154  6984 solver.cpp:253]     Train net output #0: loss = 1.62686 (* 1 = 1.62686 loss)
I0521 01:42:02.229176  6984 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0521 01:42:10.076416  6984 solver.cpp:237] Iteration 2212, loss = 1.61921
I0521 01:42:10.076449  6984 solver.cpp:253]     Train net output #0: loss = 1.61921 (* 1 = 1.61921 loss)
I0521 01:42:10.076465  6984 sgd_solver.cpp:106] Iteration 2212, lr = 0.0025
I0521 01:42:17.923857  6984 solver.cpp:237] Iteration 2240, loss = 1.61232
I0521 01:42:17.923995  6984 solver.cpp:253]     Train net output #0: loss = 1.61232 (* 1 = 1.61232 loss)
I0521 01:42:17.924008  6984 sgd_solver.cpp:106] Iteration 2240, lr = 0.0025
I0521 01:42:25.774133  6984 solver.cpp:237] Iteration 2268, loss = 1.63834
I0521 01:42:25.774178  6984 solver.cpp:253]     Train net output #0: loss = 1.63834 (* 1 = 1.63834 loss)
I0521 01:42:25.774194  6984 sgd_solver.cpp:106] Iteration 2268, lr = 0.0025
I0521 01:42:33.622575  6984 solver.cpp:237] Iteration 2296, loss = 1.64057
I0521 01:42:33.622608  6984 solver.cpp:253]     Train net output #0: loss = 1.64057 (* 1 = 1.64057 loss)
I0521 01:42:33.622625  6984 sgd_solver.cpp:106] Iteration 2296, lr = 0.0025
I0521 01:42:35.585994  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2304.caffemodel
I0521 01:42:35.819630  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2304.solverstate
I0521 01:42:35.845978  6984 solver.cpp:341] Iteration 2304, Testing net (#0)
I0521 01:43:41.787701  6984 solver.cpp:409]     Test net output #0: accuracy = 0.675487
I0521 01:43:41.787874  6984 solver.cpp:409]     Test net output #1: loss = 1.09665 (* 1 = 1.09665 loss)
I0521 01:44:09.619911  6984 solver.cpp:237] Iteration 2324, loss = 1.65686
I0521 01:44:09.619962  6984 solver.cpp:253]     Train net output #0: loss = 1.65686 (* 1 = 1.65686 loss)
I0521 01:44:09.619978  6984 sgd_solver.cpp:106] Iteration 2324, lr = 0.0025
I0521 01:44:17.473201  6984 solver.cpp:237] Iteration 2352, loss = 1.60449
I0521 01:44:17.473353  6984 solver.cpp:253]     Train net output #0: loss = 1.60449 (* 1 = 1.60449 loss)
I0521 01:44:17.473367  6984 sgd_solver.cpp:106] Iteration 2352, lr = 0.0025
I0521 01:44:25.317883  6984 solver.cpp:237] Iteration 2380, loss = 1.6706
I0521 01:44:25.317915  6984 solver.cpp:253]     Train net output #0: loss = 1.6706 (* 1 = 1.6706 loss)
I0521 01:44:25.317934  6984 sgd_solver.cpp:106] Iteration 2380, lr = 0.0025
I0521 01:44:33.157541  6984 solver.cpp:237] Iteration 2408, loss = 1.63148
I0521 01:44:33.157580  6984 solver.cpp:253]     Train net output #0: loss = 1.63148 (* 1 = 1.63148 loss)
I0521 01:44:33.157601  6984 sgd_solver.cpp:106] Iteration 2408, lr = 0.0025
I0521 01:44:41.004791  6984 solver.cpp:237] Iteration 2436, loss = 1.65262
I0521 01:44:41.004824  6984 solver.cpp:253]     Train net output #0: loss = 1.65262 (* 1 = 1.65262 loss)
I0521 01:44:41.004842  6984 sgd_solver.cpp:106] Iteration 2436, lr = 0.0025
I0521 01:44:48.846935  6984 solver.cpp:237] Iteration 2464, loss = 1.63678
I0521 01:44:48.847275  6984 solver.cpp:253]     Train net output #0: loss = 1.63678 (* 1 = 1.63678 loss)
I0521 01:44:48.847287  6984 sgd_solver.cpp:106] Iteration 2464, lr = 0.0025
I0521 01:44:56.694473  6984 solver.cpp:237] Iteration 2492, loss = 1.62056
I0521 01:44:56.694519  6984 solver.cpp:253]     Train net output #0: loss = 1.62056 (* 1 = 1.62056 loss)
I0521 01:44:56.694535  6984 sgd_solver.cpp:106] Iteration 2492, lr = 0.0025
I0521 01:45:26.661070  6984 solver.cpp:237] Iteration 2520, loss = 1.69416
I0521 01:45:26.661248  6984 solver.cpp:253]     Train net output #0: loss = 1.69416 (* 1 = 1.69416 loss)
I0521 01:45:26.661262  6984 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0521 01:45:34.508332  6984 solver.cpp:237] Iteration 2548, loss = 1.58429
I0521 01:45:34.508365  6984 solver.cpp:253]     Train net output #0: loss = 1.58429 (* 1 = 1.58429 loss)
I0521 01:45:34.508379  6984 sgd_solver.cpp:106] Iteration 2548, lr = 0.0025
I0521 01:45:42.356199  6984 solver.cpp:237] Iteration 2576, loss = 1.62181
I0521 01:45:42.356245  6984 solver.cpp:253]     Train net output #0: loss = 1.62181 (* 1 = 1.62181 loss)
I0521 01:45:42.356261  6984 sgd_solver.cpp:106] Iteration 2576, lr = 0.0025
I0521 01:45:46.563477  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2592.caffemodel
I0521 01:45:46.800042  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2592.solverstate
I0521 01:45:50.277217  6984 solver.cpp:237] Iteration 2604, loss = 1.71047
I0521 01:45:50.277266  6984 solver.cpp:253]     Train net output #0: loss = 1.71047 (* 1 = 1.71047 loss)
I0521 01:45:50.277281  6984 sgd_solver.cpp:106] Iteration 2604, lr = 0.0025
I0521 01:45:58.119478  6984 solver.cpp:237] Iteration 2632, loss = 1.56629
I0521 01:45:58.119642  6984 solver.cpp:253]     Train net output #0: loss = 1.56629 (* 1 = 1.56629 loss)
I0521 01:45:58.119657  6984 sgd_solver.cpp:106] Iteration 2632, lr = 0.0025
I0521 01:46:05.965850  6984 solver.cpp:237] Iteration 2660, loss = 1.71273
I0521 01:46:05.965883  6984 solver.cpp:253]     Train net output #0: loss = 1.71273 (* 1 = 1.71273 loss)
I0521 01:46:05.965899  6984 sgd_solver.cpp:106] Iteration 2660, lr = 0.0025
I0521 01:46:13.812966  6984 solver.cpp:237] Iteration 2688, loss = 1.66952
I0521 01:46:13.813005  6984 solver.cpp:253]     Train net output #0: loss = 1.66952 (* 1 = 1.66952 loss)
I0521 01:46:13.813025  6984 sgd_solver.cpp:106] Iteration 2688, lr = 0.0025
I0521 01:46:43.777727  6984 solver.cpp:237] Iteration 2716, loss = 1.63202
I0521 01:46:43.777895  6984 solver.cpp:253]     Train net output #0: loss = 1.63202 (* 1 = 1.63202 loss)
I0521 01:46:43.777909  6984 sgd_solver.cpp:106] Iteration 2716, lr = 0.0025
I0521 01:46:51.626091  6984 solver.cpp:237] Iteration 2744, loss = 1.69619
I0521 01:46:51.626123  6984 solver.cpp:253]     Train net output #0: loss = 1.69619 (* 1 = 1.69619 loss)
I0521 01:46:51.626138  6984 sgd_solver.cpp:106] Iteration 2744, lr = 0.0025
I0521 01:46:59.473526  6984 solver.cpp:237] Iteration 2772, loss = 1.5384
I0521 01:46:59.473572  6984 solver.cpp:253]     Train net output #0: loss = 1.5384 (* 1 = 1.5384 loss)
I0521 01:46:59.473587  6984 sgd_solver.cpp:106] Iteration 2772, lr = 0.0025
I0521 01:47:07.328065  6984 solver.cpp:237] Iteration 2800, loss = 1.66912
I0521 01:47:07.328099  6984 solver.cpp:253]     Train net output #0: loss = 1.66912 (* 1 = 1.66912 loss)
I0521 01:47:07.328112  6984 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0521 01:47:15.176061  6984 solver.cpp:237] Iteration 2828, loss = 1.55838
I0521 01:47:15.176209  6984 solver.cpp:253]     Train net output #0: loss = 1.55838 (* 1 = 1.55838 loss)
I0521 01:47:15.176223  6984 sgd_solver.cpp:106] Iteration 2828, lr = 0.0025
I0521 01:47:23.026989  6984 solver.cpp:237] Iteration 2856, loss = 1.58525
I0521 01:47:23.027034  6984 solver.cpp:253]     Train net output #0: loss = 1.58525 (* 1 = 1.58525 loss)
I0521 01:47:23.027051  6984 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0521 01:47:29.474244  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2880.caffemodel
I0521 01:47:29.709698  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2880.solverstate
I0521 01:47:29.737718  6984 solver.cpp:341] Iteration 2880, Testing net (#0)
I0521 01:48:14.820893  6984 solver.cpp:409]     Test net output #0: accuracy = 0.691646
I0521 01:48:14.821055  6984 solver.cpp:409]     Test net output #1: loss = 1.04342 (* 1 = 1.04342 loss)
I0521 01:48:15.746194  6984 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2884.caffemodel
I0521 01:48:15.982383  6984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699_iter_2884.solverstate
I0521 01:48:36.980278  6984 solver.cpp:321] Iteration 2884, loss = 1.6049
I0521 01:48:36.980322  6984 solver.cpp:326] Optimization Done.
I0521 01:48:36.980331  6984 caffe.cpp:215] Optimization Done.
Application 11236365 resources: utime ~1270s, stime ~229s, Rss ~5332984, inblocks ~3744348, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_520_2016-05-20T11.20.51.492699.solver"
	User time (seconds): 0.56
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:03.23
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15083
	Voluntary context switches: 2741
	Involuntary context switches: 69
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
