2806466
I0521 11:12:42.791918  3494 caffe.cpp:184] Using GPUs 0
I0521 11:12:43.210037  3494 solver.cpp:48] Initializing solver from parameters: 
test_iter: 151
test_interval: 303
base_lr: 0.0025
display: 15
max_iter: 1515
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 151
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002.prototxt"
I0521 11:12:43.211628  3494 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002.prototxt
I0521 11:12:43.225615  3494 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 11:12:43.225675  3494 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 11:12:43.226019  3494 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 990
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 11:12:43.226197  3494 layer_factory.hpp:77] Creating layer data_hdf5
I0521 11:12:43.226220  3494 net.cpp:106] Creating Layer data_hdf5
I0521 11:12:43.226235  3494 net.cpp:411] data_hdf5 -> data
I0521 11:12:43.226269  3494 net.cpp:411] data_hdf5 -> label
I0521 11:12:43.226327  3494 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 11:12:43.227794  3494 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 11:12:43.230140  3494 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 11:13:04.758335  3494 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 11:13:04.763461  3494 net.cpp:150] Setting up data_hdf5
I0521 11:13:04.763501  3494 net.cpp:157] Top shape: 990 1 127 50 (6286500)
I0521 11:13:04.763515  3494 net.cpp:157] Top shape: 990 (990)
I0521 11:13:04.763527  3494 net.cpp:165] Memory required for data: 25149960
I0521 11:13:04.763540  3494 layer_factory.hpp:77] Creating layer conv1
I0521 11:13:04.763574  3494 net.cpp:106] Creating Layer conv1
I0521 11:13:04.763586  3494 net.cpp:454] conv1 <- data
I0521 11:13:04.763607  3494 net.cpp:411] conv1 -> conv1
I0521 11:13:05.130190  3494 net.cpp:150] Setting up conv1
I0521 11:13:05.130236  3494 net.cpp:157] Top shape: 990 12 120 48 (68428800)
I0521 11:13:05.130247  3494 net.cpp:165] Memory required for data: 298865160
I0521 11:13:05.130278  3494 layer_factory.hpp:77] Creating layer relu1
I0521 11:13:05.130300  3494 net.cpp:106] Creating Layer relu1
I0521 11:13:05.130311  3494 net.cpp:454] relu1 <- conv1
I0521 11:13:05.130323  3494 net.cpp:397] relu1 -> conv1 (in-place)
I0521 11:13:05.130910  3494 net.cpp:150] Setting up relu1
I0521 11:13:05.130928  3494 net.cpp:157] Top shape: 990 12 120 48 (68428800)
I0521 11:13:05.130939  3494 net.cpp:165] Memory required for data: 572580360
I0521 11:13:05.130949  3494 layer_factory.hpp:77] Creating layer pool1
I0521 11:13:05.130965  3494 net.cpp:106] Creating Layer pool1
I0521 11:13:05.130975  3494 net.cpp:454] pool1 <- conv1
I0521 11:13:05.130990  3494 net.cpp:411] pool1 -> pool1
I0521 11:13:05.131069  3494 net.cpp:150] Setting up pool1
I0521 11:13:05.131083  3494 net.cpp:157] Top shape: 990 12 60 48 (34214400)
I0521 11:13:05.131093  3494 net.cpp:165] Memory required for data: 709437960
I0521 11:13:05.131103  3494 layer_factory.hpp:77] Creating layer conv2
I0521 11:13:05.131125  3494 net.cpp:106] Creating Layer conv2
I0521 11:13:05.131136  3494 net.cpp:454] conv2 <- pool1
I0521 11:13:05.131150  3494 net.cpp:411] conv2 -> conv2
I0521 11:13:05.133821  3494 net.cpp:150] Setting up conv2
I0521 11:13:05.133848  3494 net.cpp:157] Top shape: 990 20 54 46 (49183200)
I0521 11:13:05.133859  3494 net.cpp:165] Memory required for data: 906170760
I0521 11:13:05.133878  3494 layer_factory.hpp:77] Creating layer relu2
I0521 11:13:05.133893  3494 net.cpp:106] Creating Layer relu2
I0521 11:13:05.133903  3494 net.cpp:454] relu2 <- conv2
I0521 11:13:05.133916  3494 net.cpp:397] relu2 -> conv2 (in-place)
I0521 11:13:05.134245  3494 net.cpp:150] Setting up relu2
I0521 11:13:05.134260  3494 net.cpp:157] Top shape: 990 20 54 46 (49183200)
I0521 11:13:05.134271  3494 net.cpp:165] Memory required for data: 1102903560
I0521 11:13:05.134281  3494 layer_factory.hpp:77] Creating layer pool2
I0521 11:13:05.134294  3494 net.cpp:106] Creating Layer pool2
I0521 11:13:05.134305  3494 net.cpp:454] pool2 <- conv2
I0521 11:13:05.134330  3494 net.cpp:411] pool2 -> pool2
I0521 11:13:05.134398  3494 net.cpp:150] Setting up pool2
I0521 11:13:05.134412  3494 net.cpp:157] Top shape: 990 20 27 46 (24591600)
I0521 11:13:05.134419  3494 net.cpp:165] Memory required for data: 1201269960
I0521 11:13:05.134429  3494 layer_factory.hpp:77] Creating layer conv3
I0521 11:13:05.134446  3494 net.cpp:106] Creating Layer conv3
I0521 11:13:05.134457  3494 net.cpp:454] conv3 <- pool2
I0521 11:13:05.134474  3494 net.cpp:411] conv3 -> conv3
I0521 11:13:05.136384  3494 net.cpp:150] Setting up conv3
I0521 11:13:05.136407  3494 net.cpp:157] Top shape: 990 28 22 44 (26832960)
I0521 11:13:05.136420  3494 net.cpp:165] Memory required for data: 1308601800
I0521 11:13:05.136437  3494 layer_factory.hpp:77] Creating layer relu3
I0521 11:13:05.136454  3494 net.cpp:106] Creating Layer relu3
I0521 11:13:05.136464  3494 net.cpp:454] relu3 <- conv3
I0521 11:13:05.136477  3494 net.cpp:397] relu3 -> conv3 (in-place)
I0521 11:13:05.136946  3494 net.cpp:150] Setting up relu3
I0521 11:13:05.136970  3494 net.cpp:157] Top shape: 990 28 22 44 (26832960)
I0521 11:13:05.136981  3494 net.cpp:165] Memory required for data: 1415933640
I0521 11:13:05.136991  3494 layer_factory.hpp:77] Creating layer pool3
I0521 11:13:05.137003  3494 net.cpp:106] Creating Layer pool3
I0521 11:13:05.137013  3494 net.cpp:454] pool3 <- conv3
I0521 11:13:05.137024  3494 net.cpp:411] pool3 -> pool3
I0521 11:13:05.137092  3494 net.cpp:150] Setting up pool3
I0521 11:13:05.137106  3494 net.cpp:157] Top shape: 990 28 11 44 (13416480)
I0521 11:13:05.137116  3494 net.cpp:165] Memory required for data: 1469599560
I0521 11:13:05.137125  3494 layer_factory.hpp:77] Creating layer conv4
I0521 11:13:05.137141  3494 net.cpp:106] Creating Layer conv4
I0521 11:13:05.137151  3494 net.cpp:454] conv4 <- pool3
I0521 11:13:05.137166  3494 net.cpp:411] conv4 -> conv4
I0521 11:13:05.139886  3494 net.cpp:150] Setting up conv4
I0521 11:13:05.139914  3494 net.cpp:157] Top shape: 990 36 6 42 (8981280)
I0521 11:13:05.139925  3494 net.cpp:165] Memory required for data: 1505524680
I0521 11:13:05.139940  3494 layer_factory.hpp:77] Creating layer relu4
I0521 11:13:05.139953  3494 net.cpp:106] Creating Layer relu4
I0521 11:13:05.139964  3494 net.cpp:454] relu4 <- conv4
I0521 11:13:05.139976  3494 net.cpp:397] relu4 -> conv4 (in-place)
I0521 11:13:05.140439  3494 net.cpp:150] Setting up relu4
I0521 11:13:05.140455  3494 net.cpp:157] Top shape: 990 36 6 42 (8981280)
I0521 11:13:05.140466  3494 net.cpp:165] Memory required for data: 1541449800
I0521 11:13:05.140476  3494 layer_factory.hpp:77] Creating layer pool4
I0521 11:13:05.140489  3494 net.cpp:106] Creating Layer pool4
I0521 11:13:05.140499  3494 net.cpp:454] pool4 <- conv4
I0521 11:13:05.140511  3494 net.cpp:411] pool4 -> pool4
I0521 11:13:05.140580  3494 net.cpp:150] Setting up pool4
I0521 11:13:05.140594  3494 net.cpp:157] Top shape: 990 36 3 42 (4490640)
I0521 11:13:05.140604  3494 net.cpp:165] Memory required for data: 1559412360
I0521 11:13:05.140614  3494 layer_factory.hpp:77] Creating layer ip1
I0521 11:13:05.140635  3494 net.cpp:106] Creating Layer ip1
I0521 11:13:05.140645  3494 net.cpp:454] ip1 <- pool4
I0521 11:13:05.140656  3494 net.cpp:411] ip1 -> ip1
I0521 11:13:05.156075  3494 net.cpp:150] Setting up ip1
I0521 11:13:05.156102  3494 net.cpp:157] Top shape: 990 196 (194040)
I0521 11:13:05.156116  3494 net.cpp:165] Memory required for data: 1560188520
I0521 11:13:05.156137  3494 layer_factory.hpp:77] Creating layer relu5
I0521 11:13:05.156152  3494 net.cpp:106] Creating Layer relu5
I0521 11:13:05.156163  3494 net.cpp:454] relu5 <- ip1
I0521 11:13:05.156177  3494 net.cpp:397] relu5 -> ip1 (in-place)
I0521 11:13:05.156515  3494 net.cpp:150] Setting up relu5
I0521 11:13:05.156530  3494 net.cpp:157] Top shape: 990 196 (194040)
I0521 11:13:05.156540  3494 net.cpp:165] Memory required for data: 1560964680
I0521 11:13:05.156550  3494 layer_factory.hpp:77] Creating layer drop1
I0521 11:13:05.156571  3494 net.cpp:106] Creating Layer drop1
I0521 11:13:05.156581  3494 net.cpp:454] drop1 <- ip1
I0521 11:13:05.156607  3494 net.cpp:397] drop1 -> ip1 (in-place)
I0521 11:13:05.156652  3494 net.cpp:150] Setting up drop1
I0521 11:13:05.156666  3494 net.cpp:157] Top shape: 990 196 (194040)
I0521 11:13:05.156677  3494 net.cpp:165] Memory required for data: 1561740840
I0521 11:13:05.156687  3494 layer_factory.hpp:77] Creating layer ip2
I0521 11:13:05.156704  3494 net.cpp:106] Creating Layer ip2
I0521 11:13:05.156715  3494 net.cpp:454] ip2 <- ip1
I0521 11:13:05.156725  3494 net.cpp:411] ip2 -> ip2
I0521 11:13:05.157199  3494 net.cpp:150] Setting up ip2
I0521 11:13:05.157212  3494 net.cpp:157] Top shape: 990 98 (97020)
I0521 11:13:05.157222  3494 net.cpp:165] Memory required for data: 1562128920
I0521 11:13:05.157238  3494 layer_factory.hpp:77] Creating layer relu6
I0521 11:13:05.157250  3494 net.cpp:106] Creating Layer relu6
I0521 11:13:05.157260  3494 net.cpp:454] relu6 <- ip2
I0521 11:13:05.157272  3494 net.cpp:397] relu6 -> ip2 (in-place)
I0521 11:13:05.157784  3494 net.cpp:150] Setting up relu6
I0521 11:13:05.157800  3494 net.cpp:157] Top shape: 990 98 (97020)
I0521 11:13:05.157810  3494 net.cpp:165] Memory required for data: 1562517000
I0521 11:13:05.157820  3494 layer_factory.hpp:77] Creating layer drop2
I0521 11:13:05.157833  3494 net.cpp:106] Creating Layer drop2
I0521 11:13:05.157843  3494 net.cpp:454] drop2 <- ip2
I0521 11:13:05.157856  3494 net.cpp:397] drop2 -> ip2 (in-place)
I0521 11:13:05.157898  3494 net.cpp:150] Setting up drop2
I0521 11:13:05.157912  3494 net.cpp:157] Top shape: 990 98 (97020)
I0521 11:13:05.157922  3494 net.cpp:165] Memory required for data: 1562905080
I0521 11:13:05.157932  3494 layer_factory.hpp:77] Creating layer ip3
I0521 11:13:05.157944  3494 net.cpp:106] Creating Layer ip3
I0521 11:13:05.157954  3494 net.cpp:454] ip3 <- ip2
I0521 11:13:05.157970  3494 net.cpp:411] ip3 -> ip3
I0521 11:13:05.158181  3494 net.cpp:150] Setting up ip3
I0521 11:13:05.158195  3494 net.cpp:157] Top shape: 990 11 (10890)
I0521 11:13:05.158203  3494 net.cpp:165] Memory required for data: 1562948640
I0521 11:13:05.158218  3494 layer_factory.hpp:77] Creating layer drop3
I0521 11:13:05.158231  3494 net.cpp:106] Creating Layer drop3
I0521 11:13:05.158241  3494 net.cpp:454] drop3 <- ip3
I0521 11:13:05.158252  3494 net.cpp:397] drop3 -> ip3 (in-place)
I0521 11:13:05.158293  3494 net.cpp:150] Setting up drop3
I0521 11:13:05.158304  3494 net.cpp:157] Top shape: 990 11 (10890)
I0521 11:13:05.158315  3494 net.cpp:165] Memory required for data: 1562992200
I0521 11:13:05.158324  3494 layer_factory.hpp:77] Creating layer loss
I0521 11:13:05.158344  3494 net.cpp:106] Creating Layer loss
I0521 11:13:05.158354  3494 net.cpp:454] loss <- ip3
I0521 11:13:05.158365  3494 net.cpp:454] loss <- label
I0521 11:13:05.158377  3494 net.cpp:411] loss -> loss
I0521 11:13:05.158395  3494 layer_factory.hpp:77] Creating layer loss
I0521 11:13:05.159049  3494 net.cpp:150] Setting up loss
I0521 11:13:05.159065  3494 net.cpp:157] Top shape: (1)
I0521 11:13:05.159075  3494 net.cpp:160]     with loss weight 1
I0521 11:13:05.159117  3494 net.cpp:165] Memory required for data: 1562992204
I0521 11:13:05.159128  3494 net.cpp:226] loss needs backward computation.
I0521 11:13:05.159139  3494 net.cpp:226] drop3 needs backward computation.
I0521 11:13:05.159148  3494 net.cpp:226] ip3 needs backward computation.
I0521 11:13:05.159159  3494 net.cpp:226] drop2 needs backward computation.
I0521 11:13:05.159169  3494 net.cpp:226] relu6 needs backward computation.
I0521 11:13:05.159179  3494 net.cpp:226] ip2 needs backward computation.
I0521 11:13:05.159189  3494 net.cpp:226] drop1 needs backward computation.
I0521 11:13:05.159199  3494 net.cpp:226] relu5 needs backward computation.
I0521 11:13:05.159209  3494 net.cpp:226] ip1 needs backward computation.
I0521 11:13:05.159219  3494 net.cpp:226] pool4 needs backward computation.
I0521 11:13:05.159229  3494 net.cpp:226] relu4 needs backward computation.
I0521 11:13:05.159238  3494 net.cpp:226] conv4 needs backward computation.
I0521 11:13:05.159250  3494 net.cpp:226] pool3 needs backward computation.
I0521 11:13:05.159268  3494 net.cpp:226] relu3 needs backward computation.
I0521 11:13:05.159277  3494 net.cpp:226] conv3 needs backward computation.
I0521 11:13:05.159289  3494 net.cpp:226] pool2 needs backward computation.
I0521 11:13:05.159299  3494 net.cpp:226] relu2 needs backward computation.
I0521 11:13:05.159309  3494 net.cpp:226] conv2 needs backward computation.
I0521 11:13:05.159322  3494 net.cpp:226] pool1 needs backward computation.
I0521 11:13:05.159332  3494 net.cpp:226] relu1 needs backward computation.
I0521 11:13:05.159342  3494 net.cpp:226] conv1 needs backward computation.
I0521 11:13:05.159353  3494 net.cpp:228] data_hdf5 does not need backward computation.
I0521 11:13:05.159363  3494 net.cpp:270] This network produces output loss
I0521 11:13:05.159386  3494 net.cpp:283] Network initialization done.
I0521 11:13:05.161010  3494 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002.prototxt
I0521 11:13:05.161080  3494 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 11:13:05.161433  3494 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 990
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 11:13:05.161623  3494 layer_factory.hpp:77] Creating layer data_hdf5
I0521 11:13:05.161638  3494 net.cpp:106] Creating Layer data_hdf5
I0521 11:13:05.161650  3494 net.cpp:411] data_hdf5 -> data
I0521 11:13:05.161667  3494 net.cpp:411] data_hdf5 -> label
I0521 11:13:05.161684  3494 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 11:13:05.163105  3494 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 11:13:26.450700  3494 net.cpp:150] Setting up data_hdf5
I0521 11:13:26.450865  3494 net.cpp:157] Top shape: 990 1 127 50 (6286500)
I0521 11:13:26.450881  3494 net.cpp:157] Top shape: 990 (990)
I0521 11:13:26.450891  3494 net.cpp:165] Memory required for data: 25149960
I0521 11:13:26.450904  3494 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 11:13:26.450932  3494 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 11:13:26.450943  3494 net.cpp:454] label_data_hdf5_1_split <- label
I0521 11:13:26.450958  3494 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 11:13:26.450980  3494 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 11:13:26.451053  3494 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 11:13:26.451067  3494 net.cpp:157] Top shape: 990 (990)
I0521 11:13:26.451079  3494 net.cpp:157] Top shape: 990 (990)
I0521 11:13:26.451088  3494 net.cpp:165] Memory required for data: 25157880
I0521 11:13:26.451099  3494 layer_factory.hpp:77] Creating layer conv1
I0521 11:13:26.451120  3494 net.cpp:106] Creating Layer conv1
I0521 11:13:26.451131  3494 net.cpp:454] conv1 <- data
I0521 11:13:26.451145  3494 net.cpp:411] conv1 -> conv1
I0521 11:13:26.453064  3494 net.cpp:150] Setting up conv1
I0521 11:13:26.453089  3494 net.cpp:157] Top shape: 990 12 120 48 (68428800)
I0521 11:13:26.453101  3494 net.cpp:165] Memory required for data: 298873080
I0521 11:13:26.453121  3494 layer_factory.hpp:77] Creating layer relu1
I0521 11:13:26.453136  3494 net.cpp:106] Creating Layer relu1
I0521 11:13:26.453146  3494 net.cpp:454] relu1 <- conv1
I0521 11:13:26.453160  3494 net.cpp:397] relu1 -> conv1 (in-place)
I0521 11:13:26.453655  3494 net.cpp:150] Setting up relu1
I0521 11:13:26.453671  3494 net.cpp:157] Top shape: 990 12 120 48 (68428800)
I0521 11:13:26.453681  3494 net.cpp:165] Memory required for data: 572588280
I0521 11:13:26.453692  3494 layer_factory.hpp:77] Creating layer pool1
I0521 11:13:26.453708  3494 net.cpp:106] Creating Layer pool1
I0521 11:13:26.453718  3494 net.cpp:454] pool1 <- conv1
I0521 11:13:26.453732  3494 net.cpp:411] pool1 -> pool1
I0521 11:13:26.453805  3494 net.cpp:150] Setting up pool1
I0521 11:13:26.453819  3494 net.cpp:157] Top shape: 990 12 60 48 (34214400)
I0521 11:13:26.453829  3494 net.cpp:165] Memory required for data: 709445880
I0521 11:13:26.453838  3494 layer_factory.hpp:77] Creating layer conv2
I0521 11:13:26.453855  3494 net.cpp:106] Creating Layer conv2
I0521 11:13:26.453866  3494 net.cpp:454] conv2 <- pool1
I0521 11:13:26.453881  3494 net.cpp:411] conv2 -> conv2
I0521 11:13:26.455782  3494 net.cpp:150] Setting up conv2
I0521 11:13:26.455806  3494 net.cpp:157] Top shape: 990 20 54 46 (49183200)
I0521 11:13:26.455817  3494 net.cpp:165] Memory required for data: 906178680
I0521 11:13:26.455835  3494 layer_factory.hpp:77] Creating layer relu2
I0521 11:13:26.455848  3494 net.cpp:106] Creating Layer relu2
I0521 11:13:26.455858  3494 net.cpp:454] relu2 <- conv2
I0521 11:13:26.455870  3494 net.cpp:397] relu2 -> conv2 (in-place)
I0521 11:13:26.456204  3494 net.cpp:150] Setting up relu2
I0521 11:13:26.456218  3494 net.cpp:157] Top shape: 990 20 54 46 (49183200)
I0521 11:13:26.456228  3494 net.cpp:165] Memory required for data: 1102911480
I0521 11:13:26.456239  3494 layer_factory.hpp:77] Creating layer pool2
I0521 11:13:26.456253  3494 net.cpp:106] Creating Layer pool2
I0521 11:13:26.456262  3494 net.cpp:454] pool2 <- conv2
I0521 11:13:26.456275  3494 net.cpp:411] pool2 -> pool2
I0521 11:13:26.456346  3494 net.cpp:150] Setting up pool2
I0521 11:13:26.456358  3494 net.cpp:157] Top shape: 990 20 27 46 (24591600)
I0521 11:13:26.456368  3494 net.cpp:165] Memory required for data: 1201277880
I0521 11:13:26.456377  3494 layer_factory.hpp:77] Creating layer conv3
I0521 11:13:26.456395  3494 net.cpp:106] Creating Layer conv3
I0521 11:13:26.456405  3494 net.cpp:454] conv3 <- pool2
I0521 11:13:26.456419  3494 net.cpp:411] conv3 -> conv3
I0521 11:13:26.458397  3494 net.cpp:150] Setting up conv3
I0521 11:13:26.458420  3494 net.cpp:157] Top shape: 990 28 22 44 (26832960)
I0521 11:13:26.458431  3494 net.cpp:165] Memory required for data: 1308609720
I0521 11:13:26.458464  3494 layer_factory.hpp:77] Creating layer relu3
I0521 11:13:26.458477  3494 net.cpp:106] Creating Layer relu3
I0521 11:13:26.458487  3494 net.cpp:454] relu3 <- conv3
I0521 11:13:26.458500  3494 net.cpp:397] relu3 -> conv3 (in-place)
I0521 11:13:26.458968  3494 net.cpp:150] Setting up relu3
I0521 11:13:26.458984  3494 net.cpp:157] Top shape: 990 28 22 44 (26832960)
I0521 11:13:26.458994  3494 net.cpp:165] Memory required for data: 1415941560
I0521 11:13:26.459005  3494 layer_factory.hpp:77] Creating layer pool3
I0521 11:13:26.459017  3494 net.cpp:106] Creating Layer pool3
I0521 11:13:26.459028  3494 net.cpp:454] pool3 <- conv3
I0521 11:13:26.459040  3494 net.cpp:411] pool3 -> pool3
I0521 11:13:26.459112  3494 net.cpp:150] Setting up pool3
I0521 11:13:26.459125  3494 net.cpp:157] Top shape: 990 28 11 44 (13416480)
I0521 11:13:26.459136  3494 net.cpp:165] Memory required for data: 1469607480
I0521 11:13:26.459144  3494 layer_factory.hpp:77] Creating layer conv4
I0521 11:13:26.459162  3494 net.cpp:106] Creating Layer conv4
I0521 11:13:26.459172  3494 net.cpp:454] conv4 <- pool3
I0521 11:13:26.459187  3494 net.cpp:411] conv4 -> conv4
I0521 11:13:26.461241  3494 net.cpp:150] Setting up conv4
I0521 11:13:26.461264  3494 net.cpp:157] Top shape: 990 36 6 42 (8981280)
I0521 11:13:26.461277  3494 net.cpp:165] Memory required for data: 1505532600
I0521 11:13:26.461292  3494 layer_factory.hpp:77] Creating layer relu4
I0521 11:13:26.461304  3494 net.cpp:106] Creating Layer relu4
I0521 11:13:26.461314  3494 net.cpp:454] relu4 <- conv4
I0521 11:13:26.461328  3494 net.cpp:397] relu4 -> conv4 (in-place)
I0521 11:13:26.461802  3494 net.cpp:150] Setting up relu4
I0521 11:13:26.461817  3494 net.cpp:157] Top shape: 990 36 6 42 (8981280)
I0521 11:13:26.461827  3494 net.cpp:165] Memory required for data: 1541457720
I0521 11:13:26.461838  3494 layer_factory.hpp:77] Creating layer pool4
I0521 11:13:26.461850  3494 net.cpp:106] Creating Layer pool4
I0521 11:13:26.461860  3494 net.cpp:454] pool4 <- conv4
I0521 11:13:26.461874  3494 net.cpp:411] pool4 -> pool4
I0521 11:13:26.461944  3494 net.cpp:150] Setting up pool4
I0521 11:13:26.461957  3494 net.cpp:157] Top shape: 990 36 3 42 (4490640)
I0521 11:13:26.461966  3494 net.cpp:165] Memory required for data: 1559420280
I0521 11:13:26.461977  3494 layer_factory.hpp:77] Creating layer ip1
I0521 11:13:26.461992  3494 net.cpp:106] Creating Layer ip1
I0521 11:13:26.462002  3494 net.cpp:454] ip1 <- pool4
I0521 11:13:26.462016  3494 net.cpp:411] ip1 -> ip1
I0521 11:13:26.477486  3494 net.cpp:150] Setting up ip1
I0521 11:13:26.477514  3494 net.cpp:157] Top shape: 990 196 (194040)
I0521 11:13:26.477530  3494 net.cpp:165] Memory required for data: 1560196440
I0521 11:13:26.477557  3494 layer_factory.hpp:77] Creating layer relu5
I0521 11:13:26.477572  3494 net.cpp:106] Creating Layer relu5
I0521 11:13:26.477582  3494 net.cpp:454] relu5 <- ip1
I0521 11:13:26.477596  3494 net.cpp:397] relu5 -> ip1 (in-place)
I0521 11:13:26.477941  3494 net.cpp:150] Setting up relu5
I0521 11:13:26.477955  3494 net.cpp:157] Top shape: 990 196 (194040)
I0521 11:13:26.477965  3494 net.cpp:165] Memory required for data: 1560972600
I0521 11:13:26.477975  3494 layer_factory.hpp:77] Creating layer drop1
I0521 11:13:26.477994  3494 net.cpp:106] Creating Layer drop1
I0521 11:13:26.478004  3494 net.cpp:454] drop1 <- ip1
I0521 11:13:26.478018  3494 net.cpp:397] drop1 -> ip1 (in-place)
I0521 11:13:26.478061  3494 net.cpp:150] Setting up drop1
I0521 11:13:26.478073  3494 net.cpp:157] Top shape: 990 196 (194040)
I0521 11:13:26.478083  3494 net.cpp:165] Memory required for data: 1561748760
I0521 11:13:26.478093  3494 layer_factory.hpp:77] Creating layer ip2
I0521 11:13:26.478108  3494 net.cpp:106] Creating Layer ip2
I0521 11:13:26.478118  3494 net.cpp:454] ip2 <- ip1
I0521 11:13:26.478132  3494 net.cpp:411] ip2 -> ip2
I0521 11:13:26.478611  3494 net.cpp:150] Setting up ip2
I0521 11:13:26.478624  3494 net.cpp:157] Top shape: 990 98 (97020)
I0521 11:13:26.478634  3494 net.cpp:165] Memory required for data: 1562136840
I0521 11:13:26.478662  3494 layer_factory.hpp:77] Creating layer relu6
I0521 11:13:26.478675  3494 net.cpp:106] Creating Layer relu6
I0521 11:13:26.478685  3494 net.cpp:454] relu6 <- ip2
I0521 11:13:26.478698  3494 net.cpp:397] relu6 -> ip2 (in-place)
I0521 11:13:26.479228  3494 net.cpp:150] Setting up relu6
I0521 11:13:26.479243  3494 net.cpp:157] Top shape: 990 98 (97020)
I0521 11:13:26.479252  3494 net.cpp:165] Memory required for data: 1562524920
I0521 11:13:26.479264  3494 layer_factory.hpp:77] Creating layer drop2
I0521 11:13:26.479276  3494 net.cpp:106] Creating Layer drop2
I0521 11:13:26.479286  3494 net.cpp:454] drop2 <- ip2
I0521 11:13:26.479298  3494 net.cpp:397] drop2 -> ip2 (in-place)
I0521 11:13:26.479341  3494 net.cpp:150] Setting up drop2
I0521 11:13:26.479354  3494 net.cpp:157] Top shape: 990 98 (97020)
I0521 11:13:26.479364  3494 net.cpp:165] Memory required for data: 1562913000
I0521 11:13:26.479373  3494 layer_factory.hpp:77] Creating layer ip3
I0521 11:13:26.479387  3494 net.cpp:106] Creating Layer ip3
I0521 11:13:26.479398  3494 net.cpp:454] ip3 <- ip2
I0521 11:13:26.479411  3494 net.cpp:411] ip3 -> ip3
I0521 11:13:26.479635  3494 net.cpp:150] Setting up ip3
I0521 11:13:26.479647  3494 net.cpp:157] Top shape: 990 11 (10890)
I0521 11:13:26.479656  3494 net.cpp:165] Memory required for data: 1562956560
I0521 11:13:26.479672  3494 layer_factory.hpp:77] Creating layer drop3
I0521 11:13:26.479684  3494 net.cpp:106] Creating Layer drop3
I0521 11:13:26.479694  3494 net.cpp:454] drop3 <- ip3
I0521 11:13:26.479707  3494 net.cpp:397] drop3 -> ip3 (in-place)
I0521 11:13:26.479748  3494 net.cpp:150] Setting up drop3
I0521 11:13:26.479760  3494 net.cpp:157] Top shape: 990 11 (10890)
I0521 11:13:26.479770  3494 net.cpp:165] Memory required for data: 1563000120
I0521 11:13:26.479780  3494 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 11:13:26.479792  3494 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 11:13:26.479802  3494 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 11:13:26.479815  3494 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 11:13:26.479830  3494 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 11:13:26.479902  3494 net.cpp:150] Setting up ip3_drop3_0_split
I0521 11:13:26.479915  3494 net.cpp:157] Top shape: 990 11 (10890)
I0521 11:13:26.479928  3494 net.cpp:157] Top shape: 990 11 (10890)
I0521 11:13:26.479938  3494 net.cpp:165] Memory required for data: 1563087240
I0521 11:13:26.479948  3494 layer_factory.hpp:77] Creating layer accuracy
I0521 11:13:26.479969  3494 net.cpp:106] Creating Layer accuracy
I0521 11:13:26.479979  3494 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 11:13:26.479990  3494 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 11:13:26.480005  3494 net.cpp:411] accuracy -> accuracy
I0521 11:13:26.480026  3494 net.cpp:150] Setting up accuracy
I0521 11:13:26.480039  3494 net.cpp:157] Top shape: (1)
I0521 11:13:26.480049  3494 net.cpp:165] Memory required for data: 1563087244
I0521 11:13:26.480058  3494 layer_factory.hpp:77] Creating layer loss
I0521 11:13:26.480072  3494 net.cpp:106] Creating Layer loss
I0521 11:13:26.480082  3494 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 11:13:26.480093  3494 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 11:13:26.480106  3494 net.cpp:411] loss -> loss
I0521 11:13:26.480123  3494 layer_factory.hpp:77] Creating layer loss
I0521 11:13:26.480619  3494 net.cpp:150] Setting up loss
I0521 11:13:26.480633  3494 net.cpp:157] Top shape: (1)
I0521 11:13:26.480643  3494 net.cpp:160]     with loss weight 1
I0521 11:13:26.480660  3494 net.cpp:165] Memory required for data: 1563087248
I0521 11:13:26.480670  3494 net.cpp:226] loss needs backward computation.
I0521 11:13:26.480681  3494 net.cpp:228] accuracy does not need backward computation.
I0521 11:13:26.480692  3494 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 11:13:26.480703  3494 net.cpp:226] drop3 needs backward computation.
I0521 11:13:26.480711  3494 net.cpp:226] ip3 needs backward computation.
I0521 11:13:26.480729  3494 net.cpp:226] drop2 needs backward computation.
I0521 11:13:26.480739  3494 net.cpp:226] relu6 needs backward computation.
I0521 11:13:26.480749  3494 net.cpp:226] ip2 needs backward computation.
I0521 11:13:26.480759  3494 net.cpp:226] drop1 needs backward computation.
I0521 11:13:26.480768  3494 net.cpp:226] relu5 needs backward computation.
I0521 11:13:26.480778  3494 net.cpp:226] ip1 needs backward computation.
I0521 11:13:26.480787  3494 net.cpp:226] pool4 needs backward computation.
I0521 11:13:26.480798  3494 net.cpp:226] relu4 needs backward computation.
I0521 11:13:26.480808  3494 net.cpp:226] conv4 needs backward computation.
I0521 11:13:26.480818  3494 net.cpp:226] pool3 needs backward computation.
I0521 11:13:26.480829  3494 net.cpp:226] relu3 needs backward computation.
I0521 11:13:26.480836  3494 net.cpp:226] conv3 needs backward computation.
I0521 11:13:26.480846  3494 net.cpp:226] pool2 needs backward computation.
I0521 11:13:26.480857  3494 net.cpp:226] relu2 needs backward computation.
I0521 11:13:26.480868  3494 net.cpp:226] conv2 needs backward computation.
I0521 11:13:26.480878  3494 net.cpp:226] pool1 needs backward computation.
I0521 11:13:26.480890  3494 net.cpp:226] relu1 needs backward computation.
I0521 11:13:26.480898  3494 net.cpp:226] conv1 needs backward computation.
I0521 11:13:26.480909  3494 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 11:13:26.480921  3494 net.cpp:228] data_hdf5 does not need backward computation.
I0521 11:13:26.480931  3494 net.cpp:270] This network produces output accuracy
I0521 11:13:26.480942  3494 net.cpp:270] This network produces output loss
I0521 11:13:26.480978  3494 net.cpp:283] Network initialization done.
I0521 11:13:26.481112  3494 solver.cpp:60] Solver scaffolding done.
I0521 11:13:26.482235  3494 caffe.cpp:212] Starting Optimization
I0521 11:13:26.482254  3494 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 11:13:26.482267  3494 solver.cpp:289] Learning Rate Policy: fixed
I0521 11:13:26.483477  3494 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 11:14:12.294450  3494 solver.cpp:409]     Test net output #0: accuracy = 0.0849956
I0521 11:14:12.294610  3494 solver.cpp:409]     Test net output #1: loss = 2.39823 (* 1 = 2.39823 loss)
I0521 11:14:12.474126  3494 solver.cpp:237] Iteration 0, loss = 2.39869
I0521 11:14:12.474164  3494 solver.cpp:253]     Train net output #0: loss = 2.39869 (* 1 = 2.39869 loss)
I0521 11:14:12.474181  3494 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 11:14:20.420533  3494 solver.cpp:237] Iteration 15, loss = 2.38947
I0521 11:14:20.420572  3494 solver.cpp:253]     Train net output #0: loss = 2.38947 (* 1 = 2.38947 loss)
I0521 11:14:20.420589  3494 sgd_solver.cpp:106] Iteration 15, lr = 0.0025
I0521 11:14:28.363912  3494 solver.cpp:237] Iteration 30, loss = 2.37748
I0521 11:14:28.363943  3494 solver.cpp:253]     Train net output #0: loss = 2.37748 (* 1 = 2.37748 loss)
I0521 11:14:28.363960  3494 sgd_solver.cpp:106] Iteration 30, lr = 0.0025
I0521 11:14:36.311319  3494 solver.cpp:237] Iteration 45, loss = 2.36696
I0521 11:14:36.311350  3494 solver.cpp:253]     Train net output #0: loss = 2.36696 (* 1 = 2.36696 loss)
I0521 11:14:36.311367  3494 sgd_solver.cpp:106] Iteration 45, lr = 0.0025
I0521 11:14:44.260802  3494 solver.cpp:237] Iteration 60, loss = 2.35397
I0521 11:14:44.260944  3494 solver.cpp:253]     Train net output #0: loss = 2.35397 (* 1 = 2.35397 loss)
I0521 11:14:44.260965  3494 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 11:14:52.203783  3494 solver.cpp:237] Iteration 75, loss = 2.34882
I0521 11:14:52.203814  3494 solver.cpp:253]     Train net output #0: loss = 2.34882 (* 1 = 2.34882 loss)
I0521 11:14:52.203832  3494 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 11:15:00.146090  3494 solver.cpp:237] Iteration 90, loss = 2.34425
I0521 11:15:00.146121  3494 solver.cpp:253]     Train net output #0: loss = 2.34425 (* 1 = 2.34425 loss)
I0521 11:15:00.146138  3494 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 11:15:30.193567  3494 solver.cpp:237] Iteration 105, loss = 2.32071
I0521 11:15:30.193723  3494 solver.cpp:253]     Train net output #0: loss = 2.32071 (* 1 = 2.32071 loss)
I0521 11:15:30.193738  3494 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0521 11:15:38.141892  3494 solver.cpp:237] Iteration 120, loss = 2.33178
I0521 11:15:38.141926  3494 solver.cpp:253]     Train net output #0: loss = 2.33178 (* 1 = 2.33178 loss)
I0521 11:15:38.141943  3494 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 11:15:46.084409  3494 solver.cpp:237] Iteration 135, loss = 2.33678
I0521 11:15:46.084440  3494 solver.cpp:253]     Train net output #0: loss = 2.33678 (* 1 = 2.33678 loss)
I0521 11:15:46.084458  3494 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 11:15:54.034010  3494 solver.cpp:237] Iteration 150, loss = 2.3148
I0521 11:15:54.034045  3494 solver.cpp:253]     Train net output #0: loss = 2.3148 (* 1 = 2.3148 loss)
I0521 11:15:54.034066  3494 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 11:15:54.034453  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_151.caffemodel
I0521 11:15:54.450364  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_151.solverstate
I0521 11:16:02.050833  3494 solver.cpp:237] Iteration 165, loss = 2.31626
I0521 11:16:02.050995  3494 solver.cpp:253]     Train net output #0: loss = 2.31626 (* 1 = 2.31626 loss)
I0521 11:16:02.051009  3494 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0521 11:16:09.994727  3494 solver.cpp:237] Iteration 180, loss = 2.30971
I0521 11:16:09.994760  3494 solver.cpp:253]     Train net output #0: loss = 2.30971 (* 1 = 2.30971 loss)
I0521 11:16:09.994777  3494 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 11:16:17.940140  3494 solver.cpp:237] Iteration 195, loss = 2.28897
I0521 11:16:17.940171  3494 solver.cpp:253]     Train net output #0: loss = 2.28897 (* 1 = 2.28897 loss)
I0521 11:16:17.940186  3494 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0521 11:16:47.992698  3494 solver.cpp:237] Iteration 210, loss = 2.29175
I0521 11:16:47.992857  3494 solver.cpp:253]     Train net output #0: loss = 2.29175 (* 1 = 2.29175 loss)
I0521 11:16:47.992872  3494 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 11:16:55.940840  3494 solver.cpp:237] Iteration 225, loss = 2.28995
I0521 11:16:55.940871  3494 solver.cpp:253]     Train net output #0: loss = 2.28995 (* 1 = 2.28995 loss)
I0521 11:16:55.940891  3494 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 11:17:03.887101  3494 solver.cpp:237] Iteration 240, loss = 2.25451
I0521 11:17:03.887135  3494 solver.cpp:253]     Train net output #0: loss = 2.25451 (* 1 = 2.25451 loss)
I0521 11:17:03.887151  3494 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 11:17:11.837055  3494 solver.cpp:237] Iteration 255, loss = 2.23308
I0521 11:17:11.837098  3494 solver.cpp:253]     Train net output #0: loss = 2.23308 (* 1 = 2.23308 loss)
I0521 11:17:11.837116  3494 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 11:17:19.778322  3494 solver.cpp:237] Iteration 270, loss = 2.24504
I0521 11:17:19.778458  3494 solver.cpp:253]     Train net output #0: loss = 2.24504 (* 1 = 2.24504 loss)
I0521 11:17:19.778471  3494 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 11:17:27.722863  3494 solver.cpp:237] Iteration 285, loss = 2.17479
I0521 11:17:27.722895  3494 solver.cpp:253]     Train net output #0: loss = 2.17479 (* 1 = 2.17479 loss)
I0521 11:17:27.722913  3494 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 11:17:35.667696  3494 solver.cpp:237] Iteration 300, loss = 2.19856
I0521 11:17:35.667742  3494 solver.cpp:253]     Train net output #0: loss = 2.19856 (* 1 = 2.19856 loss)
I0521 11:17:35.667757  3494 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 11:17:36.196691  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_302.caffemodel
I0521 11:17:36.604990  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_302.solverstate
I0521 11:17:36.788583  3494 solver.cpp:341] Iteration 303, Testing net (#0)
I0521 11:18:22.077409  3494 solver.cpp:409]     Test net output #0: accuracy = 0.385885
I0521 11:18:22.077561  3494 solver.cpp:409]     Test net output #1: loss = 2.05633 (* 1 = 2.05633 loss)
I0521 11:18:50.686089  3494 solver.cpp:237] Iteration 315, loss = 2.14704
I0521 11:18:50.686137  3494 solver.cpp:253]     Train net output #0: loss = 2.14704 (* 1 = 2.14704 loss)
I0521 11:18:50.686153  3494 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0521 11:18:58.625757  3494 solver.cpp:237] Iteration 330, loss = 2.15203
I0521 11:18:58.625893  3494 solver.cpp:253]     Train net output #0: loss = 2.15203 (* 1 = 2.15203 loss)
I0521 11:18:58.625906  3494 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 11:19:06.565430  3494 solver.cpp:237] Iteration 345, loss = 2.11916
I0521 11:19:06.565461  3494 solver.cpp:253]     Train net output #0: loss = 2.11916 (* 1 = 2.11916 loss)
I0521 11:19:06.565479  3494 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 11:19:14.501513  3494 solver.cpp:237] Iteration 360, loss = 2.09035
I0521 11:19:14.501550  3494 solver.cpp:253]     Train net output #0: loss = 2.09035 (* 1 = 2.09035 loss)
I0521 11:19:14.501571  3494 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 11:19:22.439669  3494 solver.cpp:237] Iteration 375, loss = 2.06703
I0521 11:19:22.439702  3494 solver.cpp:253]     Train net output #0: loss = 2.06703 (* 1 = 2.06703 loss)
I0521 11:19:22.439718  3494 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 11:19:30.379204  3494 solver.cpp:237] Iteration 390, loss = 2.0821
I0521 11:19:30.379334  3494 solver.cpp:253]     Train net output #0: loss = 2.0821 (* 1 = 2.0821 loss)
I0521 11:19:30.379348  3494 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 11:20:00.445546  3494 solver.cpp:237] Iteration 405, loss = 2.04183
I0521 11:20:00.445703  3494 solver.cpp:253]     Train net output #0: loss = 2.04183 (* 1 = 2.04183 loss)
I0521 11:20:00.445719  3494 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 11:20:08.387187  3494 solver.cpp:237] Iteration 420, loss = 2.03657
I0521 11:20:08.387226  3494 solver.cpp:253]     Train net output #0: loss = 2.03657 (* 1 = 2.03657 loss)
I0521 11:20:08.387248  3494 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 11:20:16.330623  3494 solver.cpp:237] Iteration 435, loss = 2.04363
I0521 11:20:16.330654  3494 solver.cpp:253]     Train net output #0: loss = 2.04363 (* 1 = 2.04363 loss)
I0521 11:20:16.330667  3494 sgd_solver.cpp:106] Iteration 435, lr = 0.0025
I0521 11:20:24.279832  3494 solver.cpp:237] Iteration 450, loss = 1.99048
I0521 11:20:24.279865  3494 solver.cpp:253]     Train net output #0: loss = 1.99048 (* 1 = 1.99048 loss)
I0521 11:20:24.279881  3494 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 11:20:25.340761  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_453.caffemodel
I0521 11:20:25.752249  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_453.solverstate
I0521 11:20:32.287019  3494 solver.cpp:237] Iteration 465, loss = 1.97042
I0521 11:20:32.287194  3494 solver.cpp:253]     Train net output #0: loss = 1.97042 (* 1 = 1.97042 loss)
I0521 11:20:32.287209  3494 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0521 11:20:40.225705  3494 solver.cpp:237] Iteration 480, loss = 2.0013
I0521 11:20:40.225738  3494 solver.cpp:253]     Train net output #0: loss = 2.0013 (* 1 = 2.0013 loss)
I0521 11:20:40.225755  3494 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 11:20:48.167724  3494 solver.cpp:237] Iteration 495, loss = 1.96658
I0521 11:20:48.167757  3494 solver.cpp:253]     Train net output #0: loss = 1.96658 (* 1 = 1.96658 loss)
I0521 11:20:48.167774  3494 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0521 11:21:18.272953  3494 solver.cpp:237] Iteration 510, loss = 1.97222
I0521 11:21:18.273138  3494 solver.cpp:253]     Train net output #0: loss = 1.97222 (* 1 = 1.97222 loss)
I0521 11:21:18.273152  3494 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 11:21:26.219187  3494 solver.cpp:237] Iteration 525, loss = 1.96136
I0521 11:21:26.219230  3494 solver.cpp:253]     Train net output #0: loss = 1.96136 (* 1 = 1.96136 loss)
I0521 11:21:26.219249  3494 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 11:21:34.160435  3494 solver.cpp:237] Iteration 540, loss = 1.93848
I0521 11:21:34.160466  3494 solver.cpp:253]     Train net output #0: loss = 1.93848 (* 1 = 1.93848 loss)
I0521 11:21:34.160485  3494 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 11:21:42.102022  3494 solver.cpp:237] Iteration 555, loss = 1.96064
I0521 11:21:42.102054  3494 solver.cpp:253]     Train net output #0: loss = 1.96064 (* 1 = 1.96064 loss)
I0521 11:21:42.102072  3494 sgd_solver.cpp:106] Iteration 555, lr = 0.0025
I0521 11:21:50.036808  3494 solver.cpp:237] Iteration 570, loss = 1.9233
I0521 11:21:50.036947  3494 solver.cpp:253]     Train net output #0: loss = 1.9233 (* 1 = 1.9233 loss)
I0521 11:21:50.036967  3494 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 11:21:57.977293  3494 solver.cpp:237] Iteration 585, loss = 1.96569
I0521 11:21:57.977325  3494 solver.cpp:253]     Train net output #0: loss = 1.96569 (* 1 = 1.96569 loss)
I0521 11:21:57.977344  3494 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0521 11:22:05.920281  3494 solver.cpp:237] Iteration 600, loss = 1.88977
I0521 11:22:05.920312  3494 solver.cpp:253]     Train net output #0: loss = 1.88977 (* 1 = 1.88977 loss)
I0521 11:22:05.920331  3494 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 11:22:07.507951  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_604.caffemodel
I0521 11:22:07.920088  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_604.solverstate
I0521 11:22:08.634605  3494 solver.cpp:341] Iteration 606, Testing net (#0)
I0521 11:23:14.889534  3494 solver.cpp:409]     Test net output #0: accuracy = 0.570827
I0521 11:23:14.889708  3494 solver.cpp:409]     Test net output #1: loss = 1.53367 (* 1 = 1.53367 loss)
I0521 11:23:41.977313  3494 solver.cpp:237] Iteration 615, loss = 1.91244
I0521 11:23:41.977362  3494 solver.cpp:253]     Train net output #0: loss = 1.91244 (* 1 = 1.91244 loss)
I0521 11:23:41.977380  3494 sgd_solver.cpp:106] Iteration 615, lr = 0.0025
I0521 11:23:49.914880  3494 solver.cpp:237] Iteration 630, loss = 1.93373
I0521 11:23:49.915024  3494 solver.cpp:253]     Train net output #0: loss = 1.93373 (* 1 = 1.93373 loss)
I0521 11:23:49.915037  3494 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 11:23:57.850914  3494 solver.cpp:237] Iteration 645, loss = 1.93022
I0521 11:23:57.850945  3494 solver.cpp:253]     Train net output #0: loss = 1.93022 (* 1 = 1.93022 loss)
I0521 11:23:57.850965  3494 sgd_solver.cpp:106] Iteration 645, lr = 0.0025
I0521 11:24:05.787467  3494 solver.cpp:237] Iteration 660, loss = 1.8766
I0521 11:24:05.787497  3494 solver.cpp:253]     Train net output #0: loss = 1.8766 (* 1 = 1.8766 loss)
I0521 11:24:05.787514  3494 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 11:24:13.725824  3494 solver.cpp:237] Iteration 675, loss = 1.93598
I0521 11:24:13.725864  3494 solver.cpp:253]     Train net output #0: loss = 1.93598 (* 1 = 1.93598 loss)
I0521 11:24:13.725881  3494 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 11:24:21.663121  3494 solver.cpp:237] Iteration 690, loss = 1.81348
I0521 11:24:21.663266  3494 solver.cpp:253]     Train net output #0: loss = 1.81348 (* 1 = 1.81348 loss)
I0521 11:24:21.663280  3494 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 11:24:29.603056  3494 solver.cpp:237] Iteration 705, loss = 1.88208
I0521 11:24:29.603082  3494 solver.cpp:253]     Train net output #0: loss = 1.88208 (* 1 = 1.88208 loss)
I0521 11:24:29.603097  3494 sgd_solver.cpp:106] Iteration 705, lr = 0.0025
I0521 11:24:59.670672  3494 solver.cpp:237] Iteration 720, loss = 1.90635
I0521 11:24:59.670830  3494 solver.cpp:253]     Train net output #0: loss = 1.90635 (* 1 = 1.90635 loss)
I0521 11:24:59.670846  3494 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 11:25:07.611182  3494 solver.cpp:237] Iteration 735, loss = 1.86324
I0521 11:25:07.611217  3494 solver.cpp:253]     Train net output #0: loss = 1.86324 (* 1 = 1.86324 loss)
I0521 11:25:07.611235  3494 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0521 11:25:15.551784  3494 solver.cpp:237] Iteration 750, loss = 1.87737
I0521 11:25:15.551816  3494 solver.cpp:253]     Train net output #0: loss = 1.87737 (* 1 = 1.87737 loss)
I0521 11:25:15.551833  3494 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 11:25:17.669596  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_755.caffemodel
I0521 11:25:18.081830  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_755.solverstate
I0521 11:25:23.557781  3494 solver.cpp:237] Iteration 765, loss = 1.8611
I0521 11:25:23.557826  3494 solver.cpp:253]     Train net output #0: loss = 1.8611 (* 1 = 1.8611 loss)
I0521 11:25:23.557847  3494 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 11:25:31.496357  3494 solver.cpp:237] Iteration 780, loss = 1.74226
I0521 11:25:31.496498  3494 solver.cpp:253]     Train net output #0: loss = 1.74226 (* 1 = 1.74226 loss)
I0521 11:25:31.496511  3494 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 11:25:39.436756  3494 solver.cpp:237] Iteration 795, loss = 1.77615
I0521 11:25:39.436787  3494 solver.cpp:253]     Train net output #0: loss = 1.77615 (* 1 = 1.77615 loss)
I0521 11:25:39.436805  3494 sgd_solver.cpp:106] Iteration 795, lr = 0.0025
I0521 11:26:09.485680  3494 solver.cpp:237] Iteration 810, loss = 1.83013
I0521 11:26:09.485858  3494 solver.cpp:253]     Train net output #0: loss = 1.83013 (* 1 = 1.83013 loss)
I0521 11:26:09.485874  3494 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 11:26:17.427956  3494 solver.cpp:237] Iteration 825, loss = 1.86137
I0521 11:26:17.427988  3494 solver.cpp:253]     Train net output #0: loss = 1.86137 (* 1 = 1.86137 loss)
I0521 11:26:17.428005  3494 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 11:26:25.365355  3494 solver.cpp:237] Iteration 840, loss = 1.85231
I0521 11:26:25.365381  3494 solver.cpp:253]     Train net output #0: loss = 1.85231 (* 1 = 1.85231 loss)
I0521 11:26:25.365398  3494 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 11:26:33.301837  3494 solver.cpp:237] Iteration 855, loss = 1.7736
I0521 11:26:33.301869  3494 solver.cpp:253]     Train net output #0: loss = 1.7736 (* 1 = 1.7736 loss)
I0521 11:26:33.301887  3494 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 11:26:41.240016  3494 solver.cpp:237] Iteration 870, loss = 1.80692
I0521 11:26:41.240159  3494 solver.cpp:253]     Train net output #0: loss = 1.80692 (* 1 = 1.80692 loss)
I0521 11:26:41.240173  3494 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 11:26:49.178396  3494 solver.cpp:237] Iteration 885, loss = 1.93114
I0521 11:26:49.178426  3494 solver.cpp:253]     Train net output #0: loss = 1.93114 (* 1 = 1.93114 loss)
I0521 11:26:49.178442  3494 sgd_solver.cpp:106] Iteration 885, lr = 0.0025
I0521 11:26:57.119338  3494 solver.cpp:237] Iteration 900, loss = 1.83896
I0521 11:26:57.119369  3494 solver.cpp:253]     Train net output #0: loss = 1.83896 (* 1 = 1.83896 loss)
I0521 11:26:57.119386  3494 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 11:26:59.766849  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_906.caffemodel
I0521 11:27:00.177716  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_906.solverstate
I0521 11:27:01.418462  3494 solver.cpp:341] Iteration 909, Testing net (#0)
I0521 11:27:46.397845  3494 solver.cpp:409]     Test net output #0: accuracy = 0.608482
I0521 11:27:46.398011  3494 solver.cpp:409]     Test net output #1: loss = 1.37137 (* 1 = 1.37137 loss)
I0521 11:28:11.892640  3494 solver.cpp:237] Iteration 915, loss = 1.8462
I0521 11:28:11.892690  3494 solver.cpp:253]     Train net output #0: loss = 1.8462 (* 1 = 1.8462 loss)
I0521 11:28:11.892706  3494 sgd_solver.cpp:106] Iteration 915, lr = 0.0025
I0521 11:28:19.831393  3494 solver.cpp:237] Iteration 930, loss = 1.78273
I0521 11:28:19.831540  3494 solver.cpp:253]     Train net output #0: loss = 1.78273 (* 1 = 1.78273 loss)
I0521 11:28:19.831553  3494 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 11:28:27.768457  3494 solver.cpp:237] Iteration 945, loss = 1.79091
I0521 11:28:27.768499  3494 solver.cpp:253]     Train net output #0: loss = 1.79091 (* 1 = 1.79091 loss)
I0521 11:28:27.768515  3494 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 11:28:35.711585  3494 solver.cpp:237] Iteration 960, loss = 1.93633
I0521 11:28:35.711616  3494 solver.cpp:253]     Train net output #0: loss = 1.93633 (* 1 = 1.93633 loss)
I0521 11:28:35.711632  3494 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 11:28:43.652014  3494 solver.cpp:237] Iteration 975, loss = 1.74787
I0521 11:28:43.652045  3494 solver.cpp:253]     Train net output #0: loss = 1.74787 (* 1 = 1.74787 loss)
I0521 11:28:43.652061  3494 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 11:28:51.597071  3494 solver.cpp:237] Iteration 990, loss = 1.74441
I0521 11:28:51.597220  3494 solver.cpp:253]     Train net output #0: loss = 1.74441 (* 1 = 1.74441 loss)
I0521 11:28:51.597234  3494 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 11:28:59.536378  3494 solver.cpp:237] Iteration 1005, loss = 1.77387
I0521 11:28:59.536409  3494 solver.cpp:253]     Train net output #0: loss = 1.77387 (* 1 = 1.77387 loss)
I0521 11:28:59.536427  3494 sgd_solver.cpp:106] Iteration 1005, lr = 0.0025
I0521 11:29:29.603953  3494 solver.cpp:237] Iteration 1020, loss = 1.76865
I0521 11:29:29.604123  3494 solver.cpp:253]     Train net output #0: loss = 1.76865 (* 1 = 1.76865 loss)
I0521 11:29:29.604140  3494 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 11:29:37.539295  3494 solver.cpp:237] Iteration 1035, loss = 1.8171
I0521 11:29:37.539326  3494 solver.cpp:253]     Train net output #0: loss = 1.8171 (* 1 = 1.8171 loss)
I0521 11:29:37.539345  3494 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 11:29:45.485255  3494 solver.cpp:237] Iteration 1050, loss = 1.79383
I0521 11:29:45.485292  3494 solver.cpp:253]     Train net output #0: loss = 1.79383 (* 1 = 1.79383 loss)
I0521 11:29:45.485311  3494 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 11:29:48.659746  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1057.caffemodel
I0521 11:29:49.069099  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1057.solverstate
I0521 11:29:53.486112  3494 solver.cpp:237] Iteration 1065, loss = 1.86204
I0521 11:29:53.486156  3494 solver.cpp:253]     Train net output #0: loss = 1.86204 (* 1 = 1.86204 loss)
I0521 11:29:53.486172  3494 sgd_solver.cpp:106] Iteration 1065, lr = 0.0025
I0521 11:30:01.426193  3494 solver.cpp:237] Iteration 1080, loss = 1.78447
I0521 11:30:01.426336  3494 solver.cpp:253]     Train net output #0: loss = 1.78447 (* 1 = 1.78447 loss)
I0521 11:30:01.426349  3494 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 11:30:09.368106  3494 solver.cpp:237] Iteration 1095, loss = 1.77797
I0521 11:30:09.368149  3494 solver.cpp:253]     Train net output #0: loss = 1.77797 (* 1 = 1.77797 loss)
I0521 11:30:09.368163  3494 sgd_solver.cpp:106] Iteration 1095, lr = 0.0025
I0521 11:30:17.313020  3494 solver.cpp:237] Iteration 1110, loss = 1.78054
I0521 11:30:17.313051  3494 solver.cpp:253]     Train net output #0: loss = 1.78054 (* 1 = 1.78054 loss)
I0521 11:30:17.313069  3494 sgd_solver.cpp:106] Iteration 1110, lr = 0.0025
I0521 11:30:47.413302  3494 solver.cpp:237] Iteration 1125, loss = 1.76156
I0521 11:30:47.413465  3494 solver.cpp:253]     Train net output #0: loss = 1.76156 (* 1 = 1.76156 loss)
I0521 11:30:47.413480  3494 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 11:30:55.354600  3494 solver.cpp:237] Iteration 1140, loss = 1.75782
I0521 11:30:55.354640  3494 solver.cpp:253]     Train net output #0: loss = 1.75782 (* 1 = 1.75782 loss)
I0521 11:30:55.354660  3494 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 11:31:03.291926  3494 solver.cpp:237] Iteration 1155, loss = 1.73441
I0521 11:31:03.291959  3494 solver.cpp:253]     Train net output #0: loss = 1.73441 (* 1 = 1.73441 loss)
I0521 11:31:03.291975  3494 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 11:31:11.239423  3494 solver.cpp:237] Iteration 1170, loss = 1.74307
I0521 11:31:11.239456  3494 solver.cpp:253]     Train net output #0: loss = 1.74307 (* 1 = 1.74307 loss)
I0521 11:31:11.239472  3494 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 11:31:19.178386  3494 solver.cpp:237] Iteration 1185, loss = 1.71327
I0521 11:31:19.178537  3494 solver.cpp:253]     Train net output #0: loss = 1.71327 (* 1 = 1.71327 loss)
I0521 11:31:19.178551  3494 sgd_solver.cpp:106] Iteration 1185, lr = 0.0025
I0521 11:31:27.118068  3494 solver.cpp:237] Iteration 1200, loss = 1.78457
I0521 11:31:27.118099  3494 solver.cpp:253]     Train net output #0: loss = 1.78457 (* 1 = 1.78457 loss)
I0521 11:31:27.118118  3494 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 11:31:30.822305  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1208.caffemodel
I0521 11:31:31.233695  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1208.solverstate
I0521 11:31:33.005620  3494 solver.cpp:341] Iteration 1212, Testing net (#0)
I0521 11:32:39.103827  3494 solver.cpp:409]     Test net output #0: accuracy = 0.639929
I0521 11:32:39.103999  3494 solver.cpp:409]     Test net output #1: loss = 1.27811 (* 1 = 1.27811 loss)
I0521 11:33:03.007699  3494 solver.cpp:237] Iteration 1215, loss = 1.90005
I0521 11:33:03.007750  3494 solver.cpp:253]     Train net output #0: loss = 1.90005 (* 1 = 1.90005 loss)
I0521 11:33:03.007764  3494 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 11:33:10.952790  3494 solver.cpp:237] Iteration 1230, loss = 1.73286
I0521 11:33:10.952934  3494 solver.cpp:253]     Train net output #0: loss = 1.73286 (* 1 = 1.73286 loss)
I0521 11:33:10.952946  3494 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0521 11:33:18.894467  3494 solver.cpp:237] Iteration 1245, loss = 1.80191
I0521 11:33:18.894498  3494 solver.cpp:253]     Train net output #0: loss = 1.80191 (* 1 = 1.80191 loss)
I0521 11:33:18.894516  3494 sgd_solver.cpp:106] Iteration 1245, lr = 0.0025
I0521 11:33:26.830929  3494 solver.cpp:237] Iteration 1260, loss = 1.82598
I0521 11:33:26.830966  3494 solver.cpp:253]     Train net output #0: loss = 1.82598 (* 1 = 1.82598 loss)
I0521 11:33:26.830983  3494 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 11:33:34.767756  3494 solver.cpp:237] Iteration 1275, loss = 1.69917
I0521 11:33:34.767788  3494 solver.cpp:253]     Train net output #0: loss = 1.69917 (* 1 = 1.69917 loss)
I0521 11:33:34.767802  3494 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 11:33:42.705936  3494 solver.cpp:237] Iteration 1290, loss = 1.73387
I0521 11:33:42.706073  3494 solver.cpp:253]     Train net output #0: loss = 1.73387 (* 1 = 1.73387 loss)
I0521 11:33:42.706085  3494 sgd_solver.cpp:106] Iteration 1290, lr = 0.0025
I0521 11:33:50.646345  3494 solver.cpp:237] Iteration 1305, loss = 1.68396
I0521 11:33:50.646374  3494 solver.cpp:253]     Train net output #0: loss = 1.68396 (* 1 = 1.68396 loss)
I0521 11:33:50.646391  3494 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0521 11:34:20.748672  3494 solver.cpp:237] Iteration 1320, loss = 1.73555
I0521 11:34:20.748849  3494 solver.cpp:253]     Train net output #0: loss = 1.73555 (* 1 = 1.73555 loss)
I0521 11:34:20.748864  3494 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 11:34:28.688725  3494 solver.cpp:237] Iteration 1335, loss = 1.70891
I0521 11:34:28.688757  3494 solver.cpp:253]     Train net output #0: loss = 1.70891 (* 1 = 1.70891 loss)
I0521 11:34:28.688774  3494 sgd_solver.cpp:106] Iteration 1335, lr = 0.0025
I0521 11:34:36.628540  3494 solver.cpp:237] Iteration 1350, loss = 1.65695
I0521 11:34:36.628571  3494 solver.cpp:253]     Train net output #0: loss = 1.65695 (* 1 = 1.65695 loss)
I0521 11:34:36.628587  3494 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 11:34:40.866605  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1359.caffemodel
I0521 11:34:41.283452  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1359.solverstate
I0521 11:34:44.643748  3494 solver.cpp:237] Iteration 1365, loss = 1.70232
I0521 11:34:44.643797  3494 solver.cpp:253]     Train net output #0: loss = 1.70232 (* 1 = 1.70232 loss)
I0521 11:34:44.643811  3494 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0521 11:34:52.587399  3494 solver.cpp:237] Iteration 1380, loss = 1.74287
I0521 11:34:52.587556  3494 solver.cpp:253]     Train net output #0: loss = 1.74287 (* 1 = 1.74287 loss)
I0521 11:34:52.587569  3494 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 11:35:00.534250  3494 solver.cpp:237] Iteration 1395, loss = 1.69574
I0521 11:35:00.534281  3494 solver.cpp:253]     Train net output #0: loss = 1.69574 (* 1 = 1.69574 loss)
I0521 11:35:00.534296  3494 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0521 11:35:08.473249  3494 solver.cpp:237] Iteration 1410, loss = 1.71883
I0521 11:35:08.473284  3494 solver.cpp:253]     Train net output #0: loss = 1.71883 (* 1 = 1.71883 loss)
I0521 11:35:08.473302  3494 sgd_solver.cpp:106] Iteration 1410, lr = 0.0025
I0521 11:35:38.522721  3494 solver.cpp:237] Iteration 1425, loss = 1.67452
I0521 11:35:38.522886  3494 solver.cpp:253]     Train net output #0: loss = 1.67452 (* 1 = 1.67452 loss)
I0521 11:35:38.522902  3494 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 11:35:46.463822  3494 solver.cpp:237] Iteration 1440, loss = 1.71662
I0521 11:35:46.463852  3494 solver.cpp:253]     Train net output #0: loss = 1.71662 (* 1 = 1.71662 loss)
I0521 11:35:46.463871  3494 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 11:35:54.406479  3494 solver.cpp:237] Iteration 1455, loss = 1.71716
I0521 11:35:54.406525  3494 solver.cpp:253]     Train net output #0: loss = 1.71716 (* 1 = 1.71716 loss)
I0521 11:35:54.406540  3494 sgd_solver.cpp:106] Iteration 1455, lr = 0.0025
I0521 11:36:02.345821  3494 solver.cpp:237] Iteration 1470, loss = 1.67649
I0521 11:36:02.345852  3494 solver.cpp:253]     Train net output #0: loss = 1.67649 (* 1 = 1.67649 loss)
I0521 11:36:02.345870  3494 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 11:36:10.296001  3494 solver.cpp:237] Iteration 1485, loss = 1.74009
I0521 11:36:10.296142  3494 solver.cpp:253]     Train net output #0: loss = 1.74009 (* 1 = 1.74009 loss)
I0521 11:36:10.296156  3494 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 11:36:18.234516  3494 solver.cpp:237] Iteration 1500, loss = 1.70521
I0521 11:36:18.234547  3494 solver.cpp:253]     Train net output #0: loss = 1.70521 (* 1 = 1.70521 loss)
I0521 11:36:18.234566  3494 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 11:36:22.998607  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1510.caffemodel
I0521 11:36:23.410542  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1510.solverstate
I0521 11:36:25.715972  3494 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1515.caffemodel
I0521 11:36:26.127751  3494 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002_iter_1515.solverstate
I0521 11:36:47.178601  3494 solver.cpp:321] Iteration 1515, loss = 1.63136
I0521 11:36:47.178762  3494 solver.cpp:341] Iteration 1515, Testing net (#0)
I0521 11:37:32.096797  3494 solver.cpp:409]     Test net output #0: accuracy = 0.665061
I0521 11:37:32.096967  3494 solver.cpp:409]     Test net output #1: loss = 1.17323 (* 1 = 1.17323 loss)
I0521 11:37:32.096982  3494 solver.cpp:326] Optimization Done.
I0521 11:37:32.096992  3494 caffe.cpp:215] Optimization Done.
Application 11237807 resources: utime ~1264s, stime ~227s, Rss ~5332476, inblocks ~3744348, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_990_2016-05-20T11.21.08.786002.solver"
	User time (seconds): 0.58
	System time (seconds): 0.10
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:54.99
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2722
	Involuntary context switches: 117
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
