2806199
I0521 02:57:59.210850 29613 caffe.cpp:184] Using GPUs 0
I0521 02:57:59.662511 29613 solver.cpp:48] Initializing solver from parameters: 
test_iter: 254
test_interval: 508
base_lr: 0.0025
display: 25
max_iter: 2542
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 254
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944.prototxt"
I0521 02:57:59.664279 29613 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944.prototxt
I0521 02:57:59.681036 29613 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 02:57:59.681097 29613 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 02:57:59.681443 29613 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 590
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:57:59.681622 29613 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:57:59.681645 29613 net.cpp:106] Creating Layer data_hdf5
I0521 02:57:59.681659 29613 net.cpp:411] data_hdf5 -> data
I0521 02:57:59.681694 29613 net.cpp:411] data_hdf5 -> label
I0521 02:57:59.681726 29613 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 02:57:59.682916 29613 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 02:57:59.685057 29613 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 02:58:21.204646 29613 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 02:58:21.209756 29613 net.cpp:150] Setting up data_hdf5
I0521 02:58:21.209796 29613 net.cpp:157] Top shape: 590 1 127 50 (3746500)
I0521 02:58:21.209811 29613 net.cpp:157] Top shape: 590 (590)
I0521 02:58:21.209821 29613 net.cpp:165] Memory required for data: 14988360
I0521 02:58:21.209835 29613 layer_factory.hpp:77] Creating layer conv1
I0521 02:58:21.209867 29613 net.cpp:106] Creating Layer conv1
I0521 02:58:21.209879 29613 net.cpp:454] conv1 <- data
I0521 02:58:21.209903 29613 net.cpp:411] conv1 -> conv1
I0521 02:58:22.457074 29613 net.cpp:150] Setting up conv1
I0521 02:58:22.457120 29613 net.cpp:157] Top shape: 590 12 120 48 (40780800)
I0521 02:58:22.457134 29613 net.cpp:165] Memory required for data: 178111560
I0521 02:58:22.457166 29613 layer_factory.hpp:77] Creating layer relu1
I0521 02:58:22.457188 29613 net.cpp:106] Creating Layer relu1
I0521 02:58:22.457200 29613 net.cpp:454] relu1 <- conv1
I0521 02:58:22.457213 29613 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:58:22.457726 29613 net.cpp:150] Setting up relu1
I0521 02:58:22.457742 29613 net.cpp:157] Top shape: 590 12 120 48 (40780800)
I0521 02:58:22.457753 29613 net.cpp:165] Memory required for data: 341234760
I0521 02:58:22.457764 29613 layer_factory.hpp:77] Creating layer pool1
I0521 02:58:22.457780 29613 net.cpp:106] Creating Layer pool1
I0521 02:58:22.457790 29613 net.cpp:454] pool1 <- conv1
I0521 02:58:22.457804 29613 net.cpp:411] pool1 -> pool1
I0521 02:58:22.457885 29613 net.cpp:150] Setting up pool1
I0521 02:58:22.457900 29613 net.cpp:157] Top shape: 590 12 60 48 (20390400)
I0521 02:58:22.457908 29613 net.cpp:165] Memory required for data: 422796360
I0521 02:58:22.457919 29613 layer_factory.hpp:77] Creating layer conv2
I0521 02:58:22.457942 29613 net.cpp:106] Creating Layer conv2
I0521 02:58:22.457952 29613 net.cpp:454] conv2 <- pool1
I0521 02:58:22.457964 29613 net.cpp:411] conv2 -> conv2
I0521 02:58:22.460635 29613 net.cpp:150] Setting up conv2
I0521 02:58:22.460664 29613 net.cpp:157] Top shape: 590 20 54 46 (29311200)
I0521 02:58:22.460675 29613 net.cpp:165] Memory required for data: 540041160
I0521 02:58:22.460693 29613 layer_factory.hpp:77] Creating layer relu2
I0521 02:58:22.460707 29613 net.cpp:106] Creating Layer relu2
I0521 02:58:22.460717 29613 net.cpp:454] relu2 <- conv2
I0521 02:58:22.460731 29613 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:58:22.461062 29613 net.cpp:150] Setting up relu2
I0521 02:58:22.461076 29613 net.cpp:157] Top shape: 590 20 54 46 (29311200)
I0521 02:58:22.461087 29613 net.cpp:165] Memory required for data: 657285960
I0521 02:58:22.461097 29613 layer_factory.hpp:77] Creating layer pool2
I0521 02:58:22.461112 29613 net.cpp:106] Creating Layer pool2
I0521 02:58:22.461122 29613 net.cpp:454] pool2 <- conv2
I0521 02:58:22.461146 29613 net.cpp:411] pool2 -> pool2
I0521 02:58:22.461215 29613 net.cpp:150] Setting up pool2
I0521 02:58:22.461230 29613 net.cpp:157] Top shape: 590 20 27 46 (14655600)
I0521 02:58:22.461239 29613 net.cpp:165] Memory required for data: 715908360
I0521 02:58:22.461248 29613 layer_factory.hpp:77] Creating layer conv3
I0521 02:58:22.461267 29613 net.cpp:106] Creating Layer conv3
I0521 02:58:22.461277 29613 net.cpp:454] conv3 <- pool2
I0521 02:58:22.461290 29613 net.cpp:411] conv3 -> conv3
I0521 02:58:22.463207 29613 net.cpp:150] Setting up conv3
I0521 02:58:22.463229 29613 net.cpp:157] Top shape: 590 28 22 44 (15991360)
I0521 02:58:22.463243 29613 net.cpp:165] Memory required for data: 779873800
I0521 02:58:22.463260 29613 layer_factory.hpp:77] Creating layer relu3
I0521 02:58:22.463276 29613 net.cpp:106] Creating Layer relu3
I0521 02:58:22.463286 29613 net.cpp:454] relu3 <- conv3
I0521 02:58:22.463299 29613 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:58:22.463768 29613 net.cpp:150] Setting up relu3
I0521 02:58:22.463784 29613 net.cpp:157] Top shape: 590 28 22 44 (15991360)
I0521 02:58:22.463794 29613 net.cpp:165] Memory required for data: 843839240
I0521 02:58:22.463805 29613 layer_factory.hpp:77] Creating layer pool3
I0521 02:58:22.463819 29613 net.cpp:106] Creating Layer pool3
I0521 02:58:22.463827 29613 net.cpp:454] pool3 <- conv3
I0521 02:58:22.463841 29613 net.cpp:411] pool3 -> pool3
I0521 02:58:22.463909 29613 net.cpp:150] Setting up pool3
I0521 02:58:22.463922 29613 net.cpp:157] Top shape: 590 28 11 44 (7995680)
I0521 02:58:22.463932 29613 net.cpp:165] Memory required for data: 875821960
I0521 02:58:22.463939 29613 layer_factory.hpp:77] Creating layer conv4
I0521 02:58:22.463958 29613 net.cpp:106] Creating Layer conv4
I0521 02:58:22.463975 29613 net.cpp:454] conv4 <- pool3
I0521 02:58:22.463989 29613 net.cpp:411] conv4 -> conv4
I0521 02:58:22.466749 29613 net.cpp:150] Setting up conv4
I0521 02:58:22.466779 29613 net.cpp:157] Top shape: 590 36 6 42 (5352480)
I0521 02:58:22.466789 29613 net.cpp:165] Memory required for data: 897231880
I0521 02:58:22.466805 29613 layer_factory.hpp:77] Creating layer relu4
I0521 02:58:22.466820 29613 net.cpp:106] Creating Layer relu4
I0521 02:58:22.466830 29613 net.cpp:454] relu4 <- conv4
I0521 02:58:22.466842 29613 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:58:22.467313 29613 net.cpp:150] Setting up relu4
I0521 02:58:22.467329 29613 net.cpp:157] Top shape: 590 36 6 42 (5352480)
I0521 02:58:22.467339 29613 net.cpp:165] Memory required for data: 918641800
I0521 02:58:22.467350 29613 layer_factory.hpp:77] Creating layer pool4
I0521 02:58:22.467362 29613 net.cpp:106] Creating Layer pool4
I0521 02:58:22.467372 29613 net.cpp:454] pool4 <- conv4
I0521 02:58:22.467384 29613 net.cpp:411] pool4 -> pool4
I0521 02:58:22.467453 29613 net.cpp:150] Setting up pool4
I0521 02:58:22.467468 29613 net.cpp:157] Top shape: 590 36 3 42 (2676240)
I0521 02:58:22.467478 29613 net.cpp:165] Memory required for data: 929346760
I0521 02:58:22.467489 29613 layer_factory.hpp:77] Creating layer ip1
I0521 02:58:22.467509 29613 net.cpp:106] Creating Layer ip1
I0521 02:58:22.467519 29613 net.cpp:454] ip1 <- pool4
I0521 02:58:22.467531 29613 net.cpp:411] ip1 -> ip1
I0521 02:58:22.482929 29613 net.cpp:150] Setting up ip1
I0521 02:58:22.482959 29613 net.cpp:157] Top shape: 590 196 (115640)
I0521 02:58:22.482972 29613 net.cpp:165] Memory required for data: 929809320
I0521 02:58:22.482993 29613 layer_factory.hpp:77] Creating layer relu5
I0521 02:58:22.483009 29613 net.cpp:106] Creating Layer relu5
I0521 02:58:22.483019 29613 net.cpp:454] relu5 <- ip1
I0521 02:58:22.483033 29613 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:58:22.483372 29613 net.cpp:150] Setting up relu5
I0521 02:58:22.483387 29613 net.cpp:157] Top shape: 590 196 (115640)
I0521 02:58:22.483397 29613 net.cpp:165] Memory required for data: 930271880
I0521 02:58:22.483408 29613 layer_factory.hpp:77] Creating layer drop1
I0521 02:58:22.483429 29613 net.cpp:106] Creating Layer drop1
I0521 02:58:22.483440 29613 net.cpp:454] drop1 <- ip1
I0521 02:58:22.483467 29613 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:58:22.483515 29613 net.cpp:150] Setting up drop1
I0521 02:58:22.483530 29613 net.cpp:157] Top shape: 590 196 (115640)
I0521 02:58:22.483539 29613 net.cpp:165] Memory required for data: 930734440
I0521 02:58:22.483549 29613 layer_factory.hpp:77] Creating layer ip2
I0521 02:58:22.483567 29613 net.cpp:106] Creating Layer ip2
I0521 02:58:22.483578 29613 net.cpp:454] ip2 <- ip1
I0521 02:58:22.483592 29613 net.cpp:411] ip2 -> ip2
I0521 02:58:22.484066 29613 net.cpp:150] Setting up ip2
I0521 02:58:22.484078 29613 net.cpp:157] Top shape: 590 98 (57820)
I0521 02:58:22.484088 29613 net.cpp:165] Memory required for data: 930965720
I0521 02:58:22.484104 29613 layer_factory.hpp:77] Creating layer relu6
I0521 02:58:22.484117 29613 net.cpp:106] Creating Layer relu6
I0521 02:58:22.484127 29613 net.cpp:454] relu6 <- ip2
I0521 02:58:22.484138 29613 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:58:22.484661 29613 net.cpp:150] Setting up relu6
I0521 02:58:22.484678 29613 net.cpp:157] Top shape: 590 98 (57820)
I0521 02:58:22.484688 29613 net.cpp:165] Memory required for data: 931197000
I0521 02:58:22.484699 29613 layer_factory.hpp:77] Creating layer drop2
I0521 02:58:22.484711 29613 net.cpp:106] Creating Layer drop2
I0521 02:58:22.484721 29613 net.cpp:454] drop2 <- ip2
I0521 02:58:22.484733 29613 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:58:22.484776 29613 net.cpp:150] Setting up drop2
I0521 02:58:22.484788 29613 net.cpp:157] Top shape: 590 98 (57820)
I0521 02:58:22.484799 29613 net.cpp:165] Memory required for data: 931428280
I0521 02:58:22.484809 29613 layer_factory.hpp:77] Creating layer ip3
I0521 02:58:22.484823 29613 net.cpp:106] Creating Layer ip3
I0521 02:58:22.484833 29613 net.cpp:454] ip3 <- ip2
I0521 02:58:22.484845 29613 net.cpp:411] ip3 -> ip3
I0521 02:58:22.485055 29613 net.cpp:150] Setting up ip3
I0521 02:58:22.485069 29613 net.cpp:157] Top shape: 590 11 (6490)
I0521 02:58:22.485079 29613 net.cpp:165] Memory required for data: 931454240
I0521 02:58:22.485093 29613 layer_factory.hpp:77] Creating layer drop3
I0521 02:58:22.485106 29613 net.cpp:106] Creating Layer drop3
I0521 02:58:22.485116 29613 net.cpp:454] drop3 <- ip3
I0521 02:58:22.485128 29613 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:58:22.485167 29613 net.cpp:150] Setting up drop3
I0521 02:58:22.485180 29613 net.cpp:157] Top shape: 590 11 (6490)
I0521 02:58:22.485190 29613 net.cpp:165] Memory required for data: 931480200
I0521 02:58:22.485200 29613 layer_factory.hpp:77] Creating layer loss
I0521 02:58:22.485219 29613 net.cpp:106] Creating Layer loss
I0521 02:58:22.485229 29613 net.cpp:454] loss <- ip3
I0521 02:58:22.485240 29613 net.cpp:454] loss <- label
I0521 02:58:22.485254 29613 net.cpp:411] loss -> loss
I0521 02:58:22.485270 29613 layer_factory.hpp:77] Creating layer loss
I0521 02:58:22.485913 29613 net.cpp:150] Setting up loss
I0521 02:58:22.485931 29613 net.cpp:157] Top shape: (1)
I0521 02:58:22.485941 29613 net.cpp:160]     with loss weight 1
I0521 02:58:22.485986 29613 net.cpp:165] Memory required for data: 931480204
I0521 02:58:22.485996 29613 net.cpp:226] loss needs backward computation.
I0521 02:58:22.486007 29613 net.cpp:226] drop3 needs backward computation.
I0521 02:58:22.486016 29613 net.cpp:226] ip3 needs backward computation.
I0521 02:58:22.486027 29613 net.cpp:226] drop2 needs backward computation.
I0521 02:58:22.486037 29613 net.cpp:226] relu6 needs backward computation.
I0521 02:58:22.486047 29613 net.cpp:226] ip2 needs backward computation.
I0521 02:58:22.486057 29613 net.cpp:226] drop1 needs backward computation.
I0521 02:58:22.486068 29613 net.cpp:226] relu5 needs backward computation.
I0521 02:58:22.486076 29613 net.cpp:226] ip1 needs backward computation.
I0521 02:58:22.486088 29613 net.cpp:226] pool4 needs backward computation.
I0521 02:58:22.486098 29613 net.cpp:226] relu4 needs backward computation.
I0521 02:58:22.486107 29613 net.cpp:226] conv4 needs backward computation.
I0521 02:58:22.486119 29613 net.cpp:226] pool3 needs backward computation.
I0521 02:58:22.486136 29613 net.cpp:226] relu3 needs backward computation.
I0521 02:58:22.486145 29613 net.cpp:226] conv3 needs backward computation.
I0521 02:58:22.486156 29613 net.cpp:226] pool2 needs backward computation.
I0521 02:58:22.486167 29613 net.cpp:226] relu2 needs backward computation.
I0521 02:58:22.486177 29613 net.cpp:226] conv2 needs backward computation.
I0521 02:58:22.486187 29613 net.cpp:226] pool1 needs backward computation.
I0521 02:58:22.486198 29613 net.cpp:226] relu1 needs backward computation.
I0521 02:58:22.486208 29613 net.cpp:226] conv1 needs backward computation.
I0521 02:58:22.486219 29613 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:58:22.486229 29613 net.cpp:270] This network produces output loss
I0521 02:58:22.486253 29613 net.cpp:283] Network initialization done.
I0521 02:58:22.487902 29613 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944.prototxt
I0521 02:58:22.487984 29613 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 02:58:22.488340 29613 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 590
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 02:58:22.488530 29613 layer_factory.hpp:77] Creating layer data_hdf5
I0521 02:58:22.488545 29613 net.cpp:106] Creating Layer data_hdf5
I0521 02:58:22.488559 29613 net.cpp:411] data_hdf5 -> data
I0521 02:58:22.488575 29613 net.cpp:411] data_hdf5 -> label
I0521 02:58:22.488591 29613 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 02:58:22.489820 29613 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 02:58:43.788561 29613 net.cpp:150] Setting up data_hdf5
I0521 02:58:43.788728 29613 net.cpp:157] Top shape: 590 1 127 50 (3746500)
I0521 02:58:43.788743 29613 net.cpp:157] Top shape: 590 (590)
I0521 02:58:43.788754 29613 net.cpp:165] Memory required for data: 14988360
I0521 02:58:43.788767 29613 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 02:58:43.788795 29613 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 02:58:43.788806 29613 net.cpp:454] label_data_hdf5_1_split <- label
I0521 02:58:43.788821 29613 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 02:58:43.788842 29613 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 02:58:43.788915 29613 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 02:58:43.788929 29613 net.cpp:157] Top shape: 590 (590)
I0521 02:58:43.788940 29613 net.cpp:157] Top shape: 590 (590)
I0521 02:58:43.788950 29613 net.cpp:165] Memory required for data: 14993080
I0521 02:58:43.788960 29613 layer_factory.hpp:77] Creating layer conv1
I0521 02:58:43.788983 29613 net.cpp:106] Creating Layer conv1
I0521 02:58:43.788995 29613 net.cpp:454] conv1 <- data
I0521 02:58:43.789010 29613 net.cpp:411] conv1 -> conv1
I0521 02:58:43.790946 29613 net.cpp:150] Setting up conv1
I0521 02:58:43.790971 29613 net.cpp:157] Top shape: 590 12 120 48 (40780800)
I0521 02:58:43.790982 29613 net.cpp:165] Memory required for data: 178116280
I0521 02:58:43.791002 29613 layer_factory.hpp:77] Creating layer relu1
I0521 02:58:43.791018 29613 net.cpp:106] Creating Layer relu1
I0521 02:58:43.791028 29613 net.cpp:454] relu1 <- conv1
I0521 02:58:43.791040 29613 net.cpp:397] relu1 -> conv1 (in-place)
I0521 02:58:43.791538 29613 net.cpp:150] Setting up relu1
I0521 02:58:43.791553 29613 net.cpp:157] Top shape: 590 12 120 48 (40780800)
I0521 02:58:43.791564 29613 net.cpp:165] Memory required for data: 341239480
I0521 02:58:43.791575 29613 layer_factory.hpp:77] Creating layer pool1
I0521 02:58:43.791591 29613 net.cpp:106] Creating Layer pool1
I0521 02:58:43.791601 29613 net.cpp:454] pool1 <- conv1
I0521 02:58:43.791615 29613 net.cpp:411] pool1 -> pool1
I0521 02:58:43.791689 29613 net.cpp:150] Setting up pool1
I0521 02:58:43.791702 29613 net.cpp:157] Top shape: 590 12 60 48 (20390400)
I0521 02:58:43.791713 29613 net.cpp:165] Memory required for data: 422801080
I0521 02:58:43.791724 29613 layer_factory.hpp:77] Creating layer conv2
I0521 02:58:43.791741 29613 net.cpp:106] Creating Layer conv2
I0521 02:58:43.791754 29613 net.cpp:454] conv2 <- pool1
I0521 02:58:43.791767 29613 net.cpp:411] conv2 -> conv2
I0521 02:58:43.793694 29613 net.cpp:150] Setting up conv2
I0521 02:58:43.793717 29613 net.cpp:157] Top shape: 590 20 54 46 (29311200)
I0521 02:58:43.793726 29613 net.cpp:165] Memory required for data: 540045880
I0521 02:58:43.793745 29613 layer_factory.hpp:77] Creating layer relu2
I0521 02:58:43.793759 29613 net.cpp:106] Creating Layer relu2
I0521 02:58:43.793769 29613 net.cpp:454] relu2 <- conv2
I0521 02:58:43.793781 29613 net.cpp:397] relu2 -> conv2 (in-place)
I0521 02:58:43.794113 29613 net.cpp:150] Setting up relu2
I0521 02:58:43.794127 29613 net.cpp:157] Top shape: 590 20 54 46 (29311200)
I0521 02:58:43.794137 29613 net.cpp:165] Memory required for data: 657290680
I0521 02:58:43.794147 29613 layer_factory.hpp:77] Creating layer pool2
I0521 02:58:43.794162 29613 net.cpp:106] Creating Layer pool2
I0521 02:58:43.794172 29613 net.cpp:454] pool2 <- conv2
I0521 02:58:43.794183 29613 net.cpp:411] pool2 -> pool2
I0521 02:58:43.794255 29613 net.cpp:150] Setting up pool2
I0521 02:58:43.794267 29613 net.cpp:157] Top shape: 590 20 27 46 (14655600)
I0521 02:58:43.794278 29613 net.cpp:165] Memory required for data: 715913080
I0521 02:58:43.794288 29613 layer_factory.hpp:77] Creating layer conv3
I0521 02:58:43.794306 29613 net.cpp:106] Creating Layer conv3
I0521 02:58:43.794317 29613 net.cpp:454] conv3 <- pool2
I0521 02:58:43.794332 29613 net.cpp:411] conv3 -> conv3
I0521 02:58:43.796306 29613 net.cpp:150] Setting up conv3
I0521 02:58:43.796329 29613 net.cpp:157] Top shape: 590 28 22 44 (15991360)
I0521 02:58:43.796341 29613 net.cpp:165] Memory required for data: 779878520
I0521 02:58:43.796375 29613 layer_factory.hpp:77] Creating layer relu3
I0521 02:58:43.796388 29613 net.cpp:106] Creating Layer relu3
I0521 02:58:43.796399 29613 net.cpp:454] relu3 <- conv3
I0521 02:58:43.796411 29613 net.cpp:397] relu3 -> conv3 (in-place)
I0521 02:58:43.796883 29613 net.cpp:150] Setting up relu3
I0521 02:58:43.796900 29613 net.cpp:157] Top shape: 590 28 22 44 (15991360)
I0521 02:58:43.796911 29613 net.cpp:165] Memory required for data: 843843960
I0521 02:58:43.796921 29613 layer_factory.hpp:77] Creating layer pool3
I0521 02:58:43.796933 29613 net.cpp:106] Creating Layer pool3
I0521 02:58:43.796943 29613 net.cpp:454] pool3 <- conv3
I0521 02:58:43.796957 29613 net.cpp:411] pool3 -> pool3
I0521 02:58:43.797027 29613 net.cpp:150] Setting up pool3
I0521 02:58:43.797041 29613 net.cpp:157] Top shape: 590 28 11 44 (7995680)
I0521 02:58:43.797051 29613 net.cpp:165] Memory required for data: 875826680
I0521 02:58:43.797060 29613 layer_factory.hpp:77] Creating layer conv4
I0521 02:58:43.797077 29613 net.cpp:106] Creating Layer conv4
I0521 02:58:43.797087 29613 net.cpp:454] conv4 <- pool3
I0521 02:58:43.797102 29613 net.cpp:411] conv4 -> conv4
I0521 02:58:43.799154 29613 net.cpp:150] Setting up conv4
I0521 02:58:43.799176 29613 net.cpp:157] Top shape: 590 36 6 42 (5352480)
I0521 02:58:43.799190 29613 net.cpp:165] Memory required for data: 897236600
I0521 02:58:43.799204 29613 layer_factory.hpp:77] Creating layer relu4
I0521 02:58:43.799218 29613 net.cpp:106] Creating Layer relu4
I0521 02:58:43.799228 29613 net.cpp:454] relu4 <- conv4
I0521 02:58:43.799242 29613 net.cpp:397] relu4 -> conv4 (in-place)
I0521 02:58:43.799710 29613 net.cpp:150] Setting up relu4
I0521 02:58:43.799724 29613 net.cpp:157] Top shape: 590 36 6 42 (5352480)
I0521 02:58:43.799736 29613 net.cpp:165] Memory required for data: 918646520
I0521 02:58:43.799746 29613 layer_factory.hpp:77] Creating layer pool4
I0521 02:58:43.799758 29613 net.cpp:106] Creating Layer pool4
I0521 02:58:43.799768 29613 net.cpp:454] pool4 <- conv4
I0521 02:58:43.799782 29613 net.cpp:411] pool4 -> pool4
I0521 02:58:43.799854 29613 net.cpp:150] Setting up pool4
I0521 02:58:43.799867 29613 net.cpp:157] Top shape: 590 36 3 42 (2676240)
I0521 02:58:43.799877 29613 net.cpp:165] Memory required for data: 929351480
I0521 02:58:43.799885 29613 layer_factory.hpp:77] Creating layer ip1
I0521 02:58:43.799901 29613 net.cpp:106] Creating Layer ip1
I0521 02:58:43.799911 29613 net.cpp:454] ip1 <- pool4
I0521 02:58:43.799926 29613 net.cpp:411] ip1 -> ip1
I0521 02:58:43.815484 29613 net.cpp:150] Setting up ip1
I0521 02:58:43.815511 29613 net.cpp:157] Top shape: 590 196 (115640)
I0521 02:58:43.815526 29613 net.cpp:165] Memory required for data: 929814040
I0521 02:58:43.815549 29613 layer_factory.hpp:77] Creating layer relu5
I0521 02:58:43.815564 29613 net.cpp:106] Creating Layer relu5
I0521 02:58:43.815575 29613 net.cpp:454] relu5 <- ip1
I0521 02:58:43.815589 29613 net.cpp:397] relu5 -> ip1 (in-place)
I0521 02:58:43.815935 29613 net.cpp:150] Setting up relu5
I0521 02:58:43.815949 29613 net.cpp:157] Top shape: 590 196 (115640)
I0521 02:58:43.815966 29613 net.cpp:165] Memory required for data: 930276600
I0521 02:58:43.815978 29613 layer_factory.hpp:77] Creating layer drop1
I0521 02:58:43.815996 29613 net.cpp:106] Creating Layer drop1
I0521 02:58:43.816007 29613 net.cpp:454] drop1 <- ip1
I0521 02:58:43.816020 29613 net.cpp:397] drop1 -> ip1 (in-place)
I0521 02:58:43.816064 29613 net.cpp:150] Setting up drop1
I0521 02:58:43.816077 29613 net.cpp:157] Top shape: 590 196 (115640)
I0521 02:58:43.816087 29613 net.cpp:165] Memory required for data: 930739160
I0521 02:58:43.816097 29613 layer_factory.hpp:77] Creating layer ip2
I0521 02:58:43.816112 29613 net.cpp:106] Creating Layer ip2
I0521 02:58:43.816121 29613 net.cpp:454] ip2 <- ip1
I0521 02:58:43.816135 29613 net.cpp:411] ip2 -> ip2
I0521 02:58:43.816617 29613 net.cpp:150] Setting up ip2
I0521 02:58:43.816632 29613 net.cpp:157] Top shape: 590 98 (57820)
I0521 02:58:43.816642 29613 net.cpp:165] Memory required for data: 930970440
I0521 02:58:43.816669 29613 layer_factory.hpp:77] Creating layer relu6
I0521 02:58:43.816682 29613 net.cpp:106] Creating Layer relu6
I0521 02:58:43.816692 29613 net.cpp:454] relu6 <- ip2
I0521 02:58:43.816705 29613 net.cpp:397] relu6 -> ip2 (in-place)
I0521 02:58:43.817242 29613 net.cpp:150] Setting up relu6
I0521 02:58:43.817263 29613 net.cpp:157] Top shape: 590 98 (57820)
I0521 02:58:43.817273 29613 net.cpp:165] Memory required for data: 931201720
I0521 02:58:43.817284 29613 layer_factory.hpp:77] Creating layer drop2
I0521 02:58:43.817298 29613 net.cpp:106] Creating Layer drop2
I0521 02:58:43.817308 29613 net.cpp:454] drop2 <- ip2
I0521 02:58:43.817322 29613 net.cpp:397] drop2 -> ip2 (in-place)
I0521 02:58:43.817366 29613 net.cpp:150] Setting up drop2
I0521 02:58:43.817379 29613 net.cpp:157] Top shape: 590 98 (57820)
I0521 02:58:43.817389 29613 net.cpp:165] Memory required for data: 931433000
I0521 02:58:43.817399 29613 layer_factory.hpp:77] Creating layer ip3
I0521 02:58:43.817414 29613 net.cpp:106] Creating Layer ip3
I0521 02:58:43.817425 29613 net.cpp:454] ip3 <- ip2
I0521 02:58:43.817438 29613 net.cpp:411] ip3 -> ip3
I0521 02:58:43.817662 29613 net.cpp:150] Setting up ip3
I0521 02:58:43.817677 29613 net.cpp:157] Top shape: 590 11 (6490)
I0521 02:58:43.817687 29613 net.cpp:165] Memory required for data: 931458960
I0521 02:58:43.817703 29613 layer_factory.hpp:77] Creating layer drop3
I0521 02:58:43.817715 29613 net.cpp:106] Creating Layer drop3
I0521 02:58:43.817725 29613 net.cpp:454] drop3 <- ip3
I0521 02:58:43.817739 29613 net.cpp:397] drop3 -> ip3 (in-place)
I0521 02:58:43.817780 29613 net.cpp:150] Setting up drop3
I0521 02:58:43.817793 29613 net.cpp:157] Top shape: 590 11 (6490)
I0521 02:58:43.817802 29613 net.cpp:165] Memory required for data: 931484920
I0521 02:58:43.817812 29613 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 02:58:43.817826 29613 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 02:58:43.817836 29613 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 02:58:43.817847 29613 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 02:58:43.817863 29613 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 02:58:43.817936 29613 net.cpp:150] Setting up ip3_drop3_0_split
I0521 02:58:43.817950 29613 net.cpp:157] Top shape: 590 11 (6490)
I0521 02:58:43.817965 29613 net.cpp:157] Top shape: 590 11 (6490)
I0521 02:58:43.817975 29613 net.cpp:165] Memory required for data: 931536840
I0521 02:58:43.817986 29613 layer_factory.hpp:77] Creating layer accuracy
I0521 02:58:43.818006 29613 net.cpp:106] Creating Layer accuracy
I0521 02:58:43.818017 29613 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 02:58:43.818027 29613 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 02:58:43.818042 29613 net.cpp:411] accuracy -> accuracy
I0521 02:58:43.818065 29613 net.cpp:150] Setting up accuracy
I0521 02:58:43.818078 29613 net.cpp:157] Top shape: (1)
I0521 02:58:43.818087 29613 net.cpp:165] Memory required for data: 931536844
I0521 02:58:43.818099 29613 layer_factory.hpp:77] Creating layer loss
I0521 02:58:43.818112 29613 net.cpp:106] Creating Layer loss
I0521 02:58:43.818122 29613 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 02:58:43.818133 29613 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 02:58:43.818146 29613 net.cpp:411] loss -> loss
I0521 02:58:43.818164 29613 layer_factory.hpp:77] Creating layer loss
I0521 02:58:43.818656 29613 net.cpp:150] Setting up loss
I0521 02:58:43.818670 29613 net.cpp:157] Top shape: (1)
I0521 02:58:43.818680 29613 net.cpp:160]     with loss weight 1
I0521 02:58:43.818701 29613 net.cpp:165] Memory required for data: 931536848
I0521 02:58:43.818711 29613 net.cpp:226] loss needs backward computation.
I0521 02:58:43.818722 29613 net.cpp:228] accuracy does not need backward computation.
I0521 02:58:43.818733 29613 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 02:58:43.818744 29613 net.cpp:226] drop3 needs backward computation.
I0521 02:58:43.818755 29613 net.cpp:226] ip3 needs backward computation.
I0521 02:58:43.818763 29613 net.cpp:226] drop2 needs backward computation.
I0521 02:58:43.818783 29613 net.cpp:226] relu6 needs backward computation.
I0521 02:58:43.818792 29613 net.cpp:226] ip2 needs backward computation.
I0521 02:58:43.818802 29613 net.cpp:226] drop1 needs backward computation.
I0521 02:58:43.818812 29613 net.cpp:226] relu5 needs backward computation.
I0521 02:58:43.818821 29613 net.cpp:226] ip1 needs backward computation.
I0521 02:58:43.818831 29613 net.cpp:226] pool4 needs backward computation.
I0521 02:58:43.818842 29613 net.cpp:226] relu4 needs backward computation.
I0521 02:58:43.818852 29613 net.cpp:226] conv4 needs backward computation.
I0521 02:58:43.818864 29613 net.cpp:226] pool3 needs backward computation.
I0521 02:58:43.818874 29613 net.cpp:226] relu3 needs backward computation.
I0521 02:58:43.818886 29613 net.cpp:226] conv3 needs backward computation.
I0521 02:58:43.818897 29613 net.cpp:226] pool2 needs backward computation.
I0521 02:58:43.818907 29613 net.cpp:226] relu2 needs backward computation.
I0521 02:58:43.818917 29613 net.cpp:226] conv2 needs backward computation.
I0521 02:58:43.818928 29613 net.cpp:226] pool1 needs backward computation.
I0521 02:58:43.818938 29613 net.cpp:226] relu1 needs backward computation.
I0521 02:58:43.818948 29613 net.cpp:226] conv1 needs backward computation.
I0521 02:58:43.818959 29613 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 02:58:43.818971 29613 net.cpp:228] data_hdf5 does not need backward computation.
I0521 02:58:43.818981 29613 net.cpp:270] This network produces output accuracy
I0521 02:58:43.818992 29613 net.cpp:270] This network produces output loss
I0521 02:58:43.819020 29613 net.cpp:283] Network initialization done.
I0521 02:58:43.819154 29613 solver.cpp:60] Solver scaffolding done.
I0521 02:58:43.820303 29613 caffe.cpp:212] Starting Optimization
I0521 02:58:43.820322 29613 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 02:58:43.820335 29613 solver.cpp:289] Learning Rate Policy: fixed
I0521 02:58:43.821563 29613 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 02:59:29.801051 29613 solver.cpp:409]     Test net output #0: accuracy = 0.0857801
I0521 02:59:29.801218 29613 solver.cpp:409]     Test net output #1: loss = 2.39787 (* 1 = 2.39787 loss)
I0521 02:59:29.914679 29613 solver.cpp:237] Iteration 0, loss = 2.39739
I0521 02:59:29.914715 29613 solver.cpp:253]     Train net output #0: loss = 2.39739 (* 1 = 2.39739 loss)
I0521 02:59:29.914736 29613 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 02:59:37.878489 29613 solver.cpp:237] Iteration 25, loss = 2.38384
I0521 02:59:37.878535 29613 solver.cpp:253]     Train net output #0: loss = 2.38384 (* 1 = 2.38384 loss)
I0521 02:59:37.878552 29613 sgd_solver.cpp:106] Iteration 25, lr = 0.0025
I0521 02:59:45.841636 29613 solver.cpp:237] Iteration 50, loss = 2.37048
I0521 02:59:45.841670 29613 solver.cpp:253]     Train net output #0: loss = 2.37048 (* 1 = 2.37048 loss)
I0521 02:59:45.841688 29613 sgd_solver.cpp:106] Iteration 50, lr = 0.0025
I0521 02:59:53.803918 29613 solver.cpp:237] Iteration 75, loss = 2.34642
I0521 02:59:53.803951 29613 solver.cpp:253]     Train net output #0: loss = 2.34642 (* 1 = 2.34642 loss)
I0521 02:59:53.803972 29613 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 03:00:01.771124 29613 solver.cpp:237] Iteration 100, loss = 2.34829
I0521 03:00:01.771275 29613 solver.cpp:253]     Train net output #0: loss = 2.34829 (* 1 = 2.34829 loss)
I0521 03:00:01.771288 29613 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0521 03:00:09.736650 29613 solver.cpp:237] Iteration 125, loss = 2.31912
I0521 03:00:09.736681 29613 solver.cpp:253]     Train net output #0: loss = 2.31912 (* 1 = 2.31912 loss)
I0521 03:00:09.736701 29613 sgd_solver.cpp:106] Iteration 125, lr = 0.0025
I0521 03:00:17.697052 29613 solver.cpp:237] Iteration 150, loss = 2.30732
I0521 03:00:17.697085 29613 solver.cpp:253]     Train net output #0: loss = 2.30732 (* 1 = 2.30732 loss)
I0521 03:00:17.697103 29613 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 03:00:47.801915 29613 solver.cpp:237] Iteration 175, loss = 2.29615
I0521 03:00:47.802079 29613 solver.cpp:253]     Train net output #0: loss = 2.29615 (* 1 = 2.29615 loss)
I0521 03:00:47.802093 29613 sgd_solver.cpp:106] Iteration 175, lr = 0.0025
I0521 03:00:55.769364 29613 solver.cpp:237] Iteration 200, loss = 2.3089
I0521 03:00:55.769397 29613 solver.cpp:253]     Train net output #0: loss = 2.3089 (* 1 = 2.3089 loss)
I0521 03:00:55.769415 29613 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0521 03:01:03.731782 29613 solver.cpp:237] Iteration 225, loss = 2.30938
I0521 03:01:03.731817 29613 solver.cpp:253]     Train net output #0: loss = 2.30938 (* 1 = 2.30938 loss)
I0521 03:01:03.731833 29613 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 03:01:11.702664 29613 solver.cpp:237] Iteration 250, loss = 2.24378
I0521 03:01:11.702715 29613 solver.cpp:253]     Train net output #0: loss = 2.24378 (* 1 = 2.24378 loss)
I0521 03:01:11.702731 29613 sgd_solver.cpp:106] Iteration 250, lr = 0.0025
I0521 03:01:12.655189 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_254.caffemodel
I0521 03:01:12.921640 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_254.solverstate
I0521 03:01:19.733240 29613 solver.cpp:237] Iteration 275, loss = 2.25874
I0521 03:01:19.733407 29613 solver.cpp:253]     Train net output #0: loss = 2.25874 (* 1 = 2.25874 loss)
I0521 03:01:19.733422 29613 sgd_solver.cpp:106] Iteration 275, lr = 0.0025
I0521 03:01:27.698693 29613 solver.cpp:237] Iteration 300, loss = 2.17318
I0521 03:01:27.698724 29613 solver.cpp:253]     Train net output #0: loss = 2.17318 (* 1 = 2.17318 loss)
I0521 03:01:27.698742 29613 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 03:01:35.667654 29613 solver.cpp:237] Iteration 325, loss = 2.15182
I0521 03:01:35.667687 29613 solver.cpp:253]     Train net output #0: loss = 2.15182 (* 1 = 2.15182 loss)
I0521 03:01:35.667704 29613 sgd_solver.cpp:106] Iteration 325, lr = 0.0025
I0521 03:02:06.219442 29613 solver.cpp:237] Iteration 350, loss = 2.08132
I0521 03:02:06.219601 29613 solver.cpp:253]     Train net output #0: loss = 2.08132 (* 1 = 2.08132 loss)
I0521 03:02:06.219616 29613 sgd_solver.cpp:106] Iteration 350, lr = 0.0025
I0521 03:02:14.186626 29613 solver.cpp:237] Iteration 375, loss = 2.08156
I0521 03:02:14.186660 29613 solver.cpp:253]     Train net output #0: loss = 2.08156 (* 1 = 2.08156 loss)
I0521 03:02:14.186677 29613 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 03:02:22.153926 29613 solver.cpp:237] Iteration 400, loss = 2.01452
I0521 03:02:22.153960 29613 solver.cpp:253]     Train net output #0: loss = 2.01452 (* 1 = 2.01452 loss)
I0521 03:02:22.153976 29613 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 03:02:30.125867 29613 solver.cpp:237] Iteration 425, loss = 2.03565
I0521 03:02:30.125905 29613 solver.cpp:253]     Train net output #0: loss = 2.03565 (* 1 = 2.03565 loss)
I0521 03:02:30.125926 29613 sgd_solver.cpp:106] Iteration 425, lr = 0.0025
I0521 03:02:38.090584 29613 solver.cpp:237] Iteration 450, loss = 1.98077
I0521 03:02:38.090729 29613 solver.cpp:253]     Train net output #0: loss = 1.98077 (* 1 = 1.98077 loss)
I0521 03:02:38.090742 29613 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 03:02:46.060348 29613 solver.cpp:237] Iteration 475, loss = 1.97449
I0521 03:02:46.060380 29613 solver.cpp:253]     Train net output #0: loss = 1.97449 (* 1 = 1.97449 loss)
I0521 03:02:46.060398 29613 sgd_solver.cpp:106] Iteration 475, lr = 0.0025
I0521 03:02:54.024345 29613 solver.cpp:237] Iteration 500, loss = 2.0264
I0521 03:02:54.024392 29613 solver.cpp:253]     Train net output #0: loss = 2.0264 (* 1 = 2.0264 loss)
I0521 03:02:54.024410 29613 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0521 03:02:56.258185 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_508.caffemodel
I0521 03:02:56.520500 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_508.solverstate
I0521 03:02:56.545624 29613 solver.cpp:341] Iteration 508, Testing net (#0)
I0521 03:03:41.614629 29613 solver.cpp:409]     Test net output #0: accuracy = 0.556553
I0521 03:03:41.614856 29613 solver.cpp:409]     Test net output #1: loss = 1.66554 (* 1 = 1.66554 loss)
I0521 03:04:09.284816 29613 solver.cpp:237] Iteration 525, loss = 1.99326
I0521 03:04:09.284868 29613 solver.cpp:253]     Train net output #0: loss = 1.99326 (* 1 = 1.99326 loss)
I0521 03:04:09.284886 29613 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 03:04:17.247144 29613 solver.cpp:237] Iteration 550, loss = 1.93047
I0521 03:04:17.247298 29613 solver.cpp:253]     Train net output #0: loss = 1.93047 (* 1 = 1.93047 loss)
I0521 03:04:17.247313 29613 sgd_solver.cpp:106] Iteration 550, lr = 0.0025
I0521 03:04:25.214140 29613 solver.cpp:237] Iteration 575, loss = 2.02417
I0521 03:04:25.214174 29613 solver.cpp:253]     Train net output #0: loss = 2.02417 (* 1 = 2.02417 loss)
I0521 03:04:25.214190 29613 sgd_solver.cpp:106] Iteration 575, lr = 0.0025
I0521 03:04:33.183114 29613 solver.cpp:237] Iteration 600, loss = 2.00121
I0521 03:04:33.183161 29613 solver.cpp:253]     Train net output #0: loss = 2.00121 (* 1 = 2.00121 loss)
I0521 03:04:33.183177 29613 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 03:04:41.145200 29613 solver.cpp:237] Iteration 625, loss = 1.9241
I0521 03:04:41.145231 29613 solver.cpp:253]     Train net output #0: loss = 1.9241 (* 1 = 1.9241 loss)
I0521 03:04:41.145249 29613 sgd_solver.cpp:106] Iteration 625, lr = 0.0025
I0521 03:04:49.111659 29613 solver.cpp:237] Iteration 650, loss = 1.94764
I0521 03:04:49.111793 29613 solver.cpp:253]     Train net output #0: loss = 1.94764 (* 1 = 1.94764 loss)
I0521 03:04:49.111806 29613 sgd_solver.cpp:106] Iteration 650, lr = 0.0025
I0521 03:04:57.074553 29613 solver.cpp:237] Iteration 675, loss = 1.87991
I0521 03:04:57.074599 29613 solver.cpp:253]     Train net output #0: loss = 1.87991 (* 1 = 1.87991 loss)
I0521 03:04:57.074617 29613 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 03:05:27.225582 29613 solver.cpp:237] Iteration 700, loss = 1.83191
I0521 03:05:27.225755 29613 solver.cpp:253]     Train net output #0: loss = 1.83191 (* 1 = 1.83191 loss)
I0521 03:05:27.225771 29613 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0521 03:05:35.190366 29613 solver.cpp:237] Iteration 725, loss = 1.90102
I0521 03:05:35.190399 29613 solver.cpp:253]     Train net output #0: loss = 1.90102 (* 1 = 1.90102 loss)
I0521 03:05:35.190417 29613 sgd_solver.cpp:106] Iteration 725, lr = 0.0025
I0521 03:05:43.157958 29613 solver.cpp:237] Iteration 750, loss = 1.82143
I0521 03:05:43.157991 29613 solver.cpp:253]     Train net output #0: loss = 1.82143 (* 1 = 1.82143 loss)
I0521 03:05:43.158007 29613 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 03:05:46.667119 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_762.caffemodel
I0521 03:05:46.930600 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_762.solverstate
I0521 03:05:51.192701 29613 solver.cpp:237] Iteration 775, loss = 1.80805
I0521 03:05:51.192754 29613 solver.cpp:253]     Train net output #0: loss = 1.80805 (* 1 = 1.80805 loss)
I0521 03:05:51.192770 29613 sgd_solver.cpp:106] Iteration 775, lr = 0.0025
I0521 03:05:59.161720 29613 solver.cpp:237] Iteration 800, loss = 1.89382
I0521 03:05:59.161864 29613 solver.cpp:253]     Train net output #0: loss = 1.89382 (* 1 = 1.89382 loss)
I0521 03:05:59.161878 29613 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 03:06:07.128621 29613 solver.cpp:237] Iteration 825, loss = 1.76727
I0521 03:06:07.128654 29613 solver.cpp:253]     Train net output #0: loss = 1.76727 (* 1 = 1.76727 loss)
I0521 03:06:07.128671 29613 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 03:06:37.272820 29613 solver.cpp:237] Iteration 850, loss = 1.82071
I0521 03:06:37.272974 29613 solver.cpp:253]     Train net output #0: loss = 1.82071 (* 1 = 1.82071 loss)
I0521 03:06:37.272989 29613 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0521 03:06:45.234084 29613 solver.cpp:237] Iteration 875, loss = 1.75501
I0521 03:06:45.234117 29613 solver.cpp:253]     Train net output #0: loss = 1.75501 (* 1 = 1.75501 loss)
I0521 03:06:45.234134 29613 sgd_solver.cpp:106] Iteration 875, lr = 0.0025
I0521 03:06:53.201752 29613 solver.cpp:237] Iteration 900, loss = 1.79123
I0521 03:06:53.201786 29613 solver.cpp:253]     Train net output #0: loss = 1.79123 (* 1 = 1.79123 loss)
I0521 03:06:53.201803 29613 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 03:07:01.168267 29613 solver.cpp:237] Iteration 925, loss = 1.83466
I0521 03:07:01.168300 29613 solver.cpp:253]     Train net output #0: loss = 1.83466 (* 1 = 1.83466 loss)
I0521 03:07:01.168318 29613 sgd_solver.cpp:106] Iteration 925, lr = 0.0025
I0521 03:07:09.138317 29613 solver.cpp:237] Iteration 950, loss = 1.78012
I0521 03:07:09.138463 29613 solver.cpp:253]     Train net output #0: loss = 1.78012 (* 1 = 1.78012 loss)
I0521 03:07:09.138475 29613 sgd_solver.cpp:106] Iteration 950, lr = 0.0025
I0521 03:07:17.107527 29613 solver.cpp:237] Iteration 975, loss = 1.75412
I0521 03:07:17.107558 29613 solver.cpp:253]     Train net output #0: loss = 1.75412 (* 1 = 1.75412 loss)
I0521 03:07:17.107576 29613 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 03:07:25.074756 29613 solver.cpp:237] Iteration 1000, loss = 1.90023
I0521 03:07:25.074790 29613 solver.cpp:253]     Train net output #0: loss = 1.90023 (* 1 = 1.90023 loss)
I0521 03:07:25.074806 29613 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0521 03:07:29.848470 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1016.caffemodel
I0521 03:07:30.111762 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1016.solverstate
I0521 03:07:30.139909 29613 solver.cpp:341] Iteration 1016, Testing net (#0)
I0521 03:08:36.102108 29613 solver.cpp:409]     Test net output #0: accuracy = 0.61011
I0521 03:08:36.102296 29613 solver.cpp:409]     Test net output #1: loss = 1.34167 (* 1 = 1.34167 loss)
I0521 03:09:01.263638 29613 solver.cpp:237] Iteration 1025, loss = 1.76444
I0521 03:09:01.263694 29613 solver.cpp:253]     Train net output #0: loss = 1.76444 (* 1 = 1.76444 loss)
I0521 03:09:01.263710 29613 sgd_solver.cpp:106] Iteration 1025, lr = 0.0025
I0521 03:09:09.218395 29613 solver.cpp:237] Iteration 1050, loss = 1.82006
I0521 03:09:09.218557 29613 solver.cpp:253]     Train net output #0: loss = 1.82006 (* 1 = 1.82006 loss)
I0521 03:09:09.218571 29613 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 03:09:17.170498 29613 solver.cpp:237] Iteration 1075, loss = 1.8217
I0521 03:09:17.170531 29613 solver.cpp:253]     Train net output #0: loss = 1.8217 (* 1 = 1.8217 loss)
I0521 03:09:17.170548 29613 sgd_solver.cpp:106] Iteration 1075, lr = 0.0025
I0521 03:09:25.122357 29613 solver.cpp:237] Iteration 1100, loss = 1.74231
I0521 03:09:25.122390 29613 solver.cpp:253]     Train net output #0: loss = 1.74231 (* 1 = 1.74231 loss)
I0521 03:09:25.122407 29613 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 03:09:33.069527 29613 solver.cpp:237] Iteration 1125, loss = 1.78729
I0521 03:09:33.069579 29613 solver.cpp:253]     Train net output #0: loss = 1.78729 (* 1 = 1.78729 loss)
I0521 03:09:33.069597 29613 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 03:09:41.027228 29613 solver.cpp:237] Iteration 1150, loss = 1.75782
I0521 03:09:41.027365 29613 solver.cpp:253]     Train net output #0: loss = 1.75782 (* 1 = 1.75782 loss)
I0521 03:09:41.027379 29613 sgd_solver.cpp:106] Iteration 1150, lr = 0.0025
I0521 03:09:48.980726 29613 solver.cpp:237] Iteration 1175, loss = 1.74555
I0521 03:09:48.980758 29613 solver.cpp:253]     Train net output #0: loss = 1.74555 (* 1 = 1.74555 loss)
I0521 03:09:48.980777 29613 sgd_solver.cpp:106] Iteration 1175, lr = 0.0025
I0521 03:10:19.149492 29613 solver.cpp:237] Iteration 1200, loss = 1.75895
I0521 03:10:19.149657 29613 solver.cpp:253]     Train net output #0: loss = 1.75895 (* 1 = 1.75895 loss)
I0521 03:10:19.149673 29613 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 03:10:27.103843 29613 solver.cpp:237] Iteration 1225, loss = 1.65894
I0521 03:10:27.103881 29613 solver.cpp:253]     Train net output #0: loss = 1.65894 (* 1 = 1.65894 loss)
I0521 03:10:27.103904 29613 sgd_solver.cpp:106] Iteration 1225, lr = 0.0025
I0521 03:10:35.055133 29613 solver.cpp:237] Iteration 1250, loss = 1.81712
I0521 03:10:35.055166 29613 solver.cpp:253]     Train net output #0: loss = 1.81712 (* 1 = 1.81712 loss)
I0521 03:10:35.055184 29613 sgd_solver.cpp:106] Iteration 1250, lr = 0.0025
I0521 03:10:41.095391 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1270.caffemodel
I0521 03:10:41.358387 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1270.solverstate
I0521 03:10:43.073029 29613 solver.cpp:237] Iteration 1275, loss = 1.86857
I0521 03:10:43.073081 29613 solver.cpp:253]     Train net output #0: loss = 1.86857 (* 1 = 1.86857 loss)
I0521 03:10:43.073098 29613 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 03:10:51.027886 29613 solver.cpp:237] Iteration 1300, loss = 1.7323
I0521 03:10:51.028053 29613 solver.cpp:253]     Train net output #0: loss = 1.7323 (* 1 = 1.7323 loss)
I0521 03:10:51.028066 29613 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0521 03:10:58.980561 29613 solver.cpp:237] Iteration 1325, loss = 1.74656
I0521 03:10:58.980593 29613 solver.cpp:253]     Train net output #0: loss = 1.74656 (* 1 = 1.74656 loss)
I0521 03:10:58.980612 29613 sgd_solver.cpp:106] Iteration 1325, lr = 0.0025
I0521 03:11:06.932481 29613 solver.cpp:237] Iteration 1350, loss = 1.71804
I0521 03:11:06.932513 29613 solver.cpp:253]     Train net output #0: loss = 1.71804 (* 1 = 1.71804 loss)
I0521 03:11:06.932530 29613 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 03:11:37.011468 29613 solver.cpp:237] Iteration 1375, loss = 1.66347
I0521 03:11:37.011646 29613 solver.cpp:253]     Train net output #0: loss = 1.66347 (* 1 = 1.66347 loss)
I0521 03:11:37.011661 29613 sgd_solver.cpp:106] Iteration 1375, lr = 0.0025
I0521 03:11:44.969380 29613 solver.cpp:237] Iteration 1400, loss = 1.76594
I0521 03:11:44.969413 29613 solver.cpp:253]     Train net output #0: loss = 1.76594 (* 1 = 1.76594 loss)
I0521 03:11:44.969431 29613 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0521 03:11:52.920914 29613 solver.cpp:237] Iteration 1425, loss = 1.75007
I0521 03:11:52.920946 29613 solver.cpp:253]     Train net output #0: loss = 1.75007 (* 1 = 1.75007 loss)
I0521 03:11:52.920964 29613 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 03:12:00.876353 29613 solver.cpp:237] Iteration 1450, loss = 1.76808
I0521 03:12:00.876386 29613 solver.cpp:253]     Train net output #0: loss = 1.76808 (* 1 = 1.76808 loss)
I0521 03:12:00.876405 29613 sgd_solver.cpp:106] Iteration 1450, lr = 0.0025
I0521 03:12:08.831821 29613 solver.cpp:237] Iteration 1475, loss = 1.74574
I0521 03:12:08.831979 29613 solver.cpp:253]     Train net output #0: loss = 1.74574 (* 1 = 1.74574 loss)
I0521 03:12:08.831995 29613 sgd_solver.cpp:106] Iteration 1475, lr = 0.0025
I0521 03:12:16.785365 29613 solver.cpp:237] Iteration 1500, loss = 1.68439
I0521 03:12:16.785398 29613 solver.cpp:253]     Train net output #0: loss = 1.68439 (* 1 = 1.68439 loss)
I0521 03:12:16.785415 29613 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 03:12:24.104678 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1524.caffemodel
I0521 03:12:24.368304 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1524.solverstate
I0521 03:12:24.394912 29613 solver.cpp:341] Iteration 1524, Testing net (#0)
I0521 03:13:09.161211 29613 solver.cpp:409]     Test net output #0: accuracy = 0.654291
I0521 03:13:09.161383 29613 solver.cpp:409]     Test net output #1: loss = 1.21268 (* 1 = 1.21268 loss)
I0521 03:13:31.708209 29613 solver.cpp:237] Iteration 1525, loss = 1.7594
I0521 03:13:31.708264 29613 solver.cpp:253]     Train net output #0: loss = 1.7594 (* 1 = 1.7594 loss)
I0521 03:13:31.708281 29613 sgd_solver.cpp:106] Iteration 1525, lr = 0.0025
I0521 03:13:39.662536 29613 solver.cpp:237] Iteration 1550, loss = 1.73221
I0521 03:13:39.662688 29613 solver.cpp:253]     Train net output #0: loss = 1.73221 (* 1 = 1.73221 loss)
I0521 03:13:39.662701 29613 sgd_solver.cpp:106] Iteration 1550, lr = 0.0025
I0521 03:13:47.618249 29613 solver.cpp:237] Iteration 1575, loss = 1.73737
I0521 03:13:47.618288 29613 solver.cpp:253]     Train net output #0: loss = 1.73737 (* 1 = 1.73737 loss)
I0521 03:13:47.618309 29613 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0521 03:13:55.575397 29613 solver.cpp:237] Iteration 1600, loss = 1.73071
I0521 03:13:55.575431 29613 solver.cpp:253]     Train net output #0: loss = 1.73071 (* 1 = 1.73071 loss)
I0521 03:13:55.575448 29613 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 03:14:03.534754 29613 solver.cpp:237] Iteration 1625, loss = 1.74961
I0521 03:14:03.534787 29613 solver.cpp:253]     Train net output #0: loss = 1.74961 (* 1 = 1.74961 loss)
I0521 03:14:03.534806 29613 sgd_solver.cpp:106] Iteration 1625, lr = 0.0025
I0521 03:14:11.494422 29613 solver.cpp:237] Iteration 1650, loss = 1.69421
I0521 03:14:11.494577 29613 solver.cpp:253]     Train net output #0: loss = 1.69421 (* 1 = 1.69421 loss)
I0521 03:14:11.494591 29613 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 03:14:19.454358 29613 solver.cpp:237] Iteration 1675, loss = 1.70983
I0521 03:14:19.454391 29613 solver.cpp:253]     Train net output #0: loss = 1.70983 (* 1 = 1.70983 loss)
I0521 03:14:19.454408 29613 sgd_solver.cpp:106] Iteration 1675, lr = 0.0025
I0521 03:14:49.612570 29613 solver.cpp:237] Iteration 1700, loss = 1.62574
I0521 03:14:49.612751 29613 solver.cpp:253]     Train net output #0: loss = 1.62574 (* 1 = 1.62574 loss)
I0521 03:14:49.612767 29613 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 03:14:57.574105 29613 solver.cpp:237] Iteration 1725, loss = 1.72208
I0521 03:14:57.574146 29613 solver.cpp:253]     Train net output #0: loss = 1.72208 (* 1 = 1.72208 loss)
I0521 03:14:57.574168 29613 sgd_solver.cpp:106] Iteration 1725, lr = 0.0025
I0521 03:15:05.534103 29613 solver.cpp:237] Iteration 1750, loss = 1.71654
I0521 03:15:05.534137 29613 solver.cpp:253]     Train net output #0: loss = 1.71654 (* 1 = 1.71654 loss)
I0521 03:15:05.534154 29613 sgd_solver.cpp:106] Iteration 1750, lr = 0.0025
I0521 03:15:13.496194 29613 solver.cpp:237] Iteration 1775, loss = 1.69278
I0521 03:15:13.496227 29613 solver.cpp:253]     Train net output #0: loss = 1.69278 (* 1 = 1.69278 loss)
I0521 03:15:13.496244 29613 sgd_solver.cpp:106] Iteration 1775, lr = 0.0025
I0521 03:15:14.134481 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1778.caffemodel
I0521 03:15:14.395555 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_1778.solverstate
I0521 03:15:21.517735 29613 solver.cpp:237] Iteration 1800, loss = 1.64196
I0521 03:15:21.517896 29613 solver.cpp:253]     Train net output #0: loss = 1.64196 (* 1 = 1.64196 loss)
I0521 03:15:21.517910 29613 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 03:15:29.481360 29613 solver.cpp:237] Iteration 1825, loss = 1.74037
I0521 03:15:29.481398 29613 solver.cpp:253]     Train net output #0: loss = 1.74037 (* 1 = 1.74037 loss)
I0521 03:15:29.481416 29613 sgd_solver.cpp:106] Iteration 1825, lr = 0.0025
I0521 03:15:37.439636 29613 solver.cpp:237] Iteration 1850, loss = 1.65587
I0521 03:15:37.439671 29613 solver.cpp:253]     Train net output #0: loss = 1.65587 (* 1 = 1.65587 loss)
I0521 03:15:37.439687 29613 sgd_solver.cpp:106] Iteration 1850, lr = 0.0025
I0521 03:16:07.545783 29613 solver.cpp:237] Iteration 1875, loss = 1.84559
I0521 03:16:07.545953 29613 solver.cpp:253]     Train net output #0: loss = 1.84559 (* 1 = 1.84559 loss)
I0521 03:16:07.545969 29613 sgd_solver.cpp:106] Iteration 1875, lr = 0.0025
I0521 03:16:15.503970 29613 solver.cpp:237] Iteration 1900, loss = 1.67676
I0521 03:16:15.504015 29613 solver.cpp:253]     Train net output #0: loss = 1.67676 (* 1 = 1.67676 loss)
I0521 03:16:15.504034 29613 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0521 03:16:23.462405 29613 solver.cpp:237] Iteration 1925, loss = 1.74038
I0521 03:16:23.462438 29613 solver.cpp:253]     Train net output #0: loss = 1.74038 (* 1 = 1.74038 loss)
I0521 03:16:23.462456 29613 sgd_solver.cpp:106] Iteration 1925, lr = 0.0025
I0521 03:16:31.418807 29613 solver.cpp:237] Iteration 1950, loss = 1.68955
I0521 03:16:31.418840 29613 solver.cpp:253]     Train net output #0: loss = 1.68955 (* 1 = 1.68955 loss)
I0521 03:16:31.418853 29613 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0521 03:16:39.379086 29613 solver.cpp:237] Iteration 1975, loss = 1.65942
I0521 03:16:39.379243 29613 solver.cpp:253]     Train net output #0: loss = 1.65942 (* 1 = 1.65942 loss)
I0521 03:16:39.379256 29613 sgd_solver.cpp:106] Iteration 1975, lr = 0.0025
I0521 03:16:47.338599 29613 solver.cpp:237] Iteration 2000, loss = 1.67115
I0521 03:16:47.338632 29613 solver.cpp:253]     Train net output #0: loss = 1.67115 (* 1 = 1.67115 loss)
I0521 03:16:47.338649 29613 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0521 03:16:55.296653 29613 solver.cpp:237] Iteration 2025, loss = 1.70667
I0521 03:16:55.296686 29613 solver.cpp:253]     Train net output #0: loss = 1.70667 (* 1 = 1.70667 loss)
I0521 03:16:55.296702 29613 sgd_solver.cpp:106] Iteration 2025, lr = 0.0025
I0521 03:16:57.208675 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2032.caffemodel
I0521 03:16:57.469947 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2032.solverstate
I0521 03:16:57.495924 29613 solver.cpp:341] Iteration 2032, Testing net (#0)
I0521 03:18:03.407104 29613 solver.cpp:409]     Test net output #0: accuracy = 0.67178
I0521 03:18:03.407284 29613 solver.cpp:409]     Test net output #1: loss = 1.13365 (* 1 = 1.13365 loss)
I0521 03:18:31.400254 29613 solver.cpp:237] Iteration 2050, loss = 1.61985
I0521 03:18:31.400305 29613 solver.cpp:253]     Train net output #0: loss = 1.61985 (* 1 = 1.61985 loss)
I0521 03:18:31.400324 29613 sgd_solver.cpp:106] Iteration 2050, lr = 0.0025
I0521 03:18:39.360519 29613 solver.cpp:237] Iteration 2075, loss = 1.66369
I0521 03:18:39.360664 29613 solver.cpp:253]     Train net output #0: loss = 1.66369 (* 1 = 1.66369 loss)
I0521 03:18:39.360678 29613 sgd_solver.cpp:106] Iteration 2075, lr = 0.0025
I0521 03:18:47.319164 29613 solver.cpp:237] Iteration 2100, loss = 1.63265
I0521 03:18:47.319213 29613 solver.cpp:253]     Train net output #0: loss = 1.63265 (* 1 = 1.63265 loss)
I0521 03:18:47.319229 29613 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0521 03:18:55.278014 29613 solver.cpp:237] Iteration 2125, loss = 1.71413
I0521 03:18:55.278048 29613 solver.cpp:253]     Train net output #0: loss = 1.71413 (* 1 = 1.71413 loss)
I0521 03:18:55.278064 29613 sgd_solver.cpp:106] Iteration 2125, lr = 0.0025
I0521 03:19:03.236675 29613 solver.cpp:237] Iteration 2150, loss = 1.64719
I0521 03:19:03.236708 29613 solver.cpp:253]     Train net output #0: loss = 1.64719 (* 1 = 1.64719 loss)
I0521 03:19:03.236726 29613 sgd_solver.cpp:106] Iteration 2150, lr = 0.0025
I0521 03:19:11.199029 29613 solver.cpp:237] Iteration 2175, loss = 1.71406
I0521 03:19:11.199180 29613 solver.cpp:253]     Train net output #0: loss = 1.71406 (* 1 = 1.71406 loss)
I0521 03:19:11.199195 29613 sgd_solver.cpp:106] Iteration 2175, lr = 0.0025
I0521 03:19:19.155243 29613 solver.cpp:237] Iteration 2200, loss = 1.70213
I0521 03:19:19.155277 29613 solver.cpp:253]     Train net output #0: loss = 1.70213 (* 1 = 1.70213 loss)
I0521 03:19:19.155293 29613 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0521 03:19:49.386715 29613 solver.cpp:237] Iteration 2225, loss = 1.64545
I0521 03:19:49.386888 29613 solver.cpp:253]     Train net output #0: loss = 1.64545 (* 1 = 1.64545 loss)
I0521 03:19:49.386905 29613 sgd_solver.cpp:106] Iteration 2225, lr = 0.0025
I0521 03:19:57.340095 29613 solver.cpp:237] Iteration 2250, loss = 1.70061
I0521 03:19:57.340139 29613 solver.cpp:253]     Train net output #0: loss = 1.70061 (* 1 = 1.70061 loss)
I0521 03:19:57.340158 29613 sgd_solver.cpp:106] Iteration 2250, lr = 0.0025
I0521 03:20:05.296488 29613 solver.cpp:237] Iteration 2275, loss = 1.73608
I0521 03:20:05.296519 29613 solver.cpp:253]     Train net output #0: loss = 1.73608 (* 1 = 1.73608 loss)
I0521 03:20:05.296536 29613 sgd_solver.cpp:106] Iteration 2275, lr = 0.0025
I0521 03:20:08.479603 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2286.caffemodel
I0521 03:20:08.743719 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2286.solverstate
I0521 03:20:13.325095 29613 solver.cpp:237] Iteration 2300, loss = 1.60832
I0521 03:20:13.325148 29613 solver.cpp:253]     Train net output #0: loss = 1.60832 (* 1 = 1.60832 loss)
I0521 03:20:13.325165 29613 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0521 03:20:21.287298 29613 solver.cpp:237] Iteration 2325, loss = 1.66914
I0521 03:20:21.287458 29613 solver.cpp:253]     Train net output #0: loss = 1.66914 (* 1 = 1.66914 loss)
I0521 03:20:21.287472 29613 sgd_solver.cpp:106] Iteration 2325, lr = 0.0025
I0521 03:20:29.246068 29613 solver.cpp:237] Iteration 2350, loss = 1.66415
I0521 03:20:29.246117 29613 solver.cpp:253]     Train net output #0: loss = 1.66415 (* 1 = 1.66415 loss)
I0521 03:20:29.246136 29613 sgd_solver.cpp:106] Iteration 2350, lr = 0.0025
I0521 03:20:59.373564 29613 solver.cpp:237] Iteration 2375, loss = 1.64352
I0521 03:20:59.373760 29613 solver.cpp:253]     Train net output #0: loss = 1.64352 (* 1 = 1.64352 loss)
I0521 03:20:59.373777 29613 sgd_solver.cpp:106] Iteration 2375, lr = 0.0025
I0521 03:21:07.335875 29613 solver.cpp:237] Iteration 2400, loss = 1.66917
I0521 03:21:07.335908 29613 solver.cpp:253]     Train net output #0: loss = 1.66917 (* 1 = 1.66917 loss)
I0521 03:21:07.335925 29613 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0521 03:21:15.292922 29613 solver.cpp:237] Iteration 2425, loss = 1.62123
I0521 03:21:15.292965 29613 solver.cpp:253]     Train net output #0: loss = 1.62123 (* 1 = 1.62123 loss)
I0521 03:21:15.292986 29613 sgd_solver.cpp:106] Iteration 2425, lr = 0.0025
I0521 03:21:23.251775 29613 solver.cpp:237] Iteration 2450, loss = 1.6007
I0521 03:21:23.251809 29613 solver.cpp:253]     Train net output #0: loss = 1.6007 (* 1 = 1.6007 loss)
I0521 03:21:23.251826 29613 sgd_solver.cpp:106] Iteration 2450, lr = 0.0025
I0521 03:21:31.210635 29613 solver.cpp:237] Iteration 2475, loss = 1.69127
I0521 03:21:31.210788 29613 solver.cpp:253]     Train net output #0: loss = 1.69127 (* 1 = 1.69127 loss)
I0521 03:21:31.210803 29613 sgd_solver.cpp:106] Iteration 2475, lr = 0.0025
I0521 03:21:39.164384 29613 solver.cpp:237] Iteration 2500, loss = 1.64975
I0521 03:21:39.164427 29613 solver.cpp:253]     Train net output #0: loss = 1.64975 (* 1 = 1.64975 loss)
I0521 03:21:39.164449 29613 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0521 03:21:47.127543 29613 solver.cpp:237] Iteration 2525, loss = 1.70147
I0521 03:21:47.127576 29613 solver.cpp:253]     Train net output #0: loss = 1.70147 (* 1 = 1.70147 loss)
I0521 03:21:47.127593 29613 sgd_solver.cpp:106] Iteration 2525, lr = 0.0025
I0521 03:21:51.589886 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2540.caffemodel
I0521 03:21:51.854650 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2540.solverstate
I0521 03:21:51.882963 29613 solver.cpp:341] Iteration 2540, Testing net (#0)
I0521 03:22:37.011483 29613 solver.cpp:409]     Test net output #0: accuracy = 0.683458
I0521 03:22:37.011663 29613 solver.cpp:409]     Test net output #1: loss = 1.07911 (* 1 = 1.07911 loss)
I0521 03:22:37.425246 29613 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2542.caffemodel
I0521 03:22:37.715032 29613 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944_iter_2542.solverstate
I0521 03:22:37.743185 29613 solver.cpp:326] Optimization Done.
I0521 03:22:37.743213 29613 caffe.cpp:215] Optimization Done.
Application 11236680 resources: utime ~1254s, stime ~225s, Rss ~5329252, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_590_2016-05-20T11.20.54.049944.solver"
	User time (seconds): 0.55
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:44.74
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15080
	Voluntary context switches: 2719
	Involuntary context switches: 82
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
