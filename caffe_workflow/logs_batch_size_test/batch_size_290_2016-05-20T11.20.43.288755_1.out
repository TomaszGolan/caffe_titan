2805924
I0520 20:39:36.738687   534 caffe.cpp:184] Using GPUs 0
I0520 20:39:37.163928   534 solver.cpp:48] Initializing solver from parameters: 
test_iter: 517
test_interval: 1034
base_lr: 0.0025
display: 51
max_iter: 5172
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 517
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755.prototxt"
I0520 20:39:37.165726   534 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755.prototxt
I0520 20:39:37.177637   534 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 20:39:37.177698   534 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 20:39:37.178041   534 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 290
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 20:39:37.178220   534 layer_factory.hpp:77] Creating layer data_hdf5
I0520 20:39:37.178243   534 net.cpp:106] Creating Layer data_hdf5
I0520 20:39:37.178257   534 net.cpp:411] data_hdf5 -> data
I0520 20:39:37.178299   534 net.cpp:411] data_hdf5 -> label
I0520 20:39:37.178333   534 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 20:39:37.179566   534 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 20:39:37.181794   534 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 20:39:58.734149   534 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 20:39:58.739269   534 net.cpp:150] Setting up data_hdf5
I0520 20:39:58.739311   534 net.cpp:157] Top shape: 290 1 127 50 (1841500)
I0520 20:39:58.739326   534 net.cpp:157] Top shape: 290 (290)
I0520 20:39:58.739336   534 net.cpp:165] Memory required for data: 7367160
I0520 20:39:58.739349   534 layer_factory.hpp:77] Creating layer conv1
I0520 20:39:58.739383   534 net.cpp:106] Creating Layer conv1
I0520 20:39:58.739395   534 net.cpp:454] conv1 <- data
I0520 20:39:58.739418   534 net.cpp:411] conv1 -> conv1
I0520 20:39:59.444125   534 net.cpp:150] Setting up conv1
I0520 20:39:59.444169   534 net.cpp:157] Top shape: 290 12 120 48 (20044800)
I0520 20:39:59.444180   534 net.cpp:165] Memory required for data: 87546360
I0520 20:39:59.444211   534 layer_factory.hpp:77] Creating layer relu1
I0520 20:39:59.444232   534 net.cpp:106] Creating Layer relu1
I0520 20:39:59.444242   534 net.cpp:454] relu1 <- conv1
I0520 20:39:59.444257   534 net.cpp:397] relu1 -> conv1 (in-place)
I0520 20:39:59.444773   534 net.cpp:150] Setting up relu1
I0520 20:39:59.444790   534 net.cpp:157] Top shape: 290 12 120 48 (20044800)
I0520 20:39:59.444802   534 net.cpp:165] Memory required for data: 167725560
I0520 20:39:59.444813   534 layer_factory.hpp:77] Creating layer pool1
I0520 20:39:59.444829   534 net.cpp:106] Creating Layer pool1
I0520 20:39:59.444839   534 net.cpp:454] pool1 <- conv1
I0520 20:39:59.444854   534 net.cpp:411] pool1 -> pool1
I0520 20:39:59.444934   534 net.cpp:150] Setting up pool1
I0520 20:39:59.444949   534 net.cpp:157] Top shape: 290 12 60 48 (10022400)
I0520 20:39:59.444962   534 net.cpp:165] Memory required for data: 207815160
I0520 20:39:59.444972   534 layer_factory.hpp:77] Creating layer conv2
I0520 20:39:59.444994   534 net.cpp:106] Creating Layer conv2
I0520 20:39:59.445004   534 net.cpp:454] conv2 <- pool1
I0520 20:39:59.445017   534 net.cpp:411] conv2 -> conv2
I0520 20:39:59.447697   534 net.cpp:150] Setting up conv2
I0520 20:39:59.447726   534 net.cpp:157] Top shape: 290 20 54 46 (14407200)
I0520 20:39:59.447736   534 net.cpp:165] Memory required for data: 265443960
I0520 20:39:59.447756   534 layer_factory.hpp:77] Creating layer relu2
I0520 20:39:59.447769   534 net.cpp:106] Creating Layer relu2
I0520 20:39:59.447779   534 net.cpp:454] relu2 <- conv2
I0520 20:39:59.447793   534 net.cpp:397] relu2 -> conv2 (in-place)
I0520 20:39:59.448123   534 net.cpp:150] Setting up relu2
I0520 20:39:59.448138   534 net.cpp:157] Top shape: 290 20 54 46 (14407200)
I0520 20:39:59.448148   534 net.cpp:165] Memory required for data: 323072760
I0520 20:39:59.448158   534 layer_factory.hpp:77] Creating layer pool2
I0520 20:39:59.448171   534 net.cpp:106] Creating Layer pool2
I0520 20:39:59.448181   534 net.cpp:454] pool2 <- conv2
I0520 20:39:59.448206   534 net.cpp:411] pool2 -> pool2
I0520 20:39:59.448276   534 net.cpp:150] Setting up pool2
I0520 20:39:59.448288   534 net.cpp:157] Top shape: 290 20 27 46 (7203600)
I0520 20:39:59.448300   534 net.cpp:165] Memory required for data: 351887160
I0520 20:39:59.448310   534 layer_factory.hpp:77] Creating layer conv3
I0520 20:39:59.448329   534 net.cpp:106] Creating Layer conv3
I0520 20:39:59.448339   534 net.cpp:454] conv3 <- pool2
I0520 20:39:59.448353   534 net.cpp:411] conv3 -> conv3
I0520 20:39:59.450304   534 net.cpp:150] Setting up conv3
I0520 20:39:59.450328   534 net.cpp:157] Top shape: 290 28 22 44 (7860160)
I0520 20:39:59.450340   534 net.cpp:165] Memory required for data: 383327800
I0520 20:39:59.450358   534 layer_factory.hpp:77] Creating layer relu3
I0520 20:39:59.450374   534 net.cpp:106] Creating Layer relu3
I0520 20:39:59.450384   534 net.cpp:454] relu3 <- conv3
I0520 20:39:59.450397   534 net.cpp:397] relu3 -> conv3 (in-place)
I0520 20:39:59.450868   534 net.cpp:150] Setting up relu3
I0520 20:39:59.450886   534 net.cpp:157] Top shape: 290 28 22 44 (7860160)
I0520 20:39:59.450896   534 net.cpp:165] Memory required for data: 414768440
I0520 20:39:59.450906   534 layer_factory.hpp:77] Creating layer pool3
I0520 20:39:59.450919   534 net.cpp:106] Creating Layer pool3
I0520 20:39:59.450929   534 net.cpp:454] pool3 <- conv3
I0520 20:39:59.450942   534 net.cpp:411] pool3 -> pool3
I0520 20:39:59.451010   534 net.cpp:150] Setting up pool3
I0520 20:39:59.451025   534 net.cpp:157] Top shape: 290 28 11 44 (3930080)
I0520 20:39:59.451035   534 net.cpp:165] Memory required for data: 430488760
I0520 20:39:59.451045   534 layer_factory.hpp:77] Creating layer conv4
I0520 20:39:59.451061   534 net.cpp:106] Creating Layer conv4
I0520 20:39:59.451072   534 net.cpp:454] conv4 <- pool3
I0520 20:39:59.451086   534 net.cpp:411] conv4 -> conv4
I0520 20:39:59.453810   534 net.cpp:150] Setting up conv4
I0520 20:39:59.453840   534 net.cpp:157] Top shape: 290 36 6 42 (2630880)
I0520 20:39:59.453852   534 net.cpp:165] Memory required for data: 441012280
I0520 20:39:59.453868   534 layer_factory.hpp:77] Creating layer relu4
I0520 20:39:59.453881   534 net.cpp:106] Creating Layer relu4
I0520 20:39:59.453891   534 net.cpp:454] relu4 <- conv4
I0520 20:39:59.453905   534 net.cpp:397] relu4 -> conv4 (in-place)
I0520 20:39:59.454382   534 net.cpp:150] Setting up relu4
I0520 20:39:59.454398   534 net.cpp:157] Top shape: 290 36 6 42 (2630880)
I0520 20:39:59.454409   534 net.cpp:165] Memory required for data: 451535800
I0520 20:39:59.454421   534 layer_factory.hpp:77] Creating layer pool4
I0520 20:39:59.454433   534 net.cpp:106] Creating Layer pool4
I0520 20:39:59.454443   534 net.cpp:454] pool4 <- conv4
I0520 20:39:59.454457   534 net.cpp:411] pool4 -> pool4
I0520 20:39:59.454525   534 net.cpp:150] Setting up pool4
I0520 20:39:59.454540   534 net.cpp:157] Top shape: 290 36 3 42 (1315440)
I0520 20:39:59.454550   534 net.cpp:165] Memory required for data: 456797560
I0520 20:39:59.454560   534 layer_factory.hpp:77] Creating layer ip1
I0520 20:39:59.454578   534 net.cpp:106] Creating Layer ip1
I0520 20:39:59.454589   534 net.cpp:454] ip1 <- pool4
I0520 20:39:59.454603   534 net.cpp:411] ip1 -> ip1
I0520 20:39:59.470005   534 net.cpp:150] Setting up ip1
I0520 20:39:59.470033   534 net.cpp:157] Top shape: 290 196 (56840)
I0520 20:39:59.470046   534 net.cpp:165] Memory required for data: 457024920
I0520 20:39:59.470067   534 layer_factory.hpp:77] Creating layer relu5
I0520 20:39:59.470082   534 net.cpp:106] Creating Layer relu5
I0520 20:39:59.470093   534 net.cpp:454] relu5 <- ip1
I0520 20:39:59.470105   534 net.cpp:397] relu5 -> ip1 (in-place)
I0520 20:39:59.470454   534 net.cpp:150] Setting up relu5
I0520 20:39:59.470469   534 net.cpp:157] Top shape: 290 196 (56840)
I0520 20:39:59.470479   534 net.cpp:165] Memory required for data: 457252280
I0520 20:39:59.470489   534 layer_factory.hpp:77] Creating layer drop1
I0520 20:39:59.470511   534 net.cpp:106] Creating Layer drop1
I0520 20:39:59.470521   534 net.cpp:454] drop1 <- ip1
I0520 20:39:59.470547   534 net.cpp:397] drop1 -> ip1 (in-place)
I0520 20:39:59.470595   534 net.cpp:150] Setting up drop1
I0520 20:39:59.470609   534 net.cpp:157] Top shape: 290 196 (56840)
I0520 20:39:59.470619   534 net.cpp:165] Memory required for data: 457479640
I0520 20:39:59.470629   534 layer_factory.hpp:77] Creating layer ip2
I0520 20:39:59.470649   534 net.cpp:106] Creating Layer ip2
I0520 20:39:59.470659   534 net.cpp:454] ip2 <- ip1
I0520 20:39:59.470672   534 net.cpp:411] ip2 -> ip2
I0520 20:39:59.471135   534 net.cpp:150] Setting up ip2
I0520 20:39:59.471149   534 net.cpp:157] Top shape: 290 98 (28420)
I0520 20:39:59.471159   534 net.cpp:165] Memory required for data: 457593320
I0520 20:39:59.471174   534 layer_factory.hpp:77] Creating layer relu6
I0520 20:39:59.471186   534 net.cpp:106] Creating Layer relu6
I0520 20:39:59.471196   534 net.cpp:454] relu6 <- ip2
I0520 20:39:59.471207   534 net.cpp:397] relu6 -> ip2 (in-place)
I0520 20:39:59.471725   534 net.cpp:150] Setting up relu6
I0520 20:39:59.471741   534 net.cpp:157] Top shape: 290 98 (28420)
I0520 20:39:59.471751   534 net.cpp:165] Memory required for data: 457707000
I0520 20:39:59.471762   534 layer_factory.hpp:77] Creating layer drop2
I0520 20:39:59.471779   534 net.cpp:106] Creating Layer drop2
I0520 20:39:59.471789   534 net.cpp:454] drop2 <- ip2
I0520 20:39:59.471802   534 net.cpp:397] drop2 -> ip2 (in-place)
I0520 20:39:59.471844   534 net.cpp:150] Setting up drop2
I0520 20:39:59.471858   534 net.cpp:157] Top shape: 290 98 (28420)
I0520 20:39:59.471868   534 net.cpp:165] Memory required for data: 457820680
I0520 20:39:59.471879   534 layer_factory.hpp:77] Creating layer ip3
I0520 20:39:59.471892   534 net.cpp:106] Creating Layer ip3
I0520 20:39:59.471901   534 net.cpp:454] ip3 <- ip2
I0520 20:39:59.471915   534 net.cpp:411] ip3 -> ip3
I0520 20:39:59.472123   534 net.cpp:150] Setting up ip3
I0520 20:39:59.472136   534 net.cpp:157] Top shape: 290 11 (3190)
I0520 20:39:59.472147   534 net.cpp:165] Memory required for data: 457833440
I0520 20:39:59.472162   534 layer_factory.hpp:77] Creating layer drop3
I0520 20:39:59.472174   534 net.cpp:106] Creating Layer drop3
I0520 20:39:59.472184   534 net.cpp:454] drop3 <- ip3
I0520 20:39:59.472196   534 net.cpp:397] drop3 -> ip3 (in-place)
I0520 20:39:59.472235   534 net.cpp:150] Setting up drop3
I0520 20:39:59.472250   534 net.cpp:157] Top shape: 290 11 (3190)
I0520 20:39:59.472259   534 net.cpp:165] Memory required for data: 457846200
I0520 20:39:59.472270   534 layer_factory.hpp:77] Creating layer loss
I0520 20:39:59.472288   534 net.cpp:106] Creating Layer loss
I0520 20:39:59.472298   534 net.cpp:454] loss <- ip3
I0520 20:39:59.472309   534 net.cpp:454] loss <- label
I0520 20:39:59.472323   534 net.cpp:411] loss -> loss
I0520 20:39:59.472339   534 layer_factory.hpp:77] Creating layer loss
I0520 20:39:59.472982   534 net.cpp:150] Setting up loss
I0520 20:39:59.473003   534 net.cpp:157] Top shape: (1)
I0520 20:39:59.473016   534 net.cpp:160]     with loss weight 1
I0520 20:39:59.473059   534 net.cpp:165] Memory required for data: 457846204
I0520 20:39:59.473070   534 net.cpp:226] loss needs backward computation.
I0520 20:39:59.473081   534 net.cpp:226] drop3 needs backward computation.
I0520 20:39:59.473091   534 net.cpp:226] ip3 needs backward computation.
I0520 20:39:59.473100   534 net.cpp:226] drop2 needs backward computation.
I0520 20:39:59.473111   534 net.cpp:226] relu6 needs backward computation.
I0520 20:39:59.473121   534 net.cpp:226] ip2 needs backward computation.
I0520 20:39:59.473131   534 net.cpp:226] drop1 needs backward computation.
I0520 20:39:59.473140   534 net.cpp:226] relu5 needs backward computation.
I0520 20:39:59.473150   534 net.cpp:226] ip1 needs backward computation.
I0520 20:39:59.473160   534 net.cpp:226] pool4 needs backward computation.
I0520 20:39:59.473170   534 net.cpp:226] relu4 needs backward computation.
I0520 20:39:59.473179   534 net.cpp:226] conv4 needs backward computation.
I0520 20:39:59.473189   534 net.cpp:226] pool3 needs backward computation.
I0520 20:39:59.473208   534 net.cpp:226] relu3 needs backward computation.
I0520 20:39:59.473218   534 net.cpp:226] conv3 needs backward computation.
I0520 20:39:59.473227   534 net.cpp:226] pool2 needs backward computation.
I0520 20:39:59.473238   534 net.cpp:226] relu2 needs backward computation.
I0520 20:39:59.473249   534 net.cpp:226] conv2 needs backward computation.
I0520 20:39:59.473259   534 net.cpp:226] pool1 needs backward computation.
I0520 20:39:59.473270   534 net.cpp:226] relu1 needs backward computation.
I0520 20:39:59.473280   534 net.cpp:226] conv1 needs backward computation.
I0520 20:39:59.473291   534 net.cpp:228] data_hdf5 does not need backward computation.
I0520 20:39:59.473301   534 net.cpp:270] This network produces output loss
I0520 20:39:59.473325   534 net.cpp:283] Network initialization done.
I0520 20:39:59.474939   534 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755.prototxt
I0520 20:39:59.475010   534 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 20:39:59.475366   534 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 290
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 20:39:59.475556   534 layer_factory.hpp:77] Creating layer data_hdf5
I0520 20:39:59.475572   534 net.cpp:106] Creating Layer data_hdf5
I0520 20:39:59.475585   534 net.cpp:411] data_hdf5 -> data
I0520 20:39:59.475602   534 net.cpp:411] data_hdf5 -> label
I0520 20:39:59.475617   534 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 20:39:59.476907   534 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 20:40:20.809314   534 net.cpp:150] Setting up data_hdf5
I0520 20:40:20.809481   534 net.cpp:157] Top shape: 290 1 127 50 (1841500)
I0520 20:40:20.809495   534 net.cpp:157] Top shape: 290 (290)
I0520 20:40:20.809509   534 net.cpp:165] Memory required for data: 7367160
I0520 20:40:20.809521   534 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 20:40:20.809551   534 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 20:40:20.809561   534 net.cpp:454] label_data_hdf5_1_split <- label
I0520 20:40:20.809576   534 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 20:40:20.809597   534 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 20:40:20.809670   534 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 20:40:20.809684   534 net.cpp:157] Top shape: 290 (290)
I0520 20:40:20.809695   534 net.cpp:157] Top shape: 290 (290)
I0520 20:40:20.809705   534 net.cpp:165] Memory required for data: 7369480
I0520 20:40:20.809715   534 layer_factory.hpp:77] Creating layer conv1
I0520 20:40:20.809738   534 net.cpp:106] Creating Layer conv1
I0520 20:40:20.809749   534 net.cpp:454] conv1 <- data
I0520 20:40:20.809763   534 net.cpp:411] conv1 -> conv1
I0520 20:40:20.811718   534 net.cpp:150] Setting up conv1
I0520 20:40:20.811743   534 net.cpp:157] Top shape: 290 12 120 48 (20044800)
I0520 20:40:20.811754   534 net.cpp:165] Memory required for data: 87548680
I0520 20:40:20.811775   534 layer_factory.hpp:77] Creating layer relu1
I0520 20:40:20.811790   534 net.cpp:106] Creating Layer relu1
I0520 20:40:20.811800   534 net.cpp:454] relu1 <- conv1
I0520 20:40:20.811813   534 net.cpp:397] relu1 -> conv1 (in-place)
I0520 20:40:20.812314   534 net.cpp:150] Setting up relu1
I0520 20:40:20.812328   534 net.cpp:157] Top shape: 290 12 120 48 (20044800)
I0520 20:40:20.812340   534 net.cpp:165] Memory required for data: 167727880
I0520 20:40:20.812348   534 layer_factory.hpp:77] Creating layer pool1
I0520 20:40:20.812366   534 net.cpp:106] Creating Layer pool1
I0520 20:40:20.812376   534 net.cpp:454] pool1 <- conv1
I0520 20:40:20.812388   534 net.cpp:411] pool1 -> pool1
I0520 20:40:20.812463   534 net.cpp:150] Setting up pool1
I0520 20:40:20.812476   534 net.cpp:157] Top shape: 290 12 60 48 (10022400)
I0520 20:40:20.812486   534 net.cpp:165] Memory required for data: 207817480
I0520 20:40:20.812497   534 layer_factory.hpp:77] Creating layer conv2
I0520 20:40:20.812515   534 net.cpp:106] Creating Layer conv2
I0520 20:40:20.812526   534 net.cpp:454] conv2 <- pool1
I0520 20:40:20.812541   534 net.cpp:411] conv2 -> conv2
I0520 20:40:20.814455   534 net.cpp:150] Setting up conv2
I0520 20:40:20.814476   534 net.cpp:157] Top shape: 290 20 54 46 (14407200)
I0520 20:40:20.814489   534 net.cpp:165] Memory required for data: 265446280
I0520 20:40:20.814507   534 layer_factory.hpp:77] Creating layer relu2
I0520 20:40:20.814520   534 net.cpp:106] Creating Layer relu2
I0520 20:40:20.814530   534 net.cpp:454] relu2 <- conv2
I0520 20:40:20.814543   534 net.cpp:397] relu2 -> conv2 (in-place)
I0520 20:40:20.814873   534 net.cpp:150] Setting up relu2
I0520 20:40:20.814888   534 net.cpp:157] Top shape: 290 20 54 46 (14407200)
I0520 20:40:20.814898   534 net.cpp:165] Memory required for data: 323075080
I0520 20:40:20.814908   534 layer_factory.hpp:77] Creating layer pool2
I0520 20:40:20.814921   534 net.cpp:106] Creating Layer pool2
I0520 20:40:20.814931   534 net.cpp:454] pool2 <- conv2
I0520 20:40:20.814944   534 net.cpp:411] pool2 -> pool2
I0520 20:40:20.815014   534 net.cpp:150] Setting up pool2
I0520 20:40:20.815027   534 net.cpp:157] Top shape: 290 20 27 46 (7203600)
I0520 20:40:20.815038   534 net.cpp:165] Memory required for data: 351889480
I0520 20:40:20.815048   534 layer_factory.hpp:77] Creating layer conv3
I0520 20:40:20.815065   534 net.cpp:106] Creating Layer conv3
I0520 20:40:20.815076   534 net.cpp:454] conv3 <- pool2
I0520 20:40:20.815090   534 net.cpp:411] conv3 -> conv3
I0520 20:40:20.817054   534 net.cpp:150] Setting up conv3
I0520 20:40:20.817077   534 net.cpp:157] Top shape: 290 28 22 44 (7860160)
I0520 20:40:20.817088   534 net.cpp:165] Memory required for data: 383330120
I0520 20:40:20.817122   534 layer_factory.hpp:77] Creating layer relu3
I0520 20:40:20.817136   534 net.cpp:106] Creating Layer relu3
I0520 20:40:20.817145   534 net.cpp:454] relu3 <- conv3
I0520 20:40:20.817159   534 net.cpp:397] relu3 -> conv3 (in-place)
I0520 20:40:20.817628   534 net.cpp:150] Setting up relu3
I0520 20:40:20.817644   534 net.cpp:157] Top shape: 290 28 22 44 (7860160)
I0520 20:40:20.817654   534 net.cpp:165] Memory required for data: 414770760
I0520 20:40:20.817664   534 layer_factory.hpp:77] Creating layer pool3
I0520 20:40:20.817677   534 net.cpp:106] Creating Layer pool3
I0520 20:40:20.817687   534 net.cpp:454] pool3 <- conv3
I0520 20:40:20.817700   534 net.cpp:411] pool3 -> pool3
I0520 20:40:20.817771   534 net.cpp:150] Setting up pool3
I0520 20:40:20.817785   534 net.cpp:157] Top shape: 290 28 11 44 (3930080)
I0520 20:40:20.817795   534 net.cpp:165] Memory required for data: 430491080
I0520 20:40:20.817806   534 layer_factory.hpp:77] Creating layer conv4
I0520 20:40:20.817824   534 net.cpp:106] Creating Layer conv4
I0520 20:40:20.817834   534 net.cpp:454] conv4 <- pool3
I0520 20:40:20.817848   534 net.cpp:411] conv4 -> conv4
I0520 20:40:20.819921   534 net.cpp:150] Setting up conv4
I0520 20:40:20.819944   534 net.cpp:157] Top shape: 290 36 6 42 (2630880)
I0520 20:40:20.819954   534 net.cpp:165] Memory required for data: 441014600
I0520 20:40:20.819970   534 layer_factory.hpp:77] Creating layer relu4
I0520 20:40:20.819983   534 net.cpp:106] Creating Layer relu4
I0520 20:40:20.819993   534 net.cpp:454] relu4 <- conv4
I0520 20:40:20.820006   534 net.cpp:397] relu4 -> conv4 (in-place)
I0520 20:40:20.820479   534 net.cpp:150] Setting up relu4
I0520 20:40:20.820495   534 net.cpp:157] Top shape: 290 36 6 42 (2630880)
I0520 20:40:20.820505   534 net.cpp:165] Memory required for data: 451538120
I0520 20:40:20.820515   534 layer_factory.hpp:77] Creating layer pool4
I0520 20:40:20.820528   534 net.cpp:106] Creating Layer pool4
I0520 20:40:20.820538   534 net.cpp:454] pool4 <- conv4
I0520 20:40:20.820554   534 net.cpp:411] pool4 -> pool4
I0520 20:40:20.820624   534 net.cpp:150] Setting up pool4
I0520 20:40:20.820638   534 net.cpp:157] Top shape: 290 36 3 42 (1315440)
I0520 20:40:20.820648   534 net.cpp:165] Memory required for data: 456799880
I0520 20:40:20.820659   534 layer_factory.hpp:77] Creating layer ip1
I0520 20:40:20.820674   534 net.cpp:106] Creating Layer ip1
I0520 20:40:20.820684   534 net.cpp:454] ip1 <- pool4
I0520 20:40:20.820698   534 net.cpp:411] ip1 -> ip1
I0520 20:40:20.836168   534 net.cpp:150] Setting up ip1
I0520 20:40:20.836196   534 net.cpp:157] Top shape: 290 196 (56840)
I0520 20:40:20.836207   534 net.cpp:165] Memory required for data: 457027240
I0520 20:40:20.836230   534 layer_factory.hpp:77] Creating layer relu5
I0520 20:40:20.836246   534 net.cpp:106] Creating Layer relu5
I0520 20:40:20.836256   534 net.cpp:454] relu5 <- ip1
I0520 20:40:20.836269   534 net.cpp:397] relu5 -> ip1 (in-place)
I0520 20:40:20.836616   534 net.cpp:150] Setting up relu5
I0520 20:40:20.836629   534 net.cpp:157] Top shape: 290 196 (56840)
I0520 20:40:20.836639   534 net.cpp:165] Memory required for data: 457254600
I0520 20:40:20.836649   534 layer_factory.hpp:77] Creating layer drop1
I0520 20:40:20.836668   534 net.cpp:106] Creating Layer drop1
I0520 20:40:20.836679   534 net.cpp:454] drop1 <- ip1
I0520 20:40:20.836693   534 net.cpp:397] drop1 -> ip1 (in-place)
I0520 20:40:20.836736   534 net.cpp:150] Setting up drop1
I0520 20:40:20.836750   534 net.cpp:157] Top shape: 290 196 (56840)
I0520 20:40:20.836760   534 net.cpp:165] Memory required for data: 457481960
I0520 20:40:20.836771   534 layer_factory.hpp:77] Creating layer ip2
I0520 20:40:20.836784   534 net.cpp:106] Creating Layer ip2
I0520 20:40:20.836794   534 net.cpp:454] ip2 <- ip1
I0520 20:40:20.836808   534 net.cpp:411] ip2 -> ip2
I0520 20:40:20.837288   534 net.cpp:150] Setting up ip2
I0520 20:40:20.837301   534 net.cpp:157] Top shape: 290 98 (28420)
I0520 20:40:20.837311   534 net.cpp:165] Memory required for data: 457595640
I0520 20:40:20.837342   534 layer_factory.hpp:77] Creating layer relu6
I0520 20:40:20.837357   534 net.cpp:106] Creating Layer relu6
I0520 20:40:20.837366   534 net.cpp:454] relu6 <- ip2
I0520 20:40:20.837378   534 net.cpp:397] relu6 -> ip2 (in-place)
I0520 20:40:20.837913   534 net.cpp:150] Setting up relu6
I0520 20:40:20.837934   534 net.cpp:157] Top shape: 290 98 (28420)
I0520 20:40:20.837944   534 net.cpp:165] Memory required for data: 457709320
I0520 20:40:20.837954   534 layer_factory.hpp:77] Creating layer drop2
I0520 20:40:20.837970   534 net.cpp:106] Creating Layer drop2
I0520 20:40:20.837980   534 net.cpp:454] drop2 <- ip2
I0520 20:40:20.837992   534 net.cpp:397] drop2 -> ip2 (in-place)
I0520 20:40:20.838037   534 net.cpp:150] Setting up drop2
I0520 20:40:20.838050   534 net.cpp:157] Top shape: 290 98 (28420)
I0520 20:40:20.838060   534 net.cpp:165] Memory required for data: 457823000
I0520 20:40:20.838070   534 layer_factory.hpp:77] Creating layer ip3
I0520 20:40:20.838084   534 net.cpp:106] Creating Layer ip3
I0520 20:40:20.838094   534 net.cpp:454] ip3 <- ip2
I0520 20:40:20.838109   534 net.cpp:411] ip3 -> ip3
I0520 20:40:20.838341   534 net.cpp:150] Setting up ip3
I0520 20:40:20.838354   534 net.cpp:157] Top shape: 290 11 (3190)
I0520 20:40:20.838364   534 net.cpp:165] Memory required for data: 457835760
I0520 20:40:20.838379   534 layer_factory.hpp:77] Creating layer drop3
I0520 20:40:20.838393   534 net.cpp:106] Creating Layer drop3
I0520 20:40:20.838403   534 net.cpp:454] drop3 <- ip3
I0520 20:40:20.838416   534 net.cpp:397] drop3 -> ip3 (in-place)
I0520 20:40:20.838457   534 net.cpp:150] Setting up drop3
I0520 20:40:20.838470   534 net.cpp:157] Top shape: 290 11 (3190)
I0520 20:40:20.838480   534 net.cpp:165] Memory required for data: 457848520
I0520 20:40:20.838490   534 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 20:40:20.838502   534 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 20:40:20.838512   534 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 20:40:20.838526   534 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 20:40:20.838541   534 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 20:40:20.838614   534 net.cpp:150] Setting up ip3_drop3_0_split
I0520 20:40:20.838627   534 net.cpp:157] Top shape: 290 11 (3190)
I0520 20:40:20.838640   534 net.cpp:157] Top shape: 290 11 (3190)
I0520 20:40:20.838650   534 net.cpp:165] Memory required for data: 457874040
I0520 20:40:20.838661   534 layer_factory.hpp:77] Creating layer accuracy
I0520 20:40:20.838683   534 net.cpp:106] Creating Layer accuracy
I0520 20:40:20.838693   534 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 20:40:20.838704   534 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 20:40:20.838718   534 net.cpp:411] accuracy -> accuracy
I0520 20:40:20.838742   534 net.cpp:150] Setting up accuracy
I0520 20:40:20.838755   534 net.cpp:157] Top shape: (1)
I0520 20:40:20.838765   534 net.cpp:165] Memory required for data: 457874044
I0520 20:40:20.838775   534 layer_factory.hpp:77] Creating layer loss
I0520 20:40:20.838789   534 net.cpp:106] Creating Layer loss
I0520 20:40:20.838799   534 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 20:40:20.838809   534 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 20:40:20.838822   534 net.cpp:411] loss -> loss
I0520 20:40:20.838840   534 layer_factory.hpp:77] Creating layer loss
I0520 20:40:20.839329   534 net.cpp:150] Setting up loss
I0520 20:40:20.839344   534 net.cpp:157] Top shape: (1)
I0520 20:40:20.839354   534 net.cpp:160]     with loss weight 1
I0520 20:40:20.839371   534 net.cpp:165] Memory required for data: 457874048
I0520 20:40:20.839382   534 net.cpp:226] loss needs backward computation.
I0520 20:40:20.839393   534 net.cpp:228] accuracy does not need backward computation.
I0520 20:40:20.839404   534 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 20:40:20.839416   534 net.cpp:226] drop3 needs backward computation.
I0520 20:40:20.839426   534 net.cpp:226] ip3 needs backward computation.
I0520 20:40:20.839435   534 net.cpp:226] drop2 needs backward computation.
I0520 20:40:20.839454   534 net.cpp:226] relu6 needs backward computation.
I0520 20:40:20.839465   534 net.cpp:226] ip2 needs backward computation.
I0520 20:40:20.839475   534 net.cpp:226] drop1 needs backward computation.
I0520 20:40:20.839484   534 net.cpp:226] relu5 needs backward computation.
I0520 20:40:20.839494   534 net.cpp:226] ip1 needs backward computation.
I0520 20:40:20.839504   534 net.cpp:226] pool4 needs backward computation.
I0520 20:40:20.839514   534 net.cpp:226] relu4 needs backward computation.
I0520 20:40:20.839525   534 net.cpp:226] conv4 needs backward computation.
I0520 20:40:20.839536   534 net.cpp:226] pool3 needs backward computation.
I0520 20:40:20.839547   534 net.cpp:226] relu3 needs backward computation.
I0520 20:40:20.839558   534 net.cpp:226] conv3 needs backward computation.
I0520 20:40:20.839570   534 net.cpp:226] pool2 needs backward computation.
I0520 20:40:20.839579   534 net.cpp:226] relu2 needs backward computation.
I0520 20:40:20.839589   534 net.cpp:226] conv2 needs backward computation.
I0520 20:40:20.839599   534 net.cpp:226] pool1 needs backward computation.
I0520 20:40:20.839609   534 net.cpp:226] relu1 needs backward computation.
I0520 20:40:20.839619   534 net.cpp:226] conv1 needs backward computation.
I0520 20:40:20.839630   534 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 20:40:20.839643   534 net.cpp:228] data_hdf5 does not need backward computation.
I0520 20:40:20.839653   534 net.cpp:270] This network produces output accuracy
I0520 20:40:20.839663   534 net.cpp:270] This network produces output loss
I0520 20:40:20.839691   534 net.cpp:283] Network initialization done.
I0520 20:40:20.839833   534 solver.cpp:60] Solver scaffolding done.
I0520 20:40:20.840975   534 caffe.cpp:212] Starting Optimization
I0520 20:40:20.840993   534 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 20:40:20.841007   534 solver.cpp:289] Learning Rate Policy: fixed
I0520 20:40:20.842226   534 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 20:41:07.225672   534 solver.cpp:409]     Test net output #0: accuracy = 0.131862
I0520 20:41:07.225834   534 solver.cpp:409]     Test net output #1: loss = 2.39793 (* 1 = 2.39793 loss)
I0520 20:41:07.289948   534 solver.cpp:237] Iteration 0, loss = 2.395
I0520 20:41:07.289984   534 solver.cpp:253]     Train net output #0: loss = 2.395 (* 1 = 2.395 loss)
I0520 20:41:07.290002   534 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 20:41:15.441823   534 solver.cpp:237] Iteration 51, loss = 2.35038
I0520 20:41:15.441871   534 solver.cpp:253]     Train net output #0: loss = 2.35038 (* 1 = 2.35038 loss)
I0520 20:41:15.441887   534 sgd_solver.cpp:106] Iteration 51, lr = 0.0025
I0520 20:41:23.590127   534 solver.cpp:237] Iteration 102, loss = 2.30999
I0520 20:41:23.590160   534 solver.cpp:253]     Train net output #0: loss = 2.30999 (* 1 = 2.30999 loss)
I0520 20:41:23.590176   534 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0520 20:41:31.739969   534 solver.cpp:237] Iteration 153, loss = 2.30523
I0520 20:41:31.740003   534 solver.cpp:253]     Train net output #0: loss = 2.30523 (* 1 = 2.30523 loss)
I0520 20:41:31.740021   534 sgd_solver.cpp:106] Iteration 153, lr = 0.0025
I0520 20:41:39.894109   534 solver.cpp:237] Iteration 204, loss = 2.28969
I0520 20:41:39.894275   534 solver.cpp:253]     Train net output #0: loss = 2.28969 (* 1 = 2.28969 loss)
I0520 20:41:39.894290   534 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0520 20:41:48.050518   534 solver.cpp:237] Iteration 255, loss = 2.24857
I0520 20:41:48.050551   534 solver.cpp:253]     Train net output #0: loss = 2.24857 (* 1 = 2.24857 loss)
I0520 20:41:48.050568   534 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0520 20:41:56.209108   534 solver.cpp:237] Iteration 306, loss = 2.17046
I0520 20:41:56.209141   534 solver.cpp:253]     Train net output #0: loss = 2.17046 (* 1 = 2.17046 loss)
I0520 20:41:56.209156   534 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0520 20:42:26.445133   534 solver.cpp:237] Iteration 357, loss = 2.12774
I0520 20:42:26.445294   534 solver.cpp:253]     Train net output #0: loss = 2.12774 (* 1 = 2.12774 loss)
I0520 20:42:26.445309   534 sgd_solver.cpp:106] Iteration 357, lr = 0.0025
I0520 20:42:34.598186   534 solver.cpp:237] Iteration 408, loss = 2.11164
I0520 20:42:34.598219   534 solver.cpp:253]     Train net output #0: loss = 2.11164 (* 1 = 2.11164 loss)
I0520 20:42:34.598237   534 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0520 20:42:42.751561   534 solver.cpp:237] Iteration 459, loss = 2.05318
I0520 20:42:42.751595   534 solver.cpp:253]     Train net output #0: loss = 2.05318 (* 1 = 2.05318 loss)
I0520 20:42:42.751610   534 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0520 20:42:50.904772   534 solver.cpp:237] Iteration 510, loss = 1.95841
I0520 20:42:50.904815   534 solver.cpp:253]     Train net output #0: loss = 1.95841 (* 1 = 1.95841 loss)
I0520 20:42:50.904832   534 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0520 20:42:51.864262   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_517.caffemodel
I0520 20:42:52.017813   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_517.solverstate
I0520 20:42:59.122403   534 solver.cpp:237] Iteration 561, loss = 1.95758
I0520 20:42:59.122556   534 solver.cpp:253]     Train net output #0: loss = 1.95758 (* 1 = 1.95758 loss)
I0520 20:42:59.122570   534 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0520 20:43:07.274987   534 solver.cpp:237] Iteration 612, loss = 1.80883
I0520 20:43:07.275020   534 solver.cpp:253]     Train net output #0: loss = 1.80883 (* 1 = 1.80883 loss)
I0520 20:43:07.275038   534 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0520 20:43:15.430919   534 solver.cpp:237] Iteration 663, loss = 1.87106
I0520 20:43:15.430964   534 solver.cpp:253]     Train net output #0: loss = 1.87106 (* 1 = 1.87106 loss)
I0520 20:43:15.430982   534 sgd_solver.cpp:106] Iteration 663, lr = 0.0025
I0520 20:43:45.744622   534 solver.cpp:237] Iteration 714, loss = 2.01383
I0520 20:43:45.744781   534 solver.cpp:253]     Train net output #0: loss = 2.01383 (* 1 = 2.01383 loss)
I0520 20:43:45.744796   534 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0520 20:43:53.900923   534 solver.cpp:237] Iteration 765, loss = 1.78748
I0520 20:43:53.900957   534 solver.cpp:253]     Train net output #0: loss = 1.78748 (* 1 = 1.78748 loss)
I0520 20:43:53.900974   534 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0520 20:44:02.057708   534 solver.cpp:237] Iteration 816, loss = 1.83685
I0520 20:44:02.057742   534 solver.cpp:253]     Train net output #0: loss = 1.83685 (* 1 = 1.83685 loss)
I0520 20:44:02.057760   534 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0520 20:44:10.206635   534 solver.cpp:237] Iteration 867, loss = 1.92505
I0520 20:44:10.206682   534 solver.cpp:253]     Train net output #0: loss = 1.92505 (* 1 = 1.92505 loss)
I0520 20:44:10.206698   534 sgd_solver.cpp:106] Iteration 867, lr = 0.0025
I0520 20:44:18.361687   534 solver.cpp:237] Iteration 918, loss = 1.88725
I0520 20:44:18.361835   534 solver.cpp:253]     Train net output #0: loss = 1.88725 (* 1 = 1.88725 loss)
I0520 20:44:18.361850   534 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0520 20:44:26.519594   534 solver.cpp:237] Iteration 969, loss = 1.80516
I0520 20:44:26.519628   534 solver.cpp:253]     Train net output #0: loss = 1.80516 (* 1 = 1.80516 loss)
I0520 20:44:26.519644   534 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0520 20:44:34.672267   534 solver.cpp:237] Iteration 1020, loss = 1.77921
I0520 20:44:34.672310   534 solver.cpp:253]     Train net output #0: loss = 1.77921 (* 1 = 1.77921 loss)
I0520 20:44:34.672327   534 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0520 20:44:36.750355   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_1034.caffemodel
I0520 20:44:36.900813   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_1034.solverstate
I0520 20:44:36.927561   534 solver.cpp:341] Iteration 1034, Testing net (#0)
I0520 20:45:22.426435   534 solver.cpp:409]     Test net output #0: accuracy = 0.634457
I0520 20:45:22.426609   534 solver.cpp:409]     Test net output #1: loss = 1.33286 (* 1 = 1.33286 loss)
I0520 20:45:50.566414   534 solver.cpp:237] Iteration 1071, loss = 1.91797
I0520 20:45:50.566464   534 solver.cpp:253]     Train net output #0: loss = 1.91797 (* 1 = 1.91797 loss)
I0520 20:45:50.566478   534 sgd_solver.cpp:106] Iteration 1071, lr = 0.0025
I0520 20:45:58.721781   534 solver.cpp:237] Iteration 1122, loss = 1.71768
I0520 20:45:58.721930   534 solver.cpp:253]     Train net output #0: loss = 1.71768 (* 1 = 1.71768 loss)
I0520 20:45:58.721945   534 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0520 20:46:06.873216   534 solver.cpp:237] Iteration 1173, loss = 1.82036
I0520 20:46:06.873250   534 solver.cpp:253]     Train net output #0: loss = 1.82036 (* 1 = 1.82036 loss)
I0520 20:46:06.873267   534 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0520 20:46:15.027146   534 solver.cpp:237] Iteration 1224, loss = 1.89853
I0520 20:46:15.027189   534 solver.cpp:253]     Train net output #0: loss = 1.89853 (* 1 = 1.89853 loss)
I0520 20:46:15.027206   534 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0520 20:46:23.180029   534 solver.cpp:237] Iteration 1275, loss = 1.79368
I0520 20:46:23.180063   534 solver.cpp:253]     Train net output #0: loss = 1.79368 (* 1 = 1.79368 loss)
I0520 20:46:23.180080   534 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0520 20:46:31.332008   534 solver.cpp:237] Iteration 1326, loss = 1.76179
I0520 20:46:31.332142   534 solver.cpp:253]     Train net output #0: loss = 1.76179 (* 1 = 1.76179 loss)
I0520 20:46:31.332156   534 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0520 20:46:39.486896   534 solver.cpp:237] Iteration 1377, loss = 1.60062
I0520 20:46:39.486932   534 solver.cpp:253]     Train net output #0: loss = 1.60062 (* 1 = 1.60062 loss)
I0520 20:46:39.486953   534 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0520 20:47:09.829398   534 solver.cpp:237] Iteration 1428, loss = 1.71627
I0520 20:47:09.829574   534 solver.cpp:253]     Train net output #0: loss = 1.71627 (* 1 = 1.71627 loss)
I0520 20:47:09.829591   534 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0520 20:47:17.979524   534 solver.cpp:237] Iteration 1479, loss = 1.70027
I0520 20:47:17.979557   534 solver.cpp:253]     Train net output #0: loss = 1.70027 (* 1 = 1.70027 loss)
I0520 20:47:17.979575   534 sgd_solver.cpp:106] Iteration 1479, lr = 0.0025
I0520 20:47:26.128707   534 solver.cpp:237] Iteration 1530, loss = 1.68426
I0520 20:47:26.128751   534 solver.cpp:253]     Train net output #0: loss = 1.68426 (* 1 = 1.68426 loss)
I0520 20:47:26.128769   534 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0520 20:47:29.325376   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_1551.caffemodel
I0520 20:47:29.478210   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_1551.solverstate
I0520 20:47:34.346794   534 solver.cpp:237] Iteration 1581, loss = 1.67585
I0520 20:47:34.346844   534 solver.cpp:253]     Train net output #0: loss = 1.67585 (* 1 = 1.67585 loss)
I0520 20:47:34.346863   534 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0520 20:47:42.496675   534 solver.cpp:237] Iteration 1632, loss = 1.78188
I0520 20:47:42.496821   534 solver.cpp:253]     Train net output #0: loss = 1.78188 (* 1 = 1.78188 loss)
I0520 20:47:42.496834   534 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0520 20:47:50.648183   534 solver.cpp:237] Iteration 1683, loss = 1.67931
I0520 20:47:50.648222   534 solver.cpp:253]     Train net output #0: loss = 1.67931 (* 1 = 1.67931 loss)
I0520 20:47:50.648243   534 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0520 20:48:20.997489   534 solver.cpp:237] Iteration 1734, loss = 1.67518
I0520 20:48:20.997648   534 solver.cpp:253]     Train net output #0: loss = 1.67518 (* 1 = 1.67518 loss)
I0520 20:48:20.997663   534 sgd_solver.cpp:106] Iteration 1734, lr = 0.0025
I0520 20:48:29.153076   534 solver.cpp:237] Iteration 1785, loss = 1.49386
I0520 20:48:29.153110   534 solver.cpp:253]     Train net output #0: loss = 1.49386 (* 1 = 1.49386 loss)
I0520 20:48:29.153128   534 sgd_solver.cpp:106] Iteration 1785, lr = 0.0025
I0520 20:48:37.308657   534 solver.cpp:237] Iteration 1836, loss = 1.68625
I0520 20:48:37.308691   534 solver.cpp:253]     Train net output #0: loss = 1.68625 (* 1 = 1.68625 loss)
I0520 20:48:37.308707   534 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0520 20:48:45.461150   534 solver.cpp:237] Iteration 1887, loss = 1.72592
I0520 20:48:45.461189   534 solver.cpp:253]     Train net output #0: loss = 1.72592 (* 1 = 1.72592 loss)
I0520 20:48:45.461210   534 sgd_solver.cpp:106] Iteration 1887, lr = 0.0025
I0520 20:48:53.608975   534 solver.cpp:237] Iteration 1938, loss = 1.6056
I0520 20:48:53.609112   534 solver.cpp:253]     Train net output #0: loss = 1.6056 (* 1 = 1.6056 loss)
I0520 20:48:53.609124   534 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0520 20:49:01.765205   534 solver.cpp:237] Iteration 1989, loss = 1.62773
I0520 20:49:01.765239   534 solver.cpp:253]     Train net output #0: loss = 1.62773 (* 1 = 1.62773 loss)
I0520 20:49:01.765259   534 sgd_solver.cpp:106] Iteration 1989, lr = 0.0025
I0520 20:49:09.918839   534 solver.cpp:237] Iteration 2040, loss = 1.58006
I0520 20:49:09.918879   534 solver.cpp:253]     Train net output #0: loss = 1.58006 (* 1 = 1.58006 loss)
I0520 20:49:09.918894   534 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0520 20:49:14.234715   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_2068.caffemodel
I0520 20:49:14.389268   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_2068.solverstate
I0520 20:49:14.419729   534 solver.cpp:341] Iteration 2068, Testing net (#0)
I0520 20:50:20.685710   534 solver.cpp:409]     Test net output #0: accuracy = 0.677071
I0520 20:50:20.685873   534 solver.cpp:409]     Test net output #1: loss = 1.11398 (* 1 = 1.11398 loss)
I0520 20:50:46.545038   534 solver.cpp:237] Iteration 2091, loss = 1.76504
I0520 20:50:46.545089   534 solver.cpp:253]     Train net output #0: loss = 1.76504 (* 1 = 1.76504 loss)
I0520 20:50:46.545104   534 sgd_solver.cpp:106] Iteration 2091, lr = 0.0025
I0520 20:50:54.697299   534 solver.cpp:237] Iteration 2142, loss = 1.67856
I0520 20:50:54.697446   534 solver.cpp:253]     Train net output #0: loss = 1.67856 (* 1 = 1.67856 loss)
I0520 20:50:54.697460   534 sgd_solver.cpp:106] Iteration 2142, lr = 0.0025
I0520 20:51:02.848064   534 solver.cpp:237] Iteration 2193, loss = 1.72299
I0520 20:51:02.848098   534 solver.cpp:253]     Train net output #0: loss = 1.72299 (* 1 = 1.72299 loss)
I0520 20:51:02.848114   534 sgd_solver.cpp:106] Iteration 2193, lr = 0.0025
I0520 20:51:10.998353   534 solver.cpp:237] Iteration 2244, loss = 1.60655
I0520 20:51:10.998388   534 solver.cpp:253]     Train net output #0: loss = 1.60655 (* 1 = 1.60655 loss)
I0520 20:51:10.998409   534 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0520 20:51:19.145938   534 solver.cpp:237] Iteration 2295, loss = 1.65436
I0520 20:51:19.145973   534 solver.cpp:253]     Train net output #0: loss = 1.65436 (* 1 = 1.65436 loss)
I0520 20:51:19.145990   534 sgd_solver.cpp:106] Iteration 2295, lr = 0.0025
I0520 20:51:27.295166   534 solver.cpp:237] Iteration 2346, loss = 1.65058
I0520 20:51:27.295302   534 solver.cpp:253]     Train net output #0: loss = 1.65058 (* 1 = 1.65058 loss)
I0520 20:51:27.295315   534 sgd_solver.cpp:106] Iteration 2346, lr = 0.0025
I0520 20:51:35.453234   534 solver.cpp:237] Iteration 2397, loss = 1.55972
I0520 20:51:35.453270   534 solver.cpp:253]     Train net output #0: loss = 1.55972 (* 1 = 1.55972 loss)
I0520 20:51:35.453286   534 sgd_solver.cpp:106] Iteration 2397, lr = 0.0025
I0520 20:52:05.847223   534 solver.cpp:237] Iteration 2448, loss = 1.65529
I0520 20:52:05.847389   534 solver.cpp:253]     Train net output #0: loss = 1.65529 (* 1 = 1.65529 loss)
I0520 20:52:05.847405   534 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0520 20:52:13.996311   534 solver.cpp:237] Iteration 2499, loss = 1.62453
I0520 20:52:13.996345   534 solver.cpp:253]     Train net output #0: loss = 1.62453 (* 1 = 1.62453 loss)
I0520 20:52:13.996362   534 sgd_solver.cpp:106] Iteration 2499, lr = 0.0025
I0520 20:52:22.149587   534 solver.cpp:237] Iteration 2550, loss = 1.60441
I0520 20:52:22.149622   534 solver.cpp:253]     Train net output #0: loss = 1.60441 (* 1 = 1.60441 loss)
I0520 20:52:22.149636   534 sgd_solver.cpp:106] Iteration 2550, lr = 0.0025
I0520 20:52:27.586347   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_2585.caffemodel
I0520 20:52:27.739440   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_2585.solverstate
I0520 20:52:30.372190   534 solver.cpp:237] Iteration 2601, loss = 1.62344
I0520 20:52:30.372236   534 solver.cpp:253]     Train net output #0: loss = 1.62344 (* 1 = 1.62344 loss)
I0520 20:52:30.372251   534 sgd_solver.cpp:106] Iteration 2601, lr = 0.0025
I0520 20:52:38.524538   534 solver.cpp:237] Iteration 2652, loss = 1.70041
I0520 20:52:38.524684   534 solver.cpp:253]     Train net output #0: loss = 1.70041 (* 1 = 1.70041 loss)
I0520 20:52:38.524698   534 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0520 20:52:46.673007   534 solver.cpp:237] Iteration 2703, loss = 1.53629
I0520 20:52:46.673039   534 solver.cpp:253]     Train net output #0: loss = 1.53629 (* 1 = 1.53629 loss)
I0520 20:52:46.673058   534 sgd_solver.cpp:106] Iteration 2703, lr = 0.0025
I0520 20:52:54.824043   534 solver.cpp:237] Iteration 2754, loss = 1.56674
I0520 20:52:54.824089   534 solver.cpp:253]     Train net output #0: loss = 1.56674 (* 1 = 1.56674 loss)
I0520 20:52:54.824105   534 sgd_solver.cpp:106] Iteration 2754, lr = 0.0025
I0520 20:53:25.159263   534 solver.cpp:237] Iteration 2805, loss = 1.68793
I0520 20:53:25.159443   534 solver.cpp:253]     Train net output #0: loss = 1.68793 (* 1 = 1.68793 loss)
I0520 20:53:25.159459   534 sgd_solver.cpp:106] Iteration 2805, lr = 0.0025
I0520 20:53:33.310245   534 solver.cpp:237] Iteration 2856, loss = 1.53506
I0520 20:53:33.310283   534 solver.cpp:253]     Train net output #0: loss = 1.53506 (* 1 = 1.53506 loss)
I0520 20:53:33.310300   534 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0520 20:53:41.466418   534 solver.cpp:237] Iteration 2907, loss = 1.567
I0520 20:53:41.466455   534 solver.cpp:253]     Train net output #0: loss = 1.567 (* 1 = 1.567 loss)
I0520 20:53:41.466477   534 sgd_solver.cpp:106] Iteration 2907, lr = 0.0025
I0520 20:53:49.616788   534 solver.cpp:237] Iteration 2958, loss = 1.59309
I0520 20:53:49.616822   534 solver.cpp:253]     Train net output #0: loss = 1.59309 (* 1 = 1.59309 loss)
I0520 20:53:49.616839   534 sgd_solver.cpp:106] Iteration 2958, lr = 0.0025
I0520 20:53:57.767596   534 solver.cpp:237] Iteration 3009, loss = 1.62896
I0520 20:53:57.767737   534 solver.cpp:253]     Train net output #0: loss = 1.62896 (* 1 = 1.62896 loss)
I0520 20:53:57.767751   534 sgd_solver.cpp:106] Iteration 3009, lr = 0.0025
I0520 20:54:05.921772   534 solver.cpp:237] Iteration 3060, loss = 1.69969
I0520 20:54:05.921808   534 solver.cpp:253]     Train net output #0: loss = 1.69969 (* 1 = 1.69969 loss)
I0520 20:54:05.921828   534 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0520 20:54:12.475136   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_3102.caffemodel
I0520 20:54:12.623886   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_3102.solverstate
I0520 20:54:12.649884   534 solver.cpp:341] Iteration 3102, Testing net (#0)
I0520 20:54:57.752506   534 solver.cpp:409]     Test net output #0: accuracy = 0.719491
I0520 20:54:57.752667   534 solver.cpp:409]     Test net output #1: loss = 0.976228 (* 1 = 0.976228 loss)
I0520 20:55:21.435695   534 solver.cpp:237] Iteration 3111, loss = 1.57326
I0520 20:55:21.435745   534 solver.cpp:253]     Train net output #0: loss = 1.57326 (* 1 = 1.57326 loss)
I0520 20:55:21.435763   534 sgd_solver.cpp:106] Iteration 3111, lr = 0.0025
I0520 20:55:29.583679   534 solver.cpp:237] Iteration 3162, loss = 1.52423
I0520 20:55:29.583822   534 solver.cpp:253]     Train net output #0: loss = 1.52423 (* 1 = 1.52423 loss)
I0520 20:55:29.583837   534 sgd_solver.cpp:106] Iteration 3162, lr = 0.0025
I0520 20:55:37.729038   534 solver.cpp:237] Iteration 3213, loss = 1.61239
I0520 20:55:37.729069   534 solver.cpp:253]     Train net output #0: loss = 1.61239 (* 1 = 1.61239 loss)
I0520 20:55:37.729087   534 sgd_solver.cpp:106] Iteration 3213, lr = 0.0025
I0520 20:55:45.873002   534 solver.cpp:237] Iteration 3264, loss = 1.50346
I0520 20:55:45.873035   534 solver.cpp:253]     Train net output #0: loss = 1.50346 (* 1 = 1.50346 loss)
I0520 20:55:45.873062   534 sgd_solver.cpp:106] Iteration 3264, lr = 0.0025
I0520 20:55:54.023249   534 solver.cpp:237] Iteration 3315, loss = 1.54874
I0520 20:55:54.023283   534 solver.cpp:253]     Train net output #0: loss = 1.54874 (* 1 = 1.54874 loss)
I0520 20:55:54.023299   534 sgd_solver.cpp:106] Iteration 3315, lr = 0.0025
I0520 20:56:02.165657   534 solver.cpp:237] Iteration 3366, loss = 1.67859
I0520 20:56:02.165805   534 solver.cpp:253]     Train net output #0: loss = 1.67859 (* 1 = 1.67859 loss)
I0520 20:56:02.165819   534 sgd_solver.cpp:106] Iteration 3366, lr = 0.0025
I0520 20:56:10.313369   534 solver.cpp:237] Iteration 3417, loss = 1.47359
I0520 20:56:10.313402   534 solver.cpp:253]     Train net output #0: loss = 1.47359 (* 1 = 1.47359 loss)
I0520 20:56:10.313429   534 sgd_solver.cpp:106] Iteration 3417, lr = 0.0025
I0520 20:56:40.616384   534 solver.cpp:237] Iteration 3468, loss = 1.56579
I0520 20:56:40.616554   534 solver.cpp:253]     Train net output #0: loss = 1.56579 (* 1 = 1.56579 loss)
I0520 20:56:40.616569   534 sgd_solver.cpp:106] Iteration 3468, lr = 0.0025
I0520 20:56:48.763247   534 solver.cpp:237] Iteration 3519, loss = 1.45657
I0520 20:56:48.763281   534 solver.cpp:253]     Train net output #0: loss = 1.45657 (* 1 = 1.45657 loss)
I0520 20:56:48.763298   534 sgd_solver.cpp:106] Iteration 3519, lr = 0.0025
I0520 20:56:56.912590   534 solver.cpp:237] Iteration 3570, loss = 1.36432
I0520 20:56:56.912624   534 solver.cpp:253]     Train net output #0: loss = 1.36432 (* 1 = 1.36432 loss)
I0520 20:56:56.912641   534 sgd_solver.cpp:106] Iteration 3570, lr = 0.0025
I0520 20:57:04.579824   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_3619.caffemodel
I0520 20:57:04.731811   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_3619.solverstate
I0520 20:57:05.125078   534 solver.cpp:237] Iteration 3621, loss = 1.53033
I0520 20:57:05.125123   534 solver.cpp:253]     Train net output #0: loss = 1.53033 (* 1 = 1.53033 loss)
I0520 20:57:05.125140   534 sgd_solver.cpp:106] Iteration 3621, lr = 0.0025
I0520 20:57:13.271822   534 solver.cpp:237] Iteration 3672, loss = 1.47527
I0520 20:57:13.271966   534 solver.cpp:253]     Train net output #0: loss = 1.47527 (* 1 = 1.47527 loss)
I0520 20:57:13.271980   534 sgd_solver.cpp:106] Iteration 3672, lr = 0.0025
I0520 20:57:21.416844   534 solver.cpp:237] Iteration 3723, loss = 1.55093
I0520 20:57:21.416878   534 solver.cpp:253]     Train net output #0: loss = 1.55093 (* 1 = 1.55093 loss)
I0520 20:57:21.416894   534 sgd_solver.cpp:106] Iteration 3723, lr = 0.0025
I0520 20:57:29.564602   534 solver.cpp:237] Iteration 3774, loss = 1.50254
I0520 20:57:29.564637   534 solver.cpp:253]     Train net output #0: loss = 1.50254 (* 1 = 1.50254 loss)
I0520 20:57:29.564658   534 sgd_solver.cpp:106] Iteration 3774, lr = 0.0025
I0520 20:57:59.920689   534 solver.cpp:237] Iteration 3825, loss = 1.54267
I0520 20:57:59.920852   534 solver.cpp:253]     Train net output #0: loss = 1.54267 (* 1 = 1.54267 loss)
I0520 20:57:59.920867   534 sgd_solver.cpp:106] Iteration 3825, lr = 0.0025
I0520 20:58:08.065269   534 solver.cpp:237] Iteration 3876, loss = 1.52301
I0520 20:58:08.065304   534 solver.cpp:253]     Train net output #0: loss = 1.52301 (* 1 = 1.52301 loss)
I0520 20:58:08.065316   534 sgd_solver.cpp:106] Iteration 3876, lr = 0.0025
I0520 20:58:16.206866   534 solver.cpp:237] Iteration 3927, loss = 1.46559
I0520 20:58:16.206902   534 solver.cpp:253]     Train net output #0: loss = 1.46559 (* 1 = 1.46559 loss)
I0520 20:58:16.206920   534 sgd_solver.cpp:106] Iteration 3927, lr = 0.0025
I0520 20:58:24.352231   534 solver.cpp:237] Iteration 3978, loss = 1.54036
I0520 20:58:24.352263   534 solver.cpp:253]     Train net output #0: loss = 1.54036 (* 1 = 1.54036 loss)
I0520 20:58:24.352282   534 sgd_solver.cpp:106] Iteration 3978, lr = 0.0025
I0520 20:58:32.496029   534 solver.cpp:237] Iteration 4029, loss = 1.51136
I0520 20:58:32.496168   534 solver.cpp:253]     Train net output #0: loss = 1.51136 (* 1 = 1.51136 loss)
I0520 20:58:32.496181   534 sgd_solver.cpp:106] Iteration 4029, lr = 0.0025
I0520 20:58:40.643097   534 solver.cpp:237] Iteration 4080, loss = 1.53821
I0520 20:58:40.643137   534 solver.cpp:253]     Train net output #0: loss = 1.53821 (* 1 = 1.53821 loss)
I0520 20:58:40.643158   534 sgd_solver.cpp:106] Iteration 4080, lr = 0.0025
I0520 20:58:48.788285   534 solver.cpp:237] Iteration 4131, loss = 1.552
I0520 20:58:48.788318   534 solver.cpp:253]     Train net output #0: loss = 1.552 (* 1 = 1.552 loss)
I0520 20:58:48.788336   534 sgd_solver.cpp:106] Iteration 4131, lr = 0.0025
I0520 20:58:49.427791   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_4136.caffemodel
I0520 20:58:49.578833   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_4136.solverstate
I0520 20:58:49.605084   534 solver.cpp:341] Iteration 4136, Testing net (#0)
I0520 20:59:55.920123   534 solver.cpp:409]     Test net output #0: accuracy = 0.774779
I0520 20:59:55.920297   534 solver.cpp:409]     Test net output #1: loss = 0.795227 (* 1 = 0.795227 loss)
I0520 21:00:25.518174   534 solver.cpp:237] Iteration 4182, loss = 1.46868
I0520 21:00:25.518224   534 solver.cpp:253]     Train net output #0: loss = 1.46868 (* 1 = 1.46868 loss)
I0520 21:00:25.518239   534 sgd_solver.cpp:106] Iteration 4182, lr = 0.0025
I0520 21:00:33.665390   534 solver.cpp:237] Iteration 4233, loss = 1.52181
I0520 21:00:33.665535   534 solver.cpp:253]     Train net output #0: loss = 1.52181 (* 1 = 1.52181 loss)
I0520 21:00:33.665549   534 sgd_solver.cpp:106] Iteration 4233, lr = 0.0025
I0520 21:00:41.808020   534 solver.cpp:237] Iteration 4284, loss = 1.5966
I0520 21:00:41.808054   534 solver.cpp:253]     Train net output #0: loss = 1.5966 (* 1 = 1.5966 loss)
I0520 21:00:41.808073   534 sgd_solver.cpp:106] Iteration 4284, lr = 0.0025
I0520 21:00:49.951972   534 solver.cpp:237] Iteration 4335, loss = 1.49137
I0520 21:00:49.952010   534 solver.cpp:253]     Train net output #0: loss = 1.49137 (* 1 = 1.49137 loss)
I0520 21:00:49.952030   534 sgd_solver.cpp:106] Iteration 4335, lr = 0.0025
I0520 21:00:58.090747   534 solver.cpp:237] Iteration 4386, loss = 1.45365
I0520 21:00:58.090781   534 solver.cpp:253]     Train net output #0: loss = 1.45365 (* 1 = 1.45365 loss)
I0520 21:00:58.090795   534 sgd_solver.cpp:106] Iteration 4386, lr = 0.0025
I0520 21:01:06.229352   534 solver.cpp:237] Iteration 4437, loss = 1.39664
I0520 21:01:06.229497   534 solver.cpp:253]     Train net output #0: loss = 1.39664 (* 1 = 1.39664 loss)
I0520 21:01:06.229511   534 sgd_solver.cpp:106] Iteration 4437, lr = 0.0025
I0520 21:01:36.581691   534 solver.cpp:237] Iteration 4488, loss = 1.36237
I0520 21:01:36.581861   534 solver.cpp:253]     Train net output #0: loss = 1.36237 (* 1 = 1.36237 loss)
I0520 21:01:36.581877   534 sgd_solver.cpp:106] Iteration 4488, lr = 0.0025
I0520 21:01:44.725944   534 solver.cpp:237] Iteration 4539, loss = 1.38423
I0520 21:01:44.725976   534 solver.cpp:253]     Train net output #0: loss = 1.38423 (* 1 = 1.38423 loss)
I0520 21:01:44.725991   534 sgd_solver.cpp:106] Iteration 4539, lr = 0.0025
I0520 21:01:52.866346   534 solver.cpp:237] Iteration 4590, loss = 1.44801
I0520 21:01:52.866382   534 solver.cpp:253]     Train net output #0: loss = 1.44801 (* 1 = 1.44801 loss)
I0520 21:01:52.866399   534 sgd_solver.cpp:106] Iteration 4590, lr = 0.0025
I0520 21:02:01.010885   534 solver.cpp:237] Iteration 4641, loss = 1.39677
I0520 21:02:01.010920   534 solver.cpp:253]     Train net output #0: loss = 1.39677 (* 1 = 1.39677 loss)
I0520 21:02:01.010941   534 sgd_solver.cpp:106] Iteration 4641, lr = 0.0025
I0520 21:02:02.768824   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_4653.caffemodel
I0520 21:02:02.920488   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_4653.solverstate
I0520 21:02:09.228621   534 solver.cpp:237] Iteration 4692, loss = 1.51157
I0520 21:02:09.228797   534 solver.cpp:253]     Train net output #0: loss = 1.51157 (* 1 = 1.51157 loss)
I0520 21:02:09.228812   534 sgd_solver.cpp:106] Iteration 4692, lr = 0.0025
I0520 21:02:17.378638   534 solver.cpp:237] Iteration 4743, loss = 1.39315
I0520 21:02:17.378671   534 solver.cpp:253]     Train net output #0: loss = 1.39315 (* 1 = 1.39315 loss)
I0520 21:02:17.378690   534 sgd_solver.cpp:106] Iteration 4743, lr = 0.0025
I0520 21:02:25.528976   534 solver.cpp:237] Iteration 4794, loss = 1.36788
I0520 21:02:25.529021   534 solver.cpp:253]     Train net output #0: loss = 1.36788 (* 1 = 1.36788 loss)
I0520 21:02:25.529039   534 sgd_solver.cpp:106] Iteration 4794, lr = 0.0025
I0520 21:02:55.878514   534 solver.cpp:237] Iteration 4845, loss = 1.46476
I0520 21:02:55.878687   534 solver.cpp:253]     Train net output #0: loss = 1.46476 (* 1 = 1.46476 loss)
I0520 21:02:55.878703   534 sgd_solver.cpp:106] Iteration 4845, lr = 0.0025
I0520 21:03:04.031695   534 solver.cpp:237] Iteration 4896, loss = 1.4713
I0520 21:03:04.031729   534 solver.cpp:253]     Train net output #0: loss = 1.4713 (* 1 = 1.4713 loss)
I0520 21:03:04.031746   534 sgd_solver.cpp:106] Iteration 4896, lr = 0.0025
I0520 21:03:12.179858   534 solver.cpp:237] Iteration 4947, loss = 1.37127
I0520 21:03:12.179893   534 solver.cpp:253]     Train net output #0: loss = 1.37127 (* 1 = 1.37127 loss)
I0520 21:03:12.179910   534 sgd_solver.cpp:106] Iteration 4947, lr = 0.0025
I0520 21:03:20.327205   534 solver.cpp:237] Iteration 4998, loss = 1.44915
I0520 21:03:20.327247   534 solver.cpp:253]     Train net output #0: loss = 1.44915 (* 1 = 1.44915 loss)
I0520 21:03:20.327266   534 sgd_solver.cpp:106] Iteration 4998, lr = 0.0025
I0520 21:03:28.476084   534 solver.cpp:237] Iteration 5049, loss = 1.46045
I0520 21:03:28.476228   534 solver.cpp:253]     Train net output #0: loss = 1.46045 (* 1 = 1.46045 loss)
I0520 21:03:28.476243   534 sgd_solver.cpp:106] Iteration 5049, lr = 0.0025
I0520 21:03:36.627231   534 solver.cpp:237] Iteration 5100, loss = 1.40684
I0520 21:03:36.627265   534 solver.cpp:253]     Train net output #0: loss = 1.40684 (* 1 = 1.40684 loss)
I0520 21:03:36.627284   534 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0520 21:03:44.771457   534 solver.cpp:237] Iteration 5151, loss = 1.41304
I0520 21:03:44.771493   534 solver.cpp:253]     Train net output #0: loss = 1.41304 (* 1 = 1.41304 loss)
I0520 21:03:44.771515   534 sgd_solver.cpp:106] Iteration 5151, lr = 0.0025
I0520 21:03:47.644578   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_5170.caffemodel
I0520 21:03:47.797469   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_5170.solverstate
I0520 21:03:47.824996   534 solver.cpp:341] Iteration 5170, Testing net (#0)
I0520 21:04:33.235018   534 solver.cpp:409]     Test net output #0: accuracy = 0.795028
I0520 21:04:33.235190   534 solver.cpp:409]     Test net output #1: loss = 0.761629 (* 1 = 0.761629 loss)
I0520 21:04:33.442411   534 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_5172.caffemodel
I0520 21:04:33.593693   534 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755_iter_5172.solverstate
I0520 21:04:33.621968   534 solver.cpp:326] Optimization Done.
I0520 21:04:33.621997   534 caffe.cpp:215] Optimization Done.
Application 11235276 resources: utime ~1272s, stime ~227s, Rss ~5329024, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_290_2016-05-20T11.20.43.288755.solver"
	User time (seconds): 0.58
	System time (seconds): 0.10
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:03.11
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15078
	Voluntary context switches: 2756
	Involuntary context switches: 71
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
