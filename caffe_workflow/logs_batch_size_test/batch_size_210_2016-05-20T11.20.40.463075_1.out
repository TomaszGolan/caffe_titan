2805631
I0520 18:13:51.567454 24478 caffe.cpp:184] Using GPUs 0
I0520 18:13:52.014416 24478 solver.cpp:48] Initializing solver from parameters: 
test_iter: 714
test_interval: 1428
base_lr: 0.0025
display: 71
max_iter: 7142
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 714
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075.prototxt"
I0520 18:13:52.016286 24478 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075.prototxt
I0520 18:13:52.031772 24478 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 18:13:52.031831 24478 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 18:13:52.032176 24478 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 210
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 18:13:52.032353 24478 layer_factory.hpp:77] Creating layer data_hdf5
I0520 18:13:52.032377 24478 net.cpp:106] Creating Layer data_hdf5
I0520 18:13:52.032392 24478 net.cpp:411] data_hdf5 -> data
I0520 18:13:52.032425 24478 net.cpp:411] data_hdf5 -> label
I0520 18:13:52.032456 24478 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 18:13:52.033599 24478 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 18:13:52.035773 24478 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 18:14:13.582720 24478 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 18:14:13.587767 24478 net.cpp:150] Setting up data_hdf5
I0520 18:14:13.587807 24478 net.cpp:157] Top shape: 210 1 127 50 (1333500)
I0520 18:14:13.587822 24478 net.cpp:157] Top shape: 210 (210)
I0520 18:14:13.587836 24478 net.cpp:165] Memory required for data: 5334840
I0520 18:14:13.587849 24478 layer_factory.hpp:77] Creating layer conv1
I0520 18:14:13.587883 24478 net.cpp:106] Creating Layer conv1
I0520 18:14:13.587894 24478 net.cpp:454] conv1 <- data
I0520 18:14:13.587916 24478 net.cpp:411] conv1 -> conv1
I0520 18:14:13.952936 24478 net.cpp:150] Setting up conv1
I0520 18:14:13.952983 24478 net.cpp:157] Top shape: 210 12 120 48 (14515200)
I0520 18:14:13.952994 24478 net.cpp:165] Memory required for data: 63395640
I0520 18:14:13.953025 24478 layer_factory.hpp:77] Creating layer relu1
I0520 18:14:13.953047 24478 net.cpp:106] Creating Layer relu1
I0520 18:14:13.953058 24478 net.cpp:454] relu1 <- conv1
I0520 18:14:13.953071 24478 net.cpp:397] relu1 -> conv1 (in-place)
I0520 18:14:13.953586 24478 net.cpp:150] Setting up relu1
I0520 18:14:13.953603 24478 net.cpp:157] Top shape: 210 12 120 48 (14515200)
I0520 18:14:13.953613 24478 net.cpp:165] Memory required for data: 121456440
I0520 18:14:13.953622 24478 layer_factory.hpp:77] Creating layer pool1
I0520 18:14:13.953639 24478 net.cpp:106] Creating Layer pool1
I0520 18:14:13.953649 24478 net.cpp:454] pool1 <- conv1
I0520 18:14:13.953662 24478 net.cpp:411] pool1 -> pool1
I0520 18:14:13.953742 24478 net.cpp:150] Setting up pool1
I0520 18:14:13.953757 24478 net.cpp:157] Top shape: 210 12 60 48 (7257600)
I0520 18:14:13.953766 24478 net.cpp:165] Memory required for data: 150486840
I0520 18:14:13.953776 24478 layer_factory.hpp:77] Creating layer conv2
I0520 18:14:13.953799 24478 net.cpp:106] Creating Layer conv2
I0520 18:14:13.953809 24478 net.cpp:454] conv2 <- pool1
I0520 18:14:13.953822 24478 net.cpp:411] conv2 -> conv2
I0520 18:14:13.956523 24478 net.cpp:150] Setting up conv2
I0520 18:14:13.956552 24478 net.cpp:157] Top shape: 210 20 54 46 (10432800)
I0520 18:14:13.956563 24478 net.cpp:165] Memory required for data: 192218040
I0520 18:14:13.956583 24478 layer_factory.hpp:77] Creating layer relu2
I0520 18:14:13.956598 24478 net.cpp:106] Creating Layer relu2
I0520 18:14:13.956607 24478 net.cpp:454] relu2 <- conv2
I0520 18:14:13.956620 24478 net.cpp:397] relu2 -> conv2 (in-place)
I0520 18:14:13.956949 24478 net.cpp:150] Setting up relu2
I0520 18:14:13.956964 24478 net.cpp:157] Top shape: 210 20 54 46 (10432800)
I0520 18:14:13.956974 24478 net.cpp:165] Memory required for data: 233949240
I0520 18:14:13.956984 24478 layer_factory.hpp:77] Creating layer pool2
I0520 18:14:13.956996 24478 net.cpp:106] Creating Layer pool2
I0520 18:14:13.957006 24478 net.cpp:454] pool2 <- conv2
I0520 18:14:13.957031 24478 net.cpp:411] pool2 -> pool2
I0520 18:14:13.957100 24478 net.cpp:150] Setting up pool2
I0520 18:14:13.957114 24478 net.cpp:157] Top shape: 210 20 27 46 (5216400)
I0520 18:14:13.957124 24478 net.cpp:165] Memory required for data: 254814840
I0520 18:14:13.957135 24478 layer_factory.hpp:77] Creating layer conv3
I0520 18:14:13.957151 24478 net.cpp:106] Creating Layer conv3
I0520 18:14:13.957162 24478 net.cpp:454] conv3 <- pool2
I0520 18:14:13.957176 24478 net.cpp:411] conv3 -> conv3
I0520 18:14:13.959133 24478 net.cpp:150] Setting up conv3
I0520 18:14:13.959156 24478 net.cpp:157] Top shape: 210 28 22 44 (5691840)
I0520 18:14:13.959172 24478 net.cpp:165] Memory required for data: 277582200
I0520 18:14:13.959189 24478 layer_factory.hpp:77] Creating layer relu3
I0520 18:14:13.959205 24478 net.cpp:106] Creating Layer relu3
I0520 18:14:13.959215 24478 net.cpp:454] relu3 <- conv3
I0520 18:14:13.959228 24478 net.cpp:397] relu3 -> conv3 (in-place)
I0520 18:14:13.959712 24478 net.cpp:150] Setting up relu3
I0520 18:14:13.959728 24478 net.cpp:157] Top shape: 210 28 22 44 (5691840)
I0520 18:14:13.959739 24478 net.cpp:165] Memory required for data: 300349560
I0520 18:14:13.959749 24478 layer_factory.hpp:77] Creating layer pool3
I0520 18:14:13.959763 24478 net.cpp:106] Creating Layer pool3
I0520 18:14:13.959772 24478 net.cpp:454] pool3 <- conv3
I0520 18:14:13.959785 24478 net.cpp:411] pool3 -> pool3
I0520 18:14:13.959853 24478 net.cpp:150] Setting up pool3
I0520 18:14:13.959867 24478 net.cpp:157] Top shape: 210 28 11 44 (2845920)
I0520 18:14:13.959875 24478 net.cpp:165] Memory required for data: 311733240
I0520 18:14:13.959883 24478 layer_factory.hpp:77] Creating layer conv4
I0520 18:14:13.959900 24478 net.cpp:106] Creating Layer conv4
I0520 18:14:13.959911 24478 net.cpp:454] conv4 <- pool3
I0520 18:14:13.959924 24478 net.cpp:411] conv4 -> conv4
I0520 18:14:13.962638 24478 net.cpp:150] Setting up conv4
I0520 18:14:13.962666 24478 net.cpp:157] Top shape: 210 36 6 42 (1905120)
I0520 18:14:13.962677 24478 net.cpp:165] Memory required for data: 319353720
I0520 18:14:13.962692 24478 layer_factory.hpp:77] Creating layer relu4
I0520 18:14:13.962705 24478 net.cpp:106] Creating Layer relu4
I0520 18:14:13.962715 24478 net.cpp:454] relu4 <- conv4
I0520 18:14:13.962728 24478 net.cpp:397] relu4 -> conv4 (in-place)
I0520 18:14:13.963193 24478 net.cpp:150] Setting up relu4
I0520 18:14:13.963210 24478 net.cpp:157] Top shape: 210 36 6 42 (1905120)
I0520 18:14:13.963220 24478 net.cpp:165] Memory required for data: 326974200
I0520 18:14:13.963230 24478 layer_factory.hpp:77] Creating layer pool4
I0520 18:14:13.963243 24478 net.cpp:106] Creating Layer pool4
I0520 18:14:13.963253 24478 net.cpp:454] pool4 <- conv4
I0520 18:14:13.963266 24478 net.cpp:411] pool4 -> pool4
I0520 18:14:13.963335 24478 net.cpp:150] Setting up pool4
I0520 18:14:13.963349 24478 net.cpp:157] Top shape: 210 36 3 42 (952560)
I0520 18:14:13.963359 24478 net.cpp:165] Memory required for data: 330784440
I0520 18:14:13.963369 24478 layer_factory.hpp:77] Creating layer ip1
I0520 18:14:13.963389 24478 net.cpp:106] Creating Layer ip1
I0520 18:14:13.963400 24478 net.cpp:454] ip1 <- pool4
I0520 18:14:13.963413 24478 net.cpp:411] ip1 -> ip1
I0520 18:14:13.978821 24478 net.cpp:150] Setting up ip1
I0520 18:14:13.978848 24478 net.cpp:157] Top shape: 210 196 (41160)
I0520 18:14:13.978860 24478 net.cpp:165] Memory required for data: 330949080
I0520 18:14:13.978883 24478 layer_factory.hpp:77] Creating layer relu5
I0520 18:14:13.978899 24478 net.cpp:106] Creating Layer relu5
I0520 18:14:13.978909 24478 net.cpp:454] relu5 <- ip1
I0520 18:14:13.978921 24478 net.cpp:397] relu5 -> ip1 (in-place)
I0520 18:14:13.979264 24478 net.cpp:150] Setting up relu5
I0520 18:14:13.979279 24478 net.cpp:157] Top shape: 210 196 (41160)
I0520 18:14:13.979288 24478 net.cpp:165] Memory required for data: 331113720
I0520 18:14:13.979298 24478 layer_factory.hpp:77] Creating layer drop1
I0520 18:14:13.979320 24478 net.cpp:106] Creating Layer drop1
I0520 18:14:13.979329 24478 net.cpp:454] drop1 <- ip1
I0520 18:14:13.979354 24478 net.cpp:397] drop1 -> ip1 (in-place)
I0520 18:14:13.979403 24478 net.cpp:150] Setting up drop1
I0520 18:14:13.979416 24478 net.cpp:157] Top shape: 210 196 (41160)
I0520 18:14:13.979426 24478 net.cpp:165] Memory required for data: 331278360
I0520 18:14:13.979436 24478 layer_factory.hpp:77] Creating layer ip2
I0520 18:14:13.979454 24478 net.cpp:106] Creating Layer ip2
I0520 18:14:13.979465 24478 net.cpp:454] ip2 <- ip1
I0520 18:14:13.979478 24478 net.cpp:411] ip2 -> ip2
I0520 18:14:13.979948 24478 net.cpp:150] Setting up ip2
I0520 18:14:13.979961 24478 net.cpp:157] Top shape: 210 98 (20580)
I0520 18:14:13.979971 24478 net.cpp:165] Memory required for data: 331360680
I0520 18:14:13.979986 24478 layer_factory.hpp:77] Creating layer relu6
I0520 18:14:13.980000 24478 net.cpp:106] Creating Layer relu6
I0520 18:14:13.980010 24478 net.cpp:454] relu6 <- ip2
I0520 18:14:13.980021 24478 net.cpp:397] relu6 -> ip2 (in-place)
I0520 18:14:13.980533 24478 net.cpp:150] Setting up relu6
I0520 18:14:13.980550 24478 net.cpp:157] Top shape: 210 98 (20580)
I0520 18:14:13.980559 24478 net.cpp:165] Memory required for data: 331443000
I0520 18:14:13.980569 24478 layer_factory.hpp:77] Creating layer drop2
I0520 18:14:13.980582 24478 net.cpp:106] Creating Layer drop2
I0520 18:14:13.980592 24478 net.cpp:454] drop2 <- ip2
I0520 18:14:13.980604 24478 net.cpp:397] drop2 -> ip2 (in-place)
I0520 18:14:13.980648 24478 net.cpp:150] Setting up drop2
I0520 18:14:13.980660 24478 net.cpp:157] Top shape: 210 98 (20580)
I0520 18:14:13.980670 24478 net.cpp:165] Memory required for data: 331525320
I0520 18:14:13.980680 24478 layer_factory.hpp:77] Creating layer ip3
I0520 18:14:13.980693 24478 net.cpp:106] Creating Layer ip3
I0520 18:14:13.980703 24478 net.cpp:454] ip3 <- ip2
I0520 18:14:13.980716 24478 net.cpp:411] ip3 -> ip3
I0520 18:14:13.980927 24478 net.cpp:150] Setting up ip3
I0520 18:14:13.980940 24478 net.cpp:157] Top shape: 210 11 (2310)
I0520 18:14:13.980950 24478 net.cpp:165] Memory required for data: 331534560
I0520 18:14:13.980965 24478 layer_factory.hpp:77] Creating layer drop3
I0520 18:14:13.980978 24478 net.cpp:106] Creating Layer drop3
I0520 18:14:13.980988 24478 net.cpp:454] drop3 <- ip3
I0520 18:14:13.980999 24478 net.cpp:397] drop3 -> ip3 (in-place)
I0520 18:14:13.981040 24478 net.cpp:150] Setting up drop3
I0520 18:14:13.981052 24478 net.cpp:157] Top shape: 210 11 (2310)
I0520 18:14:13.981062 24478 net.cpp:165] Memory required for data: 331543800
I0520 18:14:13.981072 24478 layer_factory.hpp:77] Creating layer loss
I0520 18:14:13.981091 24478 net.cpp:106] Creating Layer loss
I0520 18:14:13.981101 24478 net.cpp:454] loss <- ip3
I0520 18:14:13.981112 24478 net.cpp:454] loss <- label
I0520 18:14:13.981124 24478 net.cpp:411] loss -> loss
I0520 18:14:13.981142 24478 layer_factory.hpp:77] Creating layer loss
I0520 18:14:13.981776 24478 net.cpp:150] Setting up loss
I0520 18:14:13.981791 24478 net.cpp:157] Top shape: (1)
I0520 18:14:13.981803 24478 net.cpp:160]     with loss weight 1
I0520 18:14:13.981845 24478 net.cpp:165] Memory required for data: 331543804
I0520 18:14:13.981856 24478 net.cpp:226] loss needs backward computation.
I0520 18:14:13.981868 24478 net.cpp:226] drop3 needs backward computation.
I0520 18:14:13.981876 24478 net.cpp:226] ip3 needs backward computation.
I0520 18:14:13.981886 24478 net.cpp:226] drop2 needs backward computation.
I0520 18:14:13.981896 24478 net.cpp:226] relu6 needs backward computation.
I0520 18:14:13.981906 24478 net.cpp:226] ip2 needs backward computation.
I0520 18:14:13.981916 24478 net.cpp:226] drop1 needs backward computation.
I0520 18:14:13.981926 24478 net.cpp:226] relu5 needs backward computation.
I0520 18:14:13.981936 24478 net.cpp:226] ip1 needs backward computation.
I0520 18:14:13.981946 24478 net.cpp:226] pool4 needs backward computation.
I0520 18:14:13.981956 24478 net.cpp:226] relu4 needs backward computation.
I0520 18:14:13.981966 24478 net.cpp:226] conv4 needs backward computation.
I0520 18:14:13.981977 24478 net.cpp:226] pool3 needs backward computation.
I0520 18:14:13.981997 24478 net.cpp:226] relu3 needs backward computation.
I0520 18:14:13.982036 24478 net.cpp:226] conv3 needs backward computation.
I0520 18:14:13.982048 24478 net.cpp:226] pool2 needs backward computation.
I0520 18:14:13.982059 24478 net.cpp:226] relu2 needs backward computation.
I0520 18:14:13.982067 24478 net.cpp:226] conv2 needs backward computation.
I0520 18:14:13.982079 24478 net.cpp:226] pool1 needs backward computation.
I0520 18:14:13.982089 24478 net.cpp:226] relu1 needs backward computation.
I0520 18:14:13.982098 24478 net.cpp:226] conv1 needs backward computation.
I0520 18:14:13.982110 24478 net.cpp:228] data_hdf5 does not need backward computation.
I0520 18:14:13.982120 24478 net.cpp:270] This network produces output loss
I0520 18:14:13.982144 24478 net.cpp:283] Network initialization done.
I0520 18:14:13.983678 24478 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075.prototxt
I0520 18:14:13.983749 24478 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 18:14:13.984103 24478 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 210
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 18:14:13.984292 24478 layer_factory.hpp:77] Creating layer data_hdf5
I0520 18:14:13.984308 24478 net.cpp:106] Creating Layer data_hdf5
I0520 18:14:13.984320 24478 net.cpp:411] data_hdf5 -> data
I0520 18:14:13.984338 24478 net.cpp:411] data_hdf5 -> label
I0520 18:14:13.984352 24478 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 18:14:13.985519 24478 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 18:14:35.365684 24478 net.cpp:150] Setting up data_hdf5
I0520 18:14:35.365849 24478 net.cpp:157] Top shape: 210 1 127 50 (1333500)
I0520 18:14:35.365864 24478 net.cpp:157] Top shape: 210 (210)
I0520 18:14:35.365876 24478 net.cpp:165] Memory required for data: 5334840
I0520 18:14:35.365890 24478 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 18:14:35.365919 24478 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 18:14:35.365931 24478 net.cpp:454] label_data_hdf5_1_split <- label
I0520 18:14:35.365945 24478 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 18:14:35.365967 24478 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 18:14:35.366039 24478 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 18:14:35.366052 24478 net.cpp:157] Top shape: 210 (210)
I0520 18:14:35.366065 24478 net.cpp:157] Top shape: 210 (210)
I0520 18:14:35.366073 24478 net.cpp:165] Memory required for data: 5336520
I0520 18:14:35.366085 24478 layer_factory.hpp:77] Creating layer conv1
I0520 18:14:35.366107 24478 net.cpp:106] Creating Layer conv1
I0520 18:14:35.366117 24478 net.cpp:454] conv1 <- data
I0520 18:14:35.366132 24478 net.cpp:411] conv1 -> conv1
I0520 18:14:35.368058 24478 net.cpp:150] Setting up conv1
I0520 18:14:35.368083 24478 net.cpp:157] Top shape: 210 12 120 48 (14515200)
I0520 18:14:35.368094 24478 net.cpp:165] Memory required for data: 63397320
I0520 18:14:35.368115 24478 layer_factory.hpp:77] Creating layer relu1
I0520 18:14:35.368130 24478 net.cpp:106] Creating Layer relu1
I0520 18:14:35.368139 24478 net.cpp:454] relu1 <- conv1
I0520 18:14:35.368152 24478 net.cpp:397] relu1 -> conv1 (in-place)
I0520 18:14:35.368649 24478 net.cpp:150] Setting up relu1
I0520 18:14:35.368664 24478 net.cpp:157] Top shape: 210 12 120 48 (14515200)
I0520 18:14:35.368675 24478 net.cpp:165] Memory required for data: 121458120
I0520 18:14:35.368685 24478 layer_factory.hpp:77] Creating layer pool1
I0520 18:14:35.368702 24478 net.cpp:106] Creating Layer pool1
I0520 18:14:35.368712 24478 net.cpp:454] pool1 <- conv1
I0520 18:14:35.368726 24478 net.cpp:411] pool1 -> pool1
I0520 18:14:35.368800 24478 net.cpp:150] Setting up pool1
I0520 18:14:35.368814 24478 net.cpp:157] Top shape: 210 12 60 48 (7257600)
I0520 18:14:35.368824 24478 net.cpp:165] Memory required for data: 150488520
I0520 18:14:35.368834 24478 layer_factory.hpp:77] Creating layer conv2
I0520 18:14:35.368852 24478 net.cpp:106] Creating Layer conv2
I0520 18:14:35.368863 24478 net.cpp:454] conv2 <- pool1
I0520 18:14:35.368877 24478 net.cpp:411] conv2 -> conv2
I0520 18:14:35.370791 24478 net.cpp:150] Setting up conv2
I0520 18:14:35.370813 24478 net.cpp:157] Top shape: 210 20 54 46 (10432800)
I0520 18:14:35.370826 24478 net.cpp:165] Memory required for data: 192219720
I0520 18:14:35.370843 24478 layer_factory.hpp:77] Creating layer relu2
I0520 18:14:35.370857 24478 net.cpp:106] Creating Layer relu2
I0520 18:14:35.370867 24478 net.cpp:454] relu2 <- conv2
I0520 18:14:35.370879 24478 net.cpp:397] relu2 -> conv2 (in-place)
I0520 18:14:35.371211 24478 net.cpp:150] Setting up relu2
I0520 18:14:35.371225 24478 net.cpp:157] Top shape: 210 20 54 46 (10432800)
I0520 18:14:35.371234 24478 net.cpp:165] Memory required for data: 233950920
I0520 18:14:35.371245 24478 layer_factory.hpp:77] Creating layer pool2
I0520 18:14:35.371258 24478 net.cpp:106] Creating Layer pool2
I0520 18:14:35.371268 24478 net.cpp:454] pool2 <- conv2
I0520 18:14:35.371279 24478 net.cpp:411] pool2 -> pool2
I0520 18:14:35.371351 24478 net.cpp:150] Setting up pool2
I0520 18:14:35.371364 24478 net.cpp:157] Top shape: 210 20 27 46 (5216400)
I0520 18:14:35.371374 24478 net.cpp:165] Memory required for data: 254816520
I0520 18:14:35.371384 24478 layer_factory.hpp:77] Creating layer conv3
I0520 18:14:35.371402 24478 net.cpp:106] Creating Layer conv3
I0520 18:14:35.371412 24478 net.cpp:454] conv3 <- pool2
I0520 18:14:35.371425 24478 net.cpp:411] conv3 -> conv3
I0520 18:14:35.373404 24478 net.cpp:150] Setting up conv3
I0520 18:14:35.373426 24478 net.cpp:157] Top shape: 210 28 22 44 (5691840)
I0520 18:14:35.373437 24478 net.cpp:165] Memory required for data: 277583880
I0520 18:14:35.373471 24478 layer_factory.hpp:77] Creating layer relu3
I0520 18:14:35.373484 24478 net.cpp:106] Creating Layer relu3
I0520 18:14:35.373494 24478 net.cpp:454] relu3 <- conv3
I0520 18:14:35.373507 24478 net.cpp:397] relu3 -> conv3 (in-place)
I0520 18:14:35.373980 24478 net.cpp:150] Setting up relu3
I0520 18:14:35.373996 24478 net.cpp:157] Top shape: 210 28 22 44 (5691840)
I0520 18:14:35.374006 24478 net.cpp:165] Memory required for data: 300351240
I0520 18:14:35.374017 24478 layer_factory.hpp:77] Creating layer pool3
I0520 18:14:35.374030 24478 net.cpp:106] Creating Layer pool3
I0520 18:14:35.374040 24478 net.cpp:454] pool3 <- conv3
I0520 18:14:35.374053 24478 net.cpp:411] pool3 -> pool3
I0520 18:14:35.374125 24478 net.cpp:150] Setting up pool3
I0520 18:14:35.374138 24478 net.cpp:157] Top shape: 210 28 11 44 (2845920)
I0520 18:14:35.374148 24478 net.cpp:165] Memory required for data: 311734920
I0520 18:14:35.374156 24478 layer_factory.hpp:77] Creating layer conv4
I0520 18:14:35.374174 24478 net.cpp:106] Creating Layer conv4
I0520 18:14:35.374184 24478 net.cpp:454] conv4 <- pool3
I0520 18:14:35.374199 24478 net.cpp:411] conv4 -> conv4
I0520 18:14:35.376256 24478 net.cpp:150] Setting up conv4
I0520 18:14:35.376278 24478 net.cpp:157] Top shape: 210 36 6 42 (1905120)
I0520 18:14:35.376291 24478 net.cpp:165] Memory required for data: 319355400
I0520 18:14:35.376307 24478 layer_factory.hpp:77] Creating layer relu4
I0520 18:14:35.376320 24478 net.cpp:106] Creating Layer relu4
I0520 18:14:35.376330 24478 net.cpp:454] relu4 <- conv4
I0520 18:14:35.376343 24478 net.cpp:397] relu4 -> conv4 (in-place)
I0520 18:14:35.376811 24478 net.cpp:150] Setting up relu4
I0520 18:14:35.376827 24478 net.cpp:157] Top shape: 210 36 6 42 (1905120)
I0520 18:14:35.376838 24478 net.cpp:165] Memory required for data: 326975880
I0520 18:14:35.376848 24478 layer_factory.hpp:77] Creating layer pool4
I0520 18:14:35.376862 24478 net.cpp:106] Creating Layer pool4
I0520 18:14:35.376870 24478 net.cpp:454] pool4 <- conv4
I0520 18:14:35.376884 24478 net.cpp:411] pool4 -> pool4
I0520 18:14:35.376955 24478 net.cpp:150] Setting up pool4
I0520 18:14:35.376970 24478 net.cpp:157] Top shape: 210 36 3 42 (952560)
I0520 18:14:35.376978 24478 net.cpp:165] Memory required for data: 330786120
I0520 18:14:35.376986 24478 layer_factory.hpp:77] Creating layer ip1
I0520 18:14:35.377002 24478 net.cpp:106] Creating Layer ip1
I0520 18:14:35.377012 24478 net.cpp:454] ip1 <- pool4
I0520 18:14:35.377027 24478 net.cpp:411] ip1 -> ip1
I0520 18:14:35.392534 24478 net.cpp:150] Setting up ip1
I0520 18:14:35.392563 24478 net.cpp:157] Top shape: 210 196 (41160)
I0520 18:14:35.392575 24478 net.cpp:165] Memory required for data: 330950760
I0520 18:14:35.392596 24478 layer_factory.hpp:77] Creating layer relu5
I0520 18:14:35.392611 24478 net.cpp:106] Creating Layer relu5
I0520 18:14:35.392622 24478 net.cpp:454] relu5 <- ip1
I0520 18:14:35.392637 24478 net.cpp:397] relu5 -> ip1 (in-place)
I0520 18:14:35.392982 24478 net.cpp:150] Setting up relu5
I0520 18:14:35.392997 24478 net.cpp:157] Top shape: 210 196 (41160)
I0520 18:14:35.393007 24478 net.cpp:165] Memory required for data: 331115400
I0520 18:14:35.393016 24478 layer_factory.hpp:77] Creating layer drop1
I0520 18:14:35.393035 24478 net.cpp:106] Creating Layer drop1
I0520 18:14:35.393046 24478 net.cpp:454] drop1 <- ip1
I0520 18:14:35.393059 24478 net.cpp:397] drop1 -> ip1 (in-place)
I0520 18:14:35.393107 24478 net.cpp:150] Setting up drop1
I0520 18:14:35.393121 24478 net.cpp:157] Top shape: 210 196 (41160)
I0520 18:14:35.393131 24478 net.cpp:165] Memory required for data: 331280040
I0520 18:14:35.393139 24478 layer_factory.hpp:77] Creating layer ip2
I0520 18:14:35.393154 24478 net.cpp:106] Creating Layer ip2
I0520 18:14:35.393163 24478 net.cpp:454] ip2 <- ip1
I0520 18:14:35.393177 24478 net.cpp:411] ip2 -> ip2
I0520 18:14:35.393658 24478 net.cpp:150] Setting up ip2
I0520 18:14:35.393671 24478 net.cpp:157] Top shape: 210 98 (20580)
I0520 18:14:35.393682 24478 net.cpp:165] Memory required for data: 331362360
I0520 18:14:35.393709 24478 layer_factory.hpp:77] Creating layer relu6
I0520 18:14:35.393723 24478 net.cpp:106] Creating Layer relu6
I0520 18:14:35.393733 24478 net.cpp:454] relu6 <- ip2
I0520 18:14:35.393745 24478 net.cpp:397] relu6 -> ip2 (in-place)
I0520 18:14:35.394274 24478 net.cpp:150] Setting up relu6
I0520 18:14:35.394295 24478 net.cpp:157] Top shape: 210 98 (20580)
I0520 18:14:35.394305 24478 net.cpp:165] Memory required for data: 331444680
I0520 18:14:35.394315 24478 layer_factory.hpp:77] Creating layer drop2
I0520 18:14:35.394330 24478 net.cpp:106] Creating Layer drop2
I0520 18:14:35.394340 24478 net.cpp:454] drop2 <- ip2
I0520 18:14:35.394352 24478 net.cpp:397] drop2 -> ip2 (in-place)
I0520 18:14:35.394397 24478 net.cpp:150] Setting up drop2
I0520 18:14:35.394410 24478 net.cpp:157] Top shape: 210 98 (20580)
I0520 18:14:35.394420 24478 net.cpp:165] Memory required for data: 331527000
I0520 18:14:35.394430 24478 layer_factory.hpp:77] Creating layer ip3
I0520 18:14:35.394444 24478 net.cpp:106] Creating Layer ip3
I0520 18:14:35.394454 24478 net.cpp:454] ip3 <- ip2
I0520 18:14:35.394469 24478 net.cpp:411] ip3 -> ip3
I0520 18:14:35.394692 24478 net.cpp:150] Setting up ip3
I0520 18:14:35.394706 24478 net.cpp:157] Top shape: 210 11 (2310)
I0520 18:14:35.394716 24478 net.cpp:165] Memory required for data: 331536240
I0520 18:14:35.394731 24478 layer_factory.hpp:77] Creating layer drop3
I0520 18:14:35.394744 24478 net.cpp:106] Creating Layer drop3
I0520 18:14:35.394753 24478 net.cpp:454] drop3 <- ip3
I0520 18:14:35.394767 24478 net.cpp:397] drop3 -> ip3 (in-place)
I0520 18:14:35.394809 24478 net.cpp:150] Setting up drop3
I0520 18:14:35.394820 24478 net.cpp:157] Top shape: 210 11 (2310)
I0520 18:14:35.394830 24478 net.cpp:165] Memory required for data: 331545480
I0520 18:14:35.394840 24478 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 18:14:35.394853 24478 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 18:14:35.394863 24478 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 18:14:35.394876 24478 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 18:14:35.394891 24478 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 18:14:35.394965 24478 net.cpp:150] Setting up ip3_drop3_0_split
I0520 18:14:35.394979 24478 net.cpp:157] Top shape: 210 11 (2310)
I0520 18:14:35.394989 24478 net.cpp:157] Top shape: 210 11 (2310)
I0520 18:14:35.395002 24478 net.cpp:165] Memory required for data: 331563960
I0520 18:14:35.395012 24478 layer_factory.hpp:77] Creating layer accuracy
I0520 18:14:35.395035 24478 net.cpp:106] Creating Layer accuracy
I0520 18:14:35.395045 24478 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 18:14:35.395056 24478 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 18:14:35.395069 24478 net.cpp:411] accuracy -> accuracy
I0520 18:14:35.395092 24478 net.cpp:150] Setting up accuracy
I0520 18:14:35.395105 24478 net.cpp:157] Top shape: (1)
I0520 18:14:35.395115 24478 net.cpp:165] Memory required for data: 331563964
I0520 18:14:35.395125 24478 layer_factory.hpp:77] Creating layer loss
I0520 18:14:35.395138 24478 net.cpp:106] Creating Layer loss
I0520 18:14:35.395149 24478 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 18:14:35.395160 24478 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 18:14:35.395174 24478 net.cpp:411] loss -> loss
I0520 18:14:35.395191 24478 layer_factory.hpp:77] Creating layer loss
I0520 18:14:35.395681 24478 net.cpp:150] Setting up loss
I0520 18:14:35.395695 24478 net.cpp:157] Top shape: (1)
I0520 18:14:35.395705 24478 net.cpp:160]     with loss weight 1
I0520 18:14:35.395723 24478 net.cpp:165] Memory required for data: 331563968
I0520 18:14:35.395735 24478 net.cpp:226] loss needs backward computation.
I0520 18:14:35.395745 24478 net.cpp:228] accuracy does not need backward computation.
I0520 18:14:35.395756 24478 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 18:14:35.395767 24478 net.cpp:226] drop3 needs backward computation.
I0520 18:14:35.395778 24478 net.cpp:226] ip3 needs backward computation.
I0520 18:14:35.395789 24478 net.cpp:226] drop2 needs backward computation.
I0520 18:14:35.395809 24478 net.cpp:226] relu6 needs backward computation.
I0520 18:14:35.395819 24478 net.cpp:226] ip2 needs backward computation.
I0520 18:14:35.395829 24478 net.cpp:226] drop1 needs backward computation.
I0520 18:14:35.395838 24478 net.cpp:226] relu5 needs backward computation.
I0520 18:14:35.395848 24478 net.cpp:226] ip1 needs backward computation.
I0520 18:14:35.395859 24478 net.cpp:226] pool4 needs backward computation.
I0520 18:14:35.395869 24478 net.cpp:226] relu4 needs backward computation.
I0520 18:14:35.395877 24478 net.cpp:226] conv4 needs backward computation.
I0520 18:14:35.395889 24478 net.cpp:226] pool3 needs backward computation.
I0520 18:14:35.395900 24478 net.cpp:226] relu3 needs backward computation.
I0520 18:14:35.395910 24478 net.cpp:226] conv3 needs backward computation.
I0520 18:14:35.395920 24478 net.cpp:226] pool2 needs backward computation.
I0520 18:14:35.395930 24478 net.cpp:226] relu2 needs backward computation.
I0520 18:14:35.395939 24478 net.cpp:226] conv2 needs backward computation.
I0520 18:14:35.395949 24478 net.cpp:226] pool1 needs backward computation.
I0520 18:14:35.395961 24478 net.cpp:226] relu1 needs backward computation.
I0520 18:14:35.395970 24478 net.cpp:226] conv1 needs backward computation.
I0520 18:14:35.395982 24478 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 18:14:35.395993 24478 net.cpp:228] data_hdf5 does not need backward computation.
I0520 18:14:35.396003 24478 net.cpp:270] This network produces output accuracy
I0520 18:14:35.396014 24478 net.cpp:270] This network produces output loss
I0520 18:14:35.396042 24478 net.cpp:283] Network initialization done.
I0520 18:14:35.396175 24478 solver.cpp:60] Solver scaffolding done.
I0520 18:14:35.397310 24478 caffe.cpp:212] Starting Optimization
I0520 18:14:35.397328 24478 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 18:14:35.397338 24478 solver.cpp:289] Learning Rate Policy: fixed
I0520 18:14:35.398553 24478 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 18:15:22.065762 24478 solver.cpp:409]     Test net output #0: accuracy = 0.0736161
I0520 18:15:22.065924 24478 solver.cpp:409]     Test net output #1: loss = 2.3991 (* 1 = 2.3991 loss)
I0520 18:15:22.117043 24478 solver.cpp:237] Iteration 0, loss = 2.39849
I0520 18:15:22.117079 24478 solver.cpp:253]     Train net output #0: loss = 2.39849 (* 1 = 2.39849 loss)
I0520 18:15:22.117096 24478 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 18:15:30.407219 24478 solver.cpp:237] Iteration 71, loss = 2.34011
I0520 18:15:30.407265 24478 solver.cpp:253]     Train net output #0: loss = 2.34011 (* 1 = 2.34011 loss)
I0520 18:15:30.407279 24478 sgd_solver.cpp:106] Iteration 71, lr = 0.0025
I0520 18:15:38.694494 24478 solver.cpp:237] Iteration 142, loss = 2.31102
I0520 18:15:38.694528 24478 solver.cpp:253]     Train net output #0: loss = 2.31102 (* 1 = 2.31102 loss)
I0520 18:15:38.694545 24478 sgd_solver.cpp:106] Iteration 142, lr = 0.0025
I0520 18:15:46.984839 24478 solver.cpp:237] Iteration 213, loss = 2.33431
I0520 18:15:46.984874 24478 solver.cpp:253]     Train net output #0: loss = 2.33431 (* 1 = 2.33431 loss)
I0520 18:15:46.984890 24478 sgd_solver.cpp:106] Iteration 213, lr = 0.0025
I0520 18:15:55.277817 24478 solver.cpp:237] Iteration 284, loss = 2.30108
I0520 18:15:55.277964 24478 solver.cpp:253]     Train net output #0: loss = 2.30108 (* 1 = 2.30108 loss)
I0520 18:15:55.277978 24478 sgd_solver.cpp:106] Iteration 284, lr = 0.0025
I0520 18:16:03.570070 24478 solver.cpp:237] Iteration 355, loss = 2.222
I0520 18:16:03.570104 24478 solver.cpp:253]     Train net output #0: loss = 2.222 (* 1 = 2.222 loss)
I0520 18:16:03.570117 24478 sgd_solver.cpp:106] Iteration 355, lr = 0.0025
I0520 18:16:11.864365 24478 solver.cpp:237] Iteration 426, loss = 2.15126
I0520 18:16:11.864399 24478 solver.cpp:253]     Train net output #0: loss = 2.15126 (* 1 = 2.15126 loss)
I0520 18:16:11.864415 24478 sgd_solver.cpp:106] Iteration 426, lr = 0.0025
I0520 18:16:42.274207 24478 solver.cpp:237] Iteration 497, loss = 2.09566
I0520 18:16:42.274365 24478 solver.cpp:253]     Train net output #0: loss = 2.09566 (* 1 = 2.09566 loss)
I0520 18:16:42.274380 24478 sgd_solver.cpp:106] Iteration 497, lr = 0.0025
I0520 18:16:50.564441 24478 solver.cpp:237] Iteration 568, loss = 1.89012
I0520 18:16:50.564476 24478 solver.cpp:253]     Train net output #0: loss = 1.89012 (* 1 = 1.89012 loss)
I0520 18:16:50.564493 24478 sgd_solver.cpp:106] Iteration 568, lr = 0.0025
I0520 18:16:58.852815 24478 solver.cpp:237] Iteration 639, loss = 1.90929
I0520 18:16:58.852850 24478 solver.cpp:253]     Train net output #0: loss = 1.90929 (* 1 = 1.90929 loss)
I0520 18:16:58.852864 24478 sgd_solver.cpp:106] Iteration 639, lr = 0.0025
I0520 18:17:07.141397 24478 solver.cpp:237] Iteration 710, loss = 1.96177
I0520 18:17:07.141438 24478 solver.cpp:253]     Train net output #0: loss = 1.96177 (* 1 = 1.96177 loss)
I0520 18:17:07.141453 24478 sgd_solver.cpp:106] Iteration 710, lr = 0.0025
I0520 18:17:07.492723 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_714.caffemodel
I0520 18:17:07.616025 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_714.solverstate
I0520 18:17:15.509878 24478 solver.cpp:237] Iteration 781, loss = 1.96747
I0520 18:17:15.510028 24478 solver.cpp:253]     Train net output #0: loss = 1.96747 (* 1 = 1.96747 loss)
I0520 18:17:15.510042 24478 sgd_solver.cpp:106] Iteration 781, lr = 0.0025
I0520 18:17:23.807515 24478 solver.cpp:237] Iteration 852, loss = 1.78514
I0520 18:17:23.807548 24478 solver.cpp:253]     Train net output #0: loss = 1.78514 (* 1 = 1.78514 loss)
I0520 18:17:23.807566 24478 sgd_solver.cpp:106] Iteration 852, lr = 0.0025
I0520 18:17:32.093857 24478 solver.cpp:237] Iteration 923, loss = 1.78083
I0520 18:17:32.093904 24478 solver.cpp:253]     Train net output #0: loss = 1.78083 (* 1 = 1.78083 loss)
I0520 18:17:32.093919 24478 sgd_solver.cpp:106] Iteration 923, lr = 0.0025
I0520 18:18:02.558679 24478 solver.cpp:237] Iteration 994, loss = 1.96305
I0520 18:18:02.558835 24478 solver.cpp:253]     Train net output #0: loss = 1.96305 (* 1 = 1.96305 loss)
I0520 18:18:02.558851 24478 sgd_solver.cpp:106] Iteration 994, lr = 0.0025
I0520 18:18:10.849865 24478 solver.cpp:237] Iteration 1065, loss = 1.69604
I0520 18:18:10.849898 24478 solver.cpp:253]     Train net output #0: loss = 1.69604 (* 1 = 1.69604 loss)
I0520 18:18:10.849916 24478 sgd_solver.cpp:106] Iteration 1065, lr = 0.0025
I0520 18:18:19.140322 24478 solver.cpp:237] Iteration 1136, loss = 1.8463
I0520 18:18:19.140357 24478 solver.cpp:253]     Train net output #0: loss = 1.8463 (* 1 = 1.8463 loss)
I0520 18:18:19.140374 24478 sgd_solver.cpp:106] Iteration 1136, lr = 0.0025
I0520 18:18:27.426378 24478 solver.cpp:237] Iteration 1207, loss = 1.71873
I0520 18:18:27.426414 24478 solver.cpp:253]     Train net output #0: loss = 1.71873 (* 1 = 1.71873 loss)
I0520 18:18:27.426435 24478 sgd_solver.cpp:106] Iteration 1207, lr = 0.0025
I0520 18:18:35.723477 24478 solver.cpp:237] Iteration 1278, loss = 1.92584
I0520 18:18:35.723641 24478 solver.cpp:253]     Train net output #0: loss = 1.92584 (* 1 = 1.92584 loss)
I0520 18:18:35.723657 24478 sgd_solver.cpp:106] Iteration 1278, lr = 0.0025
I0520 18:18:44.015565 24478 solver.cpp:237] Iteration 1349, loss = 1.69589
I0520 18:18:44.015604 24478 solver.cpp:253]     Train net output #0: loss = 1.69589 (* 1 = 1.69589 loss)
I0520 18:18:44.015617 24478 sgd_solver.cpp:106] Iteration 1349, lr = 0.0025
I0520 18:18:52.308198 24478 solver.cpp:237] Iteration 1420, loss = 1.86968
I0520 18:18:52.308246 24478 solver.cpp:253]     Train net output #0: loss = 1.86968 (* 1 = 1.86968 loss)
I0520 18:18:52.308261 24478 sgd_solver.cpp:106] Iteration 1420, lr = 0.0025
I0520 18:18:53.126026 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_1428.caffemodel
I0520 18:18:53.245473 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_1428.solverstate
I0520 18:18:53.271553 24478 solver.cpp:341] Iteration 1428, Testing net (#0)
I0520 18:19:39.001087 24478 solver.cpp:409]     Test net output #0: accuracy = 0.640242
I0520 18:19:39.001257 24478 solver.cpp:409]     Test net output #1: loss = 1.28595 (* 1 = 1.28595 loss)
I0520 18:20:08.528450 24478 solver.cpp:237] Iteration 1491, loss = 1.73655
I0520 18:20:08.528498 24478 solver.cpp:253]     Train net output #0: loss = 1.73655 (* 1 = 1.73655 loss)
I0520 18:20:08.528517 24478 sgd_solver.cpp:106] Iteration 1491, lr = 0.0025
I0520 18:20:16.822139 24478 solver.cpp:237] Iteration 1562, loss = 1.71485
I0520 18:20:16.822275 24478 solver.cpp:253]     Train net output #0: loss = 1.71485 (* 1 = 1.71485 loss)
I0520 18:20:16.822289 24478 sgd_solver.cpp:106] Iteration 1562, lr = 0.0025
I0520 18:20:25.115905 24478 solver.cpp:237] Iteration 1633, loss = 1.66411
I0520 18:20:25.115937 24478 solver.cpp:253]     Train net output #0: loss = 1.66411 (* 1 = 1.66411 loss)
I0520 18:20:25.115955 24478 sgd_solver.cpp:106] Iteration 1633, lr = 0.0025
I0520 18:20:33.408676 24478 solver.cpp:237] Iteration 1704, loss = 1.71515
I0520 18:20:33.408723 24478 solver.cpp:253]     Train net output #0: loss = 1.71515 (* 1 = 1.71515 loss)
I0520 18:20:33.408737 24478 sgd_solver.cpp:106] Iteration 1704, lr = 0.0025
I0520 18:20:41.708606 24478 solver.cpp:237] Iteration 1775, loss = 1.72634
I0520 18:20:41.708641 24478 solver.cpp:253]     Train net output #0: loss = 1.72634 (* 1 = 1.72634 loss)
I0520 18:20:41.708657 24478 sgd_solver.cpp:106] Iteration 1775, lr = 0.0025
I0520 18:20:50.001664 24478 solver.cpp:237] Iteration 1846, loss = 1.69111
I0520 18:20:50.001802 24478 solver.cpp:253]     Train net output #0: loss = 1.69111 (* 1 = 1.69111 loss)
I0520 18:20:50.001816 24478 sgd_solver.cpp:106] Iteration 1846, lr = 0.0025
I0520 18:21:20.487984 24478 solver.cpp:237] Iteration 1917, loss = 1.59144
I0520 18:21:20.488148 24478 solver.cpp:253]     Train net output #0: loss = 1.59144 (* 1 = 1.59144 loss)
I0520 18:21:20.488165 24478 sgd_solver.cpp:106] Iteration 1917, lr = 0.0025
I0520 18:21:28.788710 24478 solver.cpp:237] Iteration 1988, loss = 1.55332
I0520 18:21:28.788744 24478 solver.cpp:253]     Train net output #0: loss = 1.55332 (* 1 = 1.55332 loss)
I0520 18:21:28.788761 24478 sgd_solver.cpp:106] Iteration 1988, lr = 0.0025
I0520 18:21:37.089968 24478 solver.cpp:237] Iteration 2059, loss = 1.67674
I0520 18:21:37.090003 24478 solver.cpp:253]     Train net output #0: loss = 1.67674 (* 1 = 1.67674 loss)
I0520 18:21:37.090020 24478 sgd_solver.cpp:106] Iteration 2059, lr = 0.0025
I0520 18:21:45.387171 24478 solver.cpp:237] Iteration 2130, loss = 1.6432
I0520 18:21:45.387213 24478 solver.cpp:253]     Train net output #0: loss = 1.6432 (* 1 = 1.6432 loss)
I0520 18:21:45.387233 24478 sgd_solver.cpp:106] Iteration 2130, lr = 0.0025
I0520 18:21:46.671380 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_2142.caffemodel
I0520 18:21:46.792644 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_2142.solverstate
I0520 18:21:53.752787 24478 solver.cpp:237] Iteration 2201, loss = 1.63434
I0520 18:21:53.752956 24478 solver.cpp:253]     Train net output #0: loss = 1.63434 (* 1 = 1.63434 loss)
I0520 18:21:53.752970 24478 sgd_solver.cpp:106] Iteration 2201, lr = 0.0025
I0520 18:22:02.049159 24478 solver.cpp:237] Iteration 2272, loss = 1.60417
I0520 18:22:02.049192 24478 solver.cpp:253]     Train net output #0: loss = 1.60417 (* 1 = 1.60417 loss)
I0520 18:22:02.049211 24478 sgd_solver.cpp:106] Iteration 2272, lr = 0.0025
I0520 18:22:10.339423 24478 solver.cpp:237] Iteration 2343, loss = 1.62578
I0520 18:22:10.339469 24478 solver.cpp:253]     Train net output #0: loss = 1.62578 (* 1 = 1.62578 loss)
I0520 18:22:10.339488 24478 sgd_solver.cpp:106] Iteration 2343, lr = 0.0025
I0520 18:22:40.822885 24478 solver.cpp:237] Iteration 2414, loss = 1.58853
I0520 18:22:40.823048 24478 solver.cpp:253]     Train net output #0: loss = 1.58853 (* 1 = 1.58853 loss)
I0520 18:22:40.823063 24478 sgd_solver.cpp:106] Iteration 2414, lr = 0.0025
I0520 18:22:49.114047 24478 solver.cpp:237] Iteration 2485, loss = 1.71183
I0520 18:22:49.114079 24478 solver.cpp:253]     Train net output #0: loss = 1.71183 (* 1 = 1.71183 loss)
I0520 18:22:49.114097 24478 sgd_solver.cpp:106] Iteration 2485, lr = 0.0025
I0520 18:22:57.409241 24478 solver.cpp:237] Iteration 2556, loss = 1.54969
I0520 18:22:57.409291 24478 solver.cpp:253]     Train net output #0: loss = 1.54969 (* 1 = 1.54969 loss)
I0520 18:22:57.409307 24478 sgd_solver.cpp:106] Iteration 2556, lr = 0.0025
I0520 18:23:05.702499 24478 solver.cpp:237] Iteration 2627, loss = 1.49212
I0520 18:23:05.702534 24478 solver.cpp:253]     Train net output #0: loss = 1.49212 (* 1 = 1.49212 loss)
I0520 18:23:05.702553 24478 sgd_solver.cpp:106] Iteration 2627, lr = 0.0025
I0520 18:23:13.993494 24478 solver.cpp:237] Iteration 2698, loss = 1.59468
I0520 18:23:13.993633 24478 solver.cpp:253]     Train net output #0: loss = 1.59468 (* 1 = 1.59468 loss)
I0520 18:23:13.993648 24478 sgd_solver.cpp:106] Iteration 2698, lr = 0.0025
I0520 18:23:22.286887 24478 solver.cpp:237] Iteration 2769, loss = 1.69352
I0520 18:23:22.286924 24478 solver.cpp:253]     Train net output #0: loss = 1.69352 (* 1 = 1.69352 loss)
I0520 18:23:22.286947 24478 sgd_solver.cpp:106] Iteration 2769, lr = 0.0025
I0520 18:23:30.579013 24478 solver.cpp:237] Iteration 2840, loss = 1.70031
I0520 18:23:30.579048 24478 solver.cpp:253]     Train net output #0: loss = 1.70031 (* 1 = 1.70031 loss)
I0520 18:23:30.579066 24478 sgd_solver.cpp:106] Iteration 2840, lr = 0.0025
I0520 18:23:32.330353 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_2856.caffemodel
I0520 18:23:32.452934 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_2856.solverstate
I0520 18:23:32.481493 24478 solver.cpp:341] Iteration 2856, Testing net (#0)
I0520 18:24:39.229346 24478 solver.cpp:409]     Test net output #0: accuracy = 0.690577
I0520 18:24:39.229506 24478 solver.cpp:409]     Test net output #1: loss = 1.09633 (* 1 = 1.09633 loss)
I0520 18:25:07.879865 24478 solver.cpp:237] Iteration 2911, loss = 1.62857
I0520 18:25:07.879914 24478 solver.cpp:253]     Train net output #0: loss = 1.62857 (* 1 = 1.62857 loss)
I0520 18:25:07.879928 24478 sgd_solver.cpp:106] Iteration 2911, lr = 0.0025
I0520 18:25:16.175674 24478 solver.cpp:237] Iteration 2982, loss = 1.6398
I0520 18:25:16.175820 24478 solver.cpp:253]     Train net output #0: loss = 1.6398 (* 1 = 1.6398 loss)
I0520 18:25:16.175835 24478 sgd_solver.cpp:106] Iteration 2982, lr = 0.0025
I0520 18:25:24.466018 24478 solver.cpp:237] Iteration 3053, loss = 1.53658
I0520 18:25:24.466051 24478 solver.cpp:253]     Train net output #0: loss = 1.53658 (* 1 = 1.53658 loss)
I0520 18:25:24.466068 24478 sgd_solver.cpp:106] Iteration 3053, lr = 0.0025
I0520 18:25:32.759449 24478 solver.cpp:237] Iteration 3124, loss = 1.6898
I0520 18:25:32.759484 24478 solver.cpp:253]     Train net output #0: loss = 1.6898 (* 1 = 1.6898 loss)
I0520 18:25:32.759506 24478 sgd_solver.cpp:106] Iteration 3124, lr = 0.0025
I0520 18:25:41.052172 24478 solver.cpp:237] Iteration 3195, loss = 1.607
I0520 18:25:41.052206 24478 solver.cpp:253]     Train net output #0: loss = 1.607 (* 1 = 1.607 loss)
I0520 18:25:41.052222 24478 sgd_solver.cpp:106] Iteration 3195, lr = 0.0025
I0520 18:25:49.345677 24478 solver.cpp:237] Iteration 3266, loss = 1.47608
I0520 18:25:49.345811 24478 solver.cpp:253]     Train net output #0: loss = 1.47608 (* 1 = 1.47608 loss)
I0520 18:25:49.345825 24478 sgd_solver.cpp:106] Iteration 3266, lr = 0.0025
I0520 18:26:19.820670 24478 solver.cpp:237] Iteration 3337, loss = 1.52543
I0520 18:26:19.820834 24478 solver.cpp:253]     Train net output #0: loss = 1.52543 (* 1 = 1.52543 loss)
I0520 18:26:19.820850 24478 sgd_solver.cpp:106] Iteration 3337, lr = 0.0025
I0520 18:26:28.112083 24478 solver.cpp:237] Iteration 3408, loss = 1.6111
I0520 18:26:28.112118 24478 solver.cpp:253]     Train net output #0: loss = 1.6111 (* 1 = 1.6111 loss)
I0520 18:26:28.112135 24478 sgd_solver.cpp:106] Iteration 3408, lr = 0.0025
I0520 18:26:36.406973 24478 solver.cpp:237] Iteration 3479, loss = 1.57137
I0520 18:26:36.407008 24478 solver.cpp:253]     Train net output #0: loss = 1.57137 (* 1 = 1.57137 loss)
I0520 18:26:36.407024 24478 sgd_solver.cpp:106] Iteration 3479, lr = 0.0025
I0520 18:26:44.695261 24478 solver.cpp:237] Iteration 3550, loss = 1.54223
I0520 18:26:44.695298 24478 solver.cpp:253]     Train net output #0: loss = 1.54223 (* 1 = 1.54223 loss)
I0520 18:26:44.695319 24478 sgd_solver.cpp:106] Iteration 3550, lr = 0.0025
I0520 18:26:46.914501 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_3570.caffemodel
I0520 18:26:47.036448 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_3570.solverstate
I0520 18:26:53.046699 24478 solver.cpp:237] Iteration 3621, loss = 1.51916
I0520 18:26:53.046859 24478 solver.cpp:253]     Train net output #0: loss = 1.51916 (* 1 = 1.51916 loss)
I0520 18:26:53.046872 24478 sgd_solver.cpp:106] Iteration 3621, lr = 0.0025
I0520 18:27:01.336503 24478 solver.cpp:237] Iteration 3692, loss = 1.62942
I0520 18:27:01.336536 24478 solver.cpp:253]     Train net output #0: loss = 1.62942 (* 1 = 1.62942 loss)
I0520 18:27:01.336555 24478 sgd_solver.cpp:106] Iteration 3692, lr = 0.0025
I0520 18:27:09.629421 24478 solver.cpp:237] Iteration 3763, loss = 1.51742
I0520 18:27:09.629462 24478 solver.cpp:253]     Train net output #0: loss = 1.51742 (* 1 = 1.51742 loss)
I0520 18:27:09.629482 24478 sgd_solver.cpp:106] Iteration 3763, lr = 0.0025
I0520 18:27:40.176398 24478 solver.cpp:237] Iteration 3834, loss = 1.53568
I0520 18:27:40.176568 24478 solver.cpp:253]     Train net output #0: loss = 1.53568 (* 1 = 1.53568 loss)
I0520 18:27:40.176584 24478 sgd_solver.cpp:106] Iteration 3834, lr = 0.0025
I0520 18:27:48.464251 24478 solver.cpp:237] Iteration 3905, loss = 1.57162
I0520 18:27:48.464285 24478 solver.cpp:253]     Train net output #0: loss = 1.57162 (* 1 = 1.57162 loss)
I0520 18:27:48.464303 24478 sgd_solver.cpp:106] Iteration 3905, lr = 0.0025
I0520 18:27:56.752243 24478 solver.cpp:237] Iteration 3976, loss = 1.54915
I0520 18:27:56.752290 24478 solver.cpp:253]     Train net output #0: loss = 1.54915 (* 1 = 1.54915 loss)
I0520 18:27:56.752306 24478 sgd_solver.cpp:106] Iteration 3976, lr = 0.0025
I0520 18:28:05.047852 24478 solver.cpp:237] Iteration 4047, loss = 1.52686
I0520 18:28:05.047885 24478 solver.cpp:253]     Train net output #0: loss = 1.52686 (* 1 = 1.52686 loss)
I0520 18:28:05.047902 24478 sgd_solver.cpp:106] Iteration 4047, lr = 0.0025
I0520 18:28:13.342398 24478 solver.cpp:237] Iteration 4118, loss = 1.52242
I0520 18:28:13.342537 24478 solver.cpp:253]     Train net output #0: loss = 1.52242 (* 1 = 1.52242 loss)
I0520 18:28:13.342551 24478 sgd_solver.cpp:106] Iteration 4118, lr = 0.0025
I0520 18:28:21.642122 24478 solver.cpp:237] Iteration 4189, loss = 1.64127
I0520 18:28:21.642163 24478 solver.cpp:253]     Train net output #0: loss = 1.64127 (* 1 = 1.64127 loss)
I0520 18:28:21.642185 24478 sgd_solver.cpp:106] Iteration 4189, lr = 0.0025
I0520 18:28:29.937407 24478 solver.cpp:237] Iteration 4260, loss = 1.52039
I0520 18:28:29.937441 24478 solver.cpp:253]     Train net output #0: loss = 1.52039 (* 1 = 1.52039 loss)
I0520 18:28:29.937458 24478 sgd_solver.cpp:106] Iteration 4260, lr = 0.0025
I0520 18:28:32.624181 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_4284.caffemodel
I0520 18:28:32.744714 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_4284.solverstate
I0520 18:28:32.770946 24478 solver.cpp:341] Iteration 4284, Testing net (#0)
I0520 18:29:18.126718 24478 solver.cpp:409]     Test net output #0: accuracy = 0.760063
I0520 18:29:18.126883 24478 solver.cpp:409]     Test net output #1: loss = 0.863899 (* 1 = 0.863899 loss)
I0520 18:29:45.916198 24478 solver.cpp:237] Iteration 4331, loss = 1.46686
I0520 18:29:45.916249 24478 solver.cpp:253]     Train net output #0: loss = 1.46686 (* 1 = 1.46686 loss)
I0520 18:29:45.916265 24478 sgd_solver.cpp:106] Iteration 4331, lr = 0.0025
I0520 18:29:54.208672 24478 solver.cpp:237] Iteration 4402, loss = 1.5051
I0520 18:29:54.208822 24478 solver.cpp:253]     Train net output #0: loss = 1.5051 (* 1 = 1.5051 loss)
I0520 18:29:54.208834 24478 sgd_solver.cpp:106] Iteration 4402, lr = 0.0025
I0520 18:30:02.503485 24478 solver.cpp:237] Iteration 4473, loss = 1.48235
I0520 18:30:02.503526 24478 solver.cpp:253]     Train net output #0: loss = 1.48235 (* 1 = 1.48235 loss)
I0520 18:30:02.503546 24478 sgd_solver.cpp:106] Iteration 4473, lr = 0.0025
I0520 18:30:10.790361 24478 solver.cpp:237] Iteration 4544, loss = 1.55102
I0520 18:30:10.790396 24478 solver.cpp:253]     Train net output #0: loss = 1.55102 (* 1 = 1.55102 loss)
I0520 18:30:10.790412 24478 sgd_solver.cpp:106] Iteration 4544, lr = 0.0025
I0520 18:30:19.083125 24478 solver.cpp:237] Iteration 4615, loss = 1.5953
I0520 18:30:19.083159 24478 solver.cpp:253]     Train net output #0: loss = 1.5953 (* 1 = 1.5953 loss)
I0520 18:30:19.083176 24478 sgd_solver.cpp:106] Iteration 4615, lr = 0.0025
I0520 18:30:27.368957 24478 solver.cpp:237] Iteration 4686, loss = 1.53947
I0520 18:30:27.369137 24478 solver.cpp:253]     Train net output #0: loss = 1.53947 (* 1 = 1.53947 loss)
I0520 18:30:27.369151 24478 sgd_solver.cpp:106] Iteration 4686, lr = 0.0025
I0520 18:30:35.653195 24478 solver.cpp:237] Iteration 4757, loss = 1.50904
I0520 18:30:35.653228 24478 solver.cpp:253]     Train net output #0: loss = 1.50904 (* 1 = 1.50904 loss)
I0520 18:30:35.653246 24478 sgd_solver.cpp:106] Iteration 4757, lr = 0.0025
I0520 18:31:06.184815 24478 solver.cpp:237] Iteration 4828, loss = 1.42831
I0520 18:31:06.184983 24478 solver.cpp:253]     Train net output #0: loss = 1.42831 (* 1 = 1.42831 loss)
I0520 18:31:06.184999 24478 sgd_solver.cpp:106] Iteration 4828, lr = 0.0025
I0520 18:31:14.484220 24478 solver.cpp:237] Iteration 4899, loss = 1.41008
I0520 18:31:14.484252 24478 solver.cpp:253]     Train net output #0: loss = 1.41008 (* 1 = 1.41008 loss)
I0520 18:31:14.484271 24478 sgd_solver.cpp:106] Iteration 4899, lr = 0.0025
I0520 18:31:22.778314 24478 solver.cpp:237] Iteration 4970, loss = 1.51805
I0520 18:31:22.778352 24478 solver.cpp:253]     Train net output #0: loss = 1.51805 (* 1 = 1.51805 loss)
I0520 18:31:22.778373 24478 sgd_solver.cpp:106] Iteration 4970, lr = 0.0025
I0520 18:31:25.934293 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_4998.caffemodel
I0520 18:31:26.053696 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_4998.solverstate
I0520 18:31:31.143707 24478 solver.cpp:237] Iteration 5041, loss = 1.39956
I0520 18:31:31.143754 24478 solver.cpp:253]     Train net output #0: loss = 1.39956 (* 1 = 1.39956 loss)
I0520 18:31:31.143769 24478 sgd_solver.cpp:106] Iteration 5041, lr = 0.0025
I0520 18:31:39.437067 24478 solver.cpp:237] Iteration 5112, loss = 1.41585
I0520 18:31:39.437211 24478 solver.cpp:253]     Train net output #0: loss = 1.41585 (* 1 = 1.41585 loss)
I0520 18:31:39.437223 24478 sgd_solver.cpp:106] Iteration 5112, lr = 0.0025
I0520 18:31:47.725914 24478 solver.cpp:237] Iteration 5183, loss = 1.61026
I0520 18:31:47.725952 24478 solver.cpp:253]     Train net output #0: loss = 1.61026 (* 1 = 1.61026 loss)
I0520 18:31:47.725975 24478 sgd_solver.cpp:106] Iteration 5183, lr = 0.0025
I0520 18:32:18.230101 24478 solver.cpp:237] Iteration 5254, loss = 1.38549
I0520 18:32:18.230278 24478 solver.cpp:253]     Train net output #0: loss = 1.38549 (* 1 = 1.38549 loss)
I0520 18:32:18.230294 24478 sgd_solver.cpp:106] Iteration 5254, lr = 0.0025
I0520 18:32:26.516304 24478 solver.cpp:237] Iteration 5325, loss = 1.44005
I0520 18:32:26.516338 24478 solver.cpp:253]     Train net output #0: loss = 1.44005 (* 1 = 1.44005 loss)
I0520 18:32:26.516356 24478 sgd_solver.cpp:106] Iteration 5325, lr = 0.0025
I0520 18:32:34.806203 24478 solver.cpp:237] Iteration 5396, loss = 1.50572
I0520 18:32:34.806252 24478 solver.cpp:253]     Train net output #0: loss = 1.50572 (* 1 = 1.50572 loss)
I0520 18:32:34.806269 24478 sgd_solver.cpp:106] Iteration 5396, lr = 0.0025
I0520 18:32:43.095664 24478 solver.cpp:237] Iteration 5467, loss = 1.49331
I0520 18:32:43.095700 24478 solver.cpp:253]     Train net output #0: loss = 1.49331 (* 1 = 1.49331 loss)
I0520 18:32:43.095715 24478 sgd_solver.cpp:106] Iteration 5467, lr = 0.0025
I0520 18:32:51.385797 24478 solver.cpp:237] Iteration 5538, loss = 1.30043
I0520 18:32:51.385952 24478 solver.cpp:253]     Train net output #0: loss = 1.30043 (* 1 = 1.30043 loss)
I0520 18:32:51.385967 24478 sgd_solver.cpp:106] Iteration 5538, lr = 0.0025
I0520 18:32:59.673538 24478 solver.cpp:237] Iteration 5609, loss = 1.46633
I0520 18:32:59.673575 24478 solver.cpp:253]     Train net output #0: loss = 1.46633 (* 1 = 1.46633 loss)
I0520 18:32:59.673593 24478 sgd_solver.cpp:106] Iteration 5609, lr = 0.0025
I0520 18:33:07.961132 24478 solver.cpp:237] Iteration 5680, loss = 1.34638
I0520 18:33:07.961165 24478 solver.cpp:253]     Train net output #0: loss = 1.34638 (* 1 = 1.34638 loss)
I0520 18:33:07.961182 24478 sgd_solver.cpp:106] Iteration 5680, lr = 0.0025
I0520 18:33:11.577316 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_5712.caffemodel
I0520 18:33:11.706552 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_5712.solverstate
I0520 18:33:11.732940 24478 solver.cpp:341] Iteration 5712, Testing net (#0)
I0520 18:34:18.294059 24478 solver.cpp:409]     Test net output #0: accuracy = 0.796596
I0520 18:34:18.294231 24478 solver.cpp:409]     Test net output #1: loss = 0.718892 (* 1 = 0.718892 loss)
I0520 18:34:45.054183 24478 solver.cpp:237] Iteration 5751, loss = 1.4376
I0520 18:34:45.054234 24478 solver.cpp:253]     Train net output #0: loss = 1.4376 (* 1 = 1.4376 loss)
I0520 18:34:45.054249 24478 sgd_solver.cpp:106] Iteration 5751, lr = 0.0025
I0520 18:34:53.350554 24478 solver.cpp:237] Iteration 5822, loss = 1.29674
I0520 18:34:53.350723 24478 solver.cpp:253]     Train net output #0: loss = 1.29674 (* 1 = 1.29674 loss)
I0520 18:34:53.350736 24478 sgd_solver.cpp:106] Iteration 5822, lr = 0.0025
I0520 18:35:01.643102 24478 solver.cpp:237] Iteration 5893, loss = 1.3958
I0520 18:35:01.643141 24478 solver.cpp:253]     Train net output #0: loss = 1.3958 (* 1 = 1.3958 loss)
I0520 18:35:01.643162 24478 sgd_solver.cpp:106] Iteration 5893, lr = 0.0025
I0520 18:35:09.938755 24478 solver.cpp:237] Iteration 5964, loss = 1.33954
I0520 18:35:09.938789 24478 solver.cpp:253]     Train net output #0: loss = 1.33954 (* 1 = 1.33954 loss)
I0520 18:35:09.938805 24478 sgd_solver.cpp:106] Iteration 5964, lr = 0.0025
I0520 18:35:18.230164 24478 solver.cpp:237] Iteration 6035, loss = 1.36327
I0520 18:35:18.230196 24478 solver.cpp:253]     Train net output #0: loss = 1.36327 (* 1 = 1.36327 loss)
I0520 18:35:18.230214 24478 sgd_solver.cpp:106] Iteration 6035, lr = 0.0025
I0520 18:35:26.527951 24478 solver.cpp:237] Iteration 6106, loss = 1.41648
I0520 18:35:26.528107 24478 solver.cpp:253]     Train net output #0: loss = 1.41648 (* 1 = 1.41648 loss)
I0520 18:35:26.528121 24478 sgd_solver.cpp:106] Iteration 6106, lr = 0.0025
I0520 18:35:34.820338 24478 solver.cpp:237] Iteration 6177, loss = 1.38669
I0520 18:35:34.820372 24478 solver.cpp:253]     Train net output #0: loss = 1.38669 (* 1 = 1.38669 loss)
I0520 18:35:34.820389 24478 sgd_solver.cpp:106] Iteration 6177, lr = 0.0025
I0520 18:36:05.363708 24478 solver.cpp:237] Iteration 6248, loss = 1.42868
I0520 18:36:05.363870 24478 solver.cpp:253]     Train net output #0: loss = 1.42868 (* 1 = 1.42868 loss)
I0520 18:36:05.363885 24478 sgd_solver.cpp:106] Iteration 6248, lr = 0.0025
I0520 18:36:13.654402 24478 solver.cpp:237] Iteration 6319, loss = 1.22058
I0520 18:36:13.654434 24478 solver.cpp:253]     Train net output #0: loss = 1.22058 (* 1 = 1.22058 loss)
I0520 18:36:13.654451 24478 sgd_solver.cpp:106] Iteration 6319, lr = 0.0025
I0520 18:36:21.938432 24478 solver.cpp:237] Iteration 6390, loss = 1.45978
I0520 18:36:21.938470 24478 solver.cpp:253]     Train net output #0: loss = 1.45978 (* 1 = 1.45978 loss)
I0520 18:36:21.938488 24478 sgd_solver.cpp:106] Iteration 6390, lr = 0.0025
I0520 18:36:26.026255 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_6426.caffemodel
I0520 18:36:26.148490 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_6426.solverstate
I0520 18:36:30.299860 24478 solver.cpp:237] Iteration 6461, loss = 1.41697
I0520 18:36:30.299908 24478 solver.cpp:253]     Train net output #0: loss = 1.41697 (* 1 = 1.41697 loss)
I0520 18:36:30.299926 24478 sgd_solver.cpp:106] Iteration 6461, lr = 0.0025
I0520 18:36:38.597987 24478 solver.cpp:237] Iteration 6532, loss = 1.34667
I0520 18:36:38.598147 24478 solver.cpp:253]     Train net output #0: loss = 1.34667 (* 1 = 1.34667 loss)
I0520 18:36:38.598161 24478 sgd_solver.cpp:106] Iteration 6532, lr = 0.0025
I0520 18:36:46.890000 24478 solver.cpp:237] Iteration 6603, loss = 1.23048
I0520 18:36:46.890043 24478 solver.cpp:253]     Train net output #0: loss = 1.23048 (* 1 = 1.23048 loss)
I0520 18:36:46.890060 24478 sgd_solver.cpp:106] Iteration 6603, lr = 0.0025
I0520 18:37:17.458755 24478 solver.cpp:237] Iteration 6674, loss = 1.43757
I0520 18:37:17.458926 24478 solver.cpp:253]     Train net output #0: loss = 1.43757 (* 1 = 1.43757 loss)
I0520 18:37:17.458942 24478 sgd_solver.cpp:106] Iteration 6674, lr = 0.0025
I0520 18:37:25.756454 24478 solver.cpp:237] Iteration 6745, loss = 1.41777
I0520 18:37:25.756489 24478 solver.cpp:253]     Train net output #0: loss = 1.41777 (* 1 = 1.41777 loss)
I0520 18:37:25.756502 24478 sgd_solver.cpp:106] Iteration 6745, lr = 0.0025
I0520 18:37:34.055975 24478 solver.cpp:237] Iteration 6816, loss = 1.50817
I0520 18:37:34.056020 24478 solver.cpp:253]     Train net output #0: loss = 1.50817 (* 1 = 1.50817 loss)
I0520 18:37:34.056036 24478 sgd_solver.cpp:106] Iteration 6816, lr = 0.0025
I0520 18:37:42.346380 24478 solver.cpp:237] Iteration 6887, loss = 1.48556
I0520 18:37:42.346415 24478 solver.cpp:253]     Train net output #0: loss = 1.48556 (* 1 = 1.48556 loss)
I0520 18:37:42.346432 24478 sgd_solver.cpp:106] Iteration 6887, lr = 0.0025
I0520 18:37:50.638636 24478 solver.cpp:237] Iteration 6958, loss = 1.54462
I0520 18:37:50.638780 24478 solver.cpp:253]     Train net output #0: loss = 1.54462 (* 1 = 1.54462 loss)
I0520 18:37:50.638793 24478 sgd_solver.cpp:106] Iteration 6958, lr = 0.0025
I0520 18:37:58.930719 24478 solver.cpp:237] Iteration 7029, loss = 1.38416
I0520 18:37:58.930762 24478 solver.cpp:253]     Train net output #0: loss = 1.38416 (* 1 = 1.38416 loss)
I0520 18:37:58.930781 24478 sgd_solver.cpp:106] Iteration 7029, lr = 0.0025
I0520 18:38:07.227993 24478 solver.cpp:237] Iteration 7100, loss = 1.28148
I0520 18:38:07.228027 24478 solver.cpp:253]     Train net output #0: loss = 1.28148 (* 1 = 1.28148 loss)
I0520 18:38:07.228044 24478 sgd_solver.cpp:106] Iteration 7100, lr = 0.0025
I0520 18:38:11.785421 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_7140.caffemodel
I0520 18:38:11.908030 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_7140.solverstate
I0520 18:38:11.936612 24478 solver.cpp:341] Iteration 7140, Testing net (#0)
I0520 18:38:57.636859 24478 solver.cpp:409]     Test net output #0: accuracy = 0.816845
I0520 18:38:57.637022 24478 solver.cpp:409]     Test net output #1: loss = 0.664276 (* 1 = 0.664276 loss)
I0520 18:38:57.788848 24478 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_7142.caffemodel
I0520 18:38:57.910159 24478 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075_iter_7142.solverstate
I0520 18:38:57.938374 24478 solver.cpp:326] Optimization Done.
I0520 18:38:57.938400 24478 caffe.cpp:215] Optimization Done.
Application 11234217 resources: utime ~1280s, stime ~228s, Rss ~5329144, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_210_2016-05-20T11.20.40.463075.solver"
	User time (seconds): 0.55
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:12.12
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15081
	Voluntary context switches: 2802
	Involuntary context switches: 67
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
