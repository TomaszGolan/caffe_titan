2806348
I0521 07:52:33.882586 19348 caffe.cpp:184] Using GPUs 0
I0521 07:52:34.310644 19348 solver.cpp:48] Initializing solver from parameters: 
test_iter: 180
test_interval: 361
base_lr: 0.0025
display: 18
max_iter: 1807
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 180
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958.prototxt"
I0521 07:52:34.312649 19348 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958.prototxt
I0521 07:52:34.327002 19348 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 07:52:34.327062 19348 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 07:52:34.327405 19348 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 830
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:52:34.327589 19348 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:52:34.327613 19348 net.cpp:106] Creating Layer data_hdf5
I0521 07:52:34.327628 19348 net.cpp:411] data_hdf5 -> data
I0521 07:52:34.327662 19348 net.cpp:411] data_hdf5 -> label
I0521 07:52:34.327695 19348 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 07:52:34.328934 19348 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 07:52:34.331571 19348 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 07:52:55.937799 19348 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 07:52:55.942948 19348 net.cpp:150] Setting up data_hdf5
I0521 07:52:55.942991 19348 net.cpp:157] Top shape: 830 1 127 50 (5270500)
I0521 07:52:55.943006 19348 net.cpp:157] Top shape: 830 (830)
I0521 07:52:55.943017 19348 net.cpp:165] Memory required for data: 21085320
I0521 07:52:55.943030 19348 layer_factory.hpp:77] Creating layer conv1
I0521 07:52:55.943065 19348 net.cpp:106] Creating Layer conv1
I0521 07:52:55.943076 19348 net.cpp:454] conv1 <- data
I0521 07:52:55.943100 19348 net.cpp:411] conv1 -> conv1
I0521 07:52:56.308672 19348 net.cpp:150] Setting up conv1
I0521 07:52:56.308720 19348 net.cpp:157] Top shape: 830 12 120 48 (57369600)
I0521 07:52:56.308732 19348 net.cpp:165] Memory required for data: 250563720
I0521 07:52:56.308759 19348 layer_factory.hpp:77] Creating layer relu1
I0521 07:52:56.308781 19348 net.cpp:106] Creating Layer relu1
I0521 07:52:56.308792 19348 net.cpp:454] relu1 <- conv1
I0521 07:52:56.308806 19348 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:52:56.309397 19348 net.cpp:150] Setting up relu1
I0521 07:52:56.309414 19348 net.cpp:157] Top shape: 830 12 120 48 (57369600)
I0521 07:52:56.309427 19348 net.cpp:165] Memory required for data: 480042120
I0521 07:52:56.309437 19348 layer_factory.hpp:77] Creating layer pool1
I0521 07:52:56.309453 19348 net.cpp:106] Creating Layer pool1
I0521 07:52:56.309463 19348 net.cpp:454] pool1 <- conv1
I0521 07:52:56.309476 19348 net.cpp:411] pool1 -> pool1
I0521 07:52:56.309557 19348 net.cpp:150] Setting up pool1
I0521 07:52:56.309571 19348 net.cpp:157] Top shape: 830 12 60 48 (28684800)
I0521 07:52:56.309581 19348 net.cpp:165] Memory required for data: 594781320
I0521 07:52:56.309589 19348 layer_factory.hpp:77] Creating layer conv2
I0521 07:52:56.309612 19348 net.cpp:106] Creating Layer conv2
I0521 07:52:56.309623 19348 net.cpp:454] conv2 <- pool1
I0521 07:52:56.309636 19348 net.cpp:411] conv2 -> conv2
I0521 07:52:56.312333 19348 net.cpp:150] Setting up conv2
I0521 07:52:56.312361 19348 net.cpp:157] Top shape: 830 20 54 46 (41234400)
I0521 07:52:56.312371 19348 net.cpp:165] Memory required for data: 759718920
I0521 07:52:56.312391 19348 layer_factory.hpp:77] Creating layer relu2
I0521 07:52:56.312405 19348 net.cpp:106] Creating Layer relu2
I0521 07:52:56.312417 19348 net.cpp:454] relu2 <- conv2
I0521 07:52:56.312428 19348 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:52:56.312759 19348 net.cpp:150] Setting up relu2
I0521 07:52:56.312773 19348 net.cpp:157] Top shape: 830 20 54 46 (41234400)
I0521 07:52:56.312784 19348 net.cpp:165] Memory required for data: 924656520
I0521 07:52:56.312794 19348 layer_factory.hpp:77] Creating layer pool2
I0521 07:52:56.312806 19348 net.cpp:106] Creating Layer pool2
I0521 07:52:56.312816 19348 net.cpp:454] pool2 <- conv2
I0521 07:52:56.312842 19348 net.cpp:411] pool2 -> pool2
I0521 07:52:56.312911 19348 net.cpp:150] Setting up pool2
I0521 07:52:56.312924 19348 net.cpp:157] Top shape: 830 20 27 46 (20617200)
I0521 07:52:56.312934 19348 net.cpp:165] Memory required for data: 1007125320
I0521 07:52:56.312944 19348 layer_factory.hpp:77] Creating layer conv3
I0521 07:52:56.312964 19348 net.cpp:106] Creating Layer conv3
I0521 07:52:56.312974 19348 net.cpp:454] conv3 <- pool2
I0521 07:52:56.312988 19348 net.cpp:411] conv3 -> conv3
I0521 07:52:56.314906 19348 net.cpp:150] Setting up conv3
I0521 07:52:56.314929 19348 net.cpp:157] Top shape: 830 28 22 44 (22496320)
I0521 07:52:56.314941 19348 net.cpp:165] Memory required for data: 1097110600
I0521 07:52:56.314960 19348 layer_factory.hpp:77] Creating layer relu3
I0521 07:52:56.314975 19348 net.cpp:106] Creating Layer relu3
I0521 07:52:56.314985 19348 net.cpp:454] relu3 <- conv3
I0521 07:52:56.314998 19348 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:52:56.315469 19348 net.cpp:150] Setting up relu3
I0521 07:52:56.315485 19348 net.cpp:157] Top shape: 830 28 22 44 (22496320)
I0521 07:52:56.315496 19348 net.cpp:165] Memory required for data: 1187095880
I0521 07:52:56.315506 19348 layer_factory.hpp:77] Creating layer pool3
I0521 07:52:56.315520 19348 net.cpp:106] Creating Layer pool3
I0521 07:52:56.315528 19348 net.cpp:454] pool3 <- conv3
I0521 07:52:56.315541 19348 net.cpp:411] pool3 -> pool3
I0521 07:52:56.315608 19348 net.cpp:150] Setting up pool3
I0521 07:52:56.315621 19348 net.cpp:157] Top shape: 830 28 11 44 (11248160)
I0521 07:52:56.315631 19348 net.cpp:165] Memory required for data: 1232088520
I0521 07:52:56.315639 19348 layer_factory.hpp:77] Creating layer conv4
I0521 07:52:56.315657 19348 net.cpp:106] Creating Layer conv4
I0521 07:52:56.315668 19348 net.cpp:454] conv4 <- pool3
I0521 07:52:56.315682 19348 net.cpp:411] conv4 -> conv4
I0521 07:52:56.318409 19348 net.cpp:150] Setting up conv4
I0521 07:52:56.318437 19348 net.cpp:157] Top shape: 830 36 6 42 (7529760)
I0521 07:52:56.318449 19348 net.cpp:165] Memory required for data: 1262207560
I0521 07:52:56.318464 19348 layer_factory.hpp:77] Creating layer relu4
I0521 07:52:56.318478 19348 net.cpp:106] Creating Layer relu4
I0521 07:52:56.318488 19348 net.cpp:454] relu4 <- conv4
I0521 07:52:56.318501 19348 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:52:56.318967 19348 net.cpp:150] Setting up relu4
I0521 07:52:56.318984 19348 net.cpp:157] Top shape: 830 36 6 42 (7529760)
I0521 07:52:56.318994 19348 net.cpp:165] Memory required for data: 1292326600
I0521 07:52:56.319003 19348 layer_factory.hpp:77] Creating layer pool4
I0521 07:52:56.319016 19348 net.cpp:106] Creating Layer pool4
I0521 07:52:56.319026 19348 net.cpp:454] pool4 <- conv4
I0521 07:52:56.319039 19348 net.cpp:411] pool4 -> pool4
I0521 07:52:56.319108 19348 net.cpp:150] Setting up pool4
I0521 07:52:56.319120 19348 net.cpp:157] Top shape: 830 36 3 42 (3764880)
I0521 07:52:56.319131 19348 net.cpp:165] Memory required for data: 1307386120
I0521 07:52:56.319141 19348 layer_factory.hpp:77] Creating layer ip1
I0521 07:52:56.319164 19348 net.cpp:106] Creating Layer ip1
I0521 07:52:56.319174 19348 net.cpp:454] ip1 <- pool4
I0521 07:52:56.319187 19348 net.cpp:411] ip1 -> ip1
I0521 07:52:56.334653 19348 net.cpp:150] Setting up ip1
I0521 07:52:56.334682 19348 net.cpp:157] Top shape: 830 196 (162680)
I0521 07:52:56.334697 19348 net.cpp:165] Memory required for data: 1308036840
I0521 07:52:56.334718 19348 layer_factory.hpp:77] Creating layer relu5
I0521 07:52:56.334733 19348 net.cpp:106] Creating Layer relu5
I0521 07:52:56.334743 19348 net.cpp:454] relu5 <- ip1
I0521 07:52:56.334756 19348 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:52:56.335100 19348 net.cpp:150] Setting up relu5
I0521 07:52:56.335114 19348 net.cpp:157] Top shape: 830 196 (162680)
I0521 07:52:56.335124 19348 net.cpp:165] Memory required for data: 1308687560
I0521 07:52:56.335134 19348 layer_factory.hpp:77] Creating layer drop1
I0521 07:52:56.335156 19348 net.cpp:106] Creating Layer drop1
I0521 07:52:56.335166 19348 net.cpp:454] drop1 <- ip1
I0521 07:52:56.335191 19348 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:52:56.335237 19348 net.cpp:150] Setting up drop1
I0521 07:52:56.335252 19348 net.cpp:157] Top shape: 830 196 (162680)
I0521 07:52:56.335261 19348 net.cpp:165] Memory required for data: 1309338280
I0521 07:52:56.335271 19348 layer_factory.hpp:77] Creating layer ip2
I0521 07:52:56.335290 19348 net.cpp:106] Creating Layer ip2
I0521 07:52:56.335300 19348 net.cpp:454] ip2 <- ip1
I0521 07:52:56.335312 19348 net.cpp:411] ip2 -> ip2
I0521 07:52:56.335777 19348 net.cpp:150] Setting up ip2
I0521 07:52:56.335790 19348 net.cpp:157] Top shape: 830 98 (81340)
I0521 07:52:56.335800 19348 net.cpp:165] Memory required for data: 1309663640
I0521 07:52:56.335824 19348 layer_factory.hpp:77] Creating layer relu6
I0521 07:52:56.335835 19348 net.cpp:106] Creating Layer relu6
I0521 07:52:56.335845 19348 net.cpp:454] relu6 <- ip2
I0521 07:52:56.335857 19348 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:52:56.336386 19348 net.cpp:150] Setting up relu6
I0521 07:52:56.336402 19348 net.cpp:157] Top shape: 830 98 (81340)
I0521 07:52:56.336411 19348 net.cpp:165] Memory required for data: 1309989000
I0521 07:52:56.336422 19348 layer_factory.hpp:77] Creating layer drop2
I0521 07:52:56.336436 19348 net.cpp:106] Creating Layer drop2
I0521 07:52:56.336446 19348 net.cpp:454] drop2 <- ip2
I0521 07:52:56.336457 19348 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:52:56.336499 19348 net.cpp:150] Setting up drop2
I0521 07:52:56.336513 19348 net.cpp:157] Top shape: 830 98 (81340)
I0521 07:52:56.336524 19348 net.cpp:165] Memory required for data: 1310314360
I0521 07:52:56.336534 19348 layer_factory.hpp:77] Creating layer ip3
I0521 07:52:56.336547 19348 net.cpp:106] Creating Layer ip3
I0521 07:52:56.336557 19348 net.cpp:454] ip3 <- ip2
I0521 07:52:56.336570 19348 net.cpp:411] ip3 -> ip3
I0521 07:52:56.336781 19348 net.cpp:150] Setting up ip3
I0521 07:52:56.336793 19348 net.cpp:157] Top shape: 830 11 (9130)
I0521 07:52:56.336803 19348 net.cpp:165] Memory required for data: 1310350880
I0521 07:52:56.336818 19348 layer_factory.hpp:77] Creating layer drop3
I0521 07:52:56.336830 19348 net.cpp:106] Creating Layer drop3
I0521 07:52:56.336840 19348 net.cpp:454] drop3 <- ip3
I0521 07:52:56.336853 19348 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:52:56.336891 19348 net.cpp:150] Setting up drop3
I0521 07:52:56.336905 19348 net.cpp:157] Top shape: 830 11 (9130)
I0521 07:52:56.336915 19348 net.cpp:165] Memory required for data: 1310387400
I0521 07:52:56.336925 19348 layer_factory.hpp:77] Creating layer loss
I0521 07:52:56.336943 19348 net.cpp:106] Creating Layer loss
I0521 07:52:56.336953 19348 net.cpp:454] loss <- ip3
I0521 07:52:56.336966 19348 net.cpp:454] loss <- label
I0521 07:52:56.336977 19348 net.cpp:411] loss -> loss
I0521 07:52:56.336994 19348 layer_factory.hpp:77] Creating layer loss
I0521 07:52:56.337648 19348 net.cpp:150] Setting up loss
I0521 07:52:56.337669 19348 net.cpp:157] Top shape: (1)
I0521 07:52:56.337679 19348 net.cpp:160]     with loss weight 1
I0521 07:52:56.337721 19348 net.cpp:165] Memory required for data: 1310387404
I0521 07:52:56.337733 19348 net.cpp:226] loss needs backward computation.
I0521 07:52:56.337743 19348 net.cpp:226] drop3 needs backward computation.
I0521 07:52:56.337752 19348 net.cpp:226] ip3 needs backward computation.
I0521 07:52:56.337764 19348 net.cpp:226] drop2 needs backward computation.
I0521 07:52:56.337772 19348 net.cpp:226] relu6 needs backward computation.
I0521 07:52:56.337782 19348 net.cpp:226] ip2 needs backward computation.
I0521 07:52:56.337792 19348 net.cpp:226] drop1 needs backward computation.
I0521 07:52:56.337802 19348 net.cpp:226] relu5 needs backward computation.
I0521 07:52:56.337811 19348 net.cpp:226] ip1 needs backward computation.
I0521 07:52:56.337821 19348 net.cpp:226] pool4 needs backward computation.
I0521 07:52:56.337831 19348 net.cpp:226] relu4 needs backward computation.
I0521 07:52:56.337841 19348 net.cpp:226] conv4 needs backward computation.
I0521 07:52:56.337852 19348 net.cpp:226] pool3 needs backward computation.
I0521 07:52:56.337872 19348 net.cpp:226] relu3 needs backward computation.
I0521 07:52:56.337883 19348 net.cpp:226] conv3 needs backward computation.
I0521 07:52:56.337894 19348 net.cpp:226] pool2 needs backward computation.
I0521 07:52:56.337904 19348 net.cpp:226] relu2 needs backward computation.
I0521 07:52:56.337914 19348 net.cpp:226] conv2 needs backward computation.
I0521 07:52:56.337925 19348 net.cpp:226] pool1 needs backward computation.
I0521 07:52:56.337935 19348 net.cpp:226] relu1 needs backward computation.
I0521 07:52:56.337945 19348 net.cpp:226] conv1 needs backward computation.
I0521 07:52:56.337956 19348 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:52:56.337966 19348 net.cpp:270] This network produces output loss
I0521 07:52:56.337990 19348 net.cpp:283] Network initialization done.
I0521 07:52:56.339565 19348 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958.prototxt
I0521 07:52:56.339637 19348 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 07:52:56.340003 19348 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 830
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:52:56.340194 19348 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:52:56.340209 19348 net.cpp:106] Creating Layer data_hdf5
I0521 07:52:56.340221 19348 net.cpp:411] data_hdf5 -> data
I0521 07:52:56.340239 19348 net.cpp:411] data_hdf5 -> label
I0521 07:52:56.340255 19348 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 07:52:56.341415 19348 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 07:53:17.715383 19348 net.cpp:150] Setting up data_hdf5
I0521 07:53:17.715549 19348 net.cpp:157] Top shape: 830 1 127 50 (5270500)
I0521 07:53:17.715564 19348 net.cpp:157] Top shape: 830 (830)
I0521 07:53:17.715574 19348 net.cpp:165] Memory required for data: 21085320
I0521 07:53:17.715589 19348 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 07:53:17.715617 19348 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 07:53:17.715628 19348 net.cpp:454] label_data_hdf5_1_split <- label
I0521 07:53:17.715643 19348 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 07:53:17.715665 19348 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 07:53:17.715739 19348 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 07:53:17.715752 19348 net.cpp:157] Top shape: 830 (830)
I0521 07:53:17.715764 19348 net.cpp:157] Top shape: 830 (830)
I0521 07:53:17.715773 19348 net.cpp:165] Memory required for data: 21091960
I0521 07:53:17.715783 19348 layer_factory.hpp:77] Creating layer conv1
I0521 07:53:17.715806 19348 net.cpp:106] Creating Layer conv1
I0521 07:53:17.715826 19348 net.cpp:454] conv1 <- data
I0521 07:53:17.715840 19348 net.cpp:411] conv1 -> conv1
I0521 07:53:17.717782 19348 net.cpp:150] Setting up conv1
I0521 07:53:17.717800 19348 net.cpp:157] Top shape: 830 12 120 48 (57369600)
I0521 07:53:17.717810 19348 net.cpp:165] Memory required for data: 250570360
I0521 07:53:17.717831 19348 layer_factory.hpp:77] Creating layer relu1
I0521 07:53:17.717846 19348 net.cpp:106] Creating Layer relu1
I0521 07:53:17.717856 19348 net.cpp:454] relu1 <- conv1
I0521 07:53:17.717869 19348 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:53:17.718366 19348 net.cpp:150] Setting up relu1
I0521 07:53:17.718382 19348 net.cpp:157] Top shape: 830 12 120 48 (57369600)
I0521 07:53:17.718392 19348 net.cpp:165] Memory required for data: 480048760
I0521 07:53:17.718403 19348 layer_factory.hpp:77] Creating layer pool1
I0521 07:53:17.718420 19348 net.cpp:106] Creating Layer pool1
I0521 07:53:17.718430 19348 net.cpp:454] pool1 <- conv1
I0521 07:53:17.718442 19348 net.cpp:411] pool1 -> pool1
I0521 07:53:17.718518 19348 net.cpp:150] Setting up pool1
I0521 07:53:17.718531 19348 net.cpp:157] Top shape: 830 12 60 48 (28684800)
I0521 07:53:17.718541 19348 net.cpp:165] Memory required for data: 594787960
I0521 07:53:17.718551 19348 layer_factory.hpp:77] Creating layer conv2
I0521 07:53:17.718569 19348 net.cpp:106] Creating Layer conv2
I0521 07:53:17.718580 19348 net.cpp:454] conv2 <- pool1
I0521 07:53:17.718595 19348 net.cpp:411] conv2 -> conv2
I0521 07:53:17.720521 19348 net.cpp:150] Setting up conv2
I0521 07:53:17.720543 19348 net.cpp:157] Top shape: 830 20 54 46 (41234400)
I0521 07:53:17.720556 19348 net.cpp:165] Memory required for data: 759725560
I0521 07:53:17.720574 19348 layer_factory.hpp:77] Creating layer relu2
I0521 07:53:17.720588 19348 net.cpp:106] Creating Layer relu2
I0521 07:53:17.720598 19348 net.cpp:454] relu2 <- conv2
I0521 07:53:17.720612 19348 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:53:17.720947 19348 net.cpp:150] Setting up relu2
I0521 07:53:17.720962 19348 net.cpp:157] Top shape: 830 20 54 46 (41234400)
I0521 07:53:17.720971 19348 net.cpp:165] Memory required for data: 924663160
I0521 07:53:17.720981 19348 layer_factory.hpp:77] Creating layer pool2
I0521 07:53:17.720995 19348 net.cpp:106] Creating Layer pool2
I0521 07:53:17.721005 19348 net.cpp:454] pool2 <- conv2
I0521 07:53:17.721019 19348 net.cpp:411] pool2 -> pool2
I0521 07:53:17.721089 19348 net.cpp:150] Setting up pool2
I0521 07:53:17.721102 19348 net.cpp:157] Top shape: 830 20 27 46 (20617200)
I0521 07:53:17.721112 19348 net.cpp:165] Memory required for data: 1007131960
I0521 07:53:17.721122 19348 layer_factory.hpp:77] Creating layer conv3
I0521 07:53:17.721139 19348 net.cpp:106] Creating Layer conv3
I0521 07:53:17.721150 19348 net.cpp:454] conv3 <- pool2
I0521 07:53:17.721164 19348 net.cpp:411] conv3 -> conv3
I0521 07:53:17.723145 19348 net.cpp:150] Setting up conv3
I0521 07:53:17.723168 19348 net.cpp:157] Top shape: 830 28 22 44 (22496320)
I0521 07:53:17.723181 19348 net.cpp:165] Memory required for data: 1097117240
I0521 07:53:17.723213 19348 layer_factory.hpp:77] Creating layer relu3
I0521 07:53:17.723227 19348 net.cpp:106] Creating Layer relu3
I0521 07:53:17.723237 19348 net.cpp:454] relu3 <- conv3
I0521 07:53:17.723250 19348 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:53:17.723723 19348 net.cpp:150] Setting up relu3
I0521 07:53:17.723739 19348 net.cpp:157] Top shape: 830 28 22 44 (22496320)
I0521 07:53:17.723749 19348 net.cpp:165] Memory required for data: 1187102520
I0521 07:53:17.723759 19348 layer_factory.hpp:77] Creating layer pool3
I0521 07:53:17.723773 19348 net.cpp:106] Creating Layer pool3
I0521 07:53:17.723783 19348 net.cpp:454] pool3 <- conv3
I0521 07:53:17.723798 19348 net.cpp:411] pool3 -> pool3
I0521 07:53:17.723877 19348 net.cpp:150] Setting up pool3
I0521 07:53:17.723891 19348 net.cpp:157] Top shape: 830 28 11 44 (11248160)
I0521 07:53:17.723901 19348 net.cpp:165] Memory required for data: 1232095160
I0521 07:53:17.723912 19348 layer_factory.hpp:77] Creating layer conv4
I0521 07:53:17.723929 19348 net.cpp:106] Creating Layer conv4
I0521 07:53:17.723939 19348 net.cpp:454] conv4 <- pool3
I0521 07:53:17.723953 19348 net.cpp:411] conv4 -> conv4
I0521 07:53:17.726008 19348 net.cpp:150] Setting up conv4
I0521 07:53:17.726025 19348 net.cpp:157] Top shape: 830 36 6 42 (7529760)
I0521 07:53:17.726037 19348 net.cpp:165] Memory required for data: 1262214200
I0521 07:53:17.726052 19348 layer_factory.hpp:77] Creating layer relu4
I0521 07:53:17.726066 19348 net.cpp:106] Creating Layer relu4
I0521 07:53:17.726076 19348 net.cpp:454] relu4 <- conv4
I0521 07:53:17.726089 19348 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:53:17.726560 19348 net.cpp:150] Setting up relu4
I0521 07:53:17.726577 19348 net.cpp:157] Top shape: 830 36 6 42 (7529760)
I0521 07:53:17.726586 19348 net.cpp:165] Memory required for data: 1292333240
I0521 07:53:17.726598 19348 layer_factory.hpp:77] Creating layer pool4
I0521 07:53:17.726610 19348 net.cpp:106] Creating Layer pool4
I0521 07:53:17.726620 19348 net.cpp:454] pool4 <- conv4
I0521 07:53:17.726634 19348 net.cpp:411] pool4 -> pool4
I0521 07:53:17.726703 19348 net.cpp:150] Setting up pool4
I0521 07:53:17.726717 19348 net.cpp:157] Top shape: 830 36 3 42 (3764880)
I0521 07:53:17.726727 19348 net.cpp:165] Memory required for data: 1307392760
I0521 07:53:17.726734 19348 layer_factory.hpp:77] Creating layer ip1
I0521 07:53:17.726750 19348 net.cpp:106] Creating Layer ip1
I0521 07:53:17.726760 19348 net.cpp:454] ip1 <- pool4
I0521 07:53:17.726774 19348 net.cpp:411] ip1 -> ip1
I0521 07:53:17.742302 19348 net.cpp:150] Setting up ip1
I0521 07:53:17.742331 19348 net.cpp:157] Top shape: 830 196 (162680)
I0521 07:53:17.742344 19348 net.cpp:165] Memory required for data: 1308043480
I0521 07:53:17.742367 19348 layer_factory.hpp:77] Creating layer relu5
I0521 07:53:17.742383 19348 net.cpp:106] Creating Layer relu5
I0521 07:53:17.742393 19348 net.cpp:454] relu5 <- ip1
I0521 07:53:17.742406 19348 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:53:17.742751 19348 net.cpp:150] Setting up relu5
I0521 07:53:17.742765 19348 net.cpp:157] Top shape: 830 196 (162680)
I0521 07:53:17.742775 19348 net.cpp:165] Memory required for data: 1308694200
I0521 07:53:17.742785 19348 layer_factory.hpp:77] Creating layer drop1
I0521 07:53:17.742805 19348 net.cpp:106] Creating Layer drop1
I0521 07:53:17.742815 19348 net.cpp:454] drop1 <- ip1
I0521 07:53:17.742828 19348 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:53:17.742872 19348 net.cpp:150] Setting up drop1
I0521 07:53:17.742885 19348 net.cpp:157] Top shape: 830 196 (162680)
I0521 07:53:17.742897 19348 net.cpp:165] Memory required for data: 1309344920
I0521 07:53:17.742905 19348 layer_factory.hpp:77] Creating layer ip2
I0521 07:53:17.742920 19348 net.cpp:106] Creating Layer ip2
I0521 07:53:17.742930 19348 net.cpp:454] ip2 <- ip1
I0521 07:53:17.742944 19348 net.cpp:411] ip2 -> ip2
I0521 07:53:17.743423 19348 net.cpp:150] Setting up ip2
I0521 07:53:17.743437 19348 net.cpp:157] Top shape: 830 98 (81340)
I0521 07:53:17.743446 19348 net.cpp:165] Memory required for data: 1309670280
I0521 07:53:17.743474 19348 layer_factory.hpp:77] Creating layer relu6
I0521 07:53:17.743487 19348 net.cpp:106] Creating Layer relu6
I0521 07:53:17.743497 19348 net.cpp:454] relu6 <- ip2
I0521 07:53:17.743510 19348 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:53:17.744057 19348 net.cpp:150] Setting up relu6
I0521 07:53:17.744079 19348 net.cpp:157] Top shape: 830 98 (81340)
I0521 07:53:17.744089 19348 net.cpp:165] Memory required for data: 1309995640
I0521 07:53:17.744101 19348 layer_factory.hpp:77] Creating layer drop2
I0521 07:53:17.744114 19348 net.cpp:106] Creating Layer drop2
I0521 07:53:17.744124 19348 net.cpp:454] drop2 <- ip2
I0521 07:53:17.744137 19348 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:53:17.744181 19348 net.cpp:150] Setting up drop2
I0521 07:53:17.744194 19348 net.cpp:157] Top shape: 830 98 (81340)
I0521 07:53:17.744204 19348 net.cpp:165] Memory required for data: 1310321000
I0521 07:53:17.744213 19348 layer_factory.hpp:77] Creating layer ip3
I0521 07:53:17.744228 19348 net.cpp:106] Creating Layer ip3
I0521 07:53:17.744238 19348 net.cpp:454] ip3 <- ip2
I0521 07:53:17.744252 19348 net.cpp:411] ip3 -> ip3
I0521 07:53:17.744474 19348 net.cpp:150] Setting up ip3
I0521 07:53:17.744488 19348 net.cpp:157] Top shape: 830 11 (9130)
I0521 07:53:17.744498 19348 net.cpp:165] Memory required for data: 1310357520
I0521 07:53:17.744513 19348 layer_factory.hpp:77] Creating layer drop3
I0521 07:53:17.744526 19348 net.cpp:106] Creating Layer drop3
I0521 07:53:17.744535 19348 net.cpp:454] drop3 <- ip3
I0521 07:53:17.744549 19348 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:53:17.744590 19348 net.cpp:150] Setting up drop3
I0521 07:53:17.744602 19348 net.cpp:157] Top shape: 830 11 (9130)
I0521 07:53:17.744611 19348 net.cpp:165] Memory required for data: 1310394040
I0521 07:53:17.744621 19348 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 07:53:17.744634 19348 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 07:53:17.744644 19348 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 07:53:17.744657 19348 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 07:53:17.744673 19348 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 07:53:17.744745 19348 net.cpp:150] Setting up ip3_drop3_0_split
I0521 07:53:17.744758 19348 net.cpp:157] Top shape: 830 11 (9130)
I0521 07:53:17.744771 19348 net.cpp:157] Top shape: 830 11 (9130)
I0521 07:53:17.744781 19348 net.cpp:165] Memory required for data: 1310467080
I0521 07:53:17.744791 19348 layer_factory.hpp:77] Creating layer accuracy
I0521 07:53:17.744812 19348 net.cpp:106] Creating Layer accuracy
I0521 07:53:17.744823 19348 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 07:53:17.744834 19348 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 07:53:17.744848 19348 net.cpp:411] accuracy -> accuracy
I0521 07:53:17.744873 19348 net.cpp:150] Setting up accuracy
I0521 07:53:17.744884 19348 net.cpp:157] Top shape: (1)
I0521 07:53:17.744894 19348 net.cpp:165] Memory required for data: 1310467084
I0521 07:53:17.744904 19348 layer_factory.hpp:77] Creating layer loss
I0521 07:53:17.744918 19348 net.cpp:106] Creating Layer loss
I0521 07:53:17.744928 19348 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 07:53:17.744940 19348 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 07:53:17.744952 19348 net.cpp:411] loss -> loss
I0521 07:53:17.744969 19348 layer_factory.hpp:77] Creating layer loss
I0521 07:53:17.745465 19348 net.cpp:150] Setting up loss
I0521 07:53:17.745478 19348 net.cpp:157] Top shape: (1)
I0521 07:53:17.745488 19348 net.cpp:160]     with loss weight 1
I0521 07:53:17.745507 19348 net.cpp:165] Memory required for data: 1310467088
I0521 07:53:17.745517 19348 net.cpp:226] loss needs backward computation.
I0521 07:53:17.745528 19348 net.cpp:228] accuracy does not need backward computation.
I0521 07:53:17.745539 19348 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 07:53:17.745550 19348 net.cpp:226] drop3 needs backward computation.
I0521 07:53:17.745560 19348 net.cpp:226] ip3 needs backward computation.
I0521 07:53:17.745570 19348 net.cpp:226] drop2 needs backward computation.
I0521 07:53:17.745589 19348 net.cpp:226] relu6 needs backward computation.
I0521 07:53:17.745599 19348 net.cpp:226] ip2 needs backward computation.
I0521 07:53:17.745609 19348 net.cpp:226] drop1 needs backward computation.
I0521 07:53:17.745618 19348 net.cpp:226] relu5 needs backward computation.
I0521 07:53:17.745628 19348 net.cpp:226] ip1 needs backward computation.
I0521 07:53:17.745638 19348 net.cpp:226] pool4 needs backward computation.
I0521 07:53:17.745648 19348 net.cpp:226] relu4 needs backward computation.
I0521 07:53:17.745657 19348 net.cpp:226] conv4 needs backward computation.
I0521 07:53:17.745667 19348 net.cpp:226] pool3 needs backward computation.
I0521 07:53:17.745678 19348 net.cpp:226] relu3 needs backward computation.
I0521 07:53:17.745688 19348 net.cpp:226] conv3 needs backward computation.
I0521 07:53:17.745697 19348 net.cpp:226] pool2 needs backward computation.
I0521 07:53:17.745707 19348 net.cpp:226] relu2 needs backward computation.
I0521 07:53:17.745718 19348 net.cpp:226] conv2 needs backward computation.
I0521 07:53:17.745728 19348 net.cpp:226] pool1 needs backward computation.
I0521 07:53:17.745738 19348 net.cpp:226] relu1 needs backward computation.
I0521 07:53:17.745748 19348 net.cpp:226] conv1 needs backward computation.
I0521 07:53:17.745759 19348 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 07:53:17.745770 19348 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:53:17.745779 19348 net.cpp:270] This network produces output accuracy
I0521 07:53:17.745789 19348 net.cpp:270] This network produces output loss
I0521 07:53:17.745816 19348 net.cpp:283] Network initialization done.
I0521 07:53:17.745949 19348 solver.cpp:60] Solver scaffolding done.
I0521 07:53:17.747161 19348 caffe.cpp:212] Starting Optimization
I0521 07:53:17.747180 19348 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 07:53:17.747195 19348 solver.cpp:289] Learning Rate Policy: fixed
I0521 07:53:17.748435 19348 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 07:54:03.770264 19348 solver.cpp:409]     Test net output #0: accuracy = 0.114485
I0521 07:54:03.770426 19348 solver.cpp:409]     Test net output #1: loss = 2.39772 (* 1 = 2.39772 loss)
I0521 07:54:03.924379 19348 solver.cpp:237] Iteration 0, loss = 2.39671
I0521 07:54:03.924417 19348 solver.cpp:253]     Train net output #0: loss = 2.39671 (* 1 = 2.39671 loss)
I0521 07:54:03.924434 19348 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 07:54:11.946180 19348 solver.cpp:237] Iteration 18, loss = 2.38768
I0521 07:54:11.946215 19348 solver.cpp:253]     Train net output #0: loss = 2.38768 (* 1 = 2.38768 loss)
I0521 07:54:11.946229 19348 sgd_solver.cpp:106] Iteration 18, lr = 0.0025
I0521 07:54:19.972836 19348 solver.cpp:237] Iteration 36, loss = 2.37255
I0521 07:54:19.972882 19348 solver.cpp:253]     Train net output #0: loss = 2.37255 (* 1 = 2.37255 loss)
I0521 07:54:19.972897 19348 sgd_solver.cpp:106] Iteration 36, lr = 0.0025
I0521 07:54:28.002123 19348 solver.cpp:237] Iteration 54, loss = 2.36262
I0521 07:54:28.002156 19348 solver.cpp:253]     Train net output #0: loss = 2.36262 (* 1 = 2.36262 loss)
I0521 07:54:28.002172 19348 sgd_solver.cpp:106] Iteration 54, lr = 0.0025
I0521 07:54:36.032665 19348 solver.cpp:237] Iteration 72, loss = 2.34897
I0521 07:54:36.032819 19348 solver.cpp:253]     Train net output #0: loss = 2.34897 (* 1 = 2.34897 loss)
I0521 07:54:36.032832 19348 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0521 07:54:44.059522 19348 solver.cpp:237] Iteration 90, loss = 2.34699
I0521 07:54:44.059566 19348 solver.cpp:253]     Train net output #0: loss = 2.34699 (* 1 = 2.34699 loss)
I0521 07:54:44.059581 19348 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 07:54:52.088042 19348 solver.cpp:237] Iteration 108, loss = 2.35051
I0521 07:54:52.088073 19348 solver.cpp:253]     Train net output #0: loss = 2.35051 (* 1 = 2.35051 loss)
I0521 07:54:52.088090 19348 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0521 07:55:22.266078 19348 solver.cpp:237] Iteration 126, loss = 2.33836
I0521 07:55:22.266239 19348 solver.cpp:253]     Train net output #0: loss = 2.33836 (* 1 = 2.33836 loss)
I0521 07:55:22.266254 19348 sgd_solver.cpp:106] Iteration 126, lr = 0.0025
I0521 07:55:30.289417 19348 solver.cpp:237] Iteration 144, loss = 2.32676
I0521 07:55:30.289458 19348 solver.cpp:253]     Train net output #0: loss = 2.32676 (* 1 = 2.32676 loss)
I0521 07:55:30.289476 19348 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 07:55:38.305413 19348 solver.cpp:237] Iteration 162, loss = 2.32721
I0521 07:55:38.305446 19348 solver.cpp:253]     Train net output #0: loss = 2.32721 (* 1 = 2.32721 loss)
I0521 07:55:38.305462 19348 sgd_solver.cpp:106] Iteration 162, lr = 0.0025
I0521 07:55:45.880168 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_180.caffemodel
I0521 07:55:46.236743 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_180.solverstate
I0521 07:55:46.394815 19348 solver.cpp:237] Iteration 180, loss = 2.3094
I0521 07:55:46.394860 19348 solver.cpp:253]     Train net output #0: loss = 2.3094 (* 1 = 2.3094 loss)
I0521 07:55:46.394878 19348 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 07:55:54.416867 19348 solver.cpp:237] Iteration 198, loss = 2.33485
I0521 07:55:54.417029 19348 solver.cpp:253]     Train net output #0: loss = 2.33485 (* 1 = 2.33485 loss)
I0521 07:55:54.417043 19348 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 07:56:02.445451 19348 solver.cpp:237] Iteration 216, loss = 2.29823
I0521 07:56:02.445484 19348 solver.cpp:253]     Train net output #0: loss = 2.29823 (* 1 = 2.29823 loss)
I0521 07:56:02.445502 19348 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 07:56:10.478819 19348 solver.cpp:237] Iteration 234, loss = 2.30449
I0521 07:56:10.478852 19348 solver.cpp:253]     Train net output #0: loss = 2.30449 (* 1 = 2.30449 loss)
I0521 07:56:10.478868 19348 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0521 07:56:40.710187 19348 solver.cpp:237] Iteration 252, loss = 2.30398
I0521 07:56:40.710343 19348 solver.cpp:253]     Train net output #0: loss = 2.30398 (* 1 = 2.30398 loss)
I0521 07:56:40.710360 19348 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 07:56:48.735254 19348 solver.cpp:237] Iteration 270, loss = 2.28902
I0521 07:56:48.735294 19348 solver.cpp:253]     Train net output #0: loss = 2.28902 (* 1 = 2.28902 loss)
I0521 07:56:48.735311 19348 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 07:56:56.766705 19348 solver.cpp:237] Iteration 288, loss = 2.25821
I0521 07:56:56.766739 19348 solver.cpp:253]     Train net output #0: loss = 2.25821 (* 1 = 2.25821 loss)
I0521 07:56:56.766755 19348 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 07:57:04.799650 19348 solver.cpp:237] Iteration 306, loss = 2.27928
I0521 07:57:04.799684 19348 solver.cpp:253]     Train net output #0: loss = 2.27928 (* 1 = 2.27928 loss)
I0521 07:57:04.799700 19348 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 07:57:12.825315 19348 solver.cpp:237] Iteration 324, loss = 2.26483
I0521 07:57:12.825469 19348 solver.cpp:253]     Train net output #0: loss = 2.26483 (* 1 = 2.26483 loss)
I0521 07:57:12.825482 19348 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0521 07:57:20.857215 19348 solver.cpp:237] Iteration 342, loss = 2.2381
I0521 07:57:20.857247 19348 solver.cpp:253]     Train net output #0: loss = 2.2381 (* 1 = 2.2381 loss)
I0521 07:57:20.857267 19348 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0521 07:57:28.433217 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_360.caffemodel
I0521 07:57:28.785697 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_360.solverstate
I0521 07:57:28.944144 19348 solver.cpp:237] Iteration 360, loss = 2.209
I0521 07:57:28.944191 19348 solver.cpp:253]     Train net output #0: loss = 2.209 (* 1 = 2.209 loss)
I0521 07:57:28.944207 19348 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 07:57:28.944717 19348 solver.cpp:341] Iteration 361, Testing net (#0)
I0521 07:58:14.342142 19348 solver.cpp:409]     Test net output #0: accuracy = 0.392296
I0521 07:58:14.342303 19348 solver.cpp:409]     Test net output #1: loss = 2.11851 (* 1 = 2.11851 loss)
I0521 07:58:44.239048 19348 solver.cpp:237] Iteration 378, loss = 2.18832
I0521 07:58:44.239099 19348 solver.cpp:253]     Train net output #0: loss = 2.18832 (* 1 = 2.18832 loss)
I0521 07:58:44.239114 19348 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 07:58:52.266124 19348 solver.cpp:237] Iteration 396, loss = 2.14538
I0521 07:58:52.266289 19348 solver.cpp:253]     Train net output #0: loss = 2.14538 (* 1 = 2.14538 loss)
I0521 07:58:52.266304 19348 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 07:59:00.293685 19348 solver.cpp:237] Iteration 414, loss = 2.12566
I0521 07:59:00.293718 19348 solver.cpp:253]     Train net output #0: loss = 2.12566 (* 1 = 2.12566 loss)
I0521 07:59:00.293735 19348 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0521 07:59:08.323803 19348 solver.cpp:237] Iteration 432, loss = 2.13375
I0521 07:59:08.323840 19348 solver.cpp:253]     Train net output #0: loss = 2.13375 (* 1 = 2.13375 loss)
I0521 07:59:08.323858 19348 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 07:59:16.355958 19348 solver.cpp:237] Iteration 450, loss = 2.1136
I0521 07:59:16.355994 19348 solver.cpp:253]     Train net output #0: loss = 2.1136 (* 1 = 2.1136 loss)
I0521 07:59:16.356016 19348 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 07:59:24.395367 19348 solver.cpp:237] Iteration 468, loss = 2.07548
I0521 07:59:24.395501 19348 solver.cpp:253]     Train net output #0: loss = 2.07548 (* 1 = 2.07548 loss)
I0521 07:59:24.395515 19348 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0521 07:59:54.757509 19348 solver.cpp:237] Iteration 486, loss = 2.07561
I0521 07:59:54.757669 19348 solver.cpp:253]     Train net output #0: loss = 2.07561 (* 1 = 2.07561 loss)
I0521 07:59:54.757684 19348 sgd_solver.cpp:106] Iteration 486, lr = 0.0025
I0521 08:00:02.788662 19348 solver.cpp:237] Iteration 504, loss = 2.0786
I0521 08:00:02.788696 19348 solver.cpp:253]     Train net output #0: loss = 2.0786 (* 1 = 2.0786 loss)
I0521 08:00:02.788712 19348 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 08:00:10.819139 19348 solver.cpp:237] Iteration 522, loss = 2.06129
I0521 08:00:10.819182 19348 solver.cpp:253]     Train net output #0: loss = 2.06129 (* 1 = 2.06129 loss)
I0521 08:00:10.819198 19348 sgd_solver.cpp:106] Iteration 522, lr = 0.0025
I0521 08:00:18.398191 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_540.caffemodel
I0521 08:00:18.759801 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_540.solverstate
I0521 08:00:18.920261 19348 solver.cpp:237] Iteration 540, loss = 1.95819
I0521 08:00:18.920310 19348 solver.cpp:253]     Train net output #0: loss = 1.95819 (* 1 = 1.95819 loss)
I0521 08:00:18.920325 19348 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 08:00:26.948417 19348 solver.cpp:237] Iteration 558, loss = 2.02508
I0521 08:00:26.948567 19348 solver.cpp:253]     Train net output #0: loss = 2.02508 (* 1 = 2.02508 loss)
I0521 08:00:26.948581 19348 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0521 08:00:34.975132 19348 solver.cpp:237] Iteration 576, loss = 2.02237
I0521 08:00:34.975179 19348 solver.cpp:253]     Train net output #0: loss = 2.02237 (* 1 = 2.02237 loss)
I0521 08:00:34.975195 19348 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 08:00:43.008960 19348 solver.cpp:237] Iteration 594, loss = 2.00847
I0521 08:00:43.008993 19348 solver.cpp:253]     Train net output #0: loss = 2.00847 (* 1 = 2.00847 loss)
I0521 08:00:43.009009 19348 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 08:01:13.223167 19348 solver.cpp:237] Iteration 612, loss = 1.93639
I0521 08:01:13.223328 19348 solver.cpp:253]     Train net output #0: loss = 1.93639 (* 1 = 1.93639 loss)
I0521 08:01:13.223345 19348 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 08:01:21.247452 19348 solver.cpp:237] Iteration 630, loss = 1.99816
I0521 08:01:21.247496 19348 solver.cpp:253]     Train net output #0: loss = 1.99816 (* 1 = 1.99816 loss)
I0521 08:01:21.247514 19348 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 08:01:29.280815 19348 solver.cpp:237] Iteration 648, loss = 1.96386
I0521 08:01:29.280848 19348 solver.cpp:253]     Train net output #0: loss = 1.96386 (* 1 = 1.96386 loss)
I0521 08:01:29.280865 19348 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 08:01:37.311740 19348 solver.cpp:237] Iteration 666, loss = 1.93115
I0521 08:01:37.311774 19348 solver.cpp:253]     Train net output #0: loss = 1.93115 (* 1 = 1.93115 loss)
I0521 08:01:37.311790 19348 sgd_solver.cpp:106] Iteration 666, lr = 0.0025
I0521 08:01:45.340739 19348 solver.cpp:237] Iteration 684, loss = 1.91586
I0521 08:01:45.340884 19348 solver.cpp:253]     Train net output #0: loss = 1.91586 (* 1 = 1.91586 loss)
I0521 08:01:45.340898 19348 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0521 08:01:53.364109 19348 solver.cpp:237] Iteration 702, loss = 1.92693
I0521 08:01:53.364140 19348 solver.cpp:253]     Train net output #0: loss = 1.92693 (* 1 = 1.92693 loss)
I0521 08:01:53.364157 19348 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 08:02:00.945313 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_720.caffemodel
I0521 08:02:01.299715 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_720.solverstate
I0521 08:02:01.460299 19348 solver.cpp:237] Iteration 720, loss = 1.93581
I0521 08:02:01.460345 19348 solver.cpp:253]     Train net output #0: loss = 1.93581 (* 1 = 1.93581 loss)
I0521 08:02:01.460363 19348 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 08:02:01.906131 19348 solver.cpp:341] Iteration 722, Testing net (#0)
I0521 08:03:08.261121 19348 solver.cpp:409]     Test net output #0: accuracy = 0.574237
I0521 08:03:08.261293 19348 solver.cpp:409]     Test net output #1: loss = 1.53256 (* 1 = 1.53256 loss)
I0521 08:03:37.704381 19348 solver.cpp:237] Iteration 738, loss = 1.89581
I0521 08:03:37.704433 19348 solver.cpp:253]     Train net output #0: loss = 1.89581 (* 1 = 1.89581 loss)
I0521 08:03:37.704448 19348 sgd_solver.cpp:106] Iteration 738, lr = 0.0025
I0521 08:03:45.722626 19348 solver.cpp:237] Iteration 756, loss = 1.91054
I0521 08:03:45.722784 19348 solver.cpp:253]     Train net output #0: loss = 1.91054 (* 1 = 1.91054 loss)
I0521 08:03:45.722798 19348 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 08:03:53.742709 19348 solver.cpp:237] Iteration 774, loss = 1.87231
I0521 08:03:53.742753 19348 solver.cpp:253]     Train net output #0: loss = 1.87231 (* 1 = 1.87231 loss)
I0521 08:03:53.742770 19348 sgd_solver.cpp:106] Iteration 774, lr = 0.0025
I0521 08:04:01.759047 19348 solver.cpp:237] Iteration 792, loss = 1.83051
I0521 08:04:01.759079 19348 solver.cpp:253]     Train net output #0: loss = 1.83051 (* 1 = 1.83051 loss)
I0521 08:04:01.759096 19348 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 08:04:09.774704 19348 solver.cpp:237] Iteration 810, loss = 1.8416
I0521 08:04:09.774735 19348 solver.cpp:253]     Train net output #0: loss = 1.8416 (* 1 = 1.8416 loss)
I0521 08:04:09.774754 19348 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 08:04:17.791398 19348 solver.cpp:237] Iteration 828, loss = 1.96375
I0521 08:04:17.791548 19348 solver.cpp:253]     Train net output #0: loss = 1.96375 (* 1 = 1.96375 loss)
I0521 08:04:17.791563 19348 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0521 08:04:47.986791 19348 solver.cpp:237] Iteration 846, loss = 1.89169
I0521 08:04:47.986954 19348 solver.cpp:253]     Train net output #0: loss = 1.89169 (* 1 = 1.89169 loss)
I0521 08:04:47.986970 19348 sgd_solver.cpp:106] Iteration 846, lr = 0.0025
I0521 08:04:56.006841 19348 solver.cpp:237] Iteration 864, loss = 1.82055
I0521 08:04:56.006875 19348 solver.cpp:253]     Train net output #0: loss = 1.82055 (* 1 = 1.82055 loss)
I0521 08:04:56.006889 19348 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 08:05:04.022475 19348 solver.cpp:237] Iteration 882, loss = 1.91592
I0521 08:05:04.022511 19348 solver.cpp:253]     Train net output #0: loss = 1.91592 (* 1 = 1.91592 loss)
I0521 08:05:04.022529 19348 sgd_solver.cpp:106] Iteration 882, lr = 0.0025
I0521 08:05:11.594158 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_900.caffemodel
I0521 08:05:11.947428 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_900.solverstate
I0521 08:05:12.108736 19348 solver.cpp:237] Iteration 900, loss = 1.83148
I0521 08:05:12.108785 19348 solver.cpp:253]     Train net output #0: loss = 1.83148 (* 1 = 1.83148 loss)
I0521 08:05:12.108803 19348 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 08:05:20.127460 19348 solver.cpp:237] Iteration 918, loss = 1.77526
I0521 08:05:20.127601 19348 solver.cpp:253]     Train net output #0: loss = 1.77526 (* 1 = 1.77526 loss)
I0521 08:05:20.127615 19348 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 08:05:28.145434 19348 solver.cpp:237] Iteration 936, loss = 1.76666
I0521 08:05:28.145467 19348 solver.cpp:253]     Train net output #0: loss = 1.76666 (* 1 = 1.76666 loss)
I0521 08:05:28.145483 19348 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 08:05:36.166363 19348 solver.cpp:237] Iteration 954, loss = 1.80854
I0521 08:05:36.166400 19348 solver.cpp:253]     Train net output #0: loss = 1.80854 (* 1 = 1.80854 loss)
I0521 08:05:36.166419 19348 sgd_solver.cpp:106] Iteration 954, lr = 0.0025
I0521 08:06:06.390501 19348 solver.cpp:237] Iteration 972, loss = 1.86435
I0521 08:06:06.390677 19348 solver.cpp:253]     Train net output #0: loss = 1.86435 (* 1 = 1.86435 loss)
I0521 08:06:06.390693 19348 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0521 08:06:14.411695 19348 solver.cpp:237] Iteration 990, loss = 1.84951
I0521 08:06:14.411727 19348 solver.cpp:253]     Train net output #0: loss = 1.84951 (* 1 = 1.84951 loss)
I0521 08:06:14.411746 19348 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 08:06:22.428642 19348 solver.cpp:237] Iteration 1008, loss = 1.83259
I0521 08:06:22.428681 19348 solver.cpp:253]     Train net output #0: loss = 1.83259 (* 1 = 1.83259 loss)
I0521 08:06:22.428702 19348 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 08:06:30.447659 19348 solver.cpp:237] Iteration 1026, loss = 1.75519
I0521 08:06:30.447692 19348 solver.cpp:253]     Train net output #0: loss = 1.75519 (* 1 = 1.75519 loss)
I0521 08:06:30.447710 19348 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 08:06:38.460553 19348 solver.cpp:237] Iteration 1044, loss = 1.86874
I0521 08:06:38.460688 19348 solver.cpp:253]     Train net output #0: loss = 1.86874 (* 1 = 1.86874 loss)
I0521 08:06:38.460701 19348 sgd_solver.cpp:106] Iteration 1044, lr = 0.0025
I0521 08:06:46.476766 19348 solver.cpp:237] Iteration 1062, loss = 1.88327
I0521 08:06:46.476809 19348 solver.cpp:253]     Train net output #0: loss = 1.88327 (* 1 = 1.88327 loss)
I0521 08:06:46.476827 19348 sgd_solver.cpp:106] Iteration 1062, lr = 0.0025
I0521 08:06:54.050935 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1080.caffemodel
I0521 08:06:54.402954 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1080.solverstate
I0521 08:06:54.562611 19348 solver.cpp:237] Iteration 1080, loss = 1.8042
I0521 08:06:54.562656 19348 solver.cpp:253]     Train net output #0: loss = 1.8042 (* 1 = 1.8042 loss)
I0521 08:06:54.562674 19348 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 08:06:55.454272 19348 solver.cpp:341] Iteration 1083, Testing net (#0)
I0521 08:07:40.541697 19348 solver.cpp:409]     Test net output #0: accuracy = 0.606526
I0521 08:07:40.541860 19348 solver.cpp:409]     Test net output #1: loss = 1.3585 (* 1 = 1.3585 loss)
I0521 08:08:09.552016 19348 solver.cpp:237] Iteration 1098, loss = 1.78037
I0521 08:08:09.552067 19348 solver.cpp:253]     Train net output #0: loss = 1.78037 (* 1 = 1.78037 loss)
I0521 08:08:09.552083 19348 sgd_solver.cpp:106] Iteration 1098, lr = 0.0025
I0521 08:08:17.571075 19348 solver.cpp:237] Iteration 1116, loss = 1.81723
I0521 08:08:17.571230 19348 solver.cpp:253]     Train net output #0: loss = 1.81723 (* 1 = 1.81723 loss)
I0521 08:08:17.571244 19348 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0521 08:08:25.590540 19348 solver.cpp:237] Iteration 1134, loss = 1.77858
I0521 08:08:25.590579 19348 solver.cpp:253]     Train net output #0: loss = 1.77858 (* 1 = 1.77858 loss)
I0521 08:08:25.590597 19348 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 08:08:33.614331 19348 solver.cpp:237] Iteration 1152, loss = 1.72802
I0521 08:08:33.614363 19348 solver.cpp:253]     Train net output #0: loss = 1.72802 (* 1 = 1.72802 loss)
I0521 08:08:33.614382 19348 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 08:08:41.634248 19348 solver.cpp:237] Iteration 1170, loss = 1.7895
I0521 08:08:41.634279 19348 solver.cpp:253]     Train net output #0: loss = 1.7895 (* 1 = 1.7895 loss)
I0521 08:08:41.634296 19348 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 08:08:49.659310 19348 solver.cpp:237] Iteration 1188, loss = 1.76496
I0521 08:08:49.659451 19348 solver.cpp:253]     Train net output #0: loss = 1.76496 (* 1 = 1.76496 loss)
I0521 08:08:49.659466 19348 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 08:09:19.837302 19348 solver.cpp:237] Iteration 1206, loss = 1.74879
I0521 08:09:19.837486 19348 solver.cpp:253]     Train net output #0: loss = 1.74879 (* 1 = 1.74879 loss)
I0521 08:09:19.837502 19348 sgd_solver.cpp:106] Iteration 1206, lr = 0.0025
I0521 08:09:27.863562 19348 solver.cpp:237] Iteration 1224, loss = 1.9031
I0521 08:09:27.863595 19348 solver.cpp:253]     Train net output #0: loss = 1.9031 (* 1 = 1.9031 loss)
I0521 08:09:27.863612 19348 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 08:09:35.886090 19348 solver.cpp:237] Iteration 1242, loss = 1.82832
I0521 08:09:35.886118 19348 solver.cpp:253]     Train net output #0: loss = 1.82832 (* 1 = 1.82832 loss)
I0521 08:09:35.886132 19348 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 08:09:43.464321 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1260.caffemodel
I0521 08:09:43.814417 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1260.solverstate
I0521 08:09:43.974334 19348 solver.cpp:237] Iteration 1260, loss = 1.7718
I0521 08:09:43.974380 19348 solver.cpp:253]     Train net output #0: loss = 1.7718 (* 1 = 1.7718 loss)
I0521 08:09:43.974396 19348 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 08:09:52.002367 19348 solver.cpp:237] Iteration 1278, loss = 1.75292
I0521 08:09:52.002509 19348 solver.cpp:253]     Train net output #0: loss = 1.75292 (* 1 = 1.75292 loss)
I0521 08:09:52.002523 19348 sgd_solver.cpp:106] Iteration 1278, lr = 0.0025
I0521 08:10:00.016472 19348 solver.cpp:237] Iteration 1296, loss = 1.83253
I0521 08:10:00.016504 19348 solver.cpp:253]     Train net output #0: loss = 1.83253 (* 1 = 1.83253 loss)
I0521 08:10:00.016521 19348 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 08:10:08.043570 19348 solver.cpp:237] Iteration 1314, loss = 1.72934
I0521 08:10:08.043601 19348 solver.cpp:253]     Train net output #0: loss = 1.72934 (* 1 = 1.72934 loss)
I0521 08:10:08.043619 19348 sgd_solver.cpp:106] Iteration 1314, lr = 0.0025
I0521 08:10:38.270306 19348 solver.cpp:237] Iteration 1332, loss = 1.7347
I0521 08:10:38.270480 19348 solver.cpp:253]     Train net output #0: loss = 1.7347 (* 1 = 1.7347 loss)
I0521 08:10:38.270494 19348 sgd_solver.cpp:106] Iteration 1332, lr = 0.0025
I0521 08:10:46.289605 19348 solver.cpp:237] Iteration 1350, loss = 1.79861
I0521 08:10:46.289638 19348 solver.cpp:253]     Train net output #0: loss = 1.79861 (* 1 = 1.79861 loss)
I0521 08:10:46.289655 19348 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 08:10:54.314194 19348 solver.cpp:237] Iteration 1368, loss = 1.76988
I0521 08:10:54.314244 19348 solver.cpp:253]     Train net output #0: loss = 1.76988 (* 1 = 1.76988 loss)
I0521 08:10:54.314259 19348 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 08:11:02.337443 19348 solver.cpp:237] Iteration 1386, loss = 1.77264
I0521 08:11:02.337477 19348 solver.cpp:253]     Train net output #0: loss = 1.77264 (* 1 = 1.77264 loss)
I0521 08:11:02.337491 19348 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 08:11:10.361316 19348 solver.cpp:237] Iteration 1404, loss = 1.77054
I0521 08:11:10.361459 19348 solver.cpp:253]     Train net output #0: loss = 1.77054 (* 1 = 1.77054 loss)
I0521 08:11:10.361471 19348 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 08:11:18.380898 19348 solver.cpp:237] Iteration 1422, loss = 1.77958
I0521 08:11:18.380929 19348 solver.cpp:253]     Train net output #0: loss = 1.77958 (* 1 = 1.77958 loss)
I0521 08:11:18.380947 19348 sgd_solver.cpp:106] Iteration 1422, lr = 0.0025
I0521 08:11:25.960345 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1440.caffemodel
I0521 08:11:26.312095 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1440.solverstate
I0521 08:11:26.471653 19348 solver.cpp:237] Iteration 1440, loss = 1.73212
I0521 08:11:26.471694 19348 solver.cpp:253]     Train net output #0: loss = 1.73212 (* 1 = 1.73212 loss)
I0521 08:11:26.471714 19348 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 08:11:27.812424 19348 solver.cpp:341] Iteration 1444, Testing net (#0)
I0521 08:12:34.132515 19348 solver.cpp:409]     Test net output #0: accuracy = 0.65332
I0521 08:12:34.132694 19348 solver.cpp:409]     Test net output #1: loss = 1.21524 (* 1 = 1.21524 loss)
I0521 08:13:02.667582 19348 solver.cpp:237] Iteration 1458, loss = 1.71066
I0521 08:13:02.667629 19348 solver.cpp:253]     Train net output #0: loss = 1.71066 (* 1 = 1.71066 loss)
I0521 08:13:02.667647 19348 sgd_solver.cpp:106] Iteration 1458, lr = 0.0025
I0521 08:13:10.689875 19348 solver.cpp:237] Iteration 1476, loss = 1.69407
I0521 08:13:10.690026 19348 solver.cpp:253]     Train net output #0: loss = 1.69407 (* 1 = 1.69407 loss)
I0521 08:13:10.690039 19348 sgd_solver.cpp:106] Iteration 1476, lr = 0.0025
I0521 08:13:18.709249 19348 solver.cpp:237] Iteration 1494, loss = 1.70846
I0521 08:13:18.709280 19348 solver.cpp:253]     Train net output #0: loss = 1.70846 (* 1 = 1.70846 loss)
I0521 08:13:18.709295 19348 sgd_solver.cpp:106] Iteration 1494, lr = 0.0025
I0521 08:13:26.736135 19348 solver.cpp:237] Iteration 1512, loss = 1.77943
I0521 08:13:26.736176 19348 solver.cpp:253]     Train net output #0: loss = 1.77943 (* 1 = 1.77943 loss)
I0521 08:13:26.736191 19348 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 08:13:34.758150 19348 solver.cpp:237] Iteration 1530, loss = 1.71487
I0521 08:13:34.758184 19348 solver.cpp:253]     Train net output #0: loss = 1.71487 (* 1 = 1.71487 loss)
I0521 08:13:34.758200 19348 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 08:13:42.784554 19348 solver.cpp:237] Iteration 1548, loss = 1.67877
I0521 08:13:42.784690 19348 solver.cpp:253]     Train net output #0: loss = 1.67877 (* 1 = 1.67877 loss)
I0521 08:13:42.784703 19348 sgd_solver.cpp:106] Iteration 1548, lr = 0.0025
I0521 08:14:12.996996 19348 solver.cpp:237] Iteration 1566, loss = 1.67712
I0521 08:14:12.997176 19348 solver.cpp:253]     Train net output #0: loss = 1.67712 (* 1 = 1.67712 loss)
I0521 08:14:12.997192 19348 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 08:14:21.019486 19348 solver.cpp:237] Iteration 1584, loss = 1.72538
I0521 08:14:21.019529 19348 solver.cpp:253]     Train net output #0: loss = 1.72538 (* 1 = 1.72538 loss)
I0521 08:14:21.019546 19348 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 08:14:29.043087 19348 solver.cpp:237] Iteration 1602, loss = 1.72287
I0521 08:14:29.043119 19348 solver.cpp:253]     Train net output #0: loss = 1.72287 (* 1 = 1.72287 loss)
I0521 08:14:29.043135 19348 sgd_solver.cpp:106] Iteration 1602, lr = 0.0025
I0521 08:14:36.622707 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1620.caffemodel
I0521 08:14:36.974822 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1620.solverstate
I0521 08:14:37.136243 19348 solver.cpp:237] Iteration 1620, loss = 1.73055
I0521 08:14:37.136293 19348 solver.cpp:253]     Train net output #0: loss = 1.73055 (* 1 = 1.73055 loss)
I0521 08:14:37.136309 19348 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 08:14:45.152173 19348 solver.cpp:237] Iteration 1638, loss = 1.6589
I0521 08:14:45.152330 19348 solver.cpp:253]     Train net output #0: loss = 1.6589 (* 1 = 1.6589 loss)
I0521 08:14:45.152345 19348 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 08:14:53.169993 19348 solver.cpp:237] Iteration 1656, loss = 1.72191
I0521 08:14:53.170027 19348 solver.cpp:253]     Train net output #0: loss = 1.72191 (* 1 = 1.72191 loss)
I0521 08:14:53.170045 19348 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 08:15:01.197860 19348 solver.cpp:237] Iteration 1674, loss = 1.68301
I0521 08:15:01.197892 19348 solver.cpp:253]     Train net output #0: loss = 1.68301 (* 1 = 1.68301 loss)
I0521 08:15:01.197908 19348 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 08:15:31.421324 19348 solver.cpp:237] Iteration 1692, loss = 1.72446
I0521 08:15:31.421504 19348 solver.cpp:253]     Train net output #0: loss = 1.72446 (* 1 = 1.72446 loss)
I0521 08:15:31.421519 19348 sgd_solver.cpp:106] Iteration 1692, lr = 0.0025
I0521 08:15:39.444974 19348 solver.cpp:237] Iteration 1710, loss = 1.70845
I0521 08:15:39.445004 19348 solver.cpp:253]     Train net output #0: loss = 1.70845 (* 1 = 1.70845 loss)
I0521 08:15:39.445022 19348 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 08:15:47.471473 19348 solver.cpp:237] Iteration 1728, loss = 1.66591
I0521 08:15:47.471508 19348 solver.cpp:253]     Train net output #0: loss = 1.66591 (* 1 = 1.66591 loss)
I0521 08:15:47.471524 19348 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 08:15:55.492457 19348 solver.cpp:237] Iteration 1746, loss = 1.68575
I0521 08:15:55.492496 19348 solver.cpp:253]     Train net output #0: loss = 1.68575 (* 1 = 1.68575 loss)
I0521 08:15:55.492514 19348 sgd_solver.cpp:106] Iteration 1746, lr = 0.0025
I0521 08:16:03.519582 19348 solver.cpp:237] Iteration 1764, loss = 1.67555
I0521 08:16:03.519727 19348 solver.cpp:253]     Train net output #0: loss = 1.67555 (* 1 = 1.67555 loss)
I0521 08:16:03.519740 19348 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 08:16:11.541909 19348 solver.cpp:237] Iteration 1782, loss = 1.72411
I0521 08:16:11.541941 19348 solver.cpp:253]     Train net output #0: loss = 1.72411 (* 1 = 1.72411 loss)
I0521 08:16:11.541960 19348 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 08:16:19.118115 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1800.caffemodel
I0521 08:16:19.473026 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1800.solverstate
I0521 08:16:19.634574 19348 solver.cpp:237] Iteration 1800, loss = 1.78159
I0521 08:16:19.634625 19348 solver.cpp:253]     Train net output #0: loss = 1.78159 (* 1 = 1.78159 loss)
I0521 08:16:19.634644 19348 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 08:16:21.419656 19348 solver.cpp:341] Iteration 1805, Testing net (#0)
I0521 08:17:06.778025 19348 solver.cpp:409]     Test net output #0: accuracy = 0.665107
I0521 08:17:06.778189 19348 solver.cpp:409]     Test net output #1: loss = 1.1886 (* 1 = 1.1886 loss)
I0521 08:17:07.357944 19348 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1807.caffemodel
I0521 08:17:07.712368 19348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958_iter_1807.solverstate
I0521 08:17:07.740501 19348 solver.cpp:326] Optimization Done.
I0521 08:17:07.740530 19348 caffe.cpp:215] Optimization Done.
Application 11237270 resources: utime ~1252s, stime ~224s, Rss ~5329496, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_830_2016-05-20T11.21.02.969958.solver"
	User time (seconds): 0.56
	System time (seconds): 0.20
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.76
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15073
	Voluntary context switches: 2975
	Involuntary context switches: 215
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
