2806407
I0521 09:57:52.688797 15582 caffe.cpp:184] Using GPUs 0
I0521 09:57:53.116168 15582 solver.cpp:48] Initializing solver from parameters: 
test_iter: 161
test_interval: 322
base_lr: 0.0025
display: 16
max_iter: 1612
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 161
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537.prototxt"
I0521 09:57:53.117772 15582 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537.prototxt
I0521 09:57:53.134425 15582 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 09:57:53.134485 15582 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 09:57:53.134827 15582 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 930
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:57:53.135010 15582 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:57:53.135035 15582 net.cpp:106] Creating Layer data_hdf5
I0521 09:57:53.135048 15582 net.cpp:411] data_hdf5 -> data
I0521 09:57:53.135082 15582 net.cpp:411] data_hdf5 -> label
I0521 09:57:53.135114 15582 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 09:57:53.136397 15582 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 09:57:53.152806 15582 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 09:58:14.685571 15582 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 09:58:14.690737 15582 net.cpp:150] Setting up data_hdf5
I0521 09:58:14.690776 15582 net.cpp:157] Top shape: 930 1 127 50 (5905500)
I0521 09:58:14.690791 15582 net.cpp:157] Top shape: 930 (930)
I0521 09:58:14.690804 15582 net.cpp:165] Memory required for data: 23625720
I0521 09:58:14.690817 15582 layer_factory.hpp:77] Creating layer conv1
I0521 09:58:14.690850 15582 net.cpp:106] Creating Layer conv1
I0521 09:58:14.690861 15582 net.cpp:454] conv1 <- data
I0521 09:58:14.690882 15582 net.cpp:411] conv1 -> conv1
I0521 09:58:15.052013 15582 net.cpp:150] Setting up conv1
I0521 09:58:15.052057 15582 net.cpp:157] Top shape: 930 12 120 48 (64281600)
I0521 09:58:15.052072 15582 net.cpp:165] Memory required for data: 280752120
I0521 09:58:15.052100 15582 layer_factory.hpp:77] Creating layer relu1
I0521 09:58:15.052121 15582 net.cpp:106] Creating Layer relu1
I0521 09:58:15.052132 15582 net.cpp:454] relu1 <- conv1
I0521 09:58:15.052145 15582 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:58:15.052671 15582 net.cpp:150] Setting up relu1
I0521 09:58:15.052688 15582 net.cpp:157] Top shape: 930 12 120 48 (64281600)
I0521 09:58:15.052700 15582 net.cpp:165] Memory required for data: 537878520
I0521 09:58:15.052711 15582 layer_factory.hpp:77] Creating layer pool1
I0521 09:58:15.052726 15582 net.cpp:106] Creating Layer pool1
I0521 09:58:15.052736 15582 net.cpp:454] pool1 <- conv1
I0521 09:58:15.052749 15582 net.cpp:411] pool1 -> pool1
I0521 09:58:15.052830 15582 net.cpp:150] Setting up pool1
I0521 09:58:15.052845 15582 net.cpp:157] Top shape: 930 12 60 48 (32140800)
I0521 09:58:15.052855 15582 net.cpp:165] Memory required for data: 666441720
I0521 09:58:15.052865 15582 layer_factory.hpp:77] Creating layer conv2
I0521 09:58:15.052887 15582 net.cpp:106] Creating Layer conv2
I0521 09:58:15.052897 15582 net.cpp:454] conv2 <- pool1
I0521 09:58:15.052911 15582 net.cpp:411] conv2 -> conv2
I0521 09:58:15.055585 15582 net.cpp:150] Setting up conv2
I0521 09:58:15.055613 15582 net.cpp:157] Top shape: 930 20 54 46 (46202400)
I0521 09:58:15.055624 15582 net.cpp:165] Memory required for data: 851251320
I0521 09:58:15.055644 15582 layer_factory.hpp:77] Creating layer relu2
I0521 09:58:15.055657 15582 net.cpp:106] Creating Layer relu2
I0521 09:58:15.055667 15582 net.cpp:454] relu2 <- conv2
I0521 09:58:15.055680 15582 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:58:15.056010 15582 net.cpp:150] Setting up relu2
I0521 09:58:15.056023 15582 net.cpp:157] Top shape: 930 20 54 46 (46202400)
I0521 09:58:15.056035 15582 net.cpp:165] Memory required for data: 1036060920
I0521 09:58:15.056044 15582 layer_factory.hpp:77] Creating layer pool2
I0521 09:58:15.056057 15582 net.cpp:106] Creating Layer pool2
I0521 09:58:15.056067 15582 net.cpp:454] pool2 <- conv2
I0521 09:58:15.056092 15582 net.cpp:411] pool2 -> pool2
I0521 09:58:15.056161 15582 net.cpp:150] Setting up pool2
I0521 09:58:15.056175 15582 net.cpp:157] Top shape: 930 20 27 46 (23101200)
I0521 09:58:15.056185 15582 net.cpp:165] Memory required for data: 1128465720
I0521 09:58:15.056195 15582 layer_factory.hpp:77] Creating layer conv3
I0521 09:58:15.056213 15582 net.cpp:106] Creating Layer conv3
I0521 09:58:15.056223 15582 net.cpp:454] conv3 <- pool2
I0521 09:58:15.056236 15582 net.cpp:411] conv3 -> conv3
I0521 09:58:15.058162 15582 net.cpp:150] Setting up conv3
I0521 09:58:15.058185 15582 net.cpp:157] Top shape: 930 28 22 44 (25206720)
I0521 09:58:15.058195 15582 net.cpp:165] Memory required for data: 1229292600
I0521 09:58:15.058215 15582 layer_factory.hpp:77] Creating layer relu3
I0521 09:58:15.058231 15582 net.cpp:106] Creating Layer relu3
I0521 09:58:15.058241 15582 net.cpp:454] relu3 <- conv3
I0521 09:58:15.058254 15582 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:58:15.058722 15582 net.cpp:150] Setting up relu3
I0521 09:58:15.058739 15582 net.cpp:157] Top shape: 930 28 22 44 (25206720)
I0521 09:58:15.058749 15582 net.cpp:165] Memory required for data: 1330119480
I0521 09:58:15.058760 15582 layer_factory.hpp:77] Creating layer pool3
I0521 09:58:15.058773 15582 net.cpp:106] Creating Layer pool3
I0521 09:58:15.058784 15582 net.cpp:454] pool3 <- conv3
I0521 09:58:15.058795 15582 net.cpp:411] pool3 -> pool3
I0521 09:58:15.058863 15582 net.cpp:150] Setting up pool3
I0521 09:58:15.058877 15582 net.cpp:157] Top shape: 930 28 11 44 (12603360)
I0521 09:58:15.058887 15582 net.cpp:165] Memory required for data: 1380532920
I0521 09:58:15.058897 15582 layer_factory.hpp:77] Creating layer conv4
I0521 09:58:15.058915 15582 net.cpp:106] Creating Layer conv4
I0521 09:58:15.058925 15582 net.cpp:454] conv4 <- pool3
I0521 09:58:15.058939 15582 net.cpp:411] conv4 -> conv4
I0521 09:58:15.061682 15582 net.cpp:150] Setting up conv4
I0521 09:58:15.061710 15582 net.cpp:157] Top shape: 930 36 6 42 (8436960)
I0521 09:58:15.061722 15582 net.cpp:165] Memory required for data: 1414280760
I0521 09:58:15.061736 15582 layer_factory.hpp:77] Creating layer relu4
I0521 09:58:15.061750 15582 net.cpp:106] Creating Layer relu4
I0521 09:58:15.061761 15582 net.cpp:454] relu4 <- conv4
I0521 09:58:15.061774 15582 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:58:15.062237 15582 net.cpp:150] Setting up relu4
I0521 09:58:15.062252 15582 net.cpp:157] Top shape: 930 36 6 42 (8436960)
I0521 09:58:15.062263 15582 net.cpp:165] Memory required for data: 1448028600
I0521 09:58:15.062273 15582 layer_factory.hpp:77] Creating layer pool4
I0521 09:58:15.062286 15582 net.cpp:106] Creating Layer pool4
I0521 09:58:15.062295 15582 net.cpp:454] pool4 <- conv4
I0521 09:58:15.062309 15582 net.cpp:411] pool4 -> pool4
I0521 09:58:15.062376 15582 net.cpp:150] Setting up pool4
I0521 09:58:15.062391 15582 net.cpp:157] Top shape: 930 36 3 42 (4218480)
I0521 09:58:15.062400 15582 net.cpp:165] Memory required for data: 1464902520
I0521 09:58:15.062410 15582 layer_factory.hpp:77] Creating layer ip1
I0521 09:58:15.062433 15582 net.cpp:106] Creating Layer ip1
I0521 09:58:15.062444 15582 net.cpp:454] ip1 <- pool4
I0521 09:58:15.062460 15582 net.cpp:411] ip1 -> ip1
I0521 09:58:15.077879 15582 net.cpp:150] Setting up ip1
I0521 09:58:15.077908 15582 net.cpp:157] Top shape: 930 196 (182280)
I0521 09:58:15.077924 15582 net.cpp:165] Memory required for data: 1465631640
I0521 09:58:15.077951 15582 layer_factory.hpp:77] Creating layer relu5
I0521 09:58:15.077965 15582 net.cpp:106] Creating Layer relu5
I0521 09:58:15.077976 15582 net.cpp:454] relu5 <- ip1
I0521 09:58:15.077989 15582 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:58:15.078333 15582 net.cpp:150] Setting up relu5
I0521 09:58:15.078347 15582 net.cpp:157] Top shape: 930 196 (182280)
I0521 09:58:15.078357 15582 net.cpp:165] Memory required for data: 1466360760
I0521 09:58:15.078368 15582 layer_factory.hpp:77] Creating layer drop1
I0521 09:58:15.078390 15582 net.cpp:106] Creating Layer drop1
I0521 09:58:15.078400 15582 net.cpp:454] drop1 <- ip1
I0521 09:58:15.078424 15582 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:58:15.078471 15582 net.cpp:150] Setting up drop1
I0521 09:58:15.078485 15582 net.cpp:157] Top shape: 930 196 (182280)
I0521 09:58:15.078495 15582 net.cpp:165] Memory required for data: 1467089880
I0521 09:58:15.078505 15582 layer_factory.hpp:77] Creating layer ip2
I0521 09:58:15.078523 15582 net.cpp:106] Creating Layer ip2
I0521 09:58:15.078533 15582 net.cpp:454] ip2 <- ip1
I0521 09:58:15.078547 15582 net.cpp:411] ip2 -> ip2
I0521 09:58:15.079011 15582 net.cpp:150] Setting up ip2
I0521 09:58:15.079025 15582 net.cpp:157] Top shape: 930 98 (91140)
I0521 09:58:15.079035 15582 net.cpp:165] Memory required for data: 1467454440
I0521 09:58:15.079049 15582 layer_factory.hpp:77] Creating layer relu6
I0521 09:58:15.079061 15582 net.cpp:106] Creating Layer relu6
I0521 09:58:15.079071 15582 net.cpp:454] relu6 <- ip2
I0521 09:58:15.079083 15582 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:58:15.079599 15582 net.cpp:150] Setting up relu6
I0521 09:58:15.079615 15582 net.cpp:157] Top shape: 930 98 (91140)
I0521 09:58:15.079625 15582 net.cpp:165] Memory required for data: 1467819000
I0521 09:58:15.079637 15582 layer_factory.hpp:77] Creating layer drop2
I0521 09:58:15.079649 15582 net.cpp:106] Creating Layer drop2
I0521 09:58:15.079659 15582 net.cpp:454] drop2 <- ip2
I0521 09:58:15.079671 15582 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:58:15.079713 15582 net.cpp:150] Setting up drop2
I0521 09:58:15.079726 15582 net.cpp:157] Top shape: 930 98 (91140)
I0521 09:58:15.079737 15582 net.cpp:165] Memory required for data: 1468183560
I0521 09:58:15.079747 15582 layer_factory.hpp:77] Creating layer ip3
I0521 09:58:15.079761 15582 net.cpp:106] Creating Layer ip3
I0521 09:58:15.079771 15582 net.cpp:454] ip3 <- ip2
I0521 09:58:15.079782 15582 net.cpp:411] ip3 -> ip3
I0521 09:58:15.079991 15582 net.cpp:150] Setting up ip3
I0521 09:58:15.080005 15582 net.cpp:157] Top shape: 930 11 (10230)
I0521 09:58:15.080015 15582 net.cpp:165] Memory required for data: 1468224480
I0521 09:58:15.080030 15582 layer_factory.hpp:77] Creating layer drop3
I0521 09:58:15.080042 15582 net.cpp:106] Creating Layer drop3
I0521 09:58:15.080052 15582 net.cpp:454] drop3 <- ip3
I0521 09:58:15.080065 15582 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:58:15.080103 15582 net.cpp:150] Setting up drop3
I0521 09:58:15.080116 15582 net.cpp:157] Top shape: 930 11 (10230)
I0521 09:58:15.080127 15582 net.cpp:165] Memory required for data: 1468265400
I0521 09:58:15.080135 15582 layer_factory.hpp:77] Creating layer loss
I0521 09:58:15.080155 15582 net.cpp:106] Creating Layer loss
I0521 09:58:15.080165 15582 net.cpp:454] loss <- ip3
I0521 09:58:15.080176 15582 net.cpp:454] loss <- label
I0521 09:58:15.080188 15582 net.cpp:411] loss -> loss
I0521 09:58:15.080206 15582 layer_factory.hpp:77] Creating layer loss
I0521 09:58:15.080862 15582 net.cpp:150] Setting up loss
I0521 09:58:15.080883 15582 net.cpp:157] Top shape: (1)
I0521 09:58:15.080896 15582 net.cpp:160]     with loss weight 1
I0521 09:58:15.080940 15582 net.cpp:165] Memory required for data: 1468265404
I0521 09:58:15.080950 15582 net.cpp:226] loss needs backward computation.
I0521 09:58:15.080961 15582 net.cpp:226] drop3 needs backward computation.
I0521 09:58:15.080971 15582 net.cpp:226] ip3 needs backward computation.
I0521 09:58:15.080982 15582 net.cpp:226] drop2 needs backward computation.
I0521 09:58:15.080991 15582 net.cpp:226] relu6 needs backward computation.
I0521 09:58:15.081001 15582 net.cpp:226] ip2 needs backward computation.
I0521 09:58:15.081012 15582 net.cpp:226] drop1 needs backward computation.
I0521 09:58:15.081022 15582 net.cpp:226] relu5 needs backward computation.
I0521 09:58:15.081032 15582 net.cpp:226] ip1 needs backward computation.
I0521 09:58:15.081042 15582 net.cpp:226] pool4 needs backward computation.
I0521 09:58:15.081051 15582 net.cpp:226] relu4 needs backward computation.
I0521 09:58:15.081061 15582 net.cpp:226] conv4 needs backward computation.
I0521 09:58:15.081071 15582 net.cpp:226] pool3 needs backward computation.
I0521 09:58:15.081090 15582 net.cpp:226] relu3 needs backward computation.
I0521 09:58:15.081099 15582 net.cpp:226] conv3 needs backward computation.
I0521 09:58:15.081110 15582 net.cpp:226] pool2 needs backward computation.
I0521 09:58:15.081120 15582 net.cpp:226] relu2 needs backward computation.
I0521 09:58:15.081131 15582 net.cpp:226] conv2 needs backward computation.
I0521 09:58:15.081142 15582 net.cpp:226] pool1 needs backward computation.
I0521 09:58:15.081153 15582 net.cpp:226] relu1 needs backward computation.
I0521 09:58:15.081163 15582 net.cpp:226] conv1 needs backward computation.
I0521 09:58:15.081174 15582 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:58:15.081184 15582 net.cpp:270] This network produces output loss
I0521 09:58:15.081207 15582 net.cpp:283] Network initialization done.
I0521 09:58:15.082866 15582 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537.prototxt
I0521 09:58:15.082937 15582 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 09:58:15.083293 15582 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 930
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:58:15.083482 15582 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:58:15.083498 15582 net.cpp:106] Creating Layer data_hdf5
I0521 09:58:15.083510 15582 net.cpp:411] data_hdf5 -> data
I0521 09:58:15.083526 15582 net.cpp:411] data_hdf5 -> label
I0521 09:58:15.083542 15582 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 09:58:15.084800 15582 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 09:58:36.392734 15582 net.cpp:150] Setting up data_hdf5
I0521 09:58:36.392899 15582 net.cpp:157] Top shape: 930 1 127 50 (5905500)
I0521 09:58:36.392913 15582 net.cpp:157] Top shape: 930 (930)
I0521 09:58:36.392925 15582 net.cpp:165] Memory required for data: 23625720
I0521 09:58:36.392938 15582 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 09:58:36.392967 15582 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 09:58:36.392978 15582 net.cpp:454] label_data_hdf5_1_split <- label
I0521 09:58:36.392993 15582 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 09:58:36.393014 15582 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 09:58:36.393087 15582 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 09:58:36.393101 15582 net.cpp:157] Top shape: 930 (930)
I0521 09:58:36.393113 15582 net.cpp:157] Top shape: 930 (930)
I0521 09:58:36.393123 15582 net.cpp:165] Memory required for data: 23633160
I0521 09:58:36.393132 15582 layer_factory.hpp:77] Creating layer conv1
I0521 09:58:36.393154 15582 net.cpp:106] Creating Layer conv1
I0521 09:58:36.393165 15582 net.cpp:454] conv1 <- data
I0521 09:58:36.393182 15582 net.cpp:411] conv1 -> conv1
I0521 09:58:36.395114 15582 net.cpp:150] Setting up conv1
I0521 09:58:36.395138 15582 net.cpp:157] Top shape: 930 12 120 48 (64281600)
I0521 09:58:36.395149 15582 net.cpp:165] Memory required for data: 280759560
I0521 09:58:36.395171 15582 layer_factory.hpp:77] Creating layer relu1
I0521 09:58:36.395186 15582 net.cpp:106] Creating Layer relu1
I0521 09:58:36.395196 15582 net.cpp:454] relu1 <- conv1
I0521 09:58:36.395210 15582 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:58:36.395709 15582 net.cpp:150] Setting up relu1
I0521 09:58:36.395723 15582 net.cpp:157] Top shape: 930 12 120 48 (64281600)
I0521 09:58:36.395735 15582 net.cpp:165] Memory required for data: 537885960
I0521 09:58:36.395745 15582 layer_factory.hpp:77] Creating layer pool1
I0521 09:58:36.395761 15582 net.cpp:106] Creating Layer pool1
I0521 09:58:36.395771 15582 net.cpp:454] pool1 <- conv1
I0521 09:58:36.395783 15582 net.cpp:411] pool1 -> pool1
I0521 09:58:36.395859 15582 net.cpp:150] Setting up pool1
I0521 09:58:36.395871 15582 net.cpp:157] Top shape: 930 12 60 48 (32140800)
I0521 09:58:36.395881 15582 net.cpp:165] Memory required for data: 666449160
I0521 09:58:36.395892 15582 layer_factory.hpp:77] Creating layer conv2
I0521 09:58:36.395910 15582 net.cpp:106] Creating Layer conv2
I0521 09:58:36.395920 15582 net.cpp:454] conv2 <- pool1
I0521 09:58:36.395936 15582 net.cpp:411] conv2 -> conv2
I0521 09:58:36.397855 15582 net.cpp:150] Setting up conv2
I0521 09:58:36.397876 15582 net.cpp:157] Top shape: 930 20 54 46 (46202400)
I0521 09:58:36.397889 15582 net.cpp:165] Memory required for data: 851258760
I0521 09:58:36.397907 15582 layer_factory.hpp:77] Creating layer relu2
I0521 09:58:36.397920 15582 net.cpp:106] Creating Layer relu2
I0521 09:58:36.397930 15582 net.cpp:454] relu2 <- conv2
I0521 09:58:36.397943 15582 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:58:36.398277 15582 net.cpp:150] Setting up relu2
I0521 09:58:36.398290 15582 net.cpp:157] Top shape: 930 20 54 46 (46202400)
I0521 09:58:36.398301 15582 net.cpp:165] Memory required for data: 1036068360
I0521 09:58:36.398311 15582 layer_factory.hpp:77] Creating layer pool2
I0521 09:58:36.398324 15582 net.cpp:106] Creating Layer pool2
I0521 09:58:36.398334 15582 net.cpp:454] pool2 <- conv2
I0521 09:58:36.398346 15582 net.cpp:411] pool2 -> pool2
I0521 09:58:36.398418 15582 net.cpp:150] Setting up pool2
I0521 09:58:36.398432 15582 net.cpp:157] Top shape: 930 20 27 46 (23101200)
I0521 09:58:36.398442 15582 net.cpp:165] Memory required for data: 1128473160
I0521 09:58:36.398449 15582 layer_factory.hpp:77] Creating layer conv3
I0521 09:58:36.398469 15582 net.cpp:106] Creating Layer conv3
I0521 09:58:36.398480 15582 net.cpp:454] conv3 <- pool2
I0521 09:58:36.398494 15582 net.cpp:411] conv3 -> conv3
I0521 09:58:36.400452 15582 net.cpp:150] Setting up conv3
I0521 09:58:36.400475 15582 net.cpp:157] Top shape: 930 28 22 44 (25206720)
I0521 09:58:36.400486 15582 net.cpp:165] Memory required for data: 1229300040
I0521 09:58:36.400519 15582 layer_factory.hpp:77] Creating layer relu3
I0521 09:58:36.400533 15582 net.cpp:106] Creating Layer relu3
I0521 09:58:36.400552 15582 net.cpp:454] relu3 <- conv3
I0521 09:58:36.400564 15582 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:58:36.401036 15582 net.cpp:150] Setting up relu3
I0521 09:58:36.401051 15582 net.cpp:157] Top shape: 930 28 22 44 (25206720)
I0521 09:58:36.401062 15582 net.cpp:165] Memory required for data: 1330126920
I0521 09:58:36.401072 15582 layer_factory.hpp:77] Creating layer pool3
I0521 09:58:36.401084 15582 net.cpp:106] Creating Layer pool3
I0521 09:58:36.401094 15582 net.cpp:454] pool3 <- conv3
I0521 09:58:36.401108 15582 net.cpp:411] pool3 -> pool3
I0521 09:58:36.401180 15582 net.cpp:150] Setting up pool3
I0521 09:58:36.401193 15582 net.cpp:157] Top shape: 930 28 11 44 (12603360)
I0521 09:58:36.401202 15582 net.cpp:165] Memory required for data: 1380540360
I0521 09:58:36.401211 15582 layer_factory.hpp:77] Creating layer conv4
I0521 09:58:36.401228 15582 net.cpp:106] Creating Layer conv4
I0521 09:58:36.401238 15582 net.cpp:454] conv4 <- pool3
I0521 09:58:36.401253 15582 net.cpp:411] conv4 -> conv4
I0521 09:58:36.403298 15582 net.cpp:150] Setting up conv4
I0521 09:58:36.403321 15582 net.cpp:157] Top shape: 930 36 6 42 (8436960)
I0521 09:58:36.403333 15582 net.cpp:165] Memory required for data: 1414288200
I0521 09:58:36.403348 15582 layer_factory.hpp:77] Creating layer relu4
I0521 09:58:36.403362 15582 net.cpp:106] Creating Layer relu4
I0521 09:58:36.403372 15582 net.cpp:454] relu4 <- conv4
I0521 09:58:36.403383 15582 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:58:36.403854 15582 net.cpp:150] Setting up relu4
I0521 09:58:36.403870 15582 net.cpp:157] Top shape: 930 36 6 42 (8436960)
I0521 09:58:36.403879 15582 net.cpp:165] Memory required for data: 1448036040
I0521 09:58:36.403890 15582 layer_factory.hpp:77] Creating layer pool4
I0521 09:58:36.403903 15582 net.cpp:106] Creating Layer pool4
I0521 09:58:36.403913 15582 net.cpp:454] pool4 <- conv4
I0521 09:58:36.403926 15582 net.cpp:411] pool4 -> pool4
I0521 09:58:36.403997 15582 net.cpp:150] Setting up pool4
I0521 09:58:36.404011 15582 net.cpp:157] Top shape: 930 36 3 42 (4218480)
I0521 09:58:36.404021 15582 net.cpp:165] Memory required for data: 1464909960
I0521 09:58:36.404028 15582 layer_factory.hpp:77] Creating layer ip1
I0521 09:58:36.404044 15582 net.cpp:106] Creating Layer ip1
I0521 09:58:36.404054 15582 net.cpp:454] ip1 <- pool4
I0521 09:58:36.404069 15582 net.cpp:411] ip1 -> ip1
I0521 09:58:36.419559 15582 net.cpp:150] Setting up ip1
I0521 09:58:36.419587 15582 net.cpp:157] Top shape: 930 196 (182280)
I0521 09:58:36.419598 15582 net.cpp:165] Memory required for data: 1465639080
I0521 09:58:36.419620 15582 layer_factory.hpp:77] Creating layer relu5
I0521 09:58:36.419636 15582 net.cpp:106] Creating Layer relu5
I0521 09:58:36.419646 15582 net.cpp:454] relu5 <- ip1
I0521 09:58:36.419659 15582 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:58:36.420002 15582 net.cpp:150] Setting up relu5
I0521 09:58:36.420017 15582 net.cpp:157] Top shape: 930 196 (182280)
I0521 09:58:36.420027 15582 net.cpp:165] Memory required for data: 1466368200
I0521 09:58:36.420037 15582 layer_factory.hpp:77] Creating layer drop1
I0521 09:58:36.420055 15582 net.cpp:106] Creating Layer drop1
I0521 09:58:36.420065 15582 net.cpp:454] drop1 <- ip1
I0521 09:58:36.420078 15582 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:58:36.420121 15582 net.cpp:150] Setting up drop1
I0521 09:58:36.420135 15582 net.cpp:157] Top shape: 930 196 (182280)
I0521 09:58:36.420145 15582 net.cpp:165] Memory required for data: 1467097320
I0521 09:58:36.420153 15582 layer_factory.hpp:77] Creating layer ip2
I0521 09:58:36.420168 15582 net.cpp:106] Creating Layer ip2
I0521 09:58:36.420178 15582 net.cpp:454] ip2 <- ip1
I0521 09:58:36.420192 15582 net.cpp:411] ip2 -> ip2
I0521 09:58:36.420678 15582 net.cpp:150] Setting up ip2
I0521 09:58:36.420691 15582 net.cpp:157] Top shape: 930 98 (91140)
I0521 09:58:36.420701 15582 net.cpp:165] Memory required for data: 1467461880
I0521 09:58:36.420729 15582 layer_factory.hpp:77] Creating layer relu6
I0521 09:58:36.420743 15582 net.cpp:106] Creating Layer relu6
I0521 09:58:36.420753 15582 net.cpp:454] relu6 <- ip2
I0521 09:58:36.420765 15582 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:58:36.421293 15582 net.cpp:150] Setting up relu6
I0521 09:58:36.421316 15582 net.cpp:157] Top shape: 930 98 (91140)
I0521 09:58:36.421326 15582 net.cpp:165] Memory required for data: 1467826440
I0521 09:58:36.421336 15582 layer_factory.hpp:77] Creating layer drop2
I0521 09:58:36.421350 15582 net.cpp:106] Creating Layer drop2
I0521 09:58:36.421360 15582 net.cpp:454] drop2 <- ip2
I0521 09:58:36.421372 15582 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:58:36.421416 15582 net.cpp:150] Setting up drop2
I0521 09:58:36.421429 15582 net.cpp:157] Top shape: 930 98 (91140)
I0521 09:58:36.421440 15582 net.cpp:165] Memory required for data: 1468191000
I0521 09:58:36.421450 15582 layer_factory.hpp:77] Creating layer ip3
I0521 09:58:36.421464 15582 net.cpp:106] Creating Layer ip3
I0521 09:58:36.421474 15582 net.cpp:454] ip3 <- ip2
I0521 09:58:36.421489 15582 net.cpp:411] ip3 -> ip3
I0521 09:58:36.421711 15582 net.cpp:150] Setting up ip3
I0521 09:58:36.421725 15582 net.cpp:157] Top shape: 930 11 (10230)
I0521 09:58:36.421735 15582 net.cpp:165] Memory required for data: 1468231920
I0521 09:58:36.421751 15582 layer_factory.hpp:77] Creating layer drop3
I0521 09:58:36.421763 15582 net.cpp:106] Creating Layer drop3
I0521 09:58:36.421773 15582 net.cpp:454] drop3 <- ip3
I0521 09:58:36.421787 15582 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:58:36.421828 15582 net.cpp:150] Setting up drop3
I0521 09:58:36.421839 15582 net.cpp:157] Top shape: 930 11 (10230)
I0521 09:58:36.421849 15582 net.cpp:165] Memory required for data: 1468272840
I0521 09:58:36.421859 15582 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 09:58:36.421872 15582 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 09:58:36.421882 15582 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 09:58:36.421895 15582 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 09:58:36.421910 15582 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 09:58:36.421983 15582 net.cpp:150] Setting up ip3_drop3_0_split
I0521 09:58:36.421995 15582 net.cpp:157] Top shape: 930 11 (10230)
I0521 09:58:36.422008 15582 net.cpp:157] Top shape: 930 11 (10230)
I0521 09:58:36.422019 15582 net.cpp:165] Memory required for data: 1468354680
I0521 09:58:36.422030 15582 layer_factory.hpp:77] Creating layer accuracy
I0521 09:58:36.422056 15582 net.cpp:106] Creating Layer accuracy
I0521 09:58:36.422067 15582 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 09:58:36.422080 15582 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 09:58:36.422093 15582 net.cpp:411] accuracy -> accuracy
I0521 09:58:36.422116 15582 net.cpp:150] Setting up accuracy
I0521 09:58:36.422129 15582 net.cpp:157] Top shape: (1)
I0521 09:58:36.422139 15582 net.cpp:165] Memory required for data: 1468354684
I0521 09:58:36.422150 15582 layer_factory.hpp:77] Creating layer loss
I0521 09:58:36.422163 15582 net.cpp:106] Creating Layer loss
I0521 09:58:36.422173 15582 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 09:58:36.422185 15582 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 09:58:36.422199 15582 net.cpp:411] loss -> loss
I0521 09:58:36.422216 15582 layer_factory.hpp:77] Creating layer loss
I0521 09:58:36.422709 15582 net.cpp:150] Setting up loss
I0521 09:58:36.422724 15582 net.cpp:157] Top shape: (1)
I0521 09:58:36.422734 15582 net.cpp:160]     with loss weight 1
I0521 09:58:36.422751 15582 net.cpp:165] Memory required for data: 1468354688
I0521 09:58:36.422762 15582 net.cpp:226] loss needs backward computation.
I0521 09:58:36.422773 15582 net.cpp:228] accuracy does not need backward computation.
I0521 09:58:36.422785 15582 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 09:58:36.422794 15582 net.cpp:226] drop3 needs backward computation.
I0521 09:58:36.422806 15582 net.cpp:226] ip3 needs backward computation.
I0521 09:58:36.422824 15582 net.cpp:226] drop2 needs backward computation.
I0521 09:58:36.422834 15582 net.cpp:226] relu6 needs backward computation.
I0521 09:58:36.422843 15582 net.cpp:226] ip2 needs backward computation.
I0521 09:58:36.422853 15582 net.cpp:226] drop1 needs backward computation.
I0521 09:58:36.422863 15582 net.cpp:226] relu5 needs backward computation.
I0521 09:58:36.422873 15582 net.cpp:226] ip1 needs backward computation.
I0521 09:58:36.422883 15582 net.cpp:226] pool4 needs backward computation.
I0521 09:58:36.422893 15582 net.cpp:226] relu4 needs backward computation.
I0521 09:58:36.422902 15582 net.cpp:226] conv4 needs backward computation.
I0521 09:58:36.422912 15582 net.cpp:226] pool3 needs backward computation.
I0521 09:58:36.422921 15582 net.cpp:226] relu3 needs backward computation.
I0521 09:58:36.422931 15582 net.cpp:226] conv3 needs backward computation.
I0521 09:58:36.422941 15582 net.cpp:226] pool2 needs backward computation.
I0521 09:58:36.422953 15582 net.cpp:226] relu2 needs backward computation.
I0521 09:58:36.422962 15582 net.cpp:226] conv2 needs backward computation.
I0521 09:58:36.422972 15582 net.cpp:226] pool1 needs backward computation.
I0521 09:58:36.422982 15582 net.cpp:226] relu1 needs backward computation.
I0521 09:58:36.422992 15582 net.cpp:226] conv1 needs backward computation.
I0521 09:58:36.423004 15582 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 09:58:36.423015 15582 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:58:36.423025 15582 net.cpp:270] This network produces output accuracy
I0521 09:58:36.423033 15582 net.cpp:270] This network produces output loss
I0521 09:58:36.423063 15582 net.cpp:283] Network initialization done.
I0521 09:58:36.423195 15582 solver.cpp:60] Solver scaffolding done.
I0521 09:58:36.424319 15582 caffe.cpp:212] Starting Optimization
I0521 09:58:36.424337 15582 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 09:58:36.424350 15582 solver.cpp:289] Learning Rate Policy: fixed
I0521 09:58:36.425595 15582 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 09:59:22.356340 15582 solver.cpp:409]     Test net output #0: accuracy = 0.0557537
I0521 09:59:22.356499 15582 solver.cpp:409]     Test net output #1: loss = 2.4006 (* 1 = 2.4006 loss)
I0521 09:59:22.526666 15582 solver.cpp:237] Iteration 0, loss = 2.39961
I0521 09:59:22.526703 15582 solver.cpp:253]     Train net output #0: loss = 2.39961 (* 1 = 2.39961 loss)
I0521 09:59:22.526721 15582 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 09:59:30.504282 15582 solver.cpp:237] Iteration 16, loss = 2.39134
I0521 09:59:30.504318 15582 solver.cpp:253]     Train net output #0: loss = 2.39134 (* 1 = 2.39134 loss)
I0521 09:59:30.504333 15582 sgd_solver.cpp:106] Iteration 16, lr = 0.0025
I0521 09:59:38.484405 15582 solver.cpp:237] Iteration 32, loss = 2.37901
I0521 09:59:38.484449 15582 solver.cpp:253]     Train net output #0: loss = 2.37901 (* 1 = 2.37901 loss)
I0521 09:59:38.484465 15582 sgd_solver.cpp:106] Iteration 32, lr = 0.0025
I0521 09:59:46.460844 15582 solver.cpp:237] Iteration 48, loss = 2.36752
I0521 09:59:46.460876 15582 solver.cpp:253]     Train net output #0: loss = 2.36752 (* 1 = 2.36752 loss)
I0521 09:59:46.460891 15582 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0521 09:59:54.428045 15582 solver.cpp:237] Iteration 64, loss = 2.35773
I0521 09:59:54.428186 15582 solver.cpp:253]     Train net output #0: loss = 2.35773 (* 1 = 2.35773 loss)
I0521 09:59:54.428202 15582 sgd_solver.cpp:106] Iteration 64, lr = 0.0025
I0521 10:00:02.402144 15582 solver.cpp:237] Iteration 80, loss = 2.35703
I0521 10:00:02.402179 15582 solver.cpp:253]     Train net output #0: loss = 2.35703 (* 1 = 2.35703 loss)
I0521 10:00:02.402201 15582 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0521 10:00:10.379726 15582 solver.cpp:237] Iteration 96, loss = 2.34758
I0521 10:00:10.379758 15582 solver.cpp:253]     Train net output #0: loss = 2.34758 (* 1 = 2.34758 loss)
I0521 10:00:10.379775 15582 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 10:00:40.447171 15582 solver.cpp:237] Iteration 112, loss = 2.32877
I0521 10:00:40.447330 15582 solver.cpp:253]     Train net output #0: loss = 2.32877 (* 1 = 2.32877 loss)
I0521 10:00:40.447345 15582 sgd_solver.cpp:106] Iteration 112, lr = 0.0025
I0521 10:00:48.428247 15582 solver.cpp:237] Iteration 128, loss = 2.32539
I0521 10:00:48.428282 15582 solver.cpp:253]     Train net output #0: loss = 2.32539 (* 1 = 2.32539 loss)
I0521 10:00:48.428297 15582 sgd_solver.cpp:106] Iteration 128, lr = 0.0025
I0521 10:00:56.414530 15582 solver.cpp:237] Iteration 144, loss = 2.32418
I0521 10:00:56.414574 15582 solver.cpp:253]     Train net output #0: loss = 2.32418 (* 1 = 2.32418 loss)
I0521 10:00:56.414590 15582 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 10:01:04.391813 15582 solver.cpp:237] Iteration 160, loss = 2.30538
I0521 10:01:04.391845 15582 solver.cpp:253]     Train net output #0: loss = 2.30538 (* 1 = 2.30538 loss)
I0521 10:01:04.391861 15582 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 10:01:04.392241 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_161.caffemodel
I0521 10:01:04.784576 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_161.solverstate
I0521 10:01:12.435220 15582 solver.cpp:237] Iteration 176, loss = 2.30184
I0521 10:01:12.435374 15582 solver.cpp:253]     Train net output #0: loss = 2.30184 (* 1 = 2.30184 loss)
I0521 10:01:12.435387 15582 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 10:01:20.402930 15582 solver.cpp:237] Iteration 192, loss = 2.30285
I0521 10:01:20.402966 15582 solver.cpp:253]     Train net output #0: loss = 2.30285 (* 1 = 2.30285 loss)
I0521 10:01:20.402987 15582 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 10:01:28.380992 15582 solver.cpp:237] Iteration 208, loss = 2.29157
I0521 10:01:28.381026 15582 solver.cpp:253]     Train net output #0: loss = 2.29157 (* 1 = 2.29157 loss)
I0521 10:01:28.381043 15582 sgd_solver.cpp:106] Iteration 208, lr = 0.0025
I0521 10:01:58.445230 15582 solver.cpp:237] Iteration 224, loss = 2.28872
I0521 10:01:58.445394 15582 solver.cpp:253]     Train net output #0: loss = 2.28872 (* 1 = 2.28872 loss)
I0521 10:01:58.445410 15582 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 10:02:06.421468 15582 solver.cpp:237] Iteration 240, loss = 2.24295
I0521 10:02:06.421501 15582 solver.cpp:253]     Train net output #0: loss = 2.24295 (* 1 = 2.24295 loss)
I0521 10:02:06.421519 15582 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 10:02:14.402637 15582 solver.cpp:237] Iteration 256, loss = 2.20887
I0521 10:02:14.402678 15582 solver.cpp:253]     Train net output #0: loss = 2.20887 (* 1 = 2.20887 loss)
I0521 10:02:14.402695 15582 sgd_solver.cpp:106] Iteration 256, lr = 0.0025
I0521 10:02:22.376595 15582 solver.cpp:237] Iteration 272, loss = 2.19559
I0521 10:02:22.376628 15582 solver.cpp:253]     Train net output #0: loss = 2.19559 (* 1 = 2.19559 loss)
I0521 10:02:22.376644 15582 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 10:02:30.354359 15582 solver.cpp:237] Iteration 288, loss = 2.16594
I0521 10:02:30.354496 15582 solver.cpp:253]     Train net output #0: loss = 2.16594 (* 1 = 2.16594 loss)
I0521 10:02:30.354508 15582 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 10:02:38.327129 15582 solver.cpp:237] Iteration 304, loss = 2.10444
I0521 10:02:38.327172 15582 solver.cpp:253]     Train net output #0: loss = 2.10444 (* 1 = 2.10444 loss)
I0521 10:02:38.327189 15582 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0521 10:02:46.307633 15582 solver.cpp:237] Iteration 320, loss = 2.10552
I0521 10:02:46.307667 15582 solver.cpp:253]     Train net output #0: loss = 2.10552 (* 1 = 2.10552 loss)
I0521 10:02:46.307682 15582 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 10:02:46.807324 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_322.caffemodel
I0521 10:02:47.194738 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_322.solverstate
I0521 10:02:47.220489 15582 solver.cpp:341] Iteration 322, Testing net (#0)
I0521 10:03:32.246943 15582 solver.cpp:409]     Test net output #0: accuracy = 0.482455
I0521 10:03:32.247097 15582 solver.cpp:409]     Test net output #1: loss = 1.94956 (* 1 = 1.94956 loss)
I0521 10:04:01.438393 15582 solver.cpp:237] Iteration 336, loss = 2.09203
I0521 10:04:01.438444 15582 solver.cpp:253]     Train net output #0: loss = 2.09203 (* 1 = 2.09203 loss)
I0521 10:04:01.438458 15582 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 10:04:09.404008 15582 solver.cpp:237] Iteration 352, loss = 2.06957
I0521 10:04:09.404145 15582 solver.cpp:253]     Train net output #0: loss = 2.06957 (* 1 = 2.06957 loss)
I0521 10:04:09.404160 15582 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 10:04:17.366420 15582 solver.cpp:237] Iteration 368, loss = 2.07834
I0521 10:04:17.366457 15582 solver.cpp:253]     Train net output #0: loss = 2.07834 (* 1 = 2.07834 loss)
I0521 10:04:17.366478 15582 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0521 10:04:25.328665 15582 solver.cpp:237] Iteration 384, loss = 2.02697
I0521 10:04:25.328697 15582 solver.cpp:253]     Train net output #0: loss = 2.02697 (* 1 = 2.02697 loss)
I0521 10:04:25.328714 15582 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 10:04:33.292987 15582 solver.cpp:237] Iteration 400, loss = 2.01592
I0521 10:04:33.293018 15582 solver.cpp:253]     Train net output #0: loss = 2.01592 (* 1 = 2.01592 loss)
I0521 10:04:33.293035 15582 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 10:04:41.257261 15582 solver.cpp:237] Iteration 416, loss = 2.0066
I0521 10:04:41.257406 15582 solver.cpp:253]     Train net output #0: loss = 2.0066 (* 1 = 2.0066 loss)
I0521 10:04:41.257421 15582 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 10:05:11.339141 15582 solver.cpp:237] Iteration 432, loss = 2.01338
I0521 10:05:11.339299 15582 solver.cpp:253]     Train net output #0: loss = 2.01338 (* 1 = 2.01338 loss)
I0521 10:05:11.339315 15582 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 10:05:19.298991 15582 solver.cpp:237] Iteration 448, loss = 1.98743
I0521 10:05:19.299023 15582 solver.cpp:253]     Train net output #0: loss = 1.98743 (* 1 = 1.98743 loss)
I0521 10:05:19.299041 15582 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 10:05:27.262475 15582 solver.cpp:237] Iteration 464, loss = 1.97405
I0521 10:05:27.262521 15582 solver.cpp:253]     Train net output #0: loss = 1.97405 (* 1 = 1.97405 loss)
I0521 10:05:27.262537 15582 sgd_solver.cpp:106] Iteration 464, lr = 0.0025
I0521 10:05:35.228996 15582 solver.cpp:237] Iteration 480, loss = 2.02431
I0521 10:05:35.229028 15582 solver.cpp:253]     Train net output #0: loss = 2.02431 (* 1 = 2.02431 loss)
I0521 10:05:35.229044 15582 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 10:05:36.225884 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_483.caffemodel
I0521 10:05:36.613973 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_483.solverstate
I0521 10:05:43.263784 15582 solver.cpp:237] Iteration 496, loss = 1.96588
I0521 10:05:43.263960 15582 solver.cpp:253]     Train net output #0: loss = 1.96588 (* 1 = 1.96588 loss)
I0521 10:05:43.263974 15582 sgd_solver.cpp:106] Iteration 496, lr = 0.0025
I0521 10:05:51.224380 15582 solver.cpp:237] Iteration 512, loss = 1.89798
I0521 10:05:51.224412 15582 solver.cpp:253]     Train net output #0: loss = 1.89798 (* 1 = 1.89798 loss)
I0521 10:05:51.224429 15582 sgd_solver.cpp:106] Iteration 512, lr = 0.0025
I0521 10:05:59.189033 15582 solver.cpp:237] Iteration 528, loss = 1.87209
I0521 10:05:59.189076 15582 solver.cpp:253]     Train net output #0: loss = 1.87209 (* 1 = 1.87209 loss)
I0521 10:05:59.189093 15582 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 10:06:29.276710 15582 solver.cpp:237] Iteration 544, loss = 1.9204
I0521 10:06:29.276875 15582 solver.cpp:253]     Train net output #0: loss = 1.9204 (* 1 = 1.9204 loss)
I0521 10:06:29.276890 15582 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 10:06:37.246475 15582 solver.cpp:237] Iteration 560, loss = 1.87624
I0521 10:06:37.246508 15582 solver.cpp:253]     Train net output #0: loss = 1.87624 (* 1 = 1.87624 loss)
I0521 10:06:37.246526 15582 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 10:06:45.213191 15582 solver.cpp:237] Iteration 576, loss = 1.95015
I0521 10:06:45.213228 15582 solver.cpp:253]     Train net output #0: loss = 1.95015 (* 1 = 1.95015 loss)
I0521 10:06:45.213249 15582 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 10:06:53.183573 15582 solver.cpp:237] Iteration 592, loss = 1.90878
I0521 10:06:53.183606 15582 solver.cpp:253]     Train net output #0: loss = 1.90878 (* 1 = 1.90878 loss)
I0521 10:06:53.183621 15582 sgd_solver.cpp:106] Iteration 592, lr = 0.0025
I0521 10:07:01.152596 15582 solver.cpp:237] Iteration 608, loss = 1.89715
I0521 10:07:01.152734 15582 solver.cpp:253]     Train net output #0: loss = 1.89715 (* 1 = 1.89715 loss)
I0521 10:07:01.152747 15582 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 10:07:09.123404 15582 solver.cpp:237] Iteration 624, loss = 1.86271
I0521 10:07:09.123440 15582 solver.cpp:253]     Train net output #0: loss = 1.86271 (* 1 = 1.86271 loss)
I0521 10:07:09.123461 15582 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 10:07:17.094822 15582 solver.cpp:237] Iteration 640, loss = 1.86012
I0521 10:07:17.094856 15582 solver.cpp:253]     Train net output #0: loss = 1.86012 (* 1 = 1.86012 loss)
I0521 10:07:17.094869 15582 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 10:07:18.590512 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_644.caffemodel
I0521 10:07:18.978754 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_644.solverstate
I0521 10:07:19.005892 15582 solver.cpp:341] Iteration 644, Testing net (#0)
I0521 10:08:24.867007 15582 solver.cpp:409]     Test net output #0: accuracy = 0.595933
I0521 10:08:24.867174 15582 solver.cpp:409]     Test net output #1: loss = 1.43828 (* 1 = 1.43828 loss)
I0521 10:08:53.133640 15582 solver.cpp:237] Iteration 656, loss = 1.8633
I0521 10:08:53.133690 15582 solver.cpp:253]     Train net output #0: loss = 1.8633 (* 1 = 1.8633 loss)
I0521 10:08:53.133707 15582 sgd_solver.cpp:106] Iteration 656, lr = 0.0025
I0521 10:09:01.097358 15582 solver.cpp:237] Iteration 672, loss = 1.87996
I0521 10:09:01.097501 15582 solver.cpp:253]     Train net output #0: loss = 1.87996 (* 1 = 1.87996 loss)
I0521 10:09:01.097514 15582 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 10:09:09.061403 15582 solver.cpp:237] Iteration 688, loss = 1.84953
I0521 10:09:09.061434 15582 solver.cpp:253]     Train net output #0: loss = 1.84953 (* 1 = 1.84953 loss)
I0521 10:09:09.061450 15582 sgd_solver.cpp:106] Iteration 688, lr = 0.0025
I0521 10:09:17.022486 15582 solver.cpp:237] Iteration 704, loss = 1.84581
I0521 10:09:17.022526 15582 solver.cpp:253]     Train net output #0: loss = 1.84581 (* 1 = 1.84581 loss)
I0521 10:09:17.022546 15582 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 10:09:24.987992 15582 solver.cpp:237] Iteration 720, loss = 1.80935
I0521 10:09:24.988023 15582 solver.cpp:253]     Train net output #0: loss = 1.80935 (* 1 = 1.80935 loss)
I0521 10:09:24.988040 15582 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 10:09:32.952589 15582 solver.cpp:237] Iteration 736, loss = 1.81625
I0521 10:09:32.952725 15582 solver.cpp:253]     Train net output #0: loss = 1.81625 (* 1 = 1.81625 loss)
I0521 10:09:32.952739 15582 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 10:10:03.038539 15582 solver.cpp:237] Iteration 752, loss = 1.81072
I0521 10:10:03.038707 15582 solver.cpp:253]     Train net output #0: loss = 1.81072 (* 1 = 1.81072 loss)
I0521 10:10:03.038724 15582 sgd_solver.cpp:106] Iteration 752, lr = 0.0025
I0521 10:10:11.002351 15582 solver.cpp:237] Iteration 768, loss = 1.82617
I0521 10:10:11.002384 15582 solver.cpp:253]     Train net output #0: loss = 1.82617 (* 1 = 1.82617 loss)
I0521 10:10:11.002400 15582 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 10:10:18.968397 15582 solver.cpp:237] Iteration 784, loss = 1.79584
I0521 10:10:18.968430 15582 solver.cpp:253]     Train net output #0: loss = 1.79584 (* 1 = 1.79584 loss)
I0521 10:10:18.968447 15582 sgd_solver.cpp:106] Iteration 784, lr = 0.0025
I0521 10:10:26.937379 15582 solver.cpp:237] Iteration 800, loss = 1.78244
I0521 10:10:26.937423 15582 solver.cpp:253]     Train net output #0: loss = 1.78244 (* 1 = 1.78244 loss)
I0521 10:10:26.937438 15582 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 10:10:28.929513 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_805.caffemodel
I0521 10:10:29.317814 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_805.solverstate
I0521 10:10:34.971081 15582 solver.cpp:237] Iteration 816, loss = 1.78401
I0521 10:10:34.971240 15582 solver.cpp:253]     Train net output #0: loss = 1.78401 (* 1 = 1.78401 loss)
I0521 10:10:34.971253 15582 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 10:10:42.935204 15582 solver.cpp:237] Iteration 832, loss = 1.73685
I0521 10:10:42.935235 15582 solver.cpp:253]     Train net output #0: loss = 1.73685 (* 1 = 1.73685 loss)
I0521 10:10:42.935251 15582 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 10:10:50.897182 15582 solver.cpp:237] Iteration 848, loss = 1.78012
I0521 10:10:50.897215 15582 solver.cpp:253]     Train net output #0: loss = 1.78012 (* 1 = 1.78012 loss)
I0521 10:10:50.897231 15582 sgd_solver.cpp:106] Iteration 848, lr = 0.0025
I0521 10:11:20.986637 15582 solver.cpp:237] Iteration 864, loss = 1.81841
I0521 10:11:20.986815 15582 solver.cpp:253]     Train net output #0: loss = 1.81841 (* 1 = 1.81841 loss)
I0521 10:11:20.986831 15582 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 10:11:28.951197 15582 solver.cpp:237] Iteration 880, loss = 1.78237
I0521 10:11:28.951230 15582 solver.cpp:253]     Train net output #0: loss = 1.78237 (* 1 = 1.78237 loss)
I0521 10:11:28.951243 15582 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 10:11:36.919924 15582 solver.cpp:237] Iteration 896, loss = 1.78754
I0521 10:11:36.919956 15582 solver.cpp:253]     Train net output #0: loss = 1.78754 (* 1 = 1.78754 loss)
I0521 10:11:36.919973 15582 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 10:11:44.890930 15582 solver.cpp:237] Iteration 912, loss = 1.7842
I0521 10:11:44.890966 15582 solver.cpp:253]     Train net output #0: loss = 1.7842 (* 1 = 1.7842 loss)
I0521 10:11:44.890990 15582 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 10:11:52.861089 15582 solver.cpp:237] Iteration 928, loss = 1.74403
I0521 10:11:52.861225 15582 solver.cpp:253]     Train net output #0: loss = 1.74403 (* 1 = 1.74403 loss)
I0521 10:11:52.861238 15582 sgd_solver.cpp:106] Iteration 928, lr = 0.0025
I0521 10:12:00.828550 15582 solver.cpp:237] Iteration 944, loss = 1.79672
I0521 10:12:00.828583 15582 solver.cpp:253]     Train net output #0: loss = 1.79672 (* 1 = 1.79672 loss)
I0521 10:12:00.828598 15582 sgd_solver.cpp:106] Iteration 944, lr = 0.0025
I0521 10:12:08.794487 15582 solver.cpp:237] Iteration 960, loss = 1.80771
I0521 10:12:08.794526 15582 solver.cpp:253]     Train net output #0: loss = 1.80771 (* 1 = 1.80771 loss)
I0521 10:12:08.794548 15582 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 10:12:11.286768 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_966.caffemodel
I0521 10:12:11.673624 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_966.solverstate
I0521 10:12:11.699143 15582 solver.cpp:341] Iteration 966, Testing net (#0)
I0521 10:12:56.378222 15582 solver.cpp:409]     Test net output #0: accuracy = 0.631778
I0521 10:12:56.378383 15582 solver.cpp:409]     Test net output #1: loss = 1.34516 (* 1 = 1.34516 loss)
I0521 10:13:23.641911 15582 solver.cpp:237] Iteration 976, loss = 1.76867
I0521 10:13:23.641960 15582 solver.cpp:253]     Train net output #0: loss = 1.76867 (* 1 = 1.76867 loss)
I0521 10:13:23.641976 15582 sgd_solver.cpp:106] Iteration 976, lr = 0.0025
I0521 10:13:31.609127 15582 solver.cpp:237] Iteration 992, loss = 1.74717
I0521 10:13:31.609277 15582 solver.cpp:253]     Train net output #0: loss = 1.74717 (* 1 = 1.74717 loss)
I0521 10:13:31.609290 15582 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0521 10:13:39.579206 15582 solver.cpp:237] Iteration 1008, loss = 1.80394
I0521 10:13:39.579238 15582 solver.cpp:253]     Train net output #0: loss = 1.80394 (* 1 = 1.80394 loss)
I0521 10:13:39.579257 15582 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 10:13:47.549365 15582 solver.cpp:237] Iteration 1024, loss = 1.71057
I0521 10:13:47.549409 15582 solver.cpp:253]     Train net output #0: loss = 1.71057 (* 1 = 1.71057 loss)
I0521 10:13:47.549427 15582 sgd_solver.cpp:106] Iteration 1024, lr = 0.0025
I0521 10:13:55.516268 15582 solver.cpp:237] Iteration 1040, loss = 1.71321
I0521 10:13:55.516300 15582 solver.cpp:253]     Train net output #0: loss = 1.71321 (* 1 = 1.71321 loss)
I0521 10:13:55.516315 15582 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 10:14:03.488523 15582 solver.cpp:237] Iteration 1056, loss = 1.7346
I0521 10:14:03.488664 15582 solver.cpp:253]     Train net output #0: loss = 1.7346 (* 1 = 1.7346 loss)
I0521 10:14:03.488678 15582 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 10:14:11.462800 15582 solver.cpp:237] Iteration 1072, loss = 1.75373
I0521 10:14:11.462832 15582 solver.cpp:253]     Train net output #0: loss = 1.75373 (* 1 = 1.75373 loss)
I0521 10:14:11.462849 15582 sgd_solver.cpp:106] Iteration 1072, lr = 0.0025
I0521 10:14:41.600857 15582 solver.cpp:237] Iteration 1088, loss = 1.75807
I0521 10:14:41.601028 15582 solver.cpp:253]     Train net output #0: loss = 1.75807 (* 1 = 1.75807 loss)
I0521 10:14:41.601043 15582 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 10:14:49.568019 15582 solver.cpp:237] Iteration 1104, loss = 1.82064
I0521 10:14:49.568053 15582 solver.cpp:253]     Train net output #0: loss = 1.82064 (* 1 = 1.82064 loss)
I0521 10:14:49.568070 15582 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 10:14:57.535923 15582 solver.cpp:237] Iteration 1120, loss = 1.75495
I0521 10:14:57.535954 15582 solver.cpp:253]     Train net output #0: loss = 1.75495 (* 1 = 1.75495 loss)
I0521 10:14:57.535969 15582 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 10:15:00.523272 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1127.caffemodel
I0521 10:15:00.911974 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1127.solverstate
I0521 10:15:05.572710 15582 solver.cpp:237] Iteration 1136, loss = 1.7398
I0521 10:15:05.572753 15582 solver.cpp:253]     Train net output #0: loss = 1.7398 (* 1 = 1.7398 loss)
I0521 10:15:05.572767 15582 sgd_solver.cpp:106] Iteration 1136, lr = 0.0025
I0521 10:15:13.540032 15582 solver.cpp:237] Iteration 1152, loss = 1.66212
I0521 10:15:13.540184 15582 solver.cpp:253]     Train net output #0: loss = 1.66212 (* 1 = 1.66212 loss)
I0521 10:15:13.540199 15582 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 10:15:21.507073 15582 solver.cpp:237] Iteration 1168, loss = 1.81673
I0521 10:15:21.507104 15582 solver.cpp:253]     Train net output #0: loss = 1.81673 (* 1 = 1.81673 loss)
I0521 10:15:21.507122 15582 sgd_solver.cpp:106] Iteration 1168, lr = 0.0025
I0521 10:15:51.653388 15582 solver.cpp:237] Iteration 1184, loss = 1.73906
I0521 10:15:51.653545 15582 solver.cpp:253]     Train net output #0: loss = 1.73906 (* 1 = 1.73906 loss)
I0521 10:15:51.653563 15582 sgd_solver.cpp:106] Iteration 1184, lr = 0.0025
I0521 10:15:59.617952 15582 solver.cpp:237] Iteration 1200, loss = 1.73322
I0521 10:15:59.617985 15582 solver.cpp:253]     Train net output #0: loss = 1.73322 (* 1 = 1.73322 loss)
I0521 10:15:59.618003 15582 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 10:16:07.584846 15582 solver.cpp:237] Iteration 1216, loss = 1.70745
I0521 10:16:07.584880 15582 solver.cpp:253]     Train net output #0: loss = 1.70745 (* 1 = 1.70745 loss)
I0521 10:16:07.584897 15582 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 10:16:15.554702 15582 solver.cpp:237] Iteration 1232, loss = 1.67693
I0521 10:16:15.554734 15582 solver.cpp:253]     Train net output #0: loss = 1.67693 (* 1 = 1.67693 loss)
I0521 10:16:15.554751 15582 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 10:16:23.522305 15582 solver.cpp:237] Iteration 1248, loss = 1.70615
I0521 10:16:23.522454 15582 solver.cpp:253]     Train net output #0: loss = 1.70615 (* 1 = 1.70615 loss)
I0521 10:16:23.522469 15582 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 10:16:31.491199 15582 solver.cpp:237] Iteration 1264, loss = 1.66556
I0521 10:16:31.491230 15582 solver.cpp:253]     Train net output #0: loss = 1.66556 (* 1 = 1.66556 loss)
I0521 10:16:31.491248 15582 sgd_solver.cpp:106] Iteration 1264, lr = 0.0025
I0521 10:16:39.456948 15582 solver.cpp:237] Iteration 1280, loss = 1.77041
I0521 10:16:39.456980 15582 solver.cpp:253]     Train net output #0: loss = 1.77041 (* 1 = 1.77041 loss)
I0521 10:16:39.456996 15582 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 10:16:42.943469 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1288.caffemodel
I0521 10:16:43.330466 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1288.solverstate
I0521 10:16:43.356463 15582 solver.cpp:341] Iteration 1288, Testing net (#0)
I0521 10:17:49.286799 15582 solver.cpp:409]     Test net output #0: accuracy = 0.646731
I0521 10:17:49.286973 15582 solver.cpp:409]     Test net output #1: loss = 1.19831 (* 1 = 1.19831 loss)
I0521 10:18:15.601819 15582 solver.cpp:237] Iteration 1296, loss = 1.66881
I0521 10:18:15.601868 15582 solver.cpp:253]     Train net output #0: loss = 1.66881 (* 1 = 1.66881 loss)
I0521 10:18:15.601884 15582 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 10:18:23.574275 15582 solver.cpp:237] Iteration 1312, loss = 1.7858
I0521 10:18:23.574436 15582 solver.cpp:253]     Train net output #0: loss = 1.7858 (* 1 = 1.7858 loss)
I0521 10:18:23.574451 15582 sgd_solver.cpp:106] Iteration 1312, lr = 0.0025
I0521 10:18:31.543378 15582 solver.cpp:237] Iteration 1328, loss = 1.71346
I0521 10:18:31.543409 15582 solver.cpp:253]     Train net output #0: loss = 1.71346 (* 1 = 1.71346 loss)
I0521 10:18:31.543427 15582 sgd_solver.cpp:106] Iteration 1328, lr = 0.0025
I0521 10:18:39.521036 15582 solver.cpp:237] Iteration 1344, loss = 1.73843
I0521 10:18:39.521070 15582 solver.cpp:253]     Train net output #0: loss = 1.73843 (* 1 = 1.73843 loss)
I0521 10:18:39.521082 15582 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 10:18:47.490361 15582 solver.cpp:237] Iteration 1360, loss = 1.77703
I0521 10:18:47.490406 15582 solver.cpp:253]     Train net output #0: loss = 1.77703 (* 1 = 1.77703 loss)
I0521 10:18:47.490422 15582 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 10:18:55.462841 15582 solver.cpp:237] Iteration 1376, loss = 1.69675
I0521 10:18:55.462981 15582 solver.cpp:253]     Train net output #0: loss = 1.69675 (* 1 = 1.69675 loss)
I0521 10:18:55.462996 15582 sgd_solver.cpp:106] Iteration 1376, lr = 0.0025
I0521 10:19:03.438383 15582 solver.cpp:237] Iteration 1392, loss = 1.75631
I0521 10:19:03.438415 15582 solver.cpp:253]     Train net output #0: loss = 1.75631 (* 1 = 1.75631 loss)
I0521 10:19:03.438427 15582 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 10:19:33.601773 15582 solver.cpp:237] Iteration 1408, loss = 1.69913
I0521 10:19:33.601951 15582 solver.cpp:253]     Train net output #0: loss = 1.69913 (* 1 = 1.69913 loss)
I0521 10:19:33.601966 15582 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 10:19:41.575675 15582 solver.cpp:237] Iteration 1424, loss = 1.66489
I0521 10:19:41.575717 15582 solver.cpp:253]     Train net output #0: loss = 1.66489 (* 1 = 1.66489 loss)
I0521 10:19:41.575732 15582 sgd_solver.cpp:106] Iteration 1424, lr = 0.0025
I0521 10:19:49.548028 15582 solver.cpp:237] Iteration 1440, loss = 1.69308
I0521 10:19:49.548061 15582 solver.cpp:253]     Train net output #0: loss = 1.69308 (* 1 = 1.69308 loss)
I0521 10:19:49.548079 15582 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 10:19:53.534245 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1449.caffemodel
I0521 10:19:53.926654 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1449.solverstate
I0521 10:19:57.594350 15582 solver.cpp:237] Iteration 1456, loss = 1.72539
I0521 10:19:57.594399 15582 solver.cpp:253]     Train net output #0: loss = 1.72539 (* 1 = 1.72539 loss)
I0521 10:19:57.594414 15582 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 10:20:05.568763 15582 solver.cpp:237] Iteration 1472, loss = 1.69894
I0521 10:20:05.568931 15582 solver.cpp:253]     Train net output #0: loss = 1.69894 (* 1 = 1.69894 loss)
I0521 10:20:05.568946 15582 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 10:20:13.541823 15582 solver.cpp:237] Iteration 1488, loss = 1.76012
I0521 10:20:13.541856 15582 solver.cpp:253]     Train net output #0: loss = 1.76012 (* 1 = 1.76012 loss)
I0521 10:20:13.541873 15582 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 10:20:21.516307 15582 solver.cpp:237] Iteration 1504, loss = 1.73857
I0521 10:20:21.516340 15582 solver.cpp:253]     Train net output #0: loss = 1.73857 (* 1 = 1.73857 loss)
I0521 10:20:21.516356 15582 sgd_solver.cpp:106] Iteration 1504, lr = 0.0025
I0521 10:20:51.685312 15582 solver.cpp:237] Iteration 1520, loss = 1.66629
I0521 10:20:51.685472 15582 solver.cpp:253]     Train net output #0: loss = 1.66629 (* 1 = 1.66629 loss)
I0521 10:20:51.685487 15582 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 10:20:59.661387 15582 solver.cpp:237] Iteration 1536, loss = 1.70332
I0521 10:20:59.661419 15582 solver.cpp:253]     Train net output #0: loss = 1.70332 (* 1 = 1.70332 loss)
I0521 10:20:59.661439 15582 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 10:21:07.634953 15582 solver.cpp:237] Iteration 1552, loss = 1.66111
I0521 10:21:07.634986 15582 solver.cpp:253]     Train net output #0: loss = 1.66111 (* 1 = 1.66111 loss)
I0521 10:21:07.635004 15582 sgd_solver.cpp:106] Iteration 1552, lr = 0.0025
I0521 10:21:15.605461 15582 solver.cpp:237] Iteration 1568, loss = 1.66375
I0521 10:21:15.605494 15582 solver.cpp:253]     Train net output #0: loss = 1.66375 (* 1 = 1.66375 loss)
I0521 10:21:15.605510 15582 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 10:21:23.578378 15582 solver.cpp:237] Iteration 1584, loss = 1.66925
I0521 10:21:23.578532 15582 solver.cpp:253]     Train net output #0: loss = 1.66925 (* 1 = 1.66925 loss)
I0521 10:21:23.578547 15582 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 10:21:31.555646 15582 solver.cpp:237] Iteration 1600, loss = 1.67093
I0521 10:21:31.555678 15582 solver.cpp:253]     Train net output #0: loss = 1.67093 (* 1 = 1.67093 loss)
I0521 10:21:31.555696 15582 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 10:21:36.039319 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1610.caffemodel
I0521 10:21:36.429800 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1610.solverstate
I0521 10:21:36.457660 15582 solver.cpp:341] Iteration 1610, Testing net (#0)
I0521 10:22:21.495868 15582 solver.cpp:409]     Test net output #0: accuracy = 0.641855
I0521 10:22:21.496031 15582 solver.cpp:409]     Test net output #1: loss = 1.22073 (* 1 = 1.22073 loss)
I0521 10:22:22.141497 15582 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1612.caffemodel
I0521 10:22:22.531354 15582 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537_iter_1612.solverstate
I0521 10:22:22.559257 15582 solver.cpp:326] Optimization Done.
I0521 10:22:22.559284 15582 caffe.cpp:215] Optimization Done.
Application 11237576 resources: utime ~1247s, stime ~225s, Rss ~5328980, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_930_2016-05-20T11.21.06.614537.solver"
	User time (seconds): 0.56
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:35.78
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2708
	Involuntary context switches: 75
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
