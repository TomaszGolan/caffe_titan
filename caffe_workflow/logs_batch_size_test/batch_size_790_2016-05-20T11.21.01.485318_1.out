2806336
I0521 07:02:38.506168  2580 caffe.cpp:184] Using GPUs 0
I0521 07:02:38.931129  2580 solver.cpp:48] Initializing solver from parameters: 
test_iter: 189
test_interval: 379
base_lr: 0.0025
display: 18
max_iter: 1898
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 189
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318.prototxt"
I0521 07:02:38.932680  2580 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318.prototxt
I0521 07:02:38.959702  2580 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 07:02:38.959763  2580 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 07:02:38.960108  2580 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 790
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:02:38.960289  2580 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:02:38.960312  2580 net.cpp:106] Creating Layer data_hdf5
I0521 07:02:38.960327  2580 net.cpp:411] data_hdf5 -> data
I0521 07:02:38.960362  2580 net.cpp:411] data_hdf5 -> label
I0521 07:02:38.960394  2580 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 07:02:38.961556  2580 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 07:02:38.963738  2580 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 07:03:00.493396  2580 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 07:03:00.498528  2580 net.cpp:150] Setting up data_hdf5
I0521 07:03:00.498569  2580 net.cpp:157] Top shape: 790 1 127 50 (5016500)
I0521 07:03:00.498584  2580 net.cpp:157] Top shape: 790 (790)
I0521 07:03:00.498597  2580 net.cpp:165] Memory required for data: 20069160
I0521 07:03:00.498610  2580 layer_factory.hpp:77] Creating layer conv1
I0521 07:03:00.498643  2580 net.cpp:106] Creating Layer conv1
I0521 07:03:00.498654  2580 net.cpp:454] conv1 <- data
I0521 07:03:00.498674  2580 net.cpp:411] conv1 -> conv1
I0521 07:03:00.860226  2580 net.cpp:150] Setting up conv1
I0521 07:03:00.860268  2580 net.cpp:157] Top shape: 790 12 120 48 (54604800)
I0521 07:03:00.860280  2580 net.cpp:165] Memory required for data: 238488360
I0521 07:03:00.860308  2580 layer_factory.hpp:77] Creating layer relu1
I0521 07:03:00.860330  2580 net.cpp:106] Creating Layer relu1
I0521 07:03:00.860342  2580 net.cpp:454] relu1 <- conv1
I0521 07:03:00.860355  2580 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:03:00.860868  2580 net.cpp:150] Setting up relu1
I0521 07:03:00.860884  2580 net.cpp:157] Top shape: 790 12 120 48 (54604800)
I0521 07:03:00.860895  2580 net.cpp:165] Memory required for data: 456907560
I0521 07:03:00.860906  2580 layer_factory.hpp:77] Creating layer pool1
I0521 07:03:00.860924  2580 net.cpp:106] Creating Layer pool1
I0521 07:03:00.860934  2580 net.cpp:454] pool1 <- conv1
I0521 07:03:00.860947  2580 net.cpp:411] pool1 -> pool1
I0521 07:03:00.861027  2580 net.cpp:150] Setting up pool1
I0521 07:03:00.861042  2580 net.cpp:157] Top shape: 790 12 60 48 (27302400)
I0521 07:03:00.861052  2580 net.cpp:165] Memory required for data: 566117160
I0521 07:03:00.861059  2580 layer_factory.hpp:77] Creating layer conv2
I0521 07:03:00.861083  2580 net.cpp:106] Creating Layer conv2
I0521 07:03:00.861093  2580 net.cpp:454] conv2 <- pool1
I0521 07:03:00.861105  2580 net.cpp:411] conv2 -> conv2
I0521 07:03:00.863801  2580 net.cpp:150] Setting up conv2
I0521 07:03:00.863828  2580 net.cpp:157] Top shape: 790 20 54 46 (39247200)
I0521 07:03:00.863838  2580 net.cpp:165] Memory required for data: 723105960
I0521 07:03:00.863858  2580 layer_factory.hpp:77] Creating layer relu2
I0521 07:03:00.863873  2580 net.cpp:106] Creating Layer relu2
I0521 07:03:00.863883  2580 net.cpp:454] relu2 <- conv2
I0521 07:03:00.863895  2580 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:03:00.864224  2580 net.cpp:150] Setting up relu2
I0521 07:03:00.864238  2580 net.cpp:157] Top shape: 790 20 54 46 (39247200)
I0521 07:03:00.864249  2580 net.cpp:165] Memory required for data: 880094760
I0521 07:03:00.864259  2580 layer_factory.hpp:77] Creating layer pool2
I0521 07:03:00.864272  2580 net.cpp:106] Creating Layer pool2
I0521 07:03:00.864282  2580 net.cpp:454] pool2 <- conv2
I0521 07:03:00.864307  2580 net.cpp:411] pool2 -> pool2
I0521 07:03:00.864377  2580 net.cpp:150] Setting up pool2
I0521 07:03:00.864389  2580 net.cpp:157] Top shape: 790 20 27 46 (19623600)
I0521 07:03:00.864399  2580 net.cpp:165] Memory required for data: 958589160
I0521 07:03:00.864409  2580 layer_factory.hpp:77] Creating layer conv3
I0521 07:03:00.864424  2580 net.cpp:106] Creating Layer conv3
I0521 07:03:00.864435  2580 net.cpp:454] conv3 <- pool2
I0521 07:03:00.864449  2580 net.cpp:411] conv3 -> conv3
I0521 07:03:00.866375  2580 net.cpp:150] Setting up conv3
I0521 07:03:00.866399  2580 net.cpp:157] Top shape: 790 28 22 44 (21412160)
I0521 07:03:00.866411  2580 net.cpp:165] Memory required for data: 1044237800
I0521 07:03:00.866430  2580 layer_factory.hpp:77] Creating layer relu3
I0521 07:03:00.866446  2580 net.cpp:106] Creating Layer relu3
I0521 07:03:00.866456  2580 net.cpp:454] relu3 <- conv3
I0521 07:03:00.866468  2580 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:03:00.866941  2580 net.cpp:150] Setting up relu3
I0521 07:03:00.866958  2580 net.cpp:157] Top shape: 790 28 22 44 (21412160)
I0521 07:03:00.866969  2580 net.cpp:165] Memory required for data: 1129886440
I0521 07:03:00.866979  2580 layer_factory.hpp:77] Creating layer pool3
I0521 07:03:00.866992  2580 net.cpp:106] Creating Layer pool3
I0521 07:03:00.867002  2580 net.cpp:454] pool3 <- conv3
I0521 07:03:00.867015  2580 net.cpp:411] pool3 -> pool3
I0521 07:03:00.867082  2580 net.cpp:150] Setting up pool3
I0521 07:03:00.867095  2580 net.cpp:157] Top shape: 790 28 11 44 (10706080)
I0521 07:03:00.867105  2580 net.cpp:165] Memory required for data: 1172710760
I0521 07:03:00.867115  2580 layer_factory.hpp:77] Creating layer conv4
I0521 07:03:00.867132  2580 net.cpp:106] Creating Layer conv4
I0521 07:03:00.867143  2580 net.cpp:454] conv4 <- pool3
I0521 07:03:00.867157  2580 net.cpp:411] conv4 -> conv4
I0521 07:03:00.869869  2580 net.cpp:150] Setting up conv4
I0521 07:03:00.869897  2580 net.cpp:157] Top shape: 790 36 6 42 (7166880)
I0521 07:03:00.869910  2580 net.cpp:165] Memory required for data: 1201378280
I0521 07:03:00.869928  2580 layer_factory.hpp:77] Creating layer relu4
I0521 07:03:00.869942  2580 net.cpp:106] Creating Layer relu4
I0521 07:03:00.869952  2580 net.cpp:454] relu4 <- conv4
I0521 07:03:00.869966  2580 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:03:00.870439  2580 net.cpp:150] Setting up relu4
I0521 07:03:00.870455  2580 net.cpp:157] Top shape: 790 36 6 42 (7166880)
I0521 07:03:00.870465  2580 net.cpp:165] Memory required for data: 1230045800
I0521 07:03:00.870476  2580 layer_factory.hpp:77] Creating layer pool4
I0521 07:03:00.870488  2580 net.cpp:106] Creating Layer pool4
I0521 07:03:00.870498  2580 net.cpp:454] pool4 <- conv4
I0521 07:03:00.870512  2580 net.cpp:411] pool4 -> pool4
I0521 07:03:00.870579  2580 net.cpp:150] Setting up pool4
I0521 07:03:00.870594  2580 net.cpp:157] Top shape: 790 36 3 42 (3583440)
I0521 07:03:00.870604  2580 net.cpp:165] Memory required for data: 1244379560
I0521 07:03:00.870615  2580 layer_factory.hpp:77] Creating layer ip1
I0521 07:03:00.870635  2580 net.cpp:106] Creating Layer ip1
I0521 07:03:00.870645  2580 net.cpp:454] ip1 <- pool4
I0521 07:03:00.870657  2580 net.cpp:411] ip1 -> ip1
I0521 07:03:00.886168  2580 net.cpp:150] Setting up ip1
I0521 07:03:00.886198  2580 net.cpp:157] Top shape: 790 196 (154840)
I0521 07:03:00.886212  2580 net.cpp:165] Memory required for data: 1244998920
I0521 07:03:00.886239  2580 layer_factory.hpp:77] Creating layer relu5
I0521 07:03:00.886253  2580 net.cpp:106] Creating Layer relu5
I0521 07:03:00.886271  2580 net.cpp:454] relu5 <- ip1
I0521 07:03:00.886284  2580 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:03:00.886628  2580 net.cpp:150] Setting up relu5
I0521 07:03:00.886643  2580 net.cpp:157] Top shape: 790 196 (154840)
I0521 07:03:00.886653  2580 net.cpp:165] Memory required for data: 1245618280
I0521 07:03:00.886663  2580 layer_factory.hpp:77] Creating layer drop1
I0521 07:03:00.886685  2580 net.cpp:106] Creating Layer drop1
I0521 07:03:00.886696  2580 net.cpp:454] drop1 <- ip1
I0521 07:03:00.886723  2580 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:03:00.886770  2580 net.cpp:150] Setting up drop1
I0521 07:03:00.886782  2580 net.cpp:157] Top shape: 790 196 (154840)
I0521 07:03:00.886792  2580 net.cpp:165] Memory required for data: 1246237640
I0521 07:03:00.886802  2580 layer_factory.hpp:77] Creating layer ip2
I0521 07:03:00.886821  2580 net.cpp:106] Creating Layer ip2
I0521 07:03:00.886832  2580 net.cpp:454] ip2 <- ip1
I0521 07:03:00.886845  2580 net.cpp:411] ip2 -> ip2
I0521 07:03:00.887310  2580 net.cpp:150] Setting up ip2
I0521 07:03:00.887325  2580 net.cpp:157] Top shape: 790 98 (77420)
I0521 07:03:00.887333  2580 net.cpp:165] Memory required for data: 1246547320
I0521 07:03:00.887348  2580 layer_factory.hpp:77] Creating layer relu6
I0521 07:03:00.887362  2580 net.cpp:106] Creating Layer relu6
I0521 07:03:00.887372  2580 net.cpp:454] relu6 <- ip2
I0521 07:03:00.887383  2580 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:03:00.887902  2580 net.cpp:150] Setting up relu6
I0521 07:03:00.887919  2580 net.cpp:157] Top shape: 790 98 (77420)
I0521 07:03:00.887929  2580 net.cpp:165] Memory required for data: 1246857000
I0521 07:03:00.887939  2580 layer_factory.hpp:77] Creating layer drop2
I0521 07:03:00.887953  2580 net.cpp:106] Creating Layer drop2
I0521 07:03:00.887962  2580 net.cpp:454] drop2 <- ip2
I0521 07:03:00.887975  2580 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:03:00.888017  2580 net.cpp:150] Setting up drop2
I0521 07:03:00.888031  2580 net.cpp:157] Top shape: 790 98 (77420)
I0521 07:03:00.888041  2580 net.cpp:165] Memory required for data: 1247166680
I0521 07:03:00.888051  2580 layer_factory.hpp:77] Creating layer ip3
I0521 07:03:00.888064  2580 net.cpp:106] Creating Layer ip3
I0521 07:03:00.888074  2580 net.cpp:454] ip3 <- ip2
I0521 07:03:00.888087  2580 net.cpp:411] ip3 -> ip3
I0521 07:03:00.888298  2580 net.cpp:150] Setting up ip3
I0521 07:03:00.888311  2580 net.cpp:157] Top shape: 790 11 (8690)
I0521 07:03:00.888321  2580 net.cpp:165] Memory required for data: 1247201440
I0521 07:03:00.888335  2580 layer_factory.hpp:77] Creating layer drop3
I0521 07:03:00.888348  2580 net.cpp:106] Creating Layer drop3
I0521 07:03:00.888358  2580 net.cpp:454] drop3 <- ip3
I0521 07:03:00.888370  2580 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:03:00.888408  2580 net.cpp:150] Setting up drop3
I0521 07:03:00.888422  2580 net.cpp:157] Top shape: 790 11 (8690)
I0521 07:03:00.888432  2580 net.cpp:165] Memory required for data: 1247236200
I0521 07:03:00.888442  2580 layer_factory.hpp:77] Creating layer loss
I0521 07:03:00.888460  2580 net.cpp:106] Creating Layer loss
I0521 07:03:00.888470  2580 net.cpp:454] loss <- ip3
I0521 07:03:00.888481  2580 net.cpp:454] loss <- label
I0521 07:03:00.888494  2580 net.cpp:411] loss -> loss
I0521 07:03:00.888511  2580 layer_factory.hpp:77] Creating layer loss
I0521 07:03:00.889163  2580 net.cpp:150] Setting up loss
I0521 07:03:00.889183  2580 net.cpp:157] Top shape: (1)
I0521 07:03:00.889196  2580 net.cpp:160]     with loss weight 1
I0521 07:03:00.889238  2580 net.cpp:165] Memory required for data: 1247236204
I0521 07:03:00.889250  2580 net.cpp:226] loss needs backward computation.
I0521 07:03:00.889261  2580 net.cpp:226] drop3 needs backward computation.
I0521 07:03:00.889268  2580 net.cpp:226] ip3 needs backward computation.
I0521 07:03:00.889279  2580 net.cpp:226] drop2 needs backward computation.
I0521 07:03:00.889289  2580 net.cpp:226] relu6 needs backward computation.
I0521 07:03:00.889299  2580 net.cpp:226] ip2 needs backward computation.
I0521 07:03:00.889309  2580 net.cpp:226] drop1 needs backward computation.
I0521 07:03:00.889319  2580 net.cpp:226] relu5 needs backward computation.
I0521 07:03:00.889328  2580 net.cpp:226] ip1 needs backward computation.
I0521 07:03:00.889339  2580 net.cpp:226] pool4 needs backward computation.
I0521 07:03:00.889349  2580 net.cpp:226] relu4 needs backward computation.
I0521 07:03:00.889359  2580 net.cpp:226] conv4 needs backward computation.
I0521 07:03:00.889370  2580 net.cpp:226] pool3 needs backward computation.
I0521 07:03:00.889389  2580 net.cpp:226] relu3 needs backward computation.
I0521 07:03:00.889397  2580 net.cpp:226] conv3 needs backward computation.
I0521 07:03:00.889408  2580 net.cpp:226] pool2 needs backward computation.
I0521 07:03:00.889418  2580 net.cpp:226] relu2 needs backward computation.
I0521 07:03:00.889428  2580 net.cpp:226] conv2 needs backward computation.
I0521 07:03:00.889439  2580 net.cpp:226] pool1 needs backward computation.
I0521 07:03:00.889451  2580 net.cpp:226] relu1 needs backward computation.
I0521 07:03:00.889461  2580 net.cpp:226] conv1 needs backward computation.
I0521 07:03:00.889472  2580 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:03:00.889482  2580 net.cpp:270] This network produces output loss
I0521 07:03:00.889504  2580 net.cpp:283] Network initialization done.
I0521 07:03:00.891120  2580 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318.prototxt
I0521 07:03:00.891191  2580 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 07:03:00.891546  2580 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 790
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:03:00.891736  2580 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:03:00.891752  2580 net.cpp:106] Creating Layer data_hdf5
I0521 07:03:00.891765  2580 net.cpp:411] data_hdf5 -> data
I0521 07:03:00.891782  2580 net.cpp:411] data_hdf5 -> label
I0521 07:03:00.891796  2580 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 07:03:00.892961  2580 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 07:03:22.189015  2580 net.cpp:150] Setting up data_hdf5
I0521 07:03:22.189182  2580 net.cpp:157] Top shape: 790 1 127 50 (5016500)
I0521 07:03:22.189196  2580 net.cpp:157] Top shape: 790 (790)
I0521 07:03:22.189209  2580 net.cpp:165] Memory required for data: 20069160
I0521 07:03:22.189224  2580 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 07:03:22.189251  2580 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 07:03:22.189262  2580 net.cpp:454] label_data_hdf5_1_split <- label
I0521 07:03:22.189277  2580 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 07:03:22.189298  2580 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 07:03:22.189373  2580 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 07:03:22.189386  2580 net.cpp:157] Top shape: 790 (790)
I0521 07:03:22.189399  2580 net.cpp:157] Top shape: 790 (790)
I0521 07:03:22.189409  2580 net.cpp:165] Memory required for data: 20075480
I0521 07:03:22.189419  2580 layer_factory.hpp:77] Creating layer conv1
I0521 07:03:22.189440  2580 net.cpp:106] Creating Layer conv1
I0521 07:03:22.189451  2580 net.cpp:454] conv1 <- data
I0521 07:03:22.189466  2580 net.cpp:411] conv1 -> conv1
I0521 07:03:22.191418  2580 net.cpp:150] Setting up conv1
I0521 07:03:22.191442  2580 net.cpp:157] Top shape: 790 12 120 48 (54604800)
I0521 07:03:22.191453  2580 net.cpp:165] Memory required for data: 238494680
I0521 07:03:22.191474  2580 layer_factory.hpp:77] Creating layer relu1
I0521 07:03:22.191489  2580 net.cpp:106] Creating Layer relu1
I0521 07:03:22.191499  2580 net.cpp:454] relu1 <- conv1
I0521 07:03:22.191512  2580 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:03:22.192008  2580 net.cpp:150] Setting up relu1
I0521 07:03:22.192023  2580 net.cpp:157] Top shape: 790 12 120 48 (54604800)
I0521 07:03:22.192034  2580 net.cpp:165] Memory required for data: 456913880
I0521 07:03:22.192044  2580 layer_factory.hpp:77] Creating layer pool1
I0521 07:03:22.192060  2580 net.cpp:106] Creating Layer pool1
I0521 07:03:22.192070  2580 net.cpp:454] pool1 <- conv1
I0521 07:03:22.192083  2580 net.cpp:411] pool1 -> pool1
I0521 07:03:22.192158  2580 net.cpp:150] Setting up pool1
I0521 07:03:22.192173  2580 net.cpp:157] Top shape: 790 12 60 48 (27302400)
I0521 07:03:22.192181  2580 net.cpp:165] Memory required for data: 566123480
I0521 07:03:22.192193  2580 layer_factory.hpp:77] Creating layer conv2
I0521 07:03:22.192209  2580 net.cpp:106] Creating Layer conv2
I0521 07:03:22.192220  2580 net.cpp:454] conv2 <- pool1
I0521 07:03:22.192234  2580 net.cpp:411] conv2 -> conv2
I0521 07:03:22.194145  2580 net.cpp:150] Setting up conv2
I0521 07:03:22.194167  2580 net.cpp:157] Top shape: 790 20 54 46 (39247200)
I0521 07:03:22.194180  2580 net.cpp:165] Memory required for data: 723112280
I0521 07:03:22.194198  2580 layer_factory.hpp:77] Creating layer relu2
I0521 07:03:22.194211  2580 net.cpp:106] Creating Layer relu2
I0521 07:03:22.194221  2580 net.cpp:454] relu2 <- conv2
I0521 07:03:22.194233  2580 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:03:22.194578  2580 net.cpp:150] Setting up relu2
I0521 07:03:22.194593  2580 net.cpp:157] Top shape: 790 20 54 46 (39247200)
I0521 07:03:22.194603  2580 net.cpp:165] Memory required for data: 880101080
I0521 07:03:22.194613  2580 layer_factory.hpp:77] Creating layer pool2
I0521 07:03:22.194628  2580 net.cpp:106] Creating Layer pool2
I0521 07:03:22.194636  2580 net.cpp:454] pool2 <- conv2
I0521 07:03:22.194649  2580 net.cpp:411] pool2 -> pool2
I0521 07:03:22.194721  2580 net.cpp:150] Setting up pool2
I0521 07:03:22.194735  2580 net.cpp:157] Top shape: 790 20 27 46 (19623600)
I0521 07:03:22.194744  2580 net.cpp:165] Memory required for data: 958595480
I0521 07:03:22.194754  2580 layer_factory.hpp:77] Creating layer conv3
I0521 07:03:22.194772  2580 net.cpp:106] Creating Layer conv3
I0521 07:03:22.194782  2580 net.cpp:454] conv3 <- pool2
I0521 07:03:22.194797  2580 net.cpp:411] conv3 -> conv3
I0521 07:03:22.196768  2580 net.cpp:150] Setting up conv3
I0521 07:03:22.196791  2580 net.cpp:157] Top shape: 790 28 22 44 (21412160)
I0521 07:03:22.196802  2580 net.cpp:165] Memory required for data: 1044244120
I0521 07:03:22.196835  2580 layer_factory.hpp:77] Creating layer relu3
I0521 07:03:22.196848  2580 net.cpp:106] Creating Layer relu3
I0521 07:03:22.196859  2580 net.cpp:454] relu3 <- conv3
I0521 07:03:22.196872  2580 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:03:22.197343  2580 net.cpp:150] Setting up relu3
I0521 07:03:22.197360  2580 net.cpp:157] Top shape: 790 28 22 44 (21412160)
I0521 07:03:22.197370  2580 net.cpp:165] Memory required for data: 1129892760
I0521 07:03:22.197379  2580 layer_factory.hpp:77] Creating layer pool3
I0521 07:03:22.197392  2580 net.cpp:106] Creating Layer pool3
I0521 07:03:22.197402  2580 net.cpp:454] pool3 <- conv3
I0521 07:03:22.197417  2580 net.cpp:411] pool3 -> pool3
I0521 07:03:22.197489  2580 net.cpp:150] Setting up pool3
I0521 07:03:22.197502  2580 net.cpp:157] Top shape: 790 28 11 44 (10706080)
I0521 07:03:22.197511  2580 net.cpp:165] Memory required for data: 1172717080
I0521 07:03:22.197521  2580 layer_factory.hpp:77] Creating layer conv4
I0521 07:03:22.197540  2580 net.cpp:106] Creating Layer conv4
I0521 07:03:22.197551  2580 net.cpp:454] conv4 <- pool3
I0521 07:03:22.197564  2580 net.cpp:411] conv4 -> conv4
I0521 07:03:22.199631  2580 net.cpp:150] Setting up conv4
I0521 07:03:22.199654  2580 net.cpp:157] Top shape: 790 36 6 42 (7166880)
I0521 07:03:22.199667  2580 net.cpp:165] Memory required for data: 1201384600
I0521 07:03:22.199682  2580 layer_factory.hpp:77] Creating layer relu4
I0521 07:03:22.199695  2580 net.cpp:106] Creating Layer relu4
I0521 07:03:22.199705  2580 net.cpp:454] relu4 <- conv4
I0521 07:03:22.199718  2580 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:03:22.200189  2580 net.cpp:150] Setting up relu4
I0521 07:03:22.200206  2580 net.cpp:157] Top shape: 790 36 6 42 (7166880)
I0521 07:03:22.200215  2580 net.cpp:165] Memory required for data: 1230052120
I0521 07:03:22.200227  2580 layer_factory.hpp:77] Creating layer pool4
I0521 07:03:22.200239  2580 net.cpp:106] Creating Layer pool4
I0521 07:03:22.200249  2580 net.cpp:454] pool4 <- conv4
I0521 07:03:22.200263  2580 net.cpp:411] pool4 -> pool4
I0521 07:03:22.200335  2580 net.cpp:150] Setting up pool4
I0521 07:03:22.200348  2580 net.cpp:157] Top shape: 790 36 3 42 (3583440)
I0521 07:03:22.200357  2580 net.cpp:165] Memory required for data: 1244385880
I0521 07:03:22.200366  2580 layer_factory.hpp:77] Creating layer ip1
I0521 07:03:22.200381  2580 net.cpp:106] Creating Layer ip1
I0521 07:03:22.200392  2580 net.cpp:454] ip1 <- pool4
I0521 07:03:22.200407  2580 net.cpp:411] ip1 -> ip1
I0521 07:03:22.215843  2580 net.cpp:150] Setting up ip1
I0521 07:03:22.215873  2580 net.cpp:157] Top shape: 790 196 (154840)
I0521 07:03:22.215884  2580 net.cpp:165] Memory required for data: 1245005240
I0521 07:03:22.215912  2580 layer_factory.hpp:77] Creating layer relu5
I0521 07:03:22.215929  2580 net.cpp:106] Creating Layer relu5
I0521 07:03:22.215939  2580 net.cpp:454] relu5 <- ip1
I0521 07:03:22.215951  2580 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:03:22.216295  2580 net.cpp:150] Setting up relu5
I0521 07:03:22.216310  2580 net.cpp:157] Top shape: 790 196 (154840)
I0521 07:03:22.216320  2580 net.cpp:165] Memory required for data: 1245624600
I0521 07:03:22.216331  2580 layer_factory.hpp:77] Creating layer drop1
I0521 07:03:22.216348  2580 net.cpp:106] Creating Layer drop1
I0521 07:03:22.216358  2580 net.cpp:454] drop1 <- ip1
I0521 07:03:22.216372  2580 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:03:22.216416  2580 net.cpp:150] Setting up drop1
I0521 07:03:22.216429  2580 net.cpp:157] Top shape: 790 196 (154840)
I0521 07:03:22.216439  2580 net.cpp:165] Memory required for data: 1246243960
I0521 07:03:22.216449  2580 layer_factory.hpp:77] Creating layer ip2
I0521 07:03:22.216464  2580 net.cpp:106] Creating Layer ip2
I0521 07:03:22.216473  2580 net.cpp:454] ip2 <- ip1
I0521 07:03:22.216487  2580 net.cpp:411] ip2 -> ip2
I0521 07:03:22.216967  2580 net.cpp:150] Setting up ip2
I0521 07:03:22.216981  2580 net.cpp:157] Top shape: 790 98 (77420)
I0521 07:03:22.216991  2580 net.cpp:165] Memory required for data: 1246553640
I0521 07:03:22.217020  2580 layer_factory.hpp:77] Creating layer relu6
I0521 07:03:22.217032  2580 net.cpp:106] Creating Layer relu6
I0521 07:03:22.217042  2580 net.cpp:454] relu6 <- ip2
I0521 07:03:22.217056  2580 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:03:22.217586  2580 net.cpp:150] Setting up relu6
I0521 07:03:22.217607  2580 net.cpp:157] Top shape: 790 98 (77420)
I0521 07:03:22.217617  2580 net.cpp:165] Memory required for data: 1246863320
I0521 07:03:22.217627  2580 layer_factory.hpp:77] Creating layer drop2
I0521 07:03:22.217640  2580 net.cpp:106] Creating Layer drop2
I0521 07:03:22.217650  2580 net.cpp:454] drop2 <- ip2
I0521 07:03:22.217664  2580 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:03:22.217706  2580 net.cpp:150] Setting up drop2
I0521 07:03:22.217720  2580 net.cpp:157] Top shape: 790 98 (77420)
I0521 07:03:22.217730  2580 net.cpp:165] Memory required for data: 1247173000
I0521 07:03:22.217739  2580 layer_factory.hpp:77] Creating layer ip3
I0521 07:03:22.217753  2580 net.cpp:106] Creating Layer ip3
I0521 07:03:22.217763  2580 net.cpp:454] ip3 <- ip2
I0521 07:03:22.217777  2580 net.cpp:411] ip3 -> ip3
I0521 07:03:22.217996  2580 net.cpp:150] Setting up ip3
I0521 07:03:22.218010  2580 net.cpp:157] Top shape: 790 11 (8690)
I0521 07:03:22.218020  2580 net.cpp:165] Memory required for data: 1247207760
I0521 07:03:22.218035  2580 layer_factory.hpp:77] Creating layer drop3
I0521 07:03:22.218049  2580 net.cpp:106] Creating Layer drop3
I0521 07:03:22.218058  2580 net.cpp:454] drop3 <- ip3
I0521 07:03:22.218071  2580 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:03:22.218112  2580 net.cpp:150] Setting up drop3
I0521 07:03:22.218124  2580 net.cpp:157] Top shape: 790 11 (8690)
I0521 07:03:22.218134  2580 net.cpp:165] Memory required for data: 1247242520
I0521 07:03:22.218144  2580 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 07:03:22.218158  2580 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 07:03:22.218168  2580 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 07:03:22.218180  2580 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 07:03:22.218195  2580 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 07:03:22.218276  2580 net.cpp:150] Setting up ip3_drop3_0_split
I0521 07:03:22.218289  2580 net.cpp:157] Top shape: 790 11 (8690)
I0521 07:03:22.218302  2580 net.cpp:157] Top shape: 790 11 (8690)
I0521 07:03:22.218312  2580 net.cpp:165] Memory required for data: 1247312040
I0521 07:03:22.218322  2580 layer_factory.hpp:77] Creating layer accuracy
I0521 07:03:22.218343  2580 net.cpp:106] Creating Layer accuracy
I0521 07:03:22.218353  2580 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 07:03:22.218364  2580 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 07:03:22.218379  2580 net.cpp:411] accuracy -> accuracy
I0521 07:03:22.218401  2580 net.cpp:150] Setting up accuracy
I0521 07:03:22.218415  2580 net.cpp:157] Top shape: (1)
I0521 07:03:22.218425  2580 net.cpp:165] Memory required for data: 1247312044
I0521 07:03:22.218435  2580 layer_factory.hpp:77] Creating layer loss
I0521 07:03:22.218448  2580 net.cpp:106] Creating Layer loss
I0521 07:03:22.218458  2580 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 07:03:22.218469  2580 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 07:03:22.218482  2580 net.cpp:411] loss -> loss
I0521 07:03:22.218499  2580 layer_factory.hpp:77] Creating layer loss
I0521 07:03:22.218992  2580 net.cpp:150] Setting up loss
I0521 07:03:22.219005  2580 net.cpp:157] Top shape: (1)
I0521 07:03:22.219015  2580 net.cpp:160]     with loss weight 1
I0521 07:03:22.219034  2580 net.cpp:165] Memory required for data: 1247312048
I0521 07:03:22.219044  2580 net.cpp:226] loss needs backward computation.
I0521 07:03:22.219055  2580 net.cpp:228] accuracy does not need backward computation.
I0521 07:03:22.219066  2580 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 07:03:22.219077  2580 net.cpp:226] drop3 needs backward computation.
I0521 07:03:22.219087  2580 net.cpp:226] ip3 needs backward computation.
I0521 07:03:22.219099  2580 net.cpp:226] drop2 needs backward computation.
I0521 07:03:22.219116  2580 net.cpp:226] relu6 needs backward computation.
I0521 07:03:22.219126  2580 net.cpp:226] ip2 needs backward computation.
I0521 07:03:22.219136  2580 net.cpp:226] drop1 needs backward computation.
I0521 07:03:22.219146  2580 net.cpp:226] relu5 needs backward computation.
I0521 07:03:22.219156  2580 net.cpp:226] ip1 needs backward computation.
I0521 07:03:22.219166  2580 net.cpp:226] pool4 needs backward computation.
I0521 07:03:22.219177  2580 net.cpp:226] relu4 needs backward computation.
I0521 07:03:22.219187  2580 net.cpp:226] conv4 needs backward computation.
I0521 07:03:22.219197  2580 net.cpp:226] pool3 needs backward computation.
I0521 07:03:22.219208  2580 net.cpp:226] relu3 needs backward computation.
I0521 07:03:22.219218  2580 net.cpp:226] conv3 needs backward computation.
I0521 07:03:22.219228  2580 net.cpp:226] pool2 needs backward computation.
I0521 07:03:22.219239  2580 net.cpp:226] relu2 needs backward computation.
I0521 07:03:22.219249  2580 net.cpp:226] conv2 needs backward computation.
I0521 07:03:22.219259  2580 net.cpp:226] pool1 needs backward computation.
I0521 07:03:22.219270  2580 net.cpp:226] relu1 needs backward computation.
I0521 07:03:22.219280  2580 net.cpp:226] conv1 needs backward computation.
I0521 07:03:22.219291  2580 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 07:03:22.219303  2580 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:03:22.219312  2580 net.cpp:270] This network produces output accuracy
I0521 07:03:22.219323  2580 net.cpp:270] This network produces output loss
I0521 07:03:22.219352  2580 net.cpp:283] Network initialization done.
I0521 07:03:22.219485  2580 solver.cpp:60] Solver scaffolding done.
I0521 07:03:22.220610  2580 caffe.cpp:212] Starting Optimization
I0521 07:03:22.220628  2580 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 07:03:22.220643  2580 solver.cpp:289] Learning Rate Policy: fixed
I0521 07:03:22.221868  2580 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 07:04:08.147876  2580 solver.cpp:409]     Test net output #0: accuracy = 0.0383565
I0521 07:04:08.148036  2580 solver.cpp:409]     Test net output #1: loss = 2.3989 (* 1 = 2.3989 loss)
I0521 07:04:08.295058  2580 solver.cpp:237] Iteration 0, loss = 2.39934
I0521 07:04:08.295094  2580 solver.cpp:253]     Train net output #0: loss = 2.39934 (* 1 = 2.39934 loss)
I0521 07:04:08.295112  2580 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 07:04:15.947787  2580 solver.cpp:237] Iteration 18, loss = 2.38756
I0521 07:04:15.947821  2580 solver.cpp:253]     Train net output #0: loss = 2.38756 (* 1 = 2.38756 loss)
I0521 07:04:15.947836  2580 sgd_solver.cpp:106] Iteration 18, lr = 0.0025
I0521 07:04:23.601371  2580 solver.cpp:237] Iteration 36, loss = 2.37829
I0521 07:04:23.601402  2580 solver.cpp:253]     Train net output #0: loss = 2.37829 (* 1 = 2.37829 loss)
I0521 07:04:23.601416  2580 sgd_solver.cpp:106] Iteration 36, lr = 0.0025
I0521 07:04:31.258894  2580 solver.cpp:237] Iteration 54, loss = 2.36286
I0521 07:04:31.258934  2580 solver.cpp:253]     Train net output #0: loss = 2.36286 (* 1 = 2.36286 loss)
I0521 07:04:31.258951  2580 sgd_solver.cpp:106] Iteration 54, lr = 0.0025
I0521 07:04:38.908468  2580 solver.cpp:237] Iteration 72, loss = 2.34844
I0521 07:04:38.908612  2580 solver.cpp:253]     Train net output #0: loss = 2.34844 (* 1 = 2.34844 loss)
I0521 07:04:38.908625  2580 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0521 07:04:46.568056  2580 solver.cpp:237] Iteration 90, loss = 2.34722
I0521 07:04:46.568087  2580 solver.cpp:253]     Train net output #0: loss = 2.34722 (* 1 = 2.34722 loss)
I0521 07:04:46.568104  2580 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 07:04:54.223330  2580 solver.cpp:237] Iteration 108, loss = 2.34313
I0521 07:04:54.223373  2580 solver.cpp:253]     Train net output #0: loss = 2.34313 (* 1 = 2.34313 loss)
I0521 07:04:54.223387  2580 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0521 07:05:23.994526  2580 solver.cpp:237] Iteration 126, loss = 2.32327
I0521 07:05:23.994658  2580 solver.cpp:253]     Train net output #0: loss = 2.32327 (* 1 = 2.32327 loss)
I0521 07:05:23.994673  2580 sgd_solver.cpp:106] Iteration 126, lr = 0.0025
I0521 07:05:31.653794  2580 solver.cpp:237] Iteration 144, loss = 2.31932
I0521 07:05:31.653827  2580 solver.cpp:253]     Train net output #0: loss = 2.31932 (* 1 = 2.31932 loss)
I0521 07:05:31.653843  2580 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 07:05:39.303724  2580 solver.cpp:237] Iteration 162, loss = 2.31155
I0521 07:05:39.303756  2580 solver.cpp:253]     Train net output #0: loss = 2.31155 (* 1 = 2.31155 loss)
I0521 07:05:39.303774  2580 sgd_solver.cpp:106] Iteration 162, lr = 0.0025
I0521 07:05:46.961590  2580 solver.cpp:237] Iteration 180, loss = 2.31715
I0521 07:05:46.961628  2580 solver.cpp:253]     Train net output #0: loss = 2.31715 (* 1 = 2.31715 loss)
I0521 07:05:46.961649  2580 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 07:05:50.362947  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_189.caffemodel
I0521 07:05:50.702769  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_189.solverstate
I0521 07:05:54.681716  2580 solver.cpp:237] Iteration 198, loss = 2.31232
I0521 07:05:54.681869  2580 solver.cpp:253]     Train net output #0: loss = 2.31232 (* 1 = 2.31232 loss)
I0521 07:05:54.681884  2580 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 07:06:02.340553  2580 solver.cpp:237] Iteration 216, loss = 2.30324
I0521 07:06:02.340585  2580 solver.cpp:253]     Train net output #0: loss = 2.30324 (* 1 = 2.30324 loss)
I0521 07:06:02.340603  2580 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 07:06:09.996428  2580 solver.cpp:237] Iteration 234, loss = 2.30672
I0521 07:06:09.996475  2580 solver.cpp:253]     Train net output #0: loss = 2.30672 (* 1 = 2.30672 loss)
I0521 07:06:09.996490  2580 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0521 07:06:17.651996  2580 solver.cpp:237] Iteration 252, loss = 2.2781
I0521 07:06:17.652029  2580 solver.cpp:253]     Train net output #0: loss = 2.2781 (* 1 = 2.2781 loss)
I0521 07:06:17.652047  2580 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 07:06:47.447907  2580 solver.cpp:237] Iteration 270, loss = 2.26603
I0521 07:06:47.448082  2580 solver.cpp:253]     Train net output #0: loss = 2.26603 (* 1 = 2.26603 loss)
I0521 07:06:47.448096  2580 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 07:06:55.106191  2580 solver.cpp:237] Iteration 288, loss = 2.2473
I0521 07:06:55.106225  2580 solver.cpp:253]     Train net output #0: loss = 2.2473 (* 1 = 2.2473 loss)
I0521 07:06:55.106240  2580 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 07:07:02.766871  2580 solver.cpp:237] Iteration 306, loss = 2.2111
I0521 07:07:02.766912  2580 solver.cpp:253]     Train net output #0: loss = 2.2111 (* 1 = 2.2111 loss)
I0521 07:07:02.766932  2580 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 07:07:10.424190  2580 solver.cpp:237] Iteration 324, loss = 2.18583
I0521 07:07:10.424222  2580 solver.cpp:253]     Train net output #0: loss = 2.18583 (* 1 = 2.18583 loss)
I0521 07:07:10.424238  2580 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0521 07:07:18.083346  2580 solver.cpp:237] Iteration 342, loss = 2.19984
I0521 07:07:18.083492  2580 solver.cpp:253]     Train net output #0: loss = 2.19984 (* 1 = 2.19984 loss)
I0521 07:07:18.083505  2580 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0521 07:07:25.743295  2580 solver.cpp:237] Iteration 360, loss = 2.18367
I0521 07:07:25.743340  2580 solver.cpp:253]     Train net output #0: loss = 2.18367 (* 1 = 2.18367 loss)
I0521 07:07:25.743353  2580 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 07:07:32.977257  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_378.caffemodel
I0521 07:07:33.314200  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_378.solverstate
I0521 07:07:33.466119  2580 solver.cpp:237] Iteration 378, loss = 2.10772
I0521 07:07:33.466166  2580 solver.cpp:253]     Train net output #0: loss = 2.10772 (* 1 = 2.10772 loss)
I0521 07:07:33.466181  2580 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 07:07:33.466693  2580 solver.cpp:341] Iteration 379, Testing net (#0)
I0521 07:08:18.800623  2580 solver.cpp:409]     Test net output #0: accuracy = 0.462528
I0521 07:08:18.800783  2580 solver.cpp:409]     Test net output #1: loss = 1.95118 (* 1 = 1.95118 loss)
I0521 07:08:48.293144  2580 solver.cpp:237] Iteration 396, loss = 2.1012
I0521 07:08:48.293195  2580 solver.cpp:253]     Train net output #0: loss = 2.1012 (* 1 = 2.1012 loss)
I0521 07:08:48.293212  2580 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 07:08:55.945466  2580 solver.cpp:237] Iteration 414, loss = 2.08689
I0521 07:08:55.945611  2580 solver.cpp:253]     Train net output #0: loss = 2.08689 (* 1 = 2.08689 loss)
I0521 07:08:55.945624  2580 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0521 07:09:03.598590  2580 solver.cpp:237] Iteration 432, loss = 2.02487
I0521 07:09:03.598623  2580 solver.cpp:253]     Train net output #0: loss = 2.02487 (* 1 = 2.02487 loss)
I0521 07:09:03.598639  2580 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 07:09:11.248778  2580 solver.cpp:237] Iteration 450, loss = 2.0407
I0521 07:09:11.248822  2580 solver.cpp:253]     Train net output #0: loss = 2.0407 (* 1 = 2.0407 loss)
I0521 07:09:11.248837  2580 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 07:09:18.900009  2580 solver.cpp:237] Iteration 468, loss = 2.00462
I0521 07:09:18.900043  2580 solver.cpp:253]     Train net output #0: loss = 2.00462 (* 1 = 2.00462 loss)
I0521 07:09:18.900058  2580 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0521 07:09:26.554194  2580 solver.cpp:237] Iteration 486, loss = 1.96516
I0521 07:09:26.554337  2580 solver.cpp:253]     Train net output #0: loss = 1.96516 (* 1 = 1.96516 loss)
I0521 07:09:26.554350  2580 sgd_solver.cpp:106] Iteration 486, lr = 0.0025
I0521 07:09:34.205929  2580 solver.cpp:237] Iteration 504, loss = 2.01972
I0521 07:09:34.205967  2580 solver.cpp:253]     Train net output #0: loss = 2.01972 (* 1 = 2.01972 loss)
I0521 07:09:34.205989  2580 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 07:10:03.993672  2580 solver.cpp:237] Iteration 522, loss = 2.03286
I0521 07:10:03.993844  2580 solver.cpp:253]     Train net output #0: loss = 2.03286 (* 1 = 2.03286 loss)
I0521 07:10:03.993860  2580 sgd_solver.cpp:106] Iteration 522, lr = 0.0025
I0521 07:10:11.640754  2580 solver.cpp:237] Iteration 540, loss = 1.95677
I0521 07:10:11.640786  2580 solver.cpp:253]     Train net output #0: loss = 1.95677 (* 1 = 1.95677 loss)
I0521 07:10:11.640805  2580 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 07:10:19.295642  2580 solver.cpp:237] Iteration 558, loss = 1.92097
I0521 07:10:19.295686  2580 solver.cpp:253]     Train net output #0: loss = 1.92097 (* 1 = 1.92097 loss)
I0521 07:10:19.295701  2580 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0521 07:10:22.697310  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_567.caffemodel
I0521 07:10:23.035194  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_567.solverstate
I0521 07:10:27.015506  2580 solver.cpp:237] Iteration 576, loss = 1.91147
I0521 07:10:27.015555  2580 solver.cpp:253]     Train net output #0: loss = 1.91147 (* 1 = 1.91147 loss)
I0521 07:10:27.015569  2580 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 07:10:34.671254  2580 solver.cpp:237] Iteration 594, loss = 1.82952
I0521 07:10:34.671396  2580 solver.cpp:253]     Train net output #0: loss = 1.82952 (* 1 = 1.82952 loss)
I0521 07:10:34.671409  2580 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 07:10:42.321115  2580 solver.cpp:237] Iteration 612, loss = 1.88358
I0521 07:10:42.321148  2580 solver.cpp:253]     Train net output #0: loss = 1.88358 (* 1 = 1.88358 loss)
I0521 07:10:42.321166  2580 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 07:10:49.974266  2580 solver.cpp:237] Iteration 630, loss = 1.87373
I0521 07:10:49.974308  2580 solver.cpp:253]     Train net output #0: loss = 1.87373 (* 1 = 1.87373 loss)
I0521 07:10:49.974325  2580 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 07:11:19.749116  2580 solver.cpp:237] Iteration 648, loss = 1.86033
I0521 07:11:19.749275  2580 solver.cpp:253]     Train net output #0: loss = 1.86033 (* 1 = 1.86033 loss)
I0521 07:11:19.749289  2580 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 07:11:27.400388  2580 solver.cpp:237] Iteration 666, loss = 1.8692
I0521 07:11:27.400420  2580 solver.cpp:253]     Train net output #0: loss = 1.8692 (* 1 = 1.8692 loss)
I0521 07:11:27.400439  2580 sgd_solver.cpp:106] Iteration 666, lr = 0.0025
I0521 07:11:35.049422  2580 solver.cpp:237] Iteration 684, loss = 1.87708
I0521 07:11:35.049464  2580 solver.cpp:253]     Train net output #0: loss = 1.87708 (* 1 = 1.87708 loss)
I0521 07:11:35.049479  2580 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0521 07:11:42.701642  2580 solver.cpp:237] Iteration 702, loss = 1.89316
I0521 07:11:42.701674  2580 solver.cpp:253]     Train net output #0: loss = 1.89316 (* 1 = 1.89316 loss)
I0521 07:11:42.701690  2580 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 07:11:50.354379  2580 solver.cpp:237] Iteration 720, loss = 1.88983
I0521 07:11:50.354516  2580 solver.cpp:253]     Train net output #0: loss = 1.88983 (* 1 = 1.88983 loss)
I0521 07:11:50.354528  2580 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 07:11:58.013838  2580 solver.cpp:237] Iteration 738, loss = 1.84552
I0521 07:11:58.013870  2580 solver.cpp:253]     Train net output #0: loss = 1.84552 (* 1 = 1.84552 loss)
I0521 07:11:58.013888  2580 sgd_solver.cpp:106] Iteration 738, lr = 0.0025
I0521 07:12:05.241791  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_756.caffemodel
I0521 07:12:05.578801  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_756.solverstate
I0521 07:12:05.733422  2580 solver.cpp:237] Iteration 756, loss = 1.81096
I0521 07:12:05.733469  2580 solver.cpp:253]     Train net output #0: loss = 1.81096 (* 1 = 1.81096 loss)
I0521 07:12:05.733484  2580 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 07:12:06.158303  2580 solver.cpp:341] Iteration 758, Testing net (#0)
I0521 07:13:12.328744  2580 solver.cpp:409]     Test net output #0: accuracy = 0.602907
I0521 07:13:12.328920  2580 solver.cpp:409]     Test net output #1: loss = 1.42021 (* 1 = 1.42021 loss)
I0521 07:13:41.379979  2580 solver.cpp:237] Iteration 774, loss = 1.87235
I0521 07:13:41.380029  2580 solver.cpp:253]     Train net output #0: loss = 1.87235 (* 1 = 1.87235 loss)
I0521 07:13:41.380045  2580 sgd_solver.cpp:106] Iteration 774, lr = 0.0025
I0521 07:13:49.023000  2580 solver.cpp:237] Iteration 792, loss = 1.8127
I0521 07:13:49.023149  2580 solver.cpp:253]     Train net output #0: loss = 1.8127 (* 1 = 1.8127 loss)
I0521 07:13:49.023162  2580 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 07:13:56.669888  2580 solver.cpp:237] Iteration 810, loss = 1.88103
I0521 07:13:56.669919  2580 solver.cpp:253]     Train net output #0: loss = 1.88103 (* 1 = 1.88103 loss)
I0521 07:13:56.669937  2580 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 07:14:04.315583  2580 solver.cpp:237] Iteration 828, loss = 1.82604
I0521 07:14:04.315628  2580 solver.cpp:253]     Train net output #0: loss = 1.82604 (* 1 = 1.82604 loss)
I0521 07:14:04.315641  2580 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0521 07:14:11.957181  2580 solver.cpp:237] Iteration 846, loss = 1.8141
I0521 07:14:11.957213  2580 solver.cpp:253]     Train net output #0: loss = 1.8141 (* 1 = 1.8141 loss)
I0521 07:14:11.957227  2580 sgd_solver.cpp:106] Iteration 846, lr = 0.0025
I0521 07:14:19.599797  2580 solver.cpp:237] Iteration 864, loss = 1.80698
I0521 07:14:19.599931  2580 solver.cpp:253]     Train net output #0: loss = 1.80698 (* 1 = 1.80698 loss)
I0521 07:14:19.599944  2580 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 07:14:27.247377  2580 solver.cpp:237] Iteration 882, loss = 1.82558
I0521 07:14:27.247409  2580 solver.cpp:253]     Train net output #0: loss = 1.82558 (* 1 = 1.82558 loss)
I0521 07:14:27.247427  2580 sgd_solver.cpp:106] Iteration 882, lr = 0.0025
I0521 07:14:56.998795  2580 solver.cpp:237] Iteration 900, loss = 1.76528
I0521 07:14:56.998960  2580 solver.cpp:253]     Train net output #0: loss = 1.76528 (* 1 = 1.76528 loss)
I0521 07:14:56.998973  2580 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 07:15:04.647766  2580 solver.cpp:237] Iteration 918, loss = 1.76922
I0521 07:15:04.647799  2580 solver.cpp:253]     Train net output #0: loss = 1.76922 (* 1 = 1.76922 loss)
I0521 07:15:04.647815  2580 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 07:15:12.292230  2580 solver.cpp:237] Iteration 936, loss = 1.82938
I0521 07:15:12.292263  2580 solver.cpp:253]     Train net output #0: loss = 1.82938 (* 1 = 1.82938 loss)
I0521 07:15:12.292279  2580 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 07:15:15.691059  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_945.caffemodel
I0521 07:15:16.028975  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_945.solverstate
I0521 07:15:20.007804  2580 solver.cpp:237] Iteration 954, loss = 1.85268
I0521 07:15:20.007853  2580 solver.cpp:253]     Train net output #0: loss = 1.85268 (* 1 = 1.85268 loss)
I0521 07:15:20.007869  2580 sgd_solver.cpp:106] Iteration 954, lr = 0.0025
I0521 07:15:27.652918  2580 solver.cpp:237] Iteration 972, loss = 1.85093
I0521 07:15:27.653071  2580 solver.cpp:253]     Train net output #0: loss = 1.85093 (* 1 = 1.85093 loss)
I0521 07:15:27.653084  2580 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0521 07:15:35.293238  2580 solver.cpp:237] Iteration 990, loss = 1.78325
I0521 07:15:35.293269  2580 solver.cpp:253]     Train net output #0: loss = 1.78325 (* 1 = 1.78325 loss)
I0521 07:15:35.293287  2580 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 07:15:42.935446  2580 solver.cpp:237] Iteration 1008, loss = 1.81769
I0521 07:15:42.935479  2580 solver.cpp:253]     Train net output #0: loss = 1.81769 (* 1 = 1.81769 loss)
I0521 07:15:42.935497  2580 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 07:16:12.721351  2580 solver.cpp:237] Iteration 1026, loss = 1.87509
I0521 07:16:12.721514  2580 solver.cpp:253]     Train net output #0: loss = 1.87509 (* 1 = 1.87509 loss)
I0521 07:16:12.721529  2580 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 07:16:20.372195  2580 solver.cpp:237] Iteration 1044, loss = 1.74186
I0521 07:16:20.372228  2580 solver.cpp:253]     Train net output #0: loss = 1.74186 (* 1 = 1.74186 loss)
I0521 07:16:20.372242  2580 sgd_solver.cpp:106] Iteration 1044, lr = 0.0025
I0521 07:16:28.023514  2580 solver.cpp:237] Iteration 1062, loss = 1.79926
I0521 07:16:28.023546  2580 solver.cpp:253]     Train net output #0: loss = 1.79926 (* 1 = 1.79926 loss)
I0521 07:16:28.023561  2580 sgd_solver.cpp:106] Iteration 1062, lr = 0.0025
I0521 07:16:35.669297  2580 solver.cpp:237] Iteration 1080, loss = 1.79566
I0521 07:16:35.669332  2580 solver.cpp:253]     Train net output #0: loss = 1.79566 (* 1 = 1.79566 loss)
I0521 07:16:35.669350  2580 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 07:16:43.320688  2580 solver.cpp:237] Iteration 1098, loss = 1.77931
I0521 07:16:43.320827  2580 solver.cpp:253]     Train net output #0: loss = 1.77931 (* 1 = 1.77931 loss)
I0521 07:16:43.320840  2580 sgd_solver.cpp:106] Iteration 1098, lr = 0.0025
I0521 07:16:50.966255  2580 solver.cpp:237] Iteration 1116, loss = 1.74566
I0521 07:16:50.966292  2580 solver.cpp:253]     Train net output #0: loss = 1.74566 (* 1 = 1.74566 loss)
I0521 07:16:50.966310  2580 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0521 07:16:58.184748  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1134.caffemodel
I0521 07:16:58.520206  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1134.solverstate
I0521 07:16:58.672651  2580 solver.cpp:237] Iteration 1134, loss = 1.75514
I0521 07:16:58.672695  2580 solver.cpp:253]     Train net output #0: loss = 1.75514 (* 1 = 1.75514 loss)
I0521 07:16:58.672713  2580 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 07:16:59.524619  2580 solver.cpp:341] Iteration 1137, Testing net (#0)
I0521 07:17:44.519644  2580 solver.cpp:409]     Test net output #0: accuracy = 0.643259
I0521 07:17:44.519807  2580 solver.cpp:409]     Test net output #1: loss = 1.29957 (* 1 = 1.29957 loss)
I0521 07:18:13.154117  2580 solver.cpp:237] Iteration 1152, loss = 1.87336
I0521 07:18:13.154170  2580 solver.cpp:253]     Train net output #0: loss = 1.87336 (* 1 = 1.87336 loss)
I0521 07:18:13.154186  2580 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 07:18:20.801210  2580 solver.cpp:237] Iteration 1170, loss = 1.74563
I0521 07:18:20.801367  2580 solver.cpp:253]     Train net output #0: loss = 1.74563 (* 1 = 1.74563 loss)
I0521 07:18:20.801381  2580 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 07:18:28.450012  2580 solver.cpp:237] Iteration 1188, loss = 1.79035
I0521 07:18:28.450045  2580 solver.cpp:253]     Train net output #0: loss = 1.79035 (* 1 = 1.79035 loss)
I0521 07:18:28.450062  2580 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 07:18:36.095055  2580 solver.cpp:237] Iteration 1206, loss = 1.74216
I0521 07:18:36.095088  2580 solver.cpp:253]     Train net output #0: loss = 1.74216 (* 1 = 1.74216 loss)
I0521 07:18:36.095105  2580 sgd_solver.cpp:106] Iteration 1206, lr = 0.0025
I0521 07:18:43.750510  2580 solver.cpp:237] Iteration 1224, loss = 1.74091
I0521 07:18:43.750550  2580 solver.cpp:253]     Train net output #0: loss = 1.74091 (* 1 = 1.74091 loss)
I0521 07:18:43.750572  2580 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 07:18:51.398052  2580 solver.cpp:237] Iteration 1242, loss = 1.74805
I0521 07:18:51.398203  2580 solver.cpp:253]     Train net output #0: loss = 1.74805 (* 1 = 1.74805 loss)
I0521 07:18:51.398217  2580 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 07:18:59.045398  2580 solver.cpp:237] Iteration 1260, loss = 1.71068
I0521 07:18:59.045430  2580 solver.cpp:253]     Train net output #0: loss = 1.71068 (* 1 = 1.71068 loss)
I0521 07:18:59.045447  2580 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 07:19:28.827927  2580 solver.cpp:237] Iteration 1278, loss = 1.76012
I0521 07:19:28.828090  2580 solver.cpp:253]     Train net output #0: loss = 1.76012 (* 1 = 1.76012 loss)
I0521 07:19:28.828105  2580 sgd_solver.cpp:106] Iteration 1278, lr = 0.0025
I0521 07:19:36.476724  2580 solver.cpp:237] Iteration 1296, loss = 1.73832
I0521 07:19:36.476784  2580 solver.cpp:253]     Train net output #0: loss = 1.73832 (* 1 = 1.73832 loss)
I0521 07:19:36.476797  2580 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 07:19:44.126164  2580 solver.cpp:237] Iteration 1314, loss = 1.74889
I0521 07:19:44.126197  2580 solver.cpp:253]     Train net output #0: loss = 1.74889 (* 1 = 1.74889 loss)
I0521 07:19:44.126210  2580 sgd_solver.cpp:106] Iteration 1314, lr = 0.0025
I0521 07:19:47.523712  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1323.caffemodel
I0521 07:19:47.859704  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1323.solverstate
I0521 07:19:51.835789  2580 solver.cpp:237] Iteration 1332, loss = 1.7368
I0521 07:19:51.835836  2580 solver.cpp:253]     Train net output #0: loss = 1.7368 (* 1 = 1.7368 loss)
I0521 07:19:51.835850  2580 sgd_solver.cpp:106] Iteration 1332, lr = 0.0025
I0521 07:19:59.481745  2580 solver.cpp:237] Iteration 1350, loss = 1.7491
I0521 07:19:59.481910  2580 solver.cpp:253]     Train net output #0: loss = 1.7491 (* 1 = 1.7491 loss)
I0521 07:19:59.481925  2580 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 07:20:07.130250  2580 solver.cpp:237] Iteration 1368, loss = 1.68659
I0521 07:20:07.130287  2580 solver.cpp:253]     Train net output #0: loss = 1.68659 (* 1 = 1.68659 loss)
I0521 07:20:07.130302  2580 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 07:20:14.780575  2580 solver.cpp:237] Iteration 1386, loss = 1.74826
I0521 07:20:14.780607  2580 solver.cpp:253]     Train net output #0: loss = 1.74826 (* 1 = 1.74826 loss)
I0521 07:20:14.780624  2580 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 07:20:44.543947  2580 solver.cpp:237] Iteration 1404, loss = 1.76149
I0521 07:20:44.544107  2580 solver.cpp:253]     Train net output #0: loss = 1.76149 (* 1 = 1.76149 loss)
I0521 07:20:44.544122  2580 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 07:20:52.188315  2580 solver.cpp:237] Iteration 1422, loss = 1.69286
I0521 07:20:52.188349  2580 solver.cpp:253]     Train net output #0: loss = 1.69286 (* 1 = 1.69286 loss)
I0521 07:20:52.188366  2580 sgd_solver.cpp:106] Iteration 1422, lr = 0.0025
I0521 07:20:59.836421  2580 solver.cpp:237] Iteration 1440, loss = 1.7131
I0521 07:20:59.836453  2580 solver.cpp:253]     Train net output #0: loss = 1.7131 (* 1 = 1.7131 loss)
I0521 07:20:59.836469  2580 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 07:21:07.483042  2580 solver.cpp:237] Iteration 1458, loss = 1.6893
I0521 07:21:07.483075  2580 solver.cpp:253]     Train net output #0: loss = 1.6893 (* 1 = 1.6893 loss)
I0521 07:21:07.483091  2580 sgd_solver.cpp:106] Iteration 1458, lr = 0.0025
I0521 07:21:15.134132  2580 solver.cpp:237] Iteration 1476, loss = 1.67001
I0521 07:21:15.134299  2580 solver.cpp:253]     Train net output #0: loss = 1.67001 (* 1 = 1.67001 loss)
I0521 07:21:15.134313  2580 sgd_solver.cpp:106] Iteration 1476, lr = 0.0025
I0521 07:21:22.782692  2580 solver.cpp:237] Iteration 1494, loss = 1.69997
I0521 07:21:22.782724  2580 solver.cpp:253]     Train net output #0: loss = 1.69997 (* 1 = 1.69997 loss)
I0521 07:21:22.782738  2580 sgd_solver.cpp:106] Iteration 1494, lr = 0.0025
I0521 07:21:30.005132  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1512.caffemodel
I0521 07:21:30.341387  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1512.solverstate
I0521 07:21:30.494453  2580 solver.cpp:237] Iteration 1512, loss = 1.68506
I0521 07:21:30.494499  2580 solver.cpp:253]     Train net output #0: loss = 1.68506 (* 1 = 1.68506 loss)
I0521 07:21:30.494518  2580 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 07:21:31.770627  2580 solver.cpp:341] Iteration 1516, Testing net (#0)
I0521 07:22:37.949714  2580 solver.cpp:409]     Test net output #0: accuracy = 0.660137
I0521 07:22:37.949884  2580 solver.cpp:409]     Test net output #1: loss = 1.16586 (* 1 = 1.16586 loss)
I0521 07:23:06.147140  2580 solver.cpp:237] Iteration 1530, loss = 1.64037
I0521 07:23:06.147192  2580 solver.cpp:253]     Train net output #0: loss = 1.64037 (* 1 = 1.64037 loss)
I0521 07:23:06.147207  2580 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 07:23:13.798921  2580 solver.cpp:237] Iteration 1548, loss = 1.74235
I0521 07:23:13.799078  2580 solver.cpp:253]     Train net output #0: loss = 1.74235 (* 1 = 1.74235 loss)
I0521 07:23:13.799091  2580 sgd_solver.cpp:106] Iteration 1548, lr = 0.0025
I0521 07:23:21.448154  2580 solver.cpp:237] Iteration 1566, loss = 1.70545
I0521 07:23:21.448192  2580 solver.cpp:253]     Train net output #0: loss = 1.70545 (* 1 = 1.70545 loss)
I0521 07:23:21.448206  2580 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 07:23:29.099865  2580 solver.cpp:237] Iteration 1584, loss = 1.73693
I0521 07:23:29.099898  2580 solver.cpp:253]     Train net output #0: loss = 1.73693 (* 1 = 1.73693 loss)
I0521 07:23:29.099915  2580 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 07:23:36.750761  2580 solver.cpp:237] Iteration 1602, loss = 1.68293
I0521 07:23:36.750794  2580 solver.cpp:253]     Train net output #0: loss = 1.68293 (* 1 = 1.68293 loss)
I0521 07:23:36.750810  2580 sgd_solver.cpp:106] Iteration 1602, lr = 0.0025
I0521 07:23:44.401921  2580 solver.cpp:237] Iteration 1620, loss = 1.63922
I0521 07:23:44.402070  2580 solver.cpp:253]     Train net output #0: loss = 1.63922 (* 1 = 1.63922 loss)
I0521 07:23:44.402083  2580 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 07:23:52.052062  2580 solver.cpp:237] Iteration 1638, loss = 1.67302
I0521 07:23:52.052093  2580 solver.cpp:253]     Train net output #0: loss = 1.67302 (* 1 = 1.67302 loss)
I0521 07:23:52.052109  2580 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 07:24:21.850536  2580 solver.cpp:237] Iteration 1656, loss = 1.68743
I0521 07:24:21.850708  2580 solver.cpp:253]     Train net output #0: loss = 1.68743 (* 1 = 1.68743 loss)
I0521 07:24:21.850724  2580 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 07:24:29.504097  2580 solver.cpp:237] Iteration 1674, loss = 1.68544
I0521 07:24:29.504134  2580 solver.cpp:253]     Train net output #0: loss = 1.68544 (* 1 = 1.68544 loss)
I0521 07:24:29.504153  2580 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 07:24:37.157665  2580 solver.cpp:237] Iteration 1692, loss = 1.69619
I0521 07:24:37.157697  2580 solver.cpp:253]     Train net output #0: loss = 1.69619 (* 1 = 1.69619 loss)
I0521 07:24:37.157712  2580 sgd_solver.cpp:106] Iteration 1692, lr = 0.0025
I0521 07:24:40.559604  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1701.caffemodel
I0521 07:24:40.899377  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1701.solverstate
I0521 07:24:44.880249  2580 solver.cpp:237] Iteration 1710, loss = 1.67158
I0521 07:24:44.880295  2580 solver.cpp:253]     Train net output #0: loss = 1.67158 (* 1 = 1.67158 loss)
I0521 07:24:44.880308  2580 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 07:24:52.534414  2580 solver.cpp:237] Iteration 1728, loss = 1.7189
I0521 07:24:52.534574  2580 solver.cpp:253]     Train net output #0: loss = 1.7189 (* 1 = 1.7189 loss)
I0521 07:24:52.534586  2580 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 07:25:00.186728  2580 solver.cpp:237] Iteration 1746, loss = 1.74258
I0521 07:25:00.186764  2580 solver.cpp:253]     Train net output #0: loss = 1.74258 (* 1 = 1.74258 loss)
I0521 07:25:00.186782  2580 sgd_solver.cpp:106] Iteration 1746, lr = 0.0025
I0521 07:25:07.839275  2580 solver.cpp:237] Iteration 1764, loss = 1.63158
I0521 07:25:07.839308  2580 solver.cpp:253]     Train net output #0: loss = 1.63158 (* 1 = 1.63158 loss)
I0521 07:25:07.839321  2580 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 07:25:37.619860  2580 solver.cpp:237] Iteration 1782, loss = 1.68504
I0521 07:25:37.620029  2580 solver.cpp:253]     Train net output #0: loss = 1.68504 (* 1 = 1.68504 loss)
I0521 07:25:37.620043  2580 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 07:25:45.273589  2580 solver.cpp:237] Iteration 1800, loss = 1.69882
I0521 07:25:45.273635  2580 solver.cpp:253]     Train net output #0: loss = 1.69882 (* 1 = 1.69882 loss)
I0521 07:25:45.273653  2580 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 07:25:52.929432  2580 solver.cpp:237] Iteration 1818, loss = 1.63017
I0521 07:25:52.929464  2580 solver.cpp:253]     Train net output #0: loss = 1.63017 (* 1 = 1.63017 loss)
I0521 07:25:52.929481  2580 sgd_solver.cpp:106] Iteration 1818, lr = 0.0025
I0521 07:26:00.584805  2580 solver.cpp:237] Iteration 1836, loss = 1.65266
I0521 07:26:00.584837  2580 solver.cpp:253]     Train net output #0: loss = 1.65266 (* 1 = 1.65266 loss)
I0521 07:26:00.584851  2580 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0521 07:26:08.236943  2580 solver.cpp:237] Iteration 1854, loss = 1.65557
I0521 07:26:08.237087  2580 solver.cpp:253]     Train net output #0: loss = 1.65557 (* 1 = 1.65557 loss)
I0521 07:26:08.237102  2580 sgd_solver.cpp:106] Iteration 1854, lr = 0.0025
I0521 07:26:15.889446  2580 solver.cpp:237] Iteration 1872, loss = 1.69345
I0521 07:26:15.889478  2580 solver.cpp:253]     Train net output #0: loss = 1.69345 (* 1 = 1.69345 loss)
I0521 07:26:15.889497  2580 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0521 07:26:23.119209  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1890.caffemodel
I0521 07:26:23.459224  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1890.solverstate
I0521 07:26:23.614205  2580 solver.cpp:237] Iteration 1890, loss = 1.67981
I0521 07:26:23.614256  2580 solver.cpp:253]     Train net output #0: loss = 1.67981 (* 1 = 1.67981 loss)
I0521 07:26:23.614277  2580 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0521 07:26:25.316624  2580 solver.cpp:341] Iteration 1895, Testing net (#0)
I0521 07:27:10.652286  2580 solver.cpp:409]     Test net output #0: accuracy = 0.675139
I0521 07:27:10.652463  2580 solver.cpp:409]     Test net output #1: loss = 1.14079 (* 1 = 1.14079 loss)
I0521 07:27:11.630064  2580 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1898.caffemodel
I0521 07:27:11.969095  2580 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318_iter_1898.solverstate
I0521 07:27:11.997135  2580 solver.cpp:326] Optimization Done.
I0521 07:27:11.997164  2580 caffe.cpp:215] Optimization Done.
Application 11237131 resources: utime ~1251s, stime ~225s, Rss ~5329068, inblocks ~3594475, outblocks ~194566
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_790_2016-05-20T11.21.01.485318.solver"
	User time (seconds): 0.55
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.13
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15074
	Voluntary context switches: 2714
	Involuntary context switches: 62
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
