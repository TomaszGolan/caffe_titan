2806031
I0520 23:18:00.359302 24041 caffe.cpp:184] Using GPUs 0
I0520 23:18:00.783221 24041 solver.cpp:48] Initializing solver from parameters: 
test_iter: 357
test_interval: 714
base_lr: 0.0025
display: 35
max_iter: 3571
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 357
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254.prototxt"
I0520 23:18:00.784893 24041 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254.prototxt
I0520 23:18:00.796707 24041 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 23:18:00.796773 24041 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 23:18:00.797150 24041 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 420
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:18:00.797351 24041 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:18:00.797380 24041 net.cpp:106] Creating Layer data_hdf5
I0520 23:18:00.797397 24041 net.cpp:411] data_hdf5 -> data
I0520 23:18:00.797438 24041 net.cpp:411] data_hdf5 -> label
I0520 23:18:00.797482 24041 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 23:18:00.798707 24041 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 23:18:00.800954 24041 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 23:18:22.398586 24041 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 23:18:22.403789 24041 net.cpp:150] Setting up data_hdf5
I0520 23:18:22.403830 24041 net.cpp:157] Top shape: 420 1 127 50 (2667000)
I0520 23:18:22.403847 24041 net.cpp:157] Top shape: 420 (420)
I0520 23:18:22.403861 24041 net.cpp:165] Memory required for data: 10669680
I0520 23:18:22.403879 24041 layer_factory.hpp:77] Creating layer conv1
I0520 23:18:22.403925 24041 net.cpp:106] Creating Layer conv1
I0520 23:18:22.403940 24041 net.cpp:454] conv1 <- data
I0520 23:18:22.403965 24041 net.cpp:411] conv1 -> conv1
I0520 23:18:22.775228 24041 net.cpp:150] Setting up conv1
I0520 23:18:22.775280 24041 net.cpp:157] Top shape: 420 12 120 48 (29030400)
I0520 23:18:22.775297 24041 net.cpp:165] Memory required for data: 126791280
I0520 23:18:22.775327 24041 layer_factory.hpp:77] Creating layer relu1
I0520 23:18:22.775357 24041 net.cpp:106] Creating Layer relu1
I0520 23:18:22.775390 24041 net.cpp:454] relu1 <- conv1
I0520 23:18:22.775406 24041 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:18:22.775935 24041 net.cpp:150] Setting up relu1
I0520 23:18:22.775959 24041 net.cpp:157] Top shape: 420 12 120 48 (29030400)
I0520 23:18:22.775972 24041 net.cpp:165] Memory required for data: 242912880
I0520 23:18:22.775985 24041 layer_factory.hpp:77] Creating layer pool1
I0520 23:18:22.776007 24041 net.cpp:106] Creating Layer pool1
I0520 23:18:22.776022 24041 net.cpp:454] pool1 <- conv1
I0520 23:18:22.776046 24041 net.cpp:411] pool1 -> pool1
I0520 23:18:22.776132 24041 net.cpp:150] Setting up pool1
I0520 23:18:22.776159 24041 net.cpp:157] Top shape: 420 12 60 48 (14515200)
I0520 23:18:22.776173 24041 net.cpp:165] Memory required for data: 300973680
I0520 23:18:22.776195 24041 layer_factory.hpp:77] Creating layer conv2
I0520 23:18:22.776218 24041 net.cpp:106] Creating Layer conv2
I0520 23:18:22.776232 24041 net.cpp:454] conv2 <- pool1
I0520 23:18:22.776248 24041 net.cpp:411] conv2 -> conv2
I0520 23:18:22.778944 24041 net.cpp:150] Setting up conv2
I0520 23:18:22.778973 24041 net.cpp:157] Top shape: 420 20 54 46 (20865600)
I0520 23:18:22.778990 24041 net.cpp:165] Memory required for data: 384436080
I0520 23:18:22.779016 24041 layer_factory.hpp:77] Creating layer relu2
I0520 23:18:22.779043 24041 net.cpp:106] Creating Layer relu2
I0520 23:18:22.779057 24041 net.cpp:454] relu2 <- conv2
I0520 23:18:22.779073 24041 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:18:22.779428 24041 net.cpp:150] Setting up relu2
I0520 23:18:22.779448 24041 net.cpp:157] Top shape: 420 20 54 46 (20865600)
I0520 23:18:22.779461 24041 net.cpp:165] Memory required for data: 467898480
I0520 23:18:22.779476 24041 layer_factory.hpp:77] Creating layer pool2
I0520 23:18:22.779498 24041 net.cpp:106] Creating Layer pool2
I0520 23:18:22.779512 24041 net.cpp:454] pool2 <- conv2
I0520 23:18:22.779547 24041 net.cpp:411] pool2 -> pool2
I0520 23:18:22.779631 24041 net.cpp:150] Setting up pool2
I0520 23:18:22.779649 24041 net.cpp:157] Top shape: 420 20 27 46 (10432800)
I0520 23:18:22.779664 24041 net.cpp:165] Memory required for data: 509629680
I0520 23:18:22.779681 24041 layer_factory.hpp:77] Creating layer conv3
I0520 23:18:22.779702 24041 net.cpp:106] Creating Layer conv3
I0520 23:18:22.779716 24041 net.cpp:454] conv3 <- pool2
I0520 23:18:22.779732 24041 net.cpp:411] conv3 -> conv3
I0520 23:18:22.781682 24041 net.cpp:150] Setting up conv3
I0520 23:18:22.781707 24041 net.cpp:157] Top shape: 420 28 22 44 (11383680)
I0520 23:18:22.781726 24041 net.cpp:165] Memory required for data: 555164400
I0520 23:18:22.781749 24041 layer_factory.hpp:77] Creating layer relu3
I0520 23:18:22.781771 24041 net.cpp:106] Creating Layer relu3
I0520 23:18:22.781795 24041 net.cpp:454] relu3 <- conv3
I0520 23:18:22.781811 24041 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:18:22.782307 24041 net.cpp:150] Setting up relu3
I0520 23:18:22.782331 24041 net.cpp:157] Top shape: 420 28 22 44 (11383680)
I0520 23:18:22.782344 24041 net.cpp:165] Memory required for data: 600699120
I0520 23:18:22.782357 24041 layer_factory.hpp:77] Creating layer pool3
I0520 23:18:22.782377 24041 net.cpp:106] Creating Layer pool3
I0520 23:18:22.782398 24041 net.cpp:454] pool3 <- conv3
I0520 23:18:22.782414 24041 net.cpp:411] pool3 -> pool3
I0520 23:18:22.782497 24041 net.cpp:150] Setting up pool3
I0520 23:18:22.782519 24041 net.cpp:157] Top shape: 420 28 11 44 (5691840)
I0520 23:18:22.782532 24041 net.cpp:165] Memory required for data: 623466480
I0520 23:18:22.782547 24041 layer_factory.hpp:77] Creating layer conv4
I0520 23:18:22.782574 24041 net.cpp:106] Creating Layer conv4
I0520 23:18:22.782588 24041 net.cpp:454] conv4 <- pool3
I0520 23:18:22.782603 24041 net.cpp:411] conv4 -> conv4
I0520 23:18:22.785388 24041 net.cpp:150] Setting up conv4
I0520 23:18:22.785423 24041 net.cpp:157] Top shape: 420 36 6 42 (3810240)
I0520 23:18:22.785437 24041 net.cpp:165] Memory required for data: 638707440
I0520 23:18:22.785461 24041 layer_factory.hpp:77] Creating layer relu4
I0520 23:18:22.785478 24041 net.cpp:106] Creating Layer relu4
I0520 23:18:22.785492 24041 net.cpp:454] relu4 <- conv4
I0520 23:18:22.785507 24041 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:18:22.786031 24041 net.cpp:150] Setting up relu4
I0520 23:18:22.786054 24041 net.cpp:157] Top shape: 420 36 6 42 (3810240)
I0520 23:18:22.786067 24041 net.cpp:165] Memory required for data: 653948400
I0520 23:18:22.786083 24041 layer_factory.hpp:77] Creating layer pool4
I0520 23:18:22.786099 24041 net.cpp:106] Creating Layer pool4
I0520 23:18:22.786121 24041 net.cpp:454] pool4 <- conv4
I0520 23:18:22.786137 24041 net.cpp:411] pool4 -> pool4
I0520 23:18:22.786219 24041 net.cpp:150] Setting up pool4
I0520 23:18:22.786237 24041 net.cpp:157] Top shape: 420 36 3 42 (1905120)
I0520 23:18:22.786252 24041 net.cpp:165] Memory required for data: 661568880
I0520 23:18:22.786267 24041 layer_factory.hpp:77] Creating layer ip1
I0520 23:18:22.786294 24041 net.cpp:106] Creating Layer ip1
I0520 23:18:22.786308 24041 net.cpp:454] ip1 <- pool4
I0520 23:18:22.786330 24041 net.cpp:411] ip1 -> ip1
I0520 23:18:22.801746 24041 net.cpp:150] Setting up ip1
I0520 23:18:22.801779 24041 net.cpp:157] Top shape: 420 196 (82320)
I0520 23:18:22.801801 24041 net.cpp:165] Memory required for data: 661898160
I0520 23:18:22.801827 24041 layer_factory.hpp:77] Creating layer relu5
I0520 23:18:22.801849 24041 net.cpp:106] Creating Layer relu5
I0520 23:18:22.801874 24041 net.cpp:454] relu5 <- ip1
I0520 23:18:22.801892 24041 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:18:22.802253 24041 net.cpp:150] Setting up relu5
I0520 23:18:22.802273 24041 net.cpp:157] Top shape: 420 196 (82320)
I0520 23:18:22.802286 24041 net.cpp:165] Memory required for data: 662227440
I0520 23:18:22.802301 24041 layer_factory.hpp:77] Creating layer drop1
I0520 23:18:22.802331 24041 net.cpp:106] Creating Layer drop1
I0520 23:18:22.802345 24041 net.cpp:454] drop1 <- ip1
I0520 23:18:22.802374 24041 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:18:22.802433 24041 net.cpp:150] Setting up drop1
I0520 23:18:22.802450 24041 net.cpp:157] Top shape: 420 196 (82320)
I0520 23:18:22.802464 24041 net.cpp:165] Memory required for data: 662556720
I0520 23:18:22.802479 24041 layer_factory.hpp:77] Creating layer ip2
I0520 23:18:22.802500 24041 net.cpp:106] Creating Layer ip2
I0520 23:18:22.802518 24041 net.cpp:454] ip2 <- ip1
I0520 23:18:22.802534 24041 net.cpp:411] ip2 -> ip2
I0520 23:18:22.803021 24041 net.cpp:150] Setting up ip2
I0520 23:18:22.803040 24041 net.cpp:157] Top shape: 420 98 (41160)
I0520 23:18:22.803053 24041 net.cpp:165] Memory required for data: 662721360
I0520 23:18:22.803074 24041 layer_factory.hpp:77] Creating layer relu6
I0520 23:18:22.803097 24041 net.cpp:106] Creating Layer relu6
I0520 23:18:22.803110 24041 net.cpp:454] relu6 <- ip2
I0520 23:18:22.803125 24041 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:18:22.803676 24041 net.cpp:150] Setting up relu6
I0520 23:18:22.803699 24041 net.cpp:157] Top shape: 420 98 (41160)
I0520 23:18:22.803714 24041 net.cpp:165] Memory required for data: 662886000
I0520 23:18:22.803730 24041 layer_factory.hpp:77] Creating layer drop2
I0520 23:18:22.803753 24041 net.cpp:106] Creating Layer drop2
I0520 23:18:22.803767 24041 net.cpp:454] drop2 <- ip2
I0520 23:18:22.803782 24041 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:18:22.803833 24041 net.cpp:150] Setting up drop2
I0520 23:18:22.803855 24041 net.cpp:157] Top shape: 420 98 (41160)
I0520 23:18:22.803869 24041 net.cpp:165] Memory required for data: 663050640
I0520 23:18:22.803882 24041 layer_factory.hpp:77] Creating layer ip3
I0520 23:18:22.803897 24041 net.cpp:106] Creating Layer ip3
I0520 23:18:22.803912 24041 net.cpp:454] ip3 <- ip2
I0520 23:18:22.803935 24041 net.cpp:411] ip3 -> ip3
I0520 23:18:22.804159 24041 net.cpp:150] Setting up ip3
I0520 23:18:22.804178 24041 net.cpp:157] Top shape: 420 11 (4620)
I0520 23:18:22.804191 24041 net.cpp:165] Memory required for data: 663069120
I0520 23:18:22.804213 24041 layer_factory.hpp:77] Creating layer drop3
I0520 23:18:22.804234 24041 net.cpp:106] Creating Layer drop3
I0520 23:18:22.804247 24041 net.cpp:454] drop3 <- ip3
I0520 23:18:22.804262 24041 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:18:22.804309 24041 net.cpp:150] Setting up drop3
I0520 23:18:22.804332 24041 net.cpp:157] Top shape: 420 11 (4620)
I0520 23:18:22.804344 24041 net.cpp:165] Memory required for data: 663087600
I0520 23:18:22.804363 24041 layer_factory.hpp:77] Creating layer loss
I0520 23:18:22.804384 24041 net.cpp:106] Creating Layer loss
I0520 23:18:22.804399 24041 net.cpp:454] loss <- ip3
I0520 23:18:22.804414 24041 net.cpp:454] loss <- label
I0520 23:18:22.804435 24041 net.cpp:411] loss -> loss
I0520 23:18:22.804455 24041 layer_factory.hpp:77] Creating layer loss
I0520 23:18:22.805124 24041 net.cpp:150] Setting up loss
I0520 23:18:22.805146 24041 net.cpp:157] Top shape: (1)
I0520 23:18:22.805167 24041 net.cpp:160]     with loss weight 1
I0520 23:18:22.805219 24041 net.cpp:165] Memory required for data: 663087604
I0520 23:18:22.805243 24041 net.cpp:226] loss needs backward computation.
I0520 23:18:22.805256 24041 net.cpp:226] drop3 needs backward computation.
I0520 23:18:22.805269 24041 net.cpp:226] ip3 needs backward computation.
I0520 23:18:22.805282 24041 net.cpp:226] drop2 needs backward computation.
I0520 23:18:22.805294 24041 net.cpp:226] relu6 needs backward computation.
I0520 23:18:22.805310 24041 net.cpp:226] ip2 needs backward computation.
I0520 23:18:22.805322 24041 net.cpp:226] drop1 needs backward computation.
I0520 23:18:22.805341 24041 net.cpp:226] relu5 needs backward computation.
I0520 23:18:22.805354 24041 net.cpp:226] ip1 needs backward computation.
I0520 23:18:22.805371 24041 net.cpp:226] pool4 needs backward computation.
I0520 23:18:22.805384 24041 net.cpp:226] relu4 needs backward computation.
I0520 23:18:22.805397 24041 net.cpp:226] conv4 needs backward computation.
I0520 23:18:22.805413 24041 net.cpp:226] pool3 needs backward computation.
I0520 23:18:22.805441 24041 net.cpp:226] relu3 needs backward computation.
I0520 23:18:22.805454 24041 net.cpp:226] conv3 needs backward computation.
I0520 23:18:22.805469 24041 net.cpp:226] pool2 needs backward computation.
I0520 23:18:22.805482 24041 net.cpp:226] relu2 needs backward computation.
I0520 23:18:22.805495 24041 net.cpp:226] conv2 needs backward computation.
I0520 23:18:22.805510 24041 net.cpp:226] pool1 needs backward computation.
I0520 23:18:22.805524 24041 net.cpp:226] relu1 needs backward computation.
I0520 23:18:22.805543 24041 net.cpp:226] conv1 needs backward computation.
I0520 23:18:22.805557 24041 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:18:22.805573 24041 net.cpp:270] This network produces output loss
I0520 23:18:22.805601 24041 net.cpp:283] Network initialization done.
I0520 23:18:22.807250 24041 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254.prototxt
I0520 23:18:22.807328 24041 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 23:18:22.807706 24041 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 420
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:18:22.807934 24041 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:18:22.807953 24041 net.cpp:106] Creating Layer data_hdf5
I0520 23:18:22.807973 24041 net.cpp:411] data_hdf5 -> data
I0520 23:18:22.807993 24041 net.cpp:411] data_hdf5 -> label
I0520 23:18:22.808013 24041 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 23:18:22.809864 24041 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 23:18:44.096518 24041 net.cpp:150] Setting up data_hdf5
I0520 23:18:44.096688 24041 net.cpp:157] Top shape: 420 1 127 50 (2667000)
I0520 23:18:44.096709 24041 net.cpp:157] Top shape: 420 (420)
I0520 23:18:44.096720 24041 net.cpp:165] Memory required for data: 10669680
I0520 23:18:44.096735 24041 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 23:18:44.096771 24041 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 23:18:44.096783 24041 net.cpp:454] label_data_hdf5_1_split <- label
I0520 23:18:44.096801 24041 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 23:18:44.096842 24041 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 23:18:44.096928 24041 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 23:18:44.096946 24041 net.cpp:157] Top shape: 420 (420)
I0520 23:18:44.096961 24041 net.cpp:157] Top shape: 420 (420)
I0520 23:18:44.096974 24041 net.cpp:165] Memory required for data: 10673040
I0520 23:18:44.096987 24041 layer_factory.hpp:77] Creating layer conv1
I0520 23:18:44.097019 24041 net.cpp:106] Creating Layer conv1
I0520 23:18:44.097033 24041 net.cpp:454] conv1 <- data
I0520 23:18:44.097049 24041 net.cpp:411] conv1 -> conv1
I0520 23:18:44.099037 24041 net.cpp:150] Setting up conv1
I0520 23:18:44.099063 24041 net.cpp:157] Top shape: 420 12 120 48 (29030400)
I0520 23:18:44.099084 24041 net.cpp:165] Memory required for data: 126794640
I0520 23:18:44.099108 24041 layer_factory.hpp:77] Creating layer relu1
I0520 23:18:44.099130 24041 net.cpp:106] Creating Layer relu1
I0520 23:18:44.099153 24041 net.cpp:454] relu1 <- conv1
I0520 23:18:44.099169 24041 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:18:44.099685 24041 net.cpp:150] Setting up relu1
I0520 23:18:44.099709 24041 net.cpp:157] Top shape: 420 12 120 48 (29030400)
I0520 23:18:44.099722 24041 net.cpp:165] Memory required for data: 242916240
I0520 23:18:44.099735 24041 layer_factory.hpp:77] Creating layer pool1
I0520 23:18:44.099766 24041 net.cpp:106] Creating Layer pool1
I0520 23:18:44.099779 24041 net.cpp:454] pool1 <- conv1
I0520 23:18:44.099795 24041 net.cpp:411] pool1 -> pool1
I0520 23:18:44.099884 24041 net.cpp:150] Setting up pool1
I0520 23:18:44.099901 24041 net.cpp:157] Top shape: 420 12 60 48 (14515200)
I0520 23:18:44.099916 24041 net.cpp:165] Memory required for data: 300977040
I0520 23:18:44.099934 24041 layer_factory.hpp:77] Creating layer conv2
I0520 23:18:44.099956 24041 net.cpp:106] Creating Layer conv2
I0520 23:18:44.099968 24041 net.cpp:454] conv2 <- pool1
I0520 23:18:44.099985 24041 net.cpp:411] conv2 -> conv2
I0520 23:18:44.101935 24041 net.cpp:150] Setting up conv2
I0520 23:18:44.101960 24041 net.cpp:157] Top shape: 420 20 54 46 (20865600)
I0520 23:18:44.101990 24041 net.cpp:165] Memory required for data: 384439440
I0520 23:18:44.102022 24041 layer_factory.hpp:77] Creating layer relu2
I0520 23:18:44.102040 24041 net.cpp:106] Creating Layer relu2
I0520 23:18:44.102062 24041 net.cpp:454] relu2 <- conv2
I0520 23:18:44.102079 24041 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:18:44.102440 24041 net.cpp:150] Setting up relu2
I0520 23:18:44.102460 24041 net.cpp:157] Top shape: 420 20 54 46 (20865600)
I0520 23:18:44.102473 24041 net.cpp:165] Memory required for data: 467901840
I0520 23:18:44.102488 24041 layer_factory.hpp:77] Creating layer pool2
I0520 23:18:44.102510 24041 net.cpp:106] Creating Layer pool2
I0520 23:18:44.102524 24041 net.cpp:454] pool2 <- conv2
I0520 23:18:44.102540 24041 net.cpp:411] pool2 -> pool2
I0520 23:18:44.102629 24041 net.cpp:150] Setting up pool2
I0520 23:18:44.102648 24041 net.cpp:157] Top shape: 420 20 27 46 (10432800)
I0520 23:18:44.102666 24041 net.cpp:165] Memory required for data: 509633040
I0520 23:18:44.102679 24041 layer_factory.hpp:77] Creating layer conv3
I0520 23:18:44.102705 24041 net.cpp:106] Creating Layer conv3
I0520 23:18:44.102725 24041 net.cpp:454] conv3 <- pool2
I0520 23:18:44.102741 24041 net.cpp:411] conv3 -> conv3
I0520 23:18:44.104732 24041 net.cpp:150] Setting up conv3
I0520 23:18:44.104758 24041 net.cpp:157] Top shape: 420 28 22 44 (11383680)
I0520 23:18:44.104778 24041 net.cpp:165] Memory required for data: 555167760
I0520 23:18:44.104816 24041 layer_factory.hpp:77] Creating layer relu3
I0520 23:18:44.104841 24041 net.cpp:106] Creating Layer relu3
I0520 23:18:44.104856 24041 net.cpp:454] relu3 <- conv3
I0520 23:18:44.104871 24041 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:18:44.105370 24041 net.cpp:150] Setting up relu3
I0520 23:18:44.105393 24041 net.cpp:157] Top shape: 420 28 22 44 (11383680)
I0520 23:18:44.105406 24041 net.cpp:165] Memory required for data: 600702480
I0520 23:18:44.105422 24041 layer_factory.hpp:77] Creating layer pool3
I0520 23:18:44.105437 24041 net.cpp:106] Creating Layer pool3
I0520 23:18:44.105458 24041 net.cpp:454] pool3 <- conv3
I0520 23:18:44.105474 24041 net.cpp:411] pool3 -> pool3
I0520 23:18:44.105561 24041 net.cpp:150] Setting up pool3
I0520 23:18:44.105581 24041 net.cpp:157] Top shape: 420 28 11 44 (5691840)
I0520 23:18:44.105592 24041 net.cpp:165] Memory required for data: 623469840
I0520 23:18:44.105607 24041 layer_factory.hpp:77] Creating layer conv4
I0520 23:18:44.105633 24041 net.cpp:106] Creating Layer conv4
I0520 23:18:44.105648 24041 net.cpp:454] conv4 <- pool3
I0520 23:18:44.105665 24041 net.cpp:411] conv4 -> conv4
I0520 23:18:44.107790 24041 net.cpp:150] Setting up conv4
I0520 23:18:44.107815 24041 net.cpp:157] Top shape: 420 36 6 42 (3810240)
I0520 23:18:44.107836 24041 net.cpp:165] Memory required for data: 638710800
I0520 23:18:44.107854 24041 layer_factory.hpp:77] Creating layer relu4
I0520 23:18:44.107875 24041 net.cpp:106] Creating Layer relu4
I0520 23:18:44.107887 24041 net.cpp:454] relu4 <- conv4
I0520 23:18:44.107913 24041 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:18:44.108402 24041 net.cpp:150] Setting up relu4
I0520 23:18:44.108425 24041 net.cpp:157] Top shape: 420 36 6 42 (3810240)
I0520 23:18:44.108438 24041 net.cpp:165] Memory required for data: 653951760
I0520 23:18:44.108454 24041 layer_factory.hpp:77] Creating layer pool4
I0520 23:18:44.108469 24041 net.cpp:106] Creating Layer pool4
I0520 23:18:44.108484 24041 net.cpp:454] pool4 <- conv4
I0520 23:18:44.108500 24041 net.cpp:411] pool4 -> pool4
I0520 23:18:44.108594 24041 net.cpp:150] Setting up pool4
I0520 23:18:44.108611 24041 net.cpp:157] Top shape: 420 36 3 42 (1905120)
I0520 23:18:44.108626 24041 net.cpp:165] Memory required for data: 661572240
I0520 23:18:44.108639 24041 layer_factory.hpp:77] Creating layer ip1
I0520 23:18:44.108664 24041 net.cpp:106] Creating Layer ip1
I0520 23:18:44.108676 24041 net.cpp:454] ip1 <- pool4
I0520 23:18:44.108700 24041 net.cpp:411] ip1 -> ip1
I0520 23:18:44.124135 24041 net.cpp:150] Setting up ip1
I0520 23:18:44.124167 24041 net.cpp:157] Top shape: 420 196 (82320)
I0520 23:18:44.124187 24041 net.cpp:165] Memory required for data: 661901520
I0520 23:18:44.124213 24041 layer_factory.hpp:77] Creating layer relu5
I0520 23:18:44.124234 24041 net.cpp:106] Creating Layer relu5
I0520 23:18:44.124259 24041 net.cpp:454] relu5 <- ip1
I0520 23:18:44.124276 24041 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:18:44.124639 24041 net.cpp:150] Setting up relu5
I0520 23:18:44.124660 24041 net.cpp:157] Top shape: 420 196 (82320)
I0520 23:18:44.124672 24041 net.cpp:165] Memory required for data: 662230800
I0520 23:18:44.124687 24041 layer_factory.hpp:77] Creating layer drop1
I0520 23:18:44.124716 24041 net.cpp:106] Creating Layer drop1
I0520 23:18:44.124732 24041 net.cpp:454] drop1 <- ip1
I0520 23:18:44.124748 24041 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:18:44.124809 24041 net.cpp:150] Setting up drop1
I0520 23:18:44.124825 24041 net.cpp:157] Top shape: 420 196 (82320)
I0520 23:18:44.124843 24041 net.cpp:165] Memory required for data: 662560080
I0520 23:18:44.124856 24041 layer_factory.hpp:77] Creating layer ip2
I0520 23:18:44.124877 24041 net.cpp:106] Creating Layer ip2
I0520 23:18:44.124889 24041 net.cpp:454] ip2 <- ip1
I0520 23:18:44.124912 24041 net.cpp:411] ip2 -> ip2
I0520 23:18:44.125408 24041 net.cpp:150] Setting up ip2
I0520 23:18:44.125427 24041 net.cpp:157] Top shape: 420 98 (41160)
I0520 23:18:44.125439 24041 net.cpp:165] Memory required for data: 662724720
I0520 23:18:44.125473 24041 layer_factory.hpp:77] Creating layer relu6
I0520 23:18:44.125496 24041 net.cpp:106] Creating Layer relu6
I0520 23:18:44.125509 24041 net.cpp:454] relu6 <- ip2
I0520 23:18:44.125525 24041 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:18:44.126094 24041 net.cpp:150] Setting up relu6
I0520 23:18:44.126117 24041 net.cpp:157] Top shape: 420 98 (41160)
I0520 23:18:44.126130 24041 net.cpp:165] Memory required for data: 662889360
I0520 23:18:44.126142 24041 layer_factory.hpp:77] Creating layer drop2
I0520 23:18:44.126163 24041 net.cpp:106] Creating Layer drop2
I0520 23:18:44.126184 24041 net.cpp:454] drop2 <- ip2
I0520 23:18:44.126201 24041 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:18:44.126260 24041 net.cpp:150] Setting up drop2
I0520 23:18:44.126276 24041 net.cpp:157] Top shape: 420 98 (41160)
I0520 23:18:44.126289 24041 net.cpp:165] Memory required for data: 663054000
I0520 23:18:44.126317 24041 layer_factory.hpp:77] Creating layer ip3
I0520 23:18:44.126335 24041 net.cpp:106] Creating Layer ip3
I0520 23:18:44.126348 24041 net.cpp:454] ip3 <- ip2
I0520 23:18:44.126368 24041 net.cpp:411] ip3 -> ip3
I0520 23:18:44.126621 24041 net.cpp:150] Setting up ip3
I0520 23:18:44.126639 24041 net.cpp:157] Top shape: 420 11 (4620)
I0520 23:18:44.126652 24041 net.cpp:165] Memory required for data: 663072480
I0520 23:18:44.126669 24041 layer_factory.hpp:77] Creating layer drop3
I0520 23:18:44.126688 24041 net.cpp:106] Creating Layer drop3
I0520 23:18:44.126708 24041 net.cpp:454] drop3 <- ip3
I0520 23:18:44.126724 24041 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:18:44.126777 24041 net.cpp:150] Setting up drop3
I0520 23:18:44.126792 24041 net.cpp:157] Top shape: 420 11 (4620)
I0520 23:18:44.126811 24041 net.cpp:165] Memory required for data: 663090960
I0520 23:18:44.126824 24041 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 23:18:44.126843 24041 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 23:18:44.126857 24041 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 23:18:44.126871 24041 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 23:18:44.126893 24041 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 23:18:44.126979 24041 net.cpp:150] Setting up ip3_drop3_0_split
I0520 23:18:44.127002 24041 net.cpp:157] Top shape: 420 11 (4620)
I0520 23:18:44.127017 24041 net.cpp:157] Top shape: 420 11 (4620)
I0520 23:18:44.127030 24041 net.cpp:165] Memory required for data: 663127920
I0520 23:18:44.127043 24041 layer_factory.hpp:77] Creating layer accuracy
I0520 23:18:44.127066 24041 net.cpp:106] Creating Layer accuracy
I0520 23:18:44.127086 24041 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 23:18:44.127100 24041 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 23:18:44.127117 24041 net.cpp:411] accuracy -> accuracy
I0520 23:18:44.127146 24041 net.cpp:150] Setting up accuracy
I0520 23:18:44.127166 24041 net.cpp:157] Top shape: (1)
I0520 23:18:44.127179 24041 net.cpp:165] Memory required for data: 663127924
I0520 23:18:44.127192 24041 layer_factory.hpp:77] Creating layer loss
I0520 23:18:44.127209 24041 net.cpp:106] Creating Layer loss
I0520 23:18:44.127224 24041 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 23:18:44.127238 24041 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 23:18:44.127254 24041 net.cpp:411] loss -> loss
I0520 23:18:44.127282 24041 layer_factory.hpp:77] Creating layer loss
I0520 23:18:44.127802 24041 net.cpp:150] Setting up loss
I0520 23:18:44.127823 24041 net.cpp:157] Top shape: (1)
I0520 23:18:44.127835 24041 net.cpp:160]     with loss weight 1
I0520 23:18:44.127862 24041 net.cpp:165] Memory required for data: 663127928
I0520 23:18:44.127882 24041 net.cpp:226] loss needs backward computation.
I0520 23:18:44.127898 24041 net.cpp:228] accuracy does not need backward computation.
I0520 23:18:44.127915 24041 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 23:18:44.127928 24041 net.cpp:226] drop3 needs backward computation.
I0520 23:18:44.127940 24041 net.cpp:226] ip3 needs backward computation.
I0520 23:18:44.127956 24041 net.cpp:226] drop2 needs backward computation.
I0520 23:18:44.127985 24041 net.cpp:226] relu6 needs backward computation.
I0520 23:18:44.127997 24041 net.cpp:226] ip2 needs backward computation.
I0520 23:18:44.128013 24041 net.cpp:226] drop1 needs backward computation.
I0520 23:18:44.128026 24041 net.cpp:226] relu5 needs backward computation.
I0520 23:18:44.128037 24041 net.cpp:226] ip1 needs backward computation.
I0520 23:18:44.128052 24041 net.cpp:226] pool4 needs backward computation.
I0520 23:18:44.128064 24041 net.cpp:226] relu4 needs backward computation.
I0520 23:18:44.128084 24041 net.cpp:226] conv4 needs backward computation.
I0520 23:18:44.128098 24041 net.cpp:226] pool3 needs backward computation.
I0520 23:18:44.128114 24041 net.cpp:226] relu3 needs backward computation.
I0520 23:18:44.128128 24041 net.cpp:226] conv3 needs backward computation.
I0520 23:18:44.128139 24041 net.cpp:226] pool2 needs backward computation.
I0520 23:18:44.128155 24041 net.cpp:226] relu2 needs backward computation.
I0520 23:18:44.128167 24041 net.cpp:226] conv2 needs backward computation.
I0520 23:18:44.128187 24041 net.cpp:226] pool1 needs backward computation.
I0520 23:18:44.128201 24041 net.cpp:226] relu1 needs backward computation.
I0520 23:18:44.128216 24041 net.cpp:226] conv1 needs backward computation.
I0520 23:18:44.128232 24041 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 23:18:44.128245 24041 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:18:44.128257 24041 net.cpp:270] This network produces output accuracy
I0520 23:18:44.128273 24041 net.cpp:270] This network produces output loss
I0520 23:18:44.128304 24041 net.cpp:283] Network initialization done.
I0520 23:18:44.128440 24041 solver.cpp:60] Solver scaffolding done.
I0520 23:18:44.129590 24041 caffe.cpp:212] Starting Optimization
I0520 23:18:44.129606 24041 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 23:18:44.129621 24041 solver.cpp:289] Learning Rate Policy: fixed
I0520 23:18:44.130872 24041 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 23:19:30.767432 24041 solver.cpp:409]     Test net output #0: accuracy = 0.129972
I0520 23:19:30.767598 24041 solver.cpp:409]     Test net output #1: loss = 2.39723 (* 1 = 2.39723 loss)
I0520 23:19:30.853359 24041 solver.cpp:237] Iteration 0, loss = 2.39674
I0520 23:19:30.853399 24041 solver.cpp:253]     Train net output #0: loss = 2.39674 (* 1 = 2.39674 loss)
I0520 23:19:30.853421 24041 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 23:19:38.812157 24041 solver.cpp:237] Iteration 35, loss = 2.36939
I0520 23:19:38.812196 24041 solver.cpp:253]     Train net output #0: loss = 2.36939 (* 1 = 2.36939 loss)
I0520 23:19:38.812213 24041 sgd_solver.cpp:106] Iteration 35, lr = 0.0025
I0520 23:19:46.769145 24041 solver.cpp:237] Iteration 70, loss = 2.36305
I0520 23:19:46.769196 24041 solver.cpp:253]     Train net output #0: loss = 2.36305 (* 1 = 2.36305 loss)
I0520 23:19:46.769213 24041 sgd_solver.cpp:106] Iteration 70, lr = 0.0025
I0520 23:19:54.727715 24041 solver.cpp:237] Iteration 105, loss = 2.31922
I0520 23:19:54.727751 24041 solver.cpp:253]     Train net output #0: loss = 2.31922 (* 1 = 2.31922 loss)
I0520 23:19:54.727773 24041 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0520 23:20:02.684351 24041 solver.cpp:237] Iteration 140, loss = 2.31488
I0520 23:20:02.684497 24041 solver.cpp:253]     Train net output #0: loss = 2.31488 (* 1 = 2.31488 loss)
I0520 23:20:02.684514 24041 sgd_solver.cpp:106] Iteration 140, lr = 0.0025
I0520 23:20:10.641535 24041 solver.cpp:237] Iteration 175, loss = 2.32723
I0520 23:20:10.641584 24041 solver.cpp:253]     Train net output #0: loss = 2.32723 (* 1 = 2.32723 loss)
I0520 23:20:10.641602 24041 sgd_solver.cpp:106] Iteration 175, lr = 0.0025
I0520 23:20:18.598834 24041 solver.cpp:237] Iteration 210, loss = 2.29573
I0520 23:20:18.598868 24041 solver.cpp:253]     Train net output #0: loss = 2.29573 (* 1 = 2.29573 loss)
I0520 23:20:18.598887 24041 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0520 23:20:48.677742 24041 solver.cpp:237] Iteration 245, loss = 2.30485
I0520 23:20:48.677911 24041 solver.cpp:253]     Train net output #0: loss = 2.30485 (* 1 = 2.30485 loss)
I0520 23:20:48.677928 24041 sgd_solver.cpp:106] Iteration 245, lr = 0.0025
I0520 23:20:56.643095 24041 solver.cpp:237] Iteration 280, loss = 2.31761
I0520 23:20:56.643131 24041 solver.cpp:253]     Train net output #0: loss = 2.31761 (* 1 = 2.31761 loss)
I0520 23:20:56.643153 24041 sgd_solver.cpp:106] Iteration 280, lr = 0.0025
I0520 23:21:04.603516 24041 solver.cpp:237] Iteration 315, loss = 2.31371
I0520 23:21:04.603570 24041 solver.cpp:253]     Train net output #0: loss = 2.31371 (* 1 = 2.31371 loss)
I0520 23:21:04.603595 24041 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0520 23:21:12.569399 24041 solver.cpp:237] Iteration 350, loss = 2.24172
I0520 23:21:12.569433 24041 solver.cpp:253]     Train net output #0: loss = 2.24172 (* 1 = 2.24172 loss)
I0520 23:21:12.569456 24041 sgd_solver.cpp:106] Iteration 350, lr = 0.0025
I0520 23:21:13.935333 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_357.caffemodel
I0520 23:21:14.135937 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_357.solverstate
I0520 23:21:20.599889 24041 solver.cpp:237] Iteration 385, loss = 2.21285
I0520 23:21:20.600050 24041 solver.cpp:253]     Train net output #0: loss = 2.21285 (* 1 = 2.21285 loss)
I0520 23:21:20.600069 24041 sgd_solver.cpp:106] Iteration 385, lr = 0.0025
I0520 23:21:28.560637 24041 solver.cpp:237] Iteration 420, loss = 2.17932
I0520 23:21:28.560694 24041 solver.cpp:253]     Train net output #0: loss = 2.17932 (* 1 = 2.17932 loss)
I0520 23:21:28.560719 24041 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0520 23:21:36.519510 24041 solver.cpp:237] Iteration 455, loss = 2.13313
I0520 23:21:36.519544 24041 solver.cpp:253]     Train net output #0: loss = 2.13313 (* 1 = 2.13313 loss)
I0520 23:21:36.519568 24041 sgd_solver.cpp:106] Iteration 455, lr = 0.0025
I0520 23:22:06.585608 24041 solver.cpp:237] Iteration 490, loss = 2.04476
I0520 23:22:06.585769 24041 solver.cpp:253]     Train net output #0: loss = 2.04476 (* 1 = 2.04476 loss)
I0520 23:22:06.585786 24041 sgd_solver.cpp:106] Iteration 490, lr = 0.0025
I0520 23:22:14.549345 24041 solver.cpp:237] Iteration 525, loss = 2.04043
I0520 23:22:14.549391 24041 solver.cpp:253]     Train net output #0: loss = 2.04043 (* 1 = 2.04043 loss)
I0520 23:22:14.549408 24041 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0520 23:22:22.513708 24041 solver.cpp:237] Iteration 560, loss = 1.97761
I0520 23:22:22.513743 24041 solver.cpp:253]     Train net output #0: loss = 1.97761 (* 1 = 1.97761 loss)
I0520 23:22:22.513767 24041 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0520 23:22:30.474783 24041 solver.cpp:237] Iteration 595, loss = 2.02483
I0520 23:22:30.474817 24041 solver.cpp:253]     Train net output #0: loss = 2.02483 (* 1 = 2.02483 loss)
I0520 23:22:30.474841 24041 sgd_solver.cpp:106] Iteration 595, lr = 0.0025
I0520 23:22:38.436779 24041 solver.cpp:237] Iteration 630, loss = 1.93362
I0520 23:22:38.436926 24041 solver.cpp:253]     Train net output #0: loss = 1.93362 (* 1 = 1.93362 loss)
I0520 23:22:38.436942 24041 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0520 23:22:46.400575 24041 solver.cpp:237] Iteration 665, loss = 1.9544
I0520 23:22:46.400611 24041 solver.cpp:253]     Train net output #0: loss = 1.9544 (* 1 = 1.9544 loss)
I0520 23:22:46.400627 24041 sgd_solver.cpp:106] Iteration 665, lr = 0.0025
I0520 23:22:54.364272 24041 solver.cpp:237] Iteration 700, loss = 1.90655
I0520 23:22:54.364307 24041 solver.cpp:253]     Train net output #0: loss = 1.90655 (* 1 = 1.90655 loss)
I0520 23:22:54.364331 24041 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0520 23:22:57.322244 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_714.caffemodel
I0520 23:22:57.519611 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_714.solverstate
I0520 23:22:57.544674 24041 solver.cpp:341] Iteration 714, Testing net (#0)
I0520 23:23:42.742656 24041 solver.cpp:409]     Test net output #0: accuracy = 0.536801
I0520 23:23:42.742823 24041 solver.cpp:409]     Test net output #1: loss = 1.65106 (* 1 = 1.65106 loss)
I0520 23:24:09.693348 24041 solver.cpp:237] Iteration 735, loss = 1.88921
I0520 23:24:09.693408 24041 solver.cpp:253]     Train net output #0: loss = 1.88921 (* 1 = 1.88921 loss)
I0520 23:24:09.693434 24041 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0520 23:24:17.649991 24041 solver.cpp:237] Iteration 770, loss = 1.86172
I0520 23:24:17.650140 24041 solver.cpp:253]     Train net output #0: loss = 1.86172 (* 1 = 1.86172 loss)
I0520 23:24:17.650156 24041 sgd_solver.cpp:106] Iteration 770, lr = 0.0025
I0520 23:24:25.606035 24041 solver.cpp:237] Iteration 805, loss = 1.8532
I0520 23:24:25.606084 24041 solver.cpp:253]     Train net output #0: loss = 1.8532 (* 1 = 1.8532 loss)
I0520 23:24:25.606102 24041 sgd_solver.cpp:106] Iteration 805, lr = 0.0025
I0520 23:24:33.568361 24041 solver.cpp:237] Iteration 840, loss = 1.94223
I0520 23:24:33.568395 24041 solver.cpp:253]     Train net output #0: loss = 1.94223 (* 1 = 1.94223 loss)
I0520 23:24:33.568414 24041 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0520 23:24:41.525626 24041 solver.cpp:237] Iteration 875, loss = 1.77057
I0520 23:24:41.525660 24041 solver.cpp:253]     Train net output #0: loss = 1.77057 (* 1 = 1.77057 loss)
I0520 23:24:41.525684 24041 sgd_solver.cpp:106] Iteration 875, lr = 0.0025
I0520 23:24:49.481842 24041 solver.cpp:237] Iteration 910, loss = 1.72039
I0520 23:24:49.482005 24041 solver.cpp:253]     Train net output #0: loss = 1.72039 (* 1 = 1.72039 loss)
I0520 23:24:49.482025 24041 sgd_solver.cpp:106] Iteration 910, lr = 0.0025
I0520 23:24:57.443161 24041 solver.cpp:237] Iteration 945, loss = 1.95978
I0520 23:24:57.443197 24041 solver.cpp:253]     Train net output #0: loss = 1.95978 (* 1 = 1.95978 loss)
I0520 23:24:57.443214 24041 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0520 23:25:27.577050 24041 solver.cpp:237] Iteration 980, loss = 1.90833
I0520 23:25:27.577227 24041 solver.cpp:253]     Train net output #0: loss = 1.90833 (* 1 = 1.90833 loss)
I0520 23:25:27.577245 24041 sgd_solver.cpp:106] Iteration 980, lr = 0.0025
I0520 23:25:35.535159 24041 solver.cpp:237] Iteration 1015, loss = 1.74436
I0520 23:25:35.535212 24041 solver.cpp:253]     Train net output #0: loss = 1.74436 (* 1 = 1.74436 loss)
I0520 23:25:35.535229 24041 sgd_solver.cpp:106] Iteration 1015, lr = 0.0025
I0520 23:25:43.492395 24041 solver.cpp:237] Iteration 1050, loss = 1.83356
I0520 23:25:43.492431 24041 solver.cpp:253]     Train net output #0: loss = 1.83356 (* 1 = 1.83356 loss)
I0520 23:25:43.492455 24041 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0520 23:25:48.041574 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_1071.caffemodel
I0520 23:25:48.251420 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_1071.solverstate
I0520 23:25:51.530760 24041 solver.cpp:237] Iteration 1085, loss = 1.86634
I0520 23:25:51.530813 24041 solver.cpp:253]     Train net output #0: loss = 1.86634 (* 1 = 1.86634 loss)
I0520 23:25:51.530830 24041 sgd_solver.cpp:106] Iteration 1085, lr = 0.0025
I0520 23:25:59.492149 24041 solver.cpp:237] Iteration 1120, loss = 1.9102
I0520 23:25:59.492314 24041 solver.cpp:253]     Train net output #0: loss = 1.9102 (* 1 = 1.9102 loss)
I0520 23:25:59.492331 24041 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0520 23:26:07.449995 24041 solver.cpp:237] Iteration 1155, loss = 1.81751
I0520 23:26:07.450029 24041 solver.cpp:253]     Train net output #0: loss = 1.81751 (* 1 = 1.81751 loss)
I0520 23:26:07.450053 24041 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0520 23:26:37.535683 24041 solver.cpp:237] Iteration 1190, loss = 1.83035
I0520 23:26:37.535857 24041 solver.cpp:253]     Train net output #0: loss = 1.83035 (* 1 = 1.83035 loss)
I0520 23:26:37.535876 24041 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0520 23:26:45.497304 24041 solver.cpp:237] Iteration 1225, loss = 1.75073
I0520 23:26:45.497339 24041 solver.cpp:253]     Train net output #0: loss = 1.75073 (* 1 = 1.75073 loss)
I0520 23:26:45.497361 24041 sgd_solver.cpp:106] Iteration 1225, lr = 0.0025
I0520 23:26:53.456454 24041 solver.cpp:237] Iteration 1260, loss = 1.7671
I0520 23:26:53.456506 24041 solver.cpp:253]     Train net output #0: loss = 1.7671 (* 1 = 1.7671 loss)
I0520 23:26:53.456522 24041 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0520 23:27:01.414618 24041 solver.cpp:237] Iteration 1295, loss = 1.69784
I0520 23:27:01.414652 24041 solver.cpp:253]     Train net output #0: loss = 1.69784 (* 1 = 1.69784 loss)
I0520 23:27:01.414676 24041 sgd_solver.cpp:106] Iteration 1295, lr = 0.0025
I0520 23:27:09.371892 24041 solver.cpp:237] Iteration 1330, loss = 1.7978
I0520 23:27:09.372033 24041 solver.cpp:253]     Train net output #0: loss = 1.7978 (* 1 = 1.7978 loss)
I0520 23:27:09.372050 24041 sgd_solver.cpp:106] Iteration 1330, lr = 0.0025
I0520 23:27:17.330585 24041 solver.cpp:237] Iteration 1365, loss = 1.72746
I0520 23:27:17.330636 24041 solver.cpp:253]     Train net output #0: loss = 1.72746 (* 1 = 1.72746 loss)
I0520 23:27:17.330652 24041 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0520 23:27:25.289340 24041 solver.cpp:237] Iteration 1400, loss = 1.94861
I0520 23:27:25.289373 24041 solver.cpp:253]     Train net output #0: loss = 1.94861 (* 1 = 1.94861 loss)
I0520 23:27:25.289397 24041 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0520 23:27:31.429435 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_1428.caffemodel
I0520 23:27:31.628598 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_1428.solverstate
I0520 23:27:31.657491 24041 solver.cpp:341] Iteration 1428, Testing net (#0)
I0520 23:28:37.763933 24041 solver.cpp:409]     Test net output #0: accuracy = 0.646352
I0520 23:28:37.764109 24041 solver.cpp:409]     Test net output #1: loss = 1.3049 (* 1 = 1.3049 loss)
I0520 23:29:01.563995 24041 solver.cpp:237] Iteration 1435, loss = 1.69411
I0520 23:29:01.564054 24041 solver.cpp:253]     Train net output #0: loss = 1.69411 (* 1 = 1.69411 loss)
I0520 23:29:01.564079 24041 sgd_solver.cpp:106] Iteration 1435, lr = 0.0025
I0520 23:29:09.518388 24041 solver.cpp:237] Iteration 1470, loss = 1.84526
I0520 23:29:09.518539 24041 solver.cpp:253]     Train net output #0: loss = 1.84526 (* 1 = 1.84526 loss)
I0520 23:29:09.518556 24041 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0520 23:29:17.473076 24041 solver.cpp:237] Iteration 1505, loss = 1.75735
I0520 23:29:17.473111 24041 solver.cpp:253]     Train net output #0: loss = 1.75735 (* 1 = 1.75735 loss)
I0520 23:29:17.473134 24041 sgd_solver.cpp:106] Iteration 1505, lr = 0.0025
I0520 23:29:25.432875 24041 solver.cpp:237] Iteration 1540, loss = 1.76306
I0520 23:29:25.432915 24041 solver.cpp:253]     Train net output #0: loss = 1.76306 (* 1 = 1.76306 loss)
I0520 23:29:25.432932 24041 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0520 23:29:33.389703 24041 solver.cpp:237] Iteration 1575, loss = 1.75568
I0520 23:29:33.389739 24041 solver.cpp:253]     Train net output #0: loss = 1.75568 (* 1 = 1.75568 loss)
I0520 23:29:33.389757 24041 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0520 23:29:41.343014 24041 solver.cpp:237] Iteration 1610, loss = 1.74178
I0520 23:29:41.343155 24041 solver.cpp:253]     Train net output #0: loss = 1.74178 (* 1 = 1.74178 loss)
I0520 23:29:41.343171 24041 sgd_solver.cpp:106] Iteration 1610, lr = 0.0025
I0520 23:29:49.299533 24041 solver.cpp:237] Iteration 1645, loss = 1.75053
I0520 23:29:49.299574 24041 solver.cpp:253]     Train net output #0: loss = 1.75053 (* 1 = 1.75053 loss)
I0520 23:29:49.299590 24041 sgd_solver.cpp:106] Iteration 1645, lr = 0.0025
I0520 23:30:19.449985 24041 solver.cpp:237] Iteration 1680, loss = 1.66134
I0520 23:30:19.450156 24041 solver.cpp:253]     Train net output #0: loss = 1.66134 (* 1 = 1.66134 loss)
I0520 23:30:19.450175 24041 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0520 23:30:27.403625 24041 solver.cpp:237] Iteration 1715, loss = 1.68299
I0520 23:30:27.403661 24041 solver.cpp:253]     Train net output #0: loss = 1.68299 (* 1 = 1.68299 loss)
I0520 23:30:27.403684 24041 sgd_solver.cpp:106] Iteration 1715, lr = 0.0025
I0520 23:30:35.361830 24041 solver.cpp:237] Iteration 1750, loss = 1.76256
I0520 23:30:35.361886 24041 solver.cpp:253]     Train net output #0: loss = 1.76256 (* 1 = 1.76256 loss)
I0520 23:30:35.361902 24041 sgd_solver.cpp:106] Iteration 1750, lr = 0.0025
I0520 23:30:43.090553 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_1785.caffemodel
I0520 23:30:43.289974 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_1785.solverstate
I0520 23:30:43.406190 24041 solver.cpp:237] Iteration 1785, loss = 1.69404
I0520 23:30:43.406244 24041 solver.cpp:253]     Train net output #0: loss = 1.69404 (* 1 = 1.69404 loss)
I0520 23:30:43.406271 24041 sgd_solver.cpp:106] Iteration 1785, lr = 0.0025
I0520 23:30:51.360631 24041 solver.cpp:237] Iteration 1820, loss = 1.69639
I0520 23:30:51.360780 24041 solver.cpp:253]     Train net output #0: loss = 1.69639 (* 1 = 1.69639 loss)
I0520 23:30:51.360796 24041 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0520 23:30:59.315858 24041 solver.cpp:237] Iteration 1855, loss = 1.69879
I0520 23:30:59.315917 24041 solver.cpp:253]     Train net output #0: loss = 1.69879 (* 1 = 1.69879 loss)
I0520 23:30:59.315943 24041 sgd_solver.cpp:106] Iteration 1855, lr = 0.0025
I0520 23:31:07.266522 24041 solver.cpp:237] Iteration 1890, loss = 1.69899
I0520 23:31:07.266558 24041 solver.cpp:253]     Train net output #0: loss = 1.69899 (* 1 = 1.69899 loss)
I0520 23:31:07.266582 24041 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0520 23:31:37.386632 24041 solver.cpp:237] Iteration 1925, loss = 1.62069
I0520 23:31:37.386811 24041 solver.cpp:253]     Train net output #0: loss = 1.62069 (* 1 = 1.62069 loss)
I0520 23:31:37.386827 24041 sgd_solver.cpp:106] Iteration 1925, lr = 0.0025
I0520 23:31:45.342854 24041 solver.cpp:237] Iteration 1960, loss = 1.66095
I0520 23:31:45.342888 24041 solver.cpp:253]     Train net output #0: loss = 1.66095 (* 1 = 1.66095 loss)
I0520 23:31:45.342911 24041 sgd_solver.cpp:106] Iteration 1960, lr = 0.0025
I0520 23:31:53.298535 24041 solver.cpp:237] Iteration 1995, loss = 1.72504
I0520 23:31:53.298593 24041 solver.cpp:253]     Train net output #0: loss = 1.72504 (* 1 = 1.72504 loss)
I0520 23:31:53.298617 24041 sgd_solver.cpp:106] Iteration 1995, lr = 0.0025
I0520 23:32:01.251407 24041 solver.cpp:237] Iteration 2030, loss = 1.66162
I0520 23:32:01.251441 24041 solver.cpp:253]     Train net output #0: loss = 1.66162 (* 1 = 1.66162 loss)
I0520 23:32:01.251464 24041 sgd_solver.cpp:106] Iteration 2030, lr = 0.0025
I0520 23:32:09.205159 24041 solver.cpp:237] Iteration 2065, loss = 1.68891
I0520 23:32:09.205315 24041 solver.cpp:253]     Train net output #0: loss = 1.68891 (* 1 = 1.68891 loss)
I0520 23:32:09.205332 24041 sgd_solver.cpp:106] Iteration 2065, lr = 0.0025
I0520 23:32:17.159592 24041 solver.cpp:237] Iteration 2100, loss = 1.68463
I0520 23:32:17.159641 24041 solver.cpp:253]     Train net output #0: loss = 1.68463 (* 1 = 1.68463 loss)
I0520 23:32:17.159659 24041 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0520 23:32:25.111508 24041 solver.cpp:237] Iteration 2135, loss = 1.79392
I0520 23:32:25.111543 24041 solver.cpp:253]     Train net output #0: loss = 1.79392 (* 1 = 1.79392 loss)
I0520 23:32:25.111567 24041 sgd_solver.cpp:106] Iteration 2135, lr = 0.0025
I0520 23:32:26.476266 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_2142.caffemodel
I0520 23:32:26.672794 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_2142.solverstate
I0520 23:32:26.699323 24041 solver.cpp:341] Iteration 2142, Testing net (#0)
I0520 23:33:11.575806 24041 solver.cpp:409]     Test net output #0: accuracy = 0.667741
I0520 23:33:11.575974 24041 solver.cpp:409]     Test net output #1: loss = 1.15032 (* 1 = 1.15032 loss)
I0520 23:33:40.151460 24041 solver.cpp:237] Iteration 2170, loss = 1.65663
I0520 23:33:40.151520 24041 solver.cpp:253]     Train net output #0: loss = 1.65663 (* 1 = 1.65663 loss)
I0520 23:33:40.151538 24041 sgd_solver.cpp:106] Iteration 2170, lr = 0.0025
I0520 23:33:48.105028 24041 solver.cpp:237] Iteration 2205, loss = 1.75212
I0520 23:33:48.105181 24041 solver.cpp:253]     Train net output #0: loss = 1.75212 (* 1 = 1.75212 loss)
I0520 23:33:48.105198 24041 sgd_solver.cpp:106] Iteration 2205, lr = 0.0025
I0520 23:33:56.057205 24041 solver.cpp:237] Iteration 2240, loss = 1.70711
I0520 23:33:56.057258 24041 solver.cpp:253]     Train net output #0: loss = 1.70711 (* 1 = 1.70711 loss)
I0520 23:33:56.057276 24041 sgd_solver.cpp:106] Iteration 2240, lr = 0.0025
I0520 23:34:04.012135 24041 solver.cpp:237] Iteration 2275, loss = 1.65551
I0520 23:34:04.012169 24041 solver.cpp:253]     Train net output #0: loss = 1.65551 (* 1 = 1.65551 loss)
I0520 23:34:04.012192 24041 sgd_solver.cpp:106] Iteration 2275, lr = 0.0025
I0520 23:34:11.965832 24041 solver.cpp:237] Iteration 2310, loss = 1.65686
I0520 23:34:11.965865 24041 solver.cpp:253]     Train net output #0: loss = 1.65686 (* 1 = 1.65686 loss)
I0520 23:34:11.965889 24041 sgd_solver.cpp:106] Iteration 2310, lr = 0.0025
I0520 23:34:19.922392 24041 solver.cpp:237] Iteration 2345, loss = 1.64554
I0520 23:34:19.922565 24041 solver.cpp:253]     Train net output #0: loss = 1.64554 (* 1 = 1.64554 loss)
I0520 23:34:19.922582 24041 sgd_solver.cpp:106] Iteration 2345, lr = 0.0025
I0520 23:34:50.047346 24041 solver.cpp:237] Iteration 2380, loss = 1.61483
I0520 23:34:50.047528 24041 solver.cpp:253]     Train net output #0: loss = 1.61483 (* 1 = 1.61483 loss)
I0520 23:34:50.047546 24041 sgd_solver.cpp:106] Iteration 2380, lr = 0.0025
I0520 23:34:57.999948 24041 solver.cpp:237] Iteration 2415, loss = 1.69293
I0520 23:34:57.999984 24041 solver.cpp:253]     Train net output #0: loss = 1.69293 (* 1 = 1.69293 loss)
I0520 23:34:58.000006 24041 sgd_solver.cpp:106] Iteration 2415, lr = 0.0025
I0520 23:35:05.954499 24041 solver.cpp:237] Iteration 2450, loss = 1.5916
I0520 23:35:05.954535 24041 solver.cpp:253]     Train net output #0: loss = 1.5916 (* 1 = 1.5916 loss)
I0520 23:35:05.954553 24041 sgd_solver.cpp:106] Iteration 2450, lr = 0.0025
I0520 23:35:13.909246 24041 solver.cpp:237] Iteration 2485, loss = 1.7125
I0520 23:35:13.909296 24041 solver.cpp:253]     Train net output #0: loss = 1.7125 (* 1 = 1.7125 loss)
I0520 23:35:13.909313 24041 sgd_solver.cpp:106] Iteration 2485, lr = 0.0025
I0520 23:35:16.862152 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_2499.caffemodel
I0520 23:35:17.073050 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_2499.solverstate
I0520 23:35:21.975538 24041 solver.cpp:237] Iteration 2520, loss = 1.6771
I0520 23:35:21.975704 24041 solver.cpp:253]     Train net output #0: loss = 1.6771 (* 1 = 1.6771 loss)
I0520 23:35:21.975721 24041 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0520 23:35:29.926704 24041 solver.cpp:237] Iteration 2555, loss = 1.61561
I0520 23:35:29.926740 24041 solver.cpp:253]     Train net output #0: loss = 1.61561 (* 1 = 1.61561 loss)
I0520 23:35:29.926758 24041 sgd_solver.cpp:106] Iteration 2555, lr = 0.0025
I0520 23:35:37.881654 24041 solver.cpp:237] Iteration 2590, loss = 1.55085
I0520 23:35:37.881708 24041 solver.cpp:253]     Train net output #0: loss = 1.55085 (* 1 = 1.55085 loss)
I0520 23:35:37.881726 24041 sgd_solver.cpp:106] Iteration 2590, lr = 0.0025
I0520 23:36:08.077291 24041 solver.cpp:237] Iteration 2625, loss = 1.62478
I0520 23:36:08.077466 24041 solver.cpp:253]     Train net output #0: loss = 1.62478 (* 1 = 1.62478 loss)
I0520 23:36:08.077484 24041 sgd_solver.cpp:106] Iteration 2625, lr = 0.0025
I0520 23:36:16.038071 24041 solver.cpp:237] Iteration 2660, loss = 1.60447
I0520 23:36:16.038107 24041 solver.cpp:253]     Train net output #0: loss = 1.60447 (* 1 = 1.60447 loss)
I0520 23:36:16.038126 24041 sgd_solver.cpp:106] Iteration 2660, lr = 0.0025
I0520 23:36:24.000030 24041 solver.cpp:237] Iteration 2695, loss = 1.62249
I0520 23:36:24.000085 24041 solver.cpp:253]     Train net output #0: loss = 1.62249 (* 1 = 1.62249 loss)
I0520 23:36:24.000104 24041 sgd_solver.cpp:106] Iteration 2695, lr = 0.0025
I0520 23:36:31.959578 24041 solver.cpp:237] Iteration 2730, loss = 1.59532
I0520 23:36:31.959616 24041 solver.cpp:253]     Train net output #0: loss = 1.59532 (* 1 = 1.59532 loss)
I0520 23:36:31.959635 24041 sgd_solver.cpp:106] Iteration 2730, lr = 0.0025
I0520 23:36:39.921373 24041 solver.cpp:237] Iteration 2765, loss = 1.66412
I0520 23:36:39.921517 24041 solver.cpp:253]     Train net output #0: loss = 1.66412 (* 1 = 1.66412 loss)
I0520 23:36:39.921535 24041 sgd_solver.cpp:106] Iteration 2765, lr = 0.0025
I0520 23:36:47.883256 24041 solver.cpp:237] Iteration 2800, loss = 1.63129
I0520 23:36:47.883291 24041 solver.cpp:253]     Train net output #0: loss = 1.63129 (* 1 = 1.63129 loss)
I0520 23:36:47.883314 24041 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0520 23:36:55.838639 24041 solver.cpp:237] Iteration 2835, loss = 1.61215
I0520 23:36:55.838691 24041 solver.cpp:253]     Train net output #0: loss = 1.61215 (* 1 = 1.61215 loss)
I0520 23:36:55.838708 24041 sgd_solver.cpp:106] Iteration 2835, lr = 0.0025
I0520 23:37:00.384412 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_2856.caffemodel
I0520 23:37:01.205566 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_2856.solverstate
I0520 23:37:01.270416 24041 solver.cpp:341] Iteration 2856, Testing net (#0)
I0520 23:38:08.582473 24041 solver.cpp:409]     Test net output #0: accuracy = 0.685821
I0520 23:38:08.582651 24041 solver.cpp:409]     Test net output #1: loss = 1.06603 (* 1 = 1.06603 loss)
I0520 23:38:34.037070 24041 solver.cpp:237] Iteration 2870, loss = 1.55913
I0520 23:38:34.037128 24041 solver.cpp:253]     Train net output #0: loss = 1.55913 (* 1 = 1.55913 loss)
I0520 23:38:34.037145 24041 sgd_solver.cpp:106] Iteration 2870, lr = 0.0025
I0520 23:38:41.996839 24041 solver.cpp:237] Iteration 2905, loss = 1.66957
I0520 23:38:41.996991 24041 solver.cpp:253]     Train net output #0: loss = 1.66957 (* 1 = 1.66957 loss)
I0520 23:38:41.997009 24041 sgd_solver.cpp:106] Iteration 2905, lr = 0.0025
I0520 23:38:49.956135 24041 solver.cpp:237] Iteration 2940, loss = 1.56651
I0520 23:38:49.956169 24041 solver.cpp:253]     Train net output #0: loss = 1.56651 (* 1 = 1.56651 loss)
I0520 23:38:49.956188 24041 sgd_solver.cpp:106] Iteration 2940, lr = 0.0025
I0520 23:38:57.914683 24041 solver.cpp:237] Iteration 2975, loss = 1.56678
I0520 23:38:57.914736 24041 solver.cpp:253]     Train net output #0: loss = 1.56678 (* 1 = 1.56678 loss)
I0520 23:38:57.914752 24041 sgd_solver.cpp:106] Iteration 2975, lr = 0.0025
I0520 23:39:05.870841 24041 solver.cpp:237] Iteration 3010, loss = 1.7442
I0520 23:39:05.870873 24041 solver.cpp:253]     Train net output #0: loss = 1.7442 (* 1 = 1.7442 loss)
I0520 23:39:05.870898 24041 sgd_solver.cpp:106] Iteration 3010, lr = 0.0025
I0520 23:39:13.824548 24041 solver.cpp:237] Iteration 3045, loss = 1.65884
I0520 23:39:13.824690 24041 solver.cpp:253]     Train net output #0: loss = 1.65884 (* 1 = 1.65884 loss)
I0520 23:39:13.824707 24041 sgd_solver.cpp:106] Iteration 3045, lr = 0.0025
I0520 23:39:21.784081 24041 solver.cpp:237] Iteration 3080, loss = 1.65684
I0520 23:39:21.784134 24041 solver.cpp:253]     Train net output #0: loss = 1.65684 (* 1 = 1.65684 loss)
I0520 23:39:21.784162 24041 sgd_solver.cpp:106] Iteration 3080, lr = 0.0025
I0520 23:39:51.992980 24041 solver.cpp:237] Iteration 3115, loss = 1.49482
I0520 23:39:51.993144 24041 solver.cpp:253]     Train net output #0: loss = 1.49482 (* 1 = 1.49482 loss)
I0520 23:39:51.993161 24041 sgd_solver.cpp:106] Iteration 3115, lr = 0.0025
I0520 23:39:59.952189 24041 solver.cpp:237] Iteration 3150, loss = 1.55785
I0520 23:39:59.952224 24041 solver.cpp:253]     Train net output #0: loss = 1.55785 (* 1 = 1.55785 loss)
I0520 23:39:59.952242 24041 sgd_solver.cpp:106] Iteration 3150, lr = 0.0025
I0520 23:40:07.907768 24041 solver.cpp:237] Iteration 3185, loss = 1.56554
I0520 23:40:07.907801 24041 solver.cpp:253]     Train net output #0: loss = 1.56554 (* 1 = 1.56554 loss)
I0520 23:40:07.907825 24041 sgd_solver.cpp:106] Iteration 3185, lr = 0.0025
I0520 23:40:14.045181 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_3213.caffemodel
I0520 23:40:14.275491 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_3213.solverstate
I0520 23:40:16.009870 24041 solver.cpp:237] Iteration 3220, loss = 1.60107
I0520 23:40:16.009924 24041 solver.cpp:253]     Train net output #0: loss = 1.60107 (* 1 = 1.60107 loss)
I0520 23:40:16.009948 24041 sgd_solver.cpp:106] Iteration 3220, lr = 0.0025
I0520 23:40:23.971324 24041 solver.cpp:237] Iteration 3255, loss = 1.7102
I0520 23:40:23.971484 24041 solver.cpp:253]     Train net output #0: loss = 1.7102 (* 1 = 1.7102 loss)
I0520 23:40:23.971500 24041 sgd_solver.cpp:106] Iteration 3255, lr = 0.0025
I0520 23:40:31.931905 24041 solver.cpp:237] Iteration 3290, loss = 1.6299
I0520 23:40:31.931941 24041 solver.cpp:253]     Train net output #0: loss = 1.6299 (* 1 = 1.6299 loss)
I0520 23:40:31.931963 24041 sgd_solver.cpp:106] Iteration 3290, lr = 0.0025
I0520 23:40:39.891909 24041 solver.cpp:237] Iteration 3325, loss = 1.7052
I0520 23:40:39.891962 24041 solver.cpp:253]     Train net output #0: loss = 1.7052 (* 1 = 1.7052 loss)
I0520 23:40:39.891978 24041 sgd_solver.cpp:106] Iteration 3325, lr = 0.0025
I0520 23:41:10.099313 24041 solver.cpp:237] Iteration 3360, loss = 1.64668
I0520 23:41:10.099505 24041 solver.cpp:253]     Train net output #0: loss = 1.64668 (* 1 = 1.64668 loss)
I0520 23:41:10.099524 24041 sgd_solver.cpp:106] Iteration 3360, lr = 0.0025
I0520 23:41:18.058755 24041 solver.cpp:237] Iteration 3395, loss = 1.60709
I0520 23:41:18.058792 24041 solver.cpp:253]     Train net output #0: loss = 1.60709 (* 1 = 1.60709 loss)
I0520 23:41:18.058810 24041 sgd_solver.cpp:106] Iteration 3395, lr = 0.0025
I0520 23:41:26.018182 24041 solver.cpp:237] Iteration 3430, loss = 1.54663
I0520 23:41:26.018223 24041 solver.cpp:253]     Train net output #0: loss = 1.54663 (* 1 = 1.54663 loss)
I0520 23:41:26.018239 24041 sgd_solver.cpp:106] Iteration 3430, lr = 0.0025
I0520 23:41:33.975406 24041 solver.cpp:237] Iteration 3465, loss = 1.68505
I0520 23:41:33.975441 24041 solver.cpp:253]     Train net output #0: loss = 1.68505 (* 1 = 1.68505 loss)
I0520 23:41:33.975466 24041 sgd_solver.cpp:106] Iteration 3465, lr = 0.0025
I0520 23:41:41.930852 24041 solver.cpp:237] Iteration 3500, loss = 1.53114
I0520 23:41:41.930995 24041 solver.cpp:253]     Train net output #0: loss = 1.53114 (* 1 = 1.53114 loss)
I0520 23:41:41.931011 24041 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0520 23:41:49.888393 24041 solver.cpp:237] Iteration 3535, loss = 1.60827
I0520 23:41:49.888444 24041 solver.cpp:253]     Train net output #0: loss = 1.60827 (* 1 = 1.60827 loss)
I0520 23:41:49.888461 24041 sgd_solver.cpp:106] Iteration 3535, lr = 0.0025
I0520 23:41:57.618224 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_3570.caffemodel
I0520 23:41:57.844069 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_3570.solverstate
I0520 23:41:57.906532 24041 solver.cpp:341] Iteration 3570, Testing net (#0)
I0520 23:42:43.135308 24041 solver.cpp:409]     Test net output #0: accuracy = 0.703014
I0520 23:42:43.135478 24041 solver.cpp:409]     Test net output #1: loss = 1.00556 (* 1 = 1.00556 loss)
I0520 23:42:43.203285 24041 solver.cpp:237] Iteration 3570, loss = 1.63342
I0520 23:42:43.203315 24041 solver.cpp:253]     Train net output #0: loss = 1.63342 (* 1 = 1.63342 loss)
I0520 23:42:43.203339 24041 sgd_solver.cpp:106] Iteration 3570, lr = 0.0025
I0520 23:42:43.203752 24041 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_3571.caffemodel
I0520 23:42:43.412391 24041 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254_iter_3571.solverstate
I0520 23:42:43.478256 24041 solver.cpp:326] Optimization Done.
I0520 23:42:43.478291 24041 caffe.cpp:215] Optimization Done.
Application 11235832 resources: utime ~1257s, stime ~225s, Rss ~5329620, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_420_2016-05-20T11.20.47.957254.solver"
	User time (seconds): 0.56
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:50.24
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 1
	Minor (reclaiming a frame) page faults: 15109
	Voluntary context switches: 3376
	Involuntary context switches: 216
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
