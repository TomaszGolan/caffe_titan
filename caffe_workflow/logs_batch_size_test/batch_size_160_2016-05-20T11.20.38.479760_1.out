2805247
I0520 15:09:04.318672 31241 caffe.cpp:184] Using GPUs 0
I0520 15:09:04.747619 31241 solver.cpp:48] Initializing solver from parameters: 
test_iter: 937
test_interval: 1875
base_lr: 0.0025
display: 93
max_iter: 9375
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 937
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760.prototxt"
I0520 15:09:04.749244 31241 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760.prototxt
I0520 15:09:04.765079 31241 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 15:09:04.765144 31241 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 15:09:04.765521 31241 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 160
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:09:04.765723 31241 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:09:04.765753 31241 net.cpp:106] Creating Layer data_hdf5
I0520 15:09:04.765777 31241 net.cpp:411] data_hdf5 -> data
I0520 15:09:04.765810 31241 net.cpp:411] data_hdf5 -> label
I0520 15:09:04.765847 31241 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 15:09:04.780733 31241 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 15:09:04.782941 31241 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 15:09:26.295913 31241 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 15:09:26.301067 31241 net.cpp:150] Setting up data_hdf5
I0520 15:09:26.301108 31241 net.cpp:157] Top shape: 160 1 127 50 (1016000)
I0520 15:09:26.301126 31241 net.cpp:157] Top shape: 160 (160)
I0520 15:09:26.301139 31241 net.cpp:165] Memory required for data: 4064640
I0520 15:09:26.301159 31241 layer_factory.hpp:77] Creating layer conv1
I0520 15:09:26.301208 31241 net.cpp:106] Creating Layer conv1
I0520 15:09:26.301234 31241 net.cpp:454] conv1 <- data
I0520 15:09:26.301259 31241 net.cpp:411] conv1 -> conv1
I0520 15:09:27.026021 31241 net.cpp:150] Setting up conv1
I0520 15:09:27.026077 31241 net.cpp:157] Top shape: 160 12 120 48 (11059200)
I0520 15:09:27.026100 31241 net.cpp:165] Memory required for data: 48301440
I0520 15:09:27.026131 31241 layer_factory.hpp:77] Creating layer relu1
I0520 15:09:27.026154 31241 net.cpp:106] Creating Layer relu1
I0520 15:09:27.026173 31241 net.cpp:454] relu1 <- conv1
I0520 15:09:27.026209 31241 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:09:27.026739 31241 net.cpp:150] Setting up relu1
I0520 15:09:27.026762 31241 net.cpp:157] Top shape: 160 12 120 48 (11059200)
I0520 15:09:27.026777 31241 net.cpp:165] Memory required for data: 92538240
I0520 15:09:27.026793 31241 layer_factory.hpp:77] Creating layer pool1
I0520 15:09:27.026818 31241 net.cpp:106] Creating Layer pool1
I0520 15:09:27.026832 31241 net.cpp:454] pool1 <- conv1
I0520 15:09:27.026849 31241 net.cpp:411] pool1 -> pool1
I0520 15:09:27.026940 31241 net.cpp:150] Setting up pool1
I0520 15:09:27.026958 31241 net.cpp:157] Top shape: 160 12 60 48 (5529600)
I0520 15:09:27.026973 31241 net.cpp:165] Memory required for data: 114656640
I0520 15:09:27.026993 31241 layer_factory.hpp:77] Creating layer conv2
I0520 15:09:27.027017 31241 net.cpp:106] Creating Layer conv2
I0520 15:09:27.027031 31241 net.cpp:454] conv2 <- pool1
I0520 15:09:27.027047 31241 net.cpp:411] conv2 -> conv2
I0520 15:09:27.029759 31241 net.cpp:150] Setting up conv2
I0520 15:09:27.029789 31241 net.cpp:157] Top shape: 160 20 54 46 (7948800)
I0520 15:09:27.029805 31241 net.cpp:165] Memory required for data: 146451840
I0520 15:09:27.029834 31241 layer_factory.hpp:77] Creating layer relu2
I0520 15:09:27.029850 31241 net.cpp:106] Creating Layer relu2
I0520 15:09:27.029875 31241 net.cpp:454] relu2 <- conv2
I0520 15:09:27.029891 31241 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:09:27.030246 31241 net.cpp:150] Setting up relu2
I0520 15:09:27.030266 31241 net.cpp:157] Top shape: 160 20 54 46 (7948800)
I0520 15:09:27.030279 31241 net.cpp:165] Memory required for data: 178247040
I0520 15:09:27.030294 31241 layer_factory.hpp:77] Creating layer pool2
I0520 15:09:27.030316 31241 net.cpp:106] Creating Layer pool2
I0520 15:09:27.030330 31241 net.cpp:454] pool2 <- conv2
I0520 15:09:27.030364 31241 net.cpp:411] pool2 -> pool2
I0520 15:09:27.030448 31241 net.cpp:150] Setting up pool2
I0520 15:09:27.030470 31241 net.cpp:157] Top shape: 160 20 27 46 (3974400)
I0520 15:09:27.030483 31241 net.cpp:165] Memory required for data: 194144640
I0520 15:09:27.030495 31241 layer_factory.hpp:77] Creating layer conv3
I0520 15:09:27.030524 31241 net.cpp:106] Creating Layer conv3
I0520 15:09:27.030539 31241 net.cpp:454] conv3 <- pool2
I0520 15:09:27.030555 31241 net.cpp:411] conv3 -> conv3
I0520 15:09:27.032555 31241 net.cpp:150] Setting up conv3
I0520 15:09:27.032580 31241 net.cpp:157] Top shape: 160 28 22 44 (4336640)
I0520 15:09:27.032600 31241 net.cpp:165] Memory required for data: 211491200
I0520 15:09:27.032624 31241 layer_factory.hpp:77] Creating layer relu3
I0520 15:09:27.032645 31241 net.cpp:106] Creating Layer relu3
I0520 15:09:27.032668 31241 net.cpp:454] relu3 <- conv3
I0520 15:09:27.032685 31241 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:09:27.033171 31241 net.cpp:150] Setting up relu3
I0520 15:09:27.033195 31241 net.cpp:157] Top shape: 160 28 22 44 (4336640)
I0520 15:09:27.033208 31241 net.cpp:165] Memory required for data: 228837760
I0520 15:09:27.033224 31241 layer_factory.hpp:77] Creating layer pool3
I0520 15:09:27.033239 31241 net.cpp:106] Creating Layer pool3
I0520 15:09:27.033262 31241 net.cpp:454] pool3 <- conv3
I0520 15:09:27.033278 31241 net.cpp:411] pool3 -> pool3
I0520 15:09:27.033360 31241 net.cpp:150] Setting up pool3
I0520 15:09:27.033385 31241 net.cpp:157] Top shape: 160 28 11 44 (2168320)
I0520 15:09:27.033397 31241 net.cpp:165] Memory required for data: 237511040
I0520 15:09:27.033411 31241 layer_factory.hpp:77] Creating layer conv4
I0520 15:09:27.033432 31241 net.cpp:106] Creating Layer conv4
I0520 15:09:27.033452 31241 net.cpp:454] conv4 <- pool3
I0520 15:09:27.033468 31241 net.cpp:411] conv4 -> conv4
I0520 15:09:27.036459 31241 net.cpp:150] Setting up conv4
I0520 15:09:27.036495 31241 net.cpp:157] Top shape: 160 36 6 42 (1451520)
I0520 15:09:27.036509 31241 net.cpp:165] Memory required for data: 243317120
I0520 15:09:27.036535 31241 layer_factory.hpp:77] Creating layer relu4
I0520 15:09:27.036562 31241 net.cpp:106] Creating Layer relu4
I0520 15:09:27.036576 31241 net.cpp:454] relu4 <- conv4
I0520 15:09:27.036592 31241 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:09:27.037087 31241 net.cpp:150] Setting up relu4
I0520 15:09:27.037111 31241 net.cpp:157] Top shape: 160 36 6 42 (1451520)
I0520 15:09:27.037124 31241 net.cpp:165] Memory required for data: 249123200
I0520 15:09:27.037140 31241 layer_factory.hpp:77] Creating layer pool4
I0520 15:09:27.037156 31241 net.cpp:106] Creating Layer pool4
I0520 15:09:27.037178 31241 net.cpp:454] pool4 <- conv4
I0520 15:09:27.037194 31241 net.cpp:411] pool4 -> pool4
I0520 15:09:27.037276 31241 net.cpp:150] Setting up pool4
I0520 15:09:27.037299 31241 net.cpp:157] Top shape: 160 36 3 42 (725760)
I0520 15:09:27.037312 31241 net.cpp:165] Memory required for data: 252026240
I0520 15:09:27.037328 31241 layer_factory.hpp:77] Creating layer ip1
I0520 15:09:27.037355 31241 net.cpp:106] Creating Layer ip1
I0520 15:09:27.037369 31241 net.cpp:454] ip1 <- pool4
I0520 15:09:27.037386 31241 net.cpp:411] ip1 -> ip1
I0520 15:09:27.052834 31241 net.cpp:150] Setting up ip1
I0520 15:09:27.052867 31241 net.cpp:157] Top shape: 160 196 (31360)
I0520 15:09:27.052888 31241 net.cpp:165] Memory required for data: 252151680
I0520 15:09:27.052913 31241 layer_factory.hpp:77] Creating layer relu5
I0520 15:09:27.052932 31241 net.cpp:106] Creating Layer relu5
I0520 15:09:27.052948 31241 net.cpp:454] relu5 <- ip1
I0520 15:09:27.052978 31241 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:09:27.053335 31241 net.cpp:150] Setting up relu5
I0520 15:09:27.053356 31241 net.cpp:157] Top shape: 160 196 (31360)
I0520 15:09:27.053369 31241 net.cpp:165] Memory required for data: 252277120
I0520 15:09:27.053385 31241 layer_factory.hpp:77] Creating layer drop1
I0520 15:09:27.053416 31241 net.cpp:106] Creating Layer drop1
I0520 15:09:27.053429 31241 net.cpp:454] drop1 <- ip1
I0520 15:09:27.053458 31241 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:09:27.053519 31241 net.cpp:150] Setting up drop1
I0520 15:09:27.053544 31241 net.cpp:157] Top shape: 160 196 (31360)
I0520 15:09:27.053556 31241 net.cpp:165] Memory required for data: 252402560
I0520 15:09:27.053567 31241 layer_factory.hpp:77] Creating layer ip2
I0520 15:09:27.053591 31241 net.cpp:106] Creating Layer ip2
I0520 15:09:27.053611 31241 net.cpp:454] ip2 <- ip1
I0520 15:09:27.053627 31241 net.cpp:411] ip2 -> ip2
I0520 15:09:27.054116 31241 net.cpp:150] Setting up ip2
I0520 15:09:27.054136 31241 net.cpp:157] Top shape: 160 98 (15680)
I0520 15:09:27.054148 31241 net.cpp:165] Memory required for data: 252465280
I0520 15:09:27.054169 31241 layer_factory.hpp:77] Creating layer relu6
I0520 15:09:27.054184 31241 net.cpp:106] Creating Layer relu6
I0520 15:09:27.054204 31241 net.cpp:454] relu6 <- ip2
I0520 15:09:27.054220 31241 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:09:27.054774 31241 net.cpp:150] Setting up relu6
I0520 15:09:27.054796 31241 net.cpp:157] Top shape: 160 98 (15680)
I0520 15:09:27.054811 31241 net.cpp:165] Memory required for data: 252528000
I0520 15:09:27.054826 31241 layer_factory.hpp:77] Creating layer drop2
I0520 15:09:27.054842 31241 net.cpp:106] Creating Layer drop2
I0520 15:09:27.054863 31241 net.cpp:454] drop2 <- ip2
I0520 15:09:27.054879 31241 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:09:27.054929 31241 net.cpp:150] Setting up drop2
I0520 15:09:27.054952 31241 net.cpp:157] Top shape: 160 98 (15680)
I0520 15:09:27.054965 31241 net.cpp:165] Memory required for data: 252590720
I0520 15:09:27.054980 31241 layer_factory.hpp:77] Creating layer ip3
I0520 15:09:27.054996 31241 net.cpp:106] Creating Layer ip3
I0520 15:09:27.055011 31241 net.cpp:454] ip3 <- ip2
I0520 15:09:27.055032 31241 net.cpp:411] ip3 -> ip3
I0520 15:09:27.055256 31241 net.cpp:150] Setting up ip3
I0520 15:09:27.055275 31241 net.cpp:157] Top shape: 160 11 (1760)
I0520 15:09:27.055289 31241 net.cpp:165] Memory required for data: 252597760
I0520 15:09:27.055308 31241 layer_factory.hpp:77] Creating layer drop3
I0520 15:09:27.055330 31241 net.cpp:106] Creating Layer drop3
I0520 15:09:27.055343 31241 net.cpp:454] drop3 <- ip3
I0520 15:09:27.055358 31241 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:09:27.055407 31241 net.cpp:150] Setting up drop3
I0520 15:09:27.055435 31241 net.cpp:157] Top shape: 160 11 (1760)
I0520 15:09:27.055447 31241 net.cpp:165] Memory required for data: 252604800
I0520 15:09:27.055460 31241 layer_factory.hpp:77] Creating layer loss
I0520 15:09:27.055485 31241 net.cpp:106] Creating Layer loss
I0520 15:09:27.055497 31241 net.cpp:454] loss <- ip3
I0520 15:09:27.055517 31241 net.cpp:454] loss <- label
I0520 15:09:27.055533 31241 net.cpp:411] loss -> loss
I0520 15:09:27.055552 31241 layer_factory.hpp:77] Creating layer loss
I0520 15:09:27.056226 31241 net.cpp:150] Setting up loss
I0520 15:09:27.056249 31241 net.cpp:157] Top shape: (1)
I0520 15:09:27.056272 31241 net.cpp:160]     with loss weight 1
I0520 15:09:27.056324 31241 net.cpp:165] Memory required for data: 252604804
I0520 15:09:27.056346 31241 net.cpp:226] loss needs backward computation.
I0520 15:09:27.056360 31241 net.cpp:226] drop3 needs backward computation.
I0520 15:09:27.056373 31241 net.cpp:226] ip3 needs backward computation.
I0520 15:09:27.056386 31241 net.cpp:226] drop2 needs backward computation.
I0520 15:09:27.056398 31241 net.cpp:226] relu6 needs backward computation.
I0520 15:09:27.056413 31241 net.cpp:226] ip2 needs backward computation.
I0520 15:09:27.056427 31241 net.cpp:226] drop1 needs backward computation.
I0520 15:09:27.056445 31241 net.cpp:226] relu5 needs backward computation.
I0520 15:09:27.056458 31241 net.cpp:226] ip1 needs backward computation.
I0520 15:09:27.056473 31241 net.cpp:226] pool4 needs backward computation.
I0520 15:09:27.056485 31241 net.cpp:226] relu4 needs backward computation.
I0520 15:09:27.056500 31241 net.cpp:226] conv4 needs backward computation.
I0520 15:09:27.056514 31241 net.cpp:226] pool3 needs backward computation.
I0520 15:09:27.056534 31241 net.cpp:226] relu3 needs backward computation.
I0520 15:09:27.056555 31241 net.cpp:226] conv3 needs backward computation.
I0520 15:09:27.056568 31241 net.cpp:226] pool2 needs backward computation.
I0520 15:09:27.056581 31241 net.cpp:226] relu2 needs backward computation.
I0520 15:09:27.056597 31241 net.cpp:226] conv2 needs backward computation.
I0520 15:09:27.056610 31241 net.cpp:226] pool1 needs backward computation.
I0520 15:09:27.056629 31241 net.cpp:226] relu1 needs backward computation.
I0520 15:09:27.056643 31241 net.cpp:226] conv1 needs backward computation.
I0520 15:09:27.056658 31241 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:09:27.056669 31241 net.cpp:270] This network produces output loss
I0520 15:09:27.056699 31241 net.cpp:283] Network initialization done.
I0520 15:09:27.058248 31241 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760.prototxt
I0520 15:09:27.058322 31241 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 15:09:27.058679 31241 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 160
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:09:27.058878 31241 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:09:27.058898 31241 net.cpp:106] Creating Layer data_hdf5
I0520 15:09:27.058914 31241 net.cpp:411] data_hdf5 -> data
I0520 15:09:27.058935 31241 net.cpp:411] data_hdf5 -> label
I0520 15:09:27.058954 31241 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 15:09:27.060122 31241 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 15:09:48.398319 31241 net.cpp:150] Setting up data_hdf5
I0520 15:09:48.398488 31241 net.cpp:157] Top shape: 160 1 127 50 (1016000)
I0520 15:09:48.398507 31241 net.cpp:157] Top shape: 160 (160)
I0520 15:09:48.398520 31241 net.cpp:165] Memory required for data: 4064640
I0520 15:09:48.398535 31241 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 15:09:48.398564 31241 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 15:09:48.398582 31241 net.cpp:454] label_data_hdf5_1_split <- label
I0520 15:09:48.398599 31241 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 15:09:48.398641 31241 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 15:09:48.398721 31241 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 15:09:48.398746 31241 net.cpp:157] Top shape: 160 (160)
I0520 15:09:48.398761 31241 net.cpp:157] Top shape: 160 (160)
I0520 15:09:48.398783 31241 net.cpp:165] Memory required for data: 4065920
I0520 15:09:48.398795 31241 layer_factory.hpp:77] Creating layer conv1
I0520 15:09:48.398821 31241 net.cpp:106] Creating Layer conv1
I0520 15:09:48.398834 31241 net.cpp:454] conv1 <- data
I0520 15:09:48.398859 31241 net.cpp:411] conv1 -> conv1
I0520 15:09:48.400842 31241 net.cpp:150] Setting up conv1
I0520 15:09:48.400868 31241 net.cpp:157] Top shape: 160 12 120 48 (11059200)
I0520 15:09:48.400888 31241 net.cpp:165] Memory required for data: 48302720
I0520 15:09:48.400913 31241 layer_factory.hpp:77] Creating layer relu1
I0520 15:09:48.400933 31241 net.cpp:106] Creating Layer relu1
I0520 15:09:48.400955 31241 net.cpp:454] relu1 <- conv1
I0520 15:09:48.400971 31241 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:09:48.401486 31241 net.cpp:150] Setting up relu1
I0520 15:09:48.401509 31241 net.cpp:157] Top shape: 160 12 120 48 (11059200)
I0520 15:09:48.401522 31241 net.cpp:165] Memory required for data: 92539520
I0520 15:09:48.401535 31241 layer_factory.hpp:77] Creating layer pool1
I0520 15:09:48.401564 31241 net.cpp:106] Creating Layer pool1
I0520 15:09:48.401578 31241 net.cpp:454] pool1 <- conv1
I0520 15:09:48.401594 31241 net.cpp:411] pool1 -> pool1
I0520 15:09:48.401684 31241 net.cpp:150] Setting up pool1
I0520 15:09:48.401701 31241 net.cpp:157] Top shape: 160 12 60 48 (5529600)
I0520 15:09:48.401716 31241 net.cpp:165] Memory required for data: 114657920
I0520 15:09:48.401728 31241 layer_factory.hpp:77] Creating layer conv2
I0520 15:09:48.401748 31241 net.cpp:106] Creating Layer conv2
I0520 15:09:48.401770 31241 net.cpp:454] conv2 <- pool1
I0520 15:09:48.401788 31241 net.cpp:411] conv2 -> conv2
I0520 15:09:48.403748 31241 net.cpp:150] Setting up conv2
I0520 15:09:48.403774 31241 net.cpp:157] Top shape: 160 20 54 46 (7948800)
I0520 15:09:48.403786 31241 net.cpp:165] Memory required for data: 146453120
I0520 15:09:48.403811 31241 layer_factory.hpp:77] Creating layer relu2
I0520 15:09:48.403827 31241 net.cpp:106] Creating Layer relu2
I0520 15:09:48.403851 31241 net.cpp:454] relu2 <- conv2
I0520 15:09:48.403867 31241 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:09:48.404225 31241 net.cpp:150] Setting up relu2
I0520 15:09:48.404245 31241 net.cpp:157] Top shape: 160 20 54 46 (7948800)
I0520 15:09:48.404258 31241 net.cpp:165] Memory required for data: 178248320
I0520 15:09:48.404270 31241 layer_factory.hpp:77] Creating layer pool2
I0520 15:09:48.404290 31241 net.cpp:106] Creating Layer pool2
I0520 15:09:48.404310 31241 net.cpp:454] pool2 <- conv2
I0520 15:09:48.404326 31241 net.cpp:411] pool2 -> pool2
I0520 15:09:48.404418 31241 net.cpp:150] Setting up pool2
I0520 15:09:48.404435 31241 net.cpp:157] Top shape: 160 20 27 46 (3974400)
I0520 15:09:48.404453 31241 net.cpp:165] Memory required for data: 194145920
I0520 15:09:48.404466 31241 layer_factory.hpp:77] Creating layer conv3
I0520 15:09:48.404491 31241 net.cpp:106] Creating Layer conv3
I0520 15:09:48.404510 31241 net.cpp:454] conv3 <- pool2
I0520 15:09:48.404527 31241 net.cpp:411] conv3 -> conv3
I0520 15:09:48.406539 31241 net.cpp:150] Setting up conv3
I0520 15:09:48.406569 31241 net.cpp:157] Top shape: 160 28 22 44 (4336640)
I0520 15:09:48.406581 31241 net.cpp:165] Memory required for data: 211492480
I0520 15:09:48.406620 31241 layer_factory.hpp:77] Creating layer relu3
I0520 15:09:48.406647 31241 net.cpp:106] Creating Layer relu3
I0520 15:09:48.406661 31241 net.cpp:454] relu3 <- conv3
I0520 15:09:48.406687 31241 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:09:48.407182 31241 net.cpp:150] Setting up relu3
I0520 15:09:48.407205 31241 net.cpp:157] Top shape: 160 28 22 44 (4336640)
I0520 15:09:48.407218 31241 net.cpp:165] Memory required for data: 228839040
I0520 15:09:48.407233 31241 layer_factory.hpp:77] Creating layer pool3
I0520 15:09:48.407249 31241 net.cpp:106] Creating Layer pool3
I0520 15:09:48.407271 31241 net.cpp:454] pool3 <- conv3
I0520 15:09:48.407287 31241 net.cpp:411] pool3 -> pool3
I0520 15:09:48.407372 31241 net.cpp:150] Setting up pool3
I0520 15:09:48.407394 31241 net.cpp:157] Top shape: 160 28 11 44 (2168320)
I0520 15:09:48.407407 31241 net.cpp:165] Memory required for data: 237512320
I0520 15:09:48.407421 31241 layer_factory.hpp:77] Creating layer conv4
I0520 15:09:48.407454 31241 net.cpp:106] Creating Layer conv4
I0520 15:09:48.407469 31241 net.cpp:454] conv4 <- pool3
I0520 15:09:48.407496 31241 net.cpp:411] conv4 -> conv4
I0520 15:09:48.409611 31241 net.cpp:150] Setting up conv4
I0520 15:09:48.409636 31241 net.cpp:157] Top shape: 160 36 6 42 (1451520)
I0520 15:09:48.409657 31241 net.cpp:165] Memory required for data: 243318400
I0520 15:09:48.409675 31241 layer_factory.hpp:77] Creating layer relu4
I0520 15:09:48.409696 31241 net.cpp:106] Creating Layer relu4
I0520 15:09:48.409708 31241 net.cpp:454] relu4 <- conv4
I0520 15:09:48.409734 31241 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:09:48.410228 31241 net.cpp:150] Setting up relu4
I0520 15:09:48.410250 31241 net.cpp:157] Top shape: 160 36 6 42 (1451520)
I0520 15:09:48.410264 31241 net.cpp:165] Memory required for data: 249124480
I0520 15:09:48.410279 31241 layer_factory.hpp:77] Creating layer pool4
I0520 15:09:48.410296 31241 net.cpp:106] Creating Layer pool4
I0520 15:09:48.410317 31241 net.cpp:454] pool4 <- conv4
I0520 15:09:48.410333 31241 net.cpp:411] pool4 -> pool4
I0520 15:09:48.410419 31241 net.cpp:150] Setting up pool4
I0520 15:09:48.410436 31241 net.cpp:157] Top shape: 160 36 3 42 (725760)
I0520 15:09:48.410449 31241 net.cpp:165] Memory required for data: 252027520
I0520 15:09:48.410464 31241 layer_factory.hpp:77] Creating layer ip1
I0520 15:09:48.410480 31241 net.cpp:106] Creating Layer ip1
I0520 15:09:48.410501 31241 net.cpp:454] ip1 <- pool4
I0520 15:09:48.410523 31241 net.cpp:411] ip1 -> ip1
I0520 15:09:48.425989 31241 net.cpp:150] Setting up ip1
I0520 15:09:48.426023 31241 net.cpp:157] Top shape: 160 196 (31360)
I0520 15:09:48.426043 31241 net.cpp:165] Memory required for data: 252152960
I0520 15:09:48.426070 31241 layer_factory.hpp:77] Creating layer relu5
I0520 15:09:48.426091 31241 net.cpp:106] Creating Layer relu5
I0520 15:09:48.426105 31241 net.cpp:454] relu5 <- ip1
I0520 15:09:48.426120 31241 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:09:48.426497 31241 net.cpp:150] Setting up relu5
I0520 15:09:48.426518 31241 net.cpp:157] Top shape: 160 196 (31360)
I0520 15:09:48.426530 31241 net.cpp:165] Memory required for data: 252278400
I0520 15:09:48.426545 31241 layer_factory.hpp:77] Creating layer drop1
I0520 15:09:48.426566 31241 net.cpp:106] Creating Layer drop1
I0520 15:09:48.426586 31241 net.cpp:454] drop1 <- ip1
I0520 15:09:48.426602 31241 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:09:48.426654 31241 net.cpp:150] Setting up drop1
I0520 15:09:48.426671 31241 net.cpp:157] Top shape: 160 196 (31360)
I0520 15:09:48.426683 31241 net.cpp:165] Memory required for data: 252403840
I0520 15:09:48.426702 31241 layer_factory.hpp:77] Creating layer ip2
I0520 15:09:48.426726 31241 net.cpp:106] Creating Layer ip2
I0520 15:09:48.426743 31241 net.cpp:454] ip2 <- ip1
I0520 15:09:48.426760 31241 net.cpp:411] ip2 -> ip2
I0520 15:09:48.427258 31241 net.cpp:150] Setting up ip2
I0520 15:09:48.427276 31241 net.cpp:157] Top shape: 160 98 (15680)
I0520 15:09:48.427289 31241 net.cpp:165] Memory required for data: 252466560
I0520 15:09:48.427323 31241 layer_factory.hpp:77] Creating layer relu6
I0520 15:09:48.427346 31241 net.cpp:106] Creating Layer relu6
I0520 15:09:48.427359 31241 net.cpp:454] relu6 <- ip2
I0520 15:09:48.427376 31241 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:09:48.427947 31241 net.cpp:150] Setting up relu6
I0520 15:09:48.427970 31241 net.cpp:157] Top shape: 160 98 (15680)
I0520 15:09:48.427983 31241 net.cpp:165] Memory required for data: 252529280
I0520 15:09:48.427999 31241 layer_factory.hpp:77] Creating layer drop2
I0520 15:09:48.428016 31241 net.cpp:106] Creating Layer drop2
I0520 15:09:48.428037 31241 net.cpp:454] drop2 <- ip2
I0520 15:09:48.428055 31241 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:09:48.428107 31241 net.cpp:150] Setting up drop2
I0520 15:09:48.428123 31241 net.cpp:157] Top shape: 160 98 (15680)
I0520 15:09:48.428136 31241 net.cpp:165] Memory required for data: 252592000
I0520 15:09:48.428148 31241 layer_factory.hpp:77] Creating layer ip3
I0520 15:09:48.428166 31241 net.cpp:106] Creating Layer ip3
I0520 15:09:48.428179 31241 net.cpp:454] ip3 <- ip2
I0520 15:09:48.428215 31241 net.cpp:411] ip3 -> ip3
I0520 15:09:48.428464 31241 net.cpp:150] Setting up ip3
I0520 15:09:48.428484 31241 net.cpp:157] Top shape: 160 11 (1760)
I0520 15:09:48.428498 31241 net.cpp:165] Memory required for data: 252599040
I0520 15:09:48.428519 31241 layer_factory.hpp:77] Creating layer drop3
I0520 15:09:48.428541 31241 net.cpp:106] Creating Layer drop3
I0520 15:09:48.428555 31241 net.cpp:454] drop3 <- ip3
I0520 15:09:48.428570 31241 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:09:48.428619 31241 net.cpp:150] Setting up drop3
I0520 15:09:48.428642 31241 net.cpp:157] Top shape: 160 11 (1760)
I0520 15:09:48.428654 31241 net.cpp:165] Memory required for data: 252606080
I0520 15:09:48.428666 31241 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 15:09:48.428683 31241 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 15:09:48.428697 31241 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 15:09:48.428719 31241 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 15:09:48.428737 31241 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 15:09:48.428830 31241 net.cpp:150] Setting up ip3_drop3_0_split
I0520 15:09:48.428848 31241 net.cpp:157] Top shape: 160 11 (1760)
I0520 15:09:48.428864 31241 net.cpp:157] Top shape: 160 11 (1760)
I0520 15:09:48.428877 31241 net.cpp:165] Memory required for data: 252620160
I0520 15:09:48.428889 31241 layer_factory.hpp:77] Creating layer accuracy
I0520 15:09:48.428918 31241 net.cpp:106] Creating Layer accuracy
I0520 15:09:48.428931 31241 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 15:09:48.428946 31241 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 15:09:48.428961 31241 net.cpp:411] accuracy -> accuracy
I0520 15:09:48.428997 31241 net.cpp:150] Setting up accuracy
I0520 15:09:48.429013 31241 net.cpp:157] Top shape: (1)
I0520 15:09:48.429031 31241 net.cpp:165] Memory required for data: 252620164
I0520 15:09:48.429044 31241 layer_factory.hpp:77] Creating layer loss
I0520 15:09:48.429060 31241 net.cpp:106] Creating Layer loss
I0520 15:09:48.429075 31241 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 15:09:48.429087 31241 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 15:09:48.429105 31241 net.cpp:411] loss -> loss
I0520 15:09:48.429126 31241 layer_factory.hpp:77] Creating layer loss
I0520 15:09:48.429641 31241 net.cpp:150] Setting up loss
I0520 15:09:48.429661 31241 net.cpp:157] Top shape: (1)
I0520 15:09:48.429673 31241 net.cpp:160]     with loss weight 1
I0520 15:09:48.429699 31241 net.cpp:165] Memory required for data: 252620168
I0520 15:09:48.429718 31241 net.cpp:226] loss needs backward computation.
I0520 15:09:48.429733 31241 net.cpp:228] accuracy does not need backward computation.
I0520 15:09:48.429746 31241 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 15:09:48.429759 31241 net.cpp:226] drop3 needs backward computation.
I0520 15:09:48.429771 31241 net.cpp:226] ip3 needs backward computation.
I0520 15:09:48.429787 31241 net.cpp:226] drop2 needs backward computation.
I0520 15:09:48.429810 31241 net.cpp:226] relu6 needs backward computation.
I0520 15:09:48.429828 31241 net.cpp:226] ip2 needs backward computation.
I0520 15:09:48.429846 31241 net.cpp:226] drop1 needs backward computation.
I0520 15:09:48.429857 31241 net.cpp:226] relu5 needs backward computation.
I0520 15:09:48.429869 31241 net.cpp:226] ip1 needs backward computation.
I0520 15:09:48.429884 31241 net.cpp:226] pool4 needs backward computation.
I0520 15:09:48.429896 31241 net.cpp:226] relu4 needs backward computation.
I0520 15:09:48.429909 31241 net.cpp:226] conv4 needs backward computation.
I0520 15:09:48.429922 31241 net.cpp:226] pool3 needs backward computation.
I0520 15:09:48.429944 31241 net.cpp:226] relu3 needs backward computation.
I0520 15:09:48.429957 31241 net.cpp:226] conv3 needs backward computation.
I0520 15:09:48.429970 31241 net.cpp:226] pool2 needs backward computation.
I0520 15:09:48.429986 31241 net.cpp:226] relu2 needs backward computation.
I0520 15:09:48.429997 31241 net.cpp:226] conv2 needs backward computation.
I0520 15:09:48.430016 31241 net.cpp:226] pool1 needs backward computation.
I0520 15:09:48.430030 31241 net.cpp:226] relu1 needs backward computation.
I0520 15:09:48.430042 31241 net.cpp:226] conv1 needs backward computation.
I0520 15:09:48.430057 31241 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 15:09:48.430069 31241 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:09:48.430081 31241 net.cpp:270] This network produces output accuracy
I0520 15:09:48.430096 31241 net.cpp:270] This network produces output loss
I0520 15:09:48.430127 31241 net.cpp:283] Network initialization done.
I0520 15:09:48.430263 31241 solver.cpp:60] Solver scaffolding done.
I0520 15:09:48.431421 31241 caffe.cpp:212] Starting Optimization
I0520 15:09:48.431460 31241 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 15:09:48.431473 31241 solver.cpp:289] Learning Rate Policy: fixed
I0520 15:09:48.432559 31241 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 15:10:35.326892 31241 solver.cpp:409]     Test net output #0: accuracy = 0.0821505
I0520 15:10:35.327060 31241 solver.cpp:409]     Test net output #1: loss = 2.3997 (* 1 = 2.3997 loss)
I0520 15:10:35.369489 31241 solver.cpp:237] Iteration 0, loss = 2.40118
I0520 15:10:35.369532 31241 solver.cpp:253]     Train net output #0: loss = 2.40118 (* 1 = 2.40118 loss)
I0520 15:10:35.369554 31241 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 15:10:43.612944 31241 solver.cpp:237] Iteration 93, loss = 2.31108
I0520 15:10:43.612983 31241 solver.cpp:253]     Train net output #0: loss = 2.31108 (* 1 = 2.31108 loss)
I0520 15:10:43.613008 31241 sgd_solver.cpp:106] Iteration 93, lr = 0.0025
I0520 15:10:51.857638 31241 solver.cpp:237] Iteration 186, loss = 2.31611
I0520 15:10:51.857692 31241 solver.cpp:253]     Train net output #0: loss = 2.31611 (* 1 = 2.31611 loss)
I0520 15:10:51.857709 31241 sgd_solver.cpp:106] Iteration 186, lr = 0.0025
I0520 15:11:00.114243 31241 solver.cpp:237] Iteration 279, loss = 2.28632
I0520 15:11:00.114281 31241 solver.cpp:253]     Train net output #0: loss = 2.28632 (* 1 = 2.28632 loss)
I0520 15:11:00.114305 31241 sgd_solver.cpp:106] Iteration 279, lr = 0.0025
I0520 15:11:08.360584 31241 solver.cpp:237] Iteration 372, loss = 2.25332
I0520 15:11:08.360733 31241 solver.cpp:253]     Train net output #0: loss = 2.25332 (* 1 = 2.25332 loss)
I0520 15:11:08.360750 31241 sgd_solver.cpp:106] Iteration 372, lr = 0.0025
I0520 15:11:16.603987 31241 solver.cpp:237] Iteration 465, loss = 2.16836
I0520 15:11:16.604038 31241 solver.cpp:253]     Train net output #0: loss = 2.16836 (* 1 = 2.16836 loss)
I0520 15:11:16.604054 31241 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0520 15:11:24.842416 31241 solver.cpp:237] Iteration 558, loss = 2.03044
I0520 15:11:24.842452 31241 solver.cpp:253]     Train net output #0: loss = 2.03044 (* 1 = 2.03044 loss)
I0520 15:11:24.842475 31241 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0520 15:11:55.200794 31241 solver.cpp:237] Iteration 651, loss = 2.02989
I0520 15:11:55.200960 31241 solver.cpp:253]     Train net output #0: loss = 2.02989 (* 1 = 2.02989 loss)
I0520 15:11:55.200978 31241 sgd_solver.cpp:106] Iteration 651, lr = 0.0025
I0520 15:12:03.447695 31241 solver.cpp:237] Iteration 744, loss = 2.10819
I0520 15:12:03.447747 31241 solver.cpp:253]     Train net output #0: loss = 2.10819 (* 1 = 2.10819 loss)
I0520 15:12:03.447775 31241 sgd_solver.cpp:106] Iteration 744, lr = 0.0025
I0520 15:12:11.694480 31241 solver.cpp:237] Iteration 837, loss = 1.95226
I0520 15:12:11.694519 31241 solver.cpp:253]     Train net output #0: loss = 1.95226 (* 1 = 1.95226 loss)
I0520 15:12:11.694537 31241 sgd_solver.cpp:106] Iteration 837, lr = 0.0025
I0520 15:12:19.953438 31241 solver.cpp:237] Iteration 930, loss = 1.91409
I0520 15:12:19.953476 31241 solver.cpp:253]     Train net output #0: loss = 1.91409 (* 1 = 1.91409 loss)
I0520 15:12:19.953495 31241 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0520 15:12:20.487299 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_937.caffemodel
I0520 15:12:20.601028 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_937.solverstate
I0520 15:12:28.281214 31241 solver.cpp:237] Iteration 1023, loss = 1.92486
I0520 15:12:28.281370 31241 solver.cpp:253]     Train net output #0: loss = 1.92486 (* 1 = 1.92486 loss)
I0520 15:12:28.281390 31241 sgd_solver.cpp:106] Iteration 1023, lr = 0.0025
I0520 15:12:36.537662 31241 solver.cpp:237] Iteration 1116, loss = 1.789
I0520 15:12:36.537698 31241 solver.cpp:253]     Train net output #0: loss = 1.789 (* 1 = 1.789 loss)
I0520 15:12:36.537714 31241 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0520 15:12:44.787850 31241 solver.cpp:237] Iteration 1209, loss = 2.00808
I0520 15:12:44.787886 31241 solver.cpp:253]     Train net output #0: loss = 2.00808 (* 1 = 2.00808 loss)
I0520 15:12:44.787910 31241 sgd_solver.cpp:106] Iteration 1209, lr = 0.0025
I0520 15:13:15.126121 31241 solver.cpp:237] Iteration 1302, loss = 1.89809
I0520 15:13:15.126288 31241 solver.cpp:253]     Train net output #0: loss = 1.89809 (* 1 = 1.89809 loss)
I0520 15:13:15.126305 31241 sgd_solver.cpp:106] Iteration 1302, lr = 0.0025
I0520 15:13:23.375085 31241 solver.cpp:237] Iteration 1395, loss = 1.92861
I0520 15:13:23.375143 31241 solver.cpp:253]     Train net output #0: loss = 1.92861 (* 1 = 1.92861 loss)
I0520 15:13:23.375167 31241 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0520 15:13:31.622328 31241 solver.cpp:237] Iteration 1488, loss = 1.78813
I0520 15:13:31.622367 31241 solver.cpp:253]     Train net output #0: loss = 1.78813 (* 1 = 1.78813 loss)
I0520 15:13:31.622385 31241 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0520 15:13:39.874259 31241 solver.cpp:237] Iteration 1581, loss = 1.75346
I0520 15:13:39.874295 31241 solver.cpp:253]     Train net output #0: loss = 1.75346 (* 1 = 1.75346 loss)
I0520 15:13:39.874315 31241 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0520 15:13:48.123463 31241 solver.cpp:237] Iteration 1674, loss = 1.92216
I0520 15:13:48.123638 31241 solver.cpp:253]     Train net output #0: loss = 1.92216 (* 1 = 1.92216 loss)
I0520 15:13:48.123656 31241 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0520 15:13:56.369724 31241 solver.cpp:237] Iteration 1767, loss = 1.87392
I0520 15:13:56.369761 31241 solver.cpp:253]     Train net output #0: loss = 1.87392 (* 1 = 1.87392 loss)
I0520 15:13:56.369778 31241 sgd_solver.cpp:106] Iteration 1767, lr = 0.0025
I0520 15:14:04.612277 31241 solver.cpp:237] Iteration 1860, loss = 1.79227
I0520 15:14:04.612314 31241 solver.cpp:253]     Train net output #0: loss = 1.79227 (* 1 = 1.79227 loss)
I0520 15:14:04.612332 31241 sgd_solver.cpp:106] Iteration 1860, lr = 0.0025
I0520 15:14:05.764971 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_1874.caffemodel
I0520 15:14:05.865273 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_1874.solverstate
I0520 15:14:05.919981 31241 solver.cpp:341] Iteration 1875, Testing net (#0)
I0520 15:14:51.918382 31241 solver.cpp:409]     Test net output #0: accuracy = 0.651821
I0520 15:14:51.918548 31241 solver.cpp:409]     Test net output #1: loss = 1.2041 (* 1 = 1.2041 loss)
I0520 15:15:20.923779 31241 solver.cpp:237] Iteration 1953, loss = 1.60362
I0520 15:15:20.923836 31241 solver.cpp:253]     Train net output #0: loss = 1.60362 (* 1 = 1.60362 loss)
I0520 15:15:20.923863 31241 sgd_solver.cpp:106] Iteration 1953, lr = 0.0025
I0520 15:15:29.168306 31241 solver.cpp:237] Iteration 2046, loss = 1.68298
I0520 15:15:29.168464 31241 solver.cpp:253]     Train net output #0: loss = 1.68298 (* 1 = 1.68298 loss)
I0520 15:15:29.168483 31241 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0520 15:15:37.416559 31241 solver.cpp:237] Iteration 2139, loss = 1.83583
I0520 15:15:37.416595 31241 solver.cpp:253]     Train net output #0: loss = 1.83583 (* 1 = 1.83583 loss)
I0520 15:15:37.416613 31241 sgd_solver.cpp:106] Iteration 2139, lr = 0.0025
I0520 15:15:45.667181 31241 solver.cpp:237] Iteration 2232, loss = 1.92332
I0520 15:15:45.667217 31241 solver.cpp:253]     Train net output #0: loss = 1.92332 (* 1 = 1.92332 loss)
I0520 15:15:45.667235 31241 sgd_solver.cpp:106] Iteration 2232, lr = 0.0025
I0520 15:15:53.917572 31241 solver.cpp:237] Iteration 2325, loss = 1.67964
I0520 15:15:53.917621 31241 solver.cpp:253]     Train net output #0: loss = 1.67964 (* 1 = 1.67964 loss)
I0520 15:15:53.917649 31241 sgd_solver.cpp:106] Iteration 2325, lr = 0.0025
I0520 15:16:02.163363 31241 solver.cpp:237] Iteration 2418, loss = 1.64734
I0520 15:16:02.163511 31241 solver.cpp:253]     Train net output #0: loss = 1.64734 (* 1 = 1.64734 loss)
I0520 15:16:02.163527 31241 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0520 15:16:32.571189 31241 solver.cpp:237] Iteration 2511, loss = 1.59016
I0520 15:16:32.571358 31241 solver.cpp:253]     Train net output #0: loss = 1.59016 (* 1 = 1.59016 loss)
I0520 15:16:32.571375 31241 sgd_solver.cpp:106] Iteration 2511, lr = 0.0025
I0520 15:16:40.818783 31241 solver.cpp:237] Iteration 2604, loss = 1.72561
I0520 15:16:40.818840 31241 solver.cpp:253]     Train net output #0: loss = 1.72561 (* 1 = 1.72561 loss)
I0520 15:16:40.818866 31241 sgd_solver.cpp:106] Iteration 2604, lr = 0.0025
I0520 15:16:49.063536 31241 solver.cpp:237] Iteration 2697, loss = 1.68878
I0520 15:16:49.063572 31241 solver.cpp:253]     Train net output #0: loss = 1.68878 (* 1 = 1.68878 loss)
I0520 15:16:49.063591 31241 sgd_solver.cpp:106] Iteration 2697, lr = 0.0025
I0520 15:16:57.316593 31241 solver.cpp:237] Iteration 2790, loss = 1.69252
I0520 15:16:57.316632 31241 solver.cpp:253]     Train net output #0: loss = 1.69252 (* 1 = 1.69252 loss)
I0520 15:16:57.316648 31241 sgd_solver.cpp:106] Iteration 2790, lr = 0.0025
I0520 15:16:59.091559 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_2811.caffemodel
I0520 15:16:59.193879 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_2811.solverstate
I0520 15:17:05.633419 31241 solver.cpp:237] Iteration 2883, loss = 1.5499
I0520 15:17:05.633597 31241 solver.cpp:253]     Train net output #0: loss = 1.5499 (* 1 = 1.5499 loss)
I0520 15:17:05.633615 31241 sgd_solver.cpp:106] Iteration 2883, lr = 0.0025
I0520 15:17:13.876705 31241 solver.cpp:237] Iteration 2976, loss = 1.58065
I0520 15:17:13.876741 31241 solver.cpp:253]     Train net output #0: loss = 1.58065 (* 1 = 1.58065 loss)
I0520 15:17:13.876760 31241 sgd_solver.cpp:106] Iteration 2976, lr = 0.0025
I0520 15:17:22.118757 31241 solver.cpp:237] Iteration 3069, loss = 1.5561
I0520 15:17:22.118793 31241 solver.cpp:253]     Train net output #0: loss = 1.5561 (* 1 = 1.5561 loss)
I0520 15:17:22.118816 31241 sgd_solver.cpp:106] Iteration 3069, lr = 0.0025
I0520 15:17:52.500828 31241 solver.cpp:237] Iteration 3162, loss = 1.69763
I0520 15:17:52.500998 31241 solver.cpp:253]     Train net output #0: loss = 1.69763 (* 1 = 1.69763 loss)
I0520 15:17:52.501015 31241 sgd_solver.cpp:106] Iteration 3162, lr = 0.0025
I0520 15:18:00.743669 31241 solver.cpp:237] Iteration 3255, loss = 1.5822
I0520 15:18:00.743721 31241 solver.cpp:253]     Train net output #0: loss = 1.5822 (* 1 = 1.5822 loss)
I0520 15:18:00.743741 31241 sgd_solver.cpp:106] Iteration 3255, lr = 0.0025
I0520 15:18:08.993664 31241 solver.cpp:237] Iteration 3348, loss = 1.68096
I0520 15:18:08.993701 31241 solver.cpp:253]     Train net output #0: loss = 1.68096 (* 1 = 1.68096 loss)
I0520 15:18:08.993718 31241 sgd_solver.cpp:106] Iteration 3348, lr = 0.0025
I0520 15:18:17.238716 31241 solver.cpp:237] Iteration 3441, loss = 1.63521
I0520 15:18:17.238754 31241 solver.cpp:253]     Train net output #0: loss = 1.63521 (* 1 = 1.63521 loss)
I0520 15:18:17.238771 31241 sgd_solver.cpp:106] Iteration 3441, lr = 0.0025
I0520 15:18:25.495400 31241 solver.cpp:237] Iteration 3534, loss = 1.61933
I0520 15:18:25.495563 31241 solver.cpp:253]     Train net output #0: loss = 1.61933 (* 1 = 1.61933 loss)
I0520 15:18:25.495580 31241 sgd_solver.cpp:106] Iteration 3534, lr = 0.0025
I0520 15:18:33.750089 31241 solver.cpp:237] Iteration 3627, loss = 1.5756
I0520 15:18:33.750128 31241 solver.cpp:253]     Train net output #0: loss = 1.5756 (* 1 = 1.5756 loss)
I0520 15:18:33.750144 31241 sgd_solver.cpp:106] Iteration 3627, lr = 0.0025
I0520 15:18:42.008652 31241 solver.cpp:237] Iteration 3720, loss = 1.48653
I0520 15:18:42.008689 31241 solver.cpp:253]     Train net output #0: loss = 1.48653 (* 1 = 1.48653 loss)
I0520 15:18:42.008708 31241 sgd_solver.cpp:106] Iteration 3720, lr = 0.0025
I0520 15:18:44.402086 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_3748.caffemodel
I0520 15:18:44.505206 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_3748.solverstate
I0520 15:18:44.650007 31241 solver.cpp:341] Iteration 3750, Testing net (#0)
I0520 15:19:51.541076 31241 solver.cpp:409]     Test net output #0: accuracy = 0.729342
I0520 15:19:51.541260 31241 solver.cpp:409]     Test net output #1: loss = 0.964277 (* 1 = 0.964277 loss)
I0520 15:20:19.300422 31241 solver.cpp:237] Iteration 3813, loss = 1.68363
I0520 15:20:19.300480 31241 solver.cpp:253]     Train net output #0: loss = 1.68363 (* 1 = 1.68363 loss)
I0520 15:20:19.300508 31241 sgd_solver.cpp:106] Iteration 3813, lr = 0.0025
I0520 15:20:27.556093 31241 solver.cpp:237] Iteration 3906, loss = 1.66036
I0520 15:20:27.556263 31241 solver.cpp:253]     Train net output #0: loss = 1.66036 (* 1 = 1.66036 loss)
I0520 15:20:27.556280 31241 sgd_solver.cpp:106] Iteration 3906, lr = 0.0025
I0520 15:20:35.802476 31241 solver.cpp:237] Iteration 3999, loss = 1.66388
I0520 15:20:35.802515 31241 solver.cpp:253]     Train net output #0: loss = 1.66388 (* 1 = 1.66388 loss)
I0520 15:20:35.802531 31241 sgd_solver.cpp:106] Iteration 3999, lr = 0.0025
I0520 15:20:44.052561 31241 solver.cpp:237] Iteration 4092, loss = 1.35901
I0520 15:20:44.052597 31241 solver.cpp:253]     Train net output #0: loss = 1.35901 (* 1 = 1.35901 loss)
I0520 15:20:44.052615 31241 sgd_solver.cpp:106] Iteration 4092, lr = 0.0025
I0520 15:20:52.296572 31241 solver.cpp:237] Iteration 4185, loss = 1.44045
I0520 15:20:52.296627 31241 solver.cpp:253]     Train net output #0: loss = 1.44045 (* 1 = 1.44045 loss)
I0520 15:20:52.296654 31241 sgd_solver.cpp:106] Iteration 4185, lr = 0.0025
I0520 15:21:00.550750 31241 solver.cpp:237] Iteration 4278, loss = 1.62028
I0520 15:21:00.550894 31241 solver.cpp:253]     Train net output #0: loss = 1.62028 (* 1 = 1.62028 loss)
I0520 15:21:00.550911 31241 sgd_solver.cpp:106] Iteration 4278, lr = 0.0025
I0520 15:21:08.793707 31241 solver.cpp:237] Iteration 4371, loss = 1.48712
I0520 15:21:08.793743 31241 solver.cpp:253]     Train net output #0: loss = 1.48712 (* 1 = 1.48712 loss)
I0520 15:21:08.793762 31241 sgd_solver.cpp:106] Iteration 4371, lr = 0.0025
I0520 15:21:39.221658 31241 solver.cpp:237] Iteration 4464, loss = 1.43443
I0520 15:21:39.221827 31241 solver.cpp:253]     Train net output #0: loss = 1.43443 (* 1 = 1.43443 loss)
I0520 15:21:39.221845 31241 sgd_solver.cpp:106] Iteration 4464, lr = 0.0025
I0520 15:21:47.477969 31241 solver.cpp:237] Iteration 4557, loss = 1.45374
I0520 15:21:47.478005 31241 solver.cpp:253]     Train net output #0: loss = 1.45374 (* 1 = 1.45374 loss)
I0520 15:21:47.478024 31241 sgd_solver.cpp:106] Iteration 4557, lr = 0.0025
I0520 15:21:55.734423 31241 solver.cpp:237] Iteration 4650, loss = 1.52531
I0520 15:21:55.734460 31241 solver.cpp:253]     Train net output #0: loss = 1.52531 (* 1 = 1.52531 loss)
I0520 15:21:55.734484 31241 sgd_solver.cpp:106] Iteration 4650, lr = 0.0025
I0520 15:21:58.756602 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_4685.caffemodel
I0520 15:21:58.858978 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_4685.solverstate
I0520 15:22:04.057570 31241 solver.cpp:237] Iteration 4743, loss = 1.53611
I0520 15:22:04.057631 31241 solver.cpp:253]     Train net output #0: loss = 1.53611 (* 1 = 1.53611 loss)
I0520 15:22:04.057657 31241 sgd_solver.cpp:106] Iteration 4743, lr = 0.0025
I0520 15:22:12.297091 31241 solver.cpp:237] Iteration 4836, loss = 1.47484
I0520 15:22:12.297235 31241 solver.cpp:253]     Train net output #0: loss = 1.47484 (* 1 = 1.47484 loss)
I0520 15:22:12.297252 31241 sgd_solver.cpp:106] Iteration 4836, lr = 0.0025
I0520 15:22:20.554847 31241 solver.cpp:237] Iteration 4929, loss = 1.51397
I0520 15:22:20.554884 31241 solver.cpp:253]     Train net output #0: loss = 1.51397 (* 1 = 1.51397 loss)
I0520 15:22:20.554903 31241 sgd_solver.cpp:106] Iteration 4929, lr = 0.0025
I0520 15:22:50.932864 31241 solver.cpp:237] Iteration 5022, loss = 1.48763
I0520 15:22:50.933046 31241 solver.cpp:253]     Train net output #0: loss = 1.48763 (* 1 = 1.48763 loss)
I0520 15:22:50.933063 31241 sgd_solver.cpp:106] Iteration 5022, lr = 0.0025
I0520 15:22:59.177188 31241 solver.cpp:237] Iteration 5115, loss = 1.55721
I0520 15:22:59.177232 31241 solver.cpp:253]     Train net output #0: loss = 1.55721 (* 1 = 1.55721 loss)
I0520 15:22:59.177249 31241 sgd_solver.cpp:106] Iteration 5115, lr = 0.0025
I0520 15:23:07.425135 31241 solver.cpp:237] Iteration 5208, loss = 1.46845
I0520 15:23:07.425171 31241 solver.cpp:253]     Train net output #0: loss = 1.46845 (* 1 = 1.46845 loss)
I0520 15:23:07.425190 31241 sgd_solver.cpp:106] Iteration 5208, lr = 0.0025
I0520 15:23:15.672644 31241 solver.cpp:237] Iteration 5301, loss = 1.4442
I0520 15:23:15.672683 31241 solver.cpp:253]     Train net output #0: loss = 1.4442 (* 1 = 1.4442 loss)
I0520 15:23:15.672699 31241 sgd_solver.cpp:106] Iteration 5301, lr = 0.0025
I0520 15:23:23.935819 31241 solver.cpp:237] Iteration 5394, loss = 1.36726
I0520 15:23:23.935979 31241 solver.cpp:253]     Train net output #0: loss = 1.36726 (* 1 = 1.36726 loss)
I0520 15:23:23.935997 31241 sgd_solver.cpp:106] Iteration 5394, lr = 0.0025
I0520 15:23:32.182052 31241 solver.cpp:237] Iteration 5487, loss = 1.44518
I0520 15:23:32.182090 31241 solver.cpp:253]     Train net output #0: loss = 1.44518 (* 1 = 1.44518 loss)
I0520 15:23:32.182107 31241 sgd_solver.cpp:106] Iteration 5487, lr = 0.0025
I0520 15:23:40.422186 31241 solver.cpp:237] Iteration 5580, loss = 1.36308
I0520 15:23:40.422224 31241 solver.cpp:253]     Train net output #0: loss = 1.36308 (* 1 = 1.36308 loss)
I0520 15:23:40.422247 31241 sgd_solver.cpp:106] Iteration 5580, lr = 0.0025
I0520 15:23:44.055660 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_5622.caffemodel
I0520 15:23:44.155419 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_5622.solverstate
I0520 15:23:44.386055 31241 solver.cpp:341] Iteration 5625, Testing net (#0)
I0520 15:24:30.083453 31241 solver.cpp:409]     Test net output #0: accuracy = 0.78127
I0520 15:24:30.083621 31241 solver.cpp:409]     Test net output #1: loss = 0.732436 (* 1 = 0.732436 loss)
I0520 15:24:56.549496 31241 solver.cpp:237] Iteration 5673, loss = 1.35507
I0520 15:24:56.549557 31241 solver.cpp:253]     Train net output #0: loss = 1.35507 (* 1 = 1.35507 loss)
I0520 15:24:56.549576 31241 sgd_solver.cpp:106] Iteration 5673, lr = 0.0025
I0520 15:25:04.794989 31241 solver.cpp:237] Iteration 5766, loss = 1.42156
I0520 15:25:04.795146 31241 solver.cpp:253]     Train net output #0: loss = 1.42156 (* 1 = 1.42156 loss)
I0520 15:25:04.795166 31241 sgd_solver.cpp:106] Iteration 5766, lr = 0.0025
I0520 15:25:13.033406 31241 solver.cpp:237] Iteration 5859, loss = 1.5043
I0520 15:25:13.033443 31241 solver.cpp:253]     Train net output #0: loss = 1.5043 (* 1 = 1.5043 loss)
I0520 15:25:13.033462 31241 sgd_solver.cpp:106] Iteration 5859, lr = 0.0025
I0520 15:25:21.290273 31241 solver.cpp:237] Iteration 5952, loss = 1.50862
I0520 15:25:21.290308 31241 solver.cpp:253]     Train net output #0: loss = 1.50862 (* 1 = 1.50862 loss)
I0520 15:25:21.290333 31241 sgd_solver.cpp:106] Iteration 5952, lr = 0.0025
I0520 15:25:29.536718 31241 solver.cpp:237] Iteration 6045, loss = 1.42285
I0520 15:25:29.536763 31241 solver.cpp:253]     Train net output #0: loss = 1.42285 (* 1 = 1.42285 loss)
I0520 15:25:29.536792 31241 sgd_solver.cpp:106] Iteration 6045, lr = 0.0025
I0520 15:25:37.775646 31241 solver.cpp:237] Iteration 6138, loss = 1.33129
I0520 15:25:37.775802 31241 solver.cpp:253]     Train net output #0: loss = 1.33129 (* 1 = 1.33129 loss)
I0520 15:25:37.775820 31241 sgd_solver.cpp:106] Iteration 6138, lr = 0.0025
I0520 15:25:46.020180 31241 solver.cpp:237] Iteration 6231, loss = 1.54283
I0520 15:25:46.020216 31241 solver.cpp:253]     Train net output #0: loss = 1.54283 (* 1 = 1.54283 loss)
I0520 15:25:46.020236 31241 sgd_solver.cpp:106] Iteration 6231, lr = 0.0025
I0520 15:26:16.438380 31241 solver.cpp:237] Iteration 6324, loss = 1.40898
I0520 15:26:16.438556 31241 solver.cpp:253]     Train net output #0: loss = 1.40898 (* 1 = 1.40898 loss)
I0520 15:26:16.438573 31241 sgd_solver.cpp:106] Iteration 6324, lr = 0.0025
I0520 15:26:24.688664 31241 solver.cpp:237] Iteration 6417, loss = 1.37189
I0520 15:26:24.688699 31241 solver.cpp:253]     Train net output #0: loss = 1.37189 (* 1 = 1.37189 loss)
I0520 15:26:24.688719 31241 sgd_solver.cpp:106] Iteration 6417, lr = 0.0025
I0520 15:26:32.938776 31241 solver.cpp:237] Iteration 6510, loss = 1.45066
I0520 15:26:32.938812 31241 solver.cpp:253]     Train net output #0: loss = 1.45066 (* 1 = 1.45066 loss)
I0520 15:26:32.938830 31241 sgd_solver.cpp:106] Iteration 6510, lr = 0.0025
I0520 15:26:37.194600 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_6559.caffemodel
I0520 15:26:37.294723 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_6559.solverstate
I0520 15:26:41.258433 31241 solver.cpp:237] Iteration 6603, loss = 1.34429
I0520 15:26:41.258494 31241 solver.cpp:253]     Train net output #0: loss = 1.34429 (* 1 = 1.34429 loss)
I0520 15:26:41.258522 31241 sgd_solver.cpp:106] Iteration 6603, lr = 0.0025
I0520 15:26:49.505401 31241 solver.cpp:237] Iteration 6696, loss = 1.50064
I0520 15:26:49.505550 31241 solver.cpp:253]     Train net output #0: loss = 1.50064 (* 1 = 1.50064 loss)
I0520 15:26:49.505566 31241 sgd_solver.cpp:106] Iteration 6696, lr = 0.0025
I0520 15:26:57.750881 31241 solver.cpp:237] Iteration 6789, loss = 1.37878
I0520 15:26:57.750918 31241 solver.cpp:253]     Train net output #0: loss = 1.37878 (* 1 = 1.37878 loss)
I0520 15:26:57.750936 31241 sgd_solver.cpp:106] Iteration 6789, lr = 0.0025
I0520 15:27:28.302408 31241 solver.cpp:237] Iteration 6882, loss = 1.37822
I0520 15:27:28.302587 31241 solver.cpp:253]     Train net output #0: loss = 1.37822 (* 1 = 1.37822 loss)
I0520 15:27:28.302603 31241 sgd_solver.cpp:106] Iteration 6882, lr = 0.0025
I0520 15:27:36.556128 31241 solver.cpp:237] Iteration 6975, loss = 1.49722
I0520 15:27:36.556164 31241 solver.cpp:253]     Train net output #0: loss = 1.49722 (* 1 = 1.49722 loss)
I0520 15:27:36.556188 31241 sgd_solver.cpp:106] Iteration 6975, lr = 0.0025
I0520 15:27:44.802067 31241 solver.cpp:237] Iteration 7068, loss = 1.29454
I0520 15:27:44.802104 31241 solver.cpp:253]     Train net output #0: loss = 1.29454 (* 1 = 1.29454 loss)
I0520 15:27:44.802124 31241 sgd_solver.cpp:106] Iteration 7068, lr = 0.0025
I0520 15:27:53.058683 31241 solver.cpp:237] Iteration 7161, loss = 1.40064
I0520 15:27:53.058743 31241 solver.cpp:253]     Train net output #0: loss = 1.40064 (* 1 = 1.40064 loss)
I0520 15:27:53.058768 31241 sgd_solver.cpp:106] Iteration 7161, lr = 0.0025
I0520 15:28:01.307404 31241 solver.cpp:237] Iteration 7254, loss = 1.22372
I0520 15:28:01.307555 31241 solver.cpp:253]     Train net output #0: loss = 1.22372 (* 1 = 1.22372 loss)
I0520 15:28:01.307571 31241 sgd_solver.cpp:106] Iteration 7254, lr = 0.0025
I0520 15:28:09.561837 31241 solver.cpp:237] Iteration 7347, loss = 1.31159
I0520 15:28:09.561873 31241 solver.cpp:253]     Train net output #0: loss = 1.31159 (* 1 = 1.31159 loss)
I0520 15:28:09.561892 31241 sgd_solver.cpp:106] Iteration 7347, lr = 0.0025
I0520 15:28:17.820260 31241 solver.cpp:237] Iteration 7440, loss = 1.28235
I0520 15:28:17.820297 31241 solver.cpp:253]     Train net output #0: loss = 1.28235 (* 1 = 1.28235 loss)
I0520 15:28:17.820313 31241 sgd_solver.cpp:106] Iteration 7440, lr = 0.0025
I0520 15:28:22.706677 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_7496.caffemodel
I0520 15:28:22.806598 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_7496.solverstate
I0520 15:28:23.126463 31241 solver.cpp:341] Iteration 7500, Testing net (#0)
I0520 15:29:30.014369 31241 solver.cpp:409]     Test net output #0: accuracy = 0.811667
I0520 15:29:30.014544 31241 solver.cpp:409]     Test net output #1: loss = 0.705817 (* 1 = 0.705817 loss)
I0520 15:29:55.151548 31241 solver.cpp:237] Iteration 7533, loss = 1.23623
I0520 15:29:55.151604 31241 solver.cpp:253]     Train net output #0: loss = 1.23623 (* 1 = 1.23623 loss)
I0520 15:29:55.151630 31241 sgd_solver.cpp:106] Iteration 7533, lr = 0.0025
I0520 15:30:03.398504 31241 solver.cpp:237] Iteration 7626, loss = 1.43918
I0520 15:30:03.398677 31241 solver.cpp:253]     Train net output #0: loss = 1.43918 (* 1 = 1.43918 loss)
I0520 15:30:03.398696 31241 sgd_solver.cpp:106] Iteration 7626, lr = 0.0025
I0520 15:30:11.649758 31241 solver.cpp:237] Iteration 7719, loss = 1.35906
I0520 15:30:11.649794 31241 solver.cpp:253]     Train net output #0: loss = 1.35906 (* 1 = 1.35906 loss)
I0520 15:30:11.649818 31241 sgd_solver.cpp:106] Iteration 7719, lr = 0.0025
I0520 15:30:19.899279 31241 solver.cpp:237] Iteration 7812, loss = 1.36153
I0520 15:30:19.899315 31241 solver.cpp:253]     Train net output #0: loss = 1.36153 (* 1 = 1.36153 loss)
I0520 15:30:19.899334 31241 sgd_solver.cpp:106] Iteration 7812, lr = 0.0025
I0520 15:30:28.144489 31241 solver.cpp:237] Iteration 7905, loss = 1.45386
I0520 15:30:28.144542 31241 solver.cpp:253]     Train net output #0: loss = 1.45386 (* 1 = 1.45386 loss)
I0520 15:30:28.144569 31241 sgd_solver.cpp:106] Iteration 7905, lr = 0.0025
I0520 15:30:36.398807 31241 solver.cpp:237] Iteration 7998, loss = 1.43119
I0520 15:30:36.398955 31241 solver.cpp:253]     Train net output #0: loss = 1.43119 (* 1 = 1.43119 loss)
I0520 15:30:36.398972 31241 sgd_solver.cpp:106] Iteration 7998, lr = 0.0025
I0520 15:30:44.647364 31241 solver.cpp:237] Iteration 8091, loss = 1.26493
I0520 15:30:44.647402 31241 solver.cpp:253]     Train net output #0: loss = 1.26493 (* 1 = 1.26493 loss)
I0520 15:30:44.647418 31241 sgd_solver.cpp:106] Iteration 8091, lr = 0.0025
I0520 15:31:15.060216 31241 solver.cpp:237] Iteration 8184, loss = 1.42451
I0520 15:31:15.060386 31241 solver.cpp:253]     Train net output #0: loss = 1.42451 (* 1 = 1.42451 loss)
I0520 15:31:15.060405 31241 sgd_solver.cpp:106] Iteration 8184, lr = 0.0025
I0520 15:31:23.309366 31241 solver.cpp:237] Iteration 8277, loss = 1.30417
I0520 15:31:23.309402 31241 solver.cpp:253]     Train net output #0: loss = 1.30417 (* 1 = 1.30417 loss)
I0520 15:31:23.309422 31241 sgd_solver.cpp:106] Iteration 8277, lr = 0.0025
I0520 15:31:31.554095 31241 solver.cpp:237] Iteration 8370, loss = 1.31395
I0520 15:31:31.554131 31241 solver.cpp:253]     Train net output #0: loss = 1.31395 (* 1 = 1.31395 loss)
I0520 15:31:31.554155 31241 sgd_solver.cpp:106] Iteration 8370, lr = 0.0025
I0520 15:31:37.057103 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_8433.caffemodel
I0520 15:31:37.160013 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_8433.solverstate
I0520 15:31:39.876797 31241 solver.cpp:237] Iteration 8463, loss = 1.29674
I0520 15:31:39.876857 31241 solver.cpp:253]     Train net output #0: loss = 1.29674 (* 1 = 1.29674 loss)
I0520 15:31:39.876884 31241 sgd_solver.cpp:106] Iteration 8463, lr = 0.0025
I0520 15:31:48.127271 31241 solver.cpp:237] Iteration 8556, loss = 1.35933
I0520 15:31:48.127437 31241 solver.cpp:253]     Train net output #0: loss = 1.35933 (* 1 = 1.35933 loss)
I0520 15:31:48.127454 31241 sgd_solver.cpp:106] Iteration 8556, lr = 0.0025
I0520 15:31:56.372473 31241 solver.cpp:237] Iteration 8649, loss = 1.44547
I0520 15:31:56.372509 31241 solver.cpp:253]     Train net output #0: loss = 1.44547 (* 1 = 1.44547 loss)
I0520 15:31:56.372529 31241 sgd_solver.cpp:106] Iteration 8649, lr = 0.0025
I0520 15:32:04.619359 31241 solver.cpp:237] Iteration 8742, loss = 1.43179
I0520 15:32:04.619417 31241 solver.cpp:253]     Train net output #0: loss = 1.43179 (* 1 = 1.43179 loss)
I0520 15:32:04.619451 31241 sgd_solver.cpp:106] Iteration 8742, lr = 0.0025
I0520 15:32:34.969435 31241 solver.cpp:237] Iteration 8835, loss = 1.33704
I0520 15:32:34.969614 31241 solver.cpp:253]     Train net output #0: loss = 1.33704 (* 1 = 1.33704 loss)
I0520 15:32:34.969632 31241 sgd_solver.cpp:106] Iteration 8835, lr = 0.0025
I0520 15:32:43.214045 31241 solver.cpp:237] Iteration 8928, loss = 1.28283
I0520 15:32:43.214082 31241 solver.cpp:253]     Train net output #0: loss = 1.28283 (* 1 = 1.28283 loss)
I0520 15:32:43.214099 31241 sgd_solver.cpp:106] Iteration 8928, lr = 0.0025
I0520 15:32:51.463748 31241 solver.cpp:237] Iteration 9021, loss = 1.33417
I0520 15:32:51.463784 31241 solver.cpp:253]     Train net output #0: loss = 1.33417 (* 1 = 1.33417 loss)
I0520 15:32:51.463804 31241 sgd_solver.cpp:106] Iteration 9021, lr = 0.0025
I0520 15:32:59.710966 31241 solver.cpp:237] Iteration 9114, loss = 1.28598
I0520 15:32:59.711021 31241 solver.cpp:253]     Train net output #0: loss = 1.28598 (* 1 = 1.28598 loss)
I0520 15:32:59.711048 31241 sgd_solver.cpp:106] Iteration 9114, lr = 0.0025
I0520 15:33:07.953891 31241 solver.cpp:237] Iteration 9207, loss = 1.32963
I0520 15:33:07.954042 31241 solver.cpp:253]     Train net output #0: loss = 1.32963 (* 1 = 1.32963 loss)
I0520 15:33:07.954059 31241 sgd_solver.cpp:106] Iteration 9207, lr = 0.0025
I0520 15:33:16.196130 31241 solver.cpp:237] Iteration 9300, loss = 1.31719
I0520 15:33:16.196166 31241 solver.cpp:253]     Train net output #0: loss = 1.31719 (* 1 = 1.31719 loss)
I0520 15:33:16.196184 31241 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0520 15:33:22.324854 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_9370.caffemodel
I0520 15:33:22.427090 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_9370.solverstate
I0520 15:33:22.838845 31241 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_9375.caffemodel
I0520 15:33:22.940809 31241 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760_iter_9375.solverstate
I0520 15:33:22.969307 31241 solver.cpp:341] Iteration 9375, Testing net (#0)
I0520 15:34:08.923871 31241 solver.cpp:409]     Test net output #0: accuracy = 0.826748
I0520 15:34:08.924039 31241 solver.cpp:409]     Test net output #1: loss = 0.604925 (* 1 = 0.604925 loss)
I0520 15:34:08.924057 31241 solver.cpp:326] Optimization Done.
I0520 15:34:08.924069 31241 caffe.cpp:215] Optimization Done.
Application 11232779 resources: utime ~1277s, stime ~229s, Rss ~5329776, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_160_2016-05-20T11.20.38.479760.solver"
	User time (seconds): 0.56
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:10.69
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15112
	Voluntary context switches: 2812
	Involuntary context switches: 113
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
