2806004
I0520 22:27:49.382341  5881 caffe.cpp:184] Using GPUs 0
I0520 22:27:49.817257  5881 solver.cpp:48] Initializing solver from parameters: 
test_iter: 394
test_interval: 789
base_lr: 0.0025
display: 39
max_iter: 3947
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 394
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299.prototxt"
I0520 22:27:49.850599  5881 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299.prototxt
I0520 22:27:49.880031  5881 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 22:27:49.880092  5881 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 22:27:49.880448  5881 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 380
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:27:49.880627  5881 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:27:49.880651  5881 net.cpp:106] Creating Layer data_hdf5
I0520 22:27:49.880666  5881 net.cpp:411] data_hdf5 -> data
I0520 22:27:49.880699  5881 net.cpp:411] data_hdf5 -> label
I0520 22:27:49.880731  5881 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 22:27:49.906082  5881 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 22:27:49.930321  5881 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 22:28:11.474628  5881 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 22:28:11.485745  5881 net.cpp:150] Setting up data_hdf5
I0520 22:28:11.485787  5881 net.cpp:157] Top shape: 380 1 127 50 (2413000)
I0520 22:28:11.485802  5881 net.cpp:157] Top shape: 380 (380)
I0520 22:28:11.485816  5881 net.cpp:165] Memory required for data: 9653520
I0520 22:28:11.485831  5881 layer_factory.hpp:77] Creating layer conv1
I0520 22:28:11.485864  5881 net.cpp:106] Creating Layer conv1
I0520 22:28:11.485874  5881 net.cpp:454] conv1 <- data
I0520 22:28:11.485896  5881 net.cpp:411] conv1 -> conv1
I0520 22:28:12.374222  5881 net.cpp:150] Setting up conv1
I0520 22:28:12.374269  5881 net.cpp:157] Top shape: 380 12 120 48 (26265600)
I0520 22:28:12.374280  5881 net.cpp:165] Memory required for data: 114715920
I0520 22:28:12.374308  5881 layer_factory.hpp:77] Creating layer relu1
I0520 22:28:12.374330  5881 net.cpp:106] Creating Layer relu1
I0520 22:28:12.374341  5881 net.cpp:454] relu1 <- conv1
I0520 22:28:12.374356  5881 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:28:12.374871  5881 net.cpp:150] Setting up relu1
I0520 22:28:12.374888  5881 net.cpp:157] Top shape: 380 12 120 48 (26265600)
I0520 22:28:12.374898  5881 net.cpp:165] Memory required for data: 219778320
I0520 22:28:12.374909  5881 layer_factory.hpp:77] Creating layer pool1
I0520 22:28:12.374927  5881 net.cpp:106] Creating Layer pool1
I0520 22:28:12.374936  5881 net.cpp:454] pool1 <- conv1
I0520 22:28:12.374949  5881 net.cpp:411] pool1 -> pool1
I0520 22:28:12.375028  5881 net.cpp:150] Setting up pool1
I0520 22:28:12.375042  5881 net.cpp:157] Top shape: 380 12 60 48 (13132800)
I0520 22:28:12.375052  5881 net.cpp:165] Memory required for data: 272309520
I0520 22:28:12.375062  5881 layer_factory.hpp:77] Creating layer conv2
I0520 22:28:12.375082  5881 net.cpp:106] Creating Layer conv2
I0520 22:28:12.375093  5881 net.cpp:454] conv2 <- pool1
I0520 22:28:12.375107  5881 net.cpp:411] conv2 -> conv2
I0520 22:28:12.377777  5881 net.cpp:150] Setting up conv2
I0520 22:28:12.377804  5881 net.cpp:157] Top shape: 380 20 54 46 (18878400)
I0520 22:28:12.377815  5881 net.cpp:165] Memory required for data: 347823120
I0520 22:28:12.377835  5881 layer_factory.hpp:77] Creating layer relu2
I0520 22:28:12.377849  5881 net.cpp:106] Creating Layer relu2
I0520 22:28:12.377859  5881 net.cpp:454] relu2 <- conv2
I0520 22:28:12.377872  5881 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:28:12.378203  5881 net.cpp:150] Setting up relu2
I0520 22:28:12.378218  5881 net.cpp:157] Top shape: 380 20 54 46 (18878400)
I0520 22:28:12.378229  5881 net.cpp:165] Memory required for data: 423336720
I0520 22:28:12.378239  5881 layer_factory.hpp:77] Creating layer pool2
I0520 22:28:12.378252  5881 net.cpp:106] Creating Layer pool2
I0520 22:28:12.378262  5881 net.cpp:454] pool2 <- conv2
I0520 22:28:12.378286  5881 net.cpp:411] pool2 -> pool2
I0520 22:28:12.378356  5881 net.cpp:150] Setting up pool2
I0520 22:28:12.378370  5881 net.cpp:157] Top shape: 380 20 27 46 (9439200)
I0520 22:28:12.378379  5881 net.cpp:165] Memory required for data: 461093520
I0520 22:28:12.378389  5881 layer_factory.hpp:77] Creating layer conv3
I0520 22:28:12.378407  5881 net.cpp:106] Creating Layer conv3
I0520 22:28:12.378419  5881 net.cpp:454] conv3 <- pool2
I0520 22:28:12.378432  5881 net.cpp:411] conv3 -> conv3
I0520 22:28:12.380343  5881 net.cpp:150] Setting up conv3
I0520 22:28:12.380367  5881 net.cpp:157] Top shape: 380 28 22 44 (10299520)
I0520 22:28:12.380379  5881 net.cpp:165] Memory required for data: 502291600
I0520 22:28:12.380398  5881 layer_factory.hpp:77] Creating layer relu3
I0520 22:28:12.380414  5881 net.cpp:106] Creating Layer relu3
I0520 22:28:12.380424  5881 net.cpp:454] relu3 <- conv3
I0520 22:28:12.380436  5881 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:28:12.380904  5881 net.cpp:150] Setting up relu3
I0520 22:28:12.380923  5881 net.cpp:157] Top shape: 380 28 22 44 (10299520)
I0520 22:28:12.380933  5881 net.cpp:165] Memory required for data: 543489680
I0520 22:28:12.380944  5881 layer_factory.hpp:77] Creating layer pool3
I0520 22:28:12.380956  5881 net.cpp:106] Creating Layer pool3
I0520 22:28:12.380966  5881 net.cpp:454] pool3 <- conv3
I0520 22:28:12.380978  5881 net.cpp:411] pool3 -> pool3
I0520 22:28:12.381047  5881 net.cpp:150] Setting up pool3
I0520 22:28:12.381060  5881 net.cpp:157] Top shape: 380 28 11 44 (5149760)
I0520 22:28:12.381069  5881 net.cpp:165] Memory required for data: 564088720
I0520 22:28:12.381078  5881 layer_factory.hpp:77] Creating layer conv4
I0520 22:28:12.381095  5881 net.cpp:106] Creating Layer conv4
I0520 22:28:12.381106  5881 net.cpp:454] conv4 <- pool3
I0520 22:28:12.381119  5881 net.cpp:411] conv4 -> conv4
I0520 22:28:12.383882  5881 net.cpp:150] Setting up conv4
I0520 22:28:12.383910  5881 net.cpp:157] Top shape: 380 36 6 42 (3447360)
I0520 22:28:12.383920  5881 net.cpp:165] Memory required for data: 577878160
I0520 22:28:12.383936  5881 layer_factory.hpp:77] Creating layer relu4
I0520 22:28:12.383950  5881 net.cpp:106] Creating Layer relu4
I0520 22:28:12.383960  5881 net.cpp:454] relu4 <- conv4
I0520 22:28:12.383973  5881 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:28:12.384443  5881 net.cpp:150] Setting up relu4
I0520 22:28:12.384459  5881 net.cpp:157] Top shape: 380 36 6 42 (3447360)
I0520 22:28:12.384470  5881 net.cpp:165] Memory required for data: 591667600
I0520 22:28:12.384480  5881 layer_factory.hpp:77] Creating layer pool4
I0520 22:28:12.384493  5881 net.cpp:106] Creating Layer pool4
I0520 22:28:12.384503  5881 net.cpp:454] pool4 <- conv4
I0520 22:28:12.384516  5881 net.cpp:411] pool4 -> pool4
I0520 22:28:12.384583  5881 net.cpp:150] Setting up pool4
I0520 22:28:12.384596  5881 net.cpp:157] Top shape: 380 36 3 42 (1723680)
I0520 22:28:12.384608  5881 net.cpp:165] Memory required for data: 598562320
I0520 22:28:12.384618  5881 layer_factory.hpp:77] Creating layer ip1
I0520 22:28:12.384639  5881 net.cpp:106] Creating Layer ip1
I0520 22:28:12.384649  5881 net.cpp:454] ip1 <- pool4
I0520 22:28:12.384661  5881 net.cpp:411] ip1 -> ip1
I0520 22:28:12.400115  5881 net.cpp:150] Setting up ip1
I0520 22:28:12.400146  5881 net.cpp:157] Top shape: 380 196 (74480)
I0520 22:28:12.400157  5881 net.cpp:165] Memory required for data: 598860240
I0520 22:28:12.400179  5881 layer_factory.hpp:77] Creating layer relu5
I0520 22:28:12.400195  5881 net.cpp:106] Creating Layer relu5
I0520 22:28:12.400205  5881 net.cpp:454] relu5 <- ip1
I0520 22:28:12.400218  5881 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:28:12.400563  5881 net.cpp:150] Setting up relu5
I0520 22:28:12.400576  5881 net.cpp:157] Top shape: 380 196 (74480)
I0520 22:28:12.400588  5881 net.cpp:165] Memory required for data: 599158160
I0520 22:28:12.400598  5881 layer_factory.hpp:77] Creating layer drop1
I0520 22:28:12.400619  5881 net.cpp:106] Creating Layer drop1
I0520 22:28:12.400629  5881 net.cpp:454] drop1 <- ip1
I0520 22:28:12.400655  5881 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:28:12.400701  5881 net.cpp:150] Setting up drop1
I0520 22:28:12.400713  5881 net.cpp:157] Top shape: 380 196 (74480)
I0520 22:28:12.400723  5881 net.cpp:165] Memory required for data: 599456080
I0520 22:28:12.400733  5881 layer_factory.hpp:77] Creating layer ip2
I0520 22:28:12.400751  5881 net.cpp:106] Creating Layer ip2
I0520 22:28:12.400763  5881 net.cpp:454] ip2 <- ip1
I0520 22:28:12.400775  5881 net.cpp:411] ip2 -> ip2
I0520 22:28:12.401239  5881 net.cpp:150] Setting up ip2
I0520 22:28:12.401252  5881 net.cpp:157] Top shape: 380 98 (37240)
I0520 22:28:12.401262  5881 net.cpp:165] Memory required for data: 599605040
I0520 22:28:12.401278  5881 layer_factory.hpp:77] Creating layer relu6
I0520 22:28:12.401290  5881 net.cpp:106] Creating Layer relu6
I0520 22:28:12.401300  5881 net.cpp:454] relu6 <- ip2
I0520 22:28:12.401312  5881 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:28:12.401836  5881 net.cpp:150] Setting up relu6
I0520 22:28:12.401854  5881 net.cpp:157] Top shape: 380 98 (37240)
I0520 22:28:12.401864  5881 net.cpp:165] Memory required for data: 599754000
I0520 22:28:12.401875  5881 layer_factory.hpp:77] Creating layer drop2
I0520 22:28:12.401887  5881 net.cpp:106] Creating Layer drop2
I0520 22:28:12.401897  5881 net.cpp:454] drop2 <- ip2
I0520 22:28:12.401911  5881 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:28:12.401952  5881 net.cpp:150] Setting up drop2
I0520 22:28:12.401965  5881 net.cpp:157] Top shape: 380 98 (37240)
I0520 22:28:12.401976  5881 net.cpp:165] Memory required for data: 599902960
I0520 22:28:12.401986  5881 layer_factory.hpp:77] Creating layer ip3
I0520 22:28:12.401999  5881 net.cpp:106] Creating Layer ip3
I0520 22:28:12.402009  5881 net.cpp:454] ip3 <- ip2
I0520 22:28:12.402021  5881 net.cpp:411] ip3 -> ip3
I0520 22:28:12.402232  5881 net.cpp:150] Setting up ip3
I0520 22:28:12.402245  5881 net.cpp:157] Top shape: 380 11 (4180)
I0520 22:28:12.402256  5881 net.cpp:165] Memory required for data: 599919680
I0520 22:28:12.402271  5881 layer_factory.hpp:77] Creating layer drop3
I0520 22:28:12.402282  5881 net.cpp:106] Creating Layer drop3
I0520 22:28:12.402292  5881 net.cpp:454] drop3 <- ip3
I0520 22:28:12.402304  5881 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:28:12.402343  5881 net.cpp:150] Setting up drop3
I0520 22:28:12.402355  5881 net.cpp:157] Top shape: 380 11 (4180)
I0520 22:28:12.402366  5881 net.cpp:165] Memory required for data: 599936400
I0520 22:28:12.402376  5881 layer_factory.hpp:77] Creating layer loss
I0520 22:28:12.402395  5881 net.cpp:106] Creating Layer loss
I0520 22:28:12.402405  5881 net.cpp:454] loss <- ip3
I0520 22:28:12.402415  5881 net.cpp:454] loss <- label
I0520 22:28:12.402429  5881 net.cpp:411] loss -> loss
I0520 22:28:12.402446  5881 layer_factory.hpp:77] Creating layer loss
I0520 22:28:12.403090  5881 net.cpp:150] Setting up loss
I0520 22:28:12.403110  5881 net.cpp:157] Top shape: (1)
I0520 22:28:12.403120  5881 net.cpp:160]     with loss weight 1
I0520 22:28:12.403164  5881 net.cpp:165] Memory required for data: 599936404
I0520 22:28:12.403174  5881 net.cpp:226] loss needs backward computation.
I0520 22:28:12.403187  5881 net.cpp:226] drop3 needs backward computation.
I0520 22:28:12.403198  5881 net.cpp:226] ip3 needs backward computation.
I0520 22:28:12.403208  5881 net.cpp:226] drop2 needs backward computation.
I0520 22:28:12.403216  5881 net.cpp:226] relu6 needs backward computation.
I0520 22:28:12.403226  5881 net.cpp:226] ip2 needs backward computation.
I0520 22:28:12.403236  5881 net.cpp:226] drop1 needs backward computation.
I0520 22:28:12.403246  5881 net.cpp:226] relu5 needs backward computation.
I0520 22:28:12.403260  5881 net.cpp:226] ip1 needs backward computation.
I0520 22:28:12.403270  5881 net.cpp:226] pool4 needs backward computation.
I0520 22:28:12.403280  5881 net.cpp:226] relu4 needs backward computation.
I0520 22:28:12.403290  5881 net.cpp:226] conv4 needs backward computation.
I0520 22:28:12.403300  5881 net.cpp:226] pool3 needs backward computation.
I0520 22:28:12.403318  5881 net.cpp:226] relu3 needs backward computation.
I0520 22:28:12.403328  5881 net.cpp:226] conv3 needs backward computation.
I0520 22:28:12.403340  5881 net.cpp:226] pool2 needs backward computation.
I0520 22:28:12.403350  5881 net.cpp:226] relu2 needs backward computation.
I0520 22:28:12.403360  5881 net.cpp:226] conv2 needs backward computation.
I0520 22:28:12.403370  5881 net.cpp:226] pool1 needs backward computation.
I0520 22:28:12.403381  5881 net.cpp:226] relu1 needs backward computation.
I0520 22:28:12.403390  5881 net.cpp:226] conv1 needs backward computation.
I0520 22:28:12.403401  5881 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:28:12.403411  5881 net.cpp:270] This network produces output loss
I0520 22:28:12.403434  5881 net.cpp:283] Network initialization done.
I0520 22:28:12.417587  5881 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299.prototxt
I0520 22:28:12.417665  5881 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 22:28:12.418025  5881 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 380
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 22:28:12.418218  5881 layer_factory.hpp:77] Creating layer data_hdf5
I0520 22:28:12.418236  5881 net.cpp:106] Creating Layer data_hdf5
I0520 22:28:12.418247  5881 net.cpp:411] data_hdf5 -> data
I0520 22:28:12.418265  5881 net.cpp:411] data_hdf5 -> label
I0520 22:28:12.418282  5881 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 22:28:12.426827  5881 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 22:28:33.720441  5881 net.cpp:150] Setting up data_hdf5
I0520 22:28:33.720604  5881 net.cpp:157] Top shape: 380 1 127 50 (2413000)
I0520 22:28:33.720619  5881 net.cpp:157] Top shape: 380 (380)
I0520 22:28:33.720631  5881 net.cpp:165] Memory required for data: 9653520
I0520 22:28:33.720644  5881 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 22:28:33.720671  5881 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 22:28:33.720684  5881 net.cpp:454] label_data_hdf5_1_split <- label
I0520 22:28:33.720698  5881 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 22:28:33.720720  5881 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 22:28:33.720793  5881 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 22:28:33.720808  5881 net.cpp:157] Top shape: 380 (380)
I0520 22:28:33.720818  5881 net.cpp:157] Top shape: 380 (380)
I0520 22:28:33.720829  5881 net.cpp:165] Memory required for data: 9656560
I0520 22:28:33.720839  5881 layer_factory.hpp:77] Creating layer conv1
I0520 22:28:33.720860  5881 net.cpp:106] Creating Layer conv1
I0520 22:28:33.720870  5881 net.cpp:454] conv1 <- data
I0520 22:28:33.720885  5881 net.cpp:411] conv1 -> conv1
I0520 22:28:33.722802  5881 net.cpp:150] Setting up conv1
I0520 22:28:33.722827  5881 net.cpp:157] Top shape: 380 12 120 48 (26265600)
I0520 22:28:33.722838  5881 net.cpp:165] Memory required for data: 114718960
I0520 22:28:33.722861  5881 layer_factory.hpp:77] Creating layer relu1
I0520 22:28:33.722875  5881 net.cpp:106] Creating Layer relu1
I0520 22:28:33.722885  5881 net.cpp:454] relu1 <- conv1
I0520 22:28:33.722898  5881 net.cpp:397] relu1 -> conv1 (in-place)
I0520 22:28:33.723397  5881 net.cpp:150] Setting up relu1
I0520 22:28:33.723413  5881 net.cpp:157] Top shape: 380 12 120 48 (26265600)
I0520 22:28:33.723423  5881 net.cpp:165] Memory required for data: 219781360
I0520 22:28:33.723433  5881 layer_factory.hpp:77] Creating layer pool1
I0520 22:28:33.723450  5881 net.cpp:106] Creating Layer pool1
I0520 22:28:33.723459  5881 net.cpp:454] pool1 <- conv1
I0520 22:28:33.723474  5881 net.cpp:411] pool1 -> pool1
I0520 22:28:33.723548  5881 net.cpp:150] Setting up pool1
I0520 22:28:33.723562  5881 net.cpp:157] Top shape: 380 12 60 48 (13132800)
I0520 22:28:33.723572  5881 net.cpp:165] Memory required for data: 272312560
I0520 22:28:33.723579  5881 layer_factory.hpp:77] Creating layer conv2
I0520 22:28:33.723598  5881 net.cpp:106] Creating Layer conv2
I0520 22:28:33.723608  5881 net.cpp:454] conv2 <- pool1
I0520 22:28:33.723623  5881 net.cpp:411] conv2 -> conv2
I0520 22:28:33.725546  5881 net.cpp:150] Setting up conv2
I0520 22:28:33.725569  5881 net.cpp:157] Top shape: 380 20 54 46 (18878400)
I0520 22:28:33.725581  5881 net.cpp:165] Memory required for data: 347826160
I0520 22:28:33.725600  5881 layer_factory.hpp:77] Creating layer relu2
I0520 22:28:33.725613  5881 net.cpp:106] Creating Layer relu2
I0520 22:28:33.725623  5881 net.cpp:454] relu2 <- conv2
I0520 22:28:33.725636  5881 net.cpp:397] relu2 -> conv2 (in-place)
I0520 22:28:33.725970  5881 net.cpp:150] Setting up relu2
I0520 22:28:33.725983  5881 net.cpp:157] Top shape: 380 20 54 46 (18878400)
I0520 22:28:33.725993  5881 net.cpp:165] Memory required for data: 423339760
I0520 22:28:33.726004  5881 layer_factory.hpp:77] Creating layer pool2
I0520 22:28:33.726018  5881 net.cpp:106] Creating Layer pool2
I0520 22:28:33.726028  5881 net.cpp:454] pool2 <- conv2
I0520 22:28:33.726040  5881 net.cpp:411] pool2 -> pool2
I0520 22:28:33.726111  5881 net.cpp:150] Setting up pool2
I0520 22:28:33.726125  5881 net.cpp:157] Top shape: 380 20 27 46 (9439200)
I0520 22:28:33.726135  5881 net.cpp:165] Memory required for data: 461096560
I0520 22:28:33.726143  5881 layer_factory.hpp:77] Creating layer conv3
I0520 22:28:33.726161  5881 net.cpp:106] Creating Layer conv3
I0520 22:28:33.726172  5881 net.cpp:454] conv3 <- pool2
I0520 22:28:33.726186  5881 net.cpp:411] conv3 -> conv3
I0520 22:28:33.728152  5881 net.cpp:150] Setting up conv3
I0520 22:28:33.728174  5881 net.cpp:157] Top shape: 380 28 22 44 (10299520)
I0520 22:28:33.728186  5881 net.cpp:165] Memory required for data: 502294640
I0520 22:28:33.728219  5881 layer_factory.hpp:77] Creating layer relu3
I0520 22:28:33.728232  5881 net.cpp:106] Creating Layer relu3
I0520 22:28:33.728243  5881 net.cpp:454] relu3 <- conv3
I0520 22:28:33.728256  5881 net.cpp:397] relu3 -> conv3 (in-place)
I0520 22:28:33.728725  5881 net.cpp:150] Setting up relu3
I0520 22:28:33.728741  5881 net.cpp:157] Top shape: 380 28 22 44 (10299520)
I0520 22:28:33.728751  5881 net.cpp:165] Memory required for data: 543492720
I0520 22:28:33.728761  5881 layer_factory.hpp:77] Creating layer pool3
I0520 22:28:33.728775  5881 net.cpp:106] Creating Layer pool3
I0520 22:28:33.728785  5881 net.cpp:454] pool3 <- conv3
I0520 22:28:33.728797  5881 net.cpp:411] pool3 -> pool3
I0520 22:28:33.728869  5881 net.cpp:150] Setting up pool3
I0520 22:28:33.728883  5881 net.cpp:157] Top shape: 380 28 11 44 (5149760)
I0520 22:28:33.728893  5881 net.cpp:165] Memory required for data: 564091760
I0520 22:28:33.728902  5881 layer_factory.hpp:77] Creating layer conv4
I0520 22:28:33.728919  5881 net.cpp:106] Creating Layer conv4
I0520 22:28:33.728929  5881 net.cpp:454] conv4 <- pool3
I0520 22:28:33.728945  5881 net.cpp:411] conv4 -> conv4
I0520 22:28:33.731010  5881 net.cpp:150] Setting up conv4
I0520 22:28:33.731032  5881 net.cpp:157] Top shape: 380 36 6 42 (3447360)
I0520 22:28:33.731042  5881 net.cpp:165] Memory required for data: 577881200
I0520 22:28:33.731058  5881 layer_factory.hpp:77] Creating layer relu4
I0520 22:28:33.731071  5881 net.cpp:106] Creating Layer relu4
I0520 22:28:33.731081  5881 net.cpp:454] relu4 <- conv4
I0520 22:28:33.731094  5881 net.cpp:397] relu4 -> conv4 (in-place)
I0520 22:28:33.731561  5881 net.cpp:150] Setting up relu4
I0520 22:28:33.731577  5881 net.cpp:157] Top shape: 380 36 6 42 (3447360)
I0520 22:28:33.731587  5881 net.cpp:165] Memory required for data: 591670640
I0520 22:28:33.731597  5881 layer_factory.hpp:77] Creating layer pool4
I0520 22:28:33.731611  5881 net.cpp:106] Creating Layer pool4
I0520 22:28:33.731621  5881 net.cpp:454] pool4 <- conv4
I0520 22:28:33.731633  5881 net.cpp:411] pool4 -> pool4
I0520 22:28:33.731705  5881 net.cpp:150] Setting up pool4
I0520 22:28:33.731719  5881 net.cpp:157] Top shape: 380 36 3 42 (1723680)
I0520 22:28:33.731727  5881 net.cpp:165] Memory required for data: 598565360
I0520 22:28:33.731737  5881 layer_factory.hpp:77] Creating layer ip1
I0520 22:28:33.731752  5881 net.cpp:106] Creating Layer ip1
I0520 22:28:33.731763  5881 net.cpp:454] ip1 <- pool4
I0520 22:28:33.731777  5881 net.cpp:411] ip1 -> ip1
I0520 22:28:33.747280  5881 net.cpp:150] Setting up ip1
I0520 22:28:33.747308  5881 net.cpp:157] Top shape: 380 196 (74480)
I0520 22:28:33.747320  5881 net.cpp:165] Memory required for data: 598863280
I0520 22:28:33.747342  5881 layer_factory.hpp:77] Creating layer relu5
I0520 22:28:33.747357  5881 net.cpp:106] Creating Layer relu5
I0520 22:28:33.747369  5881 net.cpp:454] relu5 <- ip1
I0520 22:28:33.747382  5881 net.cpp:397] relu5 -> ip1 (in-place)
I0520 22:28:33.747728  5881 net.cpp:150] Setting up relu5
I0520 22:28:33.747742  5881 net.cpp:157] Top shape: 380 196 (74480)
I0520 22:28:33.747752  5881 net.cpp:165] Memory required for data: 599161200
I0520 22:28:33.747763  5881 layer_factory.hpp:77] Creating layer drop1
I0520 22:28:33.747781  5881 net.cpp:106] Creating Layer drop1
I0520 22:28:33.747792  5881 net.cpp:454] drop1 <- ip1
I0520 22:28:33.747804  5881 net.cpp:397] drop1 -> ip1 (in-place)
I0520 22:28:33.747848  5881 net.cpp:150] Setting up drop1
I0520 22:28:33.747861  5881 net.cpp:157] Top shape: 380 196 (74480)
I0520 22:28:33.747870  5881 net.cpp:165] Memory required for data: 599459120
I0520 22:28:33.747880  5881 layer_factory.hpp:77] Creating layer ip2
I0520 22:28:33.747895  5881 net.cpp:106] Creating Layer ip2
I0520 22:28:33.747905  5881 net.cpp:454] ip2 <- ip1
I0520 22:28:33.747918  5881 net.cpp:411] ip2 -> ip2
I0520 22:28:33.748399  5881 net.cpp:150] Setting up ip2
I0520 22:28:33.748412  5881 net.cpp:157] Top shape: 380 98 (37240)
I0520 22:28:33.748422  5881 net.cpp:165] Memory required for data: 599608080
I0520 22:28:33.748450  5881 layer_factory.hpp:77] Creating layer relu6
I0520 22:28:33.748463  5881 net.cpp:106] Creating Layer relu6
I0520 22:28:33.748472  5881 net.cpp:454] relu6 <- ip2
I0520 22:28:33.748486  5881 net.cpp:397] relu6 -> ip2 (in-place)
I0520 22:28:33.749020  5881 net.cpp:150] Setting up relu6
I0520 22:28:33.749037  5881 net.cpp:157] Top shape: 380 98 (37240)
I0520 22:28:33.749044  5881 net.cpp:165] Memory required for data: 599757040
I0520 22:28:33.749054  5881 layer_factory.hpp:77] Creating layer drop2
I0520 22:28:33.749068  5881 net.cpp:106] Creating Layer drop2
I0520 22:28:33.749078  5881 net.cpp:454] drop2 <- ip2
I0520 22:28:33.749091  5881 net.cpp:397] drop2 -> ip2 (in-place)
I0520 22:28:33.749135  5881 net.cpp:150] Setting up drop2
I0520 22:28:33.749148  5881 net.cpp:157] Top shape: 380 98 (37240)
I0520 22:28:33.749158  5881 net.cpp:165] Memory required for data: 599906000
I0520 22:28:33.749169  5881 layer_factory.hpp:77] Creating layer ip3
I0520 22:28:33.749183  5881 net.cpp:106] Creating Layer ip3
I0520 22:28:33.749193  5881 net.cpp:454] ip3 <- ip2
I0520 22:28:33.749207  5881 net.cpp:411] ip3 -> ip3
I0520 22:28:33.749439  5881 net.cpp:150] Setting up ip3
I0520 22:28:33.749452  5881 net.cpp:157] Top shape: 380 11 (4180)
I0520 22:28:33.749462  5881 net.cpp:165] Memory required for data: 599922720
I0520 22:28:33.749478  5881 layer_factory.hpp:77] Creating layer drop3
I0520 22:28:33.749491  5881 net.cpp:106] Creating Layer drop3
I0520 22:28:33.749501  5881 net.cpp:454] drop3 <- ip3
I0520 22:28:33.749514  5881 net.cpp:397] drop3 -> ip3 (in-place)
I0520 22:28:33.749555  5881 net.cpp:150] Setting up drop3
I0520 22:28:33.749568  5881 net.cpp:157] Top shape: 380 11 (4180)
I0520 22:28:33.749578  5881 net.cpp:165] Memory required for data: 599939440
I0520 22:28:33.749588  5881 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 22:28:33.749600  5881 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 22:28:33.749610  5881 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 22:28:33.749624  5881 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 22:28:33.749639  5881 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 22:28:33.749712  5881 net.cpp:150] Setting up ip3_drop3_0_split
I0520 22:28:33.749725  5881 net.cpp:157] Top shape: 380 11 (4180)
I0520 22:28:33.749737  5881 net.cpp:157] Top shape: 380 11 (4180)
I0520 22:28:33.749747  5881 net.cpp:165] Memory required for data: 599972880
I0520 22:28:33.749758  5881 layer_factory.hpp:77] Creating layer accuracy
I0520 22:28:33.749779  5881 net.cpp:106] Creating Layer accuracy
I0520 22:28:33.749789  5881 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 22:28:33.749800  5881 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 22:28:33.749814  5881 net.cpp:411] accuracy -> accuracy
I0520 22:28:33.749837  5881 net.cpp:150] Setting up accuracy
I0520 22:28:33.749850  5881 net.cpp:157] Top shape: (1)
I0520 22:28:33.749860  5881 net.cpp:165] Memory required for data: 599972884
I0520 22:28:33.749869  5881 layer_factory.hpp:77] Creating layer loss
I0520 22:28:33.749884  5881 net.cpp:106] Creating Layer loss
I0520 22:28:33.749894  5881 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 22:28:33.749905  5881 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 22:28:33.749918  5881 net.cpp:411] loss -> loss
I0520 22:28:33.749936  5881 layer_factory.hpp:77] Creating layer loss
I0520 22:28:33.750425  5881 net.cpp:150] Setting up loss
I0520 22:28:33.750439  5881 net.cpp:157] Top shape: (1)
I0520 22:28:33.750449  5881 net.cpp:160]     with loss weight 1
I0520 22:28:33.750468  5881 net.cpp:165] Memory required for data: 599972888
I0520 22:28:33.750478  5881 net.cpp:226] loss needs backward computation.
I0520 22:28:33.750489  5881 net.cpp:228] accuracy does not need backward computation.
I0520 22:28:33.750500  5881 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 22:28:33.750510  5881 net.cpp:226] drop3 needs backward computation.
I0520 22:28:33.750520  5881 net.cpp:226] ip3 needs backward computation.
I0520 22:28:33.750530  5881 net.cpp:226] drop2 needs backward computation.
I0520 22:28:33.750550  5881 net.cpp:226] relu6 needs backward computation.
I0520 22:28:33.750561  5881 net.cpp:226] ip2 needs backward computation.
I0520 22:28:33.750571  5881 net.cpp:226] drop1 needs backward computation.
I0520 22:28:33.750581  5881 net.cpp:226] relu5 needs backward computation.
I0520 22:28:33.750591  5881 net.cpp:226] ip1 needs backward computation.
I0520 22:28:33.750599  5881 net.cpp:226] pool4 needs backward computation.
I0520 22:28:33.750610  5881 net.cpp:226] relu4 needs backward computation.
I0520 22:28:33.750619  5881 net.cpp:226] conv4 needs backward computation.
I0520 22:28:33.750630  5881 net.cpp:226] pool3 needs backward computation.
I0520 22:28:33.750641  5881 net.cpp:226] relu3 needs backward computation.
I0520 22:28:33.750651  5881 net.cpp:226] conv3 needs backward computation.
I0520 22:28:33.750661  5881 net.cpp:226] pool2 needs backward computation.
I0520 22:28:33.750672  5881 net.cpp:226] relu2 needs backward computation.
I0520 22:28:33.750684  5881 net.cpp:226] conv2 needs backward computation.
I0520 22:28:33.750694  5881 net.cpp:226] pool1 needs backward computation.
I0520 22:28:33.750705  5881 net.cpp:226] relu1 needs backward computation.
I0520 22:28:33.750713  5881 net.cpp:226] conv1 needs backward computation.
I0520 22:28:33.750725  5881 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 22:28:33.750736  5881 net.cpp:228] data_hdf5 does not need backward computation.
I0520 22:28:33.750746  5881 net.cpp:270] This network produces output accuracy
I0520 22:28:33.750756  5881 net.cpp:270] This network produces output loss
I0520 22:28:33.750783  5881 net.cpp:283] Network initialization done.
I0520 22:28:33.750918  5881 solver.cpp:60] Solver scaffolding done.
I0520 22:28:33.752051  5881 caffe.cpp:212] Starting Optimization
I0520 22:28:33.752070  5881 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 22:28:33.752084  5881 solver.cpp:289] Learning Rate Policy: fixed
I0520 22:28:33.753295  5881 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 22:29:19.856293  5881 solver.cpp:409]     Test net output #0: accuracy = 0.0304435
I0520 22:29:19.856454  5881 solver.cpp:409]     Test net output #1: loss = 2.39814 (* 1 = 2.39814 loss)
I0520 22:29:19.935492  5881 solver.cpp:237] Iteration 0, loss = 2.40043
I0520 22:29:19.935528  5881 solver.cpp:253]     Train net output #0: loss = 2.40043 (* 1 = 2.40043 loss)
I0520 22:29:19.935546  5881 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 22:29:27.976658  5881 solver.cpp:237] Iteration 39, loss = 2.35864
I0520 22:29:27.976696  5881 solver.cpp:253]     Train net output #0: loss = 2.35864 (* 1 = 2.35864 loss)
I0520 22:29:27.976711  5881 sgd_solver.cpp:106] Iteration 39, lr = 0.0025
I0520 22:29:36.013027  5881 solver.cpp:237] Iteration 78, loss = 2.33768
I0520 22:29:36.013061  5881 solver.cpp:253]     Train net output #0: loss = 2.33768 (* 1 = 2.33768 loss)
I0520 22:29:36.013077  5881 sgd_solver.cpp:106] Iteration 78, lr = 0.0025
I0520 22:29:44.050456  5881 solver.cpp:237] Iteration 117, loss = 2.30397
I0520 22:29:44.050487  5881 solver.cpp:253]     Train net output #0: loss = 2.30397 (* 1 = 2.30397 loss)
I0520 22:29:44.050504  5881 sgd_solver.cpp:106] Iteration 117, lr = 0.0025
I0520 22:29:52.095899  5881 solver.cpp:237] Iteration 156, loss = 2.328
I0520 22:29:52.096043  5881 solver.cpp:253]     Train net output #0: loss = 2.328 (* 1 = 2.328 loss)
I0520 22:29:52.096057  5881 sgd_solver.cpp:106] Iteration 156, lr = 0.0025
I0520 22:30:00.132802  5881 solver.cpp:237] Iteration 195, loss = 2.30167
I0520 22:30:00.132833  5881 solver.cpp:253]     Train net output #0: loss = 2.30167 (* 1 = 2.30167 loss)
I0520 22:30:00.132855  5881 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0520 22:30:08.175103  5881 solver.cpp:237] Iteration 234, loss = 2.28736
I0520 22:30:08.175137  5881 solver.cpp:253]     Train net output #0: loss = 2.28736 (* 1 = 2.28736 loss)
I0520 22:30:08.175154  5881 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0520 22:30:38.324187  5881 solver.cpp:237] Iteration 273, loss = 2.18794
I0520 22:30:38.324349  5881 solver.cpp:253]     Train net output #0: loss = 2.18794 (* 1 = 2.18794 loss)
I0520 22:30:38.324364  5881 sgd_solver.cpp:106] Iteration 273, lr = 0.0025
I0520 22:30:46.364948  5881 solver.cpp:237] Iteration 312, loss = 2.117
I0520 22:30:46.364979  5881 solver.cpp:253]     Train net output #0: loss = 2.117 (* 1 = 2.117 loss)
I0520 22:30:46.364998  5881 sgd_solver.cpp:106] Iteration 312, lr = 0.0025
I0520 22:30:54.402582  5881 solver.cpp:237] Iteration 351, loss = 2.10072
I0520 22:30:54.402616  5881 solver.cpp:253]     Train net output #0: loss = 2.10072 (* 1 = 2.10072 loss)
I0520 22:30:54.402632  5881 sgd_solver.cpp:106] Iteration 351, lr = 0.0025
I0520 22:31:02.450968  5881 solver.cpp:237] Iteration 390, loss = 2.04392
I0520 22:31:02.451002  5881 solver.cpp:253]     Train net output #0: loss = 2.04392 (* 1 = 2.04392 loss)
I0520 22:31:02.451020  5881 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0520 22:31:03.069993  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_394.caffemodel
I0520 22:31:03.266188  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_394.solverstate
I0520 22:31:10.576867  5881 solver.cpp:237] Iteration 429, loss = 2.03599
I0520 22:31:10.577023  5881 solver.cpp:253]     Train net output #0: loss = 2.03599 (* 1 = 2.03599 loss)
I0520 22:31:10.577038  5881 sgd_solver.cpp:106] Iteration 429, lr = 0.0025
I0520 22:31:18.619823  5881 solver.cpp:237] Iteration 468, loss = 1.92174
I0520 22:31:18.619855  5881 solver.cpp:253]     Train net output #0: loss = 1.92174 (* 1 = 1.92174 loss)
I0520 22:31:18.619870  5881 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0520 22:31:26.661156  5881 solver.cpp:237] Iteration 507, loss = 1.95018
I0520 22:31:26.661188  5881 solver.cpp:253]     Train net output #0: loss = 1.95018 (* 1 = 1.95018 loss)
I0520 22:31:26.661206  5881 sgd_solver.cpp:106] Iteration 507, lr = 0.0025
I0520 22:31:56.852102  5881 solver.cpp:237] Iteration 546, loss = 1.94083
I0520 22:31:56.852258  5881 solver.cpp:253]     Train net output #0: loss = 1.94083 (* 1 = 1.94083 loss)
I0520 22:31:56.852274  5881 sgd_solver.cpp:106] Iteration 546, lr = 0.0025
I0520 22:32:04.897651  5881 solver.cpp:237] Iteration 585, loss = 1.93519
I0520 22:32:04.897694  5881 solver.cpp:253]     Train net output #0: loss = 1.93519 (* 1 = 1.93519 loss)
I0520 22:32:04.897713  5881 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0520 22:32:12.932133  5881 solver.cpp:237] Iteration 624, loss = 1.94587
I0520 22:32:12.932166  5881 solver.cpp:253]     Train net output #0: loss = 1.94587 (* 1 = 1.94587 loss)
I0520 22:32:12.932183  5881 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0520 22:32:20.975514  5881 solver.cpp:237] Iteration 663, loss = 1.90326
I0520 22:32:20.975548  5881 solver.cpp:253]     Train net output #0: loss = 1.90326 (* 1 = 1.90326 loss)
I0520 22:32:20.975561  5881 sgd_solver.cpp:106] Iteration 663, lr = 0.0025
I0520 22:32:29.014943  5881 solver.cpp:237] Iteration 702, loss = 1.85655
I0520 22:32:29.015100  5881 solver.cpp:253]     Train net output #0: loss = 1.85655 (* 1 = 1.85655 loss)
I0520 22:32:29.015113  5881 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0520 22:32:37.055960  5881 solver.cpp:237] Iteration 741, loss = 1.89836
I0520 22:32:37.055994  5881 solver.cpp:253]     Train net output #0: loss = 1.89836 (* 1 = 1.89836 loss)
I0520 22:32:37.056010  5881 sgd_solver.cpp:106] Iteration 741, lr = 0.0025
I0520 22:32:45.107182  5881 solver.cpp:237] Iteration 780, loss = 1.89721
I0520 22:32:45.107214  5881 solver.cpp:253]     Train net output #0: loss = 1.89721 (* 1 = 1.89721 loss)
I0520 22:32:45.107231  5881 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0520 22:32:46.553030  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_788.caffemodel
I0520 22:32:46.742045  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_788.solverstate
I0520 22:32:46.838467  5881 solver.cpp:341] Iteration 789, Testing net (#0)
I0520 22:33:32.152576  5881 solver.cpp:409]     Test net output #0: accuracy = 0.601169
I0520 22:33:32.152737  5881 solver.cpp:409]     Test net output #1: loss = 1.42334 (* 1 = 1.42334 loss)
I0520 22:34:00.607863  5881 solver.cpp:237] Iteration 819, loss = 1.84147
I0520 22:34:00.607913  5881 solver.cpp:253]     Train net output #0: loss = 1.84147 (* 1 = 1.84147 loss)
I0520 22:34:00.607928  5881 sgd_solver.cpp:106] Iteration 819, lr = 0.0025
I0520 22:34:08.640205  5881 solver.cpp:237] Iteration 858, loss = 1.71317
I0520 22:34:08.640352  5881 solver.cpp:253]     Train net output #0: loss = 1.71317 (* 1 = 1.71317 loss)
I0520 22:34:08.640365  5881 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0520 22:34:16.677209  5881 solver.cpp:237] Iteration 897, loss = 1.88904
I0520 22:34:16.677242  5881 solver.cpp:253]     Train net output #0: loss = 1.88904 (* 1 = 1.88904 loss)
I0520 22:34:16.677258  5881 sgd_solver.cpp:106] Iteration 897, lr = 0.0025
I0520 22:34:24.709597  5881 solver.cpp:237] Iteration 936, loss = 1.83031
I0520 22:34:24.709630  5881 solver.cpp:253]     Train net output #0: loss = 1.83031 (* 1 = 1.83031 loss)
I0520 22:34:24.709647  5881 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0520 22:34:32.746410  5881 solver.cpp:237] Iteration 975, loss = 1.82184
I0520 22:34:32.746454  5881 solver.cpp:253]     Train net output #0: loss = 1.82184 (* 1 = 1.82184 loss)
I0520 22:34:32.746469  5881 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0520 22:34:40.781680  5881 solver.cpp:237] Iteration 1014, loss = 1.87385
I0520 22:34:40.781831  5881 solver.cpp:253]     Train net output #0: loss = 1.87385 (* 1 = 1.87385 loss)
I0520 22:34:40.781844  5881 sgd_solver.cpp:106] Iteration 1014, lr = 0.0025
I0520 22:35:11.098686  5881 solver.cpp:237] Iteration 1053, loss = 1.81168
I0520 22:35:11.098853  5881 solver.cpp:253]     Train net output #0: loss = 1.81168 (* 1 = 1.81168 loss)
I0520 22:35:11.098868  5881 sgd_solver.cpp:106] Iteration 1053, lr = 0.0025
I0520 22:35:19.134891  5881 solver.cpp:237] Iteration 1092, loss = 1.84545
I0520 22:35:19.134924  5881 solver.cpp:253]     Train net output #0: loss = 1.84545 (* 1 = 1.84545 loss)
I0520 22:35:19.134939  5881 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0520 22:35:27.166894  5881 solver.cpp:237] Iteration 1131, loss = 1.7342
I0520 22:35:27.166925  5881 solver.cpp:253]     Train net output #0: loss = 1.7342 (* 1 = 1.7342 loss)
I0520 22:35:27.166937  5881 sgd_solver.cpp:106] Iteration 1131, lr = 0.0025
I0520 22:35:35.200124  5881 solver.cpp:237] Iteration 1170, loss = 1.78549
I0520 22:35:35.200156  5881 solver.cpp:253]     Train net output #0: loss = 1.78549 (* 1 = 1.78549 loss)
I0520 22:35:35.200171  5881 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0520 22:35:37.465126  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_1182.caffemodel
I0520 22:35:37.660734  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_1182.solverstate
I0520 22:35:43.321694  5881 solver.cpp:237] Iteration 1209, loss = 1.78332
I0520 22:35:43.321859  5881 solver.cpp:253]     Train net output #0: loss = 1.78332 (* 1 = 1.78332 loss)
I0520 22:35:43.321873  5881 sgd_solver.cpp:106] Iteration 1209, lr = 0.0025
I0520 22:35:51.357597  5881 solver.cpp:237] Iteration 1248, loss = 1.708
I0520 22:35:51.357627  5881 solver.cpp:253]     Train net output #0: loss = 1.708 (* 1 = 1.708 loss)
I0520 22:35:51.357640  5881 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0520 22:35:59.391096  5881 solver.cpp:237] Iteration 1287, loss = 1.71846
I0520 22:35:59.391129  5881 solver.cpp:253]     Train net output #0: loss = 1.71846 (* 1 = 1.71846 loss)
I0520 22:35:59.391144  5881 sgd_solver.cpp:106] Iteration 1287, lr = 0.0025
I0520 22:36:29.669491  5881 solver.cpp:237] Iteration 1326, loss = 1.71532
I0520 22:36:29.669656  5881 solver.cpp:253]     Train net output #0: loss = 1.71532 (* 1 = 1.71532 loss)
I0520 22:36:29.669672  5881 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0520 22:36:37.700417  5881 solver.cpp:237] Iteration 1365, loss = 1.82267
I0520 22:36:37.700461  5881 solver.cpp:253]     Train net output #0: loss = 1.82267 (* 1 = 1.82267 loss)
I0520 22:36:37.700475  5881 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0520 22:36:45.729270  5881 solver.cpp:237] Iteration 1404, loss = 1.70755
I0520 22:36:45.729305  5881 solver.cpp:253]     Train net output #0: loss = 1.70755 (* 1 = 1.70755 loss)
I0520 22:36:45.729320  5881 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0520 22:36:53.757704  5881 solver.cpp:237] Iteration 1443, loss = 1.67078
I0520 22:36:53.757737  5881 solver.cpp:253]     Train net output #0: loss = 1.67078 (* 1 = 1.67078 loss)
I0520 22:36:53.757752  5881 sgd_solver.cpp:106] Iteration 1443, lr = 0.0025
I0520 22:37:01.787333  5881 solver.cpp:237] Iteration 1482, loss = 1.78895
I0520 22:37:01.787484  5881 solver.cpp:253]     Train net output #0: loss = 1.78895 (* 1 = 1.78895 loss)
I0520 22:37:01.787498  5881 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0520 22:37:09.818876  5881 solver.cpp:237] Iteration 1521, loss = 1.70337
I0520 22:37:09.818908  5881 solver.cpp:253]     Train net output #0: loss = 1.70337 (* 1 = 1.70337 loss)
I0520 22:37:09.818924  5881 sgd_solver.cpp:106] Iteration 1521, lr = 0.0025
I0520 22:37:17.854212  5881 solver.cpp:237] Iteration 1560, loss = 1.72802
I0520 22:37:17.854245  5881 solver.cpp:253]     Train net output #0: loss = 1.72802 (* 1 = 1.72802 loss)
I0520 22:37:17.854259  5881 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0520 22:37:20.944660  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_1576.caffemodel
I0520 22:37:21.139926  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_1576.solverstate
I0520 22:37:21.447499  5881 solver.cpp:341] Iteration 1578, Testing net (#0)
I0520 22:38:27.625352  5881 solver.cpp:409]     Test net output #0: accuracy = 0.664901
I0520 22:38:27.625521  5881 solver.cpp:409]     Test net output #1: loss = 1.18765 (* 1 = 1.18765 loss)
I0520 22:38:54.196713  5881 solver.cpp:237] Iteration 1599, loss = 1.6997
I0520 22:38:54.196763  5881 solver.cpp:253]     Train net output #0: loss = 1.6997 (* 1 = 1.6997 loss)
I0520 22:38:54.196776  5881 sgd_solver.cpp:106] Iteration 1599, lr = 0.0025
I0520 22:39:02.241971  5881 solver.cpp:237] Iteration 1638, loss = 1.59905
I0520 22:39:02.242120  5881 solver.cpp:253]     Train net output #0: loss = 1.59905 (* 1 = 1.59905 loss)
I0520 22:39:02.242132  5881 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0520 22:39:10.283288  5881 solver.cpp:237] Iteration 1677, loss = 1.74133
I0520 22:39:10.283330  5881 solver.cpp:253]     Train net output #0: loss = 1.74133 (* 1 = 1.74133 loss)
I0520 22:39:10.283346  5881 sgd_solver.cpp:106] Iteration 1677, lr = 0.0025
I0520 22:39:18.325309  5881 solver.cpp:237] Iteration 1716, loss = 1.68392
I0520 22:39:18.325341  5881 solver.cpp:253]     Train net output #0: loss = 1.68392 (* 1 = 1.68392 loss)
I0520 22:39:18.325356  5881 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0520 22:39:26.362187  5881 solver.cpp:237] Iteration 1755, loss = 1.7663
I0520 22:39:26.362221  5881 solver.cpp:253]     Train net output #0: loss = 1.7663 (* 1 = 1.7663 loss)
I0520 22:39:26.362236  5881 sgd_solver.cpp:106] Iteration 1755, lr = 0.0025
I0520 22:39:34.405562  5881 solver.cpp:237] Iteration 1794, loss = 1.78452
I0520 22:39:34.405715  5881 solver.cpp:253]     Train net output #0: loss = 1.78452 (* 1 = 1.78452 loss)
I0520 22:39:34.405730  5881 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0520 22:39:42.449251  5881 solver.cpp:237] Iteration 1833, loss = 1.66323
I0520 22:39:42.449285  5881 solver.cpp:253]     Train net output #0: loss = 1.66323 (* 1 = 1.66323 loss)
I0520 22:39:42.449301  5881 sgd_solver.cpp:106] Iteration 1833, lr = 0.0025
I0520 22:40:12.652204  5881 solver.cpp:237] Iteration 1872, loss = 1.6663
I0520 22:40:12.652369  5881 solver.cpp:253]     Train net output #0: loss = 1.6663 (* 1 = 1.6663 loss)
I0520 22:40:12.652385  5881 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0520 22:40:20.694643  5881 solver.cpp:237] Iteration 1911, loss = 1.63852
I0520 22:40:20.694684  5881 solver.cpp:253]     Train net output #0: loss = 1.63852 (* 1 = 1.63852 loss)
I0520 22:40:20.694701  5881 sgd_solver.cpp:106] Iteration 1911, lr = 0.0025
I0520 22:40:28.732843  5881 solver.cpp:237] Iteration 1950, loss = 1.6319
I0520 22:40:28.732877  5881 solver.cpp:253]     Train net output #0: loss = 1.6319 (* 1 = 1.6319 loss)
I0520 22:40:28.732892  5881 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0520 22:40:32.650919  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_1970.caffemodel
I0520 22:40:32.835980  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_1970.solverstate
I0520 22:40:36.844532  5881 solver.cpp:237] Iteration 1989, loss = 1.72052
I0520 22:40:36.844583  5881 solver.cpp:253]     Train net output #0: loss = 1.72052 (* 1 = 1.72052 loss)
I0520 22:40:36.844596  5881 sgd_solver.cpp:106] Iteration 1989, lr = 0.0025
I0520 22:40:44.879189  5881 solver.cpp:237] Iteration 2028, loss = 1.64642
I0520 22:40:44.879330  5881 solver.cpp:253]     Train net output #0: loss = 1.64642 (* 1 = 1.64642 loss)
I0520 22:40:44.879344  5881 sgd_solver.cpp:106] Iteration 2028, lr = 0.0025
I0520 22:40:52.919495  5881 solver.cpp:237] Iteration 2067, loss = 1.62628
I0520 22:40:52.919538  5881 solver.cpp:253]     Train net output #0: loss = 1.62628 (* 1 = 1.62628 loss)
I0520 22:40:52.919553  5881 sgd_solver.cpp:106] Iteration 2067, lr = 0.0025
I0520 22:41:23.144266  5881 solver.cpp:237] Iteration 2106, loss = 1.70467
I0520 22:41:23.144439  5881 solver.cpp:253]     Train net output #0: loss = 1.70467 (* 1 = 1.70467 loss)
I0520 22:41:23.144456  5881 sgd_solver.cpp:106] Iteration 2106, lr = 0.0025
I0520 22:41:31.179836  5881 solver.cpp:237] Iteration 2145, loss = 1.54611
I0520 22:41:31.179869  5881 solver.cpp:253]     Train net output #0: loss = 1.54611 (* 1 = 1.54611 loss)
I0520 22:41:31.179884  5881 sgd_solver.cpp:106] Iteration 2145, lr = 0.0025
I0520 22:41:39.217319  5881 solver.cpp:237] Iteration 2184, loss = 1.63291
I0520 22:41:39.217357  5881 solver.cpp:253]     Train net output #0: loss = 1.63291 (* 1 = 1.63291 loss)
I0520 22:41:39.217384  5881 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0520 22:41:47.256378  5881 solver.cpp:237] Iteration 2223, loss = 1.62927
I0520 22:41:47.256410  5881 solver.cpp:253]     Train net output #0: loss = 1.62927 (* 1 = 1.62927 loss)
I0520 22:41:47.256427  5881 sgd_solver.cpp:106] Iteration 2223, lr = 0.0025
I0520 22:41:55.298938  5881 solver.cpp:237] Iteration 2262, loss = 1.67586
I0520 22:41:55.299077  5881 solver.cpp:253]     Train net output #0: loss = 1.67586 (* 1 = 1.67586 loss)
I0520 22:41:55.299090  5881 sgd_solver.cpp:106] Iteration 2262, lr = 0.0025
I0520 22:42:03.334254  5881 solver.cpp:237] Iteration 2301, loss = 1.66703
I0520 22:42:03.334300  5881 solver.cpp:253]     Train net output #0: loss = 1.66703 (* 1 = 1.66703 loss)
I0520 22:42:03.334314  5881 sgd_solver.cpp:106] Iteration 2301, lr = 0.0025
I0520 22:42:11.371920  5881 solver.cpp:237] Iteration 2340, loss = 1.63211
I0520 22:42:11.371953  5881 solver.cpp:253]     Train net output #0: loss = 1.63211 (* 1 = 1.63211 loss)
I0520 22:42:11.371968  5881 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0520 22:42:16.111883  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_2364.caffemodel
I0520 22:42:16.327219  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_2364.solverstate
I0520 22:42:16.846875  5881 solver.cpp:341] Iteration 2367, Testing net (#0)
I0520 22:43:01.833156  5881 solver.cpp:409]     Test net output #0: accuracy = 0.691998
I0520 22:43:01.833317  5881 solver.cpp:409]     Test net output #1: loss = 1.06958 (* 1 = 1.06958 loss)
I0520 22:43:26.550338  5881 solver.cpp:237] Iteration 2379, loss = 1.67079
I0520 22:43:26.550387  5881 solver.cpp:253]     Train net output #0: loss = 1.67079 (* 1 = 1.67079 loss)
I0520 22:43:26.550402  5881 sgd_solver.cpp:106] Iteration 2379, lr = 0.0025
I0520 22:43:34.595340  5881 solver.cpp:237] Iteration 2418, loss = 1.69144
I0520 22:43:34.595506  5881 solver.cpp:253]     Train net output #0: loss = 1.69144 (* 1 = 1.69144 loss)
I0520 22:43:34.595520  5881 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0520 22:43:42.640703  5881 solver.cpp:237] Iteration 2457, loss = 1.63958
I0520 22:43:42.640749  5881 solver.cpp:253]     Train net output #0: loss = 1.63958 (* 1 = 1.63958 loss)
I0520 22:43:42.640764  5881 sgd_solver.cpp:106] Iteration 2457, lr = 0.0025
I0520 22:43:50.687302  5881 solver.cpp:237] Iteration 2496, loss = 1.63566
I0520 22:43:50.687335  5881 solver.cpp:253]     Train net output #0: loss = 1.63566 (* 1 = 1.63566 loss)
I0520 22:43:50.687350  5881 sgd_solver.cpp:106] Iteration 2496, lr = 0.0025
I0520 22:43:58.728915  5881 solver.cpp:237] Iteration 2535, loss = 1.66429
I0520 22:43:58.728948  5881 solver.cpp:253]     Train net output #0: loss = 1.66429 (* 1 = 1.66429 loss)
I0520 22:43:58.728963  5881 sgd_solver.cpp:106] Iteration 2535, lr = 0.0025
I0520 22:44:06.773056  5881 solver.cpp:237] Iteration 2574, loss = 1.71729
I0520 22:44:06.773205  5881 solver.cpp:253]     Train net output #0: loss = 1.71729 (* 1 = 1.71729 loss)
I0520 22:44:06.773219  5881 sgd_solver.cpp:106] Iteration 2574, lr = 0.0025
I0520 22:44:14.813102  5881 solver.cpp:237] Iteration 2613, loss = 1.62272
I0520 22:44:14.813135  5881 solver.cpp:253]     Train net output #0: loss = 1.62272 (* 1 = 1.62272 loss)
I0520 22:44:14.813150  5881 sgd_solver.cpp:106] Iteration 2613, lr = 0.0025
I0520 22:44:45.024492  5881 solver.cpp:237] Iteration 2652, loss = 1.61914
I0520 22:44:45.024667  5881 solver.cpp:253]     Train net output #0: loss = 1.61914 (* 1 = 1.61914 loss)
I0520 22:44:45.024683  5881 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0520 22:44:53.065619  5881 solver.cpp:237] Iteration 2691, loss = 1.71277
I0520 22:44:53.065651  5881 solver.cpp:253]     Train net output #0: loss = 1.71277 (* 1 = 1.71277 loss)
I0520 22:44:53.065666  5881 sgd_solver.cpp:106] Iteration 2691, lr = 0.0025
I0520 22:45:01.104306  5881 solver.cpp:237] Iteration 2730, loss = 1.56855
I0520 22:45:01.104347  5881 solver.cpp:253]     Train net output #0: loss = 1.56855 (* 1 = 1.56855 loss)
I0520 22:45:01.104363  5881 sgd_solver.cpp:106] Iteration 2730, lr = 0.0025
I0520 22:45:06.670799  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_2758.caffemodel
I0520 22:45:06.852892  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_2758.solverstate
I0520 22:45:09.210980  5881 solver.cpp:237] Iteration 2769, loss = 1.67556
I0520 22:45:09.211026  5881 solver.cpp:253]     Train net output #0: loss = 1.67556 (* 1 = 1.67556 loss)
I0520 22:45:09.211041  5881 sgd_solver.cpp:106] Iteration 2769, lr = 0.0025
I0520 22:45:17.252509  5881 solver.cpp:237] Iteration 2808, loss = 1.57708
I0520 22:45:17.252655  5881 solver.cpp:253]     Train net output #0: loss = 1.57708 (* 1 = 1.57708 loss)
I0520 22:45:17.252670  5881 sgd_solver.cpp:106] Iteration 2808, lr = 0.0025
I0520 22:45:25.294807  5881 solver.cpp:237] Iteration 2847, loss = 1.61233
I0520 22:45:25.294848  5881 solver.cpp:253]     Train net output #0: loss = 1.61233 (* 1 = 1.61233 loss)
I0520 22:45:25.294865  5881 sgd_solver.cpp:106] Iteration 2847, lr = 0.0025
I0520 22:45:33.339051  5881 solver.cpp:237] Iteration 2886, loss = 1.51349
I0520 22:45:33.339083  5881 solver.cpp:253]     Train net output #0: loss = 1.51349 (* 1 = 1.51349 loss)
I0520 22:45:33.339098  5881 sgd_solver.cpp:106] Iteration 2886, lr = 0.0025
I0520 22:46:03.567914  5881 solver.cpp:237] Iteration 2925, loss = 1.56814
I0520 22:46:03.568083  5881 solver.cpp:253]     Train net output #0: loss = 1.56814 (* 1 = 1.56814 loss)
I0520 22:46:03.568096  5881 sgd_solver.cpp:106] Iteration 2925, lr = 0.0025
I0520 22:46:11.610333  5881 solver.cpp:237] Iteration 2964, loss = 1.56524
I0520 22:46:11.610376  5881 solver.cpp:253]     Train net output #0: loss = 1.56524 (* 1 = 1.56524 loss)
I0520 22:46:11.610394  5881 sgd_solver.cpp:106] Iteration 2964, lr = 0.0025
I0520 22:46:19.655565  5881 solver.cpp:237] Iteration 3003, loss = 1.58562
I0520 22:46:19.655598  5881 solver.cpp:253]     Train net output #0: loss = 1.58562 (* 1 = 1.58562 loss)
I0520 22:46:19.655613  5881 sgd_solver.cpp:106] Iteration 3003, lr = 0.0025
I0520 22:46:27.698742  5881 solver.cpp:237] Iteration 3042, loss = 1.5954
I0520 22:46:27.698776  5881 solver.cpp:253]     Train net output #0: loss = 1.5954 (* 1 = 1.5954 loss)
I0520 22:46:27.698791  5881 sgd_solver.cpp:106] Iteration 3042, lr = 0.0025
I0520 22:46:35.737476  5881 solver.cpp:237] Iteration 3081, loss = 1.48697
I0520 22:46:35.737627  5881 solver.cpp:253]     Train net output #0: loss = 1.48697 (* 1 = 1.48697 loss)
I0520 22:46:35.737642  5881 sgd_solver.cpp:106] Iteration 3081, lr = 0.0025
I0520 22:46:43.780468  5881 solver.cpp:237] Iteration 3120, loss = 1.49205
I0520 22:46:43.780501  5881 solver.cpp:253]     Train net output #0: loss = 1.49205 (* 1 = 1.49205 loss)
I0520 22:46:43.780517  5881 sgd_solver.cpp:106] Iteration 3120, lr = 0.0025
I0520 22:46:50.177744  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3152.caffemodel
I0520 22:46:50.361239  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3152.solverstate
I0520 22:46:51.069732  5881 solver.cpp:341] Iteration 3156, Testing net (#0)
I0520 22:47:57.249635  5881 solver.cpp:409]     Test net output #0: accuracy = 0.715442
I0520 22:47:57.249809  5881 solver.cpp:409]     Test net output #1: loss = 0.987497 (* 1 = 0.987497 loss)
I0520 22:48:20.075397  5881 solver.cpp:237] Iteration 3159, loss = 1.55971
I0520 22:48:20.075448  5881 solver.cpp:253]     Train net output #0: loss = 1.55971 (* 1 = 1.55971 loss)
I0520 22:48:20.075461  5881 sgd_solver.cpp:106] Iteration 3159, lr = 0.0025
I0520 22:48:28.124179  5881 solver.cpp:237] Iteration 3198, loss = 1.57292
I0520 22:48:28.124330  5881 solver.cpp:253]     Train net output #0: loss = 1.57292 (* 1 = 1.57292 loss)
I0520 22:48:28.124344  5881 sgd_solver.cpp:106] Iteration 3198, lr = 0.0025
I0520 22:48:36.164687  5881 solver.cpp:237] Iteration 3237, loss = 1.48463
I0520 22:48:36.164719  5881 solver.cpp:253]     Train net output #0: loss = 1.48463 (* 1 = 1.48463 loss)
I0520 22:48:36.164734  5881 sgd_solver.cpp:106] Iteration 3237, lr = 0.0025
I0520 22:48:44.206564  5881 solver.cpp:237] Iteration 3276, loss = 1.51719
I0520 22:48:44.206604  5881 solver.cpp:253]     Train net output #0: loss = 1.51719 (* 1 = 1.51719 loss)
I0520 22:48:44.206622  5881 sgd_solver.cpp:106] Iteration 3276, lr = 0.0025
I0520 22:48:52.246162  5881 solver.cpp:237] Iteration 3315, loss = 1.62714
I0520 22:48:52.246196  5881 solver.cpp:253]     Train net output #0: loss = 1.62714 (* 1 = 1.62714 loss)
I0520 22:48:52.246209  5881 sgd_solver.cpp:106] Iteration 3315, lr = 0.0025
I0520 22:49:00.280477  5881 solver.cpp:237] Iteration 3354, loss = 1.62806
I0520 22:49:00.280619  5881 solver.cpp:253]     Train net output #0: loss = 1.62806 (* 1 = 1.62806 loss)
I0520 22:49:00.280632  5881 sgd_solver.cpp:106] Iteration 3354, lr = 0.0025
I0520 22:49:08.326663  5881 solver.cpp:237] Iteration 3393, loss = 1.59372
I0520 22:49:08.326704  5881 solver.cpp:253]     Train net output #0: loss = 1.59372 (* 1 = 1.59372 loss)
I0520 22:49:08.326721  5881 sgd_solver.cpp:106] Iteration 3393, lr = 0.0025
I0520 22:49:38.506429  5881 solver.cpp:237] Iteration 3432, loss = 1.55647
I0520 22:49:38.506595  5881 solver.cpp:253]     Train net output #0: loss = 1.55647 (* 1 = 1.55647 loss)
I0520 22:49:38.506610  5881 sgd_solver.cpp:106] Iteration 3432, lr = 0.0025
I0520 22:49:46.547951  5881 solver.cpp:237] Iteration 3471, loss = 1.53936
I0520 22:49:46.547983  5881 solver.cpp:253]     Train net output #0: loss = 1.53936 (* 1 = 1.53936 loss)
I0520 22:49:46.548001  5881 sgd_solver.cpp:106] Iteration 3471, lr = 0.0025
I0520 22:49:54.583854  5881 solver.cpp:237] Iteration 3510, loss = 1.51384
I0520 22:49:54.583889  5881 solver.cpp:253]     Train net output #0: loss = 1.51384 (* 1 = 1.51384 loss)
I0520 22:49:54.583904  5881 sgd_solver.cpp:106] Iteration 3510, lr = 0.0025
I0520 22:50:01.795969  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3546.caffemodel
I0520 22:50:01.979503  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3546.solverstate
I0520 22:50:02.685925  5881 solver.cpp:237] Iteration 3549, loss = 1.53876
I0520 22:50:02.685973  5881 solver.cpp:253]     Train net output #0: loss = 1.53876 (* 1 = 1.53876 loss)
I0520 22:50:02.685988  5881 sgd_solver.cpp:106] Iteration 3549, lr = 0.0025
I0520 22:50:10.721123  5881 solver.cpp:237] Iteration 3588, loss = 1.53844
I0520 22:50:10.721289  5881 solver.cpp:253]     Train net output #0: loss = 1.53844 (* 1 = 1.53844 loss)
I0520 22:50:10.721303  5881 sgd_solver.cpp:106] Iteration 3588, lr = 0.0025
I0520 22:50:18.762570  5881 solver.cpp:237] Iteration 3627, loss = 1.50456
I0520 22:50:18.762603  5881 solver.cpp:253]     Train net output #0: loss = 1.50456 (* 1 = 1.50456 loss)
I0520 22:50:18.762619  5881 sgd_solver.cpp:106] Iteration 3627, lr = 0.0025
I0520 22:50:26.800164  5881 solver.cpp:237] Iteration 3666, loss = 1.43053
I0520 22:50:26.800206  5881 solver.cpp:253]     Train net output #0: loss = 1.43053 (* 1 = 1.43053 loss)
I0520 22:50:26.800225  5881 sgd_solver.cpp:106] Iteration 3666, lr = 0.0025
I0520 22:50:56.969352  5881 solver.cpp:237] Iteration 3705, loss = 1.38571
I0520 22:50:56.969529  5881 solver.cpp:253]     Train net output #0: loss = 1.38571 (* 1 = 1.38571 loss)
I0520 22:50:56.969545  5881 sgd_solver.cpp:106] Iteration 3705, lr = 0.0025
I0520 22:51:05.011159  5881 solver.cpp:237] Iteration 3744, loss = 1.5547
I0520 22:51:05.011193  5881 solver.cpp:253]     Train net output #0: loss = 1.5547 (* 1 = 1.5547 loss)
I0520 22:51:05.011207  5881 sgd_solver.cpp:106] Iteration 3744, lr = 0.0025
I0520 22:51:13.051930  5881 solver.cpp:237] Iteration 3783, loss = 1.49169
I0520 22:51:13.051975  5881 solver.cpp:253]     Train net output #0: loss = 1.49169 (* 1 = 1.49169 loss)
I0520 22:51:13.051988  5881 sgd_solver.cpp:106] Iteration 3783, lr = 0.0025
I0520 22:51:21.088382  5881 solver.cpp:237] Iteration 3822, loss = 1.45906
I0520 22:51:21.088414  5881 solver.cpp:253]     Train net output #0: loss = 1.45906 (* 1 = 1.45906 loss)
I0520 22:51:21.088429  5881 sgd_solver.cpp:106] Iteration 3822, lr = 0.0025
I0520 22:51:29.129509  5881 solver.cpp:237] Iteration 3861, loss = 1.51971
I0520 22:51:29.129648  5881 solver.cpp:253]     Train net output #0: loss = 1.51971 (* 1 = 1.51971 loss)
I0520 22:51:29.129662  5881 sgd_solver.cpp:106] Iteration 3861, lr = 0.0025
I0520 22:51:37.164813  5881 solver.cpp:237] Iteration 3900, loss = 1.39901
I0520 22:51:37.164855  5881 solver.cpp:253]     Train net output #0: loss = 1.39901 (* 1 = 1.39901 loss)
I0520 22:51:37.164872  5881 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0520 22:51:45.209010  5881 solver.cpp:237] Iteration 3939, loss = 1.39841
I0520 22:51:45.209044  5881 solver.cpp:253]     Train net output #0: loss = 1.39841 (* 1 = 1.39841 loss)
I0520 22:51:45.209059  5881 sgd_solver.cpp:106] Iteration 3939, lr = 0.0025
I0520 22:51:45.209440  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3940.caffemodel
I0520 22:51:45.392976  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3940.solverstate
I0520 22:51:46.308627  5881 solver.cpp:341] Iteration 3945, Testing net (#0)
I0520 22:52:31.619324  5881 solver.cpp:409]     Test net output #0: accuracy = 0.758549
I0520 22:52:31.619483  5881 solver.cpp:409]     Test net output #1: loss = 0.897043 (* 1 = 0.897043 loss)
I0520 22:52:31.888597  5881 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3947.caffemodel
I0520 22:52:32.074111  5881 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299_iter_3947.solverstate
I0520 22:52:32.102139  5881 solver.cpp:326] Optimization Done.
I0520 22:52:32.102167  5881 caffe.cpp:215] Optimization Done.
Application 11235675 resources: utime ~1257s, stime ~227s, Rss ~5329540, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_380_2016-05-20T11.20.46.530299.solver"
	User time (seconds): 0.57
	System time (seconds): 0.13
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:49.44
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15077
	Voluntary context switches: 2805
	Involuntary context switches: 189
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
