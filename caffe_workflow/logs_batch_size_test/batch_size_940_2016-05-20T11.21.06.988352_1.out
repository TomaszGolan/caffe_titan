2806413
I0521 10:20:00.256500 18395 caffe.cpp:184] Using GPUs 0
I0521 10:20:00.688832 18395 solver.cpp:48] Initializing solver from parameters: 
test_iter: 159
test_interval: 319
base_lr: 0.0025
display: 15
max_iter: 1595
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 159
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352.prototxt"
I0521 10:20:00.690275 18395 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352.prototxt
I0521 10:20:00.703228 18395 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 10:20:00.703289 18395 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 10:20:00.703631 18395 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 940
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 10:20:00.703806 18395 layer_factory.hpp:77] Creating layer data_hdf5
I0521 10:20:00.703830 18395 net.cpp:106] Creating Layer data_hdf5
I0521 10:20:00.703845 18395 net.cpp:411] data_hdf5 -> data
I0521 10:20:00.703877 18395 net.cpp:411] data_hdf5 -> label
I0521 10:20:00.703910 18395 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 10:20:00.705127 18395 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 10:20:00.707329 18395 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 10:20:22.200443 18395 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 10:20:22.205535 18395 net.cpp:150] Setting up data_hdf5
I0521 10:20:22.205586 18395 net.cpp:157] Top shape: 940 1 127 50 (5969000)
I0521 10:20:22.205601 18395 net.cpp:157] Top shape: 940 (940)
I0521 10:20:22.205613 18395 net.cpp:165] Memory required for data: 23879760
I0521 10:20:22.205627 18395 layer_factory.hpp:77] Creating layer conv1
I0521 10:20:22.205662 18395 net.cpp:106] Creating Layer conv1
I0521 10:20:22.205672 18395 net.cpp:454] conv1 <- data
I0521 10:20:22.205695 18395 net.cpp:411] conv1 -> conv1
I0521 10:20:22.564853 18395 net.cpp:150] Setting up conv1
I0521 10:20:22.564899 18395 net.cpp:157] Top shape: 940 12 120 48 (64972800)
I0521 10:20:22.564910 18395 net.cpp:165] Memory required for data: 283770960
I0521 10:20:22.564940 18395 layer_factory.hpp:77] Creating layer relu1
I0521 10:20:22.564962 18395 net.cpp:106] Creating Layer relu1
I0521 10:20:22.564973 18395 net.cpp:454] relu1 <- conv1
I0521 10:20:22.564986 18395 net.cpp:397] relu1 -> conv1 (in-place)
I0521 10:20:22.565500 18395 net.cpp:150] Setting up relu1
I0521 10:20:22.565517 18395 net.cpp:157] Top shape: 940 12 120 48 (64972800)
I0521 10:20:22.565527 18395 net.cpp:165] Memory required for data: 543662160
I0521 10:20:22.565536 18395 layer_factory.hpp:77] Creating layer pool1
I0521 10:20:22.565562 18395 net.cpp:106] Creating Layer pool1
I0521 10:20:22.565572 18395 net.cpp:454] pool1 <- conv1
I0521 10:20:22.565585 18395 net.cpp:411] pool1 -> pool1
I0521 10:20:22.565667 18395 net.cpp:150] Setting up pool1
I0521 10:20:22.565681 18395 net.cpp:157] Top shape: 940 12 60 48 (32486400)
I0521 10:20:22.565692 18395 net.cpp:165] Memory required for data: 673607760
I0521 10:20:22.565702 18395 layer_factory.hpp:77] Creating layer conv2
I0521 10:20:22.565724 18395 net.cpp:106] Creating Layer conv2
I0521 10:20:22.565735 18395 net.cpp:454] conv2 <- pool1
I0521 10:20:22.565748 18395 net.cpp:411] conv2 -> conv2
I0521 10:20:22.568392 18395 net.cpp:150] Setting up conv2
I0521 10:20:22.568419 18395 net.cpp:157] Top shape: 940 20 54 46 (46699200)
I0521 10:20:22.568430 18395 net.cpp:165] Memory required for data: 860404560
I0521 10:20:22.568449 18395 layer_factory.hpp:77] Creating layer relu2
I0521 10:20:22.568464 18395 net.cpp:106] Creating Layer relu2
I0521 10:20:22.568475 18395 net.cpp:454] relu2 <- conv2
I0521 10:20:22.568486 18395 net.cpp:397] relu2 -> conv2 (in-place)
I0521 10:20:22.568816 18395 net.cpp:150] Setting up relu2
I0521 10:20:22.568830 18395 net.cpp:157] Top shape: 940 20 54 46 (46699200)
I0521 10:20:22.568840 18395 net.cpp:165] Memory required for data: 1047201360
I0521 10:20:22.568850 18395 layer_factory.hpp:77] Creating layer pool2
I0521 10:20:22.568863 18395 net.cpp:106] Creating Layer pool2
I0521 10:20:22.568873 18395 net.cpp:454] pool2 <- conv2
I0521 10:20:22.568897 18395 net.cpp:411] pool2 -> pool2
I0521 10:20:22.568966 18395 net.cpp:150] Setting up pool2
I0521 10:20:22.568979 18395 net.cpp:157] Top shape: 940 20 27 46 (23349600)
I0521 10:20:22.568989 18395 net.cpp:165] Memory required for data: 1140599760
I0521 10:20:22.569000 18395 layer_factory.hpp:77] Creating layer conv3
I0521 10:20:22.569017 18395 net.cpp:106] Creating Layer conv3
I0521 10:20:22.569028 18395 net.cpp:454] conv3 <- pool2
I0521 10:20:22.569041 18395 net.cpp:411] conv3 -> conv3
I0521 10:20:22.570960 18395 net.cpp:150] Setting up conv3
I0521 10:20:22.570983 18395 net.cpp:157] Top shape: 940 28 22 44 (25477760)
I0521 10:20:22.570996 18395 net.cpp:165] Memory required for data: 1242510800
I0521 10:20:22.571014 18395 layer_factory.hpp:77] Creating layer relu3
I0521 10:20:22.571032 18395 net.cpp:106] Creating Layer relu3
I0521 10:20:22.571040 18395 net.cpp:454] relu3 <- conv3
I0521 10:20:22.571053 18395 net.cpp:397] relu3 -> conv3 (in-place)
I0521 10:20:22.571522 18395 net.cpp:150] Setting up relu3
I0521 10:20:22.571539 18395 net.cpp:157] Top shape: 940 28 22 44 (25477760)
I0521 10:20:22.571549 18395 net.cpp:165] Memory required for data: 1344421840
I0521 10:20:22.571559 18395 layer_factory.hpp:77] Creating layer pool3
I0521 10:20:22.571573 18395 net.cpp:106] Creating Layer pool3
I0521 10:20:22.571583 18395 net.cpp:454] pool3 <- conv3
I0521 10:20:22.571595 18395 net.cpp:411] pool3 -> pool3
I0521 10:20:22.571662 18395 net.cpp:150] Setting up pool3
I0521 10:20:22.571676 18395 net.cpp:157] Top shape: 940 28 11 44 (12738880)
I0521 10:20:22.571686 18395 net.cpp:165] Memory required for data: 1395377360
I0521 10:20:22.571694 18395 layer_factory.hpp:77] Creating layer conv4
I0521 10:20:22.571712 18395 net.cpp:106] Creating Layer conv4
I0521 10:20:22.571722 18395 net.cpp:454] conv4 <- pool3
I0521 10:20:22.571735 18395 net.cpp:411] conv4 -> conv4
I0521 10:20:22.574437 18395 net.cpp:150] Setting up conv4
I0521 10:20:22.574465 18395 net.cpp:157] Top shape: 940 36 6 42 (8527680)
I0521 10:20:22.574476 18395 net.cpp:165] Memory required for data: 1429488080
I0521 10:20:22.574491 18395 layer_factory.hpp:77] Creating layer relu4
I0521 10:20:22.574506 18395 net.cpp:106] Creating Layer relu4
I0521 10:20:22.574517 18395 net.cpp:454] relu4 <- conv4
I0521 10:20:22.574528 18395 net.cpp:397] relu4 -> conv4 (in-place)
I0521 10:20:22.574991 18395 net.cpp:150] Setting up relu4
I0521 10:20:22.575007 18395 net.cpp:157] Top shape: 940 36 6 42 (8527680)
I0521 10:20:22.575018 18395 net.cpp:165] Memory required for data: 1463598800
I0521 10:20:22.575028 18395 layer_factory.hpp:77] Creating layer pool4
I0521 10:20:22.575042 18395 net.cpp:106] Creating Layer pool4
I0521 10:20:22.575052 18395 net.cpp:454] pool4 <- conv4
I0521 10:20:22.575064 18395 net.cpp:411] pool4 -> pool4
I0521 10:20:22.575132 18395 net.cpp:150] Setting up pool4
I0521 10:20:22.575146 18395 net.cpp:157] Top shape: 940 36 3 42 (4263840)
I0521 10:20:22.575157 18395 net.cpp:165] Memory required for data: 1480654160
I0521 10:20:22.575166 18395 layer_factory.hpp:77] Creating layer ip1
I0521 10:20:22.575187 18395 net.cpp:106] Creating Layer ip1
I0521 10:20:22.575197 18395 net.cpp:454] ip1 <- pool4
I0521 10:20:22.575211 18395 net.cpp:411] ip1 -> ip1
I0521 10:20:22.590622 18395 net.cpp:150] Setting up ip1
I0521 10:20:22.590651 18395 net.cpp:157] Top shape: 940 196 (184240)
I0521 10:20:22.590668 18395 net.cpp:165] Memory required for data: 1481391120
I0521 10:20:22.590694 18395 layer_factory.hpp:77] Creating layer relu5
I0521 10:20:22.590709 18395 net.cpp:106] Creating Layer relu5
I0521 10:20:22.590719 18395 net.cpp:454] relu5 <- ip1
I0521 10:20:22.590734 18395 net.cpp:397] relu5 -> ip1 (in-place)
I0521 10:20:22.591078 18395 net.cpp:150] Setting up relu5
I0521 10:20:22.591092 18395 net.cpp:157] Top shape: 940 196 (184240)
I0521 10:20:22.591104 18395 net.cpp:165] Memory required for data: 1482128080
I0521 10:20:22.591114 18395 layer_factory.hpp:77] Creating layer drop1
I0521 10:20:22.591135 18395 net.cpp:106] Creating Layer drop1
I0521 10:20:22.591145 18395 net.cpp:454] drop1 <- ip1
I0521 10:20:22.591171 18395 net.cpp:397] drop1 -> ip1 (in-place)
I0521 10:20:22.591217 18395 net.cpp:150] Setting up drop1
I0521 10:20:22.591231 18395 net.cpp:157] Top shape: 940 196 (184240)
I0521 10:20:22.591241 18395 net.cpp:165] Memory required for data: 1482865040
I0521 10:20:22.591251 18395 layer_factory.hpp:77] Creating layer ip2
I0521 10:20:22.591270 18395 net.cpp:106] Creating Layer ip2
I0521 10:20:22.591280 18395 net.cpp:454] ip2 <- ip1
I0521 10:20:22.591292 18395 net.cpp:411] ip2 -> ip2
I0521 10:20:22.591758 18395 net.cpp:150] Setting up ip2
I0521 10:20:22.591771 18395 net.cpp:157] Top shape: 940 98 (92120)
I0521 10:20:22.591781 18395 net.cpp:165] Memory required for data: 1483233520
I0521 10:20:22.591796 18395 layer_factory.hpp:77] Creating layer relu6
I0521 10:20:22.591809 18395 net.cpp:106] Creating Layer relu6
I0521 10:20:22.591819 18395 net.cpp:454] relu6 <- ip2
I0521 10:20:22.591830 18395 net.cpp:397] relu6 -> ip2 (in-place)
I0521 10:20:22.592350 18395 net.cpp:150] Setting up relu6
I0521 10:20:22.592366 18395 net.cpp:157] Top shape: 940 98 (92120)
I0521 10:20:22.592377 18395 net.cpp:165] Memory required for data: 1483602000
I0521 10:20:22.592389 18395 layer_factory.hpp:77] Creating layer drop2
I0521 10:20:22.592402 18395 net.cpp:106] Creating Layer drop2
I0521 10:20:22.592412 18395 net.cpp:454] drop2 <- ip2
I0521 10:20:22.592424 18395 net.cpp:397] drop2 -> ip2 (in-place)
I0521 10:20:22.592466 18395 net.cpp:150] Setting up drop2
I0521 10:20:22.592480 18395 net.cpp:157] Top shape: 940 98 (92120)
I0521 10:20:22.592490 18395 net.cpp:165] Memory required for data: 1483970480
I0521 10:20:22.592499 18395 layer_factory.hpp:77] Creating layer ip3
I0521 10:20:22.592512 18395 net.cpp:106] Creating Layer ip3
I0521 10:20:22.592525 18395 net.cpp:454] ip3 <- ip2
I0521 10:20:22.592536 18395 net.cpp:411] ip3 -> ip3
I0521 10:20:22.592747 18395 net.cpp:150] Setting up ip3
I0521 10:20:22.592761 18395 net.cpp:157] Top shape: 940 11 (10340)
I0521 10:20:22.592772 18395 net.cpp:165] Memory required for data: 1484011840
I0521 10:20:22.592787 18395 layer_factory.hpp:77] Creating layer drop3
I0521 10:20:22.592798 18395 net.cpp:106] Creating Layer drop3
I0521 10:20:22.592808 18395 net.cpp:454] drop3 <- ip3
I0521 10:20:22.592820 18395 net.cpp:397] drop3 -> ip3 (in-place)
I0521 10:20:22.592859 18395 net.cpp:150] Setting up drop3
I0521 10:20:22.592872 18395 net.cpp:157] Top shape: 940 11 (10340)
I0521 10:20:22.592882 18395 net.cpp:165] Memory required for data: 1484053200
I0521 10:20:22.592891 18395 layer_factory.hpp:77] Creating layer loss
I0521 10:20:22.592911 18395 net.cpp:106] Creating Layer loss
I0521 10:20:22.592921 18395 net.cpp:454] loss <- ip3
I0521 10:20:22.592931 18395 net.cpp:454] loss <- label
I0521 10:20:22.592941 18395 net.cpp:411] loss -> loss
I0521 10:20:22.592958 18395 layer_factory.hpp:77] Creating layer loss
I0521 10:20:22.593624 18395 net.cpp:150] Setting up loss
I0521 10:20:22.593646 18395 net.cpp:157] Top shape: (1)
I0521 10:20:22.593659 18395 net.cpp:160]     with loss weight 1
I0521 10:20:22.593701 18395 net.cpp:165] Memory required for data: 1484053204
I0521 10:20:22.593713 18395 net.cpp:226] loss needs backward computation.
I0521 10:20:22.593724 18395 net.cpp:226] drop3 needs backward computation.
I0521 10:20:22.593734 18395 net.cpp:226] ip3 needs backward computation.
I0521 10:20:22.593744 18395 net.cpp:226] drop2 needs backward computation.
I0521 10:20:22.593755 18395 net.cpp:226] relu6 needs backward computation.
I0521 10:20:22.593765 18395 net.cpp:226] ip2 needs backward computation.
I0521 10:20:22.593775 18395 net.cpp:226] drop1 needs backward computation.
I0521 10:20:22.593786 18395 net.cpp:226] relu5 needs backward computation.
I0521 10:20:22.593794 18395 net.cpp:226] ip1 needs backward computation.
I0521 10:20:22.593806 18395 net.cpp:226] pool4 needs backward computation.
I0521 10:20:22.593816 18395 net.cpp:226] relu4 needs backward computation.
I0521 10:20:22.593824 18395 net.cpp:226] conv4 needs backward computation.
I0521 10:20:22.593835 18395 net.cpp:226] pool3 needs backward computation.
I0521 10:20:22.593854 18395 net.cpp:226] relu3 needs backward computation.
I0521 10:20:22.593864 18395 net.cpp:226] conv3 needs backward computation.
I0521 10:20:22.593875 18395 net.cpp:226] pool2 needs backward computation.
I0521 10:20:22.593886 18395 net.cpp:226] relu2 needs backward computation.
I0521 10:20:22.593896 18395 net.cpp:226] conv2 needs backward computation.
I0521 10:20:22.593907 18395 net.cpp:226] pool1 needs backward computation.
I0521 10:20:22.593917 18395 net.cpp:226] relu1 needs backward computation.
I0521 10:20:22.593927 18395 net.cpp:226] conv1 needs backward computation.
I0521 10:20:22.593938 18395 net.cpp:228] data_hdf5 does not need backward computation.
I0521 10:20:22.593948 18395 net.cpp:270] This network produces output loss
I0521 10:20:22.593971 18395 net.cpp:283] Network initialization done.
I0521 10:20:22.595552 18395 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352.prototxt
I0521 10:20:22.595623 18395 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 10:20:22.595976 18395 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 940
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 10:20:22.596165 18395 layer_factory.hpp:77] Creating layer data_hdf5
I0521 10:20:22.596180 18395 net.cpp:106] Creating Layer data_hdf5
I0521 10:20:22.596191 18395 net.cpp:411] data_hdf5 -> data
I0521 10:20:22.596209 18395 net.cpp:411] data_hdf5 -> label
I0521 10:20:22.596225 18395 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 10:20:22.597399 18395 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 10:20:43.854851 18395 net.cpp:150] Setting up data_hdf5
I0521 10:20:43.855013 18395 net.cpp:157] Top shape: 940 1 127 50 (5969000)
I0521 10:20:43.855027 18395 net.cpp:157] Top shape: 940 (940)
I0521 10:20:43.855037 18395 net.cpp:165] Memory required for data: 23879760
I0521 10:20:43.855051 18395 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 10:20:43.855079 18395 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 10:20:43.855090 18395 net.cpp:454] label_data_hdf5_1_split <- label
I0521 10:20:43.855105 18395 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 10:20:43.855126 18395 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 10:20:43.855200 18395 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 10:20:43.855212 18395 net.cpp:157] Top shape: 940 (940)
I0521 10:20:43.855224 18395 net.cpp:157] Top shape: 940 (940)
I0521 10:20:43.855234 18395 net.cpp:165] Memory required for data: 23887280
I0521 10:20:43.855243 18395 layer_factory.hpp:77] Creating layer conv1
I0521 10:20:43.855265 18395 net.cpp:106] Creating Layer conv1
I0521 10:20:43.855276 18395 net.cpp:454] conv1 <- data
I0521 10:20:43.855291 18395 net.cpp:411] conv1 -> conv1
I0521 10:20:43.857213 18395 net.cpp:150] Setting up conv1
I0521 10:20:43.857236 18395 net.cpp:157] Top shape: 940 12 120 48 (64972800)
I0521 10:20:43.857249 18395 net.cpp:165] Memory required for data: 283778480
I0521 10:20:43.857269 18395 layer_factory.hpp:77] Creating layer relu1
I0521 10:20:43.857283 18395 net.cpp:106] Creating Layer relu1
I0521 10:20:43.857293 18395 net.cpp:454] relu1 <- conv1
I0521 10:20:43.857306 18395 net.cpp:397] relu1 -> conv1 (in-place)
I0521 10:20:43.857811 18395 net.cpp:150] Setting up relu1
I0521 10:20:43.857827 18395 net.cpp:157] Top shape: 940 12 120 48 (64972800)
I0521 10:20:43.857838 18395 net.cpp:165] Memory required for data: 543669680
I0521 10:20:43.857849 18395 layer_factory.hpp:77] Creating layer pool1
I0521 10:20:43.857866 18395 net.cpp:106] Creating Layer pool1
I0521 10:20:43.857875 18395 net.cpp:454] pool1 <- conv1
I0521 10:20:43.857888 18395 net.cpp:411] pool1 -> pool1
I0521 10:20:43.857964 18395 net.cpp:150] Setting up pool1
I0521 10:20:43.857977 18395 net.cpp:157] Top shape: 940 12 60 48 (32486400)
I0521 10:20:43.857987 18395 net.cpp:165] Memory required for data: 673615280
I0521 10:20:43.857997 18395 layer_factory.hpp:77] Creating layer conv2
I0521 10:20:43.858016 18395 net.cpp:106] Creating Layer conv2
I0521 10:20:43.858026 18395 net.cpp:454] conv2 <- pool1
I0521 10:20:43.858041 18395 net.cpp:411] conv2 -> conv2
I0521 10:20:43.859956 18395 net.cpp:150] Setting up conv2
I0521 10:20:43.859978 18395 net.cpp:157] Top shape: 940 20 54 46 (46699200)
I0521 10:20:43.859990 18395 net.cpp:165] Memory required for data: 860412080
I0521 10:20:43.860008 18395 layer_factory.hpp:77] Creating layer relu2
I0521 10:20:43.860023 18395 net.cpp:106] Creating Layer relu2
I0521 10:20:43.860031 18395 net.cpp:454] relu2 <- conv2
I0521 10:20:43.860044 18395 net.cpp:397] relu2 -> conv2 (in-place)
I0521 10:20:43.860379 18395 net.cpp:150] Setting up relu2
I0521 10:20:43.860394 18395 net.cpp:157] Top shape: 940 20 54 46 (46699200)
I0521 10:20:43.860404 18395 net.cpp:165] Memory required for data: 1047208880
I0521 10:20:43.860414 18395 layer_factory.hpp:77] Creating layer pool2
I0521 10:20:43.860427 18395 net.cpp:106] Creating Layer pool2
I0521 10:20:43.860437 18395 net.cpp:454] pool2 <- conv2
I0521 10:20:43.860450 18395 net.cpp:411] pool2 -> pool2
I0521 10:20:43.860522 18395 net.cpp:150] Setting up pool2
I0521 10:20:43.860535 18395 net.cpp:157] Top shape: 940 20 27 46 (23349600)
I0521 10:20:43.860544 18395 net.cpp:165] Memory required for data: 1140607280
I0521 10:20:43.860554 18395 layer_factory.hpp:77] Creating layer conv3
I0521 10:20:43.860574 18395 net.cpp:106] Creating Layer conv3
I0521 10:20:43.860585 18395 net.cpp:454] conv3 <- pool2
I0521 10:20:43.860599 18395 net.cpp:411] conv3 -> conv3
I0521 10:20:43.862579 18395 net.cpp:150] Setting up conv3
I0521 10:20:43.862603 18395 net.cpp:157] Top shape: 940 28 22 44 (25477760)
I0521 10:20:43.862614 18395 net.cpp:165] Memory required for data: 1242518320
I0521 10:20:43.862646 18395 layer_factory.hpp:77] Creating layer relu3
I0521 10:20:43.862660 18395 net.cpp:106] Creating Layer relu3
I0521 10:20:43.862670 18395 net.cpp:454] relu3 <- conv3
I0521 10:20:43.862684 18395 net.cpp:397] relu3 -> conv3 (in-place)
I0521 10:20:43.863157 18395 net.cpp:150] Setting up relu3
I0521 10:20:43.863173 18395 net.cpp:157] Top shape: 940 28 22 44 (25477760)
I0521 10:20:43.863181 18395 net.cpp:165] Memory required for data: 1344429360
I0521 10:20:43.863191 18395 layer_factory.hpp:77] Creating layer pool3
I0521 10:20:43.863204 18395 net.cpp:106] Creating Layer pool3
I0521 10:20:43.863214 18395 net.cpp:454] pool3 <- conv3
I0521 10:20:43.863227 18395 net.cpp:411] pool3 -> pool3
I0521 10:20:43.863301 18395 net.cpp:150] Setting up pool3
I0521 10:20:43.863313 18395 net.cpp:157] Top shape: 940 28 11 44 (12738880)
I0521 10:20:43.863323 18395 net.cpp:165] Memory required for data: 1395384880
I0521 10:20:43.863333 18395 layer_factory.hpp:77] Creating layer conv4
I0521 10:20:43.863348 18395 net.cpp:106] Creating Layer conv4
I0521 10:20:43.863359 18395 net.cpp:454] conv4 <- pool3
I0521 10:20:43.863373 18395 net.cpp:411] conv4 -> conv4
I0521 10:20:43.865429 18395 net.cpp:150] Setting up conv4
I0521 10:20:43.865452 18395 net.cpp:157] Top shape: 940 36 6 42 (8527680)
I0521 10:20:43.865463 18395 net.cpp:165] Memory required for data: 1429495600
I0521 10:20:43.865478 18395 layer_factory.hpp:77] Creating layer relu4
I0521 10:20:43.865492 18395 net.cpp:106] Creating Layer relu4
I0521 10:20:43.865502 18395 net.cpp:454] relu4 <- conv4
I0521 10:20:43.865515 18395 net.cpp:397] relu4 -> conv4 (in-place)
I0521 10:20:43.865998 18395 net.cpp:150] Setting up relu4
I0521 10:20:43.866015 18395 net.cpp:157] Top shape: 940 36 6 42 (8527680)
I0521 10:20:43.866025 18395 net.cpp:165] Memory required for data: 1463606320
I0521 10:20:43.866035 18395 layer_factory.hpp:77] Creating layer pool4
I0521 10:20:43.866049 18395 net.cpp:106] Creating Layer pool4
I0521 10:20:43.866057 18395 net.cpp:454] pool4 <- conv4
I0521 10:20:43.866072 18395 net.cpp:411] pool4 -> pool4
I0521 10:20:43.866143 18395 net.cpp:150] Setting up pool4
I0521 10:20:43.866156 18395 net.cpp:157] Top shape: 940 36 3 42 (4263840)
I0521 10:20:43.866166 18395 net.cpp:165] Memory required for data: 1480661680
I0521 10:20:43.866174 18395 layer_factory.hpp:77] Creating layer ip1
I0521 10:20:43.866190 18395 net.cpp:106] Creating Layer ip1
I0521 10:20:43.866200 18395 net.cpp:454] ip1 <- pool4
I0521 10:20:43.866215 18395 net.cpp:411] ip1 -> ip1
I0521 10:20:43.881587 18395 net.cpp:150] Setting up ip1
I0521 10:20:43.881616 18395 net.cpp:157] Top shape: 940 196 (184240)
I0521 10:20:43.881628 18395 net.cpp:165] Memory required for data: 1481398640
I0521 10:20:43.881649 18395 layer_factory.hpp:77] Creating layer relu5
I0521 10:20:43.881664 18395 net.cpp:106] Creating Layer relu5
I0521 10:20:43.881675 18395 net.cpp:454] relu5 <- ip1
I0521 10:20:43.881690 18395 net.cpp:397] relu5 -> ip1 (in-place)
I0521 10:20:43.882033 18395 net.cpp:150] Setting up relu5
I0521 10:20:43.882047 18395 net.cpp:157] Top shape: 940 196 (184240)
I0521 10:20:43.882056 18395 net.cpp:165] Memory required for data: 1482135600
I0521 10:20:43.882066 18395 layer_factory.hpp:77] Creating layer drop1
I0521 10:20:43.882086 18395 net.cpp:106] Creating Layer drop1
I0521 10:20:43.882097 18395 net.cpp:454] drop1 <- ip1
I0521 10:20:43.882109 18395 net.cpp:397] drop1 -> ip1 (in-place)
I0521 10:20:43.882153 18395 net.cpp:150] Setting up drop1
I0521 10:20:43.882165 18395 net.cpp:157] Top shape: 940 196 (184240)
I0521 10:20:43.882174 18395 net.cpp:165] Memory required for data: 1482872560
I0521 10:20:43.882185 18395 layer_factory.hpp:77] Creating layer ip2
I0521 10:20:43.882200 18395 net.cpp:106] Creating Layer ip2
I0521 10:20:43.882208 18395 net.cpp:454] ip2 <- ip1
I0521 10:20:43.882222 18395 net.cpp:411] ip2 -> ip2
I0521 10:20:43.882699 18395 net.cpp:150] Setting up ip2
I0521 10:20:43.882712 18395 net.cpp:157] Top shape: 940 98 (92120)
I0521 10:20:43.882722 18395 net.cpp:165] Memory required for data: 1483241040
I0521 10:20:43.882750 18395 layer_factory.hpp:77] Creating layer relu6
I0521 10:20:43.882763 18395 net.cpp:106] Creating Layer relu6
I0521 10:20:43.882772 18395 net.cpp:454] relu6 <- ip2
I0521 10:20:43.882786 18395 net.cpp:397] relu6 -> ip2 (in-place)
I0521 10:20:43.883317 18395 net.cpp:150] Setting up relu6
I0521 10:20:43.883340 18395 net.cpp:157] Top shape: 940 98 (92120)
I0521 10:20:43.883350 18395 net.cpp:165] Memory required for data: 1483609520
I0521 10:20:43.883360 18395 layer_factory.hpp:77] Creating layer drop2
I0521 10:20:43.883374 18395 net.cpp:106] Creating Layer drop2
I0521 10:20:43.883384 18395 net.cpp:454] drop2 <- ip2
I0521 10:20:43.883396 18395 net.cpp:397] drop2 -> ip2 (in-place)
I0521 10:20:43.883440 18395 net.cpp:150] Setting up drop2
I0521 10:20:43.883453 18395 net.cpp:157] Top shape: 940 98 (92120)
I0521 10:20:43.883463 18395 net.cpp:165] Memory required for data: 1483978000
I0521 10:20:43.883472 18395 layer_factory.hpp:77] Creating layer ip3
I0521 10:20:43.883486 18395 net.cpp:106] Creating Layer ip3
I0521 10:20:43.883496 18395 net.cpp:454] ip3 <- ip2
I0521 10:20:43.883510 18395 net.cpp:411] ip3 -> ip3
I0521 10:20:43.883734 18395 net.cpp:150] Setting up ip3
I0521 10:20:43.883749 18395 net.cpp:157] Top shape: 940 11 (10340)
I0521 10:20:43.883759 18395 net.cpp:165] Memory required for data: 1484019360
I0521 10:20:43.883774 18395 layer_factory.hpp:77] Creating layer drop3
I0521 10:20:43.883787 18395 net.cpp:106] Creating Layer drop3
I0521 10:20:43.883796 18395 net.cpp:454] drop3 <- ip3
I0521 10:20:43.883810 18395 net.cpp:397] drop3 -> ip3 (in-place)
I0521 10:20:43.883852 18395 net.cpp:150] Setting up drop3
I0521 10:20:43.883863 18395 net.cpp:157] Top shape: 940 11 (10340)
I0521 10:20:43.883873 18395 net.cpp:165] Memory required for data: 1484060720
I0521 10:20:43.883882 18395 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 10:20:43.883895 18395 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 10:20:43.883905 18395 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 10:20:43.883919 18395 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 10:20:43.883934 18395 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 10:20:43.884006 18395 net.cpp:150] Setting up ip3_drop3_0_split
I0521 10:20:43.884019 18395 net.cpp:157] Top shape: 940 11 (10340)
I0521 10:20:43.884032 18395 net.cpp:157] Top shape: 940 11 (10340)
I0521 10:20:43.884042 18395 net.cpp:165] Memory required for data: 1484143440
I0521 10:20:43.884050 18395 layer_factory.hpp:77] Creating layer accuracy
I0521 10:20:43.884073 18395 net.cpp:106] Creating Layer accuracy
I0521 10:20:43.884083 18395 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 10:20:43.884093 18395 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 10:20:43.884107 18395 net.cpp:411] accuracy -> accuracy
I0521 10:20:43.884130 18395 net.cpp:150] Setting up accuracy
I0521 10:20:43.884143 18395 net.cpp:157] Top shape: (1)
I0521 10:20:43.884153 18395 net.cpp:165] Memory required for data: 1484143444
I0521 10:20:43.884163 18395 layer_factory.hpp:77] Creating layer loss
I0521 10:20:43.884176 18395 net.cpp:106] Creating Layer loss
I0521 10:20:43.884186 18395 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 10:20:43.884197 18395 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 10:20:43.884210 18395 net.cpp:411] loss -> loss
I0521 10:20:43.884228 18395 layer_factory.hpp:77] Creating layer loss
I0521 10:20:43.884724 18395 net.cpp:150] Setting up loss
I0521 10:20:43.884738 18395 net.cpp:157] Top shape: (1)
I0521 10:20:43.884748 18395 net.cpp:160]     with loss weight 1
I0521 10:20:43.884766 18395 net.cpp:165] Memory required for data: 1484143448
I0521 10:20:43.884776 18395 net.cpp:226] loss needs backward computation.
I0521 10:20:43.884788 18395 net.cpp:228] accuracy does not need backward computation.
I0521 10:20:43.884799 18395 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 10:20:43.884809 18395 net.cpp:226] drop3 needs backward computation.
I0521 10:20:43.884819 18395 net.cpp:226] ip3 needs backward computation.
I0521 10:20:43.884837 18395 net.cpp:226] drop2 needs backward computation.
I0521 10:20:43.884848 18395 net.cpp:226] relu6 needs backward computation.
I0521 10:20:43.884857 18395 net.cpp:226] ip2 needs backward computation.
I0521 10:20:43.884867 18395 net.cpp:226] drop1 needs backward computation.
I0521 10:20:43.884877 18395 net.cpp:226] relu5 needs backward computation.
I0521 10:20:43.884886 18395 net.cpp:226] ip1 needs backward computation.
I0521 10:20:43.884896 18395 net.cpp:226] pool4 needs backward computation.
I0521 10:20:43.884907 18395 net.cpp:226] relu4 needs backward computation.
I0521 10:20:43.884917 18395 net.cpp:226] conv4 needs backward computation.
I0521 10:20:43.884927 18395 net.cpp:226] pool3 needs backward computation.
I0521 10:20:43.884938 18395 net.cpp:226] relu3 needs backward computation.
I0521 10:20:43.884948 18395 net.cpp:226] conv3 needs backward computation.
I0521 10:20:43.884958 18395 net.cpp:226] pool2 needs backward computation.
I0521 10:20:43.884969 18395 net.cpp:226] relu2 needs backward computation.
I0521 10:20:43.884979 18395 net.cpp:226] conv2 needs backward computation.
I0521 10:20:43.884989 18395 net.cpp:226] pool1 needs backward computation.
I0521 10:20:43.885000 18395 net.cpp:226] relu1 needs backward computation.
I0521 10:20:43.885010 18395 net.cpp:226] conv1 needs backward computation.
I0521 10:20:43.885021 18395 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 10:20:43.885032 18395 net.cpp:228] data_hdf5 does not need backward computation.
I0521 10:20:43.885042 18395 net.cpp:270] This network produces output accuracy
I0521 10:20:43.885052 18395 net.cpp:270] This network produces output loss
I0521 10:20:43.885082 18395 net.cpp:283] Network initialization done.
I0521 10:20:43.885215 18395 solver.cpp:60] Solver scaffolding done.
I0521 10:20:43.886353 18395 caffe.cpp:212] Starting Optimization
I0521 10:20:43.886371 18395 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 10:20:43.886384 18395 solver.cpp:289] Learning Rate Policy: fixed
I0521 10:20:43.887599 18395 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 10:21:29.728595 18395 solver.cpp:409]     Test net output #0: accuracy = 0.144861
I0521 10:21:29.728770 18395 solver.cpp:409]     Test net output #1: loss = 2.39627 (* 1 = 2.39627 loss)
I0521 10:21:29.900589 18395 solver.cpp:237] Iteration 0, loss = 2.39585
I0521 10:21:29.900625 18395 solver.cpp:253]     Train net output #0: loss = 2.39585 (* 1 = 2.39585 loss)
I0521 10:21:29.900643 18395 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 10:21:37.456529 18395 solver.cpp:237] Iteration 15, loss = 2.38515
I0521 10:21:37.456564 18395 solver.cpp:253]     Train net output #0: loss = 2.38515 (* 1 = 2.38515 loss)
I0521 10:21:37.456579 18395 sgd_solver.cpp:106] Iteration 15, lr = 0.0025
I0521 10:21:45.014950 18395 solver.cpp:237] Iteration 30, loss = 2.37082
I0521 10:21:45.014981 18395 solver.cpp:253]     Train net output #0: loss = 2.37082 (* 1 = 2.37082 loss)
I0521 10:21:45.014997 18395 sgd_solver.cpp:106] Iteration 30, lr = 0.0025
I0521 10:21:52.573019 18395 solver.cpp:237] Iteration 45, loss = 2.3655
I0521 10:21:52.573056 18395 solver.cpp:253]     Train net output #0: loss = 2.3655 (* 1 = 2.3655 loss)
I0521 10:21:52.573077 18395 sgd_solver.cpp:106] Iteration 45, lr = 0.0025
I0521 10:22:00.132565 18395 solver.cpp:237] Iteration 60, loss = 2.34655
I0521 10:22:00.132709 18395 solver.cpp:253]     Train net output #0: loss = 2.34655 (* 1 = 2.34655 loss)
I0521 10:22:00.132722 18395 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 10:22:07.686177 18395 solver.cpp:237] Iteration 75, loss = 2.34093
I0521 10:22:07.686208 18395 solver.cpp:253]     Train net output #0: loss = 2.34093 (* 1 = 2.34093 loss)
I0521 10:22:07.686223 18395 sgd_solver.cpp:106] Iteration 75, lr = 0.0025
I0521 10:22:15.244809 18395 solver.cpp:237] Iteration 90, loss = 2.33604
I0521 10:22:15.244855 18395 solver.cpp:253]     Train net output #0: loss = 2.33604 (* 1 = 2.33604 loss)
I0521 10:22:15.244871 18395 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 10:22:22.805949 18395 solver.cpp:237] Iteration 105, loss = 2.32937
I0521 10:22:22.805981 18395 solver.cpp:253]     Train net output #0: loss = 2.32937 (* 1 = 2.32937 loss)
I0521 10:22:22.805999 18395 sgd_solver.cpp:106] Iteration 105, lr = 0.0025
I0521 10:22:52.455024 18395 solver.cpp:237] Iteration 120, loss = 2.32783
I0521 10:22:52.455185 18395 solver.cpp:253]     Train net output #0: loss = 2.32783 (* 1 = 2.32783 loss)
I0521 10:22:52.455200 18395 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 10:23:00.008663 18395 solver.cpp:237] Iteration 135, loss = 2.32081
I0521 10:23:00.008697 18395 solver.cpp:253]     Train net output #0: loss = 2.32081 (* 1 = 2.32081 loss)
I0521 10:23:00.008713 18395 sgd_solver.cpp:106] Iteration 135, lr = 0.0025
I0521 10:23:07.573464 18395 solver.cpp:237] Iteration 150, loss = 2.33391
I0521 10:23:07.573510 18395 solver.cpp:253]     Train net output #0: loss = 2.33391 (* 1 = 2.33391 loss)
I0521 10:23:07.573525 18395 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 10:23:11.605298 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_159.caffemodel
I0521 10:23:12.002138 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_159.solverstate
I0521 10:23:15.203356 18395 solver.cpp:237] Iteration 165, loss = 2.32403
I0521 10:23:15.203402 18395 solver.cpp:253]     Train net output #0: loss = 2.32403 (* 1 = 2.32403 loss)
I0521 10:23:15.203415 18395 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0521 10:23:22.768220 18395 solver.cpp:237] Iteration 180, loss = 2.3011
I0521 10:23:22.768355 18395 solver.cpp:253]     Train net output #0: loss = 2.3011 (* 1 = 2.3011 loss)
I0521 10:23:22.768369 18395 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 10:23:30.327057 18395 solver.cpp:237] Iteration 195, loss = 2.31561
I0521 10:23:30.327100 18395 solver.cpp:253]     Train net output #0: loss = 2.31561 (* 1 = 2.31561 loss)
I0521 10:23:30.327116 18395 sgd_solver.cpp:106] Iteration 195, lr = 0.0025
I0521 10:23:37.875206 18395 solver.cpp:237] Iteration 210, loss = 2.31291
I0521 10:23:37.875238 18395 solver.cpp:253]     Train net output #0: loss = 2.31291 (* 1 = 2.31291 loss)
I0521 10:23:37.875257 18395 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 10:24:07.522403 18395 solver.cpp:237] Iteration 225, loss = 2.29895
I0521 10:24:07.522565 18395 solver.cpp:253]     Train net output #0: loss = 2.29895 (* 1 = 2.29895 loss)
I0521 10:24:07.522579 18395 sgd_solver.cpp:106] Iteration 225, lr = 0.0025
I0521 10:24:15.082831 18395 solver.cpp:237] Iteration 240, loss = 2.29835
I0521 10:24:15.082864 18395 solver.cpp:253]     Train net output #0: loss = 2.29835 (* 1 = 2.29835 loss)
I0521 10:24:15.082882 18395 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 10:24:22.641034 18395 solver.cpp:237] Iteration 255, loss = 2.28172
I0521 10:24:22.641078 18395 solver.cpp:253]     Train net output #0: loss = 2.28172 (* 1 = 2.28172 loss)
I0521 10:24:22.641096 18395 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 10:24:30.200765 18395 solver.cpp:237] Iteration 270, loss = 2.2793
I0521 10:24:30.200798 18395 solver.cpp:253]     Train net output #0: loss = 2.2793 (* 1 = 2.2793 loss)
I0521 10:24:30.200815 18395 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 10:24:37.760036 18395 solver.cpp:237] Iteration 285, loss = 2.27661
I0521 10:24:37.760169 18395 solver.cpp:253]     Train net output #0: loss = 2.27661 (* 1 = 2.27661 loss)
I0521 10:24:37.760181 18395 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 10:24:45.319397 18395 solver.cpp:237] Iteration 300, loss = 2.22622
I0521 10:24:45.319442 18395 solver.cpp:253]     Train net output #0: loss = 2.22622 (* 1 = 2.22622 loss)
I0521 10:24:45.319458 18395 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 10:24:52.879082 18395 solver.cpp:237] Iteration 315, loss = 2.22704
I0521 10:24:52.879114 18395 solver.cpp:253]     Train net output #0: loss = 2.22704 (* 1 = 2.22704 loss)
I0521 10:24:52.879132 18395 sgd_solver.cpp:106] Iteration 315, lr = 0.0025
I0521 10:24:53.886106 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_318.caffemodel
I0521 10:24:54.279129 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_318.solverstate
I0521 10:24:54.455166 18395 solver.cpp:341] Iteration 319, Testing net (#0)
I0521 10:25:39.725157 18395 solver.cpp:409]     Test net output #0: accuracy = 0.389408
I0521 10:25:39.725311 18395 solver.cpp:409]     Test net output #1: loss = 2.12505 (* 1 = 2.12505 loss)
I0521 10:26:07.544456 18395 solver.cpp:237] Iteration 330, loss = 2.16571
I0521 10:26:07.544505 18395 solver.cpp:253]     Train net output #0: loss = 2.16571 (* 1 = 2.16571 loss)
I0521 10:26:07.544520 18395 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 10:26:15.105026 18395 solver.cpp:237] Iteration 345, loss = 2.16047
I0521 10:26:15.105166 18395 solver.cpp:253]     Train net output #0: loss = 2.16047 (* 1 = 2.16047 loss)
I0521 10:26:15.105180 18395 sgd_solver.cpp:106] Iteration 345, lr = 0.0025
I0521 10:26:22.665437 18395 solver.cpp:237] Iteration 360, loss = 2.14807
I0521 10:26:22.665468 18395 solver.cpp:253]     Train net output #0: loss = 2.14807 (* 1 = 2.14807 loss)
I0521 10:26:22.665487 18395 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 10:26:30.220692 18395 solver.cpp:237] Iteration 375, loss = 2.16289
I0521 10:26:30.220733 18395 solver.cpp:253]     Train net output #0: loss = 2.16289 (* 1 = 2.16289 loss)
I0521 10:26:30.220749 18395 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0521 10:26:37.782289 18395 solver.cpp:237] Iteration 390, loss = 2.10592
I0521 10:26:37.782320 18395 solver.cpp:253]     Train net output #0: loss = 2.10592 (* 1 = 2.10592 loss)
I0521 10:26:37.782337 18395 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 10:26:45.354151 18395 solver.cpp:237] Iteration 405, loss = 2.11856
I0521 10:26:45.354284 18395 solver.cpp:253]     Train net output #0: loss = 2.11856 (* 1 = 2.11856 loss)
I0521 10:26:45.354296 18395 sgd_solver.cpp:106] Iteration 405, lr = 0.0025
I0521 10:26:52.911422 18395 solver.cpp:237] Iteration 420, loss = 2.11459
I0521 10:26:52.911468 18395 solver.cpp:253]     Train net output #0: loss = 2.11459 (* 1 = 2.11459 loss)
I0521 10:26:52.911483 18395 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 10:27:22.554450 18395 solver.cpp:237] Iteration 435, loss = 2.0843
I0521 10:27:22.554616 18395 solver.cpp:253]     Train net output #0: loss = 2.0843 (* 1 = 2.0843 loss)
I0521 10:27:22.554631 18395 sgd_solver.cpp:106] Iteration 435, lr = 0.0025
I0521 10:27:30.107640 18395 solver.cpp:237] Iteration 450, loss = 2.04785
I0521 10:27:30.107672 18395 solver.cpp:253]     Train net output #0: loss = 2.04785 (* 1 = 2.04785 loss)
I0521 10:27:30.107689 18395 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 10:27:37.673406 18395 solver.cpp:237] Iteration 465, loss = 2.03872
I0521 10:27:37.673439 18395 solver.cpp:253]     Train net output #0: loss = 2.03872 (* 1 = 2.03872 loss)
I0521 10:27:37.673455 18395 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0521 10:27:43.221420 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_477.caffemodel
I0521 10:27:43.614876 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_477.solverstate
I0521 10:27:45.306181 18395 solver.cpp:237] Iteration 480, loss = 2.0141
I0521 10:27:45.306224 18395 solver.cpp:253]     Train net output #0: loss = 2.0141 (* 1 = 2.0141 loss)
I0521 10:27:45.306244 18395 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 10:27:52.867205 18395 solver.cpp:237] Iteration 495, loss = 2.00999
I0521 10:27:52.867346 18395 solver.cpp:253]     Train net output #0: loss = 2.00999 (* 1 = 2.00999 loss)
I0521 10:27:52.867359 18395 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0521 10:28:00.430160 18395 solver.cpp:237] Iteration 510, loss = 2.03984
I0521 10:28:00.430191 18395 solver.cpp:253]     Train net output #0: loss = 2.03984 (* 1 = 2.03984 loss)
I0521 10:28:00.430207 18395 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 10:28:07.989461 18395 solver.cpp:237] Iteration 525, loss = 2.00535
I0521 10:28:07.989508 18395 solver.cpp:253]     Train net output #0: loss = 2.00535 (* 1 = 2.00535 loss)
I0521 10:28:07.989524 18395 sgd_solver.cpp:106] Iteration 525, lr = 0.0025
I0521 10:28:37.633106 18395 solver.cpp:237] Iteration 540, loss = 1.96511
I0521 10:28:37.633268 18395 solver.cpp:253]     Train net output #0: loss = 1.96511 (* 1 = 1.96511 loss)
I0521 10:28:37.633282 18395 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 10:28:45.199316 18395 solver.cpp:237] Iteration 555, loss = 1.8913
I0521 10:28:45.199347 18395 solver.cpp:253]     Train net output #0: loss = 1.8913 (* 1 = 1.8913 loss)
I0521 10:28:45.199362 18395 sgd_solver.cpp:106] Iteration 555, lr = 0.0025
I0521 10:28:52.760037 18395 solver.cpp:237] Iteration 570, loss = 1.96227
I0521 10:28:52.760069 18395 solver.cpp:253]     Train net output #0: loss = 1.96227 (* 1 = 1.96227 loss)
I0521 10:28:52.760087 18395 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 10:29:00.322360 18395 solver.cpp:237] Iteration 585, loss = 1.92045
I0521 10:29:00.322407 18395 solver.cpp:253]     Train net output #0: loss = 1.92045 (* 1 = 1.92045 loss)
I0521 10:29:00.322420 18395 sgd_solver.cpp:106] Iteration 585, lr = 0.0025
I0521 10:29:07.886729 18395 solver.cpp:237] Iteration 600, loss = 1.9176
I0521 10:29:07.886864 18395 solver.cpp:253]     Train net output #0: loss = 1.9176 (* 1 = 1.9176 loss)
I0521 10:29:07.886878 18395 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 10:29:15.444895 18395 solver.cpp:237] Iteration 615, loss = 1.9282
I0521 10:29:15.444926 18395 solver.cpp:253]     Train net output #0: loss = 1.9282 (* 1 = 1.9282 loss)
I0521 10:29:15.444943 18395 sgd_solver.cpp:106] Iteration 615, lr = 0.0025
I0521 10:29:23.006428 18395 solver.cpp:237] Iteration 630, loss = 1.88123
I0521 10:29:23.006469 18395 solver.cpp:253]     Train net output #0: loss = 1.88123 (* 1 = 1.88123 loss)
I0521 10:29:23.006485 18395 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 10:29:25.525913 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_636.caffemodel
I0521 10:29:25.920243 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_636.solverstate
I0521 10:29:26.604328 18395 solver.cpp:341] Iteration 638, Testing net (#0)
I0521 10:30:32.580324 18395 solver.cpp:409]     Test net output #0: accuracy = 0.574568
I0521 10:30:32.580497 18395 solver.cpp:409]     Test net output #1: loss = 1.53669 (* 1 = 1.53669 loss)
I0521 10:30:58.425535 18395 solver.cpp:237] Iteration 645, loss = 1.88982
I0521 10:30:58.425590 18395 solver.cpp:253]     Train net output #0: loss = 1.88982 (* 1 = 1.88982 loss)
I0521 10:30:58.425606 18395 sgd_solver.cpp:106] Iteration 645, lr = 0.0025
I0521 10:31:05.978436 18395 solver.cpp:237] Iteration 660, loss = 1.89237
I0521 10:31:05.978607 18395 solver.cpp:253]     Train net output #0: loss = 1.89237 (* 1 = 1.89237 loss)
I0521 10:31:05.978622 18395 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 10:31:13.527664 18395 solver.cpp:237] Iteration 675, loss = 1.8686
I0521 10:31:13.527696 18395 solver.cpp:253]     Train net output #0: loss = 1.8686 (* 1 = 1.8686 loss)
I0521 10:31:13.527714 18395 sgd_solver.cpp:106] Iteration 675, lr = 0.0025
I0521 10:31:21.075491 18395 solver.cpp:237] Iteration 690, loss = 1.88091
I0521 10:31:21.075523 18395 solver.cpp:253]     Train net output #0: loss = 1.88091 (* 1 = 1.88091 loss)
I0521 10:31:21.075538 18395 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 10:31:28.624929 18395 solver.cpp:237] Iteration 705, loss = 1.87366
I0521 10:31:28.624974 18395 solver.cpp:253]     Train net output #0: loss = 1.87366 (* 1 = 1.87366 loss)
I0521 10:31:28.624990 18395 sgd_solver.cpp:106] Iteration 705, lr = 0.0025
I0521 10:31:36.170431 18395 solver.cpp:237] Iteration 720, loss = 1.86599
I0521 10:31:36.170568 18395 solver.cpp:253]     Train net output #0: loss = 1.86599 (* 1 = 1.86599 loss)
I0521 10:31:36.170581 18395 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 10:31:43.719091 18395 solver.cpp:237] Iteration 735, loss = 1.87846
I0521 10:31:43.719122 18395 solver.cpp:253]     Train net output #0: loss = 1.87846 (* 1 = 1.87846 loss)
I0521 10:31:43.719141 18395 sgd_solver.cpp:106] Iteration 735, lr = 0.0025
I0521 10:32:13.369676 18395 solver.cpp:237] Iteration 750, loss = 1.86039
I0521 10:32:13.369838 18395 solver.cpp:253]     Train net output #0: loss = 1.86039 (* 1 = 1.86039 loss)
I0521 10:32:13.369853 18395 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 10:32:20.917953 18395 solver.cpp:237] Iteration 765, loss = 1.8574
I0521 10:32:20.917985 18395 solver.cpp:253]     Train net output #0: loss = 1.8574 (* 1 = 1.8574 loss)
I0521 10:32:20.918004 18395 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 10:32:28.469208 18395 solver.cpp:237] Iteration 780, loss = 1.84227
I0521 10:32:28.469241 18395 solver.cpp:253]     Train net output #0: loss = 1.84227 (* 1 = 1.84227 loss)
I0521 10:32:28.469257 18395 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 10:32:35.519037 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_795.caffemodel
I0521 10:32:35.913633 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_795.solverstate
I0521 10:32:36.091431 18395 solver.cpp:237] Iteration 795, loss = 1.79106
I0521 10:32:36.091476 18395 solver.cpp:253]     Train net output #0: loss = 1.79106 (* 1 = 1.79106 loss)
I0521 10:32:36.091496 18395 sgd_solver.cpp:106] Iteration 795, lr = 0.0025
I0521 10:32:43.642904 18395 solver.cpp:237] Iteration 810, loss = 1.75101
I0521 10:32:43.643069 18395 solver.cpp:253]     Train net output #0: loss = 1.75101 (* 1 = 1.75101 loss)
I0521 10:32:43.643082 18395 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 10:32:51.192134 18395 solver.cpp:237] Iteration 825, loss = 1.8221
I0521 10:32:51.192164 18395 solver.cpp:253]     Train net output #0: loss = 1.8221 (* 1 = 1.8221 loss)
I0521 10:32:51.192183 18395 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 10:32:58.747661 18395 solver.cpp:237] Iteration 840, loss = 1.78579
I0521 10:32:58.747694 18395 solver.cpp:253]     Train net output #0: loss = 1.78579 (* 1 = 1.78579 loss)
I0521 10:32:58.747710 18395 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 10:33:28.388701 18395 solver.cpp:237] Iteration 855, loss = 1.78282
I0521 10:33:28.388878 18395 solver.cpp:253]     Train net output #0: loss = 1.78282 (* 1 = 1.78282 loss)
I0521 10:33:28.388892 18395 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 10:33:35.937453 18395 solver.cpp:237] Iteration 870, loss = 1.85982
I0521 10:33:35.937485 18395 solver.cpp:253]     Train net output #0: loss = 1.85982 (* 1 = 1.85982 loss)
I0521 10:33:35.937505 18395 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 10:33:43.488276 18395 solver.cpp:237] Iteration 885, loss = 1.83487
I0521 10:33:43.488308 18395 solver.cpp:253]     Train net output #0: loss = 1.83487 (* 1 = 1.83487 loss)
I0521 10:33:43.488322 18395 sgd_solver.cpp:106] Iteration 885, lr = 0.0025
I0521 10:33:51.035779 18395 solver.cpp:237] Iteration 900, loss = 1.85581
I0521 10:33:51.035811 18395 solver.cpp:253]     Train net output #0: loss = 1.85581 (* 1 = 1.85581 loss)
I0521 10:33:51.035828 18395 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 10:33:58.587165 18395 solver.cpp:237] Iteration 915, loss = 1.8157
I0521 10:33:58.587311 18395 solver.cpp:253]     Train net output #0: loss = 1.8157 (* 1 = 1.8157 loss)
I0521 10:33:58.587326 18395 sgd_solver.cpp:106] Iteration 915, lr = 0.0025
I0521 10:34:06.136143 18395 solver.cpp:237] Iteration 930, loss = 1.82685
I0521 10:34:06.136173 18395 solver.cpp:253]     Train net output #0: loss = 1.82685 (* 1 = 1.82685 loss)
I0521 10:34:06.136191 18395 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 10:34:13.686092 18395 solver.cpp:237] Iteration 945, loss = 1.83294
I0521 10:34:13.686125 18395 solver.cpp:253]     Train net output #0: loss = 1.83294 (* 1 = 1.83294 loss)
I0521 10:34:13.686138 18395 sgd_solver.cpp:106] Iteration 945, lr = 0.0025
I0521 10:34:17.713358 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_954.caffemodel
I0521 10:34:18.103924 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_954.solverstate
I0521 10:34:19.285132 18395 solver.cpp:341] Iteration 957, Testing net (#0)
I0521 10:35:04.192626 18395 solver.cpp:409]     Test net output #0: accuracy = 0.603439
I0521 10:35:04.192786 18395 solver.cpp:409]     Test net output #1: loss = 1.37417 (* 1 = 1.37417 loss)
I0521 10:35:27.909901 18395 solver.cpp:237] Iteration 960, loss = 1.82828
I0521 10:35:27.909951 18395 solver.cpp:253]     Train net output #0: loss = 1.82828 (* 1 = 1.82828 loss)
I0521 10:35:27.909967 18395 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 10:35:35.461443 18395 solver.cpp:237] Iteration 975, loss = 1.79312
I0521 10:35:35.461607 18395 solver.cpp:253]     Train net output #0: loss = 1.79312 (* 1 = 1.79312 loss)
I0521 10:35:35.461621 18395 sgd_solver.cpp:106] Iteration 975, lr = 0.0025
I0521 10:35:43.011169 18395 solver.cpp:237] Iteration 990, loss = 1.82353
I0521 10:35:43.011201 18395 solver.cpp:253]     Train net output #0: loss = 1.82353 (* 1 = 1.82353 loss)
I0521 10:35:43.011220 18395 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 10:35:50.562894 18395 solver.cpp:237] Iteration 1005, loss = 1.75751
I0521 10:35:50.562925 18395 solver.cpp:253]     Train net output #0: loss = 1.75751 (* 1 = 1.75751 loss)
I0521 10:35:50.562943 18395 sgd_solver.cpp:106] Iteration 1005, lr = 0.0025
I0521 10:35:58.120606 18395 solver.cpp:237] Iteration 1020, loss = 1.78592
I0521 10:35:58.120638 18395 solver.cpp:253]     Train net output #0: loss = 1.78592 (* 1 = 1.78592 loss)
I0521 10:35:58.120656 18395 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 10:36:05.674429 18395 solver.cpp:237] Iteration 1035, loss = 1.74744
I0521 10:36:05.674579 18395 solver.cpp:253]     Train net output #0: loss = 1.74744 (* 1 = 1.74744 loss)
I0521 10:36:05.674593 18395 sgd_solver.cpp:106] Iteration 1035, lr = 0.0025
I0521 10:36:13.234232 18395 solver.cpp:237] Iteration 1050, loss = 1.73277
I0521 10:36:13.234263 18395 solver.cpp:253]     Train net output #0: loss = 1.73277 (* 1 = 1.73277 loss)
I0521 10:36:13.234282 18395 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 10:36:42.879101 18395 solver.cpp:237] Iteration 1065, loss = 1.78002
I0521 10:36:42.879278 18395 solver.cpp:253]     Train net output #0: loss = 1.78002 (* 1 = 1.78002 loss)
I0521 10:36:42.879293 18395 sgd_solver.cpp:106] Iteration 1065, lr = 0.0025
I0521 10:36:50.430758 18395 solver.cpp:237] Iteration 1080, loss = 1.75802
I0521 10:36:50.430801 18395 solver.cpp:253]     Train net output #0: loss = 1.75802 (* 1 = 1.75802 loss)
I0521 10:36:50.430816 18395 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 10:36:57.983839 18395 solver.cpp:237] Iteration 1095, loss = 1.7052
I0521 10:36:57.983870 18395 solver.cpp:253]     Train net output #0: loss = 1.7052 (* 1 = 1.7052 loss)
I0521 10:36:57.983888 18395 sgd_solver.cpp:106] Iteration 1095, lr = 0.0025
I0521 10:37:05.540441 18395 solver.cpp:237] Iteration 1110, loss = 1.8633
I0521 10:37:05.540472 18395 solver.cpp:253]     Train net output #0: loss = 1.8633 (* 1 = 1.8633 loss)
I0521 10:37:05.540489 18395 sgd_solver.cpp:106] Iteration 1110, lr = 0.0025
I0521 10:37:06.547317 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1113.caffemodel
I0521 10:37:06.939224 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1113.solverstate
I0521 10:37:13.159317 18395 solver.cpp:237] Iteration 1125, loss = 1.81663
I0521 10:37:13.159473 18395 solver.cpp:253]     Train net output #0: loss = 1.81663 (* 1 = 1.81663 loss)
I0521 10:37:13.159487 18395 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0521 10:37:20.714821 18395 solver.cpp:237] Iteration 1140, loss = 1.79813
I0521 10:37:20.714865 18395 solver.cpp:253]     Train net output #0: loss = 1.79813 (* 1 = 1.79813 loss)
I0521 10:37:20.714880 18395 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 10:37:28.263587 18395 solver.cpp:237] Iteration 1155, loss = 1.78078
I0521 10:37:28.263622 18395 solver.cpp:253]     Train net output #0: loss = 1.78078 (* 1 = 1.78078 loss)
I0521 10:37:28.263638 18395 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 10:37:57.929677 18395 solver.cpp:237] Iteration 1170, loss = 1.73189
I0521 10:37:57.929850 18395 solver.cpp:253]     Train net output #0: loss = 1.73189 (* 1 = 1.73189 loss)
I0521 10:37:57.929865 18395 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 10:38:05.480484 18395 solver.cpp:237] Iteration 1185, loss = 1.75317
I0521 10:38:05.480527 18395 solver.cpp:253]     Train net output #0: loss = 1.75317 (* 1 = 1.75317 loss)
I0521 10:38:05.480545 18395 sgd_solver.cpp:106] Iteration 1185, lr = 0.0025
I0521 10:38:13.039526 18395 solver.cpp:237] Iteration 1200, loss = 1.71795
I0521 10:38:13.039561 18395 solver.cpp:253]     Train net output #0: loss = 1.71795 (* 1 = 1.71795 loss)
I0521 10:38:13.039577 18395 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 10:38:20.592056 18395 solver.cpp:237] Iteration 1215, loss = 1.83163
I0521 10:38:20.592089 18395 solver.cpp:253]     Train net output #0: loss = 1.83163 (* 1 = 1.83163 loss)
I0521 10:38:20.592105 18395 sgd_solver.cpp:106] Iteration 1215, lr = 0.0025
I0521 10:38:28.151681 18395 solver.cpp:237] Iteration 1230, loss = 1.73557
I0521 10:38:28.151830 18395 solver.cpp:253]     Train net output #0: loss = 1.73557 (* 1 = 1.73557 loss)
I0521 10:38:28.151844 18395 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0521 10:38:35.711158 18395 solver.cpp:237] Iteration 1245, loss = 1.72492
I0521 10:38:35.711202 18395 solver.cpp:253]     Train net output #0: loss = 1.72492 (* 1 = 1.72492 loss)
I0521 10:38:35.711217 18395 sgd_solver.cpp:106] Iteration 1245, lr = 0.0025
I0521 10:38:43.265602 18395 solver.cpp:237] Iteration 1260, loss = 1.68553
I0521 10:38:43.265635 18395 solver.cpp:253]     Train net output #0: loss = 1.68553 (* 1 = 1.68553 loss)
I0521 10:38:43.265650 18395 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 10:38:48.807368 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1272.caffemodel
I0521 10:38:49.199194 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1272.solverstate
I0521 10:38:50.885190 18395 solver.cpp:237] Iteration 1275, loss = 1.71636
I0521 10:38:50.885236 18395 solver.cpp:253]     Train net output #0: loss = 1.71636 (* 1 = 1.71636 loss)
I0521 10:38:50.885248 18395 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 10:38:50.885754 18395 solver.cpp:341] Iteration 1276, Testing net (#0)
I0521 10:39:56.880759 18395 solver.cpp:409]     Test net output #0: accuracy = 0.642132
I0521 10:39:56.880929 18395 solver.cpp:409]     Test net output #1: loss = 1.26104 (* 1 = 1.26104 loss)
I0521 10:40:26.168458 18395 solver.cpp:237] Iteration 1290, loss = 1.7284
I0521 10:40:26.168509 18395 solver.cpp:253]     Train net output #0: loss = 1.7284 (* 1 = 1.7284 loss)
I0521 10:40:26.168524 18395 sgd_solver.cpp:106] Iteration 1290, lr = 0.0025
I0521 10:40:33.725451 18395 solver.cpp:237] Iteration 1305, loss = 1.74804
I0521 10:40:33.725611 18395 solver.cpp:253]     Train net output #0: loss = 1.74804 (* 1 = 1.74804 loss)
I0521 10:40:33.725625 18395 sgd_solver.cpp:106] Iteration 1305, lr = 0.0025
I0521 10:40:41.283548 18395 solver.cpp:237] Iteration 1320, loss = 1.70603
I0521 10:40:41.283586 18395 solver.cpp:253]     Train net output #0: loss = 1.70603 (* 1 = 1.70603 loss)
I0521 10:40:41.283598 18395 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 10:40:48.835782 18395 solver.cpp:237] Iteration 1335, loss = 1.73768
I0521 10:40:48.835814 18395 solver.cpp:253]     Train net output #0: loss = 1.73768 (* 1 = 1.73768 loss)
I0521 10:40:48.835831 18395 sgd_solver.cpp:106] Iteration 1335, lr = 0.0025
I0521 10:40:56.392798 18395 solver.cpp:237] Iteration 1350, loss = 1.76718
I0521 10:40:56.392829 18395 solver.cpp:253]     Train net output #0: loss = 1.76718 (* 1 = 1.76718 loss)
I0521 10:40:56.392845 18395 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 10:41:03.950206 18395 solver.cpp:237] Iteration 1365, loss = 1.82902
I0521 10:41:03.950356 18395 solver.cpp:253]     Train net output #0: loss = 1.82902 (* 1 = 1.82902 loss)
I0521 10:41:03.950371 18395 sgd_solver.cpp:106] Iteration 1365, lr = 0.0025
I0521 10:41:11.503082 18395 solver.cpp:237] Iteration 1380, loss = 1.73885
I0521 10:41:11.503113 18395 solver.cpp:253]     Train net output #0: loss = 1.73885 (* 1 = 1.73885 loss)
I0521 10:41:11.503130 18395 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 10:41:41.183792 18395 solver.cpp:237] Iteration 1395, loss = 1.67236
I0521 10:41:41.183960 18395 solver.cpp:253]     Train net output #0: loss = 1.67236 (* 1 = 1.67236 loss)
I0521 10:41:41.183974 18395 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0521 10:41:48.740643 18395 solver.cpp:237] Iteration 1410, loss = 1.66991
I0521 10:41:48.740675 18395 solver.cpp:253]     Train net output #0: loss = 1.66991 (* 1 = 1.66991 loss)
I0521 10:41:48.740692 18395 sgd_solver.cpp:106] Iteration 1410, lr = 0.0025
I0521 10:41:56.289844 18395 solver.cpp:237] Iteration 1425, loss = 1.74693
I0521 10:41:56.289890 18395 solver.cpp:253]     Train net output #0: loss = 1.74693 (* 1 = 1.74693 loss)
I0521 10:41:56.289903 18395 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 10:41:58.809599 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1431.caffemodel
I0521 10:41:59.202886 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1431.solverstate
I0521 10:42:03.915791 18395 solver.cpp:237] Iteration 1440, loss = 1.70547
I0521 10:42:03.915838 18395 solver.cpp:253]     Train net output #0: loss = 1.70547 (* 1 = 1.70547 loss)
I0521 10:42:03.915855 18395 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 10:42:11.475266 18395 solver.cpp:237] Iteration 1455, loss = 1.71889
I0521 10:42:11.475422 18395 solver.cpp:253]     Train net output #0: loss = 1.71889 (* 1 = 1.71889 loss)
I0521 10:42:11.475435 18395 sgd_solver.cpp:106] Iteration 1455, lr = 0.0025
I0521 10:42:19.034317 18395 solver.cpp:237] Iteration 1470, loss = 1.66773
I0521 10:42:19.034360 18395 solver.cpp:253]     Train net output #0: loss = 1.66773 (* 1 = 1.66773 loss)
I0521 10:42:19.034379 18395 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 10:42:26.591496 18395 solver.cpp:237] Iteration 1485, loss = 1.74741
I0521 10:42:26.591528 18395 solver.cpp:253]     Train net output #0: loss = 1.74741 (* 1 = 1.74741 loss)
I0521 10:42:26.591544 18395 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 10:42:56.248149 18395 solver.cpp:237] Iteration 1500, loss = 1.81133
I0521 10:42:56.248317 18395 solver.cpp:253]     Train net output #0: loss = 1.81133 (* 1 = 1.81133 loss)
I0521 10:42:56.248330 18395 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 10:43:03.800056 18395 solver.cpp:237] Iteration 1515, loss = 1.67739
I0521 10:43:03.800088 18395 solver.cpp:253]     Train net output #0: loss = 1.67739 (* 1 = 1.67739 loss)
I0521 10:43:03.800106 18395 sgd_solver.cpp:106] Iteration 1515, lr = 0.0025
I0521 10:43:11.355775 18395 solver.cpp:237] Iteration 1530, loss = 1.67506
I0521 10:43:11.355810 18395 solver.cpp:253]     Train net output #0: loss = 1.67506 (* 1 = 1.67506 loss)
I0521 10:43:11.355829 18395 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 10:43:18.908890 18395 solver.cpp:237] Iteration 1545, loss = 1.70454
I0521 10:43:18.908922 18395 solver.cpp:253]     Train net output #0: loss = 1.70454 (* 1 = 1.70454 loss)
I0521 10:43:18.908938 18395 sgd_solver.cpp:106] Iteration 1545, lr = 0.0025
I0521 10:43:26.459910 18395 solver.cpp:237] Iteration 1560, loss = 1.72866
I0521 10:43:26.460059 18395 solver.cpp:253]     Train net output #0: loss = 1.72866 (* 1 = 1.72866 loss)
I0521 10:43:26.460072 18395 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 10:43:34.013906 18395 solver.cpp:237] Iteration 1575, loss = 1.69828
I0521 10:43:34.013944 18395 solver.cpp:253]     Train net output #0: loss = 1.69828 (* 1 = 1.69828 loss)
I0521 10:43:34.013963 18395 sgd_solver.cpp:106] Iteration 1575, lr = 0.0025
I0521 10:43:41.067610 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1590.caffemodel
I0521 10:43:41.463048 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1590.solverstate
I0521 10:43:41.641537 18395 solver.cpp:237] Iteration 1590, loss = 1.7216
I0521 10:43:41.641592 18395 solver.cpp:253]     Train net output #0: loss = 1.7216 (* 1 = 1.7216 loss)
I0521 10:43:41.641607 18395 sgd_solver.cpp:106] Iteration 1590, lr = 0.0025
I0521 10:43:43.656358 18395 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1595.caffemodel
I0521 10:43:44.056150 18395 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352_iter_1595.solverstate
I0521 10:43:44.084146 18395 solver.cpp:341] Iteration 1595, Testing net (#0)
I0521 10:44:28.983914 18395 solver.cpp:409]     Test net output #0: accuracy = 0.646514
I0521 10:44:28.984094 18395 solver.cpp:409]     Test net output #1: loss = 1.21946 (* 1 = 1.21946 loss)
I0521 10:44:28.984109 18395 solver.cpp:326] Optimization Done.
I0521 10:44:28.984122 18395 caffe.cpp:215] Optimization Done.
Application 11237634 resources: utime ~1246s, stime ~224s, Rss ~5329444, inblocks ~3594475, outblocks ~194567
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_940_2016-05-20T11.21.06.988352.solver"
	User time (seconds): 0.56
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:34.64
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2741
	Involuntary context switches: 73
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
