2806369
I0521 08:42:27.680670 15209 caffe.cpp:184] Using GPUs 0
I0521 08:42:28.103387 15209 solver.cpp:48] Initializing solver from parameters: 
test_iter: 172
test_interval: 344
base_lr: 0.0025
display: 17
max_iter: 1724
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 172
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556.prototxt"
I0521 08:42:28.105168 15209 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556.prototxt
I0521 08:42:28.118176 15209 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 08:42:28.118235 15209 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 08:42:28.118578 15209 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 870
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 08:42:28.118762 15209 layer_factory.hpp:77] Creating layer data_hdf5
I0521 08:42:28.118785 15209 net.cpp:106] Creating Layer data_hdf5
I0521 08:42:28.118799 15209 net.cpp:411] data_hdf5 -> data
I0521 08:42:28.118832 15209 net.cpp:411] data_hdf5 -> label
I0521 08:42:28.118865 15209 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 08:42:28.120096 15209 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 08:42:28.122298 15209 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 08:42:49.650499 15209 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 08:42:49.655683 15209 net.cpp:150] Setting up data_hdf5
I0521 08:42:49.655722 15209 net.cpp:157] Top shape: 870 1 127 50 (5524500)
I0521 08:42:49.655736 15209 net.cpp:157] Top shape: 870 (870)
I0521 08:42:49.655750 15209 net.cpp:165] Memory required for data: 22101480
I0521 08:42:49.655762 15209 layer_factory.hpp:77] Creating layer conv1
I0521 08:42:49.655797 15209 net.cpp:106] Creating Layer conv1
I0521 08:42:49.655807 15209 net.cpp:454] conv1 <- data
I0521 08:42:49.655829 15209 net.cpp:411] conv1 -> conv1
I0521 08:42:50.016330 15209 net.cpp:150] Setting up conv1
I0521 08:42:50.016377 15209 net.cpp:157] Top shape: 870 12 120 48 (60134400)
I0521 08:42:50.016388 15209 net.cpp:165] Memory required for data: 262639080
I0521 08:42:50.016417 15209 layer_factory.hpp:77] Creating layer relu1
I0521 08:42:50.016438 15209 net.cpp:106] Creating Layer relu1
I0521 08:42:50.016448 15209 net.cpp:454] relu1 <- conv1
I0521 08:42:50.016463 15209 net.cpp:397] relu1 -> conv1 (in-place)
I0521 08:42:50.016986 15209 net.cpp:150] Setting up relu1
I0521 08:42:50.017004 15209 net.cpp:157] Top shape: 870 12 120 48 (60134400)
I0521 08:42:50.017014 15209 net.cpp:165] Memory required for data: 503176680
I0521 08:42:50.017024 15209 layer_factory.hpp:77] Creating layer pool1
I0521 08:42:50.017040 15209 net.cpp:106] Creating Layer pool1
I0521 08:42:50.017050 15209 net.cpp:454] pool1 <- conv1
I0521 08:42:50.017063 15209 net.cpp:411] pool1 -> pool1
I0521 08:42:50.017143 15209 net.cpp:150] Setting up pool1
I0521 08:42:50.017156 15209 net.cpp:157] Top shape: 870 12 60 48 (30067200)
I0521 08:42:50.017166 15209 net.cpp:165] Memory required for data: 623445480
I0521 08:42:50.017176 15209 layer_factory.hpp:77] Creating layer conv2
I0521 08:42:50.017197 15209 net.cpp:106] Creating Layer conv2
I0521 08:42:50.017207 15209 net.cpp:454] conv2 <- pool1
I0521 08:42:50.017220 15209 net.cpp:411] conv2 -> conv2
I0521 08:42:50.019893 15209 net.cpp:150] Setting up conv2
I0521 08:42:50.019922 15209 net.cpp:157] Top shape: 870 20 54 46 (43221600)
I0521 08:42:50.019932 15209 net.cpp:165] Memory required for data: 796331880
I0521 08:42:50.019950 15209 layer_factory.hpp:77] Creating layer relu2
I0521 08:42:50.019964 15209 net.cpp:106] Creating Layer relu2
I0521 08:42:50.019974 15209 net.cpp:454] relu2 <- conv2
I0521 08:42:50.019987 15209 net.cpp:397] relu2 -> conv2 (in-place)
I0521 08:42:50.020318 15209 net.cpp:150] Setting up relu2
I0521 08:42:50.020333 15209 net.cpp:157] Top shape: 870 20 54 46 (43221600)
I0521 08:42:50.020344 15209 net.cpp:165] Memory required for data: 969218280
I0521 08:42:50.020354 15209 layer_factory.hpp:77] Creating layer pool2
I0521 08:42:50.020366 15209 net.cpp:106] Creating Layer pool2
I0521 08:42:50.020376 15209 net.cpp:454] pool2 <- conv2
I0521 08:42:50.020401 15209 net.cpp:411] pool2 -> pool2
I0521 08:42:50.020470 15209 net.cpp:150] Setting up pool2
I0521 08:42:50.020484 15209 net.cpp:157] Top shape: 870 20 27 46 (21610800)
I0521 08:42:50.020493 15209 net.cpp:165] Memory required for data: 1055661480
I0521 08:42:50.020503 15209 layer_factory.hpp:77] Creating layer conv3
I0521 08:42:50.020521 15209 net.cpp:106] Creating Layer conv3
I0521 08:42:50.020534 15209 net.cpp:454] conv3 <- pool2
I0521 08:42:50.020557 15209 net.cpp:411] conv3 -> conv3
I0521 08:42:50.022466 15209 net.cpp:150] Setting up conv3
I0521 08:42:50.022485 15209 net.cpp:157] Top shape: 870 28 22 44 (23580480)
I0521 08:42:50.022495 15209 net.cpp:165] Memory required for data: 1149983400
I0521 08:42:50.022513 15209 layer_factory.hpp:77] Creating layer relu3
I0521 08:42:50.022529 15209 net.cpp:106] Creating Layer relu3
I0521 08:42:50.022539 15209 net.cpp:454] relu3 <- conv3
I0521 08:42:50.022552 15209 net.cpp:397] relu3 -> conv3 (in-place)
I0521 08:42:50.023017 15209 net.cpp:150] Setting up relu3
I0521 08:42:50.023035 15209 net.cpp:157] Top shape: 870 28 22 44 (23580480)
I0521 08:42:50.023046 15209 net.cpp:165] Memory required for data: 1244305320
I0521 08:42:50.023056 15209 layer_factory.hpp:77] Creating layer pool3
I0521 08:42:50.023068 15209 net.cpp:106] Creating Layer pool3
I0521 08:42:50.023078 15209 net.cpp:454] pool3 <- conv3
I0521 08:42:50.023090 15209 net.cpp:411] pool3 -> pool3
I0521 08:42:50.023159 15209 net.cpp:150] Setting up pool3
I0521 08:42:50.023172 15209 net.cpp:157] Top shape: 870 28 11 44 (11790240)
I0521 08:42:50.023182 15209 net.cpp:165] Memory required for data: 1291466280
I0521 08:42:50.023191 15209 layer_factory.hpp:77] Creating layer conv4
I0521 08:42:50.023210 15209 net.cpp:106] Creating Layer conv4
I0521 08:42:50.023219 15209 net.cpp:454] conv4 <- pool3
I0521 08:42:50.023233 15209 net.cpp:411] conv4 -> conv4
I0521 08:42:50.025959 15209 net.cpp:150] Setting up conv4
I0521 08:42:50.025987 15209 net.cpp:157] Top shape: 870 36 6 42 (7892640)
I0521 08:42:50.025998 15209 net.cpp:165] Memory required for data: 1323036840
I0521 08:42:50.026013 15209 layer_factory.hpp:77] Creating layer relu4
I0521 08:42:50.026027 15209 net.cpp:106] Creating Layer relu4
I0521 08:42:50.026037 15209 net.cpp:454] relu4 <- conv4
I0521 08:42:50.026051 15209 net.cpp:397] relu4 -> conv4 (in-place)
I0521 08:42:50.026515 15209 net.cpp:150] Setting up relu4
I0521 08:42:50.026531 15209 net.cpp:157] Top shape: 870 36 6 42 (7892640)
I0521 08:42:50.026542 15209 net.cpp:165] Memory required for data: 1354607400
I0521 08:42:50.026552 15209 layer_factory.hpp:77] Creating layer pool4
I0521 08:42:50.026564 15209 net.cpp:106] Creating Layer pool4
I0521 08:42:50.026574 15209 net.cpp:454] pool4 <- conv4
I0521 08:42:50.026587 15209 net.cpp:411] pool4 -> pool4
I0521 08:42:50.026656 15209 net.cpp:150] Setting up pool4
I0521 08:42:50.026669 15209 net.cpp:157] Top shape: 870 36 3 42 (3946320)
I0521 08:42:50.026680 15209 net.cpp:165] Memory required for data: 1370392680
I0521 08:42:50.026690 15209 layer_factory.hpp:77] Creating layer ip1
I0521 08:42:50.026711 15209 net.cpp:106] Creating Layer ip1
I0521 08:42:50.026722 15209 net.cpp:454] ip1 <- pool4
I0521 08:42:50.026734 15209 net.cpp:411] ip1 -> ip1
I0521 08:42:50.042145 15209 net.cpp:150] Setting up ip1
I0521 08:42:50.042174 15209 net.cpp:157] Top shape: 870 196 (170520)
I0521 08:42:50.042187 15209 net.cpp:165] Memory required for data: 1371074760
I0521 08:42:50.042209 15209 layer_factory.hpp:77] Creating layer relu5
I0521 08:42:50.042223 15209 net.cpp:106] Creating Layer relu5
I0521 08:42:50.042234 15209 net.cpp:454] relu5 <- ip1
I0521 08:42:50.042248 15209 net.cpp:397] relu5 -> ip1 (in-place)
I0521 08:42:50.042589 15209 net.cpp:150] Setting up relu5
I0521 08:42:50.042604 15209 net.cpp:157] Top shape: 870 196 (170520)
I0521 08:42:50.042615 15209 net.cpp:165] Memory required for data: 1371756840
I0521 08:42:50.042625 15209 layer_factory.hpp:77] Creating layer drop1
I0521 08:42:50.042646 15209 net.cpp:106] Creating Layer drop1
I0521 08:42:50.042656 15209 net.cpp:454] drop1 <- ip1
I0521 08:42:50.042681 15209 net.cpp:397] drop1 -> ip1 (in-place)
I0521 08:42:50.042728 15209 net.cpp:150] Setting up drop1
I0521 08:42:50.042742 15209 net.cpp:157] Top shape: 870 196 (170520)
I0521 08:42:50.042752 15209 net.cpp:165] Memory required for data: 1372438920
I0521 08:42:50.042762 15209 layer_factory.hpp:77] Creating layer ip2
I0521 08:42:50.042780 15209 net.cpp:106] Creating Layer ip2
I0521 08:42:50.042791 15209 net.cpp:454] ip2 <- ip1
I0521 08:42:50.042804 15209 net.cpp:411] ip2 -> ip2
I0521 08:42:50.043269 15209 net.cpp:150] Setting up ip2
I0521 08:42:50.043282 15209 net.cpp:157] Top shape: 870 98 (85260)
I0521 08:42:50.043292 15209 net.cpp:165] Memory required for data: 1372779960
I0521 08:42:50.043308 15209 layer_factory.hpp:77] Creating layer relu6
I0521 08:42:50.043320 15209 net.cpp:106] Creating Layer relu6
I0521 08:42:50.043330 15209 net.cpp:454] relu6 <- ip2
I0521 08:42:50.043341 15209 net.cpp:397] relu6 -> ip2 (in-place)
I0521 08:42:50.043859 15209 net.cpp:150] Setting up relu6
I0521 08:42:50.043874 15209 net.cpp:157] Top shape: 870 98 (85260)
I0521 08:42:50.043885 15209 net.cpp:165] Memory required for data: 1373121000
I0521 08:42:50.043895 15209 layer_factory.hpp:77] Creating layer drop2
I0521 08:42:50.043908 15209 net.cpp:106] Creating Layer drop2
I0521 08:42:50.043917 15209 net.cpp:454] drop2 <- ip2
I0521 08:42:50.043931 15209 net.cpp:397] drop2 -> ip2 (in-place)
I0521 08:42:50.043972 15209 net.cpp:150] Setting up drop2
I0521 08:42:50.043987 15209 net.cpp:157] Top shape: 870 98 (85260)
I0521 08:42:50.043997 15209 net.cpp:165] Memory required for data: 1373462040
I0521 08:42:50.044006 15209 layer_factory.hpp:77] Creating layer ip3
I0521 08:42:50.044020 15209 net.cpp:106] Creating Layer ip3
I0521 08:42:50.044029 15209 net.cpp:454] ip3 <- ip2
I0521 08:42:50.044042 15209 net.cpp:411] ip3 -> ip3
I0521 08:42:50.044252 15209 net.cpp:150] Setting up ip3
I0521 08:42:50.044265 15209 net.cpp:157] Top shape: 870 11 (9570)
I0521 08:42:50.044275 15209 net.cpp:165] Memory required for data: 1373500320
I0521 08:42:50.044291 15209 layer_factory.hpp:77] Creating layer drop3
I0521 08:42:50.044303 15209 net.cpp:106] Creating Layer drop3
I0521 08:42:50.044313 15209 net.cpp:454] drop3 <- ip3
I0521 08:42:50.044324 15209 net.cpp:397] drop3 -> ip3 (in-place)
I0521 08:42:50.044363 15209 net.cpp:150] Setting up drop3
I0521 08:42:50.044376 15209 net.cpp:157] Top shape: 870 11 (9570)
I0521 08:42:50.044386 15209 net.cpp:165] Memory required for data: 1373538600
I0521 08:42:50.044396 15209 layer_factory.hpp:77] Creating layer loss
I0521 08:42:50.044415 15209 net.cpp:106] Creating Layer loss
I0521 08:42:50.044425 15209 net.cpp:454] loss <- ip3
I0521 08:42:50.044436 15209 net.cpp:454] loss <- label
I0521 08:42:50.044448 15209 net.cpp:411] loss -> loss
I0521 08:42:50.044466 15209 layer_factory.hpp:77] Creating layer loss
I0521 08:42:50.045126 15209 net.cpp:150] Setting up loss
I0521 08:42:50.045147 15209 net.cpp:157] Top shape: (1)
I0521 08:42:50.045161 15209 net.cpp:160]     with loss weight 1
I0521 08:42:50.045203 15209 net.cpp:165] Memory required for data: 1373538604
I0521 08:42:50.045214 15209 net.cpp:226] loss needs backward computation.
I0521 08:42:50.045224 15209 net.cpp:226] drop3 needs backward computation.
I0521 08:42:50.045234 15209 net.cpp:226] ip3 needs backward computation.
I0521 08:42:50.045243 15209 net.cpp:226] drop2 needs backward computation.
I0521 08:42:50.045253 15209 net.cpp:226] relu6 needs backward computation.
I0521 08:42:50.045264 15209 net.cpp:226] ip2 needs backward computation.
I0521 08:42:50.045274 15209 net.cpp:226] drop1 needs backward computation.
I0521 08:42:50.045284 15209 net.cpp:226] relu5 needs backward computation.
I0521 08:42:50.045292 15209 net.cpp:226] ip1 needs backward computation.
I0521 08:42:50.045302 15209 net.cpp:226] pool4 needs backward computation.
I0521 08:42:50.045313 15209 net.cpp:226] relu4 needs backward computation.
I0521 08:42:50.045322 15209 net.cpp:226] conv4 needs backward computation.
I0521 08:42:50.045333 15209 net.cpp:226] pool3 needs backward computation.
I0521 08:42:50.045352 15209 net.cpp:226] relu3 needs backward computation.
I0521 08:42:50.045361 15209 net.cpp:226] conv3 needs backward computation.
I0521 08:42:50.045372 15209 net.cpp:226] pool2 needs backward computation.
I0521 08:42:50.045382 15209 net.cpp:226] relu2 needs backward computation.
I0521 08:42:50.045392 15209 net.cpp:226] conv2 needs backward computation.
I0521 08:42:50.045403 15209 net.cpp:226] pool1 needs backward computation.
I0521 08:42:50.045413 15209 net.cpp:226] relu1 needs backward computation.
I0521 08:42:50.045423 15209 net.cpp:226] conv1 needs backward computation.
I0521 08:42:50.045434 15209 net.cpp:228] data_hdf5 does not need backward computation.
I0521 08:42:50.045444 15209 net.cpp:270] This network produces output loss
I0521 08:42:50.045467 15209 net.cpp:283] Network initialization done.
I0521 08:42:50.047055 15209 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556.prototxt
I0521 08:42:50.047124 15209 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 08:42:50.047478 15209 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 870
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 08:42:50.047667 15209 layer_factory.hpp:77] Creating layer data_hdf5
I0521 08:42:50.047683 15209 net.cpp:106] Creating Layer data_hdf5
I0521 08:42:50.047695 15209 net.cpp:411] data_hdf5 -> data
I0521 08:42:50.047711 15209 net.cpp:411] data_hdf5 -> label
I0521 08:42:50.047727 15209 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 08:42:50.049293 15209 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 08:43:11.321678 15209 net.cpp:150] Setting up data_hdf5
I0521 08:43:11.321844 15209 net.cpp:157] Top shape: 870 1 127 50 (5524500)
I0521 08:43:11.321859 15209 net.cpp:157] Top shape: 870 (870)
I0521 08:43:11.321871 15209 net.cpp:165] Memory required for data: 22101480
I0521 08:43:11.321884 15209 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 08:43:11.321913 15209 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 08:43:11.321923 15209 net.cpp:454] label_data_hdf5_1_split <- label
I0521 08:43:11.321938 15209 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 08:43:11.321959 15209 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 08:43:11.322032 15209 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 08:43:11.322046 15209 net.cpp:157] Top shape: 870 (870)
I0521 08:43:11.322058 15209 net.cpp:157] Top shape: 870 (870)
I0521 08:43:11.322067 15209 net.cpp:165] Memory required for data: 22108440
I0521 08:43:11.322077 15209 layer_factory.hpp:77] Creating layer conv1
I0521 08:43:11.322099 15209 net.cpp:106] Creating Layer conv1
I0521 08:43:11.322110 15209 net.cpp:454] conv1 <- data
I0521 08:43:11.322124 15209 net.cpp:411] conv1 -> conv1
I0521 08:43:11.324069 15209 net.cpp:150] Setting up conv1
I0521 08:43:11.324092 15209 net.cpp:157] Top shape: 870 12 120 48 (60134400)
I0521 08:43:11.324105 15209 net.cpp:165] Memory required for data: 262646040
I0521 08:43:11.324125 15209 layer_factory.hpp:77] Creating layer relu1
I0521 08:43:11.324139 15209 net.cpp:106] Creating Layer relu1
I0521 08:43:11.324149 15209 net.cpp:454] relu1 <- conv1
I0521 08:43:11.324162 15209 net.cpp:397] relu1 -> conv1 (in-place)
I0521 08:43:11.324669 15209 net.cpp:150] Setting up relu1
I0521 08:43:11.324687 15209 net.cpp:157] Top shape: 870 12 120 48 (60134400)
I0521 08:43:11.324697 15209 net.cpp:165] Memory required for data: 503183640
I0521 08:43:11.324707 15209 layer_factory.hpp:77] Creating layer pool1
I0521 08:43:11.324723 15209 net.cpp:106] Creating Layer pool1
I0521 08:43:11.324733 15209 net.cpp:454] pool1 <- conv1
I0521 08:43:11.324746 15209 net.cpp:411] pool1 -> pool1
I0521 08:43:11.324820 15209 net.cpp:150] Setting up pool1
I0521 08:43:11.324834 15209 net.cpp:157] Top shape: 870 12 60 48 (30067200)
I0521 08:43:11.324844 15209 net.cpp:165] Memory required for data: 623452440
I0521 08:43:11.324852 15209 layer_factory.hpp:77] Creating layer conv2
I0521 08:43:11.324870 15209 net.cpp:106] Creating Layer conv2
I0521 08:43:11.324880 15209 net.cpp:454] conv2 <- pool1
I0521 08:43:11.324893 15209 net.cpp:411] conv2 -> conv2
I0521 08:43:11.326798 15209 net.cpp:150] Setting up conv2
I0521 08:43:11.326815 15209 net.cpp:157] Top shape: 870 20 54 46 (43221600)
I0521 08:43:11.326827 15209 net.cpp:165] Memory required for data: 796338840
I0521 08:43:11.326844 15209 layer_factory.hpp:77] Creating layer relu2
I0521 08:43:11.326858 15209 net.cpp:106] Creating Layer relu2
I0521 08:43:11.326867 15209 net.cpp:454] relu2 <- conv2
I0521 08:43:11.326880 15209 net.cpp:397] relu2 -> conv2 (in-place)
I0521 08:43:11.327217 15209 net.cpp:150] Setting up relu2
I0521 08:43:11.327231 15209 net.cpp:157] Top shape: 870 20 54 46 (43221600)
I0521 08:43:11.327241 15209 net.cpp:165] Memory required for data: 969225240
I0521 08:43:11.327251 15209 layer_factory.hpp:77] Creating layer pool2
I0521 08:43:11.327265 15209 net.cpp:106] Creating Layer pool2
I0521 08:43:11.327275 15209 net.cpp:454] pool2 <- conv2
I0521 08:43:11.327287 15209 net.cpp:411] pool2 -> pool2
I0521 08:43:11.327358 15209 net.cpp:150] Setting up pool2
I0521 08:43:11.327373 15209 net.cpp:157] Top shape: 870 20 27 46 (21610800)
I0521 08:43:11.327383 15209 net.cpp:165] Memory required for data: 1055668440
I0521 08:43:11.327391 15209 layer_factory.hpp:77] Creating layer conv3
I0521 08:43:11.327411 15209 net.cpp:106] Creating Layer conv3
I0521 08:43:11.327422 15209 net.cpp:454] conv3 <- pool2
I0521 08:43:11.327435 15209 net.cpp:411] conv3 -> conv3
I0521 08:43:11.329416 15209 net.cpp:150] Setting up conv3
I0521 08:43:11.329438 15209 net.cpp:157] Top shape: 870 28 22 44 (23580480)
I0521 08:43:11.329450 15209 net.cpp:165] Memory required for data: 1149990360
I0521 08:43:11.329483 15209 layer_factory.hpp:77] Creating layer relu3
I0521 08:43:11.329496 15209 net.cpp:106] Creating Layer relu3
I0521 08:43:11.329506 15209 net.cpp:454] relu3 <- conv3
I0521 08:43:11.329519 15209 net.cpp:397] relu3 -> conv3 (in-place)
I0521 08:43:11.329991 15209 net.cpp:150] Setting up relu3
I0521 08:43:11.330008 15209 net.cpp:157] Top shape: 870 28 22 44 (23580480)
I0521 08:43:11.330018 15209 net.cpp:165] Memory required for data: 1244312280
I0521 08:43:11.330027 15209 layer_factory.hpp:77] Creating layer pool3
I0521 08:43:11.330040 15209 net.cpp:106] Creating Layer pool3
I0521 08:43:11.330052 15209 net.cpp:454] pool3 <- conv3
I0521 08:43:11.330065 15209 net.cpp:411] pool3 -> pool3
I0521 08:43:11.330137 15209 net.cpp:150] Setting up pool3
I0521 08:43:11.330152 15209 net.cpp:157] Top shape: 870 28 11 44 (11790240)
I0521 08:43:11.330162 15209 net.cpp:165] Memory required for data: 1291473240
I0521 08:43:11.330171 15209 layer_factory.hpp:77] Creating layer conv4
I0521 08:43:11.330189 15209 net.cpp:106] Creating Layer conv4
I0521 08:43:11.330199 15209 net.cpp:454] conv4 <- pool3
I0521 08:43:11.330214 15209 net.cpp:411] conv4 -> conv4
I0521 08:43:11.332274 15209 net.cpp:150] Setting up conv4
I0521 08:43:11.332296 15209 net.cpp:157] Top shape: 870 36 6 42 (7892640)
I0521 08:43:11.332309 15209 net.cpp:165] Memory required for data: 1323043800
I0521 08:43:11.332324 15209 layer_factory.hpp:77] Creating layer relu4
I0521 08:43:11.332337 15209 net.cpp:106] Creating Layer relu4
I0521 08:43:11.332347 15209 net.cpp:454] relu4 <- conv4
I0521 08:43:11.332360 15209 net.cpp:397] relu4 -> conv4 (in-place)
I0521 08:43:11.332837 15209 net.cpp:150] Setting up relu4
I0521 08:43:11.332854 15209 net.cpp:157] Top shape: 870 36 6 42 (7892640)
I0521 08:43:11.332864 15209 net.cpp:165] Memory required for data: 1354614360
I0521 08:43:11.332873 15209 layer_factory.hpp:77] Creating layer pool4
I0521 08:43:11.332886 15209 net.cpp:106] Creating Layer pool4
I0521 08:43:11.332896 15209 net.cpp:454] pool4 <- conv4
I0521 08:43:11.332909 15209 net.cpp:411] pool4 -> pool4
I0521 08:43:11.332983 15209 net.cpp:150] Setting up pool4
I0521 08:43:11.332995 15209 net.cpp:157] Top shape: 870 36 3 42 (3946320)
I0521 08:43:11.333004 15209 net.cpp:165] Memory required for data: 1370399640
I0521 08:43:11.333015 15209 layer_factory.hpp:77] Creating layer ip1
I0521 08:43:11.333030 15209 net.cpp:106] Creating Layer ip1
I0521 08:43:11.333040 15209 net.cpp:454] ip1 <- pool4
I0521 08:43:11.333055 15209 net.cpp:411] ip1 -> ip1
I0521 08:43:11.348526 15209 net.cpp:150] Setting up ip1
I0521 08:43:11.348559 15209 net.cpp:157] Top shape: 870 196 (170520)
I0521 08:43:11.348570 15209 net.cpp:165] Memory required for data: 1371081720
I0521 08:43:11.348592 15209 layer_factory.hpp:77] Creating layer relu5
I0521 08:43:11.348608 15209 net.cpp:106] Creating Layer relu5
I0521 08:43:11.348618 15209 net.cpp:454] relu5 <- ip1
I0521 08:43:11.348631 15209 net.cpp:397] relu5 -> ip1 (in-place)
I0521 08:43:11.348978 15209 net.cpp:150] Setting up relu5
I0521 08:43:11.348991 15209 net.cpp:157] Top shape: 870 196 (170520)
I0521 08:43:11.349001 15209 net.cpp:165] Memory required for data: 1371763800
I0521 08:43:11.349011 15209 layer_factory.hpp:77] Creating layer drop1
I0521 08:43:11.349030 15209 net.cpp:106] Creating Layer drop1
I0521 08:43:11.349040 15209 net.cpp:454] drop1 <- ip1
I0521 08:43:11.349053 15209 net.cpp:397] drop1 -> ip1 (in-place)
I0521 08:43:11.349097 15209 net.cpp:150] Setting up drop1
I0521 08:43:11.349110 15209 net.cpp:157] Top shape: 870 196 (170520)
I0521 08:43:11.349120 15209 net.cpp:165] Memory required for data: 1372445880
I0521 08:43:11.349130 15209 layer_factory.hpp:77] Creating layer ip2
I0521 08:43:11.349144 15209 net.cpp:106] Creating Layer ip2
I0521 08:43:11.349154 15209 net.cpp:454] ip2 <- ip1
I0521 08:43:11.349166 15209 net.cpp:411] ip2 -> ip2
I0521 08:43:11.349644 15209 net.cpp:150] Setting up ip2
I0521 08:43:11.349658 15209 net.cpp:157] Top shape: 870 98 (85260)
I0521 08:43:11.349668 15209 net.cpp:165] Memory required for data: 1372786920
I0521 08:43:11.349696 15209 layer_factory.hpp:77] Creating layer relu6
I0521 08:43:11.349709 15209 net.cpp:106] Creating Layer relu6
I0521 08:43:11.349720 15209 net.cpp:454] relu6 <- ip2
I0521 08:43:11.349732 15209 net.cpp:397] relu6 -> ip2 (in-place)
I0521 08:43:11.350263 15209 net.cpp:150] Setting up relu6
I0521 08:43:11.350280 15209 net.cpp:157] Top shape: 870 98 (85260)
I0521 08:43:11.350289 15209 net.cpp:165] Memory required for data: 1373127960
I0521 08:43:11.350299 15209 layer_factory.hpp:77] Creating layer drop2
I0521 08:43:11.350313 15209 net.cpp:106] Creating Layer drop2
I0521 08:43:11.350323 15209 net.cpp:454] drop2 <- ip2
I0521 08:43:11.350337 15209 net.cpp:397] drop2 -> ip2 (in-place)
I0521 08:43:11.350380 15209 net.cpp:150] Setting up drop2
I0521 08:43:11.350394 15209 net.cpp:157] Top shape: 870 98 (85260)
I0521 08:43:11.350404 15209 net.cpp:165] Memory required for data: 1373469000
I0521 08:43:11.350414 15209 layer_factory.hpp:77] Creating layer ip3
I0521 08:43:11.350426 15209 net.cpp:106] Creating Layer ip3
I0521 08:43:11.350437 15209 net.cpp:454] ip3 <- ip2
I0521 08:43:11.350451 15209 net.cpp:411] ip3 -> ip3
I0521 08:43:11.350672 15209 net.cpp:150] Setting up ip3
I0521 08:43:11.350685 15209 net.cpp:157] Top shape: 870 11 (9570)
I0521 08:43:11.350697 15209 net.cpp:165] Memory required for data: 1373507280
I0521 08:43:11.350711 15209 layer_factory.hpp:77] Creating layer drop3
I0521 08:43:11.350724 15209 net.cpp:106] Creating Layer drop3
I0521 08:43:11.350734 15209 net.cpp:454] drop3 <- ip3
I0521 08:43:11.350746 15209 net.cpp:397] drop3 -> ip3 (in-place)
I0521 08:43:11.350788 15209 net.cpp:150] Setting up drop3
I0521 08:43:11.350800 15209 net.cpp:157] Top shape: 870 11 (9570)
I0521 08:43:11.350811 15209 net.cpp:165] Memory required for data: 1373545560
I0521 08:43:11.350819 15209 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 08:43:11.350832 15209 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 08:43:11.350842 15209 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 08:43:11.350855 15209 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 08:43:11.350869 15209 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 08:43:11.350942 15209 net.cpp:150] Setting up ip3_drop3_0_split
I0521 08:43:11.350955 15209 net.cpp:157] Top shape: 870 11 (9570)
I0521 08:43:11.350967 15209 net.cpp:157] Top shape: 870 11 (9570)
I0521 08:43:11.350977 15209 net.cpp:165] Memory required for data: 1373622120
I0521 08:43:11.350987 15209 layer_factory.hpp:77] Creating layer accuracy
I0521 08:43:11.351008 15209 net.cpp:106] Creating Layer accuracy
I0521 08:43:11.351019 15209 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 08:43:11.351030 15209 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 08:43:11.351044 15209 net.cpp:411] accuracy -> accuracy
I0521 08:43:11.351068 15209 net.cpp:150] Setting up accuracy
I0521 08:43:11.351080 15209 net.cpp:157] Top shape: (1)
I0521 08:43:11.351090 15209 net.cpp:165] Memory required for data: 1373622124
I0521 08:43:11.351100 15209 layer_factory.hpp:77] Creating layer loss
I0521 08:43:11.351114 15209 net.cpp:106] Creating Layer loss
I0521 08:43:11.351125 15209 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 08:43:11.351135 15209 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 08:43:11.351148 15209 net.cpp:411] loss -> loss
I0521 08:43:11.351166 15209 layer_factory.hpp:77] Creating layer loss
I0521 08:43:11.351660 15209 net.cpp:150] Setting up loss
I0521 08:43:11.351673 15209 net.cpp:157] Top shape: (1)
I0521 08:43:11.351683 15209 net.cpp:160]     with loss weight 1
I0521 08:43:11.351701 15209 net.cpp:165] Memory required for data: 1373622128
I0521 08:43:11.351712 15209 net.cpp:226] loss needs backward computation.
I0521 08:43:11.351723 15209 net.cpp:228] accuracy does not need backward computation.
I0521 08:43:11.351734 15209 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 08:43:11.351744 15209 net.cpp:226] drop3 needs backward computation.
I0521 08:43:11.351755 15209 net.cpp:226] ip3 needs backward computation.
I0521 08:43:11.351765 15209 net.cpp:226] drop2 needs backward computation.
I0521 08:43:11.351783 15209 net.cpp:226] relu6 needs backward computation.
I0521 08:43:11.351794 15209 net.cpp:226] ip2 needs backward computation.
I0521 08:43:11.351804 15209 net.cpp:226] drop1 needs backward computation.
I0521 08:43:11.351814 15209 net.cpp:226] relu5 needs backward computation.
I0521 08:43:11.351822 15209 net.cpp:226] ip1 needs backward computation.
I0521 08:43:11.351832 15209 net.cpp:226] pool4 needs backward computation.
I0521 08:43:11.351842 15209 net.cpp:226] relu4 needs backward computation.
I0521 08:43:11.351852 15209 net.cpp:226] conv4 needs backward computation.
I0521 08:43:11.351862 15209 net.cpp:226] pool3 needs backward computation.
I0521 08:43:11.351873 15209 net.cpp:226] relu3 needs backward computation.
I0521 08:43:11.351883 15209 net.cpp:226] conv3 needs backward computation.
I0521 08:43:11.351894 15209 net.cpp:226] pool2 needs backward computation.
I0521 08:43:11.351904 15209 net.cpp:226] relu2 needs backward computation.
I0521 08:43:11.351912 15209 net.cpp:226] conv2 needs backward computation.
I0521 08:43:11.351923 15209 net.cpp:226] pool1 needs backward computation.
I0521 08:43:11.351934 15209 net.cpp:226] relu1 needs backward computation.
I0521 08:43:11.351943 15209 net.cpp:226] conv1 needs backward computation.
I0521 08:43:11.351954 15209 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 08:43:11.351966 15209 net.cpp:228] data_hdf5 does not need backward computation.
I0521 08:43:11.351976 15209 net.cpp:270] This network produces output accuracy
I0521 08:43:11.351986 15209 net.cpp:270] This network produces output loss
I0521 08:43:11.352015 15209 net.cpp:283] Network initialization done.
I0521 08:43:11.352146 15209 solver.cpp:60] Solver scaffolding done.
I0521 08:43:11.353369 15209 caffe.cpp:212] Starting Optimization
I0521 08:43:11.353389 15209 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 08:43:11.353402 15209 solver.cpp:289] Learning Rate Policy: fixed
I0521 08:43:11.354617 15209 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 08:43:57.236883 15209 solver.cpp:409]     Test net output #0: accuracy = 0.113432
I0521 08:43:57.237052 15209 solver.cpp:409]     Test net output #1: loss = 2.39564 (* 1 = 2.39564 loss)
I0521 08:43:57.396533 15209 solver.cpp:237] Iteration 0, loss = 2.39639
I0521 08:43:57.396575 15209 solver.cpp:253]     Train net output #0: loss = 2.39639 (* 1 = 2.39639 loss)
I0521 08:43:57.396595 15209 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 08:44:05.339851 15209 solver.cpp:237] Iteration 17, loss = 2.38655
I0521 08:44:05.339885 15209 solver.cpp:253]     Train net output #0: loss = 2.38655 (* 1 = 2.38655 loss)
I0521 08:44:05.339901 15209 sgd_solver.cpp:106] Iteration 17, lr = 0.0025
I0521 08:44:13.281736 15209 solver.cpp:237] Iteration 34, loss = 2.36702
I0521 08:44:13.281781 15209 solver.cpp:253]     Train net output #0: loss = 2.36702 (* 1 = 2.36702 loss)
I0521 08:44:13.281796 15209 sgd_solver.cpp:106] Iteration 34, lr = 0.0025
I0521 08:44:21.226650 15209 solver.cpp:237] Iteration 51, loss = 2.34784
I0521 08:44:21.226681 15209 solver.cpp:253]     Train net output #0: loss = 2.34784 (* 1 = 2.34784 loss)
I0521 08:44:21.226697 15209 sgd_solver.cpp:106] Iteration 51, lr = 0.0025
I0521 08:44:29.168573 15209 solver.cpp:237] Iteration 68, loss = 2.33713
I0521 08:44:29.168720 15209 solver.cpp:253]     Train net output #0: loss = 2.33713 (* 1 = 2.33713 loss)
I0521 08:44:29.168735 15209 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0521 08:44:37.112617 15209 solver.cpp:237] Iteration 85, loss = 2.33942
I0521 08:44:37.112661 15209 solver.cpp:253]     Train net output #0: loss = 2.33942 (* 1 = 2.33942 loss)
I0521 08:44:37.112675 15209 sgd_solver.cpp:106] Iteration 85, lr = 0.0025
I0521 08:44:45.054069 15209 solver.cpp:237] Iteration 102, loss = 2.31763
I0521 08:44:45.054100 15209 solver.cpp:253]     Train net output #0: loss = 2.31763 (* 1 = 2.31763 loss)
I0521 08:44:45.054116 15209 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0521 08:45:15.105783 15209 solver.cpp:237] Iteration 119, loss = 2.32706
I0521 08:45:15.105947 15209 solver.cpp:253]     Train net output #0: loss = 2.32706 (* 1 = 2.32706 loss)
I0521 08:45:15.105963 15209 sgd_solver.cpp:106] Iteration 119, lr = 0.0025
I0521 08:45:23.057127 15209 solver.cpp:237] Iteration 136, loss = 2.30333
I0521 08:45:23.057160 15209 solver.cpp:253]     Train net output #0: loss = 2.30333 (* 1 = 2.30333 loss)
I0521 08:45:23.057175 15209 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0521 08:45:31.010565 15209 solver.cpp:237] Iteration 153, loss = 2.32841
I0521 08:45:31.010593 15209 solver.cpp:253]     Train net output #0: loss = 2.32841 (* 1 = 2.32841 loss)
I0521 08:45:31.010606 15209 sgd_solver.cpp:106] Iteration 153, lr = 0.0025
I0521 08:45:38.954618 15209 solver.cpp:237] Iteration 170, loss = 2.29865
I0521 08:45:38.954650 15209 solver.cpp:253]     Train net output #0: loss = 2.29865 (* 1 = 2.29865 loss)
I0521 08:45:38.954664 15209 sgd_solver.cpp:106] Iteration 170, lr = 0.0025
I0521 08:45:39.423758 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_172.caffemodel
I0521 08:45:39.793187 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_172.solverstate
I0521 08:45:46.973984 15209 solver.cpp:237] Iteration 187, loss = 2.289
I0521 08:45:46.974140 15209 solver.cpp:253]     Train net output #0: loss = 2.289 (* 1 = 2.289 loss)
I0521 08:45:46.974154 15209 sgd_solver.cpp:106] Iteration 187, lr = 0.0025
I0521 08:45:54.918543 15209 solver.cpp:237] Iteration 204, loss = 2.27597
I0521 08:45:54.918581 15209 solver.cpp:253]     Train net output #0: loss = 2.27597 (* 1 = 2.27597 loss)
I0521 08:45:54.918597 15209 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0521 08:46:02.861347 15209 solver.cpp:237] Iteration 221, loss = 2.25349
I0521 08:46:02.861380 15209 solver.cpp:253]     Train net output #0: loss = 2.25349 (* 1 = 2.25349 loss)
I0521 08:46:02.861394 15209 sgd_solver.cpp:106] Iteration 221, lr = 0.0025
I0521 08:46:32.894469 15209 solver.cpp:237] Iteration 238, loss = 2.23762
I0521 08:46:32.894620 15209 solver.cpp:253]     Train net output #0: loss = 2.23762 (* 1 = 2.23762 loss)
I0521 08:46:32.894635 15209 sgd_solver.cpp:106] Iteration 238, lr = 0.0025
I0521 08:46:40.840648 15209 solver.cpp:237] Iteration 255, loss = 2.1948
I0521 08:46:40.840680 15209 solver.cpp:253]     Train net output #0: loss = 2.1948 (* 1 = 2.1948 loss)
I0521 08:46:40.840697 15209 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 08:46:48.789175 15209 solver.cpp:237] Iteration 272, loss = 2.21097
I0521 08:46:48.789216 15209 solver.cpp:253]     Train net output #0: loss = 2.21097 (* 1 = 2.21097 loss)
I0521 08:46:48.789230 15209 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 08:46:56.738991 15209 solver.cpp:237] Iteration 289, loss = 2.15542
I0521 08:46:56.739024 15209 solver.cpp:253]     Train net output #0: loss = 2.15542 (* 1 = 2.15542 loss)
I0521 08:46:56.739038 15209 sgd_solver.cpp:106] Iteration 289, lr = 0.0025
I0521 08:47:04.691303 15209 solver.cpp:237] Iteration 306, loss = 2.12616
I0521 08:47:04.691447 15209 solver.cpp:253]     Train net output #0: loss = 2.12616 (* 1 = 2.12616 loss)
I0521 08:47:04.691460 15209 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 08:47:12.641505 15209 solver.cpp:237] Iteration 323, loss = 2.11705
I0521 08:47:12.641546 15209 solver.cpp:253]     Train net output #0: loss = 2.11705 (* 1 = 2.11705 loss)
I0521 08:47:12.641563 15209 sgd_solver.cpp:106] Iteration 323, lr = 0.0025
I0521 08:47:20.581746 15209 solver.cpp:237] Iteration 340, loss = 2.15459
I0521 08:47:20.581779 15209 solver.cpp:253]     Train net output #0: loss = 2.15459 (* 1 = 2.15459 loss)
I0521 08:47:20.581794 15209 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0521 08:47:21.983300 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_344.caffemodel
I0521 08:47:22.348909 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_344.solverstate
I0521 08:47:22.374975 15209 solver.cpp:341] Iteration 344, Testing net (#0)
I0521 08:48:07.351166 15209 solver.cpp:409]     Test net output #0: accuracy = 0.463058
I0521 08:48:07.351333 15209 solver.cpp:409]     Test net output #1: loss = 1.94532 (* 1 = 1.94532 loss)
I0521 08:48:35.644944 15209 solver.cpp:237] Iteration 357, loss = 2.11072
I0521 08:48:35.644994 15209 solver.cpp:253]     Train net output #0: loss = 2.11072 (* 1 = 2.11072 loss)
I0521 08:48:35.645009 15209 sgd_solver.cpp:106] Iteration 357, lr = 0.0025
I0521 08:48:43.591172 15209 solver.cpp:237] Iteration 374, loss = 2.0788
I0521 08:48:43.591317 15209 solver.cpp:253]     Train net output #0: loss = 2.0788 (* 1 = 2.0788 loss)
I0521 08:48:43.591331 15209 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 08:48:51.532222 15209 solver.cpp:237] Iteration 391, loss = 2.03384
I0521 08:48:51.532263 15209 solver.cpp:253]     Train net output #0: loss = 2.03384 (* 1 = 2.03384 loss)
I0521 08:48:51.532279 15209 sgd_solver.cpp:106] Iteration 391, lr = 0.0025
I0521 08:48:59.477023 15209 solver.cpp:237] Iteration 408, loss = 2.0374
I0521 08:48:59.477056 15209 solver.cpp:253]     Train net output #0: loss = 2.0374 (* 1 = 2.0374 loss)
I0521 08:48:59.477069 15209 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0521 08:49:07.408753 15209 solver.cpp:237] Iteration 425, loss = 2.02774
I0521 08:49:07.408785 15209 solver.cpp:253]     Train net output #0: loss = 2.02774 (* 1 = 2.02774 loss)
I0521 08:49:07.408799 15209 sgd_solver.cpp:106] Iteration 425, lr = 0.0025
I0521 08:49:15.354801 15209 solver.cpp:237] Iteration 442, loss = 2.0236
I0521 08:49:15.354943 15209 solver.cpp:253]     Train net output #0: loss = 2.0236 (* 1 = 2.0236 loss)
I0521 08:49:15.354957 15209 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0521 08:49:45.453420 15209 solver.cpp:237] Iteration 459, loss = 2.01255
I0521 08:49:45.453588 15209 solver.cpp:253]     Train net output #0: loss = 2.01255 (* 1 = 2.01255 loss)
I0521 08:49:45.453603 15209 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0521 08:49:53.392336 15209 solver.cpp:237] Iteration 476, loss = 1.98445
I0521 08:49:53.392369 15209 solver.cpp:253]     Train net output #0: loss = 1.98445 (* 1 = 1.98445 loss)
I0521 08:49:53.392385 15209 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0521 08:50:01.334919 15209 solver.cpp:237] Iteration 493, loss = 1.97323
I0521 08:50:01.334952 15209 solver.cpp:253]     Train net output #0: loss = 1.97323 (* 1 = 1.97323 loss)
I0521 08:50:01.334967 15209 sgd_solver.cpp:106] Iteration 493, lr = 0.0025
I0521 08:50:09.281385 15209 solver.cpp:237] Iteration 510, loss = 1.95122
I0521 08:50:09.281419 15209 solver.cpp:253]     Train net output #0: loss = 1.95122 (* 1 = 1.95122 loss)
I0521 08:50:09.281435 15209 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 08:50:11.619998 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_516.caffemodel
I0521 08:50:11.987843 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_516.solverstate
I0521 08:50:17.291353 15209 solver.cpp:237] Iteration 527, loss = 1.96636
I0521 08:50:17.291528 15209 solver.cpp:253]     Train net output #0: loss = 1.96636 (* 1 = 1.96636 loss)
I0521 08:50:17.291543 15209 sgd_solver.cpp:106] Iteration 527, lr = 0.0025
I0521 08:50:25.237251 15209 solver.cpp:237] Iteration 544, loss = 1.99191
I0521 08:50:25.237282 15209 solver.cpp:253]     Train net output #0: loss = 1.99191 (* 1 = 1.99191 loss)
I0521 08:50:25.237298 15209 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 08:50:33.180549 15209 solver.cpp:237] Iteration 561, loss = 1.95158
I0521 08:50:33.180600 15209 solver.cpp:253]     Train net output #0: loss = 1.95158 (* 1 = 1.95158 loss)
I0521 08:50:33.180614 15209 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0521 08:51:03.279829 15209 solver.cpp:237] Iteration 578, loss = 1.92657
I0521 08:51:03.279994 15209 solver.cpp:253]     Train net output #0: loss = 1.92657 (* 1 = 1.92657 loss)
I0521 08:51:03.280009 15209 sgd_solver.cpp:106] Iteration 578, lr = 0.0025
I0521 08:51:11.221626 15209 solver.cpp:237] Iteration 595, loss = 1.93521
I0521 08:51:11.221658 15209 solver.cpp:253]     Train net output #0: loss = 1.93521 (* 1 = 1.93521 loss)
I0521 08:51:11.221674 15209 sgd_solver.cpp:106] Iteration 595, lr = 0.0025
I0521 08:51:19.166144 15209 solver.cpp:237] Iteration 612, loss = 1.91486
I0521 08:51:19.166187 15209 solver.cpp:253]     Train net output #0: loss = 1.91486 (* 1 = 1.91486 loss)
I0521 08:51:19.166199 15209 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 08:51:27.107214 15209 solver.cpp:237] Iteration 629, loss = 1.98504
I0521 08:51:27.107247 15209 solver.cpp:253]     Train net output #0: loss = 1.98504 (* 1 = 1.98504 loss)
I0521 08:51:27.107262 15209 sgd_solver.cpp:106] Iteration 629, lr = 0.0025
I0521 08:51:35.056761 15209 solver.cpp:237] Iteration 646, loss = 1.87049
I0521 08:51:35.056893 15209 solver.cpp:253]     Train net output #0: loss = 1.87049 (* 1 = 1.87049 loss)
I0521 08:51:35.056906 15209 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0521 08:51:43.004209 15209 solver.cpp:237] Iteration 663, loss = 1.85891
I0521 08:51:43.004257 15209 solver.cpp:253]     Train net output #0: loss = 1.85891 (* 1 = 1.85891 loss)
I0521 08:51:43.004271 15209 sgd_solver.cpp:106] Iteration 663, lr = 0.0025
I0521 08:51:50.945572 15209 solver.cpp:237] Iteration 680, loss = 1.7964
I0521 08:51:50.945605 15209 solver.cpp:253]     Train net output #0: loss = 1.7964 (* 1 = 1.7964 loss)
I0521 08:51:50.945621 15209 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0521 08:51:54.216223 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_688.caffemodel
I0521 08:51:54.584192 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_688.solverstate
I0521 08:51:54.611737 15209 solver.cpp:341] Iteration 688, Testing net (#0)
I0521 08:53:00.489568 15209 solver.cpp:409]     Test net output #0: accuracy = 0.592602
I0521 08:53:00.489740 15209 solver.cpp:409]     Test net output #1: loss = 1.48073 (* 1 = 1.48073 loss)
I0521 08:53:26.963629 15209 solver.cpp:237] Iteration 697, loss = 1.89804
I0521 08:53:26.963680 15209 solver.cpp:253]     Train net output #0: loss = 1.89804 (* 1 = 1.89804 loss)
I0521 08:53:26.963695 15209 sgd_solver.cpp:106] Iteration 697, lr = 0.0025
I0521 08:53:34.899698 15209 solver.cpp:237] Iteration 714, loss = 1.8914
I0521 08:53:34.899847 15209 solver.cpp:253]     Train net output #0: loss = 1.8914 (* 1 = 1.8914 loss)
I0521 08:53:34.899862 15209 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0521 08:53:42.835697 15209 solver.cpp:237] Iteration 731, loss = 1.80514
I0521 08:53:42.835729 15209 solver.cpp:253]     Train net output #0: loss = 1.80514 (* 1 = 1.80514 loss)
I0521 08:53:42.835744 15209 sgd_solver.cpp:106] Iteration 731, lr = 0.0025
I0521 08:53:50.775367 15209 solver.cpp:237] Iteration 748, loss = 1.8512
I0521 08:53:50.775403 15209 solver.cpp:253]     Train net output #0: loss = 1.8512 (* 1 = 1.8512 loss)
I0521 08:53:50.775418 15209 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 08:53:58.711534 15209 solver.cpp:237] Iteration 765, loss = 1.84205
I0521 08:53:58.711565 15209 solver.cpp:253]     Train net output #0: loss = 1.84205 (* 1 = 1.84205 loss)
I0521 08:53:58.711580 15209 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 08:54:06.649852 15209 solver.cpp:237] Iteration 782, loss = 1.85078
I0521 08:54:06.649996 15209 solver.cpp:253]     Train net output #0: loss = 1.85078 (* 1 = 1.85078 loss)
I0521 08:54:06.650009 15209 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0521 08:54:14.587723 15209 solver.cpp:237] Iteration 799, loss = 1.8578
I0521 08:54:14.587760 15209 solver.cpp:253]     Train net output #0: loss = 1.8578 (* 1 = 1.8578 loss)
I0521 08:54:14.587779 15209 sgd_solver.cpp:106] Iteration 799, lr = 0.0025
I0521 08:54:44.683620 15209 solver.cpp:237] Iteration 816, loss = 1.84386
I0521 08:54:44.683784 15209 solver.cpp:253]     Train net output #0: loss = 1.84386 (* 1 = 1.84386 loss)
I0521 08:54:44.683801 15209 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 08:54:52.621907 15209 solver.cpp:237] Iteration 833, loss = 1.8764
I0521 08:54:52.621939 15209 solver.cpp:253]     Train net output #0: loss = 1.8764 (* 1 = 1.8764 loss)
I0521 08:54:52.621955 15209 sgd_solver.cpp:106] Iteration 833, lr = 0.0025
I0521 08:55:00.553927 15209 solver.cpp:237] Iteration 850, loss = 1.78746
I0521 08:55:00.553961 15209 solver.cpp:253]     Train net output #0: loss = 1.78746 (* 1 = 1.78746 loss)
I0521 08:55:00.553974 15209 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0521 08:55:04.753319 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_860.caffemodel
I0521 08:55:05.122501 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_860.solverstate
I0521 08:55:08.558686 15209 solver.cpp:237] Iteration 867, loss = 1.83513
I0521 08:55:08.558734 15209 solver.cpp:253]     Train net output #0: loss = 1.83513 (* 1 = 1.83513 loss)
I0521 08:55:08.558748 15209 sgd_solver.cpp:106] Iteration 867, lr = 0.0025
I0521 08:55:16.498100 15209 solver.cpp:237] Iteration 884, loss = 1.87739
I0521 08:55:16.498242 15209 solver.cpp:253]     Train net output #0: loss = 1.87739 (* 1 = 1.87739 loss)
I0521 08:55:16.498255 15209 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0521 08:55:24.431268 15209 solver.cpp:237] Iteration 901, loss = 1.79509
I0521 08:55:24.431299 15209 solver.cpp:253]     Train net output #0: loss = 1.79509 (* 1 = 1.79509 loss)
I0521 08:55:24.431315 15209 sgd_solver.cpp:106] Iteration 901, lr = 0.0025
I0521 08:55:32.371497 15209 solver.cpp:237] Iteration 918, loss = 1.80949
I0521 08:55:32.371528 15209 solver.cpp:253]     Train net output #0: loss = 1.80949 (* 1 = 1.80949 loss)
I0521 08:55:32.371541 15209 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 08:56:02.474267 15209 solver.cpp:237] Iteration 935, loss = 1.76277
I0521 08:56:02.474444 15209 solver.cpp:253]     Train net output #0: loss = 1.76277 (* 1 = 1.76277 loss)
I0521 08:56:02.474458 15209 sgd_solver.cpp:106] Iteration 935, lr = 0.0025
I0521 08:56:10.409489 15209 solver.cpp:237] Iteration 952, loss = 1.85917
I0521 08:56:10.409521 15209 solver.cpp:253]     Train net output #0: loss = 1.85917 (* 1 = 1.85917 loss)
I0521 08:56:10.409536 15209 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0521 08:56:18.347183 15209 solver.cpp:237] Iteration 969, loss = 1.76945
I0521 08:56:18.347229 15209 solver.cpp:253]     Train net output #0: loss = 1.76945 (* 1 = 1.76945 loss)
I0521 08:56:18.347242 15209 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0521 08:56:26.283264 15209 solver.cpp:237] Iteration 986, loss = 1.71367
I0521 08:56:26.283296 15209 solver.cpp:253]     Train net output #0: loss = 1.71367 (* 1 = 1.71367 loss)
I0521 08:56:26.283310 15209 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0521 08:56:34.217007 15209 solver.cpp:237] Iteration 1003, loss = 1.75386
I0521 08:56:34.217156 15209 solver.cpp:253]     Train net output #0: loss = 1.75386 (* 1 = 1.75386 loss)
I0521 08:56:34.217170 15209 sgd_solver.cpp:106] Iteration 1003, lr = 0.0025
I0521 08:56:42.153383 15209 solver.cpp:237] Iteration 1020, loss = 1.84324
I0521 08:56:42.153420 15209 solver.cpp:253]     Train net output #0: loss = 1.84324 (* 1 = 1.84324 loss)
I0521 08:56:42.153439 15209 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 08:56:47.286936 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1032.caffemodel
I0521 08:56:47.652359 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1032.solverstate
I0521 08:56:47.678333 15209 solver.cpp:341] Iteration 1032, Testing net (#0)
I0521 08:57:32.380604 15209 solver.cpp:409]     Test net output #0: accuracy = 0.629818
I0521 08:57:32.380762 15209 solver.cpp:409]     Test net output #1: loss = 1.33399 (* 1 = 1.33399 loss)
I0521 08:57:57.028899 15209 solver.cpp:237] Iteration 1037, loss = 1.79821
I0521 08:57:57.028947 15209 solver.cpp:253]     Train net output #0: loss = 1.79821 (* 1 = 1.79821 loss)
I0521 08:57:57.028964 15209 sgd_solver.cpp:106] Iteration 1037, lr = 0.0025
I0521 08:58:04.969287 15209 solver.cpp:237] Iteration 1054, loss = 1.72214
I0521 08:58:04.969444 15209 solver.cpp:253]     Train net output #0: loss = 1.72214 (* 1 = 1.72214 loss)
I0521 08:58:04.969457 15209 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0521 08:58:12.900954 15209 solver.cpp:237] Iteration 1071, loss = 1.76312
I0521 08:58:12.900985 15209 solver.cpp:253]     Train net output #0: loss = 1.76312 (* 1 = 1.76312 loss)
I0521 08:58:12.901000 15209 sgd_solver.cpp:106] Iteration 1071, lr = 0.0025
I0521 08:58:20.839473 15209 solver.cpp:237] Iteration 1088, loss = 1.78388
I0521 08:58:20.839504 15209 solver.cpp:253]     Train net output #0: loss = 1.78388 (* 1 = 1.78388 loss)
I0521 08:58:20.839519 15209 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 08:58:28.781970 15209 solver.cpp:237] Iteration 1105, loss = 1.79876
I0521 08:58:28.782012 15209 solver.cpp:253]     Train net output #0: loss = 1.79876 (* 1 = 1.79876 loss)
I0521 08:58:28.782027 15209 sgd_solver.cpp:106] Iteration 1105, lr = 0.0025
I0521 08:58:36.719182 15209 solver.cpp:237] Iteration 1122, loss = 1.7779
I0521 08:58:36.719317 15209 solver.cpp:253]     Train net output #0: loss = 1.7779 (* 1 = 1.7779 loss)
I0521 08:58:36.719331 15209 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 08:58:44.661177 15209 solver.cpp:237] Iteration 1139, loss = 1.78226
I0521 08:58:44.661208 15209 solver.cpp:253]     Train net output #0: loss = 1.78226 (* 1 = 1.78226 loss)
I0521 08:58:44.661224 15209 sgd_solver.cpp:106] Iteration 1139, lr = 0.0025
I0521 08:59:14.745228 15209 solver.cpp:237] Iteration 1156, loss = 1.76494
I0521 08:59:14.745410 15209 solver.cpp:253]     Train net output #0: loss = 1.76494 (* 1 = 1.76494 loss)
I0521 08:59:14.745425 15209 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0521 08:59:22.685798 15209 solver.cpp:237] Iteration 1173, loss = 1.74553
I0521 08:59:22.685832 15209 solver.cpp:253]     Train net output #0: loss = 1.74553 (* 1 = 1.74553 loss)
I0521 08:59:22.685847 15209 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0521 08:59:30.621085 15209 solver.cpp:237] Iteration 1190, loss = 1.7472
I0521 08:59:30.621117 15209 solver.cpp:253]     Train net output #0: loss = 1.7472 (* 1 = 1.7472 loss)
I0521 08:59:30.621132 15209 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0521 08:59:36.697192 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1204.caffemodel
I0521 08:59:37.063120 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1204.solverstate
I0521 08:59:38.631021 15209 solver.cpp:237] Iteration 1207, loss = 1.75759
I0521 08:59:38.631067 15209 solver.cpp:253]     Train net output #0: loss = 1.75759 (* 1 = 1.75759 loss)
I0521 08:59:38.631081 15209 sgd_solver.cpp:106] Iteration 1207, lr = 0.0025
I0521 08:59:46.574978 15209 solver.cpp:237] Iteration 1224, loss = 1.7888
I0521 08:59:46.575121 15209 solver.cpp:253]     Train net output #0: loss = 1.7888 (* 1 = 1.7888 loss)
I0521 08:59:46.575135 15209 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 08:59:54.512310 15209 solver.cpp:237] Iteration 1241, loss = 1.79347
I0521 08:59:54.512341 15209 solver.cpp:253]     Train net output #0: loss = 1.79347 (* 1 = 1.79347 loss)
I0521 08:59:54.512357 15209 sgd_solver.cpp:106] Iteration 1241, lr = 0.0025
I0521 09:00:02.455121 15209 solver.cpp:237] Iteration 1258, loss = 1.73584
I0521 09:00:02.455163 15209 solver.cpp:253]     Train net output #0: loss = 1.73584 (* 1 = 1.73584 loss)
I0521 09:00:02.455178 15209 sgd_solver.cpp:106] Iteration 1258, lr = 0.0025
I0521 09:00:32.549677 15209 solver.cpp:237] Iteration 1275, loss = 1.72315
I0521 09:00:32.549847 15209 solver.cpp:253]     Train net output #0: loss = 1.72315 (* 1 = 1.72315 loss)
I0521 09:00:32.549865 15209 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 09:00:40.488337 15209 solver.cpp:237] Iteration 1292, loss = 1.78623
I0521 09:00:40.488369 15209 solver.cpp:253]     Train net output #0: loss = 1.78623 (* 1 = 1.78623 loss)
I0521 09:00:40.488385 15209 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0521 09:00:48.429164 15209 solver.cpp:237] Iteration 1309, loss = 1.66292
I0521 09:00:48.429198 15209 solver.cpp:253]     Train net output #0: loss = 1.66292 (* 1 = 1.66292 loss)
I0521 09:00:48.429213 15209 sgd_solver.cpp:106] Iteration 1309, lr = 0.0025
I0521 09:00:56.364747 15209 solver.cpp:237] Iteration 1326, loss = 1.7427
I0521 09:00:56.364792 15209 solver.cpp:253]     Train net output #0: loss = 1.7427 (* 1 = 1.7427 loss)
I0521 09:00:56.364806 15209 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0521 09:01:04.304039 15209 solver.cpp:237] Iteration 1343, loss = 1.72206
I0521 09:01:04.304178 15209 solver.cpp:253]     Train net output #0: loss = 1.72206 (* 1 = 1.72206 loss)
I0521 09:01:04.304193 15209 sgd_solver.cpp:106] Iteration 1343, lr = 0.0025
I0521 09:01:12.244637 15209 solver.cpp:237] Iteration 1360, loss = 1.72208
I0521 09:01:12.244669 15209 solver.cpp:253]     Train net output #0: loss = 1.72208 (* 1 = 1.72208 loss)
I0521 09:01:12.244684 15209 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 09:01:19.250305 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1376.caffemodel
I0521 09:01:19.616775 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1376.solverstate
I0521 09:01:19.642890 15209 solver.cpp:341] Iteration 1376, Testing net (#0)
I0521 09:02:25.548429 15209 solver.cpp:409]     Test net output #0: accuracy = 0.648911
I0521 09:02:25.548610 15209 solver.cpp:409]     Test net output #1: loss = 1.2095 (* 1 = 1.2095 loss)
I0521 09:02:26.156280 15209 solver.cpp:237] Iteration 1377, loss = 1.74364
I0521 09:02:26.156311 15209 solver.cpp:253]     Train net output #0: loss = 1.74364 (* 1 = 1.74364 loss)
I0521 09:02:26.156325 15209 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0521 09:02:56.256021 15209 solver.cpp:237] Iteration 1394, loss = 1.72249
I0521 09:02:56.256186 15209 solver.cpp:253]     Train net output #0: loss = 1.72249 (* 1 = 1.72249 loss)
I0521 09:02:56.256201 15209 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0521 09:03:04.201910 15209 solver.cpp:237] Iteration 1411, loss = 1.72622
I0521 09:03:04.201946 15209 solver.cpp:253]     Train net output #0: loss = 1.72622 (* 1 = 1.72622 loss)
I0521 09:03:04.201964 15209 sgd_solver.cpp:106] Iteration 1411, lr = 0.0025
I0521 09:03:12.149132 15209 solver.cpp:237] Iteration 1428, loss = 1.77264
I0521 09:03:12.149163 15209 solver.cpp:253]     Train net output #0: loss = 1.77264 (* 1 = 1.77264 loss)
I0521 09:03:12.149179 15209 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 09:03:20.088402 15209 solver.cpp:237] Iteration 1445, loss = 1.71458
I0521 09:03:20.088434 15209 solver.cpp:253]     Train net output #0: loss = 1.71458 (* 1 = 1.71458 loss)
I0521 09:03:20.088449 15209 sgd_solver.cpp:106] Iteration 1445, lr = 0.0025
I0521 09:03:28.029656 15209 solver.cpp:237] Iteration 1462, loss = 1.83605
I0521 09:03:28.029808 15209 solver.cpp:253]     Train net output #0: loss = 1.83605 (* 1 = 1.83605 loss)
I0521 09:03:28.029822 15209 sgd_solver.cpp:106] Iteration 1462, lr = 0.0025
I0521 09:03:35.967811 15209 solver.cpp:237] Iteration 1479, loss = 1.73226
I0521 09:03:35.967842 15209 solver.cpp:253]     Train net output #0: loss = 1.73226 (* 1 = 1.73226 loss)
I0521 09:03:35.967859 15209 sgd_solver.cpp:106] Iteration 1479, lr = 0.0025
I0521 09:04:06.075309 15209 solver.cpp:237] Iteration 1496, loss = 1.77973
I0521 09:04:06.075482 15209 solver.cpp:253]     Train net output #0: loss = 1.77973 (* 1 = 1.77973 loss)
I0521 09:04:06.075500 15209 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 09:04:14.022073 15209 solver.cpp:237] Iteration 1513, loss = 1.702
I0521 09:04:14.022112 15209 solver.cpp:253]     Train net output #0: loss = 1.702 (* 1 = 1.702 loss)
I0521 09:04:14.022131 15209 sgd_solver.cpp:106] Iteration 1513, lr = 0.0025
I0521 09:04:21.963788 15209 solver.cpp:237] Iteration 1530, loss = 1.77108
I0521 09:04:21.963821 15209 solver.cpp:253]     Train net output #0: loss = 1.77108 (* 1 = 1.77108 loss)
I0521 09:04:21.963835 15209 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 09:04:29.905074 15209 solver.cpp:237] Iteration 1547, loss = 1.66511
I0521 09:04:29.905107 15209 solver.cpp:253]     Train net output #0: loss = 1.66511 (* 1 = 1.66511 loss)
I0521 09:04:29.905122 15209 sgd_solver.cpp:106] Iteration 1547, lr = 0.0025
I0521 09:04:29.905546 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1548.caffemodel
I0521 09:04:30.273414 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1548.solverstate
I0521 09:04:37.907017 15209 solver.cpp:237] Iteration 1564, loss = 1.66121
I0521 09:04:37.907191 15209 solver.cpp:253]     Train net output #0: loss = 1.66121 (* 1 = 1.66121 loss)
I0521 09:04:37.907205 15209 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0521 09:04:45.849932 15209 solver.cpp:237] Iteration 1581, loss = 1.76504
I0521 09:04:45.849963 15209 solver.cpp:253]     Train net output #0: loss = 1.76504 (* 1 = 1.76504 loss)
I0521 09:04:45.849980 15209 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0521 09:04:53.785907 15209 solver.cpp:237] Iteration 1598, loss = 1.63393
I0521 09:04:53.785941 15209 solver.cpp:253]     Train net output #0: loss = 1.63393 (* 1 = 1.63393 loss)
I0521 09:04:53.785954 15209 sgd_solver.cpp:106] Iteration 1598, lr = 0.0025
I0521 09:05:23.921272 15209 solver.cpp:237] Iteration 1615, loss = 1.71008
I0521 09:05:23.921447 15209 solver.cpp:253]     Train net output #0: loss = 1.71008 (* 1 = 1.71008 loss)
I0521 09:05:23.921463 15209 sgd_solver.cpp:106] Iteration 1615, lr = 0.0025
I0521 09:05:31.868221 15209 solver.cpp:237] Iteration 1632, loss = 1.80862
I0521 09:05:31.868259 15209 solver.cpp:253]     Train net output #0: loss = 1.80862 (* 1 = 1.80862 loss)
I0521 09:05:31.868273 15209 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 09:05:39.809161 15209 solver.cpp:237] Iteration 1649, loss = 1.70841
I0521 09:05:39.809195 15209 solver.cpp:253]     Train net output #0: loss = 1.70841 (* 1 = 1.70841 loss)
I0521 09:05:39.809209 15209 sgd_solver.cpp:106] Iteration 1649, lr = 0.0025
I0521 09:05:47.750896 15209 solver.cpp:237] Iteration 1666, loss = 1.70072
I0521 09:05:47.750928 15209 solver.cpp:253]     Train net output #0: loss = 1.70072 (* 1 = 1.70072 loss)
I0521 09:05:47.750943 15209 sgd_solver.cpp:106] Iteration 1666, lr = 0.0025
I0521 09:05:55.697765 15209 solver.cpp:237] Iteration 1683, loss = 1.72566
I0521 09:05:55.697922 15209 solver.cpp:253]     Train net output #0: loss = 1.72566 (* 1 = 1.72566 loss)
I0521 09:05:55.697937 15209 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0521 09:06:03.642838 15209 solver.cpp:237] Iteration 1700, loss = 1.68548
I0521 09:06:03.642870 15209 solver.cpp:253]     Train net output #0: loss = 1.68548 (* 1 = 1.68548 loss)
I0521 09:06:03.642886 15209 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 09:06:11.589463 15209 solver.cpp:237] Iteration 1717, loss = 1.61313
I0521 09:06:11.589495 15209 solver.cpp:253]     Train net output #0: loss = 1.61313 (* 1 = 1.61313 loss)
I0521 09:06:11.589511 15209 sgd_solver.cpp:106] Iteration 1717, lr = 0.0025
I0521 09:06:12.524905 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1720.caffemodel
I0521 09:06:12.893563 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1720.solverstate
I0521 09:06:12.921463 15209 solver.cpp:341] Iteration 1720, Testing net (#0)
I0521 09:06:57.951341 15209 solver.cpp:409]     Test net output #0: accuracy = 0.667195
I0521 09:06:57.951506 15209 solver.cpp:409]     Test net output #1: loss = 1.15635 (* 1 = 1.15635 loss)
I0521 09:06:59.491168 15209 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1724.caffemodel
I0521 09:06:59.859236 15209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556_iter_1724.solverstate
I0521 09:06:59.887106 15209 solver.cpp:326] Optimization Done.
I0521 09:06:59.887135 15209 caffe.cpp:215] Optimization Done.
Application 11237325 resources: utime ~1250s, stime ~225s, Rss ~5329312, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_870_2016-05-20T11.21.04.451556.solver"
	User time (seconds): 0.56
	System time (seconds): 0.13
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:37.88
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15073
	Voluntary context switches: 2646
	Involuntary context switches: 87
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
