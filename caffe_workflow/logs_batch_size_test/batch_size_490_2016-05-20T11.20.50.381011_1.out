2806123
I0521 00:52:23.572753  6829 caffe.cpp:184] Using GPUs 0
I0521 00:52:23.993949  6829 solver.cpp:48] Initializing solver from parameters: 
test_iter: 306
test_interval: 612
base_lr: 0.0025
display: 30
max_iter: 3061
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 306
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011.prototxt"
I0521 00:52:23.995678  6829 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011.prototxt
I0521 00:52:24.010056  6829 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 00:52:24.010120  6829 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 00:52:24.010462  6829 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 490
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:52:24.010640  6829 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:52:24.010664  6829 net.cpp:106] Creating Layer data_hdf5
I0521 00:52:24.010679  6829 net.cpp:411] data_hdf5 -> data
I0521 00:52:24.010713  6829 net.cpp:411] data_hdf5 -> label
I0521 00:52:24.010746  6829 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 00:52:24.012038  6829 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 00:52:24.014262  6829 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 00:52:45.491724  6829 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 00:52:45.496933  6829 net.cpp:150] Setting up data_hdf5
I0521 00:52:45.496973  6829 net.cpp:157] Top shape: 490 1 127 50 (3111500)
I0521 00:52:45.496989  6829 net.cpp:157] Top shape: 490 (490)
I0521 00:52:45.496999  6829 net.cpp:165] Memory required for data: 12447960
I0521 00:52:45.497011  6829 layer_factory.hpp:77] Creating layer conv1
I0521 00:52:45.497046  6829 net.cpp:106] Creating Layer conv1
I0521 00:52:45.497058  6829 net.cpp:454] conv1 <- data
I0521 00:52:45.497081  6829 net.cpp:411] conv1 -> conv1
I0521 00:52:47.328523  6829 net.cpp:150] Setting up conv1
I0521 00:52:47.328570  6829 net.cpp:157] Top shape: 490 12 120 48 (33868800)
I0521 00:52:47.328582  6829 net.cpp:165] Memory required for data: 147923160
I0521 00:52:47.328611  6829 layer_factory.hpp:77] Creating layer relu1
I0521 00:52:47.328632  6829 net.cpp:106] Creating Layer relu1
I0521 00:52:47.328644  6829 net.cpp:454] relu1 <- conv1
I0521 00:52:47.328656  6829 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:52:47.329174  6829 net.cpp:150] Setting up relu1
I0521 00:52:47.329190  6829 net.cpp:157] Top shape: 490 12 120 48 (33868800)
I0521 00:52:47.329201  6829 net.cpp:165] Memory required for data: 283398360
I0521 00:52:47.329213  6829 layer_factory.hpp:77] Creating layer pool1
I0521 00:52:47.329229  6829 net.cpp:106] Creating Layer pool1
I0521 00:52:47.329239  6829 net.cpp:454] pool1 <- conv1
I0521 00:52:47.329253  6829 net.cpp:411] pool1 -> pool1
I0521 00:52:47.329334  6829 net.cpp:150] Setting up pool1
I0521 00:52:47.329347  6829 net.cpp:157] Top shape: 490 12 60 48 (16934400)
I0521 00:52:47.329357  6829 net.cpp:165] Memory required for data: 351135960
I0521 00:52:47.329368  6829 layer_factory.hpp:77] Creating layer conv2
I0521 00:52:47.329391  6829 net.cpp:106] Creating Layer conv2
I0521 00:52:47.329401  6829 net.cpp:454] conv2 <- pool1
I0521 00:52:47.329414  6829 net.cpp:411] conv2 -> conv2
I0521 00:52:47.332088  6829 net.cpp:150] Setting up conv2
I0521 00:52:47.332115  6829 net.cpp:157] Top shape: 490 20 54 46 (24343200)
I0521 00:52:47.332126  6829 net.cpp:165] Memory required for data: 448508760
I0521 00:52:47.332146  6829 layer_factory.hpp:77] Creating layer relu2
I0521 00:52:47.332160  6829 net.cpp:106] Creating Layer relu2
I0521 00:52:47.332170  6829 net.cpp:454] relu2 <- conv2
I0521 00:52:47.332182  6829 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:52:47.332514  6829 net.cpp:150] Setting up relu2
I0521 00:52:47.332528  6829 net.cpp:157] Top shape: 490 20 54 46 (24343200)
I0521 00:52:47.332538  6829 net.cpp:165] Memory required for data: 545881560
I0521 00:52:47.332548  6829 layer_factory.hpp:77] Creating layer pool2
I0521 00:52:47.332561  6829 net.cpp:106] Creating Layer pool2
I0521 00:52:47.332571  6829 net.cpp:454] pool2 <- conv2
I0521 00:52:47.332597  6829 net.cpp:411] pool2 -> pool2
I0521 00:52:47.332664  6829 net.cpp:150] Setting up pool2
I0521 00:52:47.332679  6829 net.cpp:157] Top shape: 490 20 27 46 (12171600)
I0521 00:52:47.332689  6829 net.cpp:165] Memory required for data: 594567960
I0521 00:52:47.332697  6829 layer_factory.hpp:77] Creating layer conv3
I0521 00:52:47.332716  6829 net.cpp:106] Creating Layer conv3
I0521 00:52:47.332726  6829 net.cpp:454] conv3 <- pool2
I0521 00:52:47.332741  6829 net.cpp:411] conv3 -> conv3
I0521 00:52:47.334655  6829 net.cpp:150] Setting up conv3
I0521 00:52:47.334678  6829 net.cpp:157] Top shape: 490 28 22 44 (13280960)
I0521 00:52:47.334691  6829 net.cpp:165] Memory required for data: 647691800
I0521 00:52:47.334708  6829 layer_factory.hpp:77] Creating layer relu3
I0521 00:52:47.334724  6829 net.cpp:106] Creating Layer relu3
I0521 00:52:47.334734  6829 net.cpp:454] relu3 <- conv3
I0521 00:52:47.334748  6829 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:52:47.335216  6829 net.cpp:150] Setting up relu3
I0521 00:52:47.335233  6829 net.cpp:157] Top shape: 490 28 22 44 (13280960)
I0521 00:52:47.335243  6829 net.cpp:165] Memory required for data: 700815640
I0521 00:52:47.335253  6829 layer_factory.hpp:77] Creating layer pool3
I0521 00:52:47.335266  6829 net.cpp:106] Creating Layer pool3
I0521 00:52:47.335276  6829 net.cpp:454] pool3 <- conv3
I0521 00:52:47.335289  6829 net.cpp:411] pool3 -> pool3
I0521 00:52:47.335366  6829 net.cpp:150] Setting up pool3
I0521 00:52:47.335378  6829 net.cpp:157] Top shape: 490 28 11 44 (6640480)
I0521 00:52:47.335388  6829 net.cpp:165] Memory required for data: 727377560
I0521 00:52:47.335397  6829 layer_factory.hpp:77] Creating layer conv4
I0521 00:52:47.335415  6829 net.cpp:106] Creating Layer conv4
I0521 00:52:47.335425  6829 net.cpp:454] conv4 <- pool3
I0521 00:52:47.335440  6829 net.cpp:411] conv4 -> conv4
I0521 00:52:47.338199  6829 net.cpp:150] Setting up conv4
I0521 00:52:47.338228  6829 net.cpp:157] Top shape: 490 36 6 42 (4445280)
I0521 00:52:47.338239  6829 net.cpp:165] Memory required for data: 745158680
I0521 00:52:47.338254  6829 layer_factory.hpp:77] Creating layer relu4
I0521 00:52:47.338269  6829 net.cpp:106] Creating Layer relu4
I0521 00:52:47.338279  6829 net.cpp:454] relu4 <- conv4
I0521 00:52:47.338292  6829 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:52:47.338763  6829 net.cpp:150] Setting up relu4
I0521 00:52:47.338779  6829 net.cpp:157] Top shape: 490 36 6 42 (4445280)
I0521 00:52:47.338790  6829 net.cpp:165] Memory required for data: 762939800
I0521 00:52:47.338800  6829 layer_factory.hpp:77] Creating layer pool4
I0521 00:52:47.338814  6829 net.cpp:106] Creating Layer pool4
I0521 00:52:47.338824  6829 net.cpp:454] pool4 <- conv4
I0521 00:52:47.338836  6829 net.cpp:411] pool4 -> pool4
I0521 00:52:47.338903  6829 net.cpp:150] Setting up pool4
I0521 00:52:47.338917  6829 net.cpp:157] Top shape: 490 36 3 42 (2222640)
I0521 00:52:47.338928  6829 net.cpp:165] Memory required for data: 771830360
I0521 00:52:47.338937  6829 layer_factory.hpp:77] Creating layer ip1
I0521 00:52:47.338955  6829 net.cpp:106] Creating Layer ip1
I0521 00:52:47.338966  6829 net.cpp:454] ip1 <- pool4
I0521 00:52:47.338979  6829 net.cpp:411] ip1 -> ip1
I0521 00:52:47.354405  6829 net.cpp:150] Setting up ip1
I0521 00:52:47.354434  6829 net.cpp:157] Top shape: 490 196 (96040)
I0521 00:52:47.354446  6829 net.cpp:165] Memory required for data: 772214520
I0521 00:52:47.354467  6829 layer_factory.hpp:77] Creating layer relu5
I0521 00:52:47.354482  6829 net.cpp:106] Creating Layer relu5
I0521 00:52:47.354492  6829 net.cpp:454] relu5 <- ip1
I0521 00:52:47.354506  6829 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:52:47.354851  6829 net.cpp:150] Setting up relu5
I0521 00:52:47.354866  6829 net.cpp:157] Top shape: 490 196 (96040)
I0521 00:52:47.354876  6829 net.cpp:165] Memory required for data: 772598680
I0521 00:52:47.354885  6829 layer_factory.hpp:77] Creating layer drop1
I0521 00:52:47.354907  6829 net.cpp:106] Creating Layer drop1
I0521 00:52:47.354918  6829 net.cpp:454] drop1 <- ip1
I0521 00:52:47.354943  6829 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:52:47.354989  6829 net.cpp:150] Setting up drop1
I0521 00:52:47.355002  6829 net.cpp:157] Top shape: 490 196 (96040)
I0521 00:52:47.355012  6829 net.cpp:165] Memory required for data: 772982840
I0521 00:52:47.355022  6829 layer_factory.hpp:77] Creating layer ip2
I0521 00:52:47.355041  6829 net.cpp:106] Creating Layer ip2
I0521 00:52:47.355051  6829 net.cpp:454] ip2 <- ip1
I0521 00:52:47.355064  6829 net.cpp:411] ip2 -> ip2
I0521 00:52:47.355537  6829 net.cpp:150] Setting up ip2
I0521 00:52:47.355551  6829 net.cpp:157] Top shape: 490 98 (48020)
I0521 00:52:47.355561  6829 net.cpp:165] Memory required for data: 773174920
I0521 00:52:47.355576  6829 layer_factory.hpp:77] Creating layer relu6
I0521 00:52:47.355588  6829 net.cpp:106] Creating Layer relu6
I0521 00:52:47.355598  6829 net.cpp:454] relu6 <- ip2
I0521 00:52:47.355610  6829 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:52:47.356130  6829 net.cpp:150] Setting up relu6
I0521 00:52:47.356147  6829 net.cpp:157] Top shape: 490 98 (48020)
I0521 00:52:47.356158  6829 net.cpp:165] Memory required for data: 773367000
I0521 00:52:47.356168  6829 layer_factory.hpp:77] Creating layer drop2
I0521 00:52:47.356180  6829 net.cpp:106] Creating Layer drop2
I0521 00:52:47.356190  6829 net.cpp:454] drop2 <- ip2
I0521 00:52:47.356204  6829 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:52:47.356245  6829 net.cpp:150] Setting up drop2
I0521 00:52:47.356258  6829 net.cpp:157] Top shape: 490 98 (48020)
I0521 00:52:47.356269  6829 net.cpp:165] Memory required for data: 773559080
I0521 00:52:47.356278  6829 layer_factory.hpp:77] Creating layer ip3
I0521 00:52:47.356292  6829 net.cpp:106] Creating Layer ip3
I0521 00:52:47.356302  6829 net.cpp:454] ip3 <- ip2
I0521 00:52:47.356315  6829 net.cpp:411] ip3 -> ip3
I0521 00:52:47.356528  6829 net.cpp:150] Setting up ip3
I0521 00:52:47.356540  6829 net.cpp:157] Top shape: 490 11 (5390)
I0521 00:52:47.356550  6829 net.cpp:165] Memory required for data: 773580640
I0521 00:52:47.356565  6829 layer_factory.hpp:77] Creating layer drop3
I0521 00:52:47.356578  6829 net.cpp:106] Creating Layer drop3
I0521 00:52:47.356588  6829 net.cpp:454] drop3 <- ip3
I0521 00:52:47.356600  6829 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:52:47.356639  6829 net.cpp:150] Setting up drop3
I0521 00:52:47.356652  6829 net.cpp:157] Top shape: 490 11 (5390)
I0521 00:52:47.356662  6829 net.cpp:165] Memory required for data: 773602200
I0521 00:52:47.356673  6829 layer_factory.hpp:77] Creating layer loss
I0521 00:52:47.356693  6829 net.cpp:106] Creating Layer loss
I0521 00:52:47.356703  6829 net.cpp:454] loss <- ip3
I0521 00:52:47.356714  6829 net.cpp:454] loss <- label
I0521 00:52:47.356726  6829 net.cpp:411] loss -> loss
I0521 00:52:47.356744  6829 layer_factory.hpp:77] Creating layer loss
I0521 00:52:47.357389  6829 net.cpp:150] Setting up loss
I0521 00:52:47.357408  6829 net.cpp:157] Top shape: (1)
I0521 00:52:47.357417  6829 net.cpp:160]     with loss weight 1
I0521 00:52:47.357458  6829 net.cpp:165] Memory required for data: 773602204
I0521 00:52:47.357470  6829 net.cpp:226] loss needs backward computation.
I0521 00:52:47.357481  6829 net.cpp:226] drop3 needs backward computation.
I0521 00:52:47.357489  6829 net.cpp:226] ip3 needs backward computation.
I0521 00:52:47.357501  6829 net.cpp:226] drop2 needs backward computation.
I0521 00:52:47.357509  6829 net.cpp:226] relu6 needs backward computation.
I0521 00:52:47.357519  6829 net.cpp:226] ip2 needs backward computation.
I0521 00:52:47.357530  6829 net.cpp:226] drop1 needs backward computation.
I0521 00:52:47.357539  6829 net.cpp:226] relu5 needs backward computation.
I0521 00:52:47.357549  6829 net.cpp:226] ip1 needs backward computation.
I0521 00:52:47.357559  6829 net.cpp:226] pool4 needs backward computation.
I0521 00:52:47.357569  6829 net.cpp:226] relu4 needs backward computation.
I0521 00:52:47.357579  6829 net.cpp:226] conv4 needs backward computation.
I0521 00:52:47.357590  6829 net.cpp:226] pool3 needs backward computation.
I0521 00:52:47.357609  6829 net.cpp:226] relu3 needs backward computation.
I0521 00:52:47.357616  6829 net.cpp:226] conv3 needs backward computation.
I0521 00:52:47.357628  6829 net.cpp:226] pool2 needs backward computation.
I0521 00:52:47.357638  6829 net.cpp:226] relu2 needs backward computation.
I0521 00:52:47.357648  6829 net.cpp:226] conv2 needs backward computation.
I0521 00:52:47.357661  6829 net.cpp:226] pool1 needs backward computation.
I0521 00:52:47.357671  6829 net.cpp:226] relu1 needs backward computation.
I0521 00:52:47.357681  6829 net.cpp:226] conv1 needs backward computation.
I0521 00:52:47.357692  6829 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:52:47.357702  6829 net.cpp:270] This network produces output loss
I0521 00:52:47.357725  6829 net.cpp:283] Network initialization done.
I0521 00:52:47.359283  6829 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011.prototxt
I0521 00:52:47.359367  6829 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 00:52:47.359724  6829 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 490
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:52:47.359913  6829 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:52:47.359930  6829 net.cpp:106] Creating Layer data_hdf5
I0521 00:52:47.359941  6829 net.cpp:411] data_hdf5 -> data
I0521 00:52:47.359958  6829 net.cpp:411] data_hdf5 -> label
I0521 00:52:47.359974  6829 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 00:52:47.361413  6829 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 00:53:08.638330  6829 net.cpp:150] Setting up data_hdf5
I0521 00:53:08.638494  6829 net.cpp:157] Top shape: 490 1 127 50 (3111500)
I0521 00:53:08.638509  6829 net.cpp:157] Top shape: 490 (490)
I0521 00:53:08.638520  6829 net.cpp:165] Memory required for data: 12447960
I0521 00:53:08.638533  6829 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 00:53:08.638562  6829 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 00:53:08.638573  6829 net.cpp:454] label_data_hdf5_1_split <- label
I0521 00:53:08.638588  6829 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 00:53:08.638610  6829 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 00:53:08.638684  6829 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 00:53:08.638697  6829 net.cpp:157] Top shape: 490 (490)
I0521 00:53:08.638710  6829 net.cpp:157] Top shape: 490 (490)
I0521 00:53:08.638720  6829 net.cpp:165] Memory required for data: 12451880
I0521 00:53:08.638730  6829 layer_factory.hpp:77] Creating layer conv1
I0521 00:53:08.638749  6829 net.cpp:106] Creating Layer conv1
I0521 00:53:08.638761  6829 net.cpp:454] conv1 <- data
I0521 00:53:08.638774  6829 net.cpp:411] conv1 -> conv1
I0521 00:53:08.640728  6829 net.cpp:150] Setting up conv1
I0521 00:53:08.640753  6829 net.cpp:157] Top shape: 490 12 120 48 (33868800)
I0521 00:53:08.640764  6829 net.cpp:165] Memory required for data: 147927080
I0521 00:53:08.640784  6829 layer_factory.hpp:77] Creating layer relu1
I0521 00:53:08.640799  6829 net.cpp:106] Creating Layer relu1
I0521 00:53:08.640810  6829 net.cpp:454] relu1 <- conv1
I0521 00:53:08.640823  6829 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:53:08.641320  6829 net.cpp:150] Setting up relu1
I0521 00:53:08.641336  6829 net.cpp:157] Top shape: 490 12 120 48 (33868800)
I0521 00:53:08.641346  6829 net.cpp:165] Memory required for data: 283402280
I0521 00:53:08.641357  6829 layer_factory.hpp:77] Creating layer pool1
I0521 00:53:08.641373  6829 net.cpp:106] Creating Layer pool1
I0521 00:53:08.641383  6829 net.cpp:454] pool1 <- conv1
I0521 00:53:08.641396  6829 net.cpp:411] pool1 -> pool1
I0521 00:53:08.641471  6829 net.cpp:150] Setting up pool1
I0521 00:53:08.641485  6829 net.cpp:157] Top shape: 490 12 60 48 (16934400)
I0521 00:53:08.641494  6829 net.cpp:165] Memory required for data: 351139880
I0521 00:53:08.641505  6829 layer_factory.hpp:77] Creating layer conv2
I0521 00:53:08.641522  6829 net.cpp:106] Creating Layer conv2
I0521 00:53:08.641533  6829 net.cpp:454] conv2 <- pool1
I0521 00:53:08.641547  6829 net.cpp:411] conv2 -> conv2
I0521 00:53:08.643472  6829 net.cpp:150] Setting up conv2
I0521 00:53:08.643496  6829 net.cpp:157] Top shape: 490 20 54 46 (24343200)
I0521 00:53:08.643508  6829 net.cpp:165] Memory required for data: 448512680
I0521 00:53:08.643527  6829 layer_factory.hpp:77] Creating layer relu2
I0521 00:53:08.643540  6829 net.cpp:106] Creating Layer relu2
I0521 00:53:08.643550  6829 net.cpp:454] relu2 <- conv2
I0521 00:53:08.643563  6829 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:53:08.643895  6829 net.cpp:150] Setting up relu2
I0521 00:53:08.643909  6829 net.cpp:157] Top shape: 490 20 54 46 (24343200)
I0521 00:53:08.643920  6829 net.cpp:165] Memory required for data: 545885480
I0521 00:53:08.643930  6829 layer_factory.hpp:77] Creating layer pool2
I0521 00:53:08.643944  6829 net.cpp:106] Creating Layer pool2
I0521 00:53:08.643954  6829 net.cpp:454] pool2 <- conv2
I0521 00:53:08.643966  6829 net.cpp:411] pool2 -> pool2
I0521 00:53:08.644038  6829 net.cpp:150] Setting up pool2
I0521 00:53:08.644052  6829 net.cpp:157] Top shape: 490 20 27 46 (12171600)
I0521 00:53:08.644062  6829 net.cpp:165] Memory required for data: 594571880
I0521 00:53:08.644069  6829 layer_factory.hpp:77] Creating layer conv3
I0521 00:53:08.644089  6829 net.cpp:106] Creating Layer conv3
I0521 00:53:08.644099  6829 net.cpp:454] conv3 <- pool2
I0521 00:53:08.644114  6829 net.cpp:411] conv3 -> conv3
I0521 00:53:08.646083  6829 net.cpp:150] Setting up conv3
I0521 00:53:08.646106  6829 net.cpp:157] Top shape: 490 28 22 44 (13280960)
I0521 00:53:08.646118  6829 net.cpp:165] Memory required for data: 647695720
I0521 00:53:08.646150  6829 layer_factory.hpp:77] Creating layer relu3
I0521 00:53:08.646164  6829 net.cpp:106] Creating Layer relu3
I0521 00:53:08.646174  6829 net.cpp:454] relu3 <- conv3
I0521 00:53:08.646188  6829 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:53:08.646659  6829 net.cpp:150] Setting up relu3
I0521 00:53:08.646677  6829 net.cpp:157] Top shape: 490 28 22 44 (13280960)
I0521 00:53:08.646687  6829 net.cpp:165] Memory required for data: 700819560
I0521 00:53:08.646697  6829 layer_factory.hpp:77] Creating layer pool3
I0521 00:53:08.646709  6829 net.cpp:106] Creating Layer pool3
I0521 00:53:08.646719  6829 net.cpp:454] pool3 <- conv3
I0521 00:53:08.646733  6829 net.cpp:411] pool3 -> pool3
I0521 00:53:08.646805  6829 net.cpp:150] Setting up pool3
I0521 00:53:08.646817  6829 net.cpp:157] Top shape: 490 28 11 44 (6640480)
I0521 00:53:08.646827  6829 net.cpp:165] Memory required for data: 727381480
I0521 00:53:08.646834  6829 layer_factory.hpp:77] Creating layer conv4
I0521 00:53:08.646853  6829 net.cpp:106] Creating Layer conv4
I0521 00:53:08.646863  6829 net.cpp:454] conv4 <- pool3
I0521 00:53:08.646878  6829 net.cpp:411] conv4 -> conv4
I0521 00:53:08.648941  6829 net.cpp:150] Setting up conv4
I0521 00:53:08.648963  6829 net.cpp:157] Top shape: 490 36 6 42 (4445280)
I0521 00:53:08.648973  6829 net.cpp:165] Memory required for data: 745162600
I0521 00:53:08.648990  6829 layer_factory.hpp:77] Creating layer relu4
I0521 00:53:08.649005  6829 net.cpp:106] Creating Layer relu4
I0521 00:53:08.649015  6829 net.cpp:454] relu4 <- conv4
I0521 00:53:08.649029  6829 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:53:08.649499  6829 net.cpp:150] Setting up relu4
I0521 00:53:08.649515  6829 net.cpp:157] Top shape: 490 36 6 42 (4445280)
I0521 00:53:08.649525  6829 net.cpp:165] Memory required for data: 762943720
I0521 00:53:08.649535  6829 layer_factory.hpp:77] Creating layer pool4
I0521 00:53:08.649549  6829 net.cpp:106] Creating Layer pool4
I0521 00:53:08.649559  6829 net.cpp:454] pool4 <- conv4
I0521 00:53:08.649572  6829 net.cpp:411] pool4 -> pool4
I0521 00:53:08.649643  6829 net.cpp:150] Setting up pool4
I0521 00:53:08.649657  6829 net.cpp:157] Top shape: 490 36 3 42 (2222640)
I0521 00:53:08.649667  6829 net.cpp:165] Memory required for data: 771834280
I0521 00:53:08.649674  6829 layer_factory.hpp:77] Creating layer ip1
I0521 00:53:08.649690  6829 net.cpp:106] Creating Layer ip1
I0521 00:53:08.649701  6829 net.cpp:454] ip1 <- pool4
I0521 00:53:08.649716  6829 net.cpp:411] ip1 -> ip1
I0521 00:53:08.665197  6829 net.cpp:150] Setting up ip1
I0521 00:53:08.665220  6829 net.cpp:157] Top shape: 490 196 (96040)
I0521 00:53:08.665231  6829 net.cpp:165] Memory required for data: 772218440
I0521 00:53:08.665253  6829 layer_factory.hpp:77] Creating layer relu5
I0521 00:53:08.665269  6829 net.cpp:106] Creating Layer relu5
I0521 00:53:08.665279  6829 net.cpp:454] relu5 <- ip1
I0521 00:53:08.665297  6829 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:53:08.665642  6829 net.cpp:150] Setting up relu5
I0521 00:53:08.665657  6829 net.cpp:157] Top shape: 490 196 (96040)
I0521 00:53:08.665665  6829 net.cpp:165] Memory required for data: 772602600
I0521 00:53:08.665676  6829 layer_factory.hpp:77] Creating layer drop1
I0521 00:53:08.665694  6829 net.cpp:106] Creating Layer drop1
I0521 00:53:08.665705  6829 net.cpp:454] drop1 <- ip1
I0521 00:53:08.665719  6829 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:53:08.665762  6829 net.cpp:150] Setting up drop1
I0521 00:53:08.665776  6829 net.cpp:157] Top shape: 490 196 (96040)
I0521 00:53:08.665786  6829 net.cpp:165] Memory required for data: 772986760
I0521 00:53:08.665796  6829 layer_factory.hpp:77] Creating layer ip2
I0521 00:53:08.665810  6829 net.cpp:106] Creating Layer ip2
I0521 00:53:08.665820  6829 net.cpp:454] ip2 <- ip1
I0521 00:53:08.665834  6829 net.cpp:411] ip2 -> ip2
I0521 00:53:08.666314  6829 net.cpp:150] Setting up ip2
I0521 00:53:08.666327  6829 net.cpp:157] Top shape: 490 98 (48020)
I0521 00:53:08.666338  6829 net.cpp:165] Memory required for data: 773178840
I0521 00:53:08.666366  6829 layer_factory.hpp:77] Creating layer relu6
I0521 00:53:08.666379  6829 net.cpp:106] Creating Layer relu6
I0521 00:53:08.666389  6829 net.cpp:454] relu6 <- ip2
I0521 00:53:08.666402  6829 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:53:08.666937  6829 net.cpp:150] Setting up relu6
I0521 00:53:08.666960  6829 net.cpp:157] Top shape: 490 98 (48020)
I0521 00:53:08.666968  6829 net.cpp:165] Memory required for data: 773370920
I0521 00:53:08.666978  6829 layer_factory.hpp:77] Creating layer drop2
I0521 00:53:08.666992  6829 net.cpp:106] Creating Layer drop2
I0521 00:53:08.667002  6829 net.cpp:454] drop2 <- ip2
I0521 00:53:08.667016  6829 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:53:08.667060  6829 net.cpp:150] Setting up drop2
I0521 00:53:08.667073  6829 net.cpp:157] Top shape: 490 98 (48020)
I0521 00:53:08.667083  6829 net.cpp:165] Memory required for data: 773563000
I0521 00:53:08.667093  6829 layer_factory.hpp:77] Creating layer ip3
I0521 00:53:08.667107  6829 net.cpp:106] Creating Layer ip3
I0521 00:53:08.667117  6829 net.cpp:454] ip3 <- ip2
I0521 00:53:08.667131  6829 net.cpp:411] ip3 -> ip3
I0521 00:53:08.667361  6829 net.cpp:150] Setting up ip3
I0521 00:53:08.667374  6829 net.cpp:157] Top shape: 490 11 (5390)
I0521 00:53:08.667384  6829 net.cpp:165] Memory required for data: 773584560
I0521 00:53:08.667400  6829 layer_factory.hpp:77] Creating layer drop3
I0521 00:53:08.667414  6829 net.cpp:106] Creating Layer drop3
I0521 00:53:08.667424  6829 net.cpp:454] drop3 <- ip3
I0521 00:53:08.667438  6829 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:53:08.667479  6829 net.cpp:150] Setting up drop3
I0521 00:53:08.667492  6829 net.cpp:157] Top shape: 490 11 (5390)
I0521 00:53:08.667502  6829 net.cpp:165] Memory required for data: 773606120
I0521 00:53:08.667511  6829 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 00:53:08.667524  6829 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 00:53:08.667534  6829 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 00:53:08.667547  6829 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 00:53:08.667562  6829 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 00:53:08.667635  6829 net.cpp:150] Setting up ip3_drop3_0_split
I0521 00:53:08.667649  6829 net.cpp:157] Top shape: 490 11 (5390)
I0521 00:53:08.667661  6829 net.cpp:157] Top shape: 490 11 (5390)
I0521 00:53:08.667671  6829 net.cpp:165] Memory required for data: 773649240
I0521 00:53:08.667680  6829 layer_factory.hpp:77] Creating layer accuracy
I0521 00:53:08.667702  6829 net.cpp:106] Creating Layer accuracy
I0521 00:53:08.667712  6829 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 00:53:08.667724  6829 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 00:53:08.667738  6829 net.cpp:411] accuracy -> accuracy
I0521 00:53:08.667762  6829 net.cpp:150] Setting up accuracy
I0521 00:53:08.667774  6829 net.cpp:157] Top shape: (1)
I0521 00:53:08.667784  6829 net.cpp:165] Memory required for data: 773649244
I0521 00:53:08.667794  6829 layer_factory.hpp:77] Creating layer loss
I0521 00:53:08.667809  6829 net.cpp:106] Creating Layer loss
I0521 00:53:08.667819  6829 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 00:53:08.667829  6829 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 00:53:08.667842  6829 net.cpp:411] loss -> loss
I0521 00:53:08.667860  6829 layer_factory.hpp:77] Creating layer loss
I0521 00:53:08.668349  6829 net.cpp:150] Setting up loss
I0521 00:53:08.668362  6829 net.cpp:157] Top shape: (1)
I0521 00:53:08.668372  6829 net.cpp:160]     with loss weight 1
I0521 00:53:08.668390  6829 net.cpp:165] Memory required for data: 773649248
I0521 00:53:08.668401  6829 net.cpp:226] loss needs backward computation.
I0521 00:53:08.668412  6829 net.cpp:228] accuracy does not need backward computation.
I0521 00:53:08.668424  6829 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 00:53:08.668434  6829 net.cpp:226] drop3 needs backward computation.
I0521 00:53:08.668445  6829 net.cpp:226] ip3 needs backward computation.
I0521 00:53:08.668457  6829 net.cpp:226] drop2 needs backward computation.
I0521 00:53:08.668474  6829 net.cpp:226] relu6 needs backward computation.
I0521 00:53:08.668485  6829 net.cpp:226] ip2 needs backward computation.
I0521 00:53:08.668496  6829 net.cpp:226] drop1 needs backward computation.
I0521 00:53:08.668505  6829 net.cpp:226] relu5 needs backward computation.
I0521 00:53:08.668515  6829 net.cpp:226] ip1 needs backward computation.
I0521 00:53:08.668525  6829 net.cpp:226] pool4 needs backward computation.
I0521 00:53:08.668535  6829 net.cpp:226] relu4 needs backward computation.
I0521 00:53:08.668545  6829 net.cpp:226] conv4 needs backward computation.
I0521 00:53:08.668556  6829 net.cpp:226] pool3 needs backward computation.
I0521 00:53:08.668567  6829 net.cpp:226] relu3 needs backward computation.
I0521 00:53:08.668578  6829 net.cpp:226] conv3 needs backward computation.
I0521 00:53:08.668588  6829 net.cpp:226] pool2 needs backward computation.
I0521 00:53:08.668601  6829 net.cpp:226] relu2 needs backward computation.
I0521 00:53:08.668611  6829 net.cpp:226] conv2 needs backward computation.
I0521 00:53:08.668622  6829 net.cpp:226] pool1 needs backward computation.
I0521 00:53:08.668632  6829 net.cpp:226] relu1 needs backward computation.
I0521 00:53:08.668642  6829 net.cpp:226] conv1 needs backward computation.
I0521 00:53:08.668653  6829 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 00:53:08.668665  6829 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:53:08.668675  6829 net.cpp:270] This network produces output accuracy
I0521 00:53:08.668686  6829 net.cpp:270] This network produces output loss
I0521 00:53:08.668715  6829 net.cpp:283] Network initialization done.
I0521 00:53:08.668855  6829 solver.cpp:60] Solver scaffolding done.
I0521 00:53:08.669992  6829 caffe.cpp:212] Starting Optimization
I0521 00:53:08.670011  6829 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 00:53:08.670024  6829 solver.cpp:289] Learning Rate Policy: fixed
I0521 00:53:08.671243  6829 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 00:53:54.763164  6829 solver.cpp:409]     Test net output #0: accuracy = 0.0891091
I0521 00:53:54.763329  6829 solver.cpp:409]     Test net output #1: loss = 2.39797 (* 1 = 2.39797 loss)
I0521 00:53:54.860642  6829 solver.cpp:237] Iteration 0, loss = 2.3988
I0521 00:53:54.860678  6829 solver.cpp:253]     Train net output #0: loss = 2.3988 (* 1 = 2.3988 loss)
I0521 00:53:54.860697  6829 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 00:54:02.836748  6829 solver.cpp:237] Iteration 30, loss = 2.37727
I0521 00:54:02.836788  6829 solver.cpp:253]     Train net output #0: loss = 2.37727 (* 1 = 2.37727 loss)
I0521 00:54:02.836808  6829 sgd_solver.cpp:106] Iteration 30, lr = 0.0025
I0521 00:54:10.813504  6829 solver.cpp:237] Iteration 60, loss = 2.36068
I0521 00:54:10.813536  6829 solver.cpp:253]     Train net output #0: loss = 2.36068 (* 1 = 2.36068 loss)
I0521 00:54:10.813554  6829 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 00:54:18.784602  6829 solver.cpp:237] Iteration 90, loss = 2.33847
I0521 00:54:18.784636  6829 solver.cpp:253]     Train net output #0: loss = 2.33847 (* 1 = 2.33847 loss)
I0521 00:54:18.784651  6829 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 00:54:26.755043  6829 solver.cpp:237] Iteration 120, loss = 2.31944
I0521 00:54:26.755198  6829 solver.cpp:253]     Train net output #0: loss = 2.31944 (* 1 = 2.31944 loss)
I0521 00:54:26.755211  6829 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 00:54:34.724992  6829 solver.cpp:237] Iteration 150, loss = 2.2963
I0521 00:54:34.725023  6829 solver.cpp:253]     Train net output #0: loss = 2.2963 (* 1 = 2.2963 loss)
I0521 00:54:34.725041  6829 sgd_solver.cpp:106] Iteration 150, lr = 0.0025
I0521 00:54:42.694890  6829 solver.cpp:237] Iteration 180, loss = 2.30526
I0521 00:54:42.694922  6829 solver.cpp:253]     Train net output #0: loss = 2.30526 (* 1 = 2.30526 loss)
I0521 00:54:42.694936  6829 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 00:55:12.768033  6829 solver.cpp:237] Iteration 210, loss = 2.28841
I0521 00:55:12.768195  6829 solver.cpp:253]     Train net output #0: loss = 2.28841 (* 1 = 2.28841 loss)
I0521 00:55:12.768211  6829 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0521 00:55:20.742252  6829 solver.cpp:237] Iteration 240, loss = 2.30241
I0521 00:55:20.742290  6829 solver.cpp:253]     Train net output #0: loss = 2.30241 (* 1 = 2.30241 loss)
I0521 00:55:20.742307  6829 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 00:55:28.719209  6829 solver.cpp:237] Iteration 270, loss = 2.25129
I0521 00:55:28.719243  6829 solver.cpp:253]     Train net output #0: loss = 2.25129 (* 1 = 2.25129 loss)
I0521 00:55:28.719259  6829 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 00:55:36.695621  6829 solver.cpp:237] Iteration 300, loss = 2.19467
I0521 00:55:36.695653  6829 solver.cpp:253]     Train net output #0: loss = 2.19467 (* 1 = 2.19467 loss)
I0521 00:55:36.695669  6829 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 00:55:38.025308  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_306.caffemodel
I0521 00:55:38.253552  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_306.solverstate
I0521 00:55:44.733355  6829 solver.cpp:237] Iteration 330, loss = 2.14722
I0521 00:55:44.733517  6829 solver.cpp:253]     Train net output #0: loss = 2.14722 (* 1 = 2.14722 loss)
I0521 00:55:44.733532  6829 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 00:55:52.708694  6829 solver.cpp:237] Iteration 360, loss = 2.14073
I0521 00:55:52.708726  6829 solver.cpp:253]     Train net output #0: loss = 2.14073 (* 1 = 2.14073 loss)
I0521 00:55:52.708745  6829 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 00:56:00.686789  6829 solver.cpp:237] Iteration 390, loss = 2.07318
I0521 00:56:00.686821  6829 solver.cpp:253]     Train net output #0: loss = 2.07318 (* 1 = 2.07318 loss)
I0521 00:56:00.686838  6829 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0521 00:56:30.817410  6829 solver.cpp:237] Iteration 420, loss = 2.05725
I0521 00:56:30.817565  6829 solver.cpp:253]     Train net output #0: loss = 2.05725 (* 1 = 2.05725 loss)
I0521 00:56:30.817579  6829 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 00:56:38.789752  6829 solver.cpp:237] Iteration 450, loss = 2.03297
I0521 00:56:38.789786  6829 solver.cpp:253]     Train net output #0: loss = 2.03297 (* 1 = 2.03297 loss)
I0521 00:56:38.789811  6829 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 00:56:46.763247  6829 solver.cpp:237] Iteration 480, loss = 1.96456
I0521 00:56:46.763279  6829 solver.cpp:253]     Train net output #0: loss = 1.96456 (* 1 = 1.96456 loss)
I0521 00:56:46.763301  6829 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 00:56:54.741047  6829 solver.cpp:237] Iteration 510, loss = 2.03013
I0521 00:56:54.741081  6829 solver.cpp:253]     Train net output #0: loss = 2.03013 (* 1 = 2.03013 loss)
I0521 00:56:54.741097  6829 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 00:57:02.711302  6829 solver.cpp:237] Iteration 540, loss = 1.93267
I0521 00:57:02.711462  6829 solver.cpp:253]     Train net output #0: loss = 1.93267 (* 1 = 1.93267 loss)
I0521 00:57:02.711475  6829 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 00:57:10.682510  6829 solver.cpp:237] Iteration 570, loss = 1.95931
I0521 00:57:10.682543  6829 solver.cpp:253]     Train net output #0: loss = 1.95931 (* 1 = 1.95931 loss)
I0521 00:57:10.682557  6829 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 00:57:18.656332  6829 solver.cpp:237] Iteration 600, loss = 1.91022
I0521 00:57:18.656365  6829 solver.cpp:253]     Train net output #0: loss = 1.91022 (* 1 = 1.91022 loss)
I0521 00:57:18.656383  6829 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 00:57:21.578724  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_612.caffemodel
I0521 00:57:21.805168  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_612.solverstate
I0521 00:57:21.830431  6829 solver.cpp:341] Iteration 612, Testing net (#0)
I0521 00:58:06.915961  6829 solver.cpp:409]     Test net output #0: accuracy = 0.566767
I0521 00:58:06.916128  6829 solver.cpp:409]     Test net output #1: loss = 1.56579 (* 1 = 1.56579 loss)
I0521 00:58:33.874193  6829 solver.cpp:237] Iteration 630, loss = 1.99821
I0521 00:58:33.874243  6829 solver.cpp:253]     Train net output #0: loss = 1.99821 (* 1 = 1.99821 loss)
I0521 00:58:33.874258  6829 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 00:58:41.836104  6829 solver.cpp:237] Iteration 660, loss = 1.87724
I0521 00:58:41.836259  6829 solver.cpp:253]     Train net output #0: loss = 1.87724 (* 1 = 1.87724 loss)
I0521 00:58:41.836274  6829 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 00:58:49.796922  6829 solver.cpp:237] Iteration 690, loss = 1.9214
I0521 00:58:49.796955  6829 solver.cpp:253]     Train net output #0: loss = 1.9214 (* 1 = 1.9214 loss)
I0521 00:58:49.796972  6829 sgd_solver.cpp:106] Iteration 690, lr = 0.0025
I0521 00:58:57.763180  6829 solver.cpp:237] Iteration 720, loss = 1.95481
I0521 00:58:57.763212  6829 solver.cpp:253]     Train net output #0: loss = 1.95481 (* 1 = 1.95481 loss)
I0521 00:58:57.763229  6829 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 00:59:05.726367  6829 solver.cpp:237] Iteration 750, loss = 1.8307
I0521 00:59:05.726410  6829 solver.cpp:253]     Train net output #0: loss = 1.8307 (* 1 = 1.8307 loss)
I0521 00:59:05.726425  6829 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0521 00:59:13.692297  6829 solver.cpp:237] Iteration 780, loss = 1.84253
I0521 00:59:13.692430  6829 solver.cpp:253]     Train net output #0: loss = 1.84253 (* 1 = 1.84253 loss)
I0521 00:59:13.692445  6829 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 00:59:21.654239  6829 solver.cpp:237] Iteration 810, loss = 1.83746
I0521 00:59:21.654271  6829 solver.cpp:253]     Train net output #0: loss = 1.83746 (* 1 = 1.83746 loss)
I0521 00:59:21.654286  6829 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 00:59:51.773447  6829 solver.cpp:237] Iteration 840, loss = 1.87162
I0521 00:59:51.773617  6829 solver.cpp:253]     Train net output #0: loss = 1.87162 (* 1 = 1.87162 loss)
I0521 00:59:51.773632  6829 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 00:59:59.737489  6829 solver.cpp:237] Iteration 870, loss = 1.82429
I0521 00:59:59.737521  6829 solver.cpp:253]     Train net output #0: loss = 1.82429 (* 1 = 1.82429 loss)
I0521 00:59:59.737540  6829 sgd_solver.cpp:106] Iteration 870, lr = 0.0025
I0521 01:00:07.703366  6829 solver.cpp:237] Iteration 900, loss = 1.76788
I0521 01:00:07.703398  6829 solver.cpp:253]     Train net output #0: loss = 1.76788 (* 1 = 1.76788 loss)
I0521 01:00:07.703415  6829 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 01:00:12.218348  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_918.caffemodel
I0521 01:00:12.444077  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_918.solverstate
I0521 01:00:15.738124  6829 solver.cpp:237] Iteration 930, loss = 1.82819
I0521 01:00:15.738171  6829 solver.cpp:253]     Train net output #0: loss = 1.82819 (* 1 = 1.82819 loss)
I0521 01:00:15.738188  6829 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 01:00:23.705117  6829 solver.cpp:237] Iteration 960, loss = 1.72449
I0521 01:00:23.705260  6829 solver.cpp:253]     Train net output #0: loss = 1.72449 (* 1 = 1.72449 loss)
I0521 01:00:23.705274  6829 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 01:00:31.671164  6829 solver.cpp:237] Iteration 990, loss = 1.81563
I0521 01:00:31.671196  6829 solver.cpp:253]     Train net output #0: loss = 1.81563 (* 1 = 1.81563 loss)
I0521 01:00:31.671213  6829 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 01:01:01.790297  6829 solver.cpp:237] Iteration 1020, loss = 1.80296
I0521 01:01:01.790460  6829 solver.cpp:253]     Train net output #0: loss = 1.80296 (* 1 = 1.80296 loss)
I0521 01:01:01.790477  6829 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 01:01:09.751677  6829 solver.cpp:237] Iteration 1050, loss = 1.83413
I0521 01:01:09.751708  6829 solver.cpp:253]     Train net output #0: loss = 1.83413 (* 1 = 1.83413 loss)
I0521 01:01:09.751726  6829 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0521 01:01:17.717746  6829 solver.cpp:237] Iteration 1080, loss = 1.73251
I0521 01:01:17.717779  6829 solver.cpp:253]     Train net output #0: loss = 1.73251 (* 1 = 1.73251 loss)
I0521 01:01:17.717797  6829 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 01:01:25.680567  6829 solver.cpp:237] Iteration 1110, loss = 1.80458
I0521 01:01:25.680600  6829 solver.cpp:253]     Train net output #0: loss = 1.80458 (* 1 = 1.80458 loss)
I0521 01:01:25.680616  6829 sgd_solver.cpp:106] Iteration 1110, lr = 0.0025
I0521 01:01:33.647320  6829 solver.cpp:237] Iteration 1140, loss = 1.75773
I0521 01:01:33.647455  6829 solver.cpp:253]     Train net output #0: loss = 1.75773 (* 1 = 1.75773 loss)
I0521 01:01:33.647469  6829 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 01:01:41.609462  6829 solver.cpp:237] Iteration 1170, loss = 1.81255
I0521 01:01:41.609495  6829 solver.cpp:253]     Train net output #0: loss = 1.81255 (* 1 = 1.81255 loss)
I0521 01:01:41.609509  6829 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 01:01:49.572334  6829 solver.cpp:237] Iteration 1200, loss = 1.79662
I0521 01:01:49.572365  6829 solver.cpp:253]     Train net output #0: loss = 1.79662 (* 1 = 1.79662 loss)
I0521 01:01:49.572381  6829 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 01:01:55.676278  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_1224.caffemodel
I0521 01:01:55.902716  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_1224.solverstate
I0521 01:01:55.930766  6829 solver.cpp:341] Iteration 1224, Testing net (#0)
I0521 01:03:01.891229  6829 solver.cpp:409]     Test net output #0: accuracy = 0.643378
I0521 01:03:01.891397  6829 solver.cpp:409]     Test net output #1: loss = 1.27225 (* 1 = 1.27225 loss)
I0521 01:03:25.659746  6829 solver.cpp:237] Iteration 1230, loss = 1.69228
I0521 01:03:25.659796  6829 solver.cpp:253]     Train net output #0: loss = 1.69228 (* 1 = 1.69228 loss)
I0521 01:03:25.659812  6829 sgd_solver.cpp:106] Iteration 1230, lr = 0.0025
I0521 01:03:33.623652  6829 solver.cpp:237] Iteration 1260, loss = 1.72685
I0521 01:03:33.623790  6829 solver.cpp:253]     Train net output #0: loss = 1.72685 (* 1 = 1.72685 loss)
I0521 01:03:33.623802  6829 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 01:03:41.585049  6829 solver.cpp:237] Iteration 1290, loss = 1.74152
I0521 01:03:41.585086  6829 solver.cpp:253]     Train net output #0: loss = 1.74152 (* 1 = 1.74152 loss)
I0521 01:03:41.585108  6829 sgd_solver.cpp:106] Iteration 1290, lr = 0.0025
I0521 01:03:49.547062  6829 solver.cpp:237] Iteration 1320, loss = 1.73911
I0521 01:03:49.547096  6829 solver.cpp:253]     Train net output #0: loss = 1.73911 (* 1 = 1.73911 loss)
I0521 01:03:49.547109  6829 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 01:03:57.507345  6829 solver.cpp:237] Iteration 1350, loss = 1.83725
I0521 01:03:57.507377  6829 solver.cpp:253]     Train net output #0: loss = 1.83725 (* 1 = 1.83725 loss)
I0521 01:03:57.507393  6829 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 01:04:05.471568  6829 solver.cpp:237] Iteration 1380, loss = 1.78601
I0521 01:04:05.471712  6829 solver.cpp:253]     Train net output #0: loss = 1.78601 (* 1 = 1.78601 loss)
I0521 01:04:05.471726  6829 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 01:04:13.435071  6829 solver.cpp:237] Iteration 1410, loss = 1.78656
I0521 01:04:13.435103  6829 solver.cpp:253]     Train net output #0: loss = 1.78656 (* 1 = 1.78656 loss)
I0521 01:04:13.435120  6829 sgd_solver.cpp:106] Iteration 1410, lr = 0.0025
I0521 01:04:43.511147  6829 solver.cpp:237] Iteration 1440, loss = 1.75126
I0521 01:04:43.511322  6829 solver.cpp:253]     Train net output #0: loss = 1.75126 (* 1 = 1.75126 loss)
I0521 01:04:43.511337  6829 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 01:04:51.475602  6829 solver.cpp:237] Iteration 1470, loss = 1.71293
I0521 01:04:51.475641  6829 solver.cpp:253]     Train net output #0: loss = 1.71293 (* 1 = 1.71293 loss)
I0521 01:04:51.475663  6829 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0521 01:04:59.435078  6829 solver.cpp:237] Iteration 1500, loss = 1.68671
I0521 01:04:59.435111  6829 solver.cpp:253]     Train net output #0: loss = 1.68671 (* 1 = 1.68671 loss)
I0521 01:04:59.435127  6829 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 01:05:07.130743  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_1530.caffemodel
I0521 01:05:07.356709  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_1530.solverstate
I0521 01:05:07.463821  6829 solver.cpp:237] Iteration 1530, loss = 1.70066
I0521 01:05:07.463871  6829 solver.cpp:253]     Train net output #0: loss = 1.70066 (* 1 = 1.70066 loss)
I0521 01:05:07.463886  6829 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 01:05:15.421975  6829 solver.cpp:237] Iteration 1560, loss = 1.67081
I0521 01:05:15.422122  6829 solver.cpp:253]     Train net output #0: loss = 1.67081 (* 1 = 1.67081 loss)
I0521 01:05:15.422135  6829 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 01:05:23.388411  6829 solver.cpp:237] Iteration 1590, loss = 1.69833
I0521 01:05:23.388447  6829 solver.cpp:253]     Train net output #0: loss = 1.69833 (* 1 = 1.69833 loss)
I0521 01:05:23.388468  6829 sgd_solver.cpp:106] Iteration 1590, lr = 0.0025
I0521 01:05:31.347332  6829 solver.cpp:237] Iteration 1620, loss = 1.73254
I0521 01:05:31.347365  6829 solver.cpp:253]     Train net output #0: loss = 1.73254 (* 1 = 1.73254 loss)
I0521 01:05:31.347381  6829 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 01:06:01.412891  6829 solver.cpp:237] Iteration 1650, loss = 1.72985
I0521 01:06:01.413054  6829 solver.cpp:253]     Train net output #0: loss = 1.72985 (* 1 = 1.72985 loss)
I0521 01:06:01.413069  6829 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 01:06:09.378631  6829 solver.cpp:237] Iteration 1680, loss = 1.68865
I0521 01:06:09.378675  6829 solver.cpp:253]     Train net output #0: loss = 1.68865 (* 1 = 1.68865 loss)
I0521 01:06:09.378692  6829 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 01:06:17.338269  6829 solver.cpp:237] Iteration 1710, loss = 1.70413
I0521 01:06:17.338301  6829 solver.cpp:253]     Train net output #0: loss = 1.70413 (* 1 = 1.70413 loss)
I0521 01:06:17.338320  6829 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 01:06:25.297317  6829 solver.cpp:237] Iteration 1740, loss = 1.62293
I0521 01:06:25.297350  6829 solver.cpp:253]     Train net output #0: loss = 1.62293 (* 1 = 1.62293 loss)
I0521 01:06:25.297368  6829 sgd_solver.cpp:106] Iteration 1740, lr = 0.0025
I0521 01:06:33.260357  6829 solver.cpp:237] Iteration 1770, loss = 1.7258
I0521 01:06:33.260509  6829 solver.cpp:253]     Train net output #0: loss = 1.7258 (* 1 = 1.7258 loss)
I0521 01:06:33.260521  6829 sgd_solver.cpp:106] Iteration 1770, lr = 0.0025
I0521 01:06:41.224617  6829 solver.cpp:237] Iteration 1800, loss = 1.72263
I0521 01:06:41.224649  6829 solver.cpp:253]     Train net output #0: loss = 1.72263 (* 1 = 1.72263 loss)
I0521 01:06:41.224668  6829 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 01:06:49.185657  6829 solver.cpp:237] Iteration 1830, loss = 1.70544
I0521 01:06:49.185690  6829 solver.cpp:253]     Train net output #0: loss = 1.70544 (* 1 = 1.70544 loss)
I0521 01:06:49.185708  6829 sgd_solver.cpp:106] Iteration 1830, lr = 0.0025
I0521 01:06:50.511982  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_1836.caffemodel
I0521 01:06:50.737172  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_1836.solverstate
I0521 01:06:50.763520  6829 solver.cpp:341] Iteration 1836, Testing net (#0)
I0521 01:07:35.566565  6829 solver.cpp:409]     Test net output #0: accuracy = 0.663952
I0521 01:07:35.566725  6829 solver.cpp:409]     Test net output #1: loss = 1.14997 (* 1 = 1.14997 loss)
I0521 01:08:04.141603  6829 solver.cpp:237] Iteration 1860, loss = 1.7076
I0521 01:08:04.141654  6829 solver.cpp:253]     Train net output #0: loss = 1.7076 (* 1 = 1.7076 loss)
I0521 01:08:04.141669  6829 sgd_solver.cpp:106] Iteration 1860, lr = 0.0025
I0521 01:08:12.110229  6829 solver.cpp:237] Iteration 1890, loss = 1.75553
I0521 01:08:12.110391  6829 solver.cpp:253]     Train net output #0: loss = 1.75553 (* 1 = 1.75553 loss)
I0521 01:08:12.110405  6829 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0521 01:08:20.072374  6829 solver.cpp:237] Iteration 1920, loss = 1.61632
I0521 01:08:20.072407  6829 solver.cpp:253]     Train net output #0: loss = 1.61632 (* 1 = 1.61632 loss)
I0521 01:08:20.072424  6829 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0521 01:08:28.044008  6829 solver.cpp:237] Iteration 1950, loss = 1.68803
I0521 01:08:28.044040  6829 solver.cpp:253]     Train net output #0: loss = 1.68803 (* 1 = 1.68803 loss)
I0521 01:08:28.044056  6829 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0521 01:08:36.007550  6829 solver.cpp:237] Iteration 1980, loss = 1.64194
I0521 01:08:36.007583  6829 solver.cpp:253]     Train net output #0: loss = 1.64194 (* 1 = 1.64194 loss)
I0521 01:08:36.007601  6829 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 01:08:43.976114  6829 solver.cpp:237] Iteration 2010, loss = 1.69871
I0521 01:08:43.976271  6829 solver.cpp:253]     Train net output #0: loss = 1.69871 (* 1 = 1.69871 loss)
I0521 01:08:43.976286  6829 sgd_solver.cpp:106] Iteration 2010, lr = 0.0025
I0521 01:09:14.034551  6829 solver.cpp:237] Iteration 2040, loss = 1.71037
I0521 01:09:14.034719  6829 solver.cpp:253]     Train net output #0: loss = 1.71037 (* 1 = 1.71037 loss)
I0521 01:09:14.034735  6829 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0521 01:09:21.995995  6829 solver.cpp:237] Iteration 2070, loss = 1.70648
I0521 01:09:21.996028  6829 solver.cpp:253]     Train net output #0: loss = 1.70648 (* 1 = 1.70648 loss)
I0521 01:09:21.996045  6829 sgd_solver.cpp:106] Iteration 2070, lr = 0.0025
I0521 01:09:29.959599  6829 solver.cpp:237] Iteration 2100, loss = 1.58616
I0521 01:09:29.959637  6829 solver.cpp:253]     Train net output #0: loss = 1.58616 (* 1 = 1.58616 loss)
I0521 01:09:29.959659  6829 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0521 01:09:37.924134  6829 solver.cpp:237] Iteration 2130, loss = 1.73249
I0521 01:09:37.924168  6829 solver.cpp:253]     Train net output #0: loss = 1.73249 (* 1 = 1.73249 loss)
I0521 01:09:37.924185  6829 sgd_solver.cpp:106] Iteration 2130, lr = 0.0025
I0521 01:09:40.845587  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_2142.caffemodel
I0521 01:09:41.070390  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_2142.solverstate
I0521 01:09:45.954535  6829 solver.cpp:237] Iteration 2160, loss = 1.6281
I0521 01:09:45.954692  6829 solver.cpp:253]     Train net output #0: loss = 1.6281 (* 1 = 1.6281 loss)
I0521 01:09:45.954706  6829 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0521 01:09:53.916960  6829 solver.cpp:237] Iteration 2190, loss = 1.71062
I0521 01:09:53.917006  6829 solver.cpp:253]     Train net output #0: loss = 1.71062 (* 1 = 1.71062 loss)
I0521 01:09:53.917022  6829 sgd_solver.cpp:106] Iteration 2190, lr = 0.0025
I0521 01:10:01.885468  6829 solver.cpp:237] Iteration 2220, loss = 1.61804
I0521 01:10:01.885500  6829 solver.cpp:253]     Train net output #0: loss = 1.61804 (* 1 = 1.61804 loss)
I0521 01:10:01.885517  6829 sgd_solver.cpp:106] Iteration 2220, lr = 0.0025
I0521 01:10:31.972015  6829 solver.cpp:237] Iteration 2250, loss = 1.62611
I0521 01:10:31.972179  6829 solver.cpp:253]     Train net output #0: loss = 1.62611 (* 1 = 1.62611 loss)
I0521 01:10:31.972194  6829 sgd_solver.cpp:106] Iteration 2250, lr = 0.0025
I0521 01:10:39.939142  6829 solver.cpp:237] Iteration 2280, loss = 1.5861
I0521 01:10:39.939175  6829 solver.cpp:253]     Train net output #0: loss = 1.5861 (* 1 = 1.5861 loss)
I0521 01:10:39.939193  6829 sgd_solver.cpp:106] Iteration 2280, lr = 0.0025
I0521 01:10:47.903515  6829 solver.cpp:237] Iteration 2310, loss = 1.68836
I0521 01:10:47.903549  6829 solver.cpp:253]     Train net output #0: loss = 1.68836 (* 1 = 1.68836 loss)
I0521 01:10:47.903571  6829 sgd_solver.cpp:106] Iteration 2310, lr = 0.0025
I0521 01:10:55.867640  6829 solver.cpp:237] Iteration 2340, loss = 1.63652
I0521 01:10:55.867673  6829 solver.cpp:253]     Train net output #0: loss = 1.63652 (* 1 = 1.63652 loss)
I0521 01:10:55.867687  6829 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0521 01:11:03.833986  6829 solver.cpp:237] Iteration 2370, loss = 1.65826
I0521 01:11:03.834125  6829 solver.cpp:253]     Train net output #0: loss = 1.65826 (* 1 = 1.65826 loss)
I0521 01:11:03.834138  6829 sgd_solver.cpp:106] Iteration 2370, lr = 0.0025
I0521 01:11:11.795928  6829 solver.cpp:237] Iteration 2400, loss = 1.67636
I0521 01:11:11.795969  6829 solver.cpp:253]     Train net output #0: loss = 1.67636 (* 1 = 1.67636 loss)
I0521 01:11:11.795984  6829 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0521 01:11:19.763557  6829 solver.cpp:237] Iteration 2430, loss = 1.60532
I0521 01:11:19.763589  6829 solver.cpp:253]     Train net output #0: loss = 1.60532 (* 1 = 1.60532 loss)
I0521 01:11:19.763607  6829 sgd_solver.cpp:106] Iteration 2430, lr = 0.0025
I0521 01:11:24.278399  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_2448.caffemodel
I0521 01:11:24.502531  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_2448.solverstate
I0521 01:11:24.528885  6829 solver.cpp:341] Iteration 2448, Testing net (#0)
I0521 01:12:30.424311  6829 solver.cpp:409]     Test net output #0: accuracy = 0.687608
I0521 01:12:30.424495  6829 solver.cpp:409]     Test net output #1: loss = 1.10162 (* 1 = 1.10162 loss)
I0521 01:12:55.772091  6829 solver.cpp:237] Iteration 2460, loss = 1.53577
I0521 01:12:55.772140  6829 solver.cpp:253]     Train net output #0: loss = 1.53577 (* 1 = 1.53577 loss)
I0521 01:12:55.772156  6829 sgd_solver.cpp:106] Iteration 2460, lr = 0.0025
I0521 01:13:03.735985  6829 solver.cpp:237] Iteration 2490, loss = 1.6366
I0521 01:13:03.736138  6829 solver.cpp:253]     Train net output #0: loss = 1.6366 (* 1 = 1.6366 loss)
I0521 01:13:03.736155  6829 sgd_solver.cpp:106] Iteration 2490, lr = 0.0025
I0521 01:13:11.702752  6829 solver.cpp:237] Iteration 2520, loss = 1.64513
I0521 01:13:11.702795  6829 solver.cpp:253]     Train net output #0: loss = 1.64513 (* 1 = 1.64513 loss)
I0521 01:13:11.702811  6829 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0521 01:13:19.666896  6829 solver.cpp:237] Iteration 2550, loss = 1.63779
I0521 01:13:19.666929  6829 solver.cpp:253]     Train net output #0: loss = 1.63779 (* 1 = 1.63779 loss)
I0521 01:13:19.666945  6829 sgd_solver.cpp:106] Iteration 2550, lr = 0.0025
I0521 01:13:27.630376  6829 solver.cpp:237] Iteration 2580, loss = 1.5854
I0521 01:13:27.630409  6829 solver.cpp:253]     Train net output #0: loss = 1.5854 (* 1 = 1.5854 loss)
I0521 01:13:27.630425  6829 sgd_solver.cpp:106] Iteration 2580, lr = 0.0025
I0521 01:13:35.594691  6829 solver.cpp:237] Iteration 2610, loss = 1.60235
I0521 01:13:35.594831  6829 solver.cpp:253]     Train net output #0: loss = 1.60235 (* 1 = 1.60235 loss)
I0521 01:13:35.594846  6829 sgd_solver.cpp:106] Iteration 2610, lr = 0.0025
I0521 01:13:43.563230  6829 solver.cpp:237] Iteration 2640, loss = 1.58279
I0521 01:13:43.563273  6829 solver.cpp:253]     Train net output #0: loss = 1.58279 (* 1 = 1.58279 loss)
I0521 01:13:43.563289  6829 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0521 01:14:13.612305  6829 solver.cpp:237] Iteration 2670, loss = 1.55883
I0521 01:14:13.612470  6829 solver.cpp:253]     Train net output #0: loss = 1.55883 (* 1 = 1.55883 loss)
I0521 01:14:13.612486  6829 sgd_solver.cpp:106] Iteration 2670, lr = 0.0025
I0521 01:14:21.574127  6829 solver.cpp:237] Iteration 2700, loss = 1.62409
I0521 01:14:21.574162  6829 solver.cpp:253]     Train net output #0: loss = 1.62409 (* 1 = 1.62409 loss)
I0521 01:14:21.574177  6829 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0521 01:14:29.537356  6829 solver.cpp:237] Iteration 2730, loss = 1.55135
I0521 01:14:29.537401  6829 solver.cpp:253]     Train net output #0: loss = 1.55135 (* 1 = 1.55135 loss)
I0521 01:14:29.537415  6829 sgd_solver.cpp:106] Iteration 2730, lr = 0.0025
I0521 01:14:35.642207  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_2754.caffemodel
I0521 01:14:35.867432  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_2754.solverstate
I0521 01:14:37.567217  6829 solver.cpp:237] Iteration 2760, loss = 1.65741
I0521 01:14:37.567263  6829 solver.cpp:253]     Train net output #0: loss = 1.65741 (* 1 = 1.65741 loss)
I0521 01:14:37.567281  6829 sgd_solver.cpp:106] Iteration 2760, lr = 0.0025
I0521 01:14:45.531772  6829 solver.cpp:237] Iteration 2790, loss = 1.64777
I0521 01:14:45.531929  6829 solver.cpp:253]     Train net output #0: loss = 1.64777 (* 1 = 1.64777 loss)
I0521 01:14:45.531942  6829 sgd_solver.cpp:106] Iteration 2790, lr = 0.0025
I0521 01:14:53.495098  6829 solver.cpp:237] Iteration 2820, loss = 1.63558
I0521 01:14:53.495148  6829 solver.cpp:253]     Train net output #0: loss = 1.63558 (* 1 = 1.63558 loss)
I0521 01:14:53.495163  6829 sgd_solver.cpp:106] Iteration 2820, lr = 0.0025
I0521 01:15:01.456264  6829 solver.cpp:237] Iteration 2850, loss = 1.67227
I0521 01:15:01.456297  6829 solver.cpp:253]     Train net output #0: loss = 1.67227 (* 1 = 1.67227 loss)
I0521 01:15:01.456315  6829 sgd_solver.cpp:106] Iteration 2850, lr = 0.0025
I0521 01:15:31.511114  6829 solver.cpp:237] Iteration 2880, loss = 1.73356
I0521 01:15:31.511286  6829 solver.cpp:253]     Train net output #0: loss = 1.73356 (* 1 = 1.73356 loss)
I0521 01:15:31.511307  6829 sgd_solver.cpp:106] Iteration 2880, lr = 0.0025
I0521 01:15:39.473832  6829 solver.cpp:237] Iteration 2910, loss = 1.58155
I0521 01:15:39.473865  6829 solver.cpp:253]     Train net output #0: loss = 1.58155 (* 1 = 1.58155 loss)
I0521 01:15:39.473883  6829 sgd_solver.cpp:106] Iteration 2910, lr = 0.0025
I0521 01:15:47.440225  6829 solver.cpp:237] Iteration 2940, loss = 1.64423
I0521 01:15:47.440260  6829 solver.cpp:253]     Train net output #0: loss = 1.64423 (* 1 = 1.64423 loss)
I0521 01:15:47.440282  6829 sgd_solver.cpp:106] Iteration 2940, lr = 0.0025
I0521 01:15:55.403950  6829 solver.cpp:237] Iteration 2970, loss = 1.6884
I0521 01:15:55.403982  6829 solver.cpp:253]     Train net output #0: loss = 1.6884 (* 1 = 1.6884 loss)
I0521 01:15:55.404000  6829 sgd_solver.cpp:106] Iteration 2970, lr = 0.0025
I0521 01:16:03.366798  6829 solver.cpp:237] Iteration 3000, loss = 1.65285
I0521 01:16:03.366948  6829 solver.cpp:253]     Train net output #0: loss = 1.65285 (* 1 = 1.65285 loss)
I0521 01:16:03.366962  6829 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0521 01:16:11.333616  6829 solver.cpp:237] Iteration 3030, loss = 1.71289
I0521 01:16:11.333652  6829 solver.cpp:253]     Train net output #0: loss = 1.71289 (* 1 = 1.71289 loss)
I0521 01:16:11.333676  6829 sgd_solver.cpp:106] Iteration 3030, lr = 0.0025
I0521 01:16:19.031810  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_3060.caffemodel
I0521 01:16:19.257864  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_3060.solverstate
I0521 01:16:19.285832  6829 solver.cpp:341] Iteration 3060, Testing net (#0)
I0521 01:17:04.403059  6829 solver.cpp:409]     Test net output #0: accuracy = 0.707296
I0521 01:17:04.403230  6829 solver.cpp:409]     Test net output #1: loss = 1.02391 (* 1 = 1.02391 loss)
I0521 01:17:04.482069  6829 solver.cpp:237] Iteration 3060, loss = 1.58128
I0521 01:17:04.482096  6829 solver.cpp:253]     Train net output #0: loss = 1.58128 (* 1 = 1.58128 loss)
I0521 01:17:04.482115  6829 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0521 01:17:04.482502  6829 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_3061.caffemodel
I0521 01:17:04.708896  6829 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011_iter_3061.solverstate
I0521 01:17:04.736795  6829 solver.cpp:326] Optimization Done.
I0521 01:17:04.736824  6829 caffe.cpp:215] Optimization Done.
Application 11236327 resources: utime ~1256s, stime ~226s, Rss ~5329264, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_490_2016-05-20T11.20.50.381011.solver"
	User time (seconds): 0.56
	System time (seconds): 0.20
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:47.84
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2911
	Involuntary context switches: 224
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
