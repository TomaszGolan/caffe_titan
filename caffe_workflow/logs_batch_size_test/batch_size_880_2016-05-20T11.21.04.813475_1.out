2806383
I0521 09:05:16.300977 31067 caffe.cpp:184] Using GPUs 0
I0521 09:05:16.728687 31067 solver.cpp:48] Initializing solver from parameters: 
test_iter: 170
test_interval: 340
base_lr: 0.0025
display: 17
max_iter: 1704
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 170
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475.prototxt"
I0521 09:05:16.730223 31067 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475.prototxt
I0521 09:05:16.742957 31067 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 09:05:16.743017 31067 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 09:05:16.743362 31067 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 880
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:05:16.743540 31067 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:05:16.743563 31067 net.cpp:106] Creating Layer data_hdf5
I0521 09:05:16.743577 31067 net.cpp:411] data_hdf5 -> data
I0521 09:05:16.743612 31067 net.cpp:411] data_hdf5 -> label
I0521 09:05:16.743644 31067 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 09:05:16.756027 31067 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 09:05:16.758232 31067 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 09:05:38.309048 31067 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 09:05:38.314204 31067 net.cpp:150] Setting up data_hdf5
I0521 09:05:38.314249 31067 net.cpp:157] Top shape: 880 1 127 50 (5588000)
I0521 09:05:38.314265 31067 net.cpp:157] Top shape: 880 (880)
I0521 09:05:38.314275 31067 net.cpp:165] Memory required for data: 22355520
I0521 09:05:38.314287 31067 layer_factory.hpp:77] Creating layer conv1
I0521 09:05:38.314321 31067 net.cpp:106] Creating Layer conv1
I0521 09:05:38.314333 31067 net.cpp:454] conv1 <- data
I0521 09:05:38.314355 31067 net.cpp:411] conv1 -> conv1
I0521 09:05:38.682238 31067 net.cpp:150] Setting up conv1
I0521 09:05:38.682287 31067 net.cpp:157] Top shape: 880 12 120 48 (60825600)
I0521 09:05:38.682298 31067 net.cpp:165] Memory required for data: 265657920
I0521 09:05:38.682327 31067 layer_factory.hpp:77] Creating layer relu1
I0521 09:05:38.682348 31067 net.cpp:106] Creating Layer relu1
I0521 09:05:38.682358 31067 net.cpp:454] relu1 <- conv1
I0521 09:05:38.682373 31067 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:05:38.682891 31067 net.cpp:150] Setting up relu1
I0521 09:05:38.682907 31067 net.cpp:157] Top shape: 880 12 120 48 (60825600)
I0521 09:05:38.682919 31067 net.cpp:165] Memory required for data: 508960320
I0521 09:05:38.682929 31067 layer_factory.hpp:77] Creating layer pool1
I0521 09:05:38.682945 31067 net.cpp:106] Creating Layer pool1
I0521 09:05:38.682956 31067 net.cpp:454] pool1 <- conv1
I0521 09:05:38.682970 31067 net.cpp:411] pool1 -> pool1
I0521 09:05:38.683049 31067 net.cpp:150] Setting up pool1
I0521 09:05:38.683063 31067 net.cpp:157] Top shape: 880 12 60 48 (30412800)
I0521 09:05:38.683073 31067 net.cpp:165] Memory required for data: 630611520
I0521 09:05:38.683084 31067 layer_factory.hpp:77] Creating layer conv2
I0521 09:05:38.683105 31067 net.cpp:106] Creating Layer conv2
I0521 09:05:38.683116 31067 net.cpp:454] conv2 <- pool1
I0521 09:05:38.683128 31067 net.cpp:411] conv2 -> conv2
I0521 09:05:38.685817 31067 net.cpp:150] Setting up conv2
I0521 09:05:38.685845 31067 net.cpp:157] Top shape: 880 20 54 46 (43718400)
I0521 09:05:38.685855 31067 net.cpp:165] Memory required for data: 805485120
I0521 09:05:38.685874 31067 layer_factory.hpp:77] Creating layer relu2
I0521 09:05:38.685889 31067 net.cpp:106] Creating Layer relu2
I0521 09:05:38.685899 31067 net.cpp:454] relu2 <- conv2
I0521 09:05:38.685911 31067 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:05:38.686241 31067 net.cpp:150] Setting up relu2
I0521 09:05:38.686256 31067 net.cpp:157] Top shape: 880 20 54 46 (43718400)
I0521 09:05:38.686266 31067 net.cpp:165] Memory required for data: 980358720
I0521 09:05:38.686276 31067 layer_factory.hpp:77] Creating layer pool2
I0521 09:05:38.686290 31067 net.cpp:106] Creating Layer pool2
I0521 09:05:38.686300 31067 net.cpp:454] pool2 <- conv2
I0521 09:05:38.686324 31067 net.cpp:411] pool2 -> pool2
I0521 09:05:38.686393 31067 net.cpp:150] Setting up pool2
I0521 09:05:38.686406 31067 net.cpp:157] Top shape: 880 20 27 46 (21859200)
I0521 09:05:38.686416 31067 net.cpp:165] Memory required for data: 1067795520
I0521 09:05:38.686426 31067 layer_factory.hpp:77] Creating layer conv3
I0521 09:05:38.686444 31067 net.cpp:106] Creating Layer conv3
I0521 09:05:38.686455 31067 net.cpp:454] conv3 <- pool2
I0521 09:05:38.686468 31067 net.cpp:411] conv3 -> conv3
I0521 09:05:38.688385 31067 net.cpp:150] Setting up conv3
I0521 09:05:38.688410 31067 net.cpp:157] Top shape: 880 28 22 44 (23851520)
I0521 09:05:38.688421 31067 net.cpp:165] Memory required for data: 1163201600
I0521 09:05:38.688439 31067 layer_factory.hpp:77] Creating layer relu3
I0521 09:05:38.688455 31067 net.cpp:106] Creating Layer relu3
I0521 09:05:38.688465 31067 net.cpp:454] relu3 <- conv3
I0521 09:05:38.688478 31067 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:05:38.688957 31067 net.cpp:150] Setting up relu3
I0521 09:05:38.688974 31067 net.cpp:157] Top shape: 880 28 22 44 (23851520)
I0521 09:05:38.688984 31067 net.cpp:165] Memory required for data: 1258607680
I0521 09:05:38.688994 31067 layer_factory.hpp:77] Creating layer pool3
I0521 09:05:38.689008 31067 net.cpp:106] Creating Layer pool3
I0521 09:05:38.689018 31067 net.cpp:454] pool3 <- conv3
I0521 09:05:38.689030 31067 net.cpp:411] pool3 -> pool3
I0521 09:05:38.689097 31067 net.cpp:150] Setting up pool3
I0521 09:05:38.689111 31067 net.cpp:157] Top shape: 880 28 11 44 (11925760)
I0521 09:05:38.689121 31067 net.cpp:165] Memory required for data: 1306310720
I0521 09:05:38.689131 31067 layer_factory.hpp:77] Creating layer conv4
I0521 09:05:38.689148 31067 net.cpp:106] Creating Layer conv4
I0521 09:05:38.689159 31067 net.cpp:454] conv4 <- pool3
I0521 09:05:38.689172 31067 net.cpp:411] conv4 -> conv4
I0521 09:05:38.691887 31067 net.cpp:150] Setting up conv4
I0521 09:05:38.691915 31067 net.cpp:157] Top shape: 880 36 6 42 (7983360)
I0521 09:05:38.691926 31067 net.cpp:165] Memory required for data: 1338244160
I0521 09:05:38.691941 31067 layer_factory.hpp:77] Creating layer relu4
I0521 09:05:38.691956 31067 net.cpp:106] Creating Layer relu4
I0521 09:05:38.691965 31067 net.cpp:454] relu4 <- conv4
I0521 09:05:38.691978 31067 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:05:38.692447 31067 net.cpp:150] Setting up relu4
I0521 09:05:38.692463 31067 net.cpp:157] Top shape: 880 36 6 42 (7983360)
I0521 09:05:38.692474 31067 net.cpp:165] Memory required for data: 1370177600
I0521 09:05:38.692484 31067 layer_factory.hpp:77] Creating layer pool4
I0521 09:05:38.692497 31067 net.cpp:106] Creating Layer pool4
I0521 09:05:38.692507 31067 net.cpp:454] pool4 <- conv4
I0521 09:05:38.692520 31067 net.cpp:411] pool4 -> pool4
I0521 09:05:38.692587 31067 net.cpp:150] Setting up pool4
I0521 09:05:38.692601 31067 net.cpp:157] Top shape: 880 36 3 42 (3991680)
I0521 09:05:38.692611 31067 net.cpp:165] Memory required for data: 1386144320
I0521 09:05:38.692621 31067 layer_factory.hpp:77] Creating layer ip1
I0521 09:05:38.692643 31067 net.cpp:106] Creating Layer ip1
I0521 09:05:38.692653 31067 net.cpp:454] ip1 <- pool4
I0521 09:05:38.692667 31067 net.cpp:411] ip1 -> ip1
I0521 09:05:38.708151 31067 net.cpp:150] Setting up ip1
I0521 09:05:38.708181 31067 net.cpp:157] Top shape: 880 196 (172480)
I0521 09:05:38.708195 31067 net.cpp:165] Memory required for data: 1386834240
I0521 09:05:38.708216 31067 layer_factory.hpp:77] Creating layer relu5
I0521 09:05:38.708231 31067 net.cpp:106] Creating Layer relu5
I0521 09:05:38.708241 31067 net.cpp:454] relu5 <- ip1
I0521 09:05:38.708256 31067 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:05:38.708598 31067 net.cpp:150] Setting up relu5
I0521 09:05:38.708613 31067 net.cpp:157] Top shape: 880 196 (172480)
I0521 09:05:38.708622 31067 net.cpp:165] Memory required for data: 1387524160
I0521 09:05:38.708633 31067 layer_factory.hpp:77] Creating layer drop1
I0521 09:05:38.708657 31067 net.cpp:106] Creating Layer drop1
I0521 09:05:38.708667 31067 net.cpp:454] drop1 <- ip1
I0521 09:05:38.708693 31067 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:05:38.708739 31067 net.cpp:150] Setting up drop1
I0521 09:05:38.708752 31067 net.cpp:157] Top shape: 880 196 (172480)
I0521 09:05:38.708762 31067 net.cpp:165] Memory required for data: 1388214080
I0521 09:05:38.708772 31067 layer_factory.hpp:77] Creating layer ip2
I0521 09:05:38.708791 31067 net.cpp:106] Creating Layer ip2
I0521 09:05:38.708801 31067 net.cpp:454] ip2 <- ip1
I0521 09:05:38.708814 31067 net.cpp:411] ip2 -> ip2
I0521 09:05:38.709287 31067 net.cpp:150] Setting up ip2
I0521 09:05:38.709300 31067 net.cpp:157] Top shape: 880 98 (86240)
I0521 09:05:38.709311 31067 net.cpp:165] Memory required for data: 1388559040
I0521 09:05:38.709326 31067 layer_factory.hpp:77] Creating layer relu6
I0521 09:05:38.709338 31067 net.cpp:106] Creating Layer relu6
I0521 09:05:38.709348 31067 net.cpp:454] relu6 <- ip2
I0521 09:05:38.709360 31067 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:05:38.709880 31067 net.cpp:150] Setting up relu6
I0521 09:05:38.709897 31067 net.cpp:157] Top shape: 880 98 (86240)
I0521 09:05:38.709906 31067 net.cpp:165] Memory required for data: 1388904000
I0521 09:05:38.709918 31067 layer_factory.hpp:77] Creating layer drop2
I0521 09:05:38.709930 31067 net.cpp:106] Creating Layer drop2
I0521 09:05:38.709940 31067 net.cpp:454] drop2 <- ip2
I0521 09:05:38.709952 31067 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:05:38.709995 31067 net.cpp:150] Setting up drop2
I0521 09:05:38.710007 31067 net.cpp:157] Top shape: 880 98 (86240)
I0521 09:05:38.710018 31067 net.cpp:165] Memory required for data: 1389248960
I0521 09:05:38.710028 31067 layer_factory.hpp:77] Creating layer ip3
I0521 09:05:38.710042 31067 net.cpp:106] Creating Layer ip3
I0521 09:05:38.710052 31067 net.cpp:454] ip3 <- ip2
I0521 09:05:38.710064 31067 net.cpp:411] ip3 -> ip3
I0521 09:05:38.710275 31067 net.cpp:150] Setting up ip3
I0521 09:05:38.710289 31067 net.cpp:157] Top shape: 880 11 (9680)
I0521 09:05:38.710299 31067 net.cpp:165] Memory required for data: 1389287680
I0521 09:05:38.710314 31067 layer_factory.hpp:77] Creating layer drop3
I0521 09:05:38.710325 31067 net.cpp:106] Creating Layer drop3
I0521 09:05:38.710335 31067 net.cpp:454] drop3 <- ip3
I0521 09:05:38.710347 31067 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:05:38.710386 31067 net.cpp:150] Setting up drop3
I0521 09:05:38.710398 31067 net.cpp:157] Top shape: 880 11 (9680)
I0521 09:05:38.710408 31067 net.cpp:165] Memory required for data: 1389326400
I0521 09:05:38.710419 31067 layer_factory.hpp:77] Creating layer loss
I0521 09:05:38.710438 31067 net.cpp:106] Creating Layer loss
I0521 09:05:38.710448 31067 net.cpp:454] loss <- ip3
I0521 09:05:38.710458 31067 net.cpp:454] loss <- label
I0521 09:05:38.710470 31067 net.cpp:411] loss -> loss
I0521 09:05:38.710487 31067 layer_factory.hpp:77] Creating layer loss
I0521 09:05:38.711139 31067 net.cpp:150] Setting up loss
I0521 09:05:38.711155 31067 net.cpp:157] Top shape: (1)
I0521 09:05:38.711169 31067 net.cpp:160]     with loss weight 1
I0521 09:05:38.711212 31067 net.cpp:165] Memory required for data: 1389326404
I0521 09:05:38.711225 31067 net.cpp:226] loss needs backward computation.
I0521 09:05:38.711236 31067 net.cpp:226] drop3 needs backward computation.
I0521 09:05:38.711243 31067 net.cpp:226] ip3 needs backward computation.
I0521 09:05:38.711254 31067 net.cpp:226] drop2 needs backward computation.
I0521 09:05:38.711264 31067 net.cpp:226] relu6 needs backward computation.
I0521 09:05:38.711274 31067 net.cpp:226] ip2 needs backward computation.
I0521 09:05:38.711284 31067 net.cpp:226] drop1 needs backward computation.
I0521 09:05:38.711293 31067 net.cpp:226] relu5 needs backward computation.
I0521 09:05:38.711303 31067 net.cpp:226] ip1 needs backward computation.
I0521 09:05:38.711313 31067 net.cpp:226] pool4 needs backward computation.
I0521 09:05:38.711324 31067 net.cpp:226] relu4 needs backward computation.
I0521 09:05:38.711333 31067 net.cpp:226] conv4 needs backward computation.
I0521 09:05:38.711344 31067 net.cpp:226] pool3 needs backward computation.
I0521 09:05:38.711364 31067 net.cpp:226] relu3 needs backward computation.
I0521 09:05:38.711375 31067 net.cpp:226] conv3 needs backward computation.
I0521 09:05:38.711386 31067 net.cpp:226] pool2 needs backward computation.
I0521 09:05:38.711397 31067 net.cpp:226] relu2 needs backward computation.
I0521 09:05:38.711407 31067 net.cpp:226] conv2 needs backward computation.
I0521 09:05:38.711417 31067 net.cpp:226] pool1 needs backward computation.
I0521 09:05:38.711428 31067 net.cpp:226] relu1 needs backward computation.
I0521 09:05:38.711437 31067 net.cpp:226] conv1 needs backward computation.
I0521 09:05:38.711448 31067 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:05:38.711458 31067 net.cpp:270] This network produces output loss
I0521 09:05:38.711482 31067 net.cpp:283] Network initialization done.
I0521 09:05:38.713100 31067 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475.prototxt
I0521 09:05:38.713172 31067 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 09:05:38.713528 31067 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 880
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:05:38.713719 31067 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:05:38.713734 31067 net.cpp:106] Creating Layer data_hdf5
I0521 09:05:38.713747 31067 net.cpp:411] data_hdf5 -> data
I0521 09:05:38.713763 31067 net.cpp:411] data_hdf5 -> label
I0521 09:05:38.713779 31067 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 09:05:38.715095 31067 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 09:06:00.050715 31067 net.cpp:150] Setting up data_hdf5
I0521 09:06:00.050884 31067 net.cpp:157] Top shape: 880 1 127 50 (5588000)
I0521 09:06:00.050899 31067 net.cpp:157] Top shape: 880 (880)
I0521 09:06:00.050911 31067 net.cpp:165] Memory required for data: 22355520
I0521 09:06:00.050925 31067 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 09:06:00.050953 31067 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 09:06:00.050964 31067 net.cpp:454] label_data_hdf5_1_split <- label
I0521 09:06:00.050979 31067 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 09:06:00.051000 31067 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 09:06:00.051074 31067 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 09:06:00.051089 31067 net.cpp:157] Top shape: 880 (880)
I0521 09:06:00.051100 31067 net.cpp:157] Top shape: 880 (880)
I0521 09:06:00.051110 31067 net.cpp:165] Memory required for data: 22362560
I0521 09:06:00.051120 31067 layer_factory.hpp:77] Creating layer conv1
I0521 09:06:00.051141 31067 net.cpp:106] Creating Layer conv1
I0521 09:06:00.051152 31067 net.cpp:454] conv1 <- data
I0521 09:06:00.051167 31067 net.cpp:411] conv1 -> conv1
I0521 09:06:00.053097 31067 net.cpp:150] Setting up conv1
I0521 09:06:00.053122 31067 net.cpp:157] Top shape: 880 12 120 48 (60825600)
I0521 09:06:00.053133 31067 net.cpp:165] Memory required for data: 265664960
I0521 09:06:00.053154 31067 layer_factory.hpp:77] Creating layer relu1
I0521 09:06:00.053169 31067 net.cpp:106] Creating Layer relu1
I0521 09:06:00.053179 31067 net.cpp:454] relu1 <- conv1
I0521 09:06:00.053192 31067 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:06:00.053694 31067 net.cpp:150] Setting up relu1
I0521 09:06:00.053711 31067 net.cpp:157] Top shape: 880 12 120 48 (60825600)
I0521 09:06:00.053721 31067 net.cpp:165] Memory required for data: 508967360
I0521 09:06:00.053756 31067 layer_factory.hpp:77] Creating layer pool1
I0521 09:06:00.053774 31067 net.cpp:106] Creating Layer pool1
I0521 09:06:00.053784 31067 net.cpp:454] pool1 <- conv1
I0521 09:06:00.053797 31067 net.cpp:411] pool1 -> pool1
I0521 09:06:00.053874 31067 net.cpp:150] Setting up pool1
I0521 09:06:00.053887 31067 net.cpp:157] Top shape: 880 12 60 48 (30412800)
I0521 09:06:00.053897 31067 net.cpp:165] Memory required for data: 630618560
I0521 09:06:00.053907 31067 layer_factory.hpp:77] Creating layer conv2
I0521 09:06:00.053926 31067 net.cpp:106] Creating Layer conv2
I0521 09:06:00.053936 31067 net.cpp:454] conv2 <- pool1
I0521 09:06:00.053951 31067 net.cpp:411] conv2 -> conv2
I0521 09:06:00.055893 31067 net.cpp:150] Setting up conv2
I0521 09:06:00.055915 31067 net.cpp:157] Top shape: 880 20 54 46 (43718400)
I0521 09:06:00.055929 31067 net.cpp:165] Memory required for data: 805492160
I0521 09:06:00.055948 31067 layer_factory.hpp:77] Creating layer relu2
I0521 09:06:00.055961 31067 net.cpp:106] Creating Layer relu2
I0521 09:06:00.055971 31067 net.cpp:454] relu2 <- conv2
I0521 09:06:00.055984 31067 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:06:00.056319 31067 net.cpp:150] Setting up relu2
I0521 09:06:00.056334 31067 net.cpp:157] Top shape: 880 20 54 46 (43718400)
I0521 09:06:00.056344 31067 net.cpp:165] Memory required for data: 980365760
I0521 09:06:00.056354 31067 layer_factory.hpp:77] Creating layer pool2
I0521 09:06:00.056366 31067 net.cpp:106] Creating Layer pool2
I0521 09:06:00.056377 31067 net.cpp:454] pool2 <- conv2
I0521 09:06:00.056390 31067 net.cpp:411] pool2 -> pool2
I0521 09:06:00.056460 31067 net.cpp:150] Setting up pool2
I0521 09:06:00.056473 31067 net.cpp:157] Top shape: 880 20 27 46 (21859200)
I0521 09:06:00.056483 31067 net.cpp:165] Memory required for data: 1067802560
I0521 09:06:00.056494 31067 layer_factory.hpp:77] Creating layer conv3
I0521 09:06:00.056512 31067 net.cpp:106] Creating Layer conv3
I0521 09:06:00.056524 31067 net.cpp:454] conv3 <- pool2
I0521 09:06:00.056538 31067 net.cpp:411] conv3 -> conv3
I0521 09:06:00.058519 31067 net.cpp:150] Setting up conv3
I0521 09:06:00.058543 31067 net.cpp:157] Top shape: 880 28 22 44 (23851520)
I0521 09:06:00.058553 31067 net.cpp:165] Memory required for data: 1163208640
I0521 09:06:00.058586 31067 layer_factory.hpp:77] Creating layer relu3
I0521 09:06:00.058599 31067 net.cpp:106] Creating Layer relu3
I0521 09:06:00.058610 31067 net.cpp:454] relu3 <- conv3
I0521 09:06:00.058624 31067 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:06:00.059098 31067 net.cpp:150] Setting up relu3
I0521 09:06:00.059114 31067 net.cpp:157] Top shape: 880 28 22 44 (23851520)
I0521 09:06:00.059124 31067 net.cpp:165] Memory required for data: 1258614720
I0521 09:06:00.059135 31067 layer_factory.hpp:77] Creating layer pool3
I0521 09:06:00.059149 31067 net.cpp:106] Creating Layer pool3
I0521 09:06:00.059159 31067 net.cpp:454] pool3 <- conv3
I0521 09:06:00.059171 31067 net.cpp:411] pool3 -> pool3
I0521 09:06:00.059243 31067 net.cpp:150] Setting up pool3
I0521 09:06:00.059257 31067 net.cpp:157] Top shape: 880 28 11 44 (11925760)
I0521 09:06:00.059267 31067 net.cpp:165] Memory required for data: 1306317760
I0521 09:06:00.059274 31067 layer_factory.hpp:77] Creating layer conv4
I0521 09:06:00.059293 31067 net.cpp:106] Creating Layer conv4
I0521 09:06:00.059303 31067 net.cpp:454] conv4 <- pool3
I0521 09:06:00.059317 31067 net.cpp:411] conv4 -> conv4
I0521 09:06:00.061377 31067 net.cpp:150] Setting up conv4
I0521 09:06:00.061399 31067 net.cpp:157] Top shape: 880 36 6 42 (7983360)
I0521 09:06:00.061413 31067 net.cpp:165] Memory required for data: 1338251200
I0521 09:06:00.061429 31067 layer_factory.hpp:77] Creating layer relu4
I0521 09:06:00.061441 31067 net.cpp:106] Creating Layer relu4
I0521 09:06:00.061451 31067 net.cpp:454] relu4 <- conv4
I0521 09:06:00.061465 31067 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:06:00.061940 31067 net.cpp:150] Setting up relu4
I0521 09:06:00.061957 31067 net.cpp:157] Top shape: 880 36 6 42 (7983360)
I0521 09:06:00.061969 31067 net.cpp:165] Memory required for data: 1370184640
I0521 09:06:00.061978 31067 layer_factory.hpp:77] Creating layer pool4
I0521 09:06:00.061991 31067 net.cpp:106] Creating Layer pool4
I0521 09:06:00.062001 31067 net.cpp:454] pool4 <- conv4
I0521 09:06:00.062016 31067 net.cpp:411] pool4 -> pool4
I0521 09:06:00.062086 31067 net.cpp:150] Setting up pool4
I0521 09:06:00.062099 31067 net.cpp:157] Top shape: 880 36 3 42 (3991680)
I0521 09:06:00.062109 31067 net.cpp:165] Memory required for data: 1386151360
I0521 09:06:00.062119 31067 layer_factory.hpp:77] Creating layer ip1
I0521 09:06:00.062135 31067 net.cpp:106] Creating Layer ip1
I0521 09:06:00.062146 31067 net.cpp:454] ip1 <- pool4
I0521 09:06:00.062160 31067 net.cpp:411] ip1 -> ip1
I0521 09:06:00.077635 31067 net.cpp:150] Setting up ip1
I0521 09:06:00.077663 31067 net.cpp:157] Top shape: 880 196 (172480)
I0521 09:06:00.077675 31067 net.cpp:165] Memory required for data: 1386841280
I0521 09:06:00.077698 31067 layer_factory.hpp:77] Creating layer relu5
I0521 09:06:00.077713 31067 net.cpp:106] Creating Layer relu5
I0521 09:06:00.077723 31067 net.cpp:454] relu5 <- ip1
I0521 09:06:00.077738 31067 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:06:00.078081 31067 net.cpp:150] Setting up relu5
I0521 09:06:00.078095 31067 net.cpp:157] Top shape: 880 196 (172480)
I0521 09:06:00.078105 31067 net.cpp:165] Memory required for data: 1387531200
I0521 09:06:00.078116 31067 layer_factory.hpp:77] Creating layer drop1
I0521 09:06:00.078135 31067 net.cpp:106] Creating Layer drop1
I0521 09:06:00.078145 31067 net.cpp:454] drop1 <- ip1
I0521 09:06:00.078158 31067 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:06:00.078202 31067 net.cpp:150] Setting up drop1
I0521 09:06:00.078215 31067 net.cpp:157] Top shape: 880 196 (172480)
I0521 09:06:00.078224 31067 net.cpp:165] Memory required for data: 1388221120
I0521 09:06:00.078235 31067 layer_factory.hpp:77] Creating layer ip2
I0521 09:06:00.078250 31067 net.cpp:106] Creating Layer ip2
I0521 09:06:00.078259 31067 net.cpp:454] ip2 <- ip1
I0521 09:06:00.078274 31067 net.cpp:411] ip2 -> ip2
I0521 09:06:00.078754 31067 net.cpp:150] Setting up ip2
I0521 09:06:00.078768 31067 net.cpp:157] Top shape: 880 98 (86240)
I0521 09:06:00.078778 31067 net.cpp:165] Memory required for data: 1388566080
I0521 09:06:00.078806 31067 layer_factory.hpp:77] Creating layer relu6
I0521 09:06:00.078819 31067 net.cpp:106] Creating Layer relu6
I0521 09:06:00.078830 31067 net.cpp:454] relu6 <- ip2
I0521 09:06:00.078842 31067 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:06:00.079372 31067 net.cpp:150] Setting up relu6
I0521 09:06:00.079394 31067 net.cpp:157] Top shape: 880 98 (86240)
I0521 09:06:00.079404 31067 net.cpp:165] Memory required for data: 1388911040
I0521 09:06:00.079416 31067 layer_factory.hpp:77] Creating layer drop2
I0521 09:06:00.079428 31067 net.cpp:106] Creating Layer drop2
I0521 09:06:00.079438 31067 net.cpp:454] drop2 <- ip2
I0521 09:06:00.079452 31067 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:06:00.079494 31067 net.cpp:150] Setting up drop2
I0521 09:06:00.079507 31067 net.cpp:157] Top shape: 880 98 (86240)
I0521 09:06:00.079519 31067 net.cpp:165] Memory required for data: 1389256000
I0521 09:06:00.079529 31067 layer_factory.hpp:77] Creating layer ip3
I0521 09:06:00.079542 31067 net.cpp:106] Creating Layer ip3
I0521 09:06:00.079552 31067 net.cpp:454] ip3 <- ip2
I0521 09:06:00.079566 31067 net.cpp:411] ip3 -> ip3
I0521 09:06:00.079787 31067 net.cpp:150] Setting up ip3
I0521 09:06:00.079800 31067 net.cpp:157] Top shape: 880 11 (9680)
I0521 09:06:00.079810 31067 net.cpp:165] Memory required for data: 1389294720
I0521 09:06:00.079825 31067 layer_factory.hpp:77] Creating layer drop3
I0521 09:06:00.079838 31067 net.cpp:106] Creating Layer drop3
I0521 09:06:00.079849 31067 net.cpp:454] drop3 <- ip3
I0521 09:06:00.079861 31067 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:06:00.079902 31067 net.cpp:150] Setting up drop3
I0521 09:06:00.079915 31067 net.cpp:157] Top shape: 880 11 (9680)
I0521 09:06:00.079924 31067 net.cpp:165] Memory required for data: 1389333440
I0521 09:06:00.079934 31067 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 09:06:00.079947 31067 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 09:06:00.079957 31067 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 09:06:00.079970 31067 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 09:06:00.079985 31067 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 09:06:00.080059 31067 net.cpp:150] Setting up ip3_drop3_0_split
I0521 09:06:00.080071 31067 net.cpp:157] Top shape: 880 11 (9680)
I0521 09:06:00.080083 31067 net.cpp:157] Top shape: 880 11 (9680)
I0521 09:06:00.080095 31067 net.cpp:165] Memory required for data: 1389410880
I0521 09:06:00.080104 31067 layer_factory.hpp:77] Creating layer accuracy
I0521 09:06:00.080127 31067 net.cpp:106] Creating Layer accuracy
I0521 09:06:00.080137 31067 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 09:06:00.080148 31067 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 09:06:00.080162 31067 net.cpp:411] accuracy -> accuracy
I0521 09:06:00.080184 31067 net.cpp:150] Setting up accuracy
I0521 09:06:00.080198 31067 net.cpp:157] Top shape: (1)
I0521 09:06:00.080207 31067 net.cpp:165] Memory required for data: 1389410884
I0521 09:06:00.080217 31067 layer_factory.hpp:77] Creating layer loss
I0521 09:06:00.080231 31067 net.cpp:106] Creating Layer loss
I0521 09:06:00.080241 31067 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 09:06:00.080252 31067 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 09:06:00.080265 31067 net.cpp:411] loss -> loss
I0521 09:06:00.080283 31067 layer_factory.hpp:77] Creating layer loss
I0521 09:06:00.080776 31067 net.cpp:150] Setting up loss
I0521 09:06:00.080790 31067 net.cpp:157] Top shape: (1)
I0521 09:06:00.080801 31067 net.cpp:160]     with loss weight 1
I0521 09:06:00.080821 31067 net.cpp:165] Memory required for data: 1389410888
I0521 09:06:00.080832 31067 net.cpp:226] loss needs backward computation.
I0521 09:06:00.080842 31067 net.cpp:228] accuracy does not need backward computation.
I0521 09:06:00.080853 31067 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 09:06:00.080871 31067 net.cpp:226] drop3 needs backward computation.
I0521 09:06:00.080883 31067 net.cpp:226] ip3 needs backward computation.
I0521 09:06:00.080893 31067 net.cpp:226] drop2 needs backward computation.
I0521 09:06:00.080912 31067 net.cpp:226] relu6 needs backward computation.
I0521 09:06:00.080922 31067 net.cpp:226] ip2 needs backward computation.
I0521 09:06:00.080932 31067 net.cpp:226] drop1 needs backward computation.
I0521 09:06:00.080942 31067 net.cpp:226] relu5 needs backward computation.
I0521 09:06:00.080952 31067 net.cpp:226] ip1 needs backward computation.
I0521 09:06:00.080962 31067 net.cpp:226] pool4 needs backward computation.
I0521 09:06:00.080971 31067 net.cpp:226] relu4 needs backward computation.
I0521 09:06:00.080981 31067 net.cpp:226] conv4 needs backward computation.
I0521 09:06:00.080991 31067 net.cpp:226] pool3 needs backward computation.
I0521 09:06:00.081002 31067 net.cpp:226] relu3 needs backward computation.
I0521 09:06:00.081012 31067 net.cpp:226] conv3 needs backward computation.
I0521 09:06:00.081022 31067 net.cpp:226] pool2 needs backward computation.
I0521 09:06:00.081032 31067 net.cpp:226] relu2 needs backward computation.
I0521 09:06:00.081042 31067 net.cpp:226] conv2 needs backward computation.
I0521 09:06:00.081051 31067 net.cpp:226] pool1 needs backward computation.
I0521 09:06:00.081063 31067 net.cpp:226] relu1 needs backward computation.
I0521 09:06:00.081071 31067 net.cpp:226] conv1 needs backward computation.
I0521 09:06:00.081084 31067 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 09:06:00.081094 31067 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:06:00.081105 31067 net.cpp:270] This network produces output accuracy
I0521 09:06:00.081115 31067 net.cpp:270] This network produces output loss
I0521 09:06:00.081145 31067 net.cpp:283] Network initialization done.
I0521 09:06:00.081279 31067 solver.cpp:60] Solver scaffolding done.
I0521 09:06:00.082406 31067 caffe.cpp:212] Starting Optimization
I0521 09:06:00.082425 31067 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 09:06:00.082434 31067 solver.cpp:289] Learning Rate Policy: fixed
I0521 09:06:00.083658 31067 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 09:06:45.977324 31067 solver.cpp:409]     Test net output #0: accuracy = 0.0914706
I0521 09:06:45.977488 31067 solver.cpp:409]     Test net output #1: loss = 2.39892 (* 1 = 2.39892 loss)
I0521 09:06:46.138896 31067 solver.cpp:237] Iteration 0, loss = 2.39868
I0521 09:06:46.138933 31067 solver.cpp:253]     Train net output #0: loss = 2.39868 (* 1 = 2.39868 loss)
I0521 09:06:46.138952 31067 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 09:06:54.151511 31067 solver.cpp:237] Iteration 17, loss = 2.38763
I0521 09:06:54.151547 31067 solver.cpp:253]     Train net output #0: loss = 2.38763 (* 1 = 2.38763 loss)
I0521 09:06:54.151563 31067 sgd_solver.cpp:106] Iteration 17, lr = 0.0025
I0521 09:07:02.171391 31067 solver.cpp:237] Iteration 34, loss = 2.37335
I0521 09:07:02.171437 31067 solver.cpp:253]     Train net output #0: loss = 2.37335 (* 1 = 2.37335 loss)
I0521 09:07:02.171450 31067 sgd_solver.cpp:106] Iteration 34, lr = 0.0025
I0521 09:07:10.185951 31067 solver.cpp:237] Iteration 51, loss = 2.36493
I0521 09:07:10.185984 31067 solver.cpp:253]     Train net output #0: loss = 2.36493 (* 1 = 2.36493 loss)
I0521 09:07:10.186002 31067 sgd_solver.cpp:106] Iteration 51, lr = 0.0025
I0521 09:07:18.197432 31067 solver.cpp:237] Iteration 68, loss = 2.35474
I0521 09:07:18.197576 31067 solver.cpp:253]     Train net output #0: loss = 2.35474 (* 1 = 2.35474 loss)
I0521 09:07:18.197589 31067 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0521 09:07:26.214861 31067 solver.cpp:237] Iteration 85, loss = 2.34737
I0521 09:07:26.214906 31067 solver.cpp:253]     Train net output #0: loss = 2.34737 (* 1 = 2.34737 loss)
I0521 09:07:26.214922 31067 sgd_solver.cpp:106] Iteration 85, lr = 0.0025
I0521 09:07:34.231423 31067 solver.cpp:237] Iteration 102, loss = 2.33629
I0521 09:07:34.231456 31067 solver.cpp:253]     Train net output #0: loss = 2.33629 (* 1 = 2.33629 loss)
I0521 09:07:34.231472 31067 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0521 09:08:04.346562 31067 solver.cpp:237] Iteration 119, loss = 2.33542
I0521 09:08:04.346730 31067 solver.cpp:253]     Train net output #0: loss = 2.33542 (* 1 = 2.33542 loss)
I0521 09:08:04.346745 31067 sgd_solver.cpp:106] Iteration 119, lr = 0.0025
I0521 09:08:12.364979 31067 solver.cpp:237] Iteration 136, loss = 2.33227
I0521 09:08:12.365012 31067 solver.cpp:253]     Train net output #0: loss = 2.33227 (* 1 = 2.33227 loss)
I0521 09:08:12.365030 31067 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0521 09:08:20.381389 31067 solver.cpp:237] Iteration 153, loss = 2.32935
I0521 09:08:20.381438 31067 solver.cpp:253]     Train net output #0: loss = 2.32935 (* 1 = 2.32935 loss)
I0521 09:08:20.381453 31067 sgd_solver.cpp:106] Iteration 153, lr = 0.0025
I0521 09:08:27.922044 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_170.caffemodel
I0521 09:08:28.295369 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_170.solverstate
I0521 09:08:28.461272 31067 solver.cpp:237] Iteration 170, loss = 2.32073
I0521 09:08:28.461323 31067 solver.cpp:253]     Train net output #0: loss = 2.32073 (* 1 = 2.32073 loss)
I0521 09:08:28.461338 31067 sgd_solver.cpp:106] Iteration 170, lr = 0.0025
I0521 09:08:36.481011 31067 solver.cpp:237] Iteration 187, loss = 2.33142
I0521 09:08:36.481150 31067 solver.cpp:253]     Train net output #0: loss = 2.33142 (* 1 = 2.33142 loss)
I0521 09:08:36.481163 31067 sgd_solver.cpp:106] Iteration 187, lr = 0.0025
I0521 09:08:44.491168 31067 solver.cpp:237] Iteration 204, loss = 2.32572
I0521 09:08:44.491211 31067 solver.cpp:253]     Train net output #0: loss = 2.32572 (* 1 = 2.32572 loss)
I0521 09:08:44.491231 31067 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0521 09:08:52.510625 31067 solver.cpp:237] Iteration 221, loss = 2.30961
I0521 09:08:52.510658 31067 solver.cpp:253]     Train net output #0: loss = 2.30961 (* 1 = 2.30961 loss)
I0521 09:08:52.510675 31067 sgd_solver.cpp:106] Iteration 221, lr = 0.0025
I0521 09:09:22.629145 31067 solver.cpp:237] Iteration 238, loss = 2.31017
I0521 09:09:22.629309 31067 solver.cpp:253]     Train net output #0: loss = 2.31017 (* 1 = 2.31017 loss)
I0521 09:09:22.629325 31067 sgd_solver.cpp:106] Iteration 238, lr = 0.0025
I0521 09:09:30.650511 31067 solver.cpp:237] Iteration 255, loss = 2.30053
I0521 09:09:30.650563 31067 solver.cpp:253]     Train net output #0: loss = 2.30053 (* 1 = 2.30053 loss)
I0521 09:09:30.650578 31067 sgd_solver.cpp:106] Iteration 255, lr = 0.0025
I0521 09:09:38.665561 31067 solver.cpp:237] Iteration 272, loss = 2.27713
I0521 09:09:38.665596 31067 solver.cpp:253]     Train net output #0: loss = 2.27713 (* 1 = 2.27713 loss)
I0521 09:09:38.665612 31067 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 09:09:46.680665 31067 solver.cpp:237] Iteration 289, loss = 2.30312
I0521 09:09:46.680699 31067 solver.cpp:253]     Train net output #0: loss = 2.30312 (* 1 = 2.30312 loss)
I0521 09:09:46.680716 31067 sgd_solver.cpp:106] Iteration 289, lr = 0.0025
I0521 09:09:54.698632 31067 solver.cpp:237] Iteration 306, loss = 2.27102
I0521 09:09:54.698770 31067 solver.cpp:253]     Train net output #0: loss = 2.27102 (* 1 = 2.27102 loss)
I0521 09:09:54.698784 31067 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 09:10:02.715598 31067 solver.cpp:237] Iteration 323, loss = 2.27615
I0521 09:10:02.715631 31067 solver.cpp:253]     Train net output #0: loss = 2.27615 (* 1 = 2.27615 loss)
I0521 09:10:02.715649 31067 sgd_solver.cpp:106] Iteration 323, lr = 0.0025
I0521 09:10:10.262570 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_340.caffemodel
I0521 09:10:10.632720 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_340.solverstate
I0521 09:10:10.657788 31067 solver.cpp:341] Iteration 340, Testing net (#0)
I0521 09:10:55.641235 31067 solver.cpp:409]     Test net output #0: accuracy = 0.362587
I0521 09:10:55.641397 31067 solver.cpp:409]     Test net output #1: loss = 2.20542 (* 1 = 2.20542 loss)
I0521 09:11:17.908092 31067 solver.cpp:237] Iteration 340, loss = 2.25404
I0521 09:11:17.908149 31067 solver.cpp:253]     Train net output #0: loss = 2.25404 (* 1 = 2.25404 loss)
I0521 09:11:17.908165 31067 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0521 09:11:25.922708 31067 solver.cpp:237] Iteration 357, loss = 2.21842
I0521 09:11:25.922850 31067 solver.cpp:253]     Train net output #0: loss = 2.21842 (* 1 = 2.21842 loss)
I0521 09:11:25.922863 31067 sgd_solver.cpp:106] Iteration 357, lr = 0.0025
I0521 09:11:33.941166 31067 solver.cpp:237] Iteration 374, loss = 2.19696
I0521 09:11:33.941200 31067 solver.cpp:253]     Train net output #0: loss = 2.19696 (* 1 = 2.19696 loss)
I0521 09:11:33.941217 31067 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 09:11:41.960700 31067 solver.cpp:237] Iteration 391, loss = 2.20364
I0521 09:11:41.960739 31067 solver.cpp:253]     Train net output #0: loss = 2.20364 (* 1 = 2.20364 loss)
I0521 09:11:41.960760 31067 sgd_solver.cpp:106] Iteration 391, lr = 0.0025
I0521 09:11:49.975278 31067 solver.cpp:237] Iteration 408, loss = 2.16688
I0521 09:11:49.975311 31067 solver.cpp:253]     Train net output #0: loss = 2.16688 (* 1 = 2.16688 loss)
I0521 09:11:49.975328 31067 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0521 09:11:57.990671 31067 solver.cpp:237] Iteration 425, loss = 2.08901
I0521 09:11:57.990806 31067 solver.cpp:253]     Train net output #0: loss = 2.08901 (* 1 = 2.08901 loss)
I0521 09:11:57.990819 31067 sgd_solver.cpp:106] Iteration 425, lr = 0.0025
I0521 09:12:06.003962 31067 solver.cpp:237] Iteration 442, loss = 2.09238
I0521 09:12:06.004004 31067 solver.cpp:253]     Train net output #0: loss = 2.09238 (* 1 = 2.09238 loss)
I0521 09:12:06.004025 31067 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0521 09:12:36.261476 31067 solver.cpp:237] Iteration 459, loss = 2.10225
I0521 09:12:36.261646 31067 solver.cpp:253]     Train net output #0: loss = 2.10225 (* 1 = 2.10225 loss)
I0521 09:12:36.261661 31067 sgd_solver.cpp:106] Iteration 459, lr = 0.0025
I0521 09:12:44.280088 31067 solver.cpp:237] Iteration 476, loss = 2.06081
I0521 09:12:44.280120 31067 solver.cpp:253]     Train net output #0: loss = 2.06081 (* 1 = 2.06081 loss)
I0521 09:12:44.280138 31067 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0521 09:12:52.293694 31067 solver.cpp:237] Iteration 493, loss = 2.08939
I0521 09:12:52.293740 31067 solver.cpp:253]     Train net output #0: loss = 2.08939 (* 1 = 2.08939 loss)
I0521 09:12:52.293758 31067 sgd_solver.cpp:106] Iteration 493, lr = 0.0025
I0521 09:12:59.842923 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_510.caffemodel
I0521 09:13:00.215234 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_510.solverstate
I0521 09:13:00.382621 31067 solver.cpp:237] Iteration 510, loss = 2.01125
I0521 09:13:00.382673 31067 solver.cpp:253]     Train net output #0: loss = 2.01125 (* 1 = 2.01125 loss)
I0521 09:13:00.382690 31067 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0521 09:13:08.404755 31067 solver.cpp:237] Iteration 527, loss = 2.01295
I0521 09:13:08.404916 31067 solver.cpp:253]     Train net output #0: loss = 2.01295 (* 1 = 2.01295 loss)
I0521 09:13:08.404928 31067 sgd_solver.cpp:106] Iteration 527, lr = 0.0025
I0521 09:13:16.420352 31067 solver.cpp:237] Iteration 544, loss = 2.00427
I0521 09:13:16.420399 31067 solver.cpp:253]     Train net output #0: loss = 2.00427 (* 1 = 2.00427 loss)
I0521 09:13:16.420413 31067 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 09:13:24.438248 31067 solver.cpp:237] Iteration 561, loss = 1.96194
I0521 09:13:24.438282 31067 solver.cpp:253]     Train net output #0: loss = 1.96194 (* 1 = 1.96194 loss)
I0521 09:13:24.438298 31067 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0521 09:13:54.601605 31067 solver.cpp:237] Iteration 578, loss = 1.91938
I0521 09:13:54.601770 31067 solver.cpp:253]     Train net output #0: loss = 1.91938 (* 1 = 1.91938 loss)
I0521 09:13:54.601785 31067 sgd_solver.cpp:106] Iteration 578, lr = 0.0025
I0521 09:14:02.621129 31067 solver.cpp:237] Iteration 595, loss = 2.0233
I0521 09:14:02.621162 31067 solver.cpp:253]     Train net output #0: loss = 2.0233 (* 1 = 2.0233 loss)
I0521 09:14:02.621179 31067 sgd_solver.cpp:106] Iteration 595, lr = 0.0025
I0521 09:14:10.639343 31067 solver.cpp:237] Iteration 612, loss = 1.94424
I0521 09:14:10.639389 31067 solver.cpp:253]     Train net output #0: loss = 1.94424 (* 1 = 1.94424 loss)
I0521 09:14:10.639405 31067 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 09:14:18.657959 31067 solver.cpp:237] Iteration 629, loss = 1.91986
I0521 09:14:18.657992 31067 solver.cpp:253]     Train net output #0: loss = 1.91986 (* 1 = 1.91986 loss)
I0521 09:14:18.658010 31067 sgd_solver.cpp:106] Iteration 629, lr = 0.0025
I0521 09:14:26.674288 31067 solver.cpp:237] Iteration 646, loss = 1.9283
I0521 09:14:26.674424 31067 solver.cpp:253]     Train net output #0: loss = 1.9283 (* 1 = 1.9283 loss)
I0521 09:14:26.674437 31067 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0521 09:14:34.690639 31067 solver.cpp:237] Iteration 663, loss = 1.92042
I0521 09:14:34.690678 31067 solver.cpp:253]     Train net output #0: loss = 1.92042 (* 1 = 1.92042 loss)
I0521 09:14:34.690696 31067 sgd_solver.cpp:106] Iteration 663, lr = 0.0025
I0521 09:14:42.233104 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_680.caffemodel
I0521 09:14:42.603363 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_680.solverstate
I0521 09:14:42.630955 31067 solver.cpp:341] Iteration 680, Testing net (#0)
I0521 09:15:48.553920 31067 solver.cpp:409]     Test net output #0: accuracy = 0.55748
I0521 09:15:48.554090 31067 solver.cpp:409]     Test net output #1: loss = 1.55791 (* 1 = 1.55791 loss)
I0521 09:15:48.694226 31067 solver.cpp:237] Iteration 680, loss = 1.92201
I0521 09:15:48.694254 31067 solver.cpp:253]     Train net output #0: loss = 1.92201 (* 1 = 1.92201 loss)
I0521 09:15:48.694272 31067 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0521 09:16:18.807806 31067 solver.cpp:237] Iteration 697, loss = 1.95326
I0521 09:16:18.807981 31067 solver.cpp:253]     Train net output #0: loss = 1.95326 (* 1 = 1.95326 loss)
I0521 09:16:18.807996 31067 sgd_solver.cpp:106] Iteration 697, lr = 0.0025
I0521 09:16:26.811344 31067 solver.cpp:237] Iteration 714, loss = 1.86877
I0521 09:16:26.811378 31067 solver.cpp:253]     Train net output #0: loss = 1.86877 (* 1 = 1.86877 loss)
I0521 09:16:26.811394 31067 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0521 09:16:34.818634 31067 solver.cpp:237] Iteration 731, loss = 1.90826
I0521 09:16:34.818666 31067 solver.cpp:253]     Train net output #0: loss = 1.90826 (* 1 = 1.90826 loss)
I0521 09:16:34.818680 31067 sgd_solver.cpp:106] Iteration 731, lr = 0.0025
I0521 09:16:42.826122 31067 solver.cpp:237] Iteration 748, loss = 1.88142
I0521 09:16:42.826155 31067 solver.cpp:253]     Train net output #0: loss = 1.88142 (* 1 = 1.88142 loss)
I0521 09:16:42.826171 31067 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 09:16:50.836566 31067 solver.cpp:237] Iteration 765, loss = 1.84513
I0521 09:16:50.836702 31067 solver.cpp:253]     Train net output #0: loss = 1.84513 (* 1 = 1.84513 loss)
I0521 09:16:50.836715 31067 sgd_solver.cpp:106] Iteration 765, lr = 0.0025
I0521 09:16:58.836230 31067 solver.cpp:237] Iteration 782, loss = 1.88772
I0521 09:16:58.836261 31067 solver.cpp:253]     Train net output #0: loss = 1.88772 (* 1 = 1.88772 loss)
I0521 09:16:58.836278 31067 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0521 09:17:29.000393 31067 solver.cpp:237] Iteration 799, loss = 1.83956
I0521 09:17:29.000558 31067 solver.cpp:253]     Train net output #0: loss = 1.83956 (* 1 = 1.83956 loss)
I0521 09:17:29.000574 31067 sgd_solver.cpp:106] Iteration 799, lr = 0.0025
I0521 09:17:37.004870 31067 solver.cpp:237] Iteration 816, loss = 1.81091
I0521 09:17:37.004904 31067 solver.cpp:253]     Train net output #0: loss = 1.81091 (* 1 = 1.81091 loss)
I0521 09:17:37.004919 31067 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 09:17:45.016445 31067 solver.cpp:237] Iteration 833, loss = 1.8721
I0521 09:17:45.016477 31067 solver.cpp:253]     Train net output #0: loss = 1.8721 (* 1 = 1.8721 loss)
I0521 09:17:45.016495 31067 sgd_solver.cpp:106] Iteration 833, lr = 0.0025
I0521 09:17:52.548355 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_850.caffemodel
I0521 09:17:52.919392 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_850.solverstate
I0521 09:17:53.087258 31067 solver.cpp:237] Iteration 850, loss = 1.83565
I0521 09:17:53.087311 31067 solver.cpp:253]     Train net output #0: loss = 1.83565 (* 1 = 1.83565 loss)
I0521 09:17:53.087329 31067 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0521 09:18:01.095996 31067 solver.cpp:237] Iteration 867, loss = 1.79979
I0521 09:18:01.096140 31067 solver.cpp:253]     Train net output #0: loss = 1.79979 (* 1 = 1.79979 loss)
I0521 09:18:01.096154 31067 sgd_solver.cpp:106] Iteration 867, lr = 0.0025
I0521 09:18:09.099115 31067 solver.cpp:237] Iteration 884, loss = 1.81825
I0521 09:18:09.099148 31067 solver.cpp:253]     Train net output #0: loss = 1.81825 (* 1 = 1.81825 loss)
I0521 09:18:09.099162 31067 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0521 09:18:17.107285 31067 solver.cpp:237] Iteration 901, loss = 1.8707
I0521 09:18:17.107332 31067 solver.cpp:253]     Train net output #0: loss = 1.8707 (* 1 = 1.8707 loss)
I0521 09:18:17.107350 31067 sgd_solver.cpp:106] Iteration 901, lr = 0.0025
I0521 09:18:47.245898 31067 solver.cpp:237] Iteration 918, loss = 1.8594
I0521 09:18:47.246081 31067 solver.cpp:253]     Train net output #0: loss = 1.8594 (* 1 = 1.8594 loss)
I0521 09:18:47.246098 31067 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 09:18:55.252264 31067 solver.cpp:237] Iteration 935, loss = 1.78706
I0521 09:18:55.252295 31067 solver.cpp:253]     Train net output #0: loss = 1.78706 (* 1 = 1.78706 loss)
I0521 09:18:55.252313 31067 sgd_solver.cpp:106] Iteration 935, lr = 0.0025
I0521 09:19:03.256537 31067 solver.cpp:237] Iteration 952, loss = 1.76755
I0521 09:19:03.256572 31067 solver.cpp:253]     Train net output #0: loss = 1.76755 (* 1 = 1.76755 loss)
I0521 09:19:03.256587 31067 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0521 09:19:11.259500 31067 solver.cpp:237] Iteration 969, loss = 1.85745
I0521 09:19:11.259539 31067 solver.cpp:253]     Train net output #0: loss = 1.85745 (* 1 = 1.85745 loss)
I0521 09:19:11.259560 31067 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0521 09:19:19.265478 31067 solver.cpp:237] Iteration 986, loss = 1.90592
I0521 09:19:19.265619 31067 solver.cpp:253]     Train net output #0: loss = 1.90592 (* 1 = 1.90592 loss)
I0521 09:19:19.265632 31067 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0521 09:19:27.273044 31067 solver.cpp:237] Iteration 1003, loss = 1.8402
I0521 09:19:27.273077 31067 solver.cpp:253]     Train net output #0: loss = 1.8402 (* 1 = 1.8402 loss)
I0521 09:19:27.273094 31067 sgd_solver.cpp:106] Iteration 1003, lr = 0.0025
I0521 09:19:34.808594 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1020.caffemodel
I0521 09:19:35.176556 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1020.solverstate
I0521 09:19:35.202811 31067 solver.cpp:341] Iteration 1020, Testing net (#0)
I0521 09:20:19.873324 31067 solver.cpp:409]     Test net output #0: accuracy = 0.625448
I0521 09:20:19.873484 31067 solver.cpp:409]     Test net output #1: loss = 1.35598 (* 1 = 1.35598 loss)
I0521 09:20:20.013852 31067 solver.cpp:237] Iteration 1020, loss = 1.74886
I0521 09:20:20.013881 31067 solver.cpp:253]     Train net output #0: loss = 1.74886 (* 1 = 1.74886 loss)
I0521 09:20:20.013900 31067 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 09:20:50.152927 31067 solver.cpp:237] Iteration 1037, loss = 1.86071
I0521 09:20:50.153100 31067 solver.cpp:253]     Train net output #0: loss = 1.86071 (* 1 = 1.86071 loss)
I0521 09:20:50.153115 31067 sgd_solver.cpp:106] Iteration 1037, lr = 0.0025
I0521 09:20:58.163630 31067 solver.cpp:237] Iteration 1054, loss = 1.85392
I0521 09:20:58.163662 31067 solver.cpp:253]     Train net output #0: loss = 1.85392 (* 1 = 1.85392 loss)
I0521 09:20:58.163681 31067 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0521 09:21:06.174885 31067 solver.cpp:237] Iteration 1071, loss = 1.78916
I0521 09:21:06.174918 31067 solver.cpp:253]     Train net output #0: loss = 1.78916 (* 1 = 1.78916 loss)
I0521 09:21:06.174934 31067 sgd_solver.cpp:106] Iteration 1071, lr = 0.0025
I0521 09:21:14.195256 31067 solver.cpp:237] Iteration 1088, loss = 1.82628
I0521 09:21:14.195298 31067 solver.cpp:253]     Train net output #0: loss = 1.82628 (* 1 = 1.82628 loss)
I0521 09:21:14.195318 31067 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 09:21:22.210322 31067 solver.cpp:237] Iteration 1105, loss = 1.74633
I0521 09:21:22.210461 31067 solver.cpp:253]     Train net output #0: loss = 1.74633 (* 1 = 1.74633 loss)
I0521 09:21:22.210474 31067 sgd_solver.cpp:106] Iteration 1105, lr = 0.0025
I0521 09:21:30.213441 31067 solver.cpp:237] Iteration 1122, loss = 1.78061
I0521 09:21:30.213474 31067 solver.cpp:253]     Train net output #0: loss = 1.78061 (* 1 = 1.78061 loss)
I0521 09:21:30.213491 31067 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 09:22:00.360803 31067 solver.cpp:237] Iteration 1139, loss = 1.76761
I0521 09:22:00.360988 31067 solver.cpp:253]     Train net output #0: loss = 1.76761 (* 1 = 1.76761 loss)
I0521 09:22:00.361006 31067 sgd_solver.cpp:106] Iteration 1139, lr = 0.0025
I0521 09:22:08.368158 31067 solver.cpp:237] Iteration 1156, loss = 1.75863
I0521 09:22:08.368191 31067 solver.cpp:253]     Train net output #0: loss = 1.75863 (* 1 = 1.75863 loss)
I0521 09:22:08.368208 31067 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0521 09:22:16.380028 31067 solver.cpp:237] Iteration 1173, loss = 1.80188
I0521 09:22:16.380062 31067 solver.cpp:253]     Train net output #0: loss = 1.80188 (* 1 = 1.80188 loss)
I0521 09:22:16.380079 31067 sgd_solver.cpp:106] Iteration 1173, lr = 0.0025
I0521 09:22:23.924830 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1190.caffemodel
I0521 09:22:24.293892 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1190.solverstate
I0521 09:22:24.460803 31067 solver.cpp:237] Iteration 1190, loss = 1.82697
I0521 09:22:24.460851 31067 solver.cpp:253]     Train net output #0: loss = 1.82697 (* 1 = 1.82697 loss)
I0521 09:22:24.460877 31067 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0521 09:22:32.478590 31067 solver.cpp:237] Iteration 1207, loss = 1.7786
I0521 09:22:32.478745 31067 solver.cpp:253]     Train net output #0: loss = 1.7786 (* 1 = 1.7786 loss)
I0521 09:22:32.478759 31067 sgd_solver.cpp:106] Iteration 1207, lr = 0.0025
I0521 09:22:40.488839 31067 solver.cpp:237] Iteration 1224, loss = 1.76101
I0521 09:22:40.488878 31067 solver.cpp:253]     Train net output #0: loss = 1.76101 (* 1 = 1.76101 loss)
I0521 09:22:40.488895 31067 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 09:22:48.498807 31067 solver.cpp:237] Iteration 1241, loss = 1.77471
I0521 09:22:48.498839 31067 solver.cpp:253]     Train net output #0: loss = 1.77471 (* 1 = 1.77471 loss)
I0521 09:22:48.498858 31067 sgd_solver.cpp:106] Iteration 1241, lr = 0.0025
I0521 09:23:18.646610 31067 solver.cpp:237] Iteration 1258, loss = 1.78219
I0521 09:23:18.646782 31067 solver.cpp:253]     Train net output #0: loss = 1.78219 (* 1 = 1.78219 loss)
I0521 09:23:18.646797 31067 sgd_solver.cpp:106] Iteration 1258, lr = 0.0025
I0521 09:23:26.664479 31067 solver.cpp:237] Iteration 1275, loss = 1.72493
I0521 09:23:26.664511 31067 solver.cpp:253]     Train net output #0: loss = 1.72493 (* 1 = 1.72493 loss)
I0521 09:23:26.664525 31067 sgd_solver.cpp:106] Iteration 1275, lr = 0.0025
I0521 09:23:34.675107 31067 solver.cpp:237] Iteration 1292, loss = 1.744
I0521 09:23:34.675139 31067 solver.cpp:253]     Train net output #0: loss = 1.744 (* 1 = 1.744 loss)
I0521 09:23:34.675155 31067 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0521 09:23:42.686064 31067 solver.cpp:237] Iteration 1309, loss = 1.85104
I0521 09:23:42.686110 31067 solver.cpp:253]     Train net output #0: loss = 1.85104 (* 1 = 1.85104 loss)
I0521 09:23:42.686128 31067 sgd_solver.cpp:106] Iteration 1309, lr = 0.0025
I0521 09:23:50.700805 31067 solver.cpp:237] Iteration 1326, loss = 1.76671
I0521 09:23:50.700953 31067 solver.cpp:253]     Train net output #0: loss = 1.76671 (* 1 = 1.76671 loss)
I0521 09:23:50.700966 31067 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0521 09:23:58.714982 31067 solver.cpp:237] Iteration 1343, loss = 1.71672
I0521 09:23:58.715014 31067 solver.cpp:253]     Train net output #0: loss = 1.71672 (* 1 = 1.71672 loss)
I0521 09:23:58.715032 31067 sgd_solver.cpp:106] Iteration 1343, lr = 0.0025
I0521 09:24:06.250603 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1360.caffemodel
I0521 09:24:06.619756 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1360.solverstate
I0521 09:24:06.645983 31067 solver.cpp:341] Iteration 1360, Testing net (#0)
I0521 09:25:12.487107 31067 solver.cpp:409]     Test net output #0: accuracy = 0.652734
I0521 09:25:12.487283 31067 solver.cpp:409]     Test net output #1: loss = 1.2152 (* 1 = 1.2152 loss)
I0521 09:25:12.627354 31067 solver.cpp:237] Iteration 1360, loss = 1.65749
I0521 09:25:12.627383 31067 solver.cpp:253]     Train net output #0: loss = 1.65749 (* 1 = 1.65749 loss)
I0521 09:25:12.627400 31067 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 09:25:42.802903 31067 solver.cpp:237] Iteration 1377, loss = 1.76163
I0521 09:25:42.803081 31067 solver.cpp:253]     Train net output #0: loss = 1.76163 (* 1 = 1.76163 loss)
I0521 09:25:42.803097 31067 sgd_solver.cpp:106] Iteration 1377, lr = 0.0025
I0521 09:25:50.819378 31067 solver.cpp:237] Iteration 1394, loss = 1.76863
I0521 09:25:50.819422 31067 solver.cpp:253]     Train net output #0: loss = 1.76863 (* 1 = 1.76863 loss)
I0521 09:25:50.819439 31067 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0521 09:25:58.831207 31067 solver.cpp:237] Iteration 1411, loss = 1.72485
I0521 09:25:58.831239 31067 solver.cpp:253]     Train net output #0: loss = 1.72485 (* 1 = 1.72485 loss)
I0521 09:25:58.831256 31067 sgd_solver.cpp:106] Iteration 1411, lr = 0.0025
I0521 09:26:06.841244 31067 solver.cpp:237] Iteration 1428, loss = 1.70679
I0521 09:26:06.841277 31067 solver.cpp:253]     Train net output #0: loss = 1.70679 (* 1 = 1.70679 loss)
I0521 09:26:06.841294 31067 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0521 09:26:14.851267 31067 solver.cpp:237] Iteration 1445, loss = 1.80973
I0521 09:26:14.851421 31067 solver.cpp:253]     Train net output #0: loss = 1.80973 (* 1 = 1.80973 loss)
I0521 09:26:14.851435 31067 sgd_solver.cpp:106] Iteration 1445, lr = 0.0025
I0521 09:26:22.861258 31067 solver.cpp:237] Iteration 1462, loss = 1.69201
I0521 09:26:22.861290 31067 solver.cpp:253]     Train net output #0: loss = 1.69201 (* 1 = 1.69201 loss)
I0521 09:26:22.861309 31067 sgd_solver.cpp:106] Iteration 1462, lr = 0.0025
I0521 09:26:53.019567 31067 solver.cpp:237] Iteration 1479, loss = 1.71796
I0521 09:26:53.019736 31067 solver.cpp:253]     Train net output #0: loss = 1.71796 (* 1 = 1.71796 loss)
I0521 09:26:53.019752 31067 sgd_solver.cpp:106] Iteration 1479, lr = 0.0025
I0521 09:27:01.038681 31067 solver.cpp:237] Iteration 1496, loss = 1.71069
I0521 09:27:01.038723 31067 solver.cpp:253]     Train net output #0: loss = 1.71069 (* 1 = 1.71069 loss)
I0521 09:27:01.038753 31067 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 09:27:09.050237 31067 solver.cpp:237] Iteration 1513, loss = 1.90458
I0521 09:27:09.050271 31067 solver.cpp:253]     Train net output #0: loss = 1.90458 (* 1 = 1.90458 loss)
I0521 09:27:09.050284 31067 sgd_solver.cpp:106] Iteration 1513, lr = 0.0025
I0521 09:27:16.593807 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1530.caffemodel
I0521 09:27:16.964453 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1530.solverstate
I0521 09:27:17.133363 31067 solver.cpp:237] Iteration 1530, loss = 1.75046
I0521 09:27:17.133411 31067 solver.cpp:253]     Train net output #0: loss = 1.75046 (* 1 = 1.75046 loss)
I0521 09:27:17.133429 31067 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 09:27:25.144402 31067 solver.cpp:237] Iteration 1547, loss = 1.72803
I0521 09:27:25.144567 31067 solver.cpp:253]     Train net output #0: loss = 1.72803 (* 1 = 1.72803 loss)
I0521 09:27:25.144582 31067 sgd_solver.cpp:106] Iteration 1547, lr = 0.0025
I0521 09:27:33.156442 31067 solver.cpp:237] Iteration 1564, loss = 1.6727
I0521 09:27:33.156476 31067 solver.cpp:253]     Train net output #0: loss = 1.6727 (* 1 = 1.6727 loss)
I0521 09:27:33.156493 31067 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0521 09:27:41.172332 31067 solver.cpp:237] Iteration 1581, loss = 1.71451
I0521 09:27:41.172365 31067 solver.cpp:253]     Train net output #0: loss = 1.71451 (* 1 = 1.71451 loss)
I0521 09:27:41.172379 31067 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0521 09:28:11.330497 31067 solver.cpp:237] Iteration 1598, loss = 1.70569
I0521 09:28:11.330682 31067 solver.cpp:253]     Train net output #0: loss = 1.70569 (* 1 = 1.70569 loss)
I0521 09:28:11.330698 31067 sgd_solver.cpp:106] Iteration 1598, lr = 0.0025
I0521 09:28:19.342499 31067 solver.cpp:237] Iteration 1615, loss = 1.6937
I0521 09:28:19.342542 31067 solver.cpp:253]     Train net output #0: loss = 1.6937 (* 1 = 1.6937 loss)
I0521 09:28:19.342560 31067 sgd_solver.cpp:106] Iteration 1615, lr = 0.0025
I0521 09:28:27.353080 31067 solver.cpp:237] Iteration 1632, loss = 1.71825
I0521 09:28:27.353114 31067 solver.cpp:253]     Train net output #0: loss = 1.71825 (* 1 = 1.71825 loss)
I0521 09:28:27.353130 31067 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 09:28:35.368366 31067 solver.cpp:237] Iteration 1649, loss = 1.71713
I0521 09:28:35.368401 31067 solver.cpp:253]     Train net output #0: loss = 1.71713 (* 1 = 1.71713 loss)
I0521 09:28:35.368417 31067 sgd_solver.cpp:106] Iteration 1649, lr = 0.0025
I0521 09:28:43.381062 31067 solver.cpp:237] Iteration 1666, loss = 1.75556
I0521 09:28:43.381227 31067 solver.cpp:253]     Train net output #0: loss = 1.75556 (* 1 = 1.75556 loss)
I0521 09:28:43.381242 31067 sgd_solver.cpp:106] Iteration 1666, lr = 0.0025
I0521 09:28:51.394307 31067 solver.cpp:237] Iteration 1683, loss = 1.70896
I0521 09:28:51.394340 31067 solver.cpp:253]     Train net output #0: loss = 1.70896 (* 1 = 1.70896 loss)
I0521 09:28:51.394357 31067 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0521 09:28:58.934023 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1700.caffemodel
I0521 09:28:59.306754 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1700.solverstate
I0521 09:28:59.335433 31067 solver.cpp:341] Iteration 1700, Testing net (#0)
I0521 09:29:44.327791 31067 solver.cpp:409]     Test net output #0: accuracy = 0.666745
I0521 09:29:44.327975 31067 solver.cpp:409]     Test net output #1: loss = 1.17806 (* 1 = 1.17806 loss)
I0521 09:29:44.468044 31067 solver.cpp:237] Iteration 1700, loss = 1.72189
I0521 09:29:44.468073 31067 solver.cpp:253]     Train net output #0: loss = 1.72189 (* 1 = 1.72189 loss)
I0521 09:29:44.468093 31067 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 09:29:45.879457 31067 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1704.caffemodel
I0521 09:29:46.248764 31067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475_iter_1704.solverstate
I0521 09:29:46.277412 31067 solver.cpp:326] Optimization Done.
I0521 09:29:46.277442 31067 caffe.cpp:215] Optimization Done.
Application 11237447 resources: utime ~1247s, stime ~225s, Rss ~5329016, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_880_2016-05-20T11.21.04.813475.solver"
	User time (seconds): 0.57
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:36.20
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15080
	Voluntary context switches: 2716
	Involuntary context switches: 92
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
