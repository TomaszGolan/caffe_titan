2806046
I0520 23:36:42.995549  1727 caffe.cpp:184] Using GPUs 0
I0520 23:36:43.417129  1727 solver.cpp:48] Initializing solver from parameters: 
test_iter: 348
test_interval: 697
base_lr: 0.0025
display: 34
max_iter: 3488
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 348
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973.prototxt"
I0520 23:36:43.513923  1727 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973.prototxt
I0520 23:36:43.554267  1727 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 23:36:43.554327  1727 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 23:36:43.554675  1727 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 430
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:36:43.554853  1727 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:36:43.554877  1727 net.cpp:106] Creating Layer data_hdf5
I0520 23:36:43.554891  1727 net.cpp:411] data_hdf5 -> data
I0520 23:36:43.554925  1727 net.cpp:411] data_hdf5 -> label
I0520 23:36:43.554960  1727 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 23:36:43.584698  1727 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 23:36:43.613198  1727 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 23:37:05.111857  1727 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 23:37:06.152571  1727 net.cpp:150] Setting up data_hdf5
I0520 23:37:06.152613  1727 net.cpp:157] Top shape: 430 1 127 50 (2730500)
I0520 23:37:06.152628  1727 net.cpp:157] Top shape: 430 (430)
I0520 23:37:06.152642  1727 net.cpp:165] Memory required for data: 10923720
I0520 23:37:06.152655  1727 layer_factory.hpp:77] Creating layer conv1
I0520 23:37:06.152689  1727 net.cpp:106] Creating Layer conv1
I0520 23:37:06.152701  1727 net.cpp:454] conv1 <- data
I0520 23:37:06.152722  1727 net.cpp:411] conv1 -> conv1
I0520 23:37:06.515064  1727 net.cpp:150] Setting up conv1
I0520 23:37:06.515105  1727 net.cpp:157] Top shape: 430 12 120 48 (29721600)
I0520 23:37:06.515116  1727 net.cpp:165] Memory required for data: 129810120
I0520 23:37:06.515144  1727 layer_factory.hpp:77] Creating layer relu1
I0520 23:37:06.515166  1727 net.cpp:106] Creating Layer relu1
I0520 23:37:06.515177  1727 net.cpp:454] relu1 <- conv1
I0520 23:37:06.515190  1727 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:37:06.515705  1727 net.cpp:150] Setting up relu1
I0520 23:37:06.515722  1727 net.cpp:157] Top shape: 430 12 120 48 (29721600)
I0520 23:37:06.515733  1727 net.cpp:165] Memory required for data: 248696520
I0520 23:37:06.515743  1727 layer_factory.hpp:77] Creating layer pool1
I0520 23:37:06.515758  1727 net.cpp:106] Creating Layer pool1
I0520 23:37:06.515769  1727 net.cpp:454] pool1 <- conv1
I0520 23:37:06.515781  1727 net.cpp:411] pool1 -> pool1
I0520 23:37:06.515861  1727 net.cpp:150] Setting up pool1
I0520 23:37:06.515875  1727 net.cpp:157] Top shape: 430 12 60 48 (14860800)
I0520 23:37:06.515885  1727 net.cpp:165] Memory required for data: 308139720
I0520 23:37:06.515895  1727 layer_factory.hpp:77] Creating layer conv2
I0520 23:37:06.515918  1727 net.cpp:106] Creating Layer conv2
I0520 23:37:06.515928  1727 net.cpp:454] conv2 <- pool1
I0520 23:37:06.515941  1727 net.cpp:411] conv2 -> conv2
I0520 23:37:06.518661  1727 net.cpp:150] Setting up conv2
I0520 23:37:06.518689  1727 net.cpp:157] Top shape: 430 20 54 46 (21362400)
I0520 23:37:06.518700  1727 net.cpp:165] Memory required for data: 393589320
I0520 23:37:06.518719  1727 layer_factory.hpp:77] Creating layer relu2
I0520 23:37:06.518734  1727 net.cpp:106] Creating Layer relu2
I0520 23:37:06.518744  1727 net.cpp:454] relu2 <- conv2
I0520 23:37:06.518759  1727 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:37:06.519091  1727 net.cpp:150] Setting up relu2
I0520 23:37:06.519105  1727 net.cpp:157] Top shape: 430 20 54 46 (21362400)
I0520 23:37:06.519115  1727 net.cpp:165] Memory required for data: 479038920
I0520 23:37:06.519125  1727 layer_factory.hpp:77] Creating layer pool2
I0520 23:37:06.519137  1727 net.cpp:106] Creating Layer pool2
I0520 23:37:06.519148  1727 net.cpp:454] pool2 <- conv2
I0520 23:37:06.519172  1727 net.cpp:411] pool2 -> pool2
I0520 23:37:06.519243  1727 net.cpp:150] Setting up pool2
I0520 23:37:06.519255  1727 net.cpp:157] Top shape: 430 20 27 46 (10681200)
I0520 23:37:06.519265  1727 net.cpp:165] Memory required for data: 521763720
I0520 23:37:06.519275  1727 layer_factory.hpp:77] Creating layer conv3
I0520 23:37:06.519294  1727 net.cpp:106] Creating Layer conv3
I0520 23:37:06.519304  1727 net.cpp:454] conv3 <- pool2
I0520 23:37:06.519316  1727 net.cpp:411] conv3 -> conv3
I0520 23:37:06.521248  1727 net.cpp:150] Setting up conv3
I0520 23:37:06.521271  1727 net.cpp:157] Top shape: 430 28 22 44 (11654720)
I0520 23:37:06.521284  1727 net.cpp:165] Memory required for data: 568382600
I0520 23:37:06.521302  1727 layer_factory.hpp:77] Creating layer relu3
I0520 23:37:06.521318  1727 net.cpp:106] Creating Layer relu3
I0520 23:37:06.521328  1727 net.cpp:454] relu3 <- conv3
I0520 23:37:06.521342  1727 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:37:06.521811  1727 net.cpp:150] Setting up relu3
I0520 23:37:06.521828  1727 net.cpp:157] Top shape: 430 28 22 44 (11654720)
I0520 23:37:06.521839  1727 net.cpp:165] Memory required for data: 615001480
I0520 23:37:06.521849  1727 layer_factory.hpp:77] Creating layer pool3
I0520 23:37:06.521862  1727 net.cpp:106] Creating Layer pool3
I0520 23:37:06.521872  1727 net.cpp:454] pool3 <- conv3
I0520 23:37:06.521885  1727 net.cpp:411] pool3 -> pool3
I0520 23:37:06.521952  1727 net.cpp:150] Setting up pool3
I0520 23:37:06.521965  1727 net.cpp:157] Top shape: 430 28 11 44 (5827360)
I0520 23:37:06.521975  1727 net.cpp:165] Memory required for data: 638310920
I0520 23:37:06.521984  1727 layer_factory.hpp:77] Creating layer conv4
I0520 23:37:06.522002  1727 net.cpp:106] Creating Layer conv4
I0520 23:37:06.522013  1727 net.cpp:454] conv4 <- pool3
I0520 23:37:06.522027  1727 net.cpp:411] conv4 -> conv4
I0520 23:37:06.524782  1727 net.cpp:150] Setting up conv4
I0520 23:37:06.524811  1727 net.cpp:157] Top shape: 430 36 6 42 (3900960)
I0520 23:37:06.524822  1727 net.cpp:165] Memory required for data: 653914760
I0520 23:37:06.524838  1727 layer_factory.hpp:77] Creating layer relu4
I0520 23:37:06.524852  1727 net.cpp:106] Creating Layer relu4
I0520 23:37:06.524863  1727 net.cpp:454] relu4 <- conv4
I0520 23:37:06.524875  1727 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:37:06.525358  1727 net.cpp:150] Setting up relu4
I0520 23:37:06.525374  1727 net.cpp:157] Top shape: 430 36 6 42 (3900960)
I0520 23:37:06.525385  1727 net.cpp:165] Memory required for data: 669518600
I0520 23:37:06.525395  1727 layer_factory.hpp:77] Creating layer pool4
I0520 23:37:06.525408  1727 net.cpp:106] Creating Layer pool4
I0520 23:37:06.525418  1727 net.cpp:454] pool4 <- conv4
I0520 23:37:06.525431  1727 net.cpp:411] pool4 -> pool4
I0520 23:37:06.525499  1727 net.cpp:150] Setting up pool4
I0520 23:37:06.525513  1727 net.cpp:157] Top shape: 430 36 3 42 (1950480)
I0520 23:37:06.525524  1727 net.cpp:165] Memory required for data: 677320520
I0520 23:37:06.525534  1727 layer_factory.hpp:77] Creating layer ip1
I0520 23:37:06.525552  1727 net.cpp:106] Creating Layer ip1
I0520 23:37:06.525563  1727 net.cpp:454] ip1 <- pool4
I0520 23:37:06.525576  1727 net.cpp:411] ip1 -> ip1
I0520 23:37:06.540998  1727 net.cpp:150] Setting up ip1
I0520 23:37:06.541026  1727 net.cpp:157] Top shape: 430 196 (84280)
I0520 23:37:06.541039  1727 net.cpp:165] Memory required for data: 677657640
I0520 23:37:06.541060  1727 layer_factory.hpp:77] Creating layer relu5
I0520 23:37:06.541075  1727 net.cpp:106] Creating Layer relu5
I0520 23:37:06.541085  1727 net.cpp:454] relu5 <- ip1
I0520 23:37:06.541100  1727 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:37:06.541443  1727 net.cpp:150] Setting up relu5
I0520 23:37:06.541457  1727 net.cpp:157] Top shape: 430 196 (84280)
I0520 23:37:06.541468  1727 net.cpp:165] Memory required for data: 677994760
I0520 23:37:06.541478  1727 layer_factory.hpp:77] Creating layer drop1
I0520 23:37:06.541499  1727 net.cpp:106] Creating Layer drop1
I0520 23:37:06.541509  1727 net.cpp:454] drop1 <- ip1
I0520 23:37:06.541533  1727 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:37:06.541579  1727 net.cpp:150] Setting up drop1
I0520 23:37:06.541594  1727 net.cpp:157] Top shape: 430 196 (84280)
I0520 23:37:06.541604  1727 net.cpp:165] Memory required for data: 678331880
I0520 23:37:06.541613  1727 layer_factory.hpp:77] Creating layer ip2
I0520 23:37:06.541631  1727 net.cpp:106] Creating Layer ip2
I0520 23:37:06.541641  1727 net.cpp:454] ip2 <- ip1
I0520 23:37:06.541654  1727 net.cpp:411] ip2 -> ip2
I0520 23:37:06.542117  1727 net.cpp:150] Setting up ip2
I0520 23:37:06.542131  1727 net.cpp:157] Top shape: 430 98 (42140)
I0520 23:37:06.542141  1727 net.cpp:165] Memory required for data: 678500440
I0520 23:37:06.542156  1727 layer_factory.hpp:77] Creating layer relu6
I0520 23:37:06.542168  1727 net.cpp:106] Creating Layer relu6
I0520 23:37:06.542178  1727 net.cpp:454] relu6 <- ip2
I0520 23:37:06.542189  1727 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:37:06.542711  1727 net.cpp:150] Setting up relu6
I0520 23:37:06.542727  1727 net.cpp:157] Top shape: 430 98 (42140)
I0520 23:37:06.542737  1727 net.cpp:165] Memory required for data: 678669000
I0520 23:37:06.542748  1727 layer_factory.hpp:77] Creating layer drop2
I0520 23:37:06.542762  1727 net.cpp:106] Creating Layer drop2
I0520 23:37:06.542770  1727 net.cpp:454] drop2 <- ip2
I0520 23:37:06.542783  1727 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:37:06.542824  1727 net.cpp:150] Setting up drop2
I0520 23:37:06.542839  1727 net.cpp:157] Top shape: 430 98 (42140)
I0520 23:37:06.542850  1727 net.cpp:165] Memory required for data: 678837560
I0520 23:37:06.542860  1727 layer_factory.hpp:77] Creating layer ip3
I0520 23:37:06.542872  1727 net.cpp:106] Creating Layer ip3
I0520 23:37:06.542882  1727 net.cpp:454] ip3 <- ip2
I0520 23:37:06.542894  1727 net.cpp:411] ip3 -> ip3
I0520 23:37:06.543104  1727 net.cpp:150] Setting up ip3
I0520 23:37:06.543118  1727 net.cpp:157] Top shape: 430 11 (4730)
I0520 23:37:06.543128  1727 net.cpp:165] Memory required for data: 678856480
I0520 23:37:06.543143  1727 layer_factory.hpp:77] Creating layer drop3
I0520 23:37:06.543155  1727 net.cpp:106] Creating Layer drop3
I0520 23:37:06.543164  1727 net.cpp:454] drop3 <- ip3
I0520 23:37:06.543176  1727 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:37:06.543216  1727 net.cpp:150] Setting up drop3
I0520 23:37:06.543229  1727 net.cpp:157] Top shape: 430 11 (4730)
I0520 23:37:06.543239  1727 net.cpp:165] Memory required for data: 678875400
I0520 23:37:06.543249  1727 layer_factory.hpp:77] Creating layer loss
I0520 23:37:06.543267  1727 net.cpp:106] Creating Layer loss
I0520 23:37:06.543277  1727 net.cpp:454] loss <- ip3
I0520 23:37:06.543288  1727 net.cpp:454] loss <- label
I0520 23:37:06.543301  1727 net.cpp:411] loss -> loss
I0520 23:37:06.543318  1727 layer_factory.hpp:77] Creating layer loss
I0520 23:37:06.543964  1727 net.cpp:150] Setting up loss
I0520 23:37:06.543984  1727 net.cpp:157] Top shape: (1)
I0520 23:37:06.543998  1727 net.cpp:160]     with loss weight 1
I0520 23:37:06.544039  1727 net.cpp:165] Memory required for data: 678875404
I0520 23:37:06.544049  1727 net.cpp:226] loss needs backward computation.
I0520 23:37:06.544060  1727 net.cpp:226] drop3 needs backward computation.
I0520 23:37:06.544070  1727 net.cpp:226] ip3 needs backward computation.
I0520 23:37:06.544081  1727 net.cpp:226] drop2 needs backward computation.
I0520 23:37:06.544090  1727 net.cpp:226] relu6 needs backward computation.
I0520 23:37:06.544100  1727 net.cpp:226] ip2 needs backward computation.
I0520 23:37:06.544111  1727 net.cpp:226] drop1 needs backward computation.
I0520 23:37:06.544121  1727 net.cpp:226] relu5 needs backward computation.
I0520 23:37:06.544129  1727 net.cpp:226] ip1 needs backward computation.
I0520 23:37:06.544140  1727 net.cpp:226] pool4 needs backward computation.
I0520 23:37:06.544150  1727 net.cpp:226] relu4 needs backward computation.
I0520 23:37:06.544159  1727 net.cpp:226] conv4 needs backward computation.
I0520 23:37:06.544170  1727 net.cpp:226] pool3 needs backward computation.
I0520 23:37:06.544189  1727 net.cpp:226] relu3 needs backward computation.
I0520 23:37:06.544198  1727 net.cpp:226] conv3 needs backward computation.
I0520 23:37:06.544208  1727 net.cpp:226] pool2 needs backward computation.
I0520 23:37:06.544219  1727 net.cpp:226] relu2 needs backward computation.
I0520 23:37:06.544229  1727 net.cpp:226] conv2 needs backward computation.
I0520 23:37:06.544239  1727 net.cpp:226] pool1 needs backward computation.
I0520 23:37:06.544251  1727 net.cpp:226] relu1 needs backward computation.
I0520 23:37:06.544261  1727 net.cpp:226] conv1 needs backward computation.
I0520 23:37:06.544272  1727 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:37:06.544282  1727 net.cpp:270] This network produces output loss
I0520 23:37:06.544306  1727 net.cpp:283] Network initialization done.
I0520 23:37:06.584576  1727 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973.prototxt
I0520 23:37:06.584656  1727 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 23:37:06.585023  1727 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 430
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 23:37:06.585218  1727 layer_factory.hpp:77] Creating layer data_hdf5
I0520 23:37:06.585233  1727 net.cpp:106] Creating Layer data_hdf5
I0520 23:37:06.585249  1727 net.cpp:411] data_hdf5 -> data
I0520 23:37:06.585268  1727 net.cpp:411] data_hdf5 -> label
I0520 23:37:06.585283  1727 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 23:37:06.619602  1727 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 23:37:27.949187  1727 net.cpp:150] Setting up data_hdf5
I0520 23:37:27.949347  1727 net.cpp:157] Top shape: 430 1 127 50 (2730500)
I0520 23:37:27.949362  1727 net.cpp:157] Top shape: 430 (430)
I0520 23:37:27.949374  1727 net.cpp:165] Memory required for data: 10923720
I0520 23:37:27.949388  1727 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 23:37:27.949416  1727 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 23:37:27.949427  1727 net.cpp:454] label_data_hdf5_1_split <- label
I0520 23:37:27.949442  1727 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 23:37:27.949465  1727 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 23:37:27.949537  1727 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 23:37:27.949551  1727 net.cpp:157] Top shape: 430 (430)
I0520 23:37:27.949563  1727 net.cpp:157] Top shape: 430 (430)
I0520 23:37:27.949573  1727 net.cpp:165] Memory required for data: 10927160
I0520 23:37:27.949582  1727 layer_factory.hpp:77] Creating layer conv1
I0520 23:37:27.949604  1727 net.cpp:106] Creating Layer conv1
I0520 23:37:27.949615  1727 net.cpp:454] conv1 <- data
I0520 23:37:27.949630  1727 net.cpp:411] conv1 -> conv1
I0520 23:37:27.951539  1727 net.cpp:150] Setting up conv1
I0520 23:37:27.951562  1727 net.cpp:157] Top shape: 430 12 120 48 (29721600)
I0520 23:37:27.951575  1727 net.cpp:165] Memory required for data: 129813560
I0520 23:37:27.951596  1727 layer_factory.hpp:77] Creating layer relu1
I0520 23:37:27.951611  1727 net.cpp:106] Creating Layer relu1
I0520 23:37:27.951620  1727 net.cpp:454] relu1 <- conv1
I0520 23:37:27.951633  1727 net.cpp:397] relu1 -> conv1 (in-place)
I0520 23:37:27.952131  1727 net.cpp:150] Setting up relu1
I0520 23:37:27.952147  1727 net.cpp:157] Top shape: 430 12 120 48 (29721600)
I0520 23:37:27.952157  1727 net.cpp:165] Memory required for data: 248699960
I0520 23:37:27.952168  1727 layer_factory.hpp:77] Creating layer pool1
I0520 23:37:27.952184  1727 net.cpp:106] Creating Layer pool1
I0520 23:37:27.952194  1727 net.cpp:454] pool1 <- conv1
I0520 23:37:27.952208  1727 net.cpp:411] pool1 -> pool1
I0520 23:37:27.952282  1727 net.cpp:150] Setting up pool1
I0520 23:37:27.952296  1727 net.cpp:157] Top shape: 430 12 60 48 (14860800)
I0520 23:37:27.952306  1727 net.cpp:165] Memory required for data: 308143160
I0520 23:37:27.952316  1727 layer_factory.hpp:77] Creating layer conv2
I0520 23:37:27.952334  1727 net.cpp:106] Creating Layer conv2
I0520 23:37:27.952344  1727 net.cpp:454] conv2 <- pool1
I0520 23:37:27.952358  1727 net.cpp:411] conv2 -> conv2
I0520 23:37:27.954282  1727 net.cpp:150] Setting up conv2
I0520 23:37:27.954303  1727 net.cpp:157] Top shape: 430 20 54 46 (21362400)
I0520 23:37:27.954316  1727 net.cpp:165] Memory required for data: 393592760
I0520 23:37:27.954334  1727 layer_factory.hpp:77] Creating layer relu2
I0520 23:37:27.954347  1727 net.cpp:106] Creating Layer relu2
I0520 23:37:27.954357  1727 net.cpp:454] relu2 <- conv2
I0520 23:37:27.954370  1727 net.cpp:397] relu2 -> conv2 (in-place)
I0520 23:37:27.954704  1727 net.cpp:150] Setting up relu2
I0520 23:37:27.954717  1727 net.cpp:157] Top shape: 430 20 54 46 (21362400)
I0520 23:37:27.954727  1727 net.cpp:165] Memory required for data: 479042360
I0520 23:37:27.954737  1727 layer_factory.hpp:77] Creating layer pool2
I0520 23:37:27.954751  1727 net.cpp:106] Creating Layer pool2
I0520 23:37:27.954761  1727 net.cpp:454] pool2 <- conv2
I0520 23:37:27.954773  1727 net.cpp:411] pool2 -> pool2
I0520 23:37:27.954845  1727 net.cpp:150] Setting up pool2
I0520 23:37:27.954859  1727 net.cpp:157] Top shape: 430 20 27 46 (10681200)
I0520 23:37:27.954869  1727 net.cpp:165] Memory required for data: 521767160
I0520 23:37:27.954879  1727 layer_factory.hpp:77] Creating layer conv3
I0520 23:37:27.954895  1727 net.cpp:106] Creating Layer conv3
I0520 23:37:27.954907  1727 net.cpp:454] conv3 <- pool2
I0520 23:37:27.954921  1727 net.cpp:411] conv3 -> conv3
I0520 23:37:27.956882  1727 net.cpp:150] Setting up conv3
I0520 23:37:27.956905  1727 net.cpp:157] Top shape: 430 28 22 44 (11654720)
I0520 23:37:27.956918  1727 net.cpp:165] Memory required for data: 568386040
I0520 23:37:27.956950  1727 layer_factory.hpp:77] Creating layer relu3
I0520 23:37:27.956971  1727 net.cpp:106] Creating Layer relu3
I0520 23:37:27.956981  1727 net.cpp:454] relu3 <- conv3
I0520 23:37:27.956995  1727 net.cpp:397] relu3 -> conv3 (in-place)
I0520 23:37:27.957468  1727 net.cpp:150] Setting up relu3
I0520 23:37:27.957484  1727 net.cpp:157] Top shape: 430 28 22 44 (11654720)
I0520 23:37:27.957494  1727 net.cpp:165] Memory required for data: 615004920
I0520 23:37:27.957504  1727 layer_factory.hpp:77] Creating layer pool3
I0520 23:37:27.957517  1727 net.cpp:106] Creating Layer pool3
I0520 23:37:27.957526  1727 net.cpp:454] pool3 <- conv3
I0520 23:37:27.957540  1727 net.cpp:411] pool3 -> pool3
I0520 23:37:27.957610  1727 net.cpp:150] Setting up pool3
I0520 23:37:27.957623  1727 net.cpp:157] Top shape: 430 28 11 44 (5827360)
I0520 23:37:27.957633  1727 net.cpp:165] Memory required for data: 638314360
I0520 23:37:27.957643  1727 layer_factory.hpp:77] Creating layer conv4
I0520 23:37:27.957661  1727 net.cpp:106] Creating Layer conv4
I0520 23:37:27.957671  1727 net.cpp:454] conv4 <- pool3
I0520 23:37:27.957685  1727 net.cpp:411] conv4 -> conv4
I0520 23:37:27.959736  1727 net.cpp:150] Setting up conv4
I0520 23:37:27.959758  1727 net.cpp:157] Top shape: 430 36 6 42 (3900960)
I0520 23:37:27.959771  1727 net.cpp:165] Memory required for data: 653918200
I0520 23:37:27.959786  1727 layer_factory.hpp:77] Creating layer relu4
I0520 23:37:27.959800  1727 net.cpp:106] Creating Layer relu4
I0520 23:37:27.959810  1727 net.cpp:454] relu4 <- conv4
I0520 23:37:27.959822  1727 net.cpp:397] relu4 -> conv4 (in-place)
I0520 23:37:27.960290  1727 net.cpp:150] Setting up relu4
I0520 23:37:27.960306  1727 net.cpp:157] Top shape: 430 36 6 42 (3900960)
I0520 23:37:27.960315  1727 net.cpp:165] Memory required for data: 669522040
I0520 23:37:27.960325  1727 layer_factory.hpp:77] Creating layer pool4
I0520 23:37:27.960338  1727 net.cpp:106] Creating Layer pool4
I0520 23:37:27.960348  1727 net.cpp:454] pool4 <- conv4
I0520 23:37:27.960362  1727 net.cpp:411] pool4 -> pool4
I0520 23:37:27.960433  1727 net.cpp:150] Setting up pool4
I0520 23:37:27.960446  1727 net.cpp:157] Top shape: 430 36 3 42 (1950480)
I0520 23:37:27.960456  1727 net.cpp:165] Memory required for data: 677323960
I0520 23:37:27.960466  1727 layer_factory.hpp:77] Creating layer ip1
I0520 23:37:27.960482  1727 net.cpp:106] Creating Layer ip1
I0520 23:37:27.960492  1727 net.cpp:454] ip1 <- pool4
I0520 23:37:27.960506  1727 net.cpp:411] ip1 -> ip1
I0520 23:37:27.976001  1727 net.cpp:150] Setting up ip1
I0520 23:37:27.976028  1727 net.cpp:157] Top shape: 430 196 (84280)
I0520 23:37:27.976045  1727 net.cpp:165] Memory required for data: 677661080
I0520 23:37:27.976073  1727 layer_factory.hpp:77] Creating layer relu5
I0520 23:37:27.976088  1727 net.cpp:106] Creating Layer relu5
I0520 23:37:27.976097  1727 net.cpp:454] relu5 <- ip1
I0520 23:37:27.976110  1727 net.cpp:397] relu5 -> ip1 (in-place)
I0520 23:37:27.976456  1727 net.cpp:150] Setting up relu5
I0520 23:37:27.976470  1727 net.cpp:157] Top shape: 430 196 (84280)
I0520 23:37:27.976480  1727 net.cpp:165] Memory required for data: 677998200
I0520 23:37:27.976490  1727 layer_factory.hpp:77] Creating layer drop1
I0520 23:37:27.976510  1727 net.cpp:106] Creating Layer drop1
I0520 23:37:27.976519  1727 net.cpp:454] drop1 <- ip1
I0520 23:37:27.976532  1727 net.cpp:397] drop1 -> ip1 (in-place)
I0520 23:37:27.976577  1727 net.cpp:150] Setting up drop1
I0520 23:37:27.976588  1727 net.cpp:157] Top shape: 430 196 (84280)
I0520 23:37:27.976599  1727 net.cpp:165] Memory required for data: 678335320
I0520 23:37:27.976609  1727 layer_factory.hpp:77] Creating layer ip2
I0520 23:37:27.976624  1727 net.cpp:106] Creating Layer ip2
I0520 23:37:27.976634  1727 net.cpp:454] ip2 <- ip1
I0520 23:37:27.976646  1727 net.cpp:411] ip2 -> ip2
I0520 23:37:27.977133  1727 net.cpp:150] Setting up ip2
I0520 23:37:27.977146  1727 net.cpp:157] Top shape: 430 98 (42140)
I0520 23:37:27.977156  1727 net.cpp:165] Memory required for data: 678503880
I0520 23:37:27.977185  1727 layer_factory.hpp:77] Creating layer relu6
I0520 23:37:27.977197  1727 net.cpp:106] Creating Layer relu6
I0520 23:37:27.977207  1727 net.cpp:454] relu6 <- ip2
I0520 23:37:27.977219  1727 net.cpp:397] relu6 -> ip2 (in-place)
I0520 23:37:27.977753  1727 net.cpp:150] Setting up relu6
I0520 23:37:27.977776  1727 net.cpp:157] Top shape: 430 98 (42140)
I0520 23:37:27.977785  1727 net.cpp:165] Memory required for data: 678672440
I0520 23:37:27.977795  1727 layer_factory.hpp:77] Creating layer drop2
I0520 23:37:27.977810  1727 net.cpp:106] Creating Layer drop2
I0520 23:37:27.977820  1727 net.cpp:454] drop2 <- ip2
I0520 23:37:27.977833  1727 net.cpp:397] drop2 -> ip2 (in-place)
I0520 23:37:27.977876  1727 net.cpp:150] Setting up drop2
I0520 23:37:27.977890  1727 net.cpp:157] Top shape: 430 98 (42140)
I0520 23:37:27.977898  1727 net.cpp:165] Memory required for data: 678841000
I0520 23:37:27.977908  1727 layer_factory.hpp:77] Creating layer ip3
I0520 23:37:27.977922  1727 net.cpp:106] Creating Layer ip3
I0520 23:37:27.977932  1727 net.cpp:454] ip3 <- ip2
I0520 23:37:27.977946  1727 net.cpp:411] ip3 -> ip3
I0520 23:37:27.978168  1727 net.cpp:150] Setting up ip3
I0520 23:37:27.978181  1727 net.cpp:157] Top shape: 430 11 (4730)
I0520 23:37:27.978191  1727 net.cpp:165] Memory required for data: 678859920
I0520 23:37:27.978206  1727 layer_factory.hpp:77] Creating layer drop3
I0520 23:37:27.978219  1727 net.cpp:106] Creating Layer drop3
I0520 23:37:27.978229  1727 net.cpp:454] drop3 <- ip3
I0520 23:37:27.978242  1727 net.cpp:397] drop3 -> ip3 (in-place)
I0520 23:37:27.978283  1727 net.cpp:150] Setting up drop3
I0520 23:37:27.978296  1727 net.cpp:157] Top shape: 430 11 (4730)
I0520 23:37:27.978305  1727 net.cpp:165] Memory required for data: 678878840
I0520 23:37:27.978315  1727 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 23:37:27.978327  1727 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 23:37:27.978338  1727 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 23:37:27.978350  1727 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 23:37:27.978366  1727 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 23:37:27.978438  1727 net.cpp:150] Setting up ip3_drop3_0_split
I0520 23:37:27.978451  1727 net.cpp:157] Top shape: 430 11 (4730)
I0520 23:37:27.978471  1727 net.cpp:157] Top shape: 430 11 (4730)
I0520 23:37:27.978480  1727 net.cpp:165] Memory required for data: 678916680
I0520 23:37:27.978490  1727 layer_factory.hpp:77] Creating layer accuracy
I0520 23:37:27.978509  1727 net.cpp:106] Creating Layer accuracy
I0520 23:37:27.978519  1727 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 23:37:27.978530  1727 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 23:37:27.978544  1727 net.cpp:411] accuracy -> accuracy
I0520 23:37:27.978569  1727 net.cpp:150] Setting up accuracy
I0520 23:37:27.978580  1727 net.cpp:157] Top shape: (1)
I0520 23:37:27.978590  1727 net.cpp:165] Memory required for data: 678916684
I0520 23:37:27.978600  1727 layer_factory.hpp:77] Creating layer loss
I0520 23:37:27.978613  1727 net.cpp:106] Creating Layer loss
I0520 23:37:27.978623  1727 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 23:37:27.978634  1727 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 23:37:27.978647  1727 net.cpp:411] loss -> loss
I0520 23:37:27.978664  1727 layer_factory.hpp:77] Creating layer loss
I0520 23:37:27.979153  1727 net.cpp:150] Setting up loss
I0520 23:37:27.979167  1727 net.cpp:157] Top shape: (1)
I0520 23:37:27.979177  1727 net.cpp:160]     with loss weight 1
I0520 23:37:27.979195  1727 net.cpp:165] Memory required for data: 678916688
I0520 23:37:27.979207  1727 net.cpp:226] loss needs backward computation.
I0520 23:37:27.979218  1727 net.cpp:228] accuracy does not need backward computation.
I0520 23:37:27.979228  1727 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 23:37:27.979239  1727 net.cpp:226] drop3 needs backward computation.
I0520 23:37:27.979250  1727 net.cpp:226] ip3 needs backward computation.
I0520 23:37:27.979260  1727 net.cpp:226] drop2 needs backward computation.
I0520 23:37:27.979279  1727 net.cpp:226] relu6 needs backward computation.
I0520 23:37:27.979290  1727 net.cpp:226] ip2 needs backward computation.
I0520 23:37:27.979300  1727 net.cpp:226] drop1 needs backward computation.
I0520 23:37:27.979310  1727 net.cpp:226] relu5 needs backward computation.
I0520 23:37:27.979318  1727 net.cpp:226] ip1 needs backward computation.
I0520 23:37:27.979329  1727 net.cpp:226] pool4 needs backward computation.
I0520 23:37:27.979339  1727 net.cpp:226] relu4 needs backward computation.
I0520 23:37:27.979347  1727 net.cpp:226] conv4 needs backward computation.
I0520 23:37:27.979357  1727 net.cpp:226] pool3 needs backward computation.
I0520 23:37:27.979368  1727 net.cpp:226] relu3 needs backward computation.
I0520 23:37:27.979378  1727 net.cpp:226] conv3 needs backward computation.
I0520 23:37:27.979388  1727 net.cpp:226] pool2 needs backward computation.
I0520 23:37:27.979398  1727 net.cpp:226] relu2 needs backward computation.
I0520 23:37:27.979408  1727 net.cpp:226] conv2 needs backward computation.
I0520 23:37:27.979418  1727 net.cpp:226] pool1 needs backward computation.
I0520 23:37:27.979429  1727 net.cpp:226] relu1 needs backward computation.
I0520 23:37:27.979439  1727 net.cpp:226] conv1 needs backward computation.
I0520 23:37:27.979450  1727 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 23:37:27.979462  1727 net.cpp:228] data_hdf5 does not need backward computation.
I0520 23:37:27.979472  1727 net.cpp:270] This network produces output accuracy
I0520 23:37:27.979482  1727 net.cpp:270] This network produces output loss
I0520 23:37:27.979511  1727 net.cpp:283] Network initialization done.
I0520 23:37:27.979645  1727 solver.cpp:60] Solver scaffolding done.
I0520 23:37:27.980774  1727 caffe.cpp:212] Starting Optimization
I0520 23:37:27.980792  1727 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 23:37:27.980806  1727 solver.cpp:289] Learning Rate Policy: fixed
I0520 23:37:27.982038  1727 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 23:38:14.266881  1727 solver.cpp:409]     Test net output #0: accuracy = 0.072514
I0520 23:38:14.267040  1727 solver.cpp:409]     Test net output #1: loss = 2.39912 (* 1 = 2.39912 loss)
I0520 23:38:14.354612  1727 solver.cpp:237] Iteration 0, loss = 2.39652
I0520 23:38:14.354647  1727 solver.cpp:253]     Train net output #0: loss = 2.39652 (* 1 = 2.39652 loss)
I0520 23:38:14.354668  1727 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 23:38:22.269796  1727 solver.cpp:237] Iteration 34, loss = 2.36848
I0520 23:38:22.269830  1727 solver.cpp:253]     Train net output #0: loss = 2.36848 (* 1 = 2.36848 loss)
I0520 23:38:22.269847  1727 sgd_solver.cpp:106] Iteration 34, lr = 0.0025
I0520 23:38:30.185570  1727 solver.cpp:237] Iteration 68, loss = 2.36289
I0520 23:38:30.185605  1727 solver.cpp:253]     Train net output #0: loss = 2.36289 (* 1 = 2.36289 loss)
I0520 23:38:30.185621  1727 sgd_solver.cpp:106] Iteration 68, lr = 0.0025
I0520 23:38:38.103451  1727 solver.cpp:237] Iteration 102, loss = 2.3321
I0520 23:38:38.103483  1727 solver.cpp:253]     Train net output #0: loss = 2.3321 (* 1 = 2.3321 loss)
I0520 23:38:38.103500  1727 sgd_solver.cpp:106] Iteration 102, lr = 0.0025
I0520 23:38:46.018081  1727 solver.cpp:237] Iteration 136, loss = 2.28549
I0520 23:38:46.018225  1727 solver.cpp:253]     Train net output #0: loss = 2.28549 (* 1 = 2.28549 loss)
I0520 23:38:46.018239  1727 sgd_solver.cpp:106] Iteration 136, lr = 0.0025
I0520 23:38:53.938699  1727 solver.cpp:237] Iteration 170, loss = 2.30955
I0520 23:38:53.938730  1727 solver.cpp:253]     Train net output #0: loss = 2.30955 (* 1 = 2.30955 loss)
I0520 23:38:53.938747  1727 sgd_solver.cpp:106] Iteration 170, lr = 0.0025
I0520 23:39:01.857130  1727 solver.cpp:237] Iteration 204, loss = 2.26729
I0520 23:39:01.857161  1727 solver.cpp:253]     Train net output #0: loss = 2.26729 (* 1 = 2.26729 loss)
I0520 23:39:01.857178  1727 sgd_solver.cpp:106] Iteration 204, lr = 0.0025
I0520 23:39:31.970777  1727 solver.cpp:237] Iteration 238, loss = 2.18576
I0520 23:39:31.970939  1727 solver.cpp:253]     Train net output #0: loss = 2.18576 (* 1 = 2.18576 loss)
I0520 23:39:31.970953  1727 sgd_solver.cpp:106] Iteration 238, lr = 0.0025
I0520 23:39:39.895015  1727 solver.cpp:237] Iteration 272, loss = 2.15122
I0520 23:39:39.895052  1727 solver.cpp:253]     Train net output #0: loss = 2.15122 (* 1 = 2.15122 loss)
I0520 23:39:39.895076  1727 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0520 23:39:47.816303  1727 solver.cpp:237] Iteration 306, loss = 2.13535
I0520 23:39:47.816336  1727 solver.cpp:253]     Train net output #0: loss = 2.13535 (* 1 = 2.13535 loss)
I0520 23:39:47.816354  1727 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0520 23:39:55.740110  1727 solver.cpp:237] Iteration 340, loss = 2.07839
I0520 23:39:55.740144  1727 solver.cpp:253]     Train net output #0: loss = 2.07839 (* 1 = 2.07839 loss)
I0520 23:39:55.740160  1727 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0520 23:39:57.373155  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_348.caffemodel
I0520 23:39:57.583667  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_348.solverstate
I0520 23:40:03.757714  1727 solver.cpp:237] Iteration 374, loss = 2.00237
I0520 23:40:03.757860  1727 solver.cpp:253]     Train net output #0: loss = 2.00237 (* 1 = 2.00237 loss)
I0520 23:40:03.757874  1727 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0520 23:40:11.675387  1727 solver.cpp:237] Iteration 408, loss = 1.95525
I0520 23:40:11.675420  1727 solver.cpp:253]     Train net output #0: loss = 1.95525 (* 1 = 1.95525 loss)
I0520 23:40:11.675438  1727 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0520 23:40:19.595685  1727 solver.cpp:237] Iteration 442, loss = 1.98417
I0520 23:40:19.595717  1727 solver.cpp:253]     Train net output #0: loss = 1.98417 (* 1 = 1.98417 loss)
I0520 23:40:19.595734  1727 sgd_solver.cpp:106] Iteration 442, lr = 0.0025
I0520 23:40:49.648658  1727 solver.cpp:237] Iteration 476, loss = 1.92201
I0520 23:40:49.648809  1727 solver.cpp:253]     Train net output #0: loss = 1.92201 (* 1 = 1.92201 loss)
I0520 23:40:49.648825  1727 sgd_solver.cpp:106] Iteration 476, lr = 0.0025
I0520 23:40:57.572523  1727 solver.cpp:237] Iteration 510, loss = 1.91951
I0520 23:40:57.572558  1727 solver.cpp:253]     Train net output #0: loss = 1.91951 (* 1 = 1.91951 loss)
I0520 23:40:57.572576  1727 sgd_solver.cpp:106] Iteration 510, lr = 0.0025
I0520 23:41:05.497462  1727 solver.cpp:237] Iteration 544, loss = 1.89343
I0520 23:41:05.497496  1727 solver.cpp:253]     Train net output #0: loss = 1.89343 (* 1 = 1.89343 loss)
I0520 23:41:05.497512  1727 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0520 23:41:13.421316  1727 solver.cpp:237] Iteration 578, loss = 1.90567
I0520 23:41:13.421350  1727 solver.cpp:253]     Train net output #0: loss = 1.90567 (* 1 = 1.90567 loss)
I0520 23:41:13.421367  1727 sgd_solver.cpp:106] Iteration 578, lr = 0.0025
I0520 23:41:21.344619  1727 solver.cpp:237] Iteration 612, loss = 1.84825
I0520 23:41:21.344774  1727 solver.cpp:253]     Train net output #0: loss = 1.84825 (* 1 = 1.84825 loss)
I0520 23:41:21.344787  1727 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0520 23:41:29.265862  1727 solver.cpp:237] Iteration 646, loss = 1.88438
I0520 23:41:29.265894  1727 solver.cpp:253]     Train net output #0: loss = 1.88438 (* 1 = 1.88438 loss)
I0520 23:41:29.265908  1727 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0520 23:41:37.190086  1727 solver.cpp:237] Iteration 680, loss = 1.8261
I0520 23:41:37.190119  1727 solver.cpp:253]     Train net output #0: loss = 1.8261 (* 1 = 1.8261 loss)
I0520 23:41:37.190135  1727 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0520 23:41:40.682827  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_696.caffemodel
I0520 23:41:41.363057  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_696.solverstate
I0520 23:41:41.486508  1727 solver.cpp:341] Iteration 697, Testing net (#0)
I0520 23:42:28.480155  1727 solver.cpp:409]     Test net output #0: accuracy = 0.599546
I0520 23:42:28.480315  1727 solver.cpp:409]     Test net output #1: loss = 1.42398 (* 1 = 1.42398 loss)
I0520 23:42:55.542215  1727 solver.cpp:237] Iteration 714, loss = 1.79613
I0520 23:42:55.542266  1727 solver.cpp:253]     Train net output #0: loss = 1.79613 (* 1 = 1.79613 loss)
I0520 23:42:55.542282  1727 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0520 23:43:03.462796  1727 solver.cpp:237] Iteration 748, loss = 1.85643
I0520 23:43:03.462965  1727 solver.cpp:253]     Train net output #0: loss = 1.85643 (* 1 = 1.85643 loss)
I0520 23:43:03.462980  1727 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0520 23:43:11.382889  1727 solver.cpp:237] Iteration 782, loss = 1.81626
I0520 23:43:11.382922  1727 solver.cpp:253]     Train net output #0: loss = 1.81626 (* 1 = 1.81626 loss)
I0520 23:43:11.382939  1727 sgd_solver.cpp:106] Iteration 782, lr = 0.0025
I0520 23:43:19.304553  1727 solver.cpp:237] Iteration 816, loss = 1.8504
I0520 23:43:19.304586  1727 solver.cpp:253]     Train net output #0: loss = 1.8504 (* 1 = 1.8504 loss)
I0520 23:43:19.304602  1727 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0520 23:43:27.227470  1727 solver.cpp:237] Iteration 850, loss = 1.7894
I0520 23:43:27.227519  1727 solver.cpp:253]     Train net output #0: loss = 1.7894 (* 1 = 1.7894 loss)
I0520 23:43:27.227532  1727 sgd_solver.cpp:106] Iteration 850, lr = 0.0025
I0520 23:43:35.148593  1727 solver.cpp:237] Iteration 884, loss = 1.8367
I0520 23:43:35.148730  1727 solver.cpp:253]     Train net output #0: loss = 1.8367 (* 1 = 1.8367 loss)
I0520 23:43:35.148744  1727 sgd_solver.cpp:106] Iteration 884, lr = 0.0025
I0520 23:43:43.072304  1727 solver.cpp:237] Iteration 918, loss = 1.79122
I0520 23:43:43.072335  1727 solver.cpp:253]     Train net output #0: loss = 1.79122 (* 1 = 1.79122 loss)
I0520 23:43:43.072352  1727 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0520 23:44:13.704152  1727 solver.cpp:237] Iteration 952, loss = 1.72309
I0520 23:44:13.704324  1727 solver.cpp:253]     Train net output #0: loss = 1.72309 (* 1 = 1.72309 loss)
I0520 23:44:13.704339  1727 sgd_solver.cpp:106] Iteration 952, lr = 0.0025
I0520 23:44:21.627638  1727 solver.cpp:237] Iteration 986, loss = 1.88565
I0520 23:44:21.627672  1727 solver.cpp:253]     Train net output #0: loss = 1.88565 (* 1 = 1.88565 loss)
I0520 23:44:21.627691  1727 sgd_solver.cpp:106] Iteration 986, lr = 0.0025
I0520 23:44:29.553047  1727 solver.cpp:237] Iteration 1020, loss = 1.9513
I0520 23:44:29.553081  1727 solver.cpp:253]     Train net output #0: loss = 1.9513 (* 1 = 1.9513 loss)
I0520 23:44:29.553097  1727 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0520 23:44:34.908877  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_1044.caffemodel
I0520 23:44:35.140527  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_1044.solverstate
I0520 23:44:37.600302  1727 solver.cpp:237] Iteration 1054, loss = 1.76075
I0520 23:44:37.600349  1727 solver.cpp:253]     Train net output #0: loss = 1.76075 (* 1 = 1.76075 loss)
I0520 23:44:37.600365  1727 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0520 23:44:45.519680  1727 solver.cpp:237] Iteration 1088, loss = 1.77863
I0520 23:44:45.519837  1727 solver.cpp:253]     Train net output #0: loss = 1.77863 (* 1 = 1.77863 loss)
I0520 23:44:45.519850  1727 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0520 23:44:53.436604  1727 solver.cpp:237] Iteration 1122, loss = 1.79138
I0520 23:44:53.436636  1727 solver.cpp:253]     Train net output #0: loss = 1.79138 (* 1 = 1.79138 loss)
I0520 23:44:53.436653  1727 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0520 23:45:01.356963  1727 solver.cpp:237] Iteration 1156, loss = 1.82312
I0520 23:45:01.356997  1727 solver.cpp:253]     Train net output #0: loss = 1.82312 (* 1 = 1.82312 loss)
I0520 23:45:01.357013  1727 sgd_solver.cpp:106] Iteration 1156, lr = 0.0025
I0520 23:45:31.444710  1727 solver.cpp:237] Iteration 1190, loss = 1.81339
I0520 23:45:31.444869  1727 solver.cpp:253]     Train net output #0: loss = 1.81339 (* 1 = 1.81339 loss)
I0520 23:45:31.444885  1727 sgd_solver.cpp:106] Iteration 1190, lr = 0.0025
I0520 23:45:39.365340  1727 solver.cpp:237] Iteration 1224, loss = 1.76775
I0520 23:45:39.365375  1727 solver.cpp:253]     Train net output #0: loss = 1.76775 (* 1 = 1.76775 loss)
I0520 23:45:39.365391  1727 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0520 23:45:47.285300  1727 solver.cpp:237] Iteration 1258, loss = 1.76275
I0520 23:45:47.285327  1727 solver.cpp:253]     Train net output #0: loss = 1.76275 (* 1 = 1.76275 loss)
I0520 23:45:47.285342  1727 sgd_solver.cpp:106] Iteration 1258, lr = 0.0025
I0520 23:45:55.208971  1727 solver.cpp:237] Iteration 1292, loss = 1.72852
I0520 23:45:55.209014  1727 solver.cpp:253]     Train net output #0: loss = 1.72852 (* 1 = 1.72852 loss)
I0520 23:45:55.209030  1727 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0520 23:46:03.128677  1727 solver.cpp:237] Iteration 1326, loss = 1.81222
I0520 23:46:03.128813  1727 solver.cpp:253]     Train net output #0: loss = 1.81222 (* 1 = 1.81222 loss)
I0520 23:46:03.128828  1727 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0520 23:46:11.047592  1727 solver.cpp:237] Iteration 1360, loss = 1.69284
I0520 23:46:11.047624  1727 solver.cpp:253]     Train net output #0: loss = 1.69284 (* 1 = 1.69284 loss)
I0520 23:46:11.047642  1727 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0520 23:46:18.275910  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_1392.caffemodel
I0520 23:46:18.478759  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_1392.solverstate
I0520 23:46:18.860982  1727 solver.cpp:341] Iteration 1394, Testing net (#0)
I0520 23:47:26.218449  1727 solver.cpp:409]     Test net output #0: accuracy = 0.657378
I0520 23:47:26.218619  1727 solver.cpp:409]     Test net output #1: loss = 1.2074 (* 1 = 1.2074 loss)
I0520 23:47:26.288033  1727 solver.cpp:237] Iteration 1394, loss = 1.74548
I0520 23:47:26.288059  1727 solver.cpp:253]     Train net output #0: loss = 1.74548 (* 1 = 1.74548 loss)
I0520 23:47:26.288077  1727 sgd_solver.cpp:106] Iteration 1394, lr = 0.0025
I0520 23:47:57.078291  1727 solver.cpp:237] Iteration 1428, loss = 1.5934
I0520 23:47:57.078454  1727 solver.cpp:253]     Train net output #0: loss = 1.5934 (* 1 = 1.5934 loss)
I0520 23:47:57.078469  1727 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0520 23:48:05.000798  1727 solver.cpp:237] Iteration 1462, loss = 1.66584
I0520 23:48:05.000836  1727 solver.cpp:253]     Train net output #0: loss = 1.66584 (* 1 = 1.66584 loss)
I0520 23:48:05.000856  1727 sgd_solver.cpp:106] Iteration 1462, lr = 0.0025
I0520 23:48:12.929390  1727 solver.cpp:237] Iteration 1496, loss = 1.74465
I0520 23:48:12.929424  1727 solver.cpp:253]     Train net output #0: loss = 1.74465 (* 1 = 1.74465 loss)
I0520 23:48:12.929440  1727 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0520 23:48:20.855161  1727 solver.cpp:237] Iteration 1530, loss = 1.64987
I0520 23:48:20.855195  1727 solver.cpp:253]     Train net output #0: loss = 1.64987 (* 1 = 1.64987 loss)
I0520 23:48:20.855211  1727 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0520 23:48:28.782135  1727 solver.cpp:237] Iteration 1564, loss = 1.71627
I0520 23:48:28.782281  1727 solver.cpp:253]     Train net output #0: loss = 1.71627 (* 1 = 1.71627 loss)
I0520 23:48:28.782296  1727 sgd_solver.cpp:106] Iteration 1564, lr = 0.0025
I0520 23:48:36.706033  1727 solver.cpp:237] Iteration 1598, loss = 1.89187
I0520 23:48:36.706066  1727 solver.cpp:253]     Train net output #0: loss = 1.89187 (* 1 = 1.89187 loss)
I0520 23:48:36.706084  1727 sgd_solver.cpp:106] Iteration 1598, lr = 0.0025
I0520 23:49:06.715106  1727 solver.cpp:237] Iteration 1632, loss = 1.68157
I0520 23:49:06.715265  1727 solver.cpp:253]     Train net output #0: loss = 1.68157 (* 1 = 1.68157 loss)
I0520 23:49:06.715279  1727 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0520 23:49:14.642930  1727 solver.cpp:237] Iteration 1666, loss = 1.63648
I0520 23:49:14.642972  1727 solver.cpp:253]     Train net output #0: loss = 1.63648 (* 1 = 1.63648 loss)
I0520 23:49:14.642987  1727 sgd_solver.cpp:106] Iteration 1666, lr = 0.0025
I0520 23:49:22.573942  1727 solver.cpp:237] Iteration 1700, loss = 1.68566
I0520 23:49:22.573976  1727 solver.cpp:253]     Train net output #0: loss = 1.68566 (* 1 = 1.68566 loss)
I0520 23:49:22.573992  1727 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0520 23:49:30.501875  1727 solver.cpp:237] Iteration 1734, loss = 1.73364
I0520 23:49:30.501909  1727 solver.cpp:253]     Train net output #0: loss = 1.73364 (* 1 = 1.73364 loss)
I0520 23:49:30.501925  1727 sgd_solver.cpp:106] Iteration 1734, lr = 0.0025
I0520 23:49:31.664557  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_1740.caffemodel
I0520 23:49:31.867971  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_1740.solverstate
I0520 23:49:38.536612  1727 solver.cpp:237] Iteration 1768, loss = 1.64297
I0520 23:49:38.536772  1727 solver.cpp:253]     Train net output #0: loss = 1.64297 (* 1 = 1.64297 loss)
I0520 23:49:38.536787  1727 sgd_solver.cpp:106] Iteration 1768, lr = 0.0025
I0520 23:49:46.464156  1727 solver.cpp:237] Iteration 1802, loss = 1.62427
I0520 23:49:46.464184  1727 solver.cpp:253]     Train net output #0: loss = 1.62427 (* 1 = 1.62427 loss)
I0520 23:49:46.464206  1727 sgd_solver.cpp:106] Iteration 1802, lr = 0.0025
I0520 23:49:54.387176  1727 solver.cpp:237] Iteration 1836, loss = 1.68365
I0520 23:49:54.387209  1727 solver.cpp:253]     Train net output #0: loss = 1.68365 (* 1 = 1.68365 loss)
I0520 23:49:54.387225  1727 sgd_solver.cpp:106] Iteration 1836, lr = 0.0025
I0520 23:50:24.493131  1727 solver.cpp:237] Iteration 1870, loss = 1.66473
I0520 23:50:24.493304  1727 solver.cpp:253]     Train net output #0: loss = 1.66473 (* 1 = 1.66473 loss)
I0520 23:50:24.493320  1727 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0520 23:50:32.417827  1727 solver.cpp:237] Iteration 1904, loss = 1.67551
I0520 23:50:32.417876  1727 solver.cpp:253]     Train net output #0: loss = 1.67551 (* 1 = 1.67551 loss)
I0520 23:50:32.417892  1727 sgd_solver.cpp:106] Iteration 1904, lr = 0.0025
I0520 23:50:40.345702  1727 solver.cpp:237] Iteration 1938, loss = 1.76539
I0520 23:50:40.345737  1727 solver.cpp:253]     Train net output #0: loss = 1.76539 (* 1 = 1.76539 loss)
I0520 23:50:40.345753  1727 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0520 23:50:48.272761  1727 solver.cpp:237] Iteration 1972, loss = 1.66439
I0520 23:50:48.272794  1727 solver.cpp:253]     Train net output #0: loss = 1.66439 (* 1 = 1.66439 loss)
I0520 23:50:48.272811  1727 sgd_solver.cpp:106] Iteration 1972, lr = 0.0025
I0520 23:50:56.203344  1727 solver.cpp:237] Iteration 2006, loss = 1.6274
I0520 23:50:56.203500  1727 solver.cpp:253]     Train net output #0: loss = 1.6274 (* 1 = 1.6274 loss)
I0520 23:50:56.203515  1727 sgd_solver.cpp:106] Iteration 2006, lr = 0.0025
I0520 23:51:04.127653  1727 solver.cpp:237] Iteration 2040, loss = 1.56489
I0520 23:51:04.127686  1727 solver.cpp:253]     Train net output #0: loss = 1.56489 (* 1 = 1.56489 loss)
I0520 23:51:04.127703  1727 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0520 23:51:12.051998  1727 solver.cpp:237] Iteration 2074, loss = 1.61145
I0520 23:51:12.052032  1727 solver.cpp:253]     Train net output #0: loss = 1.61145 (* 1 = 1.61145 loss)
I0520 23:51:12.052048  1727 sgd_solver.cpp:106] Iteration 2074, lr = 0.0025
I0520 23:51:15.081466  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_2088.caffemodel
I0520 23:51:17.114830  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_2088.solverstate
I0520 23:51:17.747985  1727 solver.cpp:341] Iteration 2091, Testing net (#0)
I0520 23:52:02.758044  1727 solver.cpp:409]     Test net output #0: accuracy = 0.676203
I0520 23:52:02.758206  1727 solver.cpp:409]     Test net output #1: loss = 1.09844 (* 1 = 1.09844 loss)
I0520 23:52:29.136947  1727 solver.cpp:237] Iteration 2108, loss = 1.66611
I0520 23:52:29.137001  1727 solver.cpp:253]     Train net output #0: loss = 1.66611 (* 1 = 1.66611 loss)
I0520 23:52:29.137017  1727 sgd_solver.cpp:106] Iteration 2108, lr = 0.0025
I0520 23:52:37.067244  1727 solver.cpp:237] Iteration 2142, loss = 1.62471
I0520 23:52:37.067405  1727 solver.cpp:253]     Train net output #0: loss = 1.62471 (* 1 = 1.62471 loss)
I0520 23:52:37.067420  1727 sgd_solver.cpp:106] Iteration 2142, lr = 0.0025
I0520 23:52:44.991117  1727 solver.cpp:237] Iteration 2176, loss = 1.70522
I0520 23:52:44.991148  1727 solver.cpp:253]     Train net output #0: loss = 1.70522 (* 1 = 1.70522 loss)
I0520 23:52:44.991165  1727 sgd_solver.cpp:106] Iteration 2176, lr = 0.0025
I0520 23:52:52.920928  1727 solver.cpp:237] Iteration 2210, loss = 1.6376
I0520 23:52:52.920969  1727 solver.cpp:253]     Train net output #0: loss = 1.6376 (* 1 = 1.6376 loss)
I0520 23:52:52.920981  1727 sgd_solver.cpp:106] Iteration 2210, lr = 0.0025
I0520 23:53:00.852574  1727 solver.cpp:237] Iteration 2244, loss = 1.55136
I0520 23:53:00.852620  1727 solver.cpp:253]     Train net output #0: loss = 1.55136 (* 1 = 1.55136 loss)
I0520 23:53:00.852634  1727 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0520 23:53:08.778375  1727 solver.cpp:237] Iteration 2278, loss = 1.63402
I0520 23:53:08.778528  1727 solver.cpp:253]     Train net output #0: loss = 1.63402 (* 1 = 1.63402 loss)
I0520 23:53:08.778544  1727 sgd_solver.cpp:106] Iteration 2278, lr = 0.0025
I0520 23:53:16.707304  1727 solver.cpp:237] Iteration 2312, loss = 1.67973
I0520 23:53:16.707336  1727 solver.cpp:253]     Train net output #0: loss = 1.67973 (* 1 = 1.67973 loss)
I0520 23:53:16.707355  1727 sgd_solver.cpp:106] Iteration 2312, lr = 0.0025
I0520 23:53:47.923214  1727 solver.cpp:237] Iteration 2346, loss = 1.63824
I0520 23:53:47.923380  1727 solver.cpp:253]     Train net output #0: loss = 1.63824 (* 1 = 1.63824 loss)
I0520 23:53:47.923396  1727 sgd_solver.cpp:106] Iteration 2346, lr = 0.0025
I0520 23:53:55.848014  1727 solver.cpp:237] Iteration 2380, loss = 1.64503
I0520 23:53:55.848052  1727 solver.cpp:253]     Train net output #0: loss = 1.64503 (* 1 = 1.64503 loss)
I0520 23:53:55.848073  1727 sgd_solver.cpp:106] Iteration 2380, lr = 0.0025
I0520 23:54:03.778439  1727 solver.cpp:237] Iteration 2414, loss = 1.63574
I0520 23:54:03.778472  1727 solver.cpp:253]     Train net output #0: loss = 1.63574 (* 1 = 1.63574 loss)
I0520 23:54:03.778488  1727 sgd_solver.cpp:106] Iteration 2414, lr = 0.0025
I0520 23:54:08.676437  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_2436.caffemodel
I0520 23:54:08.919126  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_2436.solverstate
I0520 23:54:11.821758  1727 solver.cpp:237] Iteration 2448, loss = 1.64911
I0520 23:54:11.821805  1727 solver.cpp:253]     Train net output #0: loss = 1.64911 (* 1 = 1.64911 loss)
I0520 23:54:11.821820  1727 sgd_solver.cpp:106] Iteration 2448, lr = 0.0025
I0520 23:54:19.746568  1727 solver.cpp:237] Iteration 2482, loss = 1.61273
I0520 23:54:19.746738  1727 solver.cpp:253]     Train net output #0: loss = 1.61273 (* 1 = 1.61273 loss)
I0520 23:54:19.746752  1727 sgd_solver.cpp:106] Iteration 2482, lr = 0.0025
I0520 23:54:27.667986  1727 solver.cpp:237] Iteration 2516, loss = 1.59777
I0520 23:54:27.668020  1727 solver.cpp:253]     Train net output #0: loss = 1.59777 (* 1 = 1.59777 loss)
I0520 23:54:27.668036  1727 sgd_solver.cpp:106] Iteration 2516, lr = 0.0025
I0520 23:54:35.597712  1727 solver.cpp:237] Iteration 2550, loss = 1.5703
I0520 23:54:35.597748  1727 solver.cpp:253]     Train net output #0: loss = 1.5703 (* 1 = 1.5703 loss)
I0520 23:54:35.597764  1727 sgd_solver.cpp:106] Iteration 2550, lr = 0.0025
I0520 23:55:06.511718  1727 solver.cpp:237] Iteration 2584, loss = 1.66018
I0520 23:55:06.511879  1727 solver.cpp:253]     Train net output #0: loss = 1.66018 (* 1 = 1.66018 loss)
I0520 23:55:06.511895  1727 sgd_solver.cpp:106] Iteration 2584, lr = 0.0025
I0520 23:55:14.436336  1727 solver.cpp:237] Iteration 2618, loss = 1.68906
I0520 23:55:14.436370  1727 solver.cpp:253]     Train net output #0: loss = 1.68906 (* 1 = 1.68906 loss)
I0520 23:55:14.436388  1727 sgd_solver.cpp:106] Iteration 2618, lr = 0.0025
I0520 23:55:22.358927  1727 solver.cpp:237] Iteration 2652, loss = 1.60819
I0520 23:55:22.358959  1727 solver.cpp:253]     Train net output #0: loss = 1.60819 (* 1 = 1.60819 loss)
I0520 23:55:22.358975  1727 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0520 23:55:30.287631  1727 solver.cpp:237] Iteration 2686, loss = 1.63631
I0520 23:55:30.287679  1727 solver.cpp:253]     Train net output #0: loss = 1.63631 (* 1 = 1.63631 loss)
I0520 23:55:30.287693  1727 sgd_solver.cpp:106] Iteration 2686, lr = 0.0025
I0520 23:55:38.214504  1727 solver.cpp:237] Iteration 2720, loss = 1.61096
I0520 23:55:38.214643  1727 solver.cpp:253]     Train net output #0: loss = 1.61096 (* 1 = 1.61096 loss)
I0520 23:55:38.214658  1727 sgd_solver.cpp:106] Iteration 2720, lr = 0.0025
I0520 23:55:46.143753  1727 solver.cpp:237] Iteration 2754, loss = 1.6486
I0520 23:55:46.143784  1727 solver.cpp:253]     Train net output #0: loss = 1.6486 (* 1 = 1.6486 loss)
I0520 23:55:46.143800  1727 sgd_solver.cpp:106] Iteration 2754, lr = 0.0025
I0520 23:55:52.907197  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_2784.caffemodel
I0520 23:55:53.273990  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_2784.solverstate
I0520 23:55:54.111215  1727 solver.cpp:341] Iteration 2788, Testing net (#0)
I0520 23:57:00.996031  1727 solver.cpp:409]     Test net output #0: accuracy = 0.695048
I0520 23:57:00.996203  1727 solver.cpp:409]     Test net output #1: loss = 1.03634 (* 1 = 1.03634 loss)
I0520 23:57:01.065534  1727 solver.cpp:237] Iteration 2788, loss = 1.60662
I0520 23:57:01.065562  1727 solver.cpp:253]     Train net output #0: loss = 1.60662 (* 1 = 1.60662 loss)
I0520 23:57:01.065580  1727 sgd_solver.cpp:106] Iteration 2788, lr = 0.0025
I0520 23:57:31.472002  1727 solver.cpp:237] Iteration 2822, loss = 1.56265
I0520 23:57:31.472168  1727 solver.cpp:253]     Train net output #0: loss = 1.56265 (* 1 = 1.56265 loss)
I0520 23:57:31.472183  1727 sgd_solver.cpp:106] Iteration 2822, lr = 0.0025
I0520 23:57:39.392833  1727 solver.cpp:237] Iteration 2856, loss = 1.61871
I0520 23:57:39.392873  1727 solver.cpp:253]     Train net output #0: loss = 1.61871 (* 1 = 1.61871 loss)
I0520 23:57:39.392892  1727 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0520 23:57:47.312928  1727 solver.cpp:237] Iteration 2890, loss = 1.54403
I0520 23:57:47.312968  1727 solver.cpp:253]     Train net output #0: loss = 1.54403 (* 1 = 1.54403 loss)
I0520 23:57:47.312981  1727 sgd_solver.cpp:106] Iteration 2890, lr = 0.0025
I0520 23:57:55.235046  1727 solver.cpp:237] Iteration 2924, loss = 1.5899
I0520 23:57:55.235080  1727 solver.cpp:253]     Train net output #0: loss = 1.5899 (* 1 = 1.5899 loss)
I0520 23:57:55.235095  1727 sgd_solver.cpp:106] Iteration 2924, lr = 0.0025
I0520 23:58:03.157970  1727 solver.cpp:237] Iteration 2958, loss = 1.69106
I0520 23:58:03.158128  1727 solver.cpp:253]     Train net output #0: loss = 1.69106 (* 1 = 1.69106 loss)
I0520 23:58:03.158143  1727 sgd_solver.cpp:106] Iteration 2958, lr = 0.0025
I0520 23:58:11.082083  1727 solver.cpp:237] Iteration 2992, loss = 1.57664
I0520 23:58:11.082116  1727 solver.cpp:253]     Train net output #0: loss = 1.57664 (* 1 = 1.57664 loss)
I0520 23:58:11.082134  1727 sgd_solver.cpp:106] Iteration 2992, lr = 0.0025
I0520 23:58:41.213492  1727 solver.cpp:237] Iteration 3026, loss = 1.60964
I0520 23:58:41.213659  1727 solver.cpp:253]     Train net output #0: loss = 1.60964 (* 1 = 1.60964 loss)
I0520 23:58:41.213673  1727 sgd_solver.cpp:106] Iteration 3026, lr = 0.0025
I0520 23:58:49.132807  1727 solver.cpp:237] Iteration 3060, loss = 1.54299
I0520 23:58:49.132838  1727 solver.cpp:253]     Train net output #0: loss = 1.54299 (* 1 = 1.54299 loss)
I0520 23:58:49.132864  1727 sgd_solver.cpp:106] Iteration 3060, lr = 0.0025
I0520 23:58:57.047768  1727 solver.cpp:237] Iteration 3094, loss = 1.61117
I0520 23:58:57.047801  1727 solver.cpp:253]     Train net output #0: loss = 1.61117 (* 1 = 1.61117 loss)
I0520 23:58:57.047817  1727 sgd_solver.cpp:106] Iteration 3094, lr = 0.0025
I0520 23:59:04.965978  1727 solver.cpp:237] Iteration 3128, loss = 1.61796
I0520 23:59:04.966012  1727 solver.cpp:253]     Train net output #0: loss = 1.61796 (* 1 = 1.61796 loss)
I0520 23:59:04.966028  1727 sgd_solver.cpp:106] Iteration 3128, lr = 0.0025
I0520 23:59:05.664517  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_3132.caffemodel
I0520 23:59:05.880254  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_3132.solverstate
I0520 23:59:12.979732  1727 solver.cpp:237] Iteration 3162, loss = 1.59365
I0520 23:59:12.979904  1727 solver.cpp:253]     Train net output #0: loss = 1.59365 (* 1 = 1.59365 loss)
I0520 23:59:12.979918  1727 sgd_solver.cpp:106] Iteration 3162, lr = 0.0025
I0520 23:59:20.900818  1727 solver.cpp:237] Iteration 3196, loss = 1.62577
I0520 23:59:20.900848  1727 solver.cpp:253]     Train net output #0: loss = 1.62577 (* 1 = 1.62577 loss)
I0520 23:59:20.900871  1727 sgd_solver.cpp:106] Iteration 3196, lr = 0.0025
I0520 23:59:28.824590  1727 solver.cpp:237] Iteration 3230, loss = 1.61306
I0520 23:59:28.824623  1727 solver.cpp:253]     Train net output #0: loss = 1.61306 (* 1 = 1.61306 loss)
I0520 23:59:28.824640  1727 sgd_solver.cpp:106] Iteration 3230, lr = 0.0025
I0520 23:59:58.942880  1727 solver.cpp:237] Iteration 3264, loss = 1.62413
I0520 23:59:58.943044  1727 solver.cpp:253]     Train net output #0: loss = 1.62413 (* 1 = 1.62413 loss)
I0520 23:59:58.943059  1727 sgd_solver.cpp:106] Iteration 3264, lr = 0.0025
I0521 00:00:06.855868  1727 solver.cpp:237] Iteration 3298, loss = 1.60566
I0521 00:00:06.855907  1727 solver.cpp:253]     Train net output #0: loss = 1.60566 (* 1 = 1.60566 loss)
I0521 00:00:06.855926  1727 sgd_solver.cpp:106] Iteration 3298, lr = 0.0025
I0521 00:00:14.779274  1727 solver.cpp:237] Iteration 3332, loss = 1.49875
I0521 00:00:14.779309  1727 solver.cpp:253]     Train net output #0: loss = 1.49875 (* 1 = 1.49875 loss)
I0521 00:00:14.779325  1727 sgd_solver.cpp:106] Iteration 3332, lr = 0.0025
I0521 00:00:22.696741  1727 solver.cpp:237] Iteration 3366, loss = 1.57339
I0521 00:00:22.696775  1727 solver.cpp:253]     Train net output #0: loss = 1.57339 (* 1 = 1.57339 loss)
I0521 00:00:22.696791  1727 sgd_solver.cpp:106] Iteration 3366, lr = 0.0025
I0521 00:00:30.619765  1727 solver.cpp:237] Iteration 3400, loss = 1.53063
I0521 00:00:30.619918  1727 solver.cpp:253]     Train net output #0: loss = 1.53063 (* 1 = 1.53063 loss)
I0521 00:00:30.619932  1727 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0521 00:00:38.538537  1727 solver.cpp:237] Iteration 3434, loss = 1.52393
I0521 00:00:38.538568  1727 solver.cpp:253]     Train net output #0: loss = 1.52393 (* 1 = 1.52393 loss)
I0521 00:00:38.538586  1727 sgd_solver.cpp:106] Iteration 3434, lr = 0.0025
I0521 00:00:46.455986  1727 solver.cpp:237] Iteration 3468, loss = 1.6136
I0521 00:00:46.456020  1727 solver.cpp:253]     Train net output #0: loss = 1.6136 (* 1 = 1.6136 loss)
I0521 00:00:46.456037  1727 sgd_solver.cpp:106] Iteration 3468, lr = 0.0025
I0521 00:00:49.015743  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_3480.caffemodel
I0521 00:00:49.220209  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_3480.solverstate
I0521 00:00:50.250296  1727 solver.cpp:341] Iteration 3485, Testing net (#0)
I0521 00:01:35.584529  1727 solver.cpp:409]     Test net output #0: accuracy = 0.71402
I0521 00:01:35.584692  1727 solver.cpp:409]     Test net output #1: loss = 1.01355 (* 1 = 1.01355 loss)
I0521 00:01:36.122073  1727 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_3488.caffemodel
I0521 00:01:36.325039  1727 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973_iter_3488.solverstate
I0521 00:01:36.353139  1727 solver.cpp:326] Optimization Done.
I0521 00:01:36.353166  1727 caffe.cpp:215] Optimization Done.
Application 11235991 resources: utime ~1254s, stime ~229s, Rss ~5329616, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_430_2016-05-20T11.20.48.294973.solver"
	User time (seconds): 0.52
	System time (seconds): 0.18
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:01.66
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 2729
	Involuntary context switches: 72
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
