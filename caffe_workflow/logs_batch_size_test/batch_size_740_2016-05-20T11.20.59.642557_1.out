2806313
I0521 06:10:29.094810  9072 caffe.cpp:184] Using GPUs 0
I0521 06:10:29.523418  9072 solver.cpp:48] Initializing solver from parameters: 
test_iter: 202
test_interval: 405
base_lr: 0.0025
display: 20
max_iter: 2027
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 202
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557.prototxt"
I0521 06:10:29.525030  9072 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557.prototxt
I0521 06:10:29.541805  9072 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 06:10:29.541865  9072 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 06:10:29.542210  9072 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 740
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 06:10:29.542388  9072 layer_factory.hpp:77] Creating layer data_hdf5
I0521 06:10:29.542412  9072 net.cpp:106] Creating Layer data_hdf5
I0521 06:10:29.542428  9072 net.cpp:411] data_hdf5 -> data
I0521 06:10:29.542460  9072 net.cpp:411] data_hdf5 -> label
I0521 06:10:29.542492  9072 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 06:10:29.543718  9072 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 06:10:29.545900  9072 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 06:10:51.124228  9072 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 06:10:51.129393  9072 net.cpp:150] Setting up data_hdf5
I0521 06:10:51.129434  9072 net.cpp:157] Top shape: 740 1 127 50 (4699000)
I0521 06:10:51.129448  9072 net.cpp:157] Top shape: 740 (740)
I0521 06:10:51.129458  9072 net.cpp:165] Memory required for data: 18798960
I0521 06:10:51.129472  9072 layer_factory.hpp:77] Creating layer conv1
I0521 06:10:51.129508  9072 net.cpp:106] Creating Layer conv1
I0521 06:10:51.129518  9072 net.cpp:454] conv1 <- data
I0521 06:10:51.129541  9072 net.cpp:411] conv1 -> conv1
I0521 06:10:51.960253  9072 net.cpp:150] Setting up conv1
I0521 06:10:51.960299  9072 net.cpp:157] Top shape: 740 12 120 48 (51148800)
I0521 06:10:51.960310  9072 net.cpp:165] Memory required for data: 223394160
I0521 06:10:51.960340  9072 layer_factory.hpp:77] Creating layer relu1
I0521 06:10:51.960361  9072 net.cpp:106] Creating Layer relu1
I0521 06:10:51.960371  9072 net.cpp:454] relu1 <- conv1
I0521 06:10:51.960386  9072 net.cpp:397] relu1 -> conv1 (in-place)
I0521 06:10:51.960901  9072 net.cpp:150] Setting up relu1
I0521 06:10:51.960918  9072 net.cpp:157] Top shape: 740 12 120 48 (51148800)
I0521 06:10:51.960929  9072 net.cpp:165] Memory required for data: 427989360
I0521 06:10:51.960940  9072 layer_factory.hpp:77] Creating layer pool1
I0521 06:10:51.960958  9072 net.cpp:106] Creating Layer pool1
I0521 06:10:51.960968  9072 net.cpp:454] pool1 <- conv1
I0521 06:10:51.960983  9072 net.cpp:411] pool1 -> pool1
I0521 06:10:51.961061  9072 net.cpp:150] Setting up pool1
I0521 06:10:51.961076  9072 net.cpp:157] Top shape: 740 12 60 48 (25574400)
I0521 06:10:51.961086  9072 net.cpp:165] Memory required for data: 530286960
I0521 06:10:51.961096  9072 layer_factory.hpp:77] Creating layer conv2
I0521 06:10:51.961118  9072 net.cpp:106] Creating Layer conv2
I0521 06:10:51.961128  9072 net.cpp:454] conv2 <- pool1
I0521 06:10:51.961141  9072 net.cpp:411] conv2 -> conv2
I0521 06:10:51.963819  9072 net.cpp:150] Setting up conv2
I0521 06:10:51.963845  9072 net.cpp:157] Top shape: 740 20 54 46 (36763200)
I0521 06:10:51.963856  9072 net.cpp:165] Memory required for data: 677339760
I0521 06:10:51.963876  9072 layer_factory.hpp:77] Creating layer relu2
I0521 06:10:51.963891  9072 net.cpp:106] Creating Layer relu2
I0521 06:10:51.963901  9072 net.cpp:454] relu2 <- conv2
I0521 06:10:51.963912  9072 net.cpp:397] relu2 -> conv2 (in-place)
I0521 06:10:51.964244  9072 net.cpp:150] Setting up relu2
I0521 06:10:51.964259  9072 net.cpp:157] Top shape: 740 20 54 46 (36763200)
I0521 06:10:51.964269  9072 net.cpp:165] Memory required for data: 824392560
I0521 06:10:51.964280  9072 layer_factory.hpp:77] Creating layer pool2
I0521 06:10:51.964293  9072 net.cpp:106] Creating Layer pool2
I0521 06:10:51.964303  9072 net.cpp:454] pool2 <- conv2
I0521 06:10:51.964329  9072 net.cpp:411] pool2 -> pool2
I0521 06:10:51.964398  9072 net.cpp:150] Setting up pool2
I0521 06:10:51.964411  9072 net.cpp:157] Top shape: 740 20 27 46 (18381600)
I0521 06:10:51.964421  9072 net.cpp:165] Memory required for data: 897918960
I0521 06:10:51.964432  9072 layer_factory.hpp:77] Creating layer conv3
I0521 06:10:51.964448  9072 net.cpp:106] Creating Layer conv3
I0521 06:10:51.964459  9072 net.cpp:454] conv3 <- pool2
I0521 06:10:51.964473  9072 net.cpp:411] conv3 -> conv3
I0521 06:10:51.966393  9072 net.cpp:150] Setting up conv3
I0521 06:10:51.966416  9072 net.cpp:157] Top shape: 740 28 22 44 (20056960)
I0521 06:10:51.966428  9072 net.cpp:165] Memory required for data: 978146800
I0521 06:10:51.966446  9072 layer_factory.hpp:77] Creating layer relu3
I0521 06:10:51.966462  9072 net.cpp:106] Creating Layer relu3
I0521 06:10:51.966472  9072 net.cpp:454] relu3 <- conv3
I0521 06:10:51.966485  9072 net.cpp:397] relu3 -> conv3 (in-place)
I0521 06:10:51.966956  9072 net.cpp:150] Setting up relu3
I0521 06:10:51.966974  9072 net.cpp:157] Top shape: 740 28 22 44 (20056960)
I0521 06:10:51.966984  9072 net.cpp:165] Memory required for data: 1058374640
I0521 06:10:51.966994  9072 layer_factory.hpp:77] Creating layer pool3
I0521 06:10:51.967007  9072 net.cpp:106] Creating Layer pool3
I0521 06:10:51.967016  9072 net.cpp:454] pool3 <- conv3
I0521 06:10:51.967030  9072 net.cpp:411] pool3 -> pool3
I0521 06:10:51.967097  9072 net.cpp:150] Setting up pool3
I0521 06:10:51.967109  9072 net.cpp:157] Top shape: 740 28 11 44 (10028480)
I0521 06:10:51.967119  9072 net.cpp:165] Memory required for data: 1098488560
I0521 06:10:51.967129  9072 layer_factory.hpp:77] Creating layer conv4
I0521 06:10:51.967146  9072 net.cpp:106] Creating Layer conv4
I0521 06:10:51.967157  9072 net.cpp:454] conv4 <- pool3
I0521 06:10:51.967171  9072 net.cpp:411] conv4 -> conv4
I0521 06:10:51.969975  9072 net.cpp:150] Setting up conv4
I0521 06:10:51.970002  9072 net.cpp:157] Top shape: 740 36 6 42 (6713280)
I0521 06:10:51.970015  9072 net.cpp:165] Memory required for data: 1125341680
I0521 06:10:51.970029  9072 layer_factory.hpp:77] Creating layer relu4
I0521 06:10:51.970043  9072 net.cpp:106] Creating Layer relu4
I0521 06:10:51.970053  9072 net.cpp:454] relu4 <- conv4
I0521 06:10:51.970067  9072 net.cpp:397] relu4 -> conv4 (in-place)
I0521 06:10:51.970537  9072 net.cpp:150] Setting up relu4
I0521 06:10:51.970553  9072 net.cpp:157] Top shape: 740 36 6 42 (6713280)
I0521 06:10:51.970563  9072 net.cpp:165] Memory required for data: 1152194800
I0521 06:10:51.970574  9072 layer_factory.hpp:77] Creating layer pool4
I0521 06:10:51.970587  9072 net.cpp:106] Creating Layer pool4
I0521 06:10:51.970597  9072 net.cpp:454] pool4 <- conv4
I0521 06:10:51.970609  9072 net.cpp:411] pool4 -> pool4
I0521 06:10:51.970677  9072 net.cpp:150] Setting up pool4
I0521 06:10:51.970690  9072 net.cpp:157] Top shape: 740 36 3 42 (3356640)
I0521 06:10:51.970701  9072 net.cpp:165] Memory required for data: 1165621360
I0521 06:10:51.970711  9072 layer_factory.hpp:77] Creating layer ip1
I0521 06:10:51.970731  9072 net.cpp:106] Creating Layer ip1
I0521 06:10:51.970742  9072 net.cpp:454] ip1 <- pool4
I0521 06:10:51.970754  9072 net.cpp:411] ip1 -> ip1
I0521 06:10:51.986248  9072 net.cpp:150] Setting up ip1
I0521 06:10:51.986277  9072 net.cpp:157] Top shape: 740 196 (145040)
I0521 06:10:51.986290  9072 net.cpp:165] Memory required for data: 1166201520
I0521 06:10:51.986313  9072 layer_factory.hpp:77] Creating layer relu5
I0521 06:10:51.986327  9072 net.cpp:106] Creating Layer relu5
I0521 06:10:51.986338  9072 net.cpp:454] relu5 <- ip1
I0521 06:10:51.986351  9072 net.cpp:397] relu5 -> ip1 (in-place)
I0521 06:10:51.986695  9072 net.cpp:150] Setting up relu5
I0521 06:10:51.986709  9072 net.cpp:157] Top shape: 740 196 (145040)
I0521 06:10:51.986719  9072 net.cpp:165] Memory required for data: 1166781680
I0521 06:10:51.986729  9072 layer_factory.hpp:77] Creating layer drop1
I0521 06:10:51.986752  9072 net.cpp:106] Creating Layer drop1
I0521 06:10:51.986763  9072 net.cpp:454] drop1 <- ip1
I0521 06:10:51.986788  9072 net.cpp:397] drop1 -> ip1 (in-place)
I0521 06:10:51.986835  9072 net.cpp:150] Setting up drop1
I0521 06:10:51.986847  9072 net.cpp:157] Top shape: 740 196 (145040)
I0521 06:10:51.986857  9072 net.cpp:165] Memory required for data: 1167361840
I0521 06:10:51.986867  9072 layer_factory.hpp:77] Creating layer ip2
I0521 06:10:51.986886  9072 net.cpp:106] Creating Layer ip2
I0521 06:10:51.986896  9072 net.cpp:454] ip2 <- ip1
I0521 06:10:51.986909  9072 net.cpp:411] ip2 -> ip2
I0521 06:10:51.987373  9072 net.cpp:150] Setting up ip2
I0521 06:10:51.987386  9072 net.cpp:157] Top shape: 740 98 (72520)
I0521 06:10:51.987396  9072 net.cpp:165] Memory required for data: 1167651920
I0521 06:10:51.987411  9072 layer_factory.hpp:77] Creating layer relu6
I0521 06:10:51.987423  9072 net.cpp:106] Creating Layer relu6
I0521 06:10:51.987433  9072 net.cpp:454] relu6 <- ip2
I0521 06:10:51.987445  9072 net.cpp:397] relu6 -> ip2 (in-place)
I0521 06:10:51.987972  9072 net.cpp:150] Setting up relu6
I0521 06:10:51.987987  9072 net.cpp:157] Top shape: 740 98 (72520)
I0521 06:10:51.987998  9072 net.cpp:165] Memory required for data: 1167942000
I0521 06:10:51.988008  9072 layer_factory.hpp:77] Creating layer drop2
I0521 06:10:51.988021  9072 net.cpp:106] Creating Layer drop2
I0521 06:10:51.988031  9072 net.cpp:454] drop2 <- ip2
I0521 06:10:51.988044  9072 net.cpp:397] drop2 -> ip2 (in-place)
I0521 06:10:51.988085  9072 net.cpp:150] Setting up drop2
I0521 06:10:51.988098  9072 net.cpp:157] Top shape: 740 98 (72520)
I0521 06:10:51.988109  9072 net.cpp:165] Memory required for data: 1168232080
I0521 06:10:51.988119  9072 layer_factory.hpp:77] Creating layer ip3
I0521 06:10:51.988132  9072 net.cpp:106] Creating Layer ip3
I0521 06:10:51.988142  9072 net.cpp:454] ip3 <- ip2
I0521 06:10:51.988155  9072 net.cpp:411] ip3 -> ip3
I0521 06:10:51.988364  9072 net.cpp:150] Setting up ip3
I0521 06:10:51.988379  9072 net.cpp:157] Top shape: 740 11 (8140)
I0521 06:10:51.988387  9072 net.cpp:165] Memory required for data: 1168264640
I0521 06:10:51.988402  9072 layer_factory.hpp:77] Creating layer drop3
I0521 06:10:51.988415  9072 net.cpp:106] Creating Layer drop3
I0521 06:10:51.988425  9072 net.cpp:454] drop3 <- ip3
I0521 06:10:51.988436  9072 net.cpp:397] drop3 -> ip3 (in-place)
I0521 06:10:51.988476  9072 net.cpp:150] Setting up drop3
I0521 06:10:51.988488  9072 net.cpp:157] Top shape: 740 11 (8140)
I0521 06:10:51.988499  9072 net.cpp:165] Memory required for data: 1168297200
I0521 06:10:51.988509  9072 layer_factory.hpp:77] Creating layer loss
I0521 06:10:51.988529  9072 net.cpp:106] Creating Layer loss
I0521 06:10:51.988539  9072 net.cpp:454] loss <- ip3
I0521 06:10:51.988550  9072 net.cpp:454] loss <- label
I0521 06:10:51.988562  9072 net.cpp:411] loss -> loss
I0521 06:10:51.988579  9072 layer_factory.hpp:77] Creating layer loss
I0521 06:10:51.989235  9072 net.cpp:150] Setting up loss
I0521 06:10:51.989256  9072 net.cpp:157] Top shape: (1)
I0521 06:10:51.989266  9072 net.cpp:160]     with loss weight 1
I0521 06:10:51.989310  9072 net.cpp:165] Memory required for data: 1168297204
I0521 06:10:51.989321  9072 net.cpp:226] loss needs backward computation.
I0521 06:10:51.989332  9072 net.cpp:226] drop3 needs backward computation.
I0521 06:10:51.989343  9072 net.cpp:226] ip3 needs backward computation.
I0521 06:10:51.989353  9072 net.cpp:226] drop2 needs backward computation.
I0521 06:10:51.989363  9072 net.cpp:226] relu6 needs backward computation.
I0521 06:10:51.989372  9072 net.cpp:226] ip2 needs backward computation.
I0521 06:10:51.989382  9072 net.cpp:226] drop1 needs backward computation.
I0521 06:10:51.989392  9072 net.cpp:226] relu5 needs backward computation.
I0521 06:10:51.989401  9072 net.cpp:226] ip1 needs backward computation.
I0521 06:10:51.989411  9072 net.cpp:226] pool4 needs backward computation.
I0521 06:10:51.989423  9072 net.cpp:226] relu4 needs backward computation.
I0521 06:10:51.989433  9072 net.cpp:226] conv4 needs backward computation.
I0521 06:10:51.989444  9072 net.cpp:226] pool3 needs backward computation.
I0521 06:10:51.989462  9072 net.cpp:226] relu3 needs backward computation.
I0521 06:10:51.989472  9072 net.cpp:226] conv3 needs backward computation.
I0521 06:10:51.989483  9072 net.cpp:226] pool2 needs backward computation.
I0521 06:10:51.989493  9072 net.cpp:226] relu2 needs backward computation.
I0521 06:10:51.989503  9072 net.cpp:226] conv2 needs backward computation.
I0521 06:10:51.989513  9072 net.cpp:226] pool1 needs backward computation.
I0521 06:10:51.989526  9072 net.cpp:226] relu1 needs backward computation.
I0521 06:10:51.989534  9072 net.cpp:226] conv1 needs backward computation.
I0521 06:10:51.989545  9072 net.cpp:228] data_hdf5 does not need backward computation.
I0521 06:10:51.989555  9072 net.cpp:270] This network produces output loss
I0521 06:10:51.989579  9072 net.cpp:283] Network initialization done.
I0521 06:10:51.991168  9072 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557.prototxt
I0521 06:10:51.991240  9072 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 06:10:51.991595  9072 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 740
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 06:10:51.991793  9072 layer_factory.hpp:77] Creating layer data_hdf5
I0521 06:10:51.991808  9072 net.cpp:106] Creating Layer data_hdf5
I0521 06:10:51.991821  9072 net.cpp:411] data_hdf5 -> data
I0521 06:10:51.991837  9072 net.cpp:411] data_hdf5 -> label
I0521 06:10:51.991852  9072 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 06:10:51.993015  9072 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 06:11:13.449553  9072 net.cpp:150] Setting up data_hdf5
I0521 06:11:13.449718  9072 net.cpp:157] Top shape: 740 1 127 50 (4699000)
I0521 06:11:13.449733  9072 net.cpp:157] Top shape: 740 (740)
I0521 06:11:13.449744  9072 net.cpp:165] Memory required for data: 18798960
I0521 06:11:13.449759  9072 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 06:11:13.449787  9072 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 06:11:13.449797  9072 net.cpp:454] label_data_hdf5_1_split <- label
I0521 06:11:13.449812  9072 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 06:11:13.449834  9072 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 06:11:13.449906  9072 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 06:11:13.449920  9072 net.cpp:157] Top shape: 740 (740)
I0521 06:11:13.449933  9072 net.cpp:157] Top shape: 740 (740)
I0521 06:11:13.449941  9072 net.cpp:165] Memory required for data: 18804880
I0521 06:11:13.449951  9072 layer_factory.hpp:77] Creating layer conv1
I0521 06:11:13.449973  9072 net.cpp:106] Creating Layer conv1
I0521 06:11:13.449983  9072 net.cpp:454] conv1 <- data
I0521 06:11:13.449998  9072 net.cpp:411] conv1 -> conv1
I0521 06:11:13.451930  9072 net.cpp:150] Setting up conv1
I0521 06:11:13.451953  9072 net.cpp:157] Top shape: 740 12 120 48 (51148800)
I0521 06:11:13.451966  9072 net.cpp:165] Memory required for data: 223400080
I0521 06:11:13.451985  9072 layer_factory.hpp:77] Creating layer relu1
I0521 06:11:13.452000  9072 net.cpp:106] Creating Layer relu1
I0521 06:11:13.452010  9072 net.cpp:454] relu1 <- conv1
I0521 06:11:13.452023  9072 net.cpp:397] relu1 -> conv1 (in-place)
I0521 06:11:13.452525  9072 net.cpp:150] Setting up relu1
I0521 06:11:13.452541  9072 net.cpp:157] Top shape: 740 12 120 48 (51148800)
I0521 06:11:13.452553  9072 net.cpp:165] Memory required for data: 427995280
I0521 06:11:13.452563  9072 layer_factory.hpp:77] Creating layer pool1
I0521 06:11:13.452579  9072 net.cpp:106] Creating Layer pool1
I0521 06:11:13.452589  9072 net.cpp:454] pool1 <- conv1
I0521 06:11:13.452601  9072 net.cpp:411] pool1 -> pool1
I0521 06:11:13.452677  9072 net.cpp:150] Setting up pool1
I0521 06:11:13.452689  9072 net.cpp:157] Top shape: 740 12 60 48 (25574400)
I0521 06:11:13.452699  9072 net.cpp:165] Memory required for data: 530292880
I0521 06:11:13.452709  9072 layer_factory.hpp:77] Creating layer conv2
I0521 06:11:13.452726  9072 net.cpp:106] Creating Layer conv2
I0521 06:11:13.452738  9072 net.cpp:454] conv2 <- pool1
I0521 06:11:13.452752  9072 net.cpp:411] conv2 -> conv2
I0521 06:11:13.454663  9072 net.cpp:150] Setting up conv2
I0521 06:11:13.454684  9072 net.cpp:157] Top shape: 740 20 54 46 (36763200)
I0521 06:11:13.454696  9072 net.cpp:165] Memory required for data: 677345680
I0521 06:11:13.454715  9072 layer_factory.hpp:77] Creating layer relu2
I0521 06:11:13.454727  9072 net.cpp:106] Creating Layer relu2
I0521 06:11:13.454737  9072 net.cpp:454] relu2 <- conv2
I0521 06:11:13.454751  9072 net.cpp:397] relu2 -> conv2 (in-place)
I0521 06:11:13.455085  9072 net.cpp:150] Setting up relu2
I0521 06:11:13.455099  9072 net.cpp:157] Top shape: 740 20 54 46 (36763200)
I0521 06:11:13.455111  9072 net.cpp:165] Memory required for data: 824398480
I0521 06:11:13.455121  9072 layer_factory.hpp:77] Creating layer pool2
I0521 06:11:13.455133  9072 net.cpp:106] Creating Layer pool2
I0521 06:11:13.455143  9072 net.cpp:454] pool2 <- conv2
I0521 06:11:13.455157  9072 net.cpp:411] pool2 -> pool2
I0521 06:11:13.455226  9072 net.cpp:150] Setting up pool2
I0521 06:11:13.455240  9072 net.cpp:157] Top shape: 740 20 27 46 (18381600)
I0521 06:11:13.455250  9072 net.cpp:165] Memory required for data: 897924880
I0521 06:11:13.455260  9072 layer_factory.hpp:77] Creating layer conv3
I0521 06:11:13.455281  9072 net.cpp:106] Creating Layer conv3
I0521 06:11:13.455291  9072 net.cpp:454] conv3 <- pool2
I0521 06:11:13.455305  9072 net.cpp:411] conv3 -> conv3
I0521 06:11:13.457286  9072 net.cpp:150] Setting up conv3
I0521 06:11:13.457310  9072 net.cpp:157] Top shape: 740 28 22 44 (20056960)
I0521 06:11:13.457320  9072 net.cpp:165] Memory required for data: 978152720
I0521 06:11:13.457352  9072 layer_factory.hpp:77] Creating layer relu3
I0521 06:11:13.457365  9072 net.cpp:106] Creating Layer relu3
I0521 06:11:13.457376  9072 net.cpp:454] relu3 <- conv3
I0521 06:11:13.457388  9072 net.cpp:397] relu3 -> conv3 (in-place)
I0521 06:11:13.457860  9072 net.cpp:150] Setting up relu3
I0521 06:11:13.457877  9072 net.cpp:157] Top shape: 740 28 22 44 (20056960)
I0521 06:11:13.457887  9072 net.cpp:165] Memory required for data: 1058380560
I0521 06:11:13.457897  9072 layer_factory.hpp:77] Creating layer pool3
I0521 06:11:13.457911  9072 net.cpp:106] Creating Layer pool3
I0521 06:11:13.457921  9072 net.cpp:454] pool3 <- conv3
I0521 06:11:13.457933  9072 net.cpp:411] pool3 -> pool3
I0521 06:11:13.458004  9072 net.cpp:150] Setting up pool3
I0521 06:11:13.458017  9072 net.cpp:157] Top shape: 740 28 11 44 (10028480)
I0521 06:11:13.458026  9072 net.cpp:165] Memory required for data: 1098494480
I0521 06:11:13.458036  9072 layer_factory.hpp:77] Creating layer conv4
I0521 06:11:13.458055  9072 net.cpp:106] Creating Layer conv4
I0521 06:11:13.458065  9072 net.cpp:454] conv4 <- pool3
I0521 06:11:13.458078  9072 net.cpp:411] conv4 -> conv4
I0521 06:11:13.460134  9072 net.cpp:150] Setting up conv4
I0521 06:11:13.460157  9072 net.cpp:157] Top shape: 740 36 6 42 (6713280)
I0521 06:11:13.460170  9072 net.cpp:165] Memory required for data: 1125347600
I0521 06:11:13.460186  9072 layer_factory.hpp:77] Creating layer relu4
I0521 06:11:13.460198  9072 net.cpp:106] Creating Layer relu4
I0521 06:11:13.460208  9072 net.cpp:454] relu4 <- conv4
I0521 06:11:13.460222  9072 net.cpp:397] relu4 -> conv4 (in-place)
I0521 06:11:13.460693  9072 net.cpp:150] Setting up relu4
I0521 06:11:13.460710  9072 net.cpp:157] Top shape: 740 36 6 42 (6713280)
I0521 06:11:13.460721  9072 net.cpp:165] Memory required for data: 1152200720
I0521 06:11:13.460731  9072 layer_factory.hpp:77] Creating layer pool4
I0521 06:11:13.460743  9072 net.cpp:106] Creating Layer pool4
I0521 06:11:13.460753  9072 net.cpp:454] pool4 <- conv4
I0521 06:11:13.460767  9072 net.cpp:411] pool4 -> pool4
I0521 06:11:13.460837  9072 net.cpp:150] Setting up pool4
I0521 06:11:13.460851  9072 net.cpp:157] Top shape: 740 36 3 42 (3356640)
I0521 06:11:13.460860  9072 net.cpp:165] Memory required for data: 1165627280
I0521 06:11:13.460870  9072 layer_factory.hpp:77] Creating layer ip1
I0521 06:11:13.460886  9072 net.cpp:106] Creating Layer ip1
I0521 06:11:13.460896  9072 net.cpp:454] ip1 <- pool4
I0521 06:11:13.460911  9072 net.cpp:411] ip1 -> ip1
I0521 06:11:13.476400  9072 net.cpp:150] Setting up ip1
I0521 06:11:13.476429  9072 net.cpp:157] Top shape: 740 196 (145040)
I0521 06:11:13.476440  9072 net.cpp:165] Memory required for data: 1166207440
I0521 06:11:13.476462  9072 layer_factory.hpp:77] Creating layer relu5
I0521 06:11:13.476478  9072 net.cpp:106] Creating Layer relu5
I0521 06:11:13.476488  9072 net.cpp:454] relu5 <- ip1
I0521 06:11:13.476502  9072 net.cpp:397] relu5 -> ip1 (in-place)
I0521 06:11:13.476850  9072 net.cpp:150] Setting up relu5
I0521 06:11:13.476863  9072 net.cpp:157] Top shape: 740 196 (145040)
I0521 06:11:13.476873  9072 net.cpp:165] Memory required for data: 1166787600
I0521 06:11:13.476883  9072 layer_factory.hpp:77] Creating layer drop1
I0521 06:11:13.476902  9072 net.cpp:106] Creating Layer drop1
I0521 06:11:13.476912  9072 net.cpp:454] drop1 <- ip1
I0521 06:11:13.476925  9072 net.cpp:397] drop1 -> ip1 (in-place)
I0521 06:11:13.476969  9072 net.cpp:150] Setting up drop1
I0521 06:11:13.476982  9072 net.cpp:157] Top shape: 740 196 (145040)
I0521 06:11:13.476992  9072 net.cpp:165] Memory required for data: 1167367760
I0521 06:11:13.477002  9072 layer_factory.hpp:77] Creating layer ip2
I0521 06:11:13.477017  9072 net.cpp:106] Creating Layer ip2
I0521 06:11:13.477026  9072 net.cpp:454] ip2 <- ip1
I0521 06:11:13.477041  9072 net.cpp:411] ip2 -> ip2
I0521 06:11:13.477519  9072 net.cpp:150] Setting up ip2
I0521 06:11:13.477532  9072 net.cpp:157] Top shape: 740 98 (72520)
I0521 06:11:13.477542  9072 net.cpp:165] Memory required for data: 1167657840
I0521 06:11:13.477571  9072 layer_factory.hpp:77] Creating layer relu6
I0521 06:11:13.477584  9072 net.cpp:106] Creating Layer relu6
I0521 06:11:13.477594  9072 net.cpp:454] relu6 <- ip2
I0521 06:11:13.477607  9072 net.cpp:397] relu6 -> ip2 (in-place)
I0521 06:11:13.478139  9072 net.cpp:150] Setting up relu6
I0521 06:11:13.478160  9072 net.cpp:157] Top shape: 740 98 (72520)
I0521 06:11:13.478170  9072 net.cpp:165] Memory required for data: 1167947920
I0521 06:11:13.478180  9072 layer_factory.hpp:77] Creating layer drop2
I0521 06:11:13.478195  9072 net.cpp:106] Creating Layer drop2
I0521 06:11:13.478205  9072 net.cpp:454] drop2 <- ip2
I0521 06:11:13.478219  9072 net.cpp:397] drop2 -> ip2 (in-place)
I0521 06:11:13.478261  9072 net.cpp:150] Setting up drop2
I0521 06:11:13.478274  9072 net.cpp:157] Top shape: 740 98 (72520)
I0521 06:11:13.478286  9072 net.cpp:165] Memory required for data: 1168238000
I0521 06:11:13.478294  9072 layer_factory.hpp:77] Creating layer ip3
I0521 06:11:13.478309  9072 net.cpp:106] Creating Layer ip3
I0521 06:11:13.478319  9072 net.cpp:454] ip3 <- ip2
I0521 06:11:13.478333  9072 net.cpp:411] ip3 -> ip3
I0521 06:11:13.478557  9072 net.cpp:150] Setting up ip3
I0521 06:11:13.478570  9072 net.cpp:157] Top shape: 740 11 (8140)
I0521 06:11:13.478580  9072 net.cpp:165] Memory required for data: 1168270560
I0521 06:11:13.478596  9072 layer_factory.hpp:77] Creating layer drop3
I0521 06:11:13.478610  9072 net.cpp:106] Creating Layer drop3
I0521 06:11:13.478620  9072 net.cpp:454] drop3 <- ip3
I0521 06:11:13.478632  9072 net.cpp:397] drop3 -> ip3 (in-place)
I0521 06:11:13.478673  9072 net.cpp:150] Setting up drop3
I0521 06:11:13.478685  9072 net.cpp:157] Top shape: 740 11 (8140)
I0521 06:11:13.478695  9072 net.cpp:165] Memory required for data: 1168303120
I0521 06:11:13.478704  9072 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 06:11:13.478718  9072 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 06:11:13.478727  9072 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 06:11:13.478739  9072 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 06:11:13.478754  9072 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 06:11:13.478827  9072 net.cpp:150] Setting up ip3_drop3_0_split
I0521 06:11:13.478840  9072 net.cpp:157] Top shape: 740 11 (8140)
I0521 06:11:13.478850  9072 net.cpp:157] Top shape: 740 11 (8140)
I0521 06:11:13.478860  9072 net.cpp:165] Memory required for data: 1168368240
I0521 06:11:13.478870  9072 layer_factory.hpp:77] Creating layer accuracy
I0521 06:11:13.478893  9072 net.cpp:106] Creating Layer accuracy
I0521 06:11:13.478904  9072 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 06:11:13.478914  9072 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 06:11:13.478927  9072 net.cpp:411] accuracy -> accuracy
I0521 06:11:13.478951  9072 net.cpp:150] Setting up accuracy
I0521 06:11:13.478965  9072 net.cpp:157] Top shape: (1)
I0521 06:11:13.478974  9072 net.cpp:165] Memory required for data: 1168368244
I0521 06:11:13.478984  9072 layer_factory.hpp:77] Creating layer loss
I0521 06:11:13.478997  9072 net.cpp:106] Creating Layer loss
I0521 06:11:13.479007  9072 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 06:11:13.479018  9072 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 06:11:13.479032  9072 net.cpp:411] loss -> loss
I0521 06:11:13.479049  9072 layer_factory.hpp:77] Creating layer loss
I0521 06:11:13.479542  9072 net.cpp:150] Setting up loss
I0521 06:11:13.479557  9072 net.cpp:157] Top shape: (1)
I0521 06:11:13.479567  9072 net.cpp:160]     with loss weight 1
I0521 06:11:13.479585  9072 net.cpp:165] Memory required for data: 1168368248
I0521 06:11:13.479595  9072 net.cpp:226] loss needs backward computation.
I0521 06:11:13.479606  9072 net.cpp:228] accuracy does not need backward computation.
I0521 06:11:13.479617  9072 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 06:11:13.479629  9072 net.cpp:226] drop3 needs backward computation.
I0521 06:11:13.479638  9072 net.cpp:226] ip3 needs backward computation.
I0521 06:11:13.479656  9072 net.cpp:226] drop2 needs backward computation.
I0521 06:11:13.479674  9072 net.cpp:226] relu6 needs backward computation.
I0521 06:11:13.479684  9072 net.cpp:226] ip2 needs backward computation.
I0521 06:11:13.479694  9072 net.cpp:226] drop1 needs backward computation.
I0521 06:11:13.479704  9072 net.cpp:226] relu5 needs backward computation.
I0521 06:11:13.479713  9072 net.cpp:226] ip1 needs backward computation.
I0521 06:11:13.479723  9072 net.cpp:226] pool4 needs backward computation.
I0521 06:11:13.479734  9072 net.cpp:226] relu4 needs backward computation.
I0521 06:11:13.479744  9072 net.cpp:226] conv4 needs backward computation.
I0521 06:11:13.479755  9072 net.cpp:226] pool3 needs backward computation.
I0521 06:11:13.479765  9072 net.cpp:226] relu3 needs backward computation.
I0521 06:11:13.479775  9072 net.cpp:226] conv3 needs backward computation.
I0521 06:11:13.479785  9072 net.cpp:226] pool2 needs backward computation.
I0521 06:11:13.479795  9072 net.cpp:226] relu2 needs backward computation.
I0521 06:11:13.479805  9072 net.cpp:226] conv2 needs backward computation.
I0521 06:11:13.479815  9072 net.cpp:226] pool1 needs backward computation.
I0521 06:11:13.479825  9072 net.cpp:226] relu1 needs backward computation.
I0521 06:11:13.479835  9072 net.cpp:226] conv1 needs backward computation.
I0521 06:11:13.479846  9072 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 06:11:13.479859  9072 net.cpp:228] data_hdf5 does not need backward computation.
I0521 06:11:13.479868  9072 net.cpp:270] This network produces output accuracy
I0521 06:11:13.479879  9072 net.cpp:270] This network produces output loss
I0521 06:11:13.479907  9072 net.cpp:283] Network initialization done.
I0521 06:11:13.480041  9072 solver.cpp:60] Solver scaffolding done.
I0521 06:11:13.481170  9072 caffe.cpp:212] Starting Optimization
I0521 06:11:13.481189  9072 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 06:11:13.481199  9072 solver.cpp:289] Learning Rate Policy: fixed
I0521 06:11:13.482415  9072 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 06:11:59.487306  9072 solver.cpp:409]     Test net output #0: accuracy = 0.0803921
I0521 06:11:59.487468  9072 solver.cpp:409]     Test net output #1: loss = 2.39746 (* 1 = 2.39746 loss)
I0521 06:11:59.625690  9072 solver.cpp:237] Iteration 0, loss = 2.39815
I0521 06:11:59.625727  9072 solver.cpp:253]     Train net output #0: loss = 2.39815 (* 1 = 2.39815 loss)
I0521 06:11:59.625744  9072 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 06:12:07.567497  9072 solver.cpp:237] Iteration 20, loss = 2.38623
I0521 06:12:07.567543  9072 solver.cpp:253]     Train net output #0: loss = 2.38623 (* 1 = 2.38623 loss)
I0521 06:12:07.567560  9072 sgd_solver.cpp:106] Iteration 20, lr = 0.0025
I0521 06:12:15.516096  9072 solver.cpp:237] Iteration 40, loss = 2.37165
I0521 06:12:15.516129  9072 solver.cpp:253]     Train net output #0: loss = 2.37165 (* 1 = 2.37165 loss)
I0521 06:12:15.516145  9072 sgd_solver.cpp:106] Iteration 40, lr = 0.0025
I0521 06:12:23.456187  9072 solver.cpp:237] Iteration 60, loss = 2.35835
I0521 06:12:23.456218  9072 solver.cpp:253]     Train net output #0: loss = 2.35835 (* 1 = 2.35835 loss)
I0521 06:12:23.456235  9072 sgd_solver.cpp:106] Iteration 60, lr = 0.0025
I0521 06:12:31.398289  9072 solver.cpp:237] Iteration 80, loss = 2.34847
I0521 06:12:31.398435  9072 solver.cpp:253]     Train net output #0: loss = 2.34847 (* 1 = 2.34847 loss)
I0521 06:12:31.398449  9072 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0521 06:12:39.340025  9072 solver.cpp:237] Iteration 100, loss = 2.34492
I0521 06:12:39.340060  9072 solver.cpp:253]     Train net output #0: loss = 2.34492 (* 1 = 2.34492 loss)
I0521 06:12:39.340080  9072 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0521 06:12:47.286133  9072 solver.cpp:237] Iteration 120, loss = 2.31548
I0521 06:12:47.286165  9072 solver.cpp:253]     Train net output #0: loss = 2.31548 (* 1 = 2.31548 loss)
I0521 06:12:47.286182  9072 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 06:13:17.414048  9072 solver.cpp:237] Iteration 140, loss = 2.30327
I0521 06:13:17.414227  9072 solver.cpp:253]     Train net output #0: loss = 2.30327 (* 1 = 2.30327 loss)
I0521 06:13:17.414242  9072 sgd_solver.cpp:106] Iteration 140, lr = 0.0025
I0521 06:13:25.361451  9072 solver.cpp:237] Iteration 160, loss = 2.31347
I0521 06:13:25.361493  9072 solver.cpp:253]     Train net output #0: loss = 2.31347 (* 1 = 2.31347 loss)
I0521 06:13:25.361511  9072 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 06:13:33.314227  9072 solver.cpp:237] Iteration 180, loss = 2.32171
I0521 06:13:33.314261  9072 solver.cpp:253]     Train net output #0: loss = 2.32171 (* 1 = 2.32171 loss)
I0521 06:13:33.314277  9072 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 06:13:41.261840  9072 solver.cpp:237] Iteration 200, loss = 2.31273
I0521 06:13:41.261873  9072 solver.cpp:253]     Train net output #0: loss = 2.31273 (* 1 = 2.31273 loss)
I0521 06:13:41.261889  9072 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0521 06:13:41.660146  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_202.caffemodel
I0521 06:13:41.980888  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_202.solverstate
I0521 06:13:49.275948  9072 solver.cpp:237] Iteration 220, loss = 2.2917
I0521 06:13:49.276103  9072 solver.cpp:253]     Train net output #0: loss = 2.2917 (* 1 = 2.2917 loss)
I0521 06:13:49.276116  9072 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0521 06:13:57.224633  9072 solver.cpp:237] Iteration 240, loss = 2.27225
I0521 06:13:57.224665  9072 solver.cpp:253]     Train net output #0: loss = 2.27225 (* 1 = 2.27225 loss)
I0521 06:13:57.224683  9072 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 06:14:05.173393  9072 solver.cpp:237] Iteration 260, loss = 2.25189
I0521 06:14:05.173427  9072 solver.cpp:253]     Train net output #0: loss = 2.25189 (* 1 = 2.25189 loss)
I0521 06:14:05.173444  9072 sgd_solver.cpp:106] Iteration 260, lr = 0.0025
I0521 06:14:35.287102  9072 solver.cpp:237] Iteration 280, loss = 2.2147
I0521 06:14:35.287258  9072 solver.cpp:253]     Train net output #0: loss = 2.2147 (* 1 = 2.2147 loss)
I0521 06:14:35.287274  9072 sgd_solver.cpp:106] Iteration 280, lr = 0.0025
I0521 06:14:43.234056  9072 solver.cpp:237] Iteration 300, loss = 2.20736
I0521 06:14:43.234097  9072 solver.cpp:253]     Train net output #0: loss = 2.20736 (* 1 = 2.20736 loss)
I0521 06:14:43.234114  9072 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0521 06:14:51.184455  9072 solver.cpp:237] Iteration 320, loss = 2.21231
I0521 06:14:51.184489  9072 solver.cpp:253]     Train net output #0: loss = 2.21231 (* 1 = 2.21231 loss)
I0521 06:14:51.184505  9072 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 06:14:59.132936  9072 solver.cpp:237] Iteration 340, loss = 2.1614
I0521 06:14:59.132971  9072 solver.cpp:253]     Train net output #0: loss = 2.1614 (* 1 = 2.1614 loss)
I0521 06:14:59.132987  9072 sgd_solver.cpp:106] Iteration 340, lr = 0.0025
I0521 06:15:07.086473  9072 solver.cpp:237] Iteration 360, loss = 2.09767
I0521 06:15:07.086632  9072 solver.cpp:253]     Train net output #0: loss = 2.09767 (* 1 = 2.09767 loss)
I0521 06:15:07.086647  9072 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 06:15:15.033507  9072 solver.cpp:237] Iteration 380, loss = 2.13571
I0521 06:15:15.033540  9072 solver.cpp:253]     Train net output #0: loss = 2.13571 (* 1 = 2.13571 loss)
I0521 06:15:15.033560  9072 sgd_solver.cpp:106] Iteration 380, lr = 0.0025
I0521 06:15:22.980628  9072 solver.cpp:237] Iteration 400, loss = 2.09985
I0521 06:15:22.980660  9072 solver.cpp:253]     Train net output #0: loss = 2.09985 (* 1 = 2.09985 loss)
I0521 06:15:22.980677  9072 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 06:15:24.173730  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_404.caffemodel
I0521 06:15:24.491708  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_404.solverstate
I0521 06:15:24.635387  9072 solver.cpp:341] Iteration 405, Testing net (#0)
I0521 06:16:09.890750  9072 solver.cpp:409]     Test net output #0: accuracy = 0.47781
I0521 06:16:09.890911  9072 solver.cpp:409]     Test net output #1: loss = 1.88904 (* 1 = 1.88904 loss)
I0521 06:16:38.143079  9072 solver.cpp:237] Iteration 420, loss = 2.07877
I0521 06:16:38.143131  9072 solver.cpp:253]     Train net output #0: loss = 2.07877 (* 1 = 2.07877 loss)
I0521 06:16:38.143146  9072 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0521 06:16:46.093967  9072 solver.cpp:237] Iteration 440, loss = 2.06473
I0521 06:16:46.094131  9072 solver.cpp:253]     Train net output #0: loss = 2.06473 (* 1 = 2.06473 loss)
I0521 06:16:46.094146  9072 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0521 06:16:54.039350  9072 solver.cpp:237] Iteration 460, loss = 2.03657
I0521 06:16:54.039382  9072 solver.cpp:253]     Train net output #0: loss = 2.03657 (* 1 = 2.03657 loss)
I0521 06:16:54.039399  9072 sgd_solver.cpp:106] Iteration 460, lr = 0.0025
I0521 06:17:01.987962  9072 solver.cpp:237] Iteration 480, loss = 1.99918
I0521 06:17:01.987995  9072 solver.cpp:253]     Train net output #0: loss = 1.99918 (* 1 = 1.99918 loss)
I0521 06:17:01.988013  9072 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 06:17:09.934028  9072 solver.cpp:237] Iteration 500, loss = 1.96259
I0521 06:17:09.934070  9072 solver.cpp:253]     Train net output #0: loss = 1.96259 (* 1 = 1.96259 loss)
I0521 06:17:09.934089  9072 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0521 06:17:17.882738  9072 solver.cpp:237] Iteration 520, loss = 2.00907
I0521 06:17:17.882870  9072 solver.cpp:253]     Train net output #0: loss = 2.00907 (* 1 = 2.00907 loss)
I0521 06:17:17.882884  9072 sgd_solver.cpp:106] Iteration 520, lr = 0.0025
I0521 06:17:48.064685  9072 solver.cpp:237] Iteration 540, loss = 1.92248
I0521 06:17:48.064856  9072 solver.cpp:253]     Train net output #0: loss = 1.92248 (* 1 = 1.92248 loss)
I0521 06:17:48.064872  9072 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 06:17:56.010460  9072 solver.cpp:237] Iteration 560, loss = 1.94409
I0521 06:17:56.010493  9072 solver.cpp:253]     Train net output #0: loss = 1.94409 (* 1 = 1.94409 loss)
I0521 06:17:56.010510  9072 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 06:18:03.958945  9072 solver.cpp:237] Iteration 580, loss = 1.86585
I0521 06:18:03.958981  9072 solver.cpp:253]     Train net output #0: loss = 1.86585 (* 1 = 1.86585 loss)
I0521 06:18:03.959002  9072 sgd_solver.cpp:106] Iteration 580, lr = 0.0025
I0521 06:18:11.904314  9072 solver.cpp:237] Iteration 600, loss = 1.9137
I0521 06:18:11.904346  9072 solver.cpp:253]     Train net output #0: loss = 1.9137 (* 1 = 1.9137 loss)
I0521 06:18:11.904362  9072 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 06:18:13.888909  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_606.caffemodel
I0521 06:18:14.209728  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_606.solverstate
I0521 06:18:19.916116  9072 solver.cpp:237] Iteration 620, loss = 1.89716
I0521 06:18:19.916291  9072 solver.cpp:253]     Train net output #0: loss = 1.89716 (* 1 = 1.89716 loss)
I0521 06:18:19.916306  9072 sgd_solver.cpp:106] Iteration 620, lr = 0.0025
I0521 06:18:27.859268  9072 solver.cpp:237] Iteration 640, loss = 1.91639
I0521 06:18:27.859309  9072 solver.cpp:253]     Train net output #0: loss = 1.91639 (* 1 = 1.91639 loss)
I0521 06:18:27.859329  9072 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 06:18:35.802407  9072 solver.cpp:237] Iteration 660, loss = 1.85604
I0521 06:18:35.802440  9072 solver.cpp:253]     Train net output #0: loss = 1.85604 (* 1 = 1.85604 loss)
I0521 06:18:35.802456  9072 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 06:19:05.981076  9072 solver.cpp:237] Iteration 680, loss = 1.91241
I0521 06:19:05.981237  9072 solver.cpp:253]     Train net output #0: loss = 1.91241 (* 1 = 1.91241 loss)
I0521 06:19:05.981253  9072 sgd_solver.cpp:106] Iteration 680, lr = 0.0025
I0521 06:19:13.925849  9072 solver.cpp:237] Iteration 700, loss = 1.84847
I0521 06:19:13.925889  9072 solver.cpp:253]     Train net output #0: loss = 1.84847 (* 1 = 1.84847 loss)
I0521 06:19:13.925911  9072 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0521 06:19:21.866582  9072 solver.cpp:237] Iteration 720, loss = 1.83941
I0521 06:19:21.866617  9072 solver.cpp:253]     Train net output #0: loss = 1.83941 (* 1 = 1.83941 loss)
I0521 06:19:21.866633  9072 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 06:19:29.810199  9072 solver.cpp:237] Iteration 740, loss = 1.96154
I0521 06:19:29.810231  9072 solver.cpp:253]     Train net output #0: loss = 1.96154 (* 1 = 1.96154 loss)
I0521 06:19:29.810245  9072 sgd_solver.cpp:106] Iteration 740, lr = 0.0025
I0521 06:19:37.759049  9072 solver.cpp:237] Iteration 760, loss = 1.84165
I0521 06:19:37.759196  9072 solver.cpp:253]     Train net output #0: loss = 1.84165 (* 1 = 1.84165 loss)
I0521 06:19:37.759209  9072 sgd_solver.cpp:106] Iteration 760, lr = 0.0025
I0521 06:19:45.705648  9072 solver.cpp:237] Iteration 780, loss = 1.83016
I0521 06:19:45.705680  9072 solver.cpp:253]     Train net output #0: loss = 1.83016 (* 1 = 1.83016 loss)
I0521 06:19:45.705698  9072 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0521 06:19:53.651809  9072 solver.cpp:237] Iteration 800, loss = 1.92028
I0521 06:19:53.651842  9072 solver.cpp:253]     Train net output #0: loss = 1.92028 (* 1 = 1.92028 loss)
I0521 06:19:53.651859  9072 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 06:19:56.433926  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_808.caffemodel
I0521 06:19:56.763191  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_808.solverstate
I0521 06:19:57.307448  9072 solver.cpp:341] Iteration 810, Testing net (#0)
I0521 06:21:03.472781  9072 solver.cpp:409]     Test net output #0: accuracy = 0.617414
I0521 06:21:03.472960  9072 solver.cpp:409]     Test net output #1: loss = 1.40526 (* 1 = 1.40526 loss)
I0521 06:21:29.756430  9072 solver.cpp:237] Iteration 820, loss = 1.84528
I0521 06:21:29.756480  9072 solver.cpp:253]     Train net output #0: loss = 1.84528 (* 1 = 1.84528 loss)
I0521 06:21:29.756497  9072 sgd_solver.cpp:106] Iteration 820, lr = 0.0025
I0521 06:21:37.699941  9072 solver.cpp:237] Iteration 840, loss = 1.83616
I0521 06:21:37.700091  9072 solver.cpp:253]     Train net output #0: loss = 1.83616 (* 1 = 1.83616 loss)
I0521 06:21:37.700104  9072 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 06:21:45.634804  9072 solver.cpp:237] Iteration 860, loss = 1.84304
I0521 06:21:45.634840  9072 solver.cpp:253]     Train net output #0: loss = 1.84304 (* 1 = 1.84304 loss)
I0521 06:21:45.634857  9072 sgd_solver.cpp:106] Iteration 860, lr = 0.0025
I0521 06:21:53.569779  9072 solver.cpp:237] Iteration 880, loss = 1.76143
I0521 06:21:53.569811  9072 solver.cpp:253]     Train net output #0: loss = 1.76143 (* 1 = 1.76143 loss)
I0521 06:21:53.569828  9072 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 06:22:01.506012  9072 solver.cpp:237] Iteration 900, loss = 1.90439
I0521 06:22:01.506045  9072 solver.cpp:253]     Train net output #0: loss = 1.90439 (* 1 = 1.90439 loss)
I0521 06:22:01.506062  9072 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 06:22:09.443334  9072 solver.cpp:237] Iteration 920, loss = 1.78479
I0521 06:22:09.443485  9072 solver.cpp:253]     Train net output #0: loss = 1.78479 (* 1 = 1.78479 loss)
I0521 06:22:09.443498  9072 sgd_solver.cpp:106] Iteration 920, lr = 0.0025
I0521 06:22:17.376911  9072 solver.cpp:237] Iteration 940, loss = 1.7568
I0521 06:22:17.376943  9072 solver.cpp:253]     Train net output #0: loss = 1.7568 (* 1 = 1.7568 loss)
I0521 06:22:17.376960  9072 sgd_solver.cpp:106] Iteration 940, lr = 0.0025
I0521 06:22:47.435034  9072 solver.cpp:237] Iteration 960, loss = 1.8713
I0521 06:22:47.435209  9072 solver.cpp:253]     Train net output #0: loss = 1.8713 (* 1 = 1.8713 loss)
I0521 06:22:47.435223  9072 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 06:22:55.377146  9072 solver.cpp:237] Iteration 980, loss = 1.79218
I0521 06:22:55.377177  9072 solver.cpp:253]     Train net output #0: loss = 1.79218 (* 1 = 1.79218 loss)
I0521 06:22:55.377193  9072 sgd_solver.cpp:106] Iteration 980, lr = 0.0025
I0521 06:23:03.316172  9072 solver.cpp:237] Iteration 1000, loss = 1.79036
I0521 06:23:03.316201  9072 solver.cpp:253]     Train net output #0: loss = 1.79036 (* 1 = 1.79036 loss)
I0521 06:23:03.316218  9072 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0521 06:23:06.887958  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1010.caffemodel
I0521 06:23:07.208722  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1010.solverstate
I0521 06:23:11.324681  9072 solver.cpp:237] Iteration 1020, loss = 1.83915
I0521 06:23:11.324724  9072 solver.cpp:253]     Train net output #0: loss = 1.83915 (* 1 = 1.83915 loss)
I0521 06:23:11.324744  9072 sgd_solver.cpp:106] Iteration 1020, lr = 0.0025
I0521 06:23:19.259719  9072 solver.cpp:237] Iteration 1040, loss = 1.737
I0521 06:23:19.259858  9072 solver.cpp:253]     Train net output #0: loss = 1.737 (* 1 = 1.737 loss)
I0521 06:23:19.259872  9072 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 06:23:27.196877  9072 solver.cpp:237] Iteration 1060, loss = 1.78286
I0521 06:23:27.196918  9072 solver.cpp:253]     Train net output #0: loss = 1.78286 (* 1 = 1.78286 loss)
I0521 06:23:27.196938  9072 sgd_solver.cpp:106] Iteration 1060, lr = 0.0025
I0521 06:23:35.133507  9072 solver.cpp:237] Iteration 1080, loss = 1.71571
I0521 06:23:35.133539  9072 solver.cpp:253]     Train net output #0: loss = 1.71571 (* 1 = 1.71571 loss)
I0521 06:23:35.133556  9072 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 06:24:05.272887  9072 solver.cpp:237] Iteration 1100, loss = 1.67984
I0521 06:24:05.273056  9072 solver.cpp:253]     Train net output #0: loss = 1.67984 (* 1 = 1.67984 loss)
I0521 06:24:05.273072  9072 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 06:24:13.206738  9072 solver.cpp:237] Iteration 1120, loss = 1.7829
I0521 06:24:13.206784  9072 solver.cpp:253]     Train net output #0: loss = 1.7829 (* 1 = 1.7829 loss)
I0521 06:24:13.206802  9072 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 06:24:21.140069  9072 solver.cpp:237] Iteration 1140, loss = 1.8052
I0521 06:24:21.140103  9072 solver.cpp:253]     Train net output #0: loss = 1.8052 (* 1 = 1.8052 loss)
I0521 06:24:21.140120  9072 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 06:24:29.077852  9072 solver.cpp:237] Iteration 1160, loss = 1.72565
I0521 06:24:29.077883  9072 solver.cpp:253]     Train net output #0: loss = 1.72565 (* 1 = 1.72565 loss)
I0521 06:24:29.077895  9072 sgd_solver.cpp:106] Iteration 1160, lr = 0.0025
I0521 06:24:37.015471  9072 solver.cpp:237] Iteration 1180, loss = 1.74199
I0521 06:24:37.015626  9072 solver.cpp:253]     Train net output #0: loss = 1.74199 (* 1 = 1.74199 loss)
I0521 06:24:37.015646  9072 sgd_solver.cpp:106] Iteration 1180, lr = 0.0025
I0521 06:24:44.949136  9072 solver.cpp:237] Iteration 1200, loss = 1.74132
I0521 06:24:44.949167  9072 solver.cpp:253]     Train net output #0: loss = 1.74132 (* 1 = 1.74132 loss)
I0521 06:24:44.949185  9072 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 06:24:49.316051  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1212.caffemodel
I0521 06:24:49.632858  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1212.solverstate
I0521 06:24:50.573348  9072 solver.cpp:341] Iteration 1215, Testing net (#0)
I0521 06:25:35.562796  9072 solver.cpp:409]     Test net output #0: accuracy = 0.644247
I0521 06:25:35.562966  9072 solver.cpp:409]     Test net output #1: loss = 1.23654 (* 1 = 1.23654 loss)
I0521 06:25:59.826304  9072 solver.cpp:237] Iteration 1220, loss = 1.73232
I0521 06:25:59.826355  9072 solver.cpp:253]     Train net output #0: loss = 1.73232 (* 1 = 1.73232 loss)
I0521 06:25:59.826372  9072 sgd_solver.cpp:106] Iteration 1220, lr = 0.0025
I0521 06:26:07.763576  9072 solver.cpp:237] Iteration 1240, loss = 1.7685
I0521 06:26:07.763728  9072 solver.cpp:253]     Train net output #0: loss = 1.7685 (* 1 = 1.7685 loss)
I0521 06:26:07.763742  9072 sgd_solver.cpp:106] Iteration 1240, lr = 0.0025
I0521 06:26:15.702540  9072 solver.cpp:237] Iteration 1260, loss = 1.71441
I0521 06:26:15.702572  9072 solver.cpp:253]     Train net output #0: loss = 1.71441 (* 1 = 1.71441 loss)
I0521 06:26:15.702589  9072 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 06:26:23.640008  9072 solver.cpp:237] Iteration 1280, loss = 1.69087
I0521 06:26:23.640046  9072 solver.cpp:253]     Train net output #0: loss = 1.69087 (* 1 = 1.69087 loss)
I0521 06:26:23.640067  9072 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 06:26:31.576267  9072 solver.cpp:237] Iteration 1300, loss = 1.75974
I0521 06:26:31.576300  9072 solver.cpp:253]     Train net output #0: loss = 1.75974 (* 1 = 1.75974 loss)
I0521 06:26:31.576318  9072 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0521 06:26:39.513881  9072 solver.cpp:237] Iteration 1320, loss = 1.69137
I0521 06:26:39.514020  9072 solver.cpp:253]     Train net output #0: loss = 1.69137 (* 1 = 1.69137 loss)
I0521 06:26:39.514034  9072 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 06:26:47.452623  9072 solver.cpp:237] Iteration 1340, loss = 1.75693
I0521 06:26:47.452668  9072 solver.cpp:253]     Train net output #0: loss = 1.75693 (* 1 = 1.75693 loss)
I0521 06:26:47.452687  9072 sgd_solver.cpp:106] Iteration 1340, lr = 0.0025
I0521 06:27:17.553809  9072 solver.cpp:237] Iteration 1360, loss = 1.68644
I0521 06:27:17.553984  9072 solver.cpp:253]     Train net output #0: loss = 1.68644 (* 1 = 1.68644 loss)
I0521 06:27:17.554000  9072 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 06:27:25.492046  9072 solver.cpp:237] Iteration 1380, loss = 1.78714
I0521 06:27:25.492075  9072 solver.cpp:253]     Train net output #0: loss = 1.78714 (* 1 = 1.78714 loss)
I0521 06:27:25.492087  9072 sgd_solver.cpp:106] Iteration 1380, lr = 0.0025
I0521 06:27:33.427623  9072 solver.cpp:237] Iteration 1400, loss = 1.72045
I0521 06:27:33.427673  9072 solver.cpp:253]     Train net output #0: loss = 1.72045 (* 1 = 1.72045 loss)
I0521 06:27:33.427690  9072 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0521 06:27:38.587466  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1414.caffemodel
I0521 06:27:38.904736  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1414.solverstate
I0521 06:27:41.427942  9072 solver.cpp:237] Iteration 1420, loss = 1.76324
I0521 06:27:41.427983  9072 solver.cpp:253]     Train net output #0: loss = 1.76324 (* 1 = 1.76324 loss)
I0521 06:27:41.428004  9072 sgd_solver.cpp:106] Iteration 1420, lr = 0.0025
I0521 06:27:49.363572  9072 solver.cpp:237] Iteration 1440, loss = 1.65109
I0521 06:27:49.363720  9072 solver.cpp:253]     Train net output #0: loss = 1.65109 (* 1 = 1.65109 loss)
I0521 06:27:49.363734  9072 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 06:27:57.302167  9072 solver.cpp:237] Iteration 1460, loss = 1.63278
I0521 06:27:57.302206  9072 solver.cpp:253]     Train net output #0: loss = 1.63278 (* 1 = 1.63278 loss)
I0521 06:27:57.302227  9072 sgd_solver.cpp:106] Iteration 1460, lr = 0.0025
I0521 06:28:05.239711  9072 solver.cpp:237] Iteration 1480, loss = 1.70309
I0521 06:28:05.239745  9072 solver.cpp:253]     Train net output #0: loss = 1.70309 (* 1 = 1.70309 loss)
I0521 06:28:05.239763  9072 sgd_solver.cpp:106] Iteration 1480, lr = 0.0025
I0521 06:28:35.333817  9072 solver.cpp:237] Iteration 1500, loss = 1.68463
I0521 06:28:35.333986  9072 solver.cpp:253]     Train net output #0: loss = 1.68463 (* 1 = 1.68463 loss)
I0521 06:28:35.334000  9072 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0521 06:28:43.266388  9072 solver.cpp:237] Iteration 1520, loss = 1.733
I0521 06:28:43.266420  9072 solver.cpp:253]     Train net output #0: loss = 1.733 (* 1 = 1.733 loss)
I0521 06:28:43.266434  9072 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 06:28:51.199235  9072 solver.cpp:237] Iteration 1540, loss = 1.74273
I0521 06:28:51.199275  9072 solver.cpp:253]     Train net output #0: loss = 1.74273 (* 1 = 1.74273 loss)
I0521 06:28:51.199295  9072 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 06:28:59.138223  9072 solver.cpp:237] Iteration 1560, loss = 1.71217
I0521 06:28:59.138257  9072 solver.cpp:253]     Train net output #0: loss = 1.71217 (* 1 = 1.71217 loss)
I0521 06:28:59.138270  9072 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 06:29:07.074285  9072 solver.cpp:237] Iteration 1580, loss = 1.70175
I0521 06:29:07.074430  9072 solver.cpp:253]     Train net output #0: loss = 1.70175 (* 1 = 1.70175 loss)
I0521 06:29:07.074443  9072 sgd_solver.cpp:106] Iteration 1580, lr = 0.0025
I0521 06:29:15.010752  9072 solver.cpp:237] Iteration 1600, loss = 1.66001
I0521 06:29:15.010792  9072 solver.cpp:253]     Train net output #0: loss = 1.66001 (* 1 = 1.66001 loss)
I0521 06:29:15.010813  9072 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 06:29:20.965772  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1616.caffemodel
I0521 06:29:21.282912  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1616.solverstate
I0521 06:29:22.617843  9072 solver.cpp:341] Iteration 1620, Testing net (#0)
I0521 06:30:28.804318  9072 solver.cpp:409]     Test net output #0: accuracy = 0.660296
I0521 06:30:28.804484  9072 solver.cpp:409]     Test net output #1: loss = 1.15974 (* 1 = 1.15974 loss)
I0521 06:30:28.922577  9072 solver.cpp:237] Iteration 1620, loss = 1.68887
I0521 06:30:28.922605  9072 solver.cpp:253]     Train net output #0: loss = 1.68887 (* 1 = 1.68887 loss)
I0521 06:30:28.922623  9072 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 06:30:59.042374  9072 solver.cpp:237] Iteration 1640, loss = 1.68492
I0521 06:30:59.042539  9072 solver.cpp:253]     Train net output #0: loss = 1.68492 (* 1 = 1.68492 loss)
I0521 06:30:59.042556  9072 sgd_solver.cpp:106] Iteration 1640, lr = 0.0025
I0521 06:31:06.983201  9072 solver.cpp:237] Iteration 1660, loss = 1.72021
I0521 06:31:06.983232  9072 solver.cpp:253]     Train net output #0: loss = 1.72021 (* 1 = 1.72021 loss)
I0521 06:31:06.983250  9072 sgd_solver.cpp:106] Iteration 1660, lr = 0.0025
I0521 06:31:14.922500  9072 solver.cpp:237] Iteration 1680, loss = 1.67792
I0521 06:31:14.922534  9072 solver.cpp:253]     Train net output #0: loss = 1.67792 (* 1 = 1.67792 loss)
I0521 06:31:14.922550  9072 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 06:31:22.863572  9072 solver.cpp:237] Iteration 1700, loss = 1.67384
I0521 06:31:22.863612  9072 solver.cpp:253]     Train net output #0: loss = 1.67384 (* 1 = 1.67384 loss)
I0521 06:31:22.863633  9072 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0521 06:31:30.805805  9072 solver.cpp:237] Iteration 1720, loss = 1.702
I0521 06:31:30.805946  9072 solver.cpp:253]     Train net output #0: loss = 1.702 (* 1 = 1.702 loss)
I0521 06:31:30.805959  9072 sgd_solver.cpp:106] Iteration 1720, lr = 0.0025
I0521 06:31:38.745774  9072 solver.cpp:237] Iteration 1740, loss = 1.75709
I0521 06:31:38.745805  9072 solver.cpp:253]     Train net output #0: loss = 1.75709 (* 1 = 1.75709 loss)
I0521 06:31:38.745822  9072 sgd_solver.cpp:106] Iteration 1740, lr = 0.0025
I0521 06:32:08.887161  9072 solver.cpp:237] Iteration 1760, loss = 1.62964
I0521 06:32:08.887328  9072 solver.cpp:253]     Train net output #0: loss = 1.62964 (* 1 = 1.62964 loss)
I0521 06:32:08.887344  9072 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0521 06:32:16.826701  9072 solver.cpp:237] Iteration 1780, loss = 1.6993
I0521 06:32:16.826733  9072 solver.cpp:253]     Train net output #0: loss = 1.6993 (* 1 = 1.6993 loss)
I0521 06:32:16.826750  9072 sgd_solver.cpp:106] Iteration 1780, lr = 0.0025
I0521 06:32:24.764832  9072 solver.cpp:237] Iteration 1800, loss = 1.69533
I0521 06:32:24.764865  9072 solver.cpp:253]     Train net output #0: loss = 1.69533 (* 1 = 1.69533 loss)
I0521 06:32:24.764881  9072 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 06:32:31.512673  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1818.caffemodel
I0521 06:32:31.833283  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_1818.solverstate
I0521 06:32:32.773923  9072 solver.cpp:237] Iteration 1820, loss = 1.66928
I0521 06:32:32.773972  9072 solver.cpp:253]     Train net output #0: loss = 1.66928 (* 1 = 1.66928 loss)
I0521 06:32:32.773988  9072 sgd_solver.cpp:106] Iteration 1820, lr = 0.0025
I0521 06:32:40.717219  9072 solver.cpp:237] Iteration 1840, loss = 1.64022
I0521 06:32:40.717376  9072 solver.cpp:253]     Train net output #0: loss = 1.64022 (* 1 = 1.64022 loss)
I0521 06:32:40.717391  9072 sgd_solver.cpp:106] Iteration 1840, lr = 0.0025
I0521 06:32:48.659273  9072 solver.cpp:237] Iteration 1860, loss = 1.61226
I0521 06:32:48.659306  9072 solver.cpp:253]     Train net output #0: loss = 1.61226 (* 1 = 1.61226 loss)
I0521 06:32:48.659323  9072 sgd_solver.cpp:106] Iteration 1860, lr = 0.0025
I0521 06:32:56.598809  9072 solver.cpp:237] Iteration 1880, loss = 1.68985
I0521 06:32:56.598852  9072 solver.cpp:253]     Train net output #0: loss = 1.68985 (* 1 = 1.68985 loss)
I0521 06:32:56.598866  9072 sgd_solver.cpp:106] Iteration 1880, lr = 0.0025
I0521 06:33:26.777683  9072 solver.cpp:237] Iteration 1900, loss = 1.68701
I0521 06:33:26.777853  9072 solver.cpp:253]     Train net output #0: loss = 1.68701 (* 1 = 1.68701 loss)
I0521 06:33:26.777869  9072 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0521 06:33:34.718781  9072 solver.cpp:237] Iteration 1920, loss = 1.59425
I0521 06:33:34.718814  9072 solver.cpp:253]     Train net output #0: loss = 1.59425 (* 1 = 1.59425 loss)
I0521 06:33:34.718830  9072 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0521 06:33:42.659036  9072 solver.cpp:237] Iteration 1940, loss = 1.66632
I0521 06:33:42.659070  9072 solver.cpp:253]     Train net output #0: loss = 1.66632 (* 1 = 1.66632 loss)
I0521 06:33:42.659086  9072 sgd_solver.cpp:106] Iteration 1940, lr = 0.0025
I0521 06:33:50.594277  9072 solver.cpp:237] Iteration 1960, loss = 1.71182
I0521 06:33:50.594319  9072 solver.cpp:253]     Train net output #0: loss = 1.71182 (* 1 = 1.71182 loss)
I0521 06:33:50.594337  9072 sgd_solver.cpp:106] Iteration 1960, lr = 0.0025
I0521 06:33:58.533846  9072 solver.cpp:237] Iteration 1980, loss = 1.63135
I0521 06:33:58.533990  9072 solver.cpp:253]     Train net output #0: loss = 1.63135 (* 1 = 1.63135 loss)
I0521 06:33:58.534003  9072 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 06:34:06.469015  9072 solver.cpp:237] Iteration 2000, loss = 1.64392
I0521 06:34:06.469048  9072 solver.cpp:253]     Train net output #0: loss = 1.64392 (* 1 = 1.64392 loss)
I0521 06:34:06.469063  9072 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0521 06:34:14.011656  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_2020.caffemodel
I0521 06:34:14.335705  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_2020.solverstate
I0521 06:34:14.482784  9072 solver.cpp:237] Iteration 2020, loss = 1.76113
I0521 06:34:14.482833  9072 solver.cpp:253]     Train net output #0: loss = 1.76113 (* 1 = 1.76113 loss)
I0521 06:34:14.482851  9072 sgd_solver.cpp:106] Iteration 2020, lr = 0.0025
I0521 06:34:16.070590  9072 solver.cpp:341] Iteration 2025, Testing net (#0)
I0521 06:35:01.395630  9072 solver.cpp:409]     Test net output #0: accuracy = 0.672645
I0521 06:35:01.395800  9072 solver.cpp:409]     Test net output #1: loss = 1.13832 (* 1 = 1.13832 loss)
I0521 06:35:01.911325  9072 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_2027.caffemodel
I0521 06:35:02.231192  9072 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557_iter_2027.solverstate
I0521 06:35:02.259770  9072 solver.cpp:326] Optimization Done.
I0521 06:35:02.259799  9072 caffe.cpp:215] Optimization Done.
Application 11237066 resources: utime ~1249s, stime ~225s, Rss ~5332612, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_740_2016-05-20T11.20.59.642557.solver"
	User time (seconds): 0.54
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.56
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15081
	Voluntary context switches: 2712
	Involuntary context switches: 79
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
