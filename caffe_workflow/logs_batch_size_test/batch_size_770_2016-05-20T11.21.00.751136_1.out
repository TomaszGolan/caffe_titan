2806323
I0521 06:37:42.031301 19036 caffe.cpp:184] Using GPUs 0
I0521 06:37:42.453126 19036 solver.cpp:48] Initializing solver from parameters: 
test_iter: 194
test_interval: 389
base_lr: 0.0025
display: 19
max_iter: 1948
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 194
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136.prototxt"
I0521 06:37:42.454778 19036 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136.prototxt
I0521 06:37:42.473129 19036 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 06:37:42.473188 19036 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 06:37:42.473532 19036 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 770
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 06:37:42.473714 19036 layer_factory.hpp:77] Creating layer data_hdf5
I0521 06:37:42.473738 19036 net.cpp:106] Creating Layer data_hdf5
I0521 06:37:42.473752 19036 net.cpp:411] data_hdf5 -> data
I0521 06:37:42.473786 19036 net.cpp:411] data_hdf5 -> label
I0521 06:37:42.473820 19036 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 06:37:42.475177 19036 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 06:37:42.477427 19036 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 06:38:03.983042 19036 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 06:38:03.988158 19036 net.cpp:150] Setting up data_hdf5
I0521 06:38:03.988198 19036 net.cpp:157] Top shape: 770 1 127 50 (4889500)
I0521 06:38:03.988212 19036 net.cpp:157] Top shape: 770 (770)
I0521 06:38:03.988224 19036 net.cpp:165] Memory required for data: 19561080
I0521 06:38:03.988239 19036 layer_factory.hpp:77] Creating layer conv1
I0521 06:38:03.988272 19036 net.cpp:106] Creating Layer conv1
I0521 06:38:03.988283 19036 net.cpp:454] conv1 <- data
I0521 06:38:03.988307 19036 net.cpp:411] conv1 -> conv1
I0521 06:38:04.355200 19036 net.cpp:150] Setting up conv1
I0521 06:38:04.355245 19036 net.cpp:157] Top shape: 770 12 120 48 (53222400)
I0521 06:38:04.355257 19036 net.cpp:165] Memory required for data: 232450680
I0521 06:38:04.355286 19036 layer_factory.hpp:77] Creating layer relu1
I0521 06:38:04.355307 19036 net.cpp:106] Creating Layer relu1
I0521 06:38:04.355319 19036 net.cpp:454] relu1 <- conv1
I0521 06:38:04.355332 19036 net.cpp:397] relu1 -> conv1 (in-place)
I0521 06:38:04.355854 19036 net.cpp:150] Setting up relu1
I0521 06:38:04.355870 19036 net.cpp:157] Top shape: 770 12 120 48 (53222400)
I0521 06:38:04.355881 19036 net.cpp:165] Memory required for data: 445340280
I0521 06:38:04.355891 19036 layer_factory.hpp:77] Creating layer pool1
I0521 06:38:04.355908 19036 net.cpp:106] Creating Layer pool1
I0521 06:38:04.355918 19036 net.cpp:454] pool1 <- conv1
I0521 06:38:04.355933 19036 net.cpp:411] pool1 -> pool1
I0521 06:38:04.356014 19036 net.cpp:150] Setting up pool1
I0521 06:38:04.356029 19036 net.cpp:157] Top shape: 770 12 60 48 (26611200)
I0521 06:38:04.356039 19036 net.cpp:165] Memory required for data: 551785080
I0521 06:38:04.356048 19036 layer_factory.hpp:77] Creating layer conv2
I0521 06:38:04.356071 19036 net.cpp:106] Creating Layer conv2
I0521 06:38:04.356082 19036 net.cpp:454] conv2 <- pool1
I0521 06:38:04.356096 19036 net.cpp:411] conv2 -> conv2
I0521 06:38:04.358773 19036 net.cpp:150] Setting up conv2
I0521 06:38:04.358800 19036 net.cpp:157] Top shape: 770 20 54 46 (38253600)
I0521 06:38:04.358811 19036 net.cpp:165] Memory required for data: 704799480
I0521 06:38:04.358831 19036 layer_factory.hpp:77] Creating layer relu2
I0521 06:38:04.358846 19036 net.cpp:106] Creating Layer relu2
I0521 06:38:04.358856 19036 net.cpp:454] relu2 <- conv2
I0521 06:38:04.358870 19036 net.cpp:397] relu2 -> conv2 (in-place)
I0521 06:38:04.359199 19036 net.cpp:150] Setting up relu2
I0521 06:38:04.359212 19036 net.cpp:157] Top shape: 770 20 54 46 (38253600)
I0521 06:38:04.359223 19036 net.cpp:165] Memory required for data: 857813880
I0521 06:38:04.359233 19036 layer_factory.hpp:77] Creating layer pool2
I0521 06:38:04.359246 19036 net.cpp:106] Creating Layer pool2
I0521 06:38:04.359256 19036 net.cpp:454] pool2 <- conv2
I0521 06:38:04.359282 19036 net.cpp:411] pool2 -> pool2
I0521 06:38:04.359349 19036 net.cpp:150] Setting up pool2
I0521 06:38:04.359362 19036 net.cpp:157] Top shape: 770 20 27 46 (19126800)
I0521 06:38:04.359374 19036 net.cpp:165] Memory required for data: 934321080
I0521 06:38:04.359381 19036 layer_factory.hpp:77] Creating layer conv3
I0521 06:38:04.359400 19036 net.cpp:106] Creating Layer conv3
I0521 06:38:04.359411 19036 net.cpp:454] conv3 <- pool2
I0521 06:38:04.359424 19036 net.cpp:411] conv3 -> conv3
I0521 06:38:04.361352 19036 net.cpp:150] Setting up conv3
I0521 06:38:04.361377 19036 net.cpp:157] Top shape: 770 28 22 44 (20870080)
I0521 06:38:04.361387 19036 net.cpp:165] Memory required for data: 1017801400
I0521 06:38:04.361407 19036 layer_factory.hpp:77] Creating layer relu3
I0521 06:38:04.361423 19036 net.cpp:106] Creating Layer relu3
I0521 06:38:04.361433 19036 net.cpp:454] relu3 <- conv3
I0521 06:38:04.361445 19036 net.cpp:397] relu3 -> conv3 (in-place)
I0521 06:38:04.361914 19036 net.cpp:150] Setting up relu3
I0521 06:38:04.361932 19036 net.cpp:157] Top shape: 770 28 22 44 (20870080)
I0521 06:38:04.361943 19036 net.cpp:165] Memory required for data: 1101281720
I0521 06:38:04.361953 19036 layer_factory.hpp:77] Creating layer pool3
I0521 06:38:04.361966 19036 net.cpp:106] Creating Layer pool3
I0521 06:38:04.361975 19036 net.cpp:454] pool3 <- conv3
I0521 06:38:04.361989 19036 net.cpp:411] pool3 -> pool3
I0521 06:38:04.362056 19036 net.cpp:150] Setting up pool3
I0521 06:38:04.362069 19036 net.cpp:157] Top shape: 770 28 11 44 (10435040)
I0521 06:38:04.362079 19036 net.cpp:165] Memory required for data: 1143021880
I0521 06:38:04.362087 19036 layer_factory.hpp:77] Creating layer conv4
I0521 06:38:04.362105 19036 net.cpp:106] Creating Layer conv4
I0521 06:38:04.362115 19036 net.cpp:454] conv4 <- pool3
I0521 06:38:04.362129 19036 net.cpp:411] conv4 -> conv4
I0521 06:38:04.364935 19036 net.cpp:150] Setting up conv4
I0521 06:38:04.364964 19036 net.cpp:157] Top shape: 770 36 6 42 (6985440)
I0521 06:38:04.364977 19036 net.cpp:165] Memory required for data: 1170963640
I0521 06:38:04.364995 19036 layer_factory.hpp:77] Creating layer relu4
I0521 06:38:04.365008 19036 net.cpp:106] Creating Layer relu4
I0521 06:38:04.365020 19036 net.cpp:454] relu4 <- conv4
I0521 06:38:04.365032 19036 net.cpp:397] relu4 -> conv4 (in-place)
I0521 06:38:04.365507 19036 net.cpp:150] Setting up relu4
I0521 06:38:04.365523 19036 net.cpp:157] Top shape: 770 36 6 42 (6985440)
I0521 06:38:04.365535 19036 net.cpp:165] Memory required for data: 1198905400
I0521 06:38:04.365546 19036 layer_factory.hpp:77] Creating layer pool4
I0521 06:38:04.365559 19036 net.cpp:106] Creating Layer pool4
I0521 06:38:04.365569 19036 net.cpp:454] pool4 <- conv4
I0521 06:38:04.365583 19036 net.cpp:411] pool4 -> pool4
I0521 06:38:04.365651 19036 net.cpp:150] Setting up pool4
I0521 06:38:04.365665 19036 net.cpp:157] Top shape: 770 36 3 42 (3492720)
I0521 06:38:04.365676 19036 net.cpp:165] Memory required for data: 1212876280
I0521 06:38:04.365686 19036 layer_factory.hpp:77] Creating layer ip1
I0521 06:38:04.365706 19036 net.cpp:106] Creating Layer ip1
I0521 06:38:04.365717 19036 net.cpp:454] ip1 <- pool4
I0521 06:38:04.365730 19036 net.cpp:411] ip1 -> ip1
I0521 06:38:04.381199 19036 net.cpp:150] Setting up ip1
I0521 06:38:04.381228 19036 net.cpp:157] Top shape: 770 196 (150920)
I0521 06:38:04.381242 19036 net.cpp:165] Memory required for data: 1213479960
I0521 06:38:04.381263 19036 layer_factory.hpp:77] Creating layer relu5
I0521 06:38:04.381278 19036 net.cpp:106] Creating Layer relu5
I0521 06:38:04.381289 19036 net.cpp:454] relu5 <- ip1
I0521 06:38:04.381301 19036 net.cpp:397] relu5 -> ip1 (in-place)
I0521 06:38:04.381645 19036 net.cpp:150] Setting up relu5
I0521 06:38:04.381660 19036 net.cpp:157] Top shape: 770 196 (150920)
I0521 06:38:04.381670 19036 net.cpp:165] Memory required for data: 1214083640
I0521 06:38:04.381681 19036 layer_factory.hpp:77] Creating layer drop1
I0521 06:38:04.381706 19036 net.cpp:106] Creating Layer drop1
I0521 06:38:04.381716 19036 net.cpp:454] drop1 <- ip1
I0521 06:38:04.381742 19036 net.cpp:397] drop1 -> ip1 (in-place)
I0521 06:38:04.381816 19036 net.cpp:150] Setting up drop1
I0521 06:38:04.381830 19036 net.cpp:157] Top shape: 770 196 (150920)
I0521 06:38:04.381841 19036 net.cpp:165] Memory required for data: 1214687320
I0521 06:38:04.381850 19036 layer_factory.hpp:77] Creating layer ip2
I0521 06:38:04.381870 19036 net.cpp:106] Creating Layer ip2
I0521 06:38:04.381880 19036 net.cpp:454] ip2 <- ip1
I0521 06:38:04.381892 19036 net.cpp:411] ip2 -> ip2
I0521 06:38:04.382359 19036 net.cpp:150] Setting up ip2
I0521 06:38:04.382374 19036 net.cpp:157] Top shape: 770 98 (75460)
I0521 06:38:04.382383 19036 net.cpp:165] Memory required for data: 1214989160
I0521 06:38:04.382398 19036 layer_factory.hpp:77] Creating layer relu6
I0521 06:38:04.382411 19036 net.cpp:106] Creating Layer relu6
I0521 06:38:04.382421 19036 net.cpp:454] relu6 <- ip2
I0521 06:38:04.382432 19036 net.cpp:397] relu6 -> ip2 (in-place)
I0521 06:38:04.382958 19036 net.cpp:150] Setting up relu6
I0521 06:38:04.382974 19036 net.cpp:157] Top shape: 770 98 (75460)
I0521 06:38:04.382987 19036 net.cpp:165] Memory required for data: 1215291000
I0521 06:38:04.382997 19036 layer_factory.hpp:77] Creating layer drop2
I0521 06:38:04.383010 19036 net.cpp:106] Creating Layer drop2
I0521 06:38:04.383020 19036 net.cpp:454] drop2 <- ip2
I0521 06:38:04.383033 19036 net.cpp:397] drop2 -> ip2 (in-place)
I0521 06:38:04.383075 19036 net.cpp:150] Setting up drop2
I0521 06:38:04.383088 19036 net.cpp:157] Top shape: 770 98 (75460)
I0521 06:38:04.383098 19036 net.cpp:165] Memory required for data: 1215592840
I0521 06:38:04.383108 19036 layer_factory.hpp:77] Creating layer ip3
I0521 06:38:04.383121 19036 net.cpp:106] Creating Layer ip3
I0521 06:38:04.383132 19036 net.cpp:454] ip3 <- ip2
I0521 06:38:04.383146 19036 net.cpp:411] ip3 -> ip3
I0521 06:38:04.383355 19036 net.cpp:150] Setting up ip3
I0521 06:38:04.383368 19036 net.cpp:157] Top shape: 770 11 (8470)
I0521 06:38:04.383379 19036 net.cpp:165] Memory required for data: 1215626720
I0521 06:38:04.383394 19036 layer_factory.hpp:77] Creating layer drop3
I0521 06:38:04.383407 19036 net.cpp:106] Creating Layer drop3
I0521 06:38:04.383417 19036 net.cpp:454] drop3 <- ip3
I0521 06:38:04.383430 19036 net.cpp:397] drop3 -> ip3 (in-place)
I0521 06:38:04.383468 19036 net.cpp:150] Setting up drop3
I0521 06:38:04.383481 19036 net.cpp:157] Top shape: 770 11 (8470)
I0521 06:38:04.383491 19036 net.cpp:165] Memory required for data: 1215660600
I0521 06:38:04.383502 19036 layer_factory.hpp:77] Creating layer loss
I0521 06:38:04.383520 19036 net.cpp:106] Creating Layer loss
I0521 06:38:04.383530 19036 net.cpp:454] loss <- ip3
I0521 06:38:04.383540 19036 net.cpp:454] loss <- label
I0521 06:38:04.383553 19036 net.cpp:411] loss -> loss
I0521 06:38:04.383570 19036 layer_factory.hpp:77] Creating layer loss
I0521 06:38:04.384232 19036 net.cpp:150] Setting up loss
I0521 06:38:04.384253 19036 net.cpp:157] Top shape: (1)
I0521 06:38:04.384268 19036 net.cpp:160]     with loss weight 1
I0521 06:38:04.384313 19036 net.cpp:165] Memory required for data: 1215660604
I0521 06:38:04.384321 19036 net.cpp:226] loss needs backward computation.
I0521 06:38:04.384330 19036 net.cpp:226] drop3 needs backward computation.
I0521 06:38:04.384341 19036 net.cpp:226] ip3 needs backward computation.
I0521 06:38:04.384351 19036 net.cpp:226] drop2 needs backward computation.
I0521 06:38:04.384361 19036 net.cpp:226] relu6 needs backward computation.
I0521 06:38:04.384371 19036 net.cpp:226] ip2 needs backward computation.
I0521 06:38:04.384382 19036 net.cpp:226] drop1 needs backward computation.
I0521 06:38:04.384392 19036 net.cpp:226] relu5 needs backward computation.
I0521 06:38:04.384402 19036 net.cpp:226] ip1 needs backward computation.
I0521 06:38:04.384413 19036 net.cpp:226] pool4 needs backward computation.
I0521 06:38:04.384420 19036 net.cpp:226] relu4 needs backward computation.
I0521 06:38:04.384430 19036 net.cpp:226] conv4 needs backward computation.
I0521 06:38:04.384441 19036 net.cpp:226] pool3 needs backward computation.
I0521 06:38:04.384462 19036 net.cpp:226] relu3 needs backward computation.
I0521 06:38:04.384472 19036 net.cpp:226] conv3 needs backward computation.
I0521 06:38:04.384484 19036 net.cpp:226] pool2 needs backward computation.
I0521 06:38:04.384493 19036 net.cpp:226] relu2 needs backward computation.
I0521 06:38:04.384503 19036 net.cpp:226] conv2 needs backward computation.
I0521 06:38:04.384512 19036 net.cpp:226] pool1 needs backward computation.
I0521 06:38:04.384522 19036 net.cpp:226] relu1 needs backward computation.
I0521 06:38:04.384531 19036 net.cpp:226] conv1 needs backward computation.
I0521 06:38:04.384543 19036 net.cpp:228] data_hdf5 does not need backward computation.
I0521 06:38:04.384553 19036 net.cpp:270] This network produces output loss
I0521 06:38:04.384577 19036 net.cpp:283] Network initialization done.
I0521 06:38:04.386173 19036 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136.prototxt
I0521 06:38:04.386245 19036 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 06:38:04.386600 19036 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 770
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 06:38:04.386790 19036 layer_factory.hpp:77] Creating layer data_hdf5
I0521 06:38:04.386806 19036 net.cpp:106] Creating Layer data_hdf5
I0521 06:38:04.386818 19036 net.cpp:411] data_hdf5 -> data
I0521 06:38:04.386836 19036 net.cpp:411] data_hdf5 -> label
I0521 06:38:04.386852 19036 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 06:38:04.387995 19036 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 06:38:25.725605 19036 net.cpp:150] Setting up data_hdf5
I0521 06:38:25.725771 19036 net.cpp:157] Top shape: 770 1 127 50 (4889500)
I0521 06:38:25.725786 19036 net.cpp:157] Top shape: 770 (770)
I0521 06:38:25.725798 19036 net.cpp:165] Memory required for data: 19561080
I0521 06:38:25.725811 19036 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 06:38:25.725839 19036 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 06:38:25.725850 19036 net.cpp:454] label_data_hdf5_1_split <- label
I0521 06:38:25.725865 19036 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 06:38:25.725886 19036 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 06:38:25.725960 19036 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 06:38:25.725973 19036 net.cpp:157] Top shape: 770 (770)
I0521 06:38:25.725986 19036 net.cpp:157] Top shape: 770 (770)
I0521 06:38:25.725996 19036 net.cpp:165] Memory required for data: 19567240
I0521 06:38:25.726006 19036 layer_factory.hpp:77] Creating layer conv1
I0521 06:38:25.726027 19036 net.cpp:106] Creating Layer conv1
I0521 06:38:25.726037 19036 net.cpp:454] conv1 <- data
I0521 06:38:25.726052 19036 net.cpp:411] conv1 -> conv1
I0521 06:38:25.727991 19036 net.cpp:150] Setting up conv1
I0521 06:38:25.728015 19036 net.cpp:157] Top shape: 770 12 120 48 (53222400)
I0521 06:38:25.728027 19036 net.cpp:165] Memory required for data: 232456840
I0521 06:38:25.728049 19036 layer_factory.hpp:77] Creating layer relu1
I0521 06:38:25.728063 19036 net.cpp:106] Creating Layer relu1
I0521 06:38:25.728073 19036 net.cpp:454] relu1 <- conv1
I0521 06:38:25.728086 19036 net.cpp:397] relu1 -> conv1 (in-place)
I0521 06:38:25.728585 19036 net.cpp:150] Setting up relu1
I0521 06:38:25.728601 19036 net.cpp:157] Top shape: 770 12 120 48 (53222400)
I0521 06:38:25.728613 19036 net.cpp:165] Memory required for data: 445346440
I0521 06:38:25.728623 19036 layer_factory.hpp:77] Creating layer pool1
I0521 06:38:25.728639 19036 net.cpp:106] Creating Layer pool1
I0521 06:38:25.728649 19036 net.cpp:454] pool1 <- conv1
I0521 06:38:25.728660 19036 net.cpp:411] pool1 -> pool1
I0521 06:38:25.728735 19036 net.cpp:150] Setting up pool1
I0521 06:38:25.728749 19036 net.cpp:157] Top shape: 770 12 60 48 (26611200)
I0521 06:38:25.728759 19036 net.cpp:165] Memory required for data: 551791240
I0521 06:38:25.728770 19036 layer_factory.hpp:77] Creating layer conv2
I0521 06:38:25.728785 19036 net.cpp:106] Creating Layer conv2
I0521 06:38:25.728796 19036 net.cpp:454] conv2 <- pool1
I0521 06:38:25.728809 19036 net.cpp:411] conv2 -> conv2
I0521 06:38:25.730723 19036 net.cpp:150] Setting up conv2
I0521 06:38:25.730746 19036 net.cpp:157] Top shape: 770 20 54 46 (38253600)
I0521 06:38:25.730759 19036 net.cpp:165] Memory required for data: 704805640
I0521 06:38:25.730777 19036 layer_factory.hpp:77] Creating layer relu2
I0521 06:38:25.730789 19036 net.cpp:106] Creating Layer relu2
I0521 06:38:25.730799 19036 net.cpp:454] relu2 <- conv2
I0521 06:38:25.730813 19036 net.cpp:397] relu2 -> conv2 (in-place)
I0521 06:38:25.731144 19036 net.cpp:150] Setting up relu2
I0521 06:38:25.731158 19036 net.cpp:157] Top shape: 770 20 54 46 (38253600)
I0521 06:38:25.731168 19036 net.cpp:165] Memory required for data: 857820040
I0521 06:38:25.731178 19036 layer_factory.hpp:77] Creating layer pool2
I0521 06:38:25.731192 19036 net.cpp:106] Creating Layer pool2
I0521 06:38:25.731201 19036 net.cpp:454] pool2 <- conv2
I0521 06:38:25.731215 19036 net.cpp:411] pool2 -> pool2
I0521 06:38:25.731286 19036 net.cpp:150] Setting up pool2
I0521 06:38:25.731299 19036 net.cpp:157] Top shape: 770 20 27 46 (19126800)
I0521 06:38:25.731309 19036 net.cpp:165] Memory required for data: 934327240
I0521 06:38:25.731319 19036 layer_factory.hpp:77] Creating layer conv3
I0521 06:38:25.731336 19036 net.cpp:106] Creating Layer conv3
I0521 06:38:25.731346 19036 net.cpp:454] conv3 <- pool2
I0521 06:38:25.731360 19036 net.cpp:411] conv3 -> conv3
I0521 06:38:25.733345 19036 net.cpp:150] Setting up conv3
I0521 06:38:25.733367 19036 net.cpp:157] Top shape: 770 28 22 44 (20870080)
I0521 06:38:25.733377 19036 net.cpp:165] Memory required for data: 1017807560
I0521 06:38:25.733410 19036 layer_factory.hpp:77] Creating layer relu3
I0521 06:38:25.733424 19036 net.cpp:106] Creating Layer relu3
I0521 06:38:25.733434 19036 net.cpp:454] relu3 <- conv3
I0521 06:38:25.733448 19036 net.cpp:397] relu3 -> conv3 (in-place)
I0521 06:38:25.733918 19036 net.cpp:150] Setting up relu3
I0521 06:38:25.733933 19036 net.cpp:157] Top shape: 770 28 22 44 (20870080)
I0521 06:38:25.733944 19036 net.cpp:165] Memory required for data: 1101287880
I0521 06:38:25.733954 19036 layer_factory.hpp:77] Creating layer pool3
I0521 06:38:25.733968 19036 net.cpp:106] Creating Layer pool3
I0521 06:38:25.733978 19036 net.cpp:454] pool3 <- conv3
I0521 06:38:25.733990 19036 net.cpp:411] pool3 -> pool3
I0521 06:38:25.734062 19036 net.cpp:150] Setting up pool3
I0521 06:38:25.734076 19036 net.cpp:157] Top shape: 770 28 11 44 (10435040)
I0521 06:38:25.734086 19036 net.cpp:165] Memory required for data: 1143028040
I0521 06:38:25.734097 19036 layer_factory.hpp:77] Creating layer conv4
I0521 06:38:25.734113 19036 net.cpp:106] Creating Layer conv4
I0521 06:38:25.734124 19036 net.cpp:454] conv4 <- pool3
I0521 06:38:25.734138 19036 net.cpp:411] conv4 -> conv4
I0521 06:38:25.736191 19036 net.cpp:150] Setting up conv4
I0521 06:38:25.736212 19036 net.cpp:157] Top shape: 770 36 6 42 (6985440)
I0521 06:38:25.736225 19036 net.cpp:165] Memory required for data: 1170969800
I0521 06:38:25.736240 19036 layer_factory.hpp:77] Creating layer relu4
I0521 06:38:25.736253 19036 net.cpp:106] Creating Layer relu4
I0521 06:38:25.736263 19036 net.cpp:454] relu4 <- conv4
I0521 06:38:25.736276 19036 net.cpp:397] relu4 -> conv4 (in-place)
I0521 06:38:25.736752 19036 net.cpp:150] Setting up relu4
I0521 06:38:25.736768 19036 net.cpp:157] Top shape: 770 36 6 42 (6985440)
I0521 06:38:25.736779 19036 net.cpp:165] Memory required for data: 1198911560
I0521 06:38:25.736789 19036 layer_factory.hpp:77] Creating layer pool4
I0521 06:38:25.736802 19036 net.cpp:106] Creating Layer pool4
I0521 06:38:25.736812 19036 net.cpp:454] pool4 <- conv4
I0521 06:38:25.736825 19036 net.cpp:411] pool4 -> pool4
I0521 06:38:25.736897 19036 net.cpp:150] Setting up pool4
I0521 06:38:25.736910 19036 net.cpp:157] Top shape: 770 36 3 42 (3492720)
I0521 06:38:25.736920 19036 net.cpp:165] Memory required for data: 1212882440
I0521 06:38:25.736928 19036 layer_factory.hpp:77] Creating layer ip1
I0521 06:38:25.736944 19036 net.cpp:106] Creating Layer ip1
I0521 06:38:25.736954 19036 net.cpp:454] ip1 <- pool4
I0521 06:38:25.736968 19036 net.cpp:411] ip1 -> ip1
I0521 06:38:25.752418 19036 net.cpp:150] Setting up ip1
I0521 06:38:25.752445 19036 net.cpp:157] Top shape: 770 196 (150920)
I0521 06:38:25.752457 19036 net.cpp:165] Memory required for data: 1213486120
I0521 06:38:25.752480 19036 layer_factory.hpp:77] Creating layer relu5
I0521 06:38:25.752495 19036 net.cpp:106] Creating Layer relu5
I0521 06:38:25.752504 19036 net.cpp:454] relu5 <- ip1
I0521 06:38:25.752518 19036 net.cpp:397] relu5 -> ip1 (in-place)
I0521 06:38:25.752863 19036 net.cpp:150] Setting up relu5
I0521 06:38:25.752877 19036 net.cpp:157] Top shape: 770 196 (150920)
I0521 06:38:25.752887 19036 net.cpp:165] Memory required for data: 1214089800
I0521 06:38:25.752897 19036 layer_factory.hpp:77] Creating layer drop1
I0521 06:38:25.752917 19036 net.cpp:106] Creating Layer drop1
I0521 06:38:25.752926 19036 net.cpp:454] drop1 <- ip1
I0521 06:38:25.752939 19036 net.cpp:397] drop1 -> ip1 (in-place)
I0521 06:38:25.752984 19036 net.cpp:150] Setting up drop1
I0521 06:38:25.752996 19036 net.cpp:157] Top shape: 770 196 (150920)
I0521 06:38:25.753006 19036 net.cpp:165] Memory required for data: 1214693480
I0521 06:38:25.753015 19036 layer_factory.hpp:77] Creating layer ip2
I0521 06:38:25.753031 19036 net.cpp:106] Creating Layer ip2
I0521 06:38:25.753041 19036 net.cpp:454] ip2 <- ip1
I0521 06:38:25.753053 19036 net.cpp:411] ip2 -> ip2
I0521 06:38:25.753532 19036 net.cpp:150] Setting up ip2
I0521 06:38:25.753545 19036 net.cpp:157] Top shape: 770 98 (75460)
I0521 06:38:25.753556 19036 net.cpp:165] Memory required for data: 1214995320
I0521 06:38:25.753583 19036 layer_factory.hpp:77] Creating layer relu6
I0521 06:38:25.753597 19036 net.cpp:106] Creating Layer relu6
I0521 06:38:25.753607 19036 net.cpp:454] relu6 <- ip2
I0521 06:38:25.753618 19036 net.cpp:397] relu6 -> ip2 (in-place)
I0521 06:38:25.754154 19036 net.cpp:150] Setting up relu6
I0521 06:38:25.754170 19036 net.cpp:157] Top shape: 770 98 (75460)
I0521 06:38:25.754180 19036 net.cpp:165] Memory required for data: 1215297160
I0521 06:38:25.754190 19036 layer_factory.hpp:77] Creating layer drop2
I0521 06:38:25.754204 19036 net.cpp:106] Creating Layer drop2
I0521 06:38:25.754215 19036 net.cpp:454] drop2 <- ip2
I0521 06:38:25.754227 19036 net.cpp:397] drop2 -> ip2 (in-place)
I0521 06:38:25.754271 19036 net.cpp:150] Setting up drop2
I0521 06:38:25.754284 19036 net.cpp:157] Top shape: 770 98 (75460)
I0521 06:38:25.754294 19036 net.cpp:165] Memory required for data: 1215599000
I0521 06:38:25.754304 19036 layer_factory.hpp:77] Creating layer ip3
I0521 06:38:25.754319 19036 net.cpp:106] Creating Layer ip3
I0521 06:38:25.754329 19036 net.cpp:454] ip3 <- ip2
I0521 06:38:25.754343 19036 net.cpp:411] ip3 -> ip3
I0521 06:38:25.754567 19036 net.cpp:150] Setting up ip3
I0521 06:38:25.754581 19036 net.cpp:157] Top shape: 770 11 (8470)
I0521 06:38:25.754590 19036 net.cpp:165] Memory required for data: 1215632880
I0521 06:38:25.754606 19036 layer_factory.hpp:77] Creating layer drop3
I0521 06:38:25.754621 19036 net.cpp:106] Creating Layer drop3
I0521 06:38:25.754631 19036 net.cpp:454] drop3 <- ip3
I0521 06:38:25.754643 19036 net.cpp:397] drop3 -> ip3 (in-place)
I0521 06:38:25.754685 19036 net.cpp:150] Setting up drop3
I0521 06:38:25.754698 19036 net.cpp:157] Top shape: 770 11 (8470)
I0521 06:38:25.754709 19036 net.cpp:165] Memory required for data: 1215666760
I0521 06:38:25.754719 19036 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 06:38:25.754730 19036 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 06:38:25.754740 19036 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 06:38:25.754753 19036 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 06:38:25.754768 19036 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 06:38:25.754843 19036 net.cpp:150] Setting up ip3_drop3_0_split
I0521 06:38:25.754855 19036 net.cpp:157] Top shape: 770 11 (8470)
I0521 06:38:25.754868 19036 net.cpp:157] Top shape: 770 11 (8470)
I0521 06:38:25.754878 19036 net.cpp:165] Memory required for data: 1215734520
I0521 06:38:25.754886 19036 layer_factory.hpp:77] Creating layer accuracy
I0521 06:38:25.754909 19036 net.cpp:106] Creating Layer accuracy
I0521 06:38:25.754920 19036 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 06:38:25.754930 19036 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 06:38:25.754945 19036 net.cpp:411] accuracy -> accuracy
I0521 06:38:25.754968 19036 net.cpp:150] Setting up accuracy
I0521 06:38:25.754981 19036 net.cpp:157] Top shape: (1)
I0521 06:38:25.754992 19036 net.cpp:165] Memory required for data: 1215734524
I0521 06:38:25.755002 19036 layer_factory.hpp:77] Creating layer loss
I0521 06:38:25.755017 19036 net.cpp:106] Creating Layer loss
I0521 06:38:25.755026 19036 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 06:38:25.755038 19036 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 06:38:25.755050 19036 net.cpp:411] loss -> loss
I0521 06:38:25.755069 19036 layer_factory.hpp:77] Creating layer loss
I0521 06:38:25.755563 19036 net.cpp:150] Setting up loss
I0521 06:38:25.755575 19036 net.cpp:157] Top shape: (1)
I0521 06:38:25.755584 19036 net.cpp:160]     with loss weight 1
I0521 06:38:25.755606 19036 net.cpp:165] Memory required for data: 1215734528
I0521 06:38:25.755617 19036 net.cpp:226] loss needs backward computation.
I0521 06:38:25.755628 19036 net.cpp:228] accuracy does not need backward computation.
I0521 06:38:25.755640 19036 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 06:38:25.755650 19036 net.cpp:226] drop3 needs backward computation.
I0521 06:38:25.755661 19036 net.cpp:226] ip3 needs backward computation.
I0521 06:38:25.755671 19036 net.cpp:226] drop2 needs backward computation.
I0521 06:38:25.755689 19036 net.cpp:226] relu6 needs backward computation.
I0521 06:38:25.755698 19036 net.cpp:226] ip2 needs backward computation.
I0521 06:38:25.755708 19036 net.cpp:226] drop1 needs backward computation.
I0521 06:38:25.755718 19036 net.cpp:226] relu5 needs backward computation.
I0521 06:38:25.755728 19036 net.cpp:226] ip1 needs backward computation.
I0521 06:38:25.755736 19036 net.cpp:226] pool4 needs backward computation.
I0521 06:38:25.755748 19036 net.cpp:226] relu4 needs backward computation.
I0521 06:38:25.755758 19036 net.cpp:226] conv4 needs backward computation.
I0521 06:38:25.755769 19036 net.cpp:226] pool3 needs backward computation.
I0521 06:38:25.755779 19036 net.cpp:226] relu3 needs backward computation.
I0521 06:38:25.755789 19036 net.cpp:226] conv3 needs backward computation.
I0521 06:38:25.755798 19036 net.cpp:226] pool2 needs backward computation.
I0521 06:38:25.755810 19036 net.cpp:226] relu2 needs backward computation.
I0521 06:38:25.755826 19036 net.cpp:226] conv2 needs backward computation.
I0521 06:38:25.755837 19036 net.cpp:226] pool1 needs backward computation.
I0521 06:38:25.755847 19036 net.cpp:226] relu1 needs backward computation.
I0521 06:38:25.755857 19036 net.cpp:226] conv1 needs backward computation.
I0521 06:38:25.755869 19036 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 06:38:25.755882 19036 net.cpp:228] data_hdf5 does not need backward computation.
I0521 06:38:25.755892 19036 net.cpp:270] This network produces output accuracy
I0521 06:38:25.755902 19036 net.cpp:270] This network produces output loss
I0521 06:38:25.755929 19036 net.cpp:283] Network initialization done.
I0521 06:38:25.756060 19036 solver.cpp:60] Solver scaffolding done.
I0521 06:38:25.757194 19036 caffe.cpp:212] Starting Optimization
I0521 06:38:25.757212 19036 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 06:38:25.757226 19036 solver.cpp:289] Learning Rate Policy: fixed
I0521 06:38:25.758446 19036 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 06:39:11.712697 19036 solver.cpp:409]     Test net output #0: accuracy = 0.118369
I0521 06:39:11.712870 19036 solver.cpp:409]     Test net output #1: loss = 2.39851 (* 1 = 2.39851 loss)
I0521 06:39:11.856698 19036 solver.cpp:237] Iteration 0, loss = 2.39789
I0521 06:39:11.856734 19036 solver.cpp:253]     Train net output #0: loss = 2.39789 (* 1 = 2.39789 loss)
I0521 06:39:11.856753 19036 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 06:39:19.721717 19036 solver.cpp:237] Iteration 19, loss = 2.38659
I0521 06:39:19.721760 19036 solver.cpp:253]     Train net output #0: loss = 2.38659 (* 1 = 2.38659 loss)
I0521 06:39:19.721776 19036 sgd_solver.cpp:106] Iteration 19, lr = 0.0025
I0521 06:39:27.585687 19036 solver.cpp:237] Iteration 38, loss = 2.3727
I0521 06:39:27.585721 19036 solver.cpp:253]     Train net output #0: loss = 2.3727 (* 1 = 2.3727 loss)
I0521 06:39:27.585734 19036 sgd_solver.cpp:106] Iteration 38, lr = 0.0025
I0521 06:39:35.451606 19036 solver.cpp:237] Iteration 57, loss = 2.36326
I0521 06:39:35.451639 19036 solver.cpp:253]     Train net output #0: loss = 2.36326 (* 1 = 2.36326 loss)
I0521 06:39:35.451653 19036 sgd_solver.cpp:106] Iteration 57, lr = 0.0025
I0521 06:39:43.320436 19036 solver.cpp:237] Iteration 76, loss = 2.34502
I0521 06:39:43.320591 19036 solver.cpp:253]     Train net output #0: loss = 2.34502 (* 1 = 2.34502 loss)
I0521 06:39:43.320605 19036 sgd_solver.cpp:106] Iteration 76, lr = 0.0025
I0521 06:39:51.183430 19036 solver.cpp:237] Iteration 95, loss = 2.33356
I0521 06:39:51.183462 19036 solver.cpp:253]     Train net output #0: loss = 2.33356 (* 1 = 2.33356 loss)
I0521 06:39:51.183480 19036 sgd_solver.cpp:106] Iteration 95, lr = 0.0025
I0521 06:39:59.046788 19036 solver.cpp:237] Iteration 114, loss = 2.32721
I0521 06:39:59.046821 19036 solver.cpp:253]     Train net output #0: loss = 2.32721 (* 1 = 2.32721 loss)
I0521 06:39:59.046838 19036 sgd_solver.cpp:106] Iteration 114, lr = 0.0025
I0521 06:40:29.021317 19036 solver.cpp:237] Iteration 133, loss = 2.32732
I0521 06:40:29.021478 19036 solver.cpp:253]     Train net output #0: loss = 2.32732 (* 1 = 2.32732 loss)
I0521 06:40:29.021492 19036 sgd_solver.cpp:106] Iteration 133, lr = 0.0025
I0521 06:40:36.895278 19036 solver.cpp:237] Iteration 152, loss = 2.31406
I0521 06:40:36.895311 19036 solver.cpp:253]     Train net output #0: loss = 2.31406 (* 1 = 2.31406 loss)
I0521 06:40:36.895328 19036 sgd_solver.cpp:106] Iteration 152, lr = 0.0025
I0521 06:40:44.759698 19036 solver.cpp:237] Iteration 171, loss = 2.32072
I0521 06:40:44.759733 19036 solver.cpp:253]     Train net output #0: loss = 2.32072 (* 1 = 2.32072 loss)
I0521 06:40:44.759749 19036 sgd_solver.cpp:106] Iteration 171, lr = 0.0025
I0521 06:40:52.624893 19036 solver.cpp:237] Iteration 190, loss = 2.31139
I0521 06:40:52.624927 19036 solver.cpp:253]     Train net output #0: loss = 2.31139 (* 1 = 2.31139 loss)
I0521 06:40:52.624943 19036 sgd_solver.cpp:106] Iteration 190, lr = 0.0025
I0521 06:40:53.869570 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_194.caffemodel
I0521 06:40:54.200420 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_194.solverstate
I0521 06:41:00.560869 19036 solver.cpp:237] Iteration 209, loss = 2.32323
I0521 06:41:00.561024 19036 solver.cpp:253]     Train net output #0: loss = 2.32323 (* 1 = 2.32323 loss)
I0521 06:41:00.561038 19036 sgd_solver.cpp:106] Iteration 209, lr = 0.0025
I0521 06:41:08.430538 19036 solver.cpp:237] Iteration 228, loss = 2.30465
I0521 06:41:08.430570 19036 solver.cpp:253]     Train net output #0: loss = 2.30465 (* 1 = 2.30465 loss)
I0521 06:41:08.430588 19036 sgd_solver.cpp:106] Iteration 228, lr = 0.0025
I0521 06:41:16.298085 19036 solver.cpp:237] Iteration 247, loss = 2.28697
I0521 06:41:16.298118 19036 solver.cpp:253]     Train net output #0: loss = 2.28697 (* 1 = 2.28697 loss)
I0521 06:41:16.298135 19036 sgd_solver.cpp:106] Iteration 247, lr = 0.0025
I0521 06:41:46.297538 19036 solver.cpp:237] Iteration 266, loss = 2.26459
I0521 06:41:46.297706 19036 solver.cpp:253]     Train net output #0: loss = 2.26459 (* 1 = 2.26459 loss)
I0521 06:41:46.297721 19036 sgd_solver.cpp:106] Iteration 266, lr = 0.0025
I0521 06:41:54.166623 19036 solver.cpp:237] Iteration 285, loss = 2.25996
I0521 06:41:54.166656 19036 solver.cpp:253]     Train net output #0: loss = 2.25996 (* 1 = 2.25996 loss)
I0521 06:41:54.166674 19036 sgd_solver.cpp:106] Iteration 285, lr = 0.0025
I0521 06:42:02.033697 19036 solver.cpp:237] Iteration 304, loss = 2.24347
I0521 06:42:02.033732 19036 solver.cpp:253]     Train net output #0: loss = 2.24347 (* 1 = 2.24347 loss)
I0521 06:42:02.033746 19036 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0521 06:42:09.904098 19036 solver.cpp:237] Iteration 323, loss = 2.20687
I0521 06:42:09.904145 19036 solver.cpp:253]     Train net output #0: loss = 2.20687 (* 1 = 2.20687 loss)
I0521 06:42:09.904160 19036 sgd_solver.cpp:106] Iteration 323, lr = 0.0025
I0521 06:42:17.774026 19036 solver.cpp:237] Iteration 342, loss = 2.20628
I0521 06:42:17.774181 19036 solver.cpp:253]     Train net output #0: loss = 2.20628 (* 1 = 2.20628 loss)
I0521 06:42:17.774195 19036 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0521 06:42:25.638851 19036 solver.cpp:237] Iteration 361, loss = 2.15992
I0521 06:42:25.638885 19036 solver.cpp:253]     Train net output #0: loss = 2.15992 (* 1 = 2.15992 loss)
I0521 06:42:25.638902 19036 sgd_solver.cpp:106] Iteration 361, lr = 0.0025
I0521 06:42:33.505646 19036 solver.cpp:237] Iteration 380, loss = 2.11061
I0521 06:42:33.505697 19036 solver.cpp:253]     Train net output #0: loss = 2.11061 (* 1 = 2.11061 loss)
I0521 06:42:33.505715 19036 sgd_solver.cpp:106] Iteration 380, lr = 0.0025
I0521 06:42:36.405158 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_388.caffemodel
I0521 06:42:36.734133 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_388.solverstate
I0521 06:42:36.883563 19036 solver.cpp:341] Iteration 389, Testing net (#0)
I0521 06:43:22.226260 19036 solver.cpp:409]     Test net output #0: accuracy = 0.458
I0521 06:43:22.226425 19036 solver.cpp:409]     Test net output #1: loss = 1.96012 (* 1 = 1.96012 loss)
I0521 06:43:48.616578 19036 solver.cpp:237] Iteration 399, loss = 2.10652
I0521 06:43:48.616634 19036 solver.cpp:253]     Train net output #0: loss = 2.10652 (* 1 = 2.10652 loss)
I0521 06:43:48.616649 19036 sgd_solver.cpp:106] Iteration 399, lr = 0.0025
I0521 06:43:56.485069 19036 solver.cpp:237] Iteration 418, loss = 2.07568
I0521 06:43:56.485222 19036 solver.cpp:253]     Train net output #0: loss = 2.07568 (* 1 = 2.07568 loss)
I0521 06:43:56.485236 19036 sgd_solver.cpp:106] Iteration 418, lr = 0.0025
I0521 06:44:04.351250 19036 solver.cpp:237] Iteration 437, loss = 2.06631
I0521 06:44:04.351282 19036 solver.cpp:253]     Train net output #0: loss = 2.06631 (* 1 = 2.06631 loss)
I0521 06:44:04.351300 19036 sgd_solver.cpp:106] Iteration 437, lr = 0.0025
I0521 06:44:12.220018 19036 solver.cpp:237] Iteration 456, loss = 2.03828
I0521 06:44:12.220052 19036 solver.cpp:253]     Train net output #0: loss = 2.03828 (* 1 = 2.03828 loss)
I0521 06:44:12.220067 19036 sgd_solver.cpp:106] Iteration 456, lr = 0.0025
I0521 06:44:20.094689 19036 solver.cpp:237] Iteration 475, loss = 2.01667
I0521 06:44:20.094738 19036 solver.cpp:253]     Train net output #0: loss = 2.01667 (* 1 = 2.01667 loss)
I0521 06:44:20.094756 19036 sgd_solver.cpp:106] Iteration 475, lr = 0.0025
I0521 06:44:27.963353 19036 solver.cpp:237] Iteration 494, loss = 2.01244
I0521 06:44:27.963487 19036 solver.cpp:253]     Train net output #0: loss = 2.01244 (* 1 = 2.01244 loss)
I0521 06:44:27.963500 19036 sgd_solver.cpp:106] Iteration 494, lr = 0.0025
I0521 06:44:35.837788 19036 solver.cpp:237] Iteration 513, loss = 2.00753
I0521 06:44:35.837821 19036 solver.cpp:253]     Train net output #0: loss = 2.00753 (* 1 = 2.00753 loss)
I0521 06:44:35.837838 19036 sgd_solver.cpp:106] Iteration 513, lr = 0.0025
I0521 06:45:05.846231 19036 solver.cpp:237] Iteration 532, loss = 1.98895
I0521 06:45:05.846406 19036 solver.cpp:253]     Train net output #0: loss = 1.98895 (* 1 = 1.98895 loss)
I0521 06:45:05.846421 19036 sgd_solver.cpp:106] Iteration 532, lr = 0.0025
I0521 06:45:13.716434 19036 solver.cpp:237] Iteration 551, loss = 1.94958
I0521 06:45:13.716467 19036 solver.cpp:253]     Train net output #0: loss = 1.94958 (* 1 = 1.94958 loss)
I0521 06:45:13.716485 19036 sgd_solver.cpp:106] Iteration 551, lr = 0.0025
I0521 06:45:21.586590 19036 solver.cpp:237] Iteration 570, loss = 1.94122
I0521 06:45:21.586623 19036 solver.cpp:253]     Train net output #0: loss = 1.94122 (* 1 = 1.94122 loss)
I0521 06:45:21.586639 19036 sgd_solver.cpp:106] Iteration 570, lr = 0.0025
I0521 06:45:26.138543 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_582.caffemodel
I0521 06:45:26.470480 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_582.solverstate
I0521 06:45:29.520313 19036 solver.cpp:237] Iteration 589, loss = 1.97884
I0521 06:45:29.520364 19036 solver.cpp:253]     Train net output #0: loss = 1.97884 (* 1 = 1.97884 loss)
I0521 06:45:29.520383 19036 sgd_solver.cpp:106] Iteration 589, lr = 0.0025
I0521 06:45:37.387819 19036 solver.cpp:237] Iteration 608, loss = 1.88349
I0521 06:45:37.387959 19036 solver.cpp:253]     Train net output #0: loss = 1.88349 (* 1 = 1.88349 loss)
I0521 06:45:37.387974 19036 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 06:45:45.256108 19036 solver.cpp:237] Iteration 627, loss = 1.9033
I0521 06:45:45.256140 19036 solver.cpp:253]     Train net output #0: loss = 1.9033 (* 1 = 1.9033 loss)
I0521 06:45:45.256157 19036 sgd_solver.cpp:106] Iteration 627, lr = 0.0025
I0521 06:45:53.124724 19036 solver.cpp:237] Iteration 646, loss = 1.94854
I0521 06:45:53.124758 19036 solver.cpp:253]     Train net output #0: loss = 1.94854 (* 1 = 1.94854 loss)
I0521 06:45:53.124773 19036 sgd_solver.cpp:106] Iteration 646, lr = 0.0025
I0521 06:46:23.149307 19036 solver.cpp:237] Iteration 665, loss = 1.9222
I0521 06:46:23.149484 19036 solver.cpp:253]     Train net output #0: loss = 1.9222 (* 1 = 1.9222 loss)
I0521 06:46:23.149499 19036 sgd_solver.cpp:106] Iteration 665, lr = 0.0025
I0521 06:46:31.019327 19036 solver.cpp:237] Iteration 684, loss = 1.90897
I0521 06:46:31.019361 19036 solver.cpp:253]     Train net output #0: loss = 1.90897 (* 1 = 1.90897 loss)
I0521 06:46:31.019378 19036 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0521 06:46:38.885421 19036 solver.cpp:237] Iteration 703, loss = 1.8623
I0521 06:46:38.885454 19036 solver.cpp:253]     Train net output #0: loss = 1.8623 (* 1 = 1.8623 loss)
I0521 06:46:38.885471 19036 sgd_solver.cpp:106] Iteration 703, lr = 0.0025
I0521 06:46:46.759768 19036 solver.cpp:237] Iteration 722, loss = 1.87235
I0521 06:46:46.759800 19036 solver.cpp:253]     Train net output #0: loss = 1.87235 (* 1 = 1.87235 loss)
I0521 06:46:46.759824 19036 sgd_solver.cpp:106] Iteration 722, lr = 0.0025
I0521 06:46:54.628530 19036 solver.cpp:237] Iteration 741, loss = 1.86074
I0521 06:46:54.628671 19036 solver.cpp:253]     Train net output #0: loss = 1.86074 (* 1 = 1.86074 loss)
I0521 06:46:54.628684 19036 sgd_solver.cpp:106] Iteration 741, lr = 0.0025
I0521 06:47:02.497798 19036 solver.cpp:237] Iteration 760, loss = 1.88204
I0521 06:47:02.497830 19036 solver.cpp:253]     Train net output #0: loss = 1.88204 (* 1 = 1.88204 loss)
I0521 06:47:02.497848 19036 sgd_solver.cpp:106] Iteration 760, lr = 0.0025
I0521 06:47:08.706001 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_776.caffemodel
I0521 06:47:09.037199 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_776.solverstate
I0521 06:47:09.601529 19036 solver.cpp:341] Iteration 778, Testing net (#0)
I0521 06:48:15.851657 19036 solver.cpp:409]     Test net output #0: accuracy = 0.567446
I0521 06:48:15.851840 19036 solver.cpp:409]     Test net output #1: loss = 1.47295 (* 1 = 1.47295 loss)
I0521 06:48:38.531962 19036 solver.cpp:237] Iteration 779, loss = 1.93341
I0521 06:48:38.532016 19036 solver.cpp:253]     Train net output #0: loss = 1.93341 (* 1 = 1.93341 loss)
I0521 06:48:38.532029 19036 sgd_solver.cpp:106] Iteration 779, lr = 0.0025
I0521 06:48:46.387778 19036 solver.cpp:237] Iteration 798, loss = 1.84828
I0521 06:48:46.387938 19036 solver.cpp:253]     Train net output #0: loss = 1.84828 (* 1 = 1.84828 loss)
I0521 06:48:46.387953 19036 sgd_solver.cpp:106] Iteration 798, lr = 0.0025
I0521 06:48:54.240346 19036 solver.cpp:237] Iteration 817, loss = 1.8247
I0521 06:48:54.240386 19036 solver.cpp:253]     Train net output #0: loss = 1.8247 (* 1 = 1.8247 loss)
I0521 06:48:54.240402 19036 sgd_solver.cpp:106] Iteration 817, lr = 0.0025
I0521 06:49:02.098649 19036 solver.cpp:237] Iteration 836, loss = 1.83972
I0521 06:49:02.098683 19036 solver.cpp:253]     Train net output #0: loss = 1.83972 (* 1 = 1.83972 loss)
I0521 06:49:02.098696 19036 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0521 06:49:09.955346 19036 solver.cpp:237] Iteration 855, loss = 1.87546
I0521 06:49:09.955377 19036 solver.cpp:253]     Train net output #0: loss = 1.87546 (* 1 = 1.87546 loss)
I0521 06:49:09.955394 19036 sgd_solver.cpp:106] Iteration 855, lr = 0.0025
I0521 06:49:17.808658 19036 solver.cpp:237] Iteration 874, loss = 1.82782
I0521 06:49:17.808811 19036 solver.cpp:253]     Train net output #0: loss = 1.82782 (* 1 = 1.82782 loss)
I0521 06:49:17.808825 19036 sgd_solver.cpp:106] Iteration 874, lr = 0.0025
I0521 06:49:25.667218 19036 solver.cpp:237] Iteration 893, loss = 1.80084
I0521 06:49:25.667251 19036 solver.cpp:253]     Train net output #0: loss = 1.80084 (* 1 = 1.80084 loss)
I0521 06:49:25.667268 19036 sgd_solver.cpp:106] Iteration 893, lr = 0.0025
I0521 06:49:55.692605 19036 solver.cpp:237] Iteration 912, loss = 1.89056
I0521 06:49:55.692776 19036 solver.cpp:253]     Train net output #0: loss = 1.89056 (* 1 = 1.89056 loss)
I0521 06:49:55.692791 19036 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 06:50:03.551434 19036 solver.cpp:237] Iteration 931, loss = 1.75587
I0521 06:50:03.551479 19036 solver.cpp:253]     Train net output #0: loss = 1.75587 (* 1 = 1.75587 loss)
I0521 06:50:03.551496 19036 sgd_solver.cpp:106] Iteration 931, lr = 0.0025
I0521 06:50:11.405572 19036 solver.cpp:237] Iteration 950, loss = 1.83811
I0521 06:50:11.405606 19036 solver.cpp:253]     Train net output #0: loss = 1.83811 (* 1 = 1.83811 loss)
I0521 06:50:11.405622 19036 sgd_solver.cpp:106] Iteration 950, lr = 0.0025
I0521 06:50:19.259516 19036 solver.cpp:237] Iteration 969, loss = 1.83135
I0521 06:50:19.259547 19036 solver.cpp:253]     Train net output #0: loss = 1.83135 (* 1 = 1.83135 loss)
I0521 06:50:19.259564 19036 sgd_solver.cpp:106] Iteration 969, lr = 0.0025
I0521 06:50:19.259953 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_970.caffemodel
I0521 06:50:19.589576 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_970.solverstate
I0521 06:50:27.183717 19036 solver.cpp:237] Iteration 988, loss = 1.86915
I0521 06:50:27.183898 19036 solver.cpp:253]     Train net output #0: loss = 1.86915 (* 1 = 1.86915 loss)
I0521 06:50:27.183913 19036 sgd_solver.cpp:106] Iteration 988, lr = 0.0025
I0521 06:50:35.041774 19036 solver.cpp:237] Iteration 1007, loss = 1.90374
I0521 06:50:35.041821 19036 solver.cpp:253]     Train net output #0: loss = 1.90374 (* 1 = 1.90374 loss)
I0521 06:50:35.041841 19036 sgd_solver.cpp:106] Iteration 1007, lr = 0.0025
I0521 06:50:42.903983 19036 solver.cpp:237] Iteration 1026, loss = 1.77373
I0521 06:50:42.904016 19036 solver.cpp:253]     Train net output #0: loss = 1.77373 (* 1 = 1.77373 loss)
I0521 06:50:42.904032 19036 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 06:51:12.920635 19036 solver.cpp:237] Iteration 1045, loss = 1.77578
I0521 06:51:12.920810 19036 solver.cpp:253]     Train net output #0: loss = 1.77578 (* 1 = 1.77578 loss)
I0521 06:51:12.920825 19036 sgd_solver.cpp:106] Iteration 1045, lr = 0.0025
I0521 06:51:20.778254 19036 solver.cpp:237] Iteration 1064, loss = 1.75359
I0521 06:51:20.778292 19036 solver.cpp:253]     Train net output #0: loss = 1.75359 (* 1 = 1.75359 loss)
I0521 06:51:20.778314 19036 sgd_solver.cpp:106] Iteration 1064, lr = 0.0025
I0521 06:51:28.641772 19036 solver.cpp:237] Iteration 1083, loss = 1.79328
I0521 06:51:28.641805 19036 solver.cpp:253]     Train net output #0: loss = 1.79328 (* 1 = 1.79328 loss)
I0521 06:51:28.641822 19036 sgd_solver.cpp:106] Iteration 1083, lr = 0.0025
I0521 06:51:36.500799 19036 solver.cpp:237] Iteration 1102, loss = 1.93546
I0521 06:51:36.500833 19036 solver.cpp:253]     Train net output #0: loss = 1.93546 (* 1 = 1.93546 loss)
I0521 06:51:36.500850 19036 sgd_solver.cpp:106] Iteration 1102, lr = 0.0025
I0521 06:51:44.360100 19036 solver.cpp:237] Iteration 1121, loss = 1.76188
I0521 06:51:44.360261 19036 solver.cpp:253]     Train net output #0: loss = 1.76188 (* 1 = 1.76188 loss)
I0521 06:51:44.360276 19036 sgd_solver.cpp:106] Iteration 1121, lr = 0.0025
I0521 06:51:52.213951 19036 solver.cpp:237] Iteration 1140, loss = 1.76052
I0521 06:51:52.213982 19036 solver.cpp:253]     Train net output #0: loss = 1.76052 (* 1 = 1.76052 loss)
I0521 06:51:52.214000 19036 sgd_solver.cpp:106] Iteration 1140, lr = 0.0025
I0521 06:52:00.069188 19036 solver.cpp:237] Iteration 1159, loss = 1.79677
I0521 06:52:00.069221 19036 solver.cpp:253]     Train net output #0: loss = 1.79677 (* 1 = 1.79677 loss)
I0521 06:52:00.069237 19036 sgd_solver.cpp:106] Iteration 1159, lr = 0.0025
I0521 06:52:01.721344 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1164.caffemodel
I0521 06:52:02.048789 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1164.solverstate
I0521 06:52:03.025735 19036 solver.cpp:341] Iteration 1167, Testing net (#0)
I0521 06:52:48.062058 19036 solver.cpp:409]     Test net output #0: accuracy = 0.631657
I0521 06:52:48.062222 19036 solver.cpp:409]     Test net output #1: loss = 1.29128 (* 1 = 1.29128 loss)
I0521 06:53:14.873162 19036 solver.cpp:237] Iteration 1178, loss = 1.81148
I0521 06:53:14.873216 19036 solver.cpp:253]     Train net output #0: loss = 1.81148 (* 1 = 1.81148 loss)
I0521 06:53:14.873232 19036 sgd_solver.cpp:106] Iteration 1178, lr = 0.0025
I0521 06:53:22.739811 19036 solver.cpp:237] Iteration 1197, loss = 1.76157
I0521 06:53:22.739962 19036 solver.cpp:253]     Train net output #0: loss = 1.76157 (* 1 = 1.76157 loss)
I0521 06:53:22.739975 19036 sgd_solver.cpp:106] Iteration 1197, lr = 0.0025
I0521 06:53:30.602473 19036 solver.cpp:237] Iteration 1216, loss = 1.80701
I0521 06:53:30.602514 19036 solver.cpp:253]     Train net output #0: loss = 1.80701 (* 1 = 1.80701 loss)
I0521 06:53:30.602531 19036 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 06:53:38.461905 19036 solver.cpp:237] Iteration 1235, loss = 1.76964
I0521 06:53:38.461937 19036 solver.cpp:253]     Train net output #0: loss = 1.76964 (* 1 = 1.76964 loss)
I0521 06:53:38.461953 19036 sgd_solver.cpp:106] Iteration 1235, lr = 0.0025
I0521 06:53:46.326092 19036 solver.cpp:237] Iteration 1254, loss = 1.87482
I0521 06:53:46.326125 19036 solver.cpp:253]     Train net output #0: loss = 1.87482 (* 1 = 1.87482 loss)
I0521 06:53:46.326138 19036 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0521 06:53:54.189752 19036 solver.cpp:237] Iteration 1273, loss = 1.73506
I0521 06:53:54.189913 19036 solver.cpp:253]     Train net output #0: loss = 1.73506 (* 1 = 1.73506 loss)
I0521 06:53:54.189926 19036 sgd_solver.cpp:106] Iteration 1273, lr = 0.0025
I0521 06:54:02.049202 19036 solver.cpp:237] Iteration 1292, loss = 1.73347
I0521 06:54:02.049234 19036 solver.cpp:253]     Train net output #0: loss = 1.73347 (* 1 = 1.73347 loss)
I0521 06:54:02.049249 19036 sgd_solver.cpp:106] Iteration 1292, lr = 0.0025
I0521 06:54:32.051908 19036 solver.cpp:237] Iteration 1311, loss = 1.7381
I0521 06:54:32.052083 19036 solver.cpp:253]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0521 06:54:32.052098 19036 sgd_solver.cpp:106] Iteration 1311, lr = 0.0025
I0521 06:54:39.911928 19036 solver.cpp:237] Iteration 1330, loss = 1.70996
I0521 06:54:39.911973 19036 solver.cpp:253]     Train net output #0: loss = 1.70996 (* 1 = 1.70996 loss)
I0521 06:54:39.911994 19036 sgd_solver.cpp:106] Iteration 1330, lr = 0.0025
I0521 06:54:47.771474 19036 solver.cpp:237] Iteration 1349, loss = 1.74919
I0521 06:54:47.771508 19036 solver.cpp:253]     Train net output #0: loss = 1.74919 (* 1 = 1.74919 loss)
I0521 06:54:47.771527 19036 sgd_solver.cpp:106] Iteration 1349, lr = 0.0025
I0521 06:54:51.078972 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1358.caffemodel
I0521 06:54:51.407889 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1358.solverstate
I0521 06:54:55.694712 19036 solver.cpp:237] Iteration 1368, loss = 1.7583
I0521 06:54:55.694761 19036 solver.cpp:253]     Train net output #0: loss = 1.7583 (* 1 = 1.7583 loss)
I0521 06:54:55.694777 19036 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 06:55:03.556344 19036 solver.cpp:237] Iteration 1387, loss = 1.76205
I0521 06:55:03.556499 19036 solver.cpp:253]     Train net output #0: loss = 1.76205 (* 1 = 1.76205 loss)
I0521 06:55:03.556514 19036 sgd_solver.cpp:106] Iteration 1387, lr = 0.0025
I0521 06:55:11.406230 19036 solver.cpp:237] Iteration 1406, loss = 1.67373
I0521 06:55:11.406263 19036 solver.cpp:253]     Train net output #0: loss = 1.67373 (* 1 = 1.67373 loss)
I0521 06:55:11.406281 19036 sgd_solver.cpp:106] Iteration 1406, lr = 0.0025
I0521 06:55:19.261346 19036 solver.cpp:237] Iteration 1425, loss = 1.68986
I0521 06:55:19.261379 19036 solver.cpp:253]     Train net output #0: loss = 1.68986 (* 1 = 1.68986 loss)
I0521 06:55:19.261394 19036 sgd_solver.cpp:106] Iteration 1425, lr = 0.0025
I0521 06:55:49.291972 19036 solver.cpp:237] Iteration 1444, loss = 1.71518
I0521 06:55:49.292141 19036 solver.cpp:253]     Train net output #0: loss = 1.71518 (* 1 = 1.71518 loss)
I0521 06:55:49.292156 19036 sgd_solver.cpp:106] Iteration 1444, lr = 0.0025
I0521 06:55:57.152163 19036 solver.cpp:237] Iteration 1463, loss = 1.73824
I0521 06:55:57.152209 19036 solver.cpp:253]     Train net output #0: loss = 1.73824 (* 1 = 1.73824 loss)
I0521 06:55:57.152228 19036 sgd_solver.cpp:106] Iteration 1463, lr = 0.0025
I0521 06:56:05.012724 19036 solver.cpp:237] Iteration 1482, loss = 1.78553
I0521 06:56:05.012758 19036 solver.cpp:253]     Train net output #0: loss = 1.78553 (* 1 = 1.78553 loss)
I0521 06:56:05.012773 19036 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0521 06:56:12.875432 19036 solver.cpp:237] Iteration 1501, loss = 1.70547
I0521 06:56:12.875464 19036 solver.cpp:253]     Train net output #0: loss = 1.70547 (* 1 = 1.70547 loss)
I0521 06:56:12.875481 19036 sgd_solver.cpp:106] Iteration 1501, lr = 0.0025
I0521 06:56:20.736461 19036 solver.cpp:237] Iteration 1520, loss = 1.69827
I0521 06:56:20.736619 19036 solver.cpp:253]     Train net output #0: loss = 1.69827 (* 1 = 1.69827 loss)
I0521 06:56:20.736634 19036 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 06:56:28.604969 19036 solver.cpp:237] Iteration 1539, loss = 1.74133
I0521 06:56:28.605000 19036 solver.cpp:253]     Train net output #0: loss = 1.74133 (* 1 = 1.74133 loss)
I0521 06:56:28.605020 19036 sgd_solver.cpp:106] Iteration 1539, lr = 0.0025
I0521 06:56:33.570067 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1552.caffemodel
I0521 06:56:33.898293 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1552.solverstate
I0521 06:56:35.291623 19036 solver.cpp:341] Iteration 1556, Testing net (#0)
I0521 06:57:41.565847 19036 solver.cpp:409]     Test net output #0: accuracy = 0.659412
I0521 06:57:41.566027 19036 solver.cpp:409]     Test net output #1: loss = 1.16726 (* 1 = 1.16726 loss)
I0521 06:58:04.636684 19036 solver.cpp:237] Iteration 1558, loss = 1.65921
I0521 06:58:04.636739 19036 solver.cpp:253]     Train net output #0: loss = 1.65921 (* 1 = 1.65921 loss)
I0521 06:58:04.636756 19036 sgd_solver.cpp:106] Iteration 1558, lr = 0.0025
I0521 06:58:12.495421 19036 solver.cpp:237] Iteration 1577, loss = 1.70097
I0521 06:58:12.495575 19036 solver.cpp:253]     Train net output #0: loss = 1.70097 (* 1 = 1.70097 loss)
I0521 06:58:12.495589 19036 sgd_solver.cpp:106] Iteration 1577, lr = 0.0025
I0521 06:58:20.355973 19036 solver.cpp:237] Iteration 1596, loss = 1.63203
I0521 06:58:20.356006 19036 solver.cpp:253]     Train net output #0: loss = 1.63203 (* 1 = 1.63203 loss)
I0521 06:58:20.356022 19036 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0521 06:58:28.221714 19036 solver.cpp:237] Iteration 1615, loss = 1.68495
I0521 06:58:28.221762 19036 solver.cpp:253]     Train net output #0: loss = 1.68495 (* 1 = 1.68495 loss)
I0521 06:58:28.221778 19036 sgd_solver.cpp:106] Iteration 1615, lr = 0.0025
I0521 06:58:36.082635 19036 solver.cpp:237] Iteration 1634, loss = 1.72987
I0521 06:58:36.082669 19036 solver.cpp:253]     Train net output #0: loss = 1.72987 (* 1 = 1.72987 loss)
I0521 06:58:36.082684 19036 sgd_solver.cpp:106] Iteration 1634, lr = 0.0025
I0521 06:58:43.945529 19036 solver.cpp:237] Iteration 1653, loss = 1.60717
I0521 06:58:43.945680 19036 solver.cpp:253]     Train net output #0: loss = 1.60717 (* 1 = 1.60717 loss)
I0521 06:58:43.945693 19036 sgd_solver.cpp:106] Iteration 1653, lr = 0.0025
I0521 06:58:51.804313 19036 solver.cpp:237] Iteration 1672, loss = 1.68843
I0521 06:58:51.804350 19036 solver.cpp:253]     Train net output #0: loss = 1.68843 (* 1 = 1.68843 loss)
I0521 06:58:51.804368 19036 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0521 06:59:21.833446 19036 solver.cpp:237] Iteration 1691, loss = 1.68986
I0521 06:59:21.833621 19036 solver.cpp:253]     Train net output #0: loss = 1.68986 (* 1 = 1.68986 loss)
I0521 06:59:21.833636 19036 sgd_solver.cpp:106] Iteration 1691, lr = 0.0025
I0521 06:59:29.699775 19036 solver.cpp:237] Iteration 1710, loss = 1.71294
I0521 06:59:29.699807 19036 solver.cpp:253]     Train net output #0: loss = 1.71294 (* 1 = 1.71294 loss)
I0521 06:59:29.699827 19036 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 06:59:37.559939 19036 solver.cpp:237] Iteration 1729, loss = 1.72286
I0521 06:59:37.559968 19036 solver.cpp:253]     Train net output #0: loss = 1.72286 (* 1 = 1.72286 loss)
I0521 06:59:37.559983 19036 sgd_solver.cpp:106] Iteration 1729, lr = 0.0025
I0521 06:59:44.186604 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1746.caffemodel
I0521 06:59:44.517518 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1746.solverstate
I0521 06:59:45.497336 19036 solver.cpp:237] Iteration 1748, loss = 1.60506
I0521 06:59:45.497391 19036 solver.cpp:253]     Train net output #0: loss = 1.60506 (* 1 = 1.60506 loss)
I0521 06:59:45.497406 19036 sgd_solver.cpp:106] Iteration 1748, lr = 0.0025
I0521 06:59:53.359074 19036 solver.cpp:237] Iteration 1767, loss = 1.78811
I0521 06:59:53.359233 19036 solver.cpp:253]     Train net output #0: loss = 1.78811 (* 1 = 1.78811 loss)
I0521 06:59:53.359247 19036 sgd_solver.cpp:106] Iteration 1767, lr = 0.0025
I0521 07:00:01.224313 19036 solver.cpp:237] Iteration 1786, loss = 1.64604
I0521 07:00:01.224346 19036 solver.cpp:253]     Train net output #0: loss = 1.64604 (* 1 = 1.64604 loss)
I0521 07:00:01.224364 19036 sgd_solver.cpp:106] Iteration 1786, lr = 0.0025
I0521 07:00:09.089784 19036 solver.cpp:237] Iteration 1805, loss = 1.62639
I0521 07:00:09.089828 19036 solver.cpp:253]     Train net output #0: loss = 1.62639 (* 1 = 1.62639 loss)
I0521 07:00:09.089848 19036 sgd_solver.cpp:106] Iteration 1805, lr = 0.0025
I0521 07:00:39.111178 19036 solver.cpp:237] Iteration 1824, loss = 1.67632
I0521 07:00:39.111354 19036 solver.cpp:253]     Train net output #0: loss = 1.67632 (* 1 = 1.67632 loss)
I0521 07:00:39.111368 19036 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0521 07:00:46.975499 19036 solver.cpp:237] Iteration 1843, loss = 1.62387
I0521 07:00:46.975531 19036 solver.cpp:253]     Train net output #0: loss = 1.62387 (* 1 = 1.62387 loss)
I0521 07:00:46.975549 19036 sgd_solver.cpp:106] Iteration 1843, lr = 0.0025
I0521 07:00:54.835314 19036 solver.cpp:237] Iteration 1862, loss = 1.71147
I0521 07:00:54.835363 19036 solver.cpp:253]     Train net output #0: loss = 1.71147 (* 1 = 1.71147 loss)
I0521 07:00:54.835381 19036 sgd_solver.cpp:106] Iteration 1862, lr = 0.0025
I0521 07:01:02.696915 19036 solver.cpp:237] Iteration 1881, loss = 1.7008
I0521 07:01:02.696949 19036 solver.cpp:253]     Train net output #0: loss = 1.7008 (* 1 = 1.7008 loss)
I0521 07:01:02.696965 19036 sgd_solver.cpp:106] Iteration 1881, lr = 0.0025
I0521 07:01:10.556713 19036 solver.cpp:237] Iteration 1900, loss = 1.73885
I0521 07:01:10.556854 19036 solver.cpp:253]     Train net output #0: loss = 1.73885 (* 1 = 1.73885 loss)
I0521 07:01:10.556867 19036 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0521 07:01:18.420308 19036 solver.cpp:237] Iteration 1919, loss = 1.61816
I0521 07:01:18.420339 19036 solver.cpp:253]     Train net output #0: loss = 1.61816 (* 1 = 1.61816 loss)
I0521 07:01:18.420357 19036 sgd_solver.cpp:106] Iteration 1919, lr = 0.0025
I0521 07:01:26.283334 19036 solver.cpp:237] Iteration 1938, loss = 1.71564
I0521 07:01:26.283362 19036 solver.cpp:253]     Train net output #0: loss = 1.71564 (* 1 = 1.71564 loss)
I0521 07:01:26.283380 19036 sgd_solver.cpp:106] Iteration 1938, lr = 0.0025
I0521 07:01:26.697090 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1940.caffemodel
I0521 07:01:27.027235 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1940.solverstate
I0521 07:01:28.834697 19036 solver.cpp:341] Iteration 1945, Testing net (#0)
I0521 07:02:14.246237 19036 solver.cpp:409]     Test net output #0: accuracy = 0.675519
I0521 07:02:14.246408 19036 solver.cpp:409]     Test net output #1: loss = 1.12195 (* 1 = 1.12195 loss)
I0521 07:02:15.199522 19036 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1948.caffemodel
I0521 07:02:15.528571 19036 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136_iter_1948.solverstate
I0521 07:02:15.557099 19036 solver.cpp:326] Optimization Done.
I0521 07:02:15.557128 19036 caffe.cpp:215] Optimization Done.
Application 11237101 resources: utime ~1250s, stime ~226s, Rss ~5329440, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_770_2016-05-20T11.21.00.751136.solver"
	User time (seconds): 0.54
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:39.38
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15076
	Voluntary context switches: 2705
	Involuntary context switches: 84
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
