2805054
I0520 12:27:19.456506 25059 caffe.cpp:184] Using GPUs 0
I0520 12:27:19.880759 25059 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3750
test_interval: 7500
base_lr: 0.0025
display: 375
max_iter: 37500
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 3750
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583.prototxt"
I0520 12:27:19.882261 25059 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583.prototxt
I0520 12:27:19.886001 25059 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 12:27:19.886059 25059 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 12:27:19.886406 25059 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 40
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 12:27:19.886587 25059 layer_factory.hpp:77] Creating layer data_hdf5
I0520 12:27:19.886611 25059 net.cpp:106] Creating Layer data_hdf5
I0520 12:27:19.886626 25059 net.cpp:411] data_hdf5 -> data
I0520 12:27:19.886659 25059 net.cpp:411] data_hdf5 -> label
I0520 12:27:19.886693 25059 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 12:27:19.888036 25059 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 12:27:19.890270 25059 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 12:27:41.410823 25059 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 12:27:41.415920 25059 net.cpp:150] Setting up data_hdf5
I0520 12:27:41.415959 25059 net.cpp:157] Top shape: 40 1 127 50 (254000)
I0520 12:27:41.415973 25059 net.cpp:157] Top shape: 40 (40)
I0520 12:27:41.415987 25059 net.cpp:165] Memory required for data: 1016160
I0520 12:27:41.415999 25059 layer_factory.hpp:77] Creating layer conv1
I0520 12:27:41.416033 25059 net.cpp:106] Creating Layer conv1
I0520 12:27:41.416043 25059 net.cpp:454] conv1 <- data
I0520 12:27:41.416064 25059 net.cpp:411] conv1 -> conv1
I0520 12:27:41.790956 25059 net.cpp:150] Setting up conv1
I0520 12:27:41.791002 25059 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0520 12:27:41.791013 25059 net.cpp:165] Memory required for data: 12075360
I0520 12:27:41.791043 25059 layer_factory.hpp:77] Creating layer relu1
I0520 12:27:41.791064 25059 net.cpp:106] Creating Layer relu1
I0520 12:27:41.791075 25059 net.cpp:454] relu1 <- conv1
I0520 12:27:41.791088 25059 net.cpp:397] relu1 -> conv1 (in-place)
I0520 12:27:41.791611 25059 net.cpp:150] Setting up relu1
I0520 12:27:41.791627 25059 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0520 12:27:41.791638 25059 net.cpp:165] Memory required for data: 23134560
I0520 12:27:41.791648 25059 layer_factory.hpp:77] Creating layer pool1
I0520 12:27:41.791666 25059 net.cpp:106] Creating Layer pool1
I0520 12:27:41.791676 25059 net.cpp:454] pool1 <- conv1
I0520 12:27:41.791689 25059 net.cpp:411] pool1 -> pool1
I0520 12:27:41.791770 25059 net.cpp:150] Setting up pool1
I0520 12:27:41.791785 25059 net.cpp:157] Top shape: 40 12 60 48 (1382400)
I0520 12:27:41.791795 25059 net.cpp:165] Memory required for data: 28664160
I0520 12:27:41.791805 25059 layer_factory.hpp:77] Creating layer conv2
I0520 12:27:41.791826 25059 net.cpp:106] Creating Layer conv2
I0520 12:27:41.791837 25059 net.cpp:454] conv2 <- pool1
I0520 12:27:41.791851 25059 net.cpp:411] conv2 -> conv2
I0520 12:27:41.794538 25059 net.cpp:150] Setting up conv2
I0520 12:27:41.794566 25059 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0520 12:27:41.794576 25059 net.cpp:165] Memory required for data: 36612960
I0520 12:27:41.794596 25059 layer_factory.hpp:77] Creating layer relu2
I0520 12:27:41.794610 25059 net.cpp:106] Creating Layer relu2
I0520 12:27:41.794620 25059 net.cpp:454] relu2 <- conv2
I0520 12:27:41.794632 25059 net.cpp:397] relu2 -> conv2 (in-place)
I0520 12:27:41.794962 25059 net.cpp:150] Setting up relu2
I0520 12:27:41.794977 25059 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0520 12:27:41.794992 25059 net.cpp:165] Memory required for data: 44561760
I0520 12:27:41.795002 25059 layer_factory.hpp:77] Creating layer pool2
I0520 12:27:41.795014 25059 net.cpp:106] Creating Layer pool2
I0520 12:27:41.795024 25059 net.cpp:454] pool2 <- conv2
I0520 12:27:41.795049 25059 net.cpp:411] pool2 -> pool2
I0520 12:27:41.795119 25059 net.cpp:150] Setting up pool2
I0520 12:27:41.795131 25059 net.cpp:157] Top shape: 40 20 27 46 (993600)
I0520 12:27:41.795141 25059 net.cpp:165] Memory required for data: 48536160
I0520 12:27:41.795151 25059 layer_factory.hpp:77] Creating layer conv3
I0520 12:27:41.795171 25059 net.cpp:106] Creating Layer conv3
I0520 12:27:41.795181 25059 net.cpp:454] conv3 <- pool2
I0520 12:27:41.795195 25059 net.cpp:411] conv3 -> conv3
I0520 12:27:41.797132 25059 net.cpp:150] Setting up conv3
I0520 12:27:41.797155 25059 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0520 12:27:41.797168 25059 net.cpp:165] Memory required for data: 52872800
I0520 12:27:41.797188 25059 layer_factory.hpp:77] Creating layer relu3
I0520 12:27:41.797204 25059 net.cpp:106] Creating Layer relu3
I0520 12:27:41.797214 25059 net.cpp:454] relu3 <- conv3
I0520 12:27:41.797225 25059 net.cpp:397] relu3 -> conv3 (in-place)
I0520 12:27:41.797695 25059 net.cpp:150] Setting up relu3
I0520 12:27:41.797713 25059 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0520 12:27:41.797724 25059 net.cpp:165] Memory required for data: 57209440
I0520 12:27:41.797734 25059 layer_factory.hpp:77] Creating layer pool3
I0520 12:27:41.797746 25059 net.cpp:106] Creating Layer pool3
I0520 12:27:41.797756 25059 net.cpp:454] pool3 <- conv3
I0520 12:27:41.797770 25059 net.cpp:411] pool3 -> pool3
I0520 12:27:41.797837 25059 net.cpp:150] Setting up pool3
I0520 12:27:41.797850 25059 net.cpp:157] Top shape: 40 28 11 44 (542080)
I0520 12:27:41.797860 25059 net.cpp:165] Memory required for data: 59377760
I0520 12:27:41.797868 25059 layer_factory.hpp:77] Creating layer conv4
I0520 12:27:41.797886 25059 net.cpp:106] Creating Layer conv4
I0520 12:27:41.797896 25059 net.cpp:454] conv4 <- pool3
I0520 12:27:41.797910 25059 net.cpp:411] conv4 -> conv4
I0520 12:27:41.800676 25059 net.cpp:150] Setting up conv4
I0520 12:27:41.800704 25059 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0520 12:27:41.800715 25059 net.cpp:165] Memory required for data: 60829280
I0520 12:27:41.800730 25059 layer_factory.hpp:77] Creating layer relu4
I0520 12:27:41.800745 25059 net.cpp:106] Creating Layer relu4
I0520 12:27:41.800755 25059 net.cpp:454] relu4 <- conv4
I0520 12:27:41.800767 25059 net.cpp:397] relu4 -> conv4 (in-place)
I0520 12:27:41.801254 25059 net.cpp:150] Setting up relu4
I0520 12:27:41.801270 25059 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0520 12:27:41.801280 25059 net.cpp:165] Memory required for data: 62280800
I0520 12:27:41.801291 25059 layer_factory.hpp:77] Creating layer pool4
I0520 12:27:41.801304 25059 net.cpp:106] Creating Layer pool4
I0520 12:27:41.801314 25059 net.cpp:454] pool4 <- conv4
I0520 12:27:41.801327 25059 net.cpp:411] pool4 -> pool4
I0520 12:27:41.801395 25059 net.cpp:150] Setting up pool4
I0520 12:27:41.801409 25059 net.cpp:157] Top shape: 40 36 3 42 (181440)
I0520 12:27:41.801419 25059 net.cpp:165] Memory required for data: 63006560
I0520 12:27:41.801429 25059 layer_factory.hpp:77] Creating layer ip1
I0520 12:27:41.801450 25059 net.cpp:106] Creating Layer ip1
I0520 12:27:41.801460 25059 net.cpp:454] ip1 <- pool4
I0520 12:27:41.801472 25059 net.cpp:411] ip1 -> ip1
I0520 12:27:41.816953 25059 net.cpp:150] Setting up ip1
I0520 12:27:41.816983 25059 net.cpp:157] Top shape: 40 196 (7840)
I0520 12:27:41.816999 25059 net.cpp:165] Memory required for data: 63037920
I0520 12:27:41.817028 25059 layer_factory.hpp:77] Creating layer relu5
I0520 12:27:41.817042 25059 net.cpp:106] Creating Layer relu5
I0520 12:27:41.817052 25059 net.cpp:454] relu5 <- ip1
I0520 12:27:41.817066 25059 net.cpp:397] relu5 -> ip1 (in-place)
I0520 12:27:41.817414 25059 net.cpp:150] Setting up relu5
I0520 12:27:41.817430 25059 net.cpp:157] Top shape: 40 196 (7840)
I0520 12:27:41.817440 25059 net.cpp:165] Memory required for data: 63069280
I0520 12:27:41.817451 25059 layer_factory.hpp:77] Creating layer drop1
I0520 12:27:41.817471 25059 net.cpp:106] Creating Layer drop1
I0520 12:27:41.817481 25059 net.cpp:454] drop1 <- ip1
I0520 12:27:41.817493 25059 net.cpp:397] drop1 -> ip1 (in-place)
I0520 12:27:41.817551 25059 net.cpp:150] Setting up drop1
I0520 12:27:41.817565 25059 net.cpp:157] Top shape: 40 196 (7840)
I0520 12:27:41.817575 25059 net.cpp:165] Memory required for data: 63100640
I0520 12:27:41.817585 25059 layer_factory.hpp:77] Creating layer ip2
I0520 12:27:41.817603 25059 net.cpp:106] Creating Layer ip2
I0520 12:27:41.817613 25059 net.cpp:454] ip2 <- ip1
I0520 12:27:41.817626 25059 net.cpp:411] ip2 -> ip2
I0520 12:27:41.818089 25059 net.cpp:150] Setting up ip2
I0520 12:27:41.818102 25059 net.cpp:157] Top shape: 40 98 (3920)
I0520 12:27:41.818111 25059 net.cpp:165] Memory required for data: 63116320
I0520 12:27:41.818127 25059 layer_factory.hpp:77] Creating layer relu6
I0520 12:27:41.818140 25059 net.cpp:106] Creating Layer relu6
I0520 12:27:41.818150 25059 net.cpp:454] relu6 <- ip2
I0520 12:27:41.818161 25059 net.cpp:397] relu6 -> ip2 (in-place)
I0520 12:27:41.818681 25059 net.cpp:150] Setting up relu6
I0520 12:27:41.818696 25059 net.cpp:157] Top shape: 40 98 (3920)
I0520 12:27:41.818707 25059 net.cpp:165] Memory required for data: 63132000
I0520 12:27:41.818717 25059 layer_factory.hpp:77] Creating layer drop2
I0520 12:27:41.818730 25059 net.cpp:106] Creating Layer drop2
I0520 12:27:41.818740 25059 net.cpp:454] drop2 <- ip2
I0520 12:27:41.818753 25059 net.cpp:397] drop2 -> ip2 (in-place)
I0520 12:27:41.818795 25059 net.cpp:150] Setting up drop2
I0520 12:27:41.818809 25059 net.cpp:157] Top shape: 40 98 (3920)
I0520 12:27:41.818819 25059 net.cpp:165] Memory required for data: 63147680
I0520 12:27:41.818830 25059 layer_factory.hpp:77] Creating layer ip3
I0520 12:27:41.818843 25059 net.cpp:106] Creating Layer ip3
I0520 12:27:41.818852 25059 net.cpp:454] ip3 <- ip2
I0520 12:27:41.818866 25059 net.cpp:411] ip3 -> ip3
I0520 12:27:41.819074 25059 net.cpp:150] Setting up ip3
I0520 12:27:41.819088 25059 net.cpp:157] Top shape: 40 11 (440)
I0520 12:27:41.819098 25059 net.cpp:165] Memory required for data: 63149440
I0520 12:27:41.819113 25059 layer_factory.hpp:77] Creating layer drop3
I0520 12:27:41.819125 25059 net.cpp:106] Creating Layer drop3
I0520 12:27:41.819135 25059 net.cpp:454] drop3 <- ip3
I0520 12:27:41.819147 25059 net.cpp:397] drop3 -> ip3 (in-place)
I0520 12:27:41.819186 25059 net.cpp:150] Setting up drop3
I0520 12:27:41.819198 25059 net.cpp:157] Top shape: 40 11 (440)
I0520 12:27:41.819210 25059 net.cpp:165] Memory required for data: 63151200
I0520 12:27:41.819219 25059 layer_factory.hpp:77] Creating layer loss
I0520 12:27:41.819238 25059 net.cpp:106] Creating Layer loss
I0520 12:27:41.819247 25059 net.cpp:454] loss <- ip3
I0520 12:27:41.819258 25059 net.cpp:454] loss <- label
I0520 12:27:41.819272 25059 net.cpp:411] loss -> loss
I0520 12:27:41.819289 25059 layer_factory.hpp:77] Creating layer loss
I0520 12:27:41.819928 25059 net.cpp:150] Setting up loss
I0520 12:27:41.819949 25059 net.cpp:157] Top shape: (1)
I0520 12:27:41.819962 25059 net.cpp:160]     with loss weight 1
I0520 12:27:41.820004 25059 net.cpp:165] Memory required for data: 63151204
I0520 12:27:41.820015 25059 net.cpp:226] loss needs backward computation.
I0520 12:27:41.820026 25059 net.cpp:226] drop3 needs backward computation.
I0520 12:27:41.820036 25059 net.cpp:226] ip3 needs backward computation.
I0520 12:27:41.820046 25059 net.cpp:226] drop2 needs backward computation.
I0520 12:27:41.820056 25059 net.cpp:226] relu6 needs backward computation.
I0520 12:27:41.820066 25059 net.cpp:226] ip2 needs backward computation.
I0520 12:27:41.820077 25059 net.cpp:226] drop1 needs backward computation.
I0520 12:27:41.820087 25059 net.cpp:226] relu5 needs backward computation.
I0520 12:27:41.820096 25059 net.cpp:226] ip1 needs backward computation.
I0520 12:27:41.820106 25059 net.cpp:226] pool4 needs backward computation.
I0520 12:27:41.820117 25059 net.cpp:226] relu4 needs backward computation.
I0520 12:27:41.820127 25059 net.cpp:226] conv4 needs backward computation.
I0520 12:27:41.820137 25059 net.cpp:226] pool3 needs backward computation.
I0520 12:27:41.820147 25059 net.cpp:226] relu3 needs backward computation.
I0520 12:27:41.820165 25059 net.cpp:226] conv3 needs backward computation.
I0520 12:27:41.820174 25059 net.cpp:226] pool2 needs backward computation.
I0520 12:27:41.820184 25059 net.cpp:226] relu2 needs backward computation.
I0520 12:27:41.820194 25059 net.cpp:226] conv2 needs backward computation.
I0520 12:27:41.820206 25059 net.cpp:226] pool1 needs backward computation.
I0520 12:27:41.820216 25059 net.cpp:226] relu1 needs backward computation.
I0520 12:27:41.820226 25059 net.cpp:226] conv1 needs backward computation.
I0520 12:27:41.820237 25059 net.cpp:228] data_hdf5 does not need backward computation.
I0520 12:27:41.820246 25059 net.cpp:270] This network produces output loss
I0520 12:27:41.820271 25059 net.cpp:283] Network initialization done.
I0520 12:27:41.821846 25059 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583.prototxt
I0520 12:27:41.821920 25059 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 12:27:41.822278 25059 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 40
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 12:27:41.822468 25059 layer_factory.hpp:77] Creating layer data_hdf5
I0520 12:27:41.822481 25059 net.cpp:106] Creating Layer data_hdf5
I0520 12:27:41.822494 25059 net.cpp:411] data_hdf5 -> data
I0520 12:27:41.822511 25059 net.cpp:411] data_hdf5 -> label
I0520 12:27:41.822527 25059 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 12:27:41.823886 25059 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 12:28:03.108777 25059 net.cpp:150] Setting up data_hdf5
I0520 12:28:03.108944 25059 net.cpp:157] Top shape: 40 1 127 50 (254000)
I0520 12:28:03.108958 25059 net.cpp:157] Top shape: 40 (40)
I0520 12:28:03.108971 25059 net.cpp:165] Memory required for data: 1016160
I0520 12:28:03.108984 25059 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 12:28:03.109012 25059 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 12:28:03.109024 25059 net.cpp:454] label_data_hdf5_1_split <- label
I0520 12:28:03.109038 25059 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 12:28:03.109060 25059 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 12:28:03.109143 25059 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 12:28:03.109155 25059 net.cpp:157] Top shape: 40 (40)
I0520 12:28:03.109168 25059 net.cpp:157] Top shape: 40 (40)
I0520 12:28:03.109177 25059 net.cpp:165] Memory required for data: 1016480
I0520 12:28:03.109187 25059 layer_factory.hpp:77] Creating layer conv1
I0520 12:28:03.109210 25059 net.cpp:106] Creating Layer conv1
I0520 12:28:03.109221 25059 net.cpp:454] conv1 <- data
I0520 12:28:03.109236 25059 net.cpp:411] conv1 -> conv1
I0520 12:28:03.111179 25059 net.cpp:150] Setting up conv1
I0520 12:28:03.111198 25059 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0520 12:28:03.111208 25059 net.cpp:165] Memory required for data: 12075680
I0520 12:28:03.111228 25059 layer_factory.hpp:77] Creating layer relu1
I0520 12:28:03.111243 25059 net.cpp:106] Creating Layer relu1
I0520 12:28:03.111253 25059 net.cpp:454] relu1 <- conv1
I0520 12:28:03.111266 25059 net.cpp:397] relu1 -> conv1 (in-place)
I0520 12:28:03.111764 25059 net.cpp:150] Setting up relu1
I0520 12:28:03.111780 25059 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0520 12:28:03.111790 25059 net.cpp:165] Memory required for data: 23134880
I0520 12:28:03.111801 25059 layer_factory.hpp:77] Creating layer pool1
I0520 12:28:03.111817 25059 net.cpp:106] Creating Layer pool1
I0520 12:28:03.111827 25059 net.cpp:454] pool1 <- conv1
I0520 12:28:03.111840 25059 net.cpp:411] pool1 -> pool1
I0520 12:28:03.111915 25059 net.cpp:150] Setting up pool1
I0520 12:28:03.111928 25059 net.cpp:157] Top shape: 40 12 60 48 (1382400)
I0520 12:28:03.111938 25059 net.cpp:165] Memory required for data: 28664480
I0520 12:28:03.111949 25059 layer_factory.hpp:77] Creating layer conv2
I0520 12:28:03.111966 25059 net.cpp:106] Creating Layer conv2
I0520 12:28:03.111976 25059 net.cpp:454] conv2 <- pool1
I0520 12:28:03.111990 25059 net.cpp:411] conv2 -> conv2
I0520 12:28:03.113904 25059 net.cpp:150] Setting up conv2
I0520 12:28:03.113926 25059 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0520 12:28:03.113940 25059 net.cpp:165] Memory required for data: 36613280
I0520 12:28:03.113957 25059 layer_factory.hpp:77] Creating layer relu2
I0520 12:28:03.113971 25059 net.cpp:106] Creating Layer relu2
I0520 12:28:03.113981 25059 net.cpp:454] relu2 <- conv2
I0520 12:28:03.113994 25059 net.cpp:397] relu2 -> conv2 (in-place)
I0520 12:28:03.114326 25059 net.cpp:150] Setting up relu2
I0520 12:28:03.114341 25059 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0520 12:28:03.114351 25059 net.cpp:165] Memory required for data: 44562080
I0520 12:28:03.114361 25059 layer_factory.hpp:77] Creating layer pool2
I0520 12:28:03.114373 25059 net.cpp:106] Creating Layer pool2
I0520 12:28:03.114383 25059 net.cpp:454] pool2 <- conv2
I0520 12:28:03.114395 25059 net.cpp:411] pool2 -> pool2
I0520 12:28:03.114466 25059 net.cpp:150] Setting up pool2
I0520 12:28:03.114480 25059 net.cpp:157] Top shape: 40 20 27 46 (993600)
I0520 12:28:03.114490 25059 net.cpp:165] Memory required for data: 48536480
I0520 12:28:03.114500 25059 layer_factory.hpp:77] Creating layer conv3
I0520 12:28:03.114517 25059 net.cpp:106] Creating Layer conv3
I0520 12:28:03.114528 25059 net.cpp:454] conv3 <- pool2
I0520 12:28:03.114542 25059 net.cpp:411] conv3 -> conv3
I0520 12:28:03.116502 25059 net.cpp:150] Setting up conv3
I0520 12:28:03.116524 25059 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0520 12:28:03.116538 25059 net.cpp:165] Memory required for data: 52873120
I0520 12:28:03.116569 25059 layer_factory.hpp:77] Creating layer relu3
I0520 12:28:03.116583 25059 net.cpp:106] Creating Layer relu3
I0520 12:28:03.116593 25059 net.cpp:454] relu3 <- conv3
I0520 12:28:03.116606 25059 net.cpp:397] relu3 -> conv3 (in-place)
I0520 12:28:03.117076 25059 net.cpp:150] Setting up relu3
I0520 12:28:03.117094 25059 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0520 12:28:03.117111 25059 net.cpp:165] Memory required for data: 57209760
I0520 12:28:03.117121 25059 layer_factory.hpp:77] Creating layer pool3
I0520 12:28:03.117135 25059 net.cpp:106] Creating Layer pool3
I0520 12:28:03.117144 25059 net.cpp:454] pool3 <- conv3
I0520 12:28:03.117157 25059 net.cpp:411] pool3 -> pool3
I0520 12:28:03.117230 25059 net.cpp:150] Setting up pool3
I0520 12:28:03.117244 25059 net.cpp:157] Top shape: 40 28 11 44 (542080)
I0520 12:28:03.117254 25059 net.cpp:165] Memory required for data: 59378080
I0520 12:28:03.117265 25059 layer_factory.hpp:77] Creating layer conv4
I0520 12:28:03.117282 25059 net.cpp:106] Creating Layer conv4
I0520 12:28:03.117293 25059 net.cpp:454] conv4 <- pool3
I0520 12:28:03.117307 25059 net.cpp:411] conv4 -> conv4
I0520 12:28:03.119362 25059 net.cpp:150] Setting up conv4
I0520 12:28:03.119380 25059 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0520 12:28:03.119391 25059 net.cpp:165] Memory required for data: 60829600
I0520 12:28:03.119406 25059 layer_factory.hpp:77] Creating layer relu4
I0520 12:28:03.119420 25059 net.cpp:106] Creating Layer relu4
I0520 12:28:03.119429 25059 net.cpp:454] relu4 <- conv4
I0520 12:28:03.119442 25059 net.cpp:397] relu4 -> conv4 (in-place)
I0520 12:28:03.119912 25059 net.cpp:150] Setting up relu4
I0520 12:28:03.119928 25059 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0520 12:28:03.119940 25059 net.cpp:165] Memory required for data: 62281120
I0520 12:28:03.119949 25059 layer_factory.hpp:77] Creating layer pool4
I0520 12:28:03.119962 25059 net.cpp:106] Creating Layer pool4
I0520 12:28:03.119972 25059 net.cpp:454] pool4 <- conv4
I0520 12:28:03.119985 25059 net.cpp:411] pool4 -> pool4
I0520 12:28:03.120056 25059 net.cpp:150] Setting up pool4
I0520 12:28:03.120070 25059 net.cpp:157] Top shape: 40 36 3 42 (181440)
I0520 12:28:03.120079 25059 net.cpp:165] Memory required for data: 63006880
I0520 12:28:03.120090 25059 layer_factory.hpp:77] Creating layer ip1
I0520 12:28:03.120102 25059 net.cpp:106] Creating Layer ip1
I0520 12:28:03.120113 25059 net.cpp:454] ip1 <- pool4
I0520 12:28:03.120126 25059 net.cpp:411] ip1 -> ip1
I0520 12:28:03.135637 25059 net.cpp:150] Setting up ip1
I0520 12:28:03.135664 25059 net.cpp:157] Top shape: 40 196 (7840)
I0520 12:28:03.135680 25059 net.cpp:165] Memory required for data: 63038240
I0520 12:28:03.135709 25059 layer_factory.hpp:77] Creating layer relu5
I0520 12:28:03.135723 25059 net.cpp:106] Creating Layer relu5
I0520 12:28:03.135735 25059 net.cpp:454] relu5 <- ip1
I0520 12:28:03.135747 25059 net.cpp:397] relu5 -> ip1 (in-place)
I0520 12:28:03.136093 25059 net.cpp:150] Setting up relu5
I0520 12:28:03.136107 25059 net.cpp:157] Top shape: 40 196 (7840)
I0520 12:28:03.136117 25059 net.cpp:165] Memory required for data: 63069600
I0520 12:28:03.136127 25059 layer_factory.hpp:77] Creating layer drop1
I0520 12:28:03.136147 25059 net.cpp:106] Creating Layer drop1
I0520 12:28:03.136157 25059 net.cpp:454] drop1 <- ip1
I0520 12:28:03.136169 25059 net.cpp:397] drop1 -> ip1 (in-place)
I0520 12:28:03.136215 25059 net.cpp:150] Setting up drop1
I0520 12:28:03.136229 25059 net.cpp:157] Top shape: 40 196 (7840)
I0520 12:28:03.136239 25059 net.cpp:165] Memory required for data: 63100960
I0520 12:28:03.136248 25059 layer_factory.hpp:77] Creating layer ip2
I0520 12:28:03.136262 25059 net.cpp:106] Creating Layer ip2
I0520 12:28:03.136272 25059 net.cpp:454] ip2 <- ip1
I0520 12:28:03.136286 25059 net.cpp:411] ip2 -> ip2
I0520 12:28:03.136764 25059 net.cpp:150] Setting up ip2
I0520 12:28:03.136777 25059 net.cpp:157] Top shape: 40 98 (3920)
I0520 12:28:03.136787 25059 net.cpp:165] Memory required for data: 63116640
I0520 12:28:03.136803 25059 layer_factory.hpp:77] Creating layer relu6
I0520 12:28:03.136828 25059 net.cpp:106] Creating Layer relu6
I0520 12:28:03.136837 25059 net.cpp:454] relu6 <- ip2
I0520 12:28:03.136850 25059 net.cpp:397] relu6 -> ip2 (in-place)
I0520 12:28:03.137401 25059 net.cpp:150] Setting up relu6
I0520 12:28:03.137423 25059 net.cpp:157] Top shape: 40 98 (3920)
I0520 12:28:03.137433 25059 net.cpp:165] Memory required for data: 63132320
I0520 12:28:03.137444 25059 layer_factory.hpp:77] Creating layer drop2
I0520 12:28:03.137459 25059 net.cpp:106] Creating Layer drop2
I0520 12:28:03.137468 25059 net.cpp:454] drop2 <- ip2
I0520 12:28:03.137481 25059 net.cpp:397] drop2 -> ip2 (in-place)
I0520 12:28:03.137526 25059 net.cpp:150] Setting up drop2
I0520 12:28:03.137538 25059 net.cpp:157] Top shape: 40 98 (3920)
I0520 12:28:03.137547 25059 net.cpp:165] Memory required for data: 63148000
I0520 12:28:03.137557 25059 layer_factory.hpp:77] Creating layer ip3
I0520 12:28:03.137570 25059 net.cpp:106] Creating Layer ip3
I0520 12:28:03.137580 25059 net.cpp:454] ip3 <- ip2
I0520 12:28:03.137594 25059 net.cpp:411] ip3 -> ip3
I0520 12:28:03.137815 25059 net.cpp:150] Setting up ip3
I0520 12:28:03.137830 25059 net.cpp:157] Top shape: 40 11 (440)
I0520 12:28:03.137840 25059 net.cpp:165] Memory required for data: 63149760
I0520 12:28:03.137855 25059 layer_factory.hpp:77] Creating layer drop3
I0520 12:28:03.137867 25059 net.cpp:106] Creating Layer drop3
I0520 12:28:03.137877 25059 net.cpp:454] drop3 <- ip3
I0520 12:28:03.137889 25059 net.cpp:397] drop3 -> ip3 (in-place)
I0520 12:28:03.137930 25059 net.cpp:150] Setting up drop3
I0520 12:28:03.137943 25059 net.cpp:157] Top shape: 40 11 (440)
I0520 12:28:03.137953 25059 net.cpp:165] Memory required for data: 63151520
I0520 12:28:03.137962 25059 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 12:28:03.137975 25059 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 12:28:03.137985 25059 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 12:28:03.137998 25059 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 12:28:03.138013 25059 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 12:28:03.138087 25059 net.cpp:150] Setting up ip3_drop3_0_split
I0520 12:28:03.138100 25059 net.cpp:157] Top shape: 40 11 (440)
I0520 12:28:03.138113 25059 net.cpp:157] Top shape: 40 11 (440)
I0520 12:28:03.138123 25059 net.cpp:165] Memory required for data: 63155040
I0520 12:28:03.138131 25059 layer_factory.hpp:77] Creating layer accuracy
I0520 12:28:03.138154 25059 net.cpp:106] Creating Layer accuracy
I0520 12:28:03.138164 25059 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 12:28:03.138175 25059 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 12:28:03.138188 25059 net.cpp:411] accuracy -> accuracy
I0520 12:28:03.138212 25059 net.cpp:150] Setting up accuracy
I0520 12:28:03.138224 25059 net.cpp:157] Top shape: (1)
I0520 12:28:03.138234 25059 net.cpp:165] Memory required for data: 63155044
I0520 12:28:03.138242 25059 layer_factory.hpp:77] Creating layer loss
I0520 12:28:03.138257 25059 net.cpp:106] Creating Layer loss
I0520 12:28:03.138267 25059 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 12:28:03.138278 25059 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 12:28:03.138291 25059 net.cpp:411] loss -> loss
I0520 12:28:03.138309 25059 layer_factory.hpp:77] Creating layer loss
I0520 12:28:03.138792 25059 net.cpp:150] Setting up loss
I0520 12:28:03.138806 25059 net.cpp:157] Top shape: (1)
I0520 12:28:03.138818 25059 net.cpp:160]     with loss weight 1
I0520 12:28:03.138835 25059 net.cpp:165] Memory required for data: 63155048
I0520 12:28:03.138845 25059 net.cpp:226] loss needs backward computation.
I0520 12:28:03.138857 25059 net.cpp:228] accuracy does not need backward computation.
I0520 12:28:03.138869 25059 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 12:28:03.138880 25059 net.cpp:226] drop3 needs backward computation.
I0520 12:28:03.138887 25059 net.cpp:226] ip3 needs backward computation.
I0520 12:28:03.138898 25059 net.cpp:226] drop2 needs backward computation.
I0520 12:28:03.138907 25059 net.cpp:226] relu6 needs backward computation.
I0520 12:28:03.138926 25059 net.cpp:226] ip2 needs backward computation.
I0520 12:28:03.138936 25059 net.cpp:226] drop1 needs backward computation.
I0520 12:28:03.138945 25059 net.cpp:226] relu5 needs backward computation.
I0520 12:28:03.138955 25059 net.cpp:226] ip1 needs backward computation.
I0520 12:28:03.138965 25059 net.cpp:226] pool4 needs backward computation.
I0520 12:28:03.138975 25059 net.cpp:226] relu4 needs backward computation.
I0520 12:28:03.138985 25059 net.cpp:226] conv4 needs backward computation.
I0520 12:28:03.138994 25059 net.cpp:226] pool3 needs backward computation.
I0520 12:28:03.139005 25059 net.cpp:226] relu3 needs backward computation.
I0520 12:28:03.139015 25059 net.cpp:226] conv3 needs backward computation.
I0520 12:28:03.139025 25059 net.cpp:226] pool2 needs backward computation.
I0520 12:28:03.139036 25059 net.cpp:226] relu2 needs backward computation.
I0520 12:28:03.139046 25059 net.cpp:226] conv2 needs backward computation.
I0520 12:28:03.139057 25059 net.cpp:226] pool1 needs backward computation.
I0520 12:28:03.139067 25059 net.cpp:226] relu1 needs backward computation.
I0520 12:28:03.139077 25059 net.cpp:226] conv1 needs backward computation.
I0520 12:28:03.139088 25059 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 12:28:03.139101 25059 net.cpp:228] data_hdf5 does not need backward computation.
I0520 12:28:03.139111 25059 net.cpp:270] This network produces output accuracy
I0520 12:28:03.139122 25059 net.cpp:270] This network produces output loss
I0520 12:28:03.139150 25059 net.cpp:283] Network initialization done.
I0520 12:28:03.139283 25059 solver.cpp:60] Solver scaffolding done.
I0520 12:28:03.140409 25059 caffe.cpp:212] Starting Optimization
I0520 12:28:03.140427 25059 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 12:28:03.140440 25059 solver.cpp:289] Learning Rate Policy: fixed
I0520 12:28:03.141677 25059 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 12:28:52.414729 25059 solver.cpp:409]     Test net output #0: accuracy = 0.0889335
I0520 12:28:52.414897 25059 solver.cpp:409]     Test net output #1: loss = 2.39797 (* 1 = 2.39797 loss)
I0520 12:28:52.437404 25059 solver.cpp:237] Iteration 0, loss = 2.39913
I0520 12:28:52.437441 25059 solver.cpp:253]     Train net output #0: loss = 2.39913 (* 1 = 2.39913 loss)
I0520 12:28:52.437459 25059 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 12:29:02.278266 25059 solver.cpp:237] Iteration 375, loss = 2.05624
I0520 12:29:02.278303 25059 solver.cpp:253]     Train net output #0: loss = 2.05624 (* 1 = 2.05624 loss)
I0520 12:29:02.278319 25059 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0520 12:29:12.118038 25059 solver.cpp:237] Iteration 750, loss = 2.07733
I0520 12:29:12.118074 25059 solver.cpp:253]     Train net output #0: loss = 2.07733 (* 1 = 2.07733 loss)
I0520 12:29:12.118089 25059 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0520 12:29:21.965755 25059 solver.cpp:237] Iteration 1125, loss = 2.05104
I0520 12:29:21.965788 25059 solver.cpp:253]     Train net output #0: loss = 2.05104 (* 1 = 2.05104 loss)
I0520 12:29:21.965809 25059 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0520 12:29:31.816397 25059 solver.cpp:237] Iteration 1500, loss = 1.60308
I0520 12:29:31.816545 25059 solver.cpp:253]     Train net output #0: loss = 1.60308 (* 1 = 1.60308 loss)
I0520 12:29:31.816560 25059 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0520 12:29:41.657752 25059 solver.cpp:237] Iteration 1875, loss = 1.80247
I0520 12:29:41.657795 25059 solver.cpp:253]     Train net output #0: loss = 1.80247 (* 1 = 1.80247 loss)
I0520 12:29:41.657810 25059 sgd_solver.cpp:106] Iteration 1875, lr = 0.0025
I0520 12:29:51.500475 25059 solver.cpp:237] Iteration 2250, loss = 1.82936
I0520 12:29:51.500510 25059 solver.cpp:253]     Train net output #0: loss = 1.82936 (* 1 = 1.82936 loss)
I0520 12:29:51.500526 25059 sgd_solver.cpp:106] Iteration 2250, lr = 0.0025
I0520 12:30:23.425793 25059 solver.cpp:237] Iteration 2625, loss = 1.61252
I0520 12:30:23.425958 25059 solver.cpp:253]     Train net output #0: loss = 1.61252 (* 1 = 1.61252 loss)
I0520 12:30:23.425973 25059 sgd_solver.cpp:106] Iteration 2625, lr = 0.0025
I0520 12:30:33.265779 25059 solver.cpp:237] Iteration 3000, loss = 1.31549
I0520 12:30:33.265827 25059 solver.cpp:253]     Train net output #0: loss = 1.31549 (* 1 = 1.31549 loss)
I0520 12:30:33.265844 25059 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0520 12:30:43.107283 25059 solver.cpp:237] Iteration 3375, loss = 1.96557
I0520 12:30:43.107319 25059 solver.cpp:253]     Train net output #0: loss = 1.96557 (* 1 = 1.96557 loss)
I0520 12:30:43.107336 25059 sgd_solver.cpp:106] Iteration 3375, lr = 0.0025
I0520 12:30:52.929328 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_3750.caffemodel
I0520 12:30:52.988872 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_3750.solverstate
I0520 12:30:53.022364 25059 solver.cpp:237] Iteration 3750, loss = 1.87844
I0520 12:30:53.022410 25059 solver.cpp:253]     Train net output #0: loss = 1.87844 (* 1 = 1.87844 loss)
I0520 12:30:53.022428 25059 sgd_solver.cpp:106] Iteration 3750, lr = 0.0025
I0520 12:31:02.865772 25059 solver.cpp:237] Iteration 4125, loss = 1.6217
I0520 12:31:02.865913 25059 solver.cpp:253]     Train net output #0: loss = 1.6217 (* 1 = 1.6217 loss)
I0520 12:31:02.865927 25059 sgd_solver.cpp:106] Iteration 4125, lr = 0.0025
I0520 12:31:12.720711 25059 solver.cpp:237] Iteration 4500, loss = 1.32219
I0520 12:31:12.720746 25059 solver.cpp:253]     Train net output #0: loss = 1.32219 (* 1 = 1.32219 loss)
I0520 12:31:12.720760 25059 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0520 12:31:22.560283 25059 solver.cpp:237] Iteration 4875, loss = 1.47985
I0520 12:31:22.560331 25059 solver.cpp:253]     Train net output #0: loss = 1.47985 (* 1 = 1.47985 loss)
I0520 12:31:22.560348 25059 sgd_solver.cpp:106] Iteration 4875, lr = 0.0025
I0520 12:31:54.487229 25059 solver.cpp:237] Iteration 5250, loss = 1.23983
I0520 12:31:54.487383 25059 solver.cpp:253]     Train net output #0: loss = 1.23983 (* 1 = 1.23983 loss)
I0520 12:31:54.487399 25059 sgd_solver.cpp:106] Iteration 5250, lr = 0.0025
I0520 12:32:04.329880 25059 solver.cpp:237] Iteration 5625, loss = 1.57914
I0520 12:32:04.329916 25059 solver.cpp:253]     Train net output #0: loss = 1.57914 (* 1 = 1.57914 loss)
I0520 12:32:04.329932 25059 sgd_solver.cpp:106] Iteration 5625, lr = 0.0025
I0520 12:32:14.173040 25059 solver.cpp:237] Iteration 6000, loss = 1.4718
I0520 12:32:14.173085 25059 solver.cpp:253]     Train net output #0: loss = 1.4718 (* 1 = 1.4718 loss)
I0520 12:32:14.173099 25059 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0520 12:32:24.015584 25059 solver.cpp:237] Iteration 6375, loss = 1.54278
I0520 12:32:24.015621 25059 solver.cpp:253]     Train net output #0: loss = 1.54278 (* 1 = 1.54278 loss)
I0520 12:32:24.015637 25059 sgd_solver.cpp:106] Iteration 6375, lr = 0.0025
I0520 12:32:33.865226 25059 solver.cpp:237] Iteration 6750, loss = 1.37082
I0520 12:32:33.865389 25059 solver.cpp:253]     Train net output #0: loss = 1.37082 (* 1 = 1.37082 loss)
I0520 12:32:33.865403 25059 sgd_solver.cpp:106] Iteration 6750, lr = 0.0025
I0520 12:32:43.707523 25059 solver.cpp:237] Iteration 7125, loss = 1.46477
I0520 12:32:43.707559 25059 solver.cpp:253]     Train net output #0: loss = 1.46477 (* 1 = 1.46477 loss)
I0520 12:32:43.707577 25059 sgd_solver.cpp:106] Iteration 7125, lr = 0.0025
I0520 12:32:53.523857 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_7500.caffemodel
I0520 12:32:53.580531 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_7500.solverstate
I0520 12:32:53.606385 25059 solver.cpp:341] Iteration 7500, Testing net (#0)
I0520 12:33:42.016302 25059 solver.cpp:409]     Test net output #0: accuracy = 0.805246
I0520 12:33:42.016461 25059 solver.cpp:409]     Test net output #1: loss = 0.681776 (* 1 = 0.681776 loss)
I0520 12:34:04.144522 25059 solver.cpp:237] Iteration 7500, loss = 1.346
I0520 12:34:04.144577 25059 solver.cpp:253]     Train net output #0: loss = 1.346 (* 1 = 1.346 loss)
I0520 12:34:04.144592 25059 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0520 12:34:14.048552 25059 solver.cpp:237] Iteration 7875, loss = 1.29322
I0520 12:34:14.048703 25059 solver.cpp:253]     Train net output #0: loss = 1.29322 (* 1 = 1.29322 loss)
I0520 12:34:14.048717 25059 sgd_solver.cpp:106] Iteration 7875, lr = 0.0025
I0520 12:34:23.958367 25059 solver.cpp:237] Iteration 8250, loss = 1.58403
I0520 12:34:23.958402 25059 solver.cpp:253]     Train net output #0: loss = 1.58403 (* 1 = 1.58403 loss)
I0520 12:34:23.958420 25059 sgd_solver.cpp:106] Iteration 8250, lr = 0.0025
I0520 12:34:33.865509 25059 solver.cpp:237] Iteration 8625, loss = 1.48949
I0520 12:34:33.865545 25059 solver.cpp:253]     Train net output #0: loss = 1.48949 (* 1 = 1.48949 loss)
I0520 12:34:33.865561 25059 sgd_solver.cpp:106] Iteration 8625, lr = 0.0025
I0520 12:34:43.773553 25059 solver.cpp:237] Iteration 9000, loss = 1.21469
I0520 12:34:43.773596 25059 solver.cpp:253]     Train net output #0: loss = 1.21469 (* 1 = 1.21469 loss)
I0520 12:34:43.773615 25059 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0520 12:34:53.672348 25059 solver.cpp:237] Iteration 9375, loss = 1.57587
I0520 12:34:53.672485 25059 solver.cpp:253]     Train net output #0: loss = 1.57587 (* 1 = 1.57587 loss)
I0520 12:34:53.672502 25059 sgd_solver.cpp:106] Iteration 9375, lr = 0.0025
I0520 12:35:03.585227 25059 solver.cpp:237] Iteration 9750, loss = 1.35795
I0520 12:35:03.585273 25059 solver.cpp:253]     Train net output #0: loss = 1.35795 (* 1 = 1.35795 loss)
I0520 12:35:03.585289 25059 sgd_solver.cpp:106] Iteration 9750, lr = 0.0025
I0520 12:35:35.609854 25059 solver.cpp:237] Iteration 10125, loss = 1.63713
I0520 12:35:35.610018 25059 solver.cpp:253]     Train net output #0: loss = 1.63713 (* 1 = 1.63713 loss)
I0520 12:35:35.610033 25059 sgd_solver.cpp:106] Iteration 10125, lr = 0.0025
I0520 12:35:45.519003 25059 solver.cpp:237] Iteration 10500, loss = 1.2959
I0520 12:35:45.519038 25059 solver.cpp:253]     Train net output #0: loss = 1.2959 (* 1 = 1.2959 loss)
I0520 12:35:45.519054 25059 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0520 12:35:55.419312 25059 solver.cpp:237] Iteration 10875, loss = 1.25671
I0520 12:35:55.419359 25059 solver.cpp:253]     Train net output #0: loss = 1.25671 (* 1 = 1.25671 loss)
I0520 12:35:55.419373 25059 sgd_solver.cpp:106] Iteration 10875, lr = 0.0025
I0520 12:36:05.296922 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_11250.caffemodel
I0520 12:36:05.355216 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_11250.solverstate
I0520 12:36:05.391685 25059 solver.cpp:237] Iteration 11250, loss = 1.25046
I0520 12:36:05.391736 25059 solver.cpp:253]     Train net output #0: loss = 1.25046 (* 1 = 1.25046 loss)
I0520 12:36:05.391749 25059 sgd_solver.cpp:106] Iteration 11250, lr = 0.0025
I0520 12:36:15.300026 25059 solver.cpp:237] Iteration 11625, loss = 1.30293
I0520 12:36:15.300179 25059 solver.cpp:253]     Train net output #0: loss = 1.30293 (* 1 = 1.30293 loss)
I0520 12:36:15.300191 25059 sgd_solver.cpp:106] Iteration 11625, lr = 0.0025
I0520 12:36:25.211424 25059 solver.cpp:237] Iteration 12000, loss = 1.09345
I0520 12:36:25.211462 25059 solver.cpp:253]     Train net output #0: loss = 1.09345 (* 1 = 1.09345 loss)
I0520 12:36:25.211483 25059 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0520 12:36:35.121865 25059 solver.cpp:237] Iteration 12375, loss = 1.68273
I0520 12:36:35.121901 25059 solver.cpp:253]     Train net output #0: loss = 1.68273 (* 1 = 1.68273 loss)
I0520 12:36:35.121915 25059 sgd_solver.cpp:106] Iteration 12375, lr = 0.0025
I0520 12:37:07.127378 25059 solver.cpp:237] Iteration 12750, loss = 1.22195
I0520 12:37:07.127542 25059 solver.cpp:253]     Train net output #0: loss = 1.22195 (* 1 = 1.22195 loss)
I0520 12:37:07.127557 25059 sgd_solver.cpp:106] Iteration 12750, lr = 0.0025
I0520 12:37:17.043736 25059 solver.cpp:237] Iteration 13125, loss = 1.36625
I0520 12:37:17.043773 25059 solver.cpp:253]     Train net output #0: loss = 1.36625 (* 1 = 1.36625 loss)
I0520 12:37:17.043790 25059 sgd_solver.cpp:106] Iteration 13125, lr = 0.0025
I0520 12:37:26.984135 25059 solver.cpp:237] Iteration 13500, loss = 1.31767
I0520 12:37:26.984171 25059 solver.cpp:253]     Train net output #0: loss = 1.31767 (* 1 = 1.31767 loss)
I0520 12:37:26.984189 25059 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0520 12:37:36.931165 25059 solver.cpp:237] Iteration 13875, loss = 1.46437
I0520 12:37:36.931210 25059 solver.cpp:253]     Train net output #0: loss = 1.46437 (* 1 = 1.46437 loss)
I0520 12:37:36.931226 25059 sgd_solver.cpp:106] Iteration 13875, lr = 0.0025
I0520 12:37:46.880107 25059 solver.cpp:237] Iteration 14250, loss = 1.47583
I0520 12:37:46.880246 25059 solver.cpp:253]     Train net output #0: loss = 1.47583 (* 1 = 1.47583 loss)
I0520 12:37:46.880259 25059 sgd_solver.cpp:106] Iteration 14250, lr = 0.0025
I0520 12:37:56.822401 25059 solver.cpp:237] Iteration 14625, loss = 1.26513
I0520 12:37:56.822435 25059 solver.cpp:253]     Train net output #0: loss = 1.26513 (* 1 = 1.26513 loss)
I0520 12:37:56.822451 25059 sgd_solver.cpp:106] Iteration 14625, lr = 0.0025
I0520 12:38:06.744719 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_15000.caffemodel
I0520 12:38:06.803750 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_15000.solverstate
I0520 12:38:06.832383 25059 solver.cpp:341] Iteration 15000, Testing net (#0)
I0520 12:39:16.148627 25059 solver.cpp:409]     Test net output #0: accuracy = 0.845306
I0520 12:39:16.148795 25059 solver.cpp:409]     Test net output #1: loss = 0.546164 (* 1 = 0.546164 loss)
I0520 12:39:38.256595 25059 solver.cpp:237] Iteration 15000, loss = 1.23437
I0520 12:39:38.256649 25059 solver.cpp:253]     Train net output #0: loss = 1.23437 (* 1 = 1.23437 loss)
I0520 12:39:38.256664 25059 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0520 12:39:47.999104 25059 solver.cpp:237] Iteration 15375, loss = 0.941288
I0520 12:39:47.999251 25059 solver.cpp:253]     Train net output #0: loss = 0.941288 (* 1 = 0.941288 loss)
I0520 12:39:47.999265 25059 sgd_solver.cpp:106] Iteration 15375, lr = 0.0025
I0520 12:39:57.744303 25059 solver.cpp:237] Iteration 15750, loss = 1.47635
I0520 12:39:57.744338 25059 solver.cpp:253]     Train net output #0: loss = 1.47634 (* 1 = 1.47634 loss)
I0520 12:39:57.744356 25059 sgd_solver.cpp:106] Iteration 15750, lr = 0.0025
I0520 12:40:07.488605 25059 solver.cpp:237] Iteration 16125, loss = 1.33159
I0520 12:40:07.488649 25059 solver.cpp:253]     Train net output #0: loss = 1.33159 (* 1 = 1.33159 loss)
I0520 12:40:07.488668 25059 sgd_solver.cpp:106] Iteration 16125, lr = 0.0025
I0520 12:40:17.238338 25059 solver.cpp:237] Iteration 16500, loss = 1.56362
I0520 12:40:17.238374 25059 solver.cpp:253]     Train net output #0: loss = 1.56362 (* 1 = 1.56362 loss)
I0520 12:40:17.238390 25059 sgd_solver.cpp:106] Iteration 16500, lr = 0.0025
I0520 12:40:26.979431 25059 solver.cpp:237] Iteration 16875, loss = 1.21331
I0520 12:40:26.979573 25059 solver.cpp:253]     Train net output #0: loss = 1.21331 (* 1 = 1.21331 loss)
I0520 12:40:26.979586 25059 sgd_solver.cpp:106] Iteration 16875, lr = 0.0025
I0520 12:40:36.725523 25059 solver.cpp:237] Iteration 17250, loss = 1.20811
I0520 12:40:36.725570 25059 solver.cpp:253]     Train net output #0: loss = 1.20811 (* 1 = 1.20811 loss)
I0520 12:40:36.725584 25059 sgd_solver.cpp:106] Iteration 17250, lr = 0.0025
I0520 12:41:08.553741 25059 solver.cpp:237] Iteration 17625, loss = 1.24594
I0520 12:41:08.553900 25059 solver.cpp:253]     Train net output #0: loss = 1.24594 (* 1 = 1.24594 loss)
I0520 12:41:08.553915 25059 sgd_solver.cpp:106] Iteration 17625, lr = 0.0025
I0520 12:41:18.301668 25059 solver.cpp:237] Iteration 18000, loss = 1.16086
I0520 12:41:18.301717 25059 solver.cpp:253]     Train net output #0: loss = 1.16086 (* 1 = 1.16086 loss)
I0520 12:41:18.301731 25059 sgd_solver.cpp:106] Iteration 18000, lr = 0.0025
I0520 12:41:28.039985 25059 solver.cpp:237] Iteration 18375, loss = 1.07535
I0520 12:41:28.040021 25059 solver.cpp:253]     Train net output #0: loss = 1.07535 (* 1 = 1.07535 loss)
I0520 12:41:28.040040 25059 sgd_solver.cpp:106] Iteration 18375, lr = 0.0025
I0520 12:41:37.750166 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_18750.caffemodel
I0520 12:41:37.808403 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_18750.solverstate
I0520 12:41:37.844029 25059 solver.cpp:237] Iteration 18750, loss = 1.25808
I0520 12:41:37.844079 25059 solver.cpp:253]     Train net output #0: loss = 1.25808 (* 1 = 1.25808 loss)
I0520 12:41:37.844094 25059 sgd_solver.cpp:106] Iteration 18750, lr = 0.0025
I0520 12:41:47.599778 25059 solver.cpp:237] Iteration 19125, loss = 1.18774
I0520 12:41:47.599942 25059 solver.cpp:253]     Train net output #0: loss = 1.18774 (* 1 = 1.18774 loss)
I0520 12:41:47.599956 25059 sgd_solver.cpp:106] Iteration 19125, lr = 0.0025
I0520 12:41:57.342037 25059 solver.cpp:237] Iteration 19500, loss = 1.42259
I0520 12:41:57.342072 25059 solver.cpp:253]     Train net output #0: loss = 1.42259 (* 1 = 1.42259 loss)
I0520 12:41:57.342085 25059 sgd_solver.cpp:106] Iteration 19500, lr = 0.0025
I0520 12:42:07.087894 25059 solver.cpp:237] Iteration 19875, loss = 1.51392
I0520 12:42:07.087930 25059 solver.cpp:253]     Train net output #0: loss = 1.51392 (* 1 = 1.51392 loss)
I0520 12:42:07.087946 25059 sgd_solver.cpp:106] Iteration 19875, lr = 0.0025
I0520 12:42:38.961038 25059 solver.cpp:237] Iteration 20250, loss = 1.3348
I0520 12:42:38.961220 25059 solver.cpp:253]     Train net output #0: loss = 1.3348 (* 1 = 1.3348 loss)
I0520 12:42:38.961236 25059 sgd_solver.cpp:106] Iteration 20250, lr = 0.0025
I0520 12:42:48.718673 25059 solver.cpp:237] Iteration 20625, loss = 1.16222
I0520 12:42:48.718708 25059 solver.cpp:253]     Train net output #0: loss = 1.16222 (* 1 = 1.16222 loss)
I0520 12:42:48.718725 25059 sgd_solver.cpp:106] Iteration 20625, lr = 0.0025
I0520 12:42:58.459981 25059 solver.cpp:237] Iteration 21000, loss = 1.25906
I0520 12:42:58.460031 25059 solver.cpp:253]     Train net output #0: loss = 1.25906 (* 1 = 1.25906 loss)
I0520 12:42:58.460046 25059 sgd_solver.cpp:106] Iteration 21000, lr = 0.0025
I0520 12:43:08.210945 25059 solver.cpp:237] Iteration 21375, loss = 1.59929
I0520 12:43:08.210980 25059 solver.cpp:253]     Train net output #0: loss = 1.59929 (* 1 = 1.59929 loss)
I0520 12:43:08.211005 25059 sgd_solver.cpp:106] Iteration 21375, lr = 0.0025
I0520 12:43:17.964206 25059 solver.cpp:237] Iteration 21750, loss = 1.16277
I0520 12:43:17.964360 25059 solver.cpp:253]     Train net output #0: loss = 1.16277 (* 1 = 1.16277 loss)
I0520 12:43:17.964375 25059 sgd_solver.cpp:106] Iteration 21750, lr = 0.0025
I0520 12:43:27.709455 25059 solver.cpp:237] Iteration 22125, loss = 0.795099
I0520 12:43:27.709504 25059 solver.cpp:253]     Train net output #0: loss = 0.795099 (* 1 = 0.795099 loss)
I0520 12:43:27.709519 25059 sgd_solver.cpp:106] Iteration 22125, lr = 0.0025
I0520 12:43:37.436287 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_22500.caffemodel
I0520 12:43:37.492017 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_22500.solverstate
I0520 12:43:37.516949 25059 solver.cpp:341] Iteration 22500, Testing net (#0)
I0520 12:44:25.623571 25059 solver.cpp:409]     Test net output #0: accuracy = 0.859827
I0520 12:44:25.623731 25059 solver.cpp:409]     Test net output #1: loss = 0.449415 (* 1 = 0.449415 loss)
I0520 12:44:47.708829 25059 solver.cpp:237] Iteration 22500, loss = 1.15
I0520 12:44:47.708880 25059 solver.cpp:253]     Train net output #0: loss = 1.15 (* 1 = 1.15 loss)
I0520 12:44:47.708899 25059 sgd_solver.cpp:106] Iteration 22500, lr = 0.0025
I0520 12:44:57.455080 25059 solver.cpp:237] Iteration 22875, loss = 1.29903
I0520 12:44:57.455232 25059 solver.cpp:253]     Train net output #0: loss = 1.29903 (* 1 = 1.29903 loss)
I0520 12:44:57.455247 25059 sgd_solver.cpp:106] Iteration 22875, lr = 0.0025
I0520 12:45:07.208578 25059 solver.cpp:237] Iteration 23250, loss = 1.17553
I0520 12:45:07.208621 25059 solver.cpp:253]     Train net output #0: loss = 1.17553 (* 1 = 1.17553 loss)
I0520 12:45:07.208642 25059 sgd_solver.cpp:106] Iteration 23250, lr = 0.0025
I0520 12:45:16.963356 25059 solver.cpp:237] Iteration 23625, loss = 1.42445
I0520 12:45:16.963392 25059 solver.cpp:253]     Train net output #0: loss = 1.42445 (* 1 = 1.42445 loss)
I0520 12:45:16.963405 25059 sgd_solver.cpp:106] Iteration 23625, lr = 0.0025
I0520 12:45:26.715389 25059 solver.cpp:237] Iteration 24000, loss = 0.896622
I0520 12:45:26.715425 25059 solver.cpp:253]     Train net output #0: loss = 0.896622 (* 1 = 0.896622 loss)
I0520 12:45:26.715442 25059 sgd_solver.cpp:106] Iteration 24000, lr = 0.0025
I0520 12:45:36.467519 25059 solver.cpp:237] Iteration 24375, loss = 1.33839
I0520 12:45:36.467670 25059 solver.cpp:253]     Train net output #0: loss = 1.33838 (* 1 = 1.33838 loss)
I0520 12:45:36.467684 25059 sgd_solver.cpp:106] Iteration 24375, lr = 0.0025
I0520 12:45:46.223274 25059 solver.cpp:237] Iteration 24750, loss = 1.11012
I0520 12:45:46.223309 25059 solver.cpp:253]     Train net output #0: loss = 1.11012 (* 1 = 1.11012 loss)
I0520 12:45:46.223323 25059 sgd_solver.cpp:106] Iteration 24750, lr = 0.0025
I0520 12:46:18.078054 25059 solver.cpp:237] Iteration 25125, loss = 0.969044
I0520 12:46:18.078228 25059 solver.cpp:253]     Train net output #0: loss = 0.969044 (* 1 = 0.969044 loss)
I0520 12:46:18.078243 25059 sgd_solver.cpp:106] Iteration 25125, lr = 0.0025
I0520 12:46:27.825183 25059 solver.cpp:237] Iteration 25500, loss = 1.114
I0520 12:46:27.825219 25059 solver.cpp:253]     Train net output #0: loss = 1.114 (* 1 = 1.114 loss)
I0520 12:46:27.825233 25059 sgd_solver.cpp:106] Iteration 25500, lr = 0.0025
I0520 12:46:37.574880 25059 solver.cpp:237] Iteration 25875, loss = 1.06912
I0520 12:46:37.574916 25059 solver.cpp:253]     Train net output #0: loss = 1.06912 (* 1 = 1.06912 loss)
I0520 12:46:37.574934 25059 sgd_solver.cpp:106] Iteration 25875, lr = 0.0025
I0520 12:46:47.300343 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_26250.caffemodel
I0520 12:46:47.356650 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_26250.solverstate
I0520 12:46:47.390018 25059 solver.cpp:237] Iteration 26250, loss = 1.19176
I0520 12:46:47.390064 25059 solver.cpp:253]     Train net output #0: loss = 1.19176 (* 1 = 1.19176 loss)
I0520 12:46:47.390081 25059 sgd_solver.cpp:106] Iteration 26250, lr = 0.0025
I0520 12:46:57.145005 25059 solver.cpp:237] Iteration 26625, loss = 1.15812
I0520 12:46:57.145164 25059 solver.cpp:253]     Train net output #0: loss = 1.15812 (* 1 = 1.15812 loss)
I0520 12:46:57.145179 25059 sgd_solver.cpp:106] Iteration 26625, lr = 0.0025
I0520 12:47:06.895027 25059 solver.cpp:237] Iteration 27000, loss = 1.20003
I0520 12:47:06.895062 25059 solver.cpp:253]     Train net output #0: loss = 1.20003 (* 1 = 1.20003 loss)
I0520 12:47:06.895079 25059 sgd_solver.cpp:106] Iteration 27000, lr = 0.0025
I0520 12:47:16.646780 25059 solver.cpp:237] Iteration 27375, loss = 1.07225
I0520 12:47:16.646824 25059 solver.cpp:253]     Train net output #0: loss = 1.07225 (* 1 = 1.07225 loss)
I0520 12:47:16.646841 25059 sgd_solver.cpp:106] Iteration 27375, lr = 0.0025
I0520 12:47:48.539275 25059 solver.cpp:237] Iteration 27750, loss = 1.30838
I0520 12:47:48.539444 25059 solver.cpp:253]     Train net output #0: loss = 1.30838 (* 1 = 1.30838 loss)
I0520 12:47:48.539459 25059 sgd_solver.cpp:106] Iteration 27750, lr = 0.0025
I0520 12:47:58.288092 25059 solver.cpp:237] Iteration 28125, loss = 1.31913
I0520 12:47:58.288136 25059 solver.cpp:253]     Train net output #0: loss = 1.31913 (* 1 = 1.31913 loss)
I0520 12:47:58.288151 25059 sgd_solver.cpp:106] Iteration 28125, lr = 0.0025
I0520 12:48:08.044193 25059 solver.cpp:237] Iteration 28500, loss = 0.865986
I0520 12:48:08.044229 25059 solver.cpp:253]     Train net output #0: loss = 0.865986 (* 1 = 0.865986 loss)
I0520 12:48:08.044244 25059 sgd_solver.cpp:106] Iteration 28500, lr = 0.0025
I0520 12:48:17.794515 25059 solver.cpp:237] Iteration 28875, loss = 1.1389
I0520 12:48:17.794550 25059 solver.cpp:253]     Train net output #0: loss = 1.1389 (* 1 = 1.1389 loss)
I0520 12:48:17.794566 25059 sgd_solver.cpp:106] Iteration 28875, lr = 0.0025
I0520 12:48:27.544836 25059 solver.cpp:237] Iteration 29250, loss = 1.30551
I0520 12:48:27.544994 25059 solver.cpp:253]     Train net output #0: loss = 1.30551 (* 1 = 1.30551 loss)
I0520 12:48:27.545008 25059 sgd_solver.cpp:106] Iteration 29250, lr = 0.0025
I0520 12:48:37.291152 25059 solver.cpp:237] Iteration 29625, loss = 1.24931
I0520 12:48:37.291188 25059 solver.cpp:253]     Train net output #0: loss = 1.24931 (* 1 = 1.24931 loss)
I0520 12:48:37.291205 25059 sgd_solver.cpp:106] Iteration 29625, lr = 0.0025
I0520 12:48:47.022166 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_30000.caffemodel
I0520 12:48:47.079032 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_30000.solverstate
I0520 12:48:47.104405 25059 solver.cpp:341] Iteration 30000, Testing net (#0)
I0520 12:49:56.354120 25059 solver.cpp:409]     Test net output #0: accuracy = 0.8671
I0520 12:49:56.354295 25059 solver.cpp:409]     Test net output #1: loss = 0.43088 (* 1 = 0.43088 loss)
I0520 12:50:18.438479 25059 solver.cpp:237] Iteration 30000, loss = 1.20582
I0520 12:50:18.438532 25059 solver.cpp:253]     Train net output #0: loss = 1.20582 (* 1 = 1.20582 loss)
I0520 12:50:18.438549 25059 sgd_solver.cpp:106] Iteration 30000, lr = 0.0025
I0520 12:50:28.321874 25059 solver.cpp:237] Iteration 30375, loss = 1.26871
I0520 12:50:28.322046 25059 solver.cpp:253]     Train net output #0: loss = 1.26871 (* 1 = 1.26871 loss)
I0520 12:50:28.322060 25059 sgd_solver.cpp:106] Iteration 30375, lr = 0.0025
I0520 12:50:38.206315 25059 solver.cpp:237] Iteration 30750, loss = 1.18792
I0520 12:50:38.206351 25059 solver.cpp:253]     Train net output #0: loss = 1.18792 (* 1 = 1.18792 loss)
I0520 12:50:38.206369 25059 sgd_solver.cpp:106] Iteration 30750, lr = 0.0025
I0520 12:50:48.092661 25059 solver.cpp:237] Iteration 31125, loss = 0.997904
I0520 12:50:48.092697 25059 solver.cpp:253]     Train net output #0: loss = 0.997904 (* 1 = 0.997904 loss)
I0520 12:50:48.092713 25059 sgd_solver.cpp:106] Iteration 31125, lr = 0.0025
I0520 12:50:57.979938 25059 solver.cpp:237] Iteration 31500, loss = 1.18778
I0520 12:50:57.979976 25059 solver.cpp:253]     Train net output #0: loss = 1.18778 (* 1 = 1.18778 loss)
I0520 12:50:57.979995 25059 sgd_solver.cpp:106] Iteration 31500, lr = 0.0025
I0520 12:51:07.865098 25059 solver.cpp:237] Iteration 31875, loss = 0.998878
I0520 12:51:07.865247 25059 solver.cpp:253]     Train net output #0: loss = 0.998878 (* 1 = 0.998878 loss)
I0520 12:51:07.865260 25059 sgd_solver.cpp:106] Iteration 31875, lr = 0.0025
I0520 12:51:17.748975 25059 solver.cpp:237] Iteration 32250, loss = 1.27749
I0520 12:51:17.749020 25059 solver.cpp:253]     Train net output #0: loss = 1.27749 (* 1 = 1.27749 loss)
I0520 12:51:17.749038 25059 sgd_solver.cpp:106] Iteration 32250, lr = 0.0025
I0520 12:51:49.721074 25059 solver.cpp:237] Iteration 32625, loss = 1.11747
I0520 12:51:49.721247 25059 solver.cpp:253]     Train net output #0: loss = 1.11747 (* 1 = 1.11747 loss)
I0520 12:51:49.721263 25059 sgd_solver.cpp:106] Iteration 32625, lr = 0.0025
I0520 12:51:59.612457 25059 solver.cpp:237] Iteration 33000, loss = 1.60243
I0520 12:51:59.612491 25059 solver.cpp:253]     Train net output #0: loss = 1.60243 (* 1 = 1.60243 loss)
I0520 12:51:59.612506 25059 sgd_solver.cpp:106] Iteration 33000, lr = 0.0025
I0520 12:52:09.497241 25059 solver.cpp:237] Iteration 33375, loss = 1.24697
I0520 12:52:09.497287 25059 solver.cpp:253]     Train net output #0: loss = 1.24697 (* 1 = 1.24697 loss)
I0520 12:52:09.497303 25059 sgd_solver.cpp:106] Iteration 33375, lr = 0.0025
I0520 12:52:19.356667 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_33750.caffemodel
I0520 12:52:19.414986 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_33750.solverstate
I0520 12:52:19.450765 25059 solver.cpp:237] Iteration 33750, loss = 1.34375
I0520 12:52:19.450816 25059 solver.cpp:253]     Train net output #0: loss = 1.34375 (* 1 = 1.34375 loss)
I0520 12:52:19.450831 25059 sgd_solver.cpp:106] Iteration 33750, lr = 0.0025
I0520 12:52:29.336098 25059 solver.cpp:237] Iteration 34125, loss = 1.31088
I0520 12:52:29.336249 25059 solver.cpp:253]     Train net output #0: loss = 1.31088 (* 1 = 1.31088 loss)
I0520 12:52:29.336264 25059 sgd_solver.cpp:106] Iteration 34125, lr = 0.0025
I0520 12:52:39.226788 25059 solver.cpp:237] Iteration 34500, loss = 1.38447
I0520 12:52:39.226835 25059 solver.cpp:253]     Train net output #0: loss = 1.38447 (* 1 = 1.38447 loss)
I0520 12:52:39.226855 25059 sgd_solver.cpp:106] Iteration 34500, lr = 0.0025
I0520 12:52:49.110808 25059 solver.cpp:237] Iteration 34875, loss = 1.36513
I0520 12:52:49.110843 25059 solver.cpp:253]     Train net output #0: loss = 1.36513 (* 1 = 1.36513 loss)
I0520 12:52:49.110857 25059 sgd_solver.cpp:106] Iteration 34875, lr = 0.0025
I0520 12:53:21.166587 25059 solver.cpp:237] Iteration 35250, loss = 1.09901
I0520 12:53:21.166772 25059 solver.cpp:253]     Train net output #0: loss = 1.09901 (* 1 = 1.09901 loss)
I0520 12:53:21.166788 25059 sgd_solver.cpp:106] Iteration 35250, lr = 0.0025
I0520 12:53:31.048974 25059 solver.cpp:237] Iteration 35625, loss = 1.34737
I0520 12:53:31.049017 25059 solver.cpp:253]     Train net output #0: loss = 1.34737 (* 1 = 1.34737 loss)
I0520 12:53:31.049034 25059 sgd_solver.cpp:106] Iteration 35625, lr = 0.0025
I0520 12:53:40.937897 25059 solver.cpp:237] Iteration 36000, loss = 1.00933
I0520 12:53:40.937933 25059 solver.cpp:253]     Train net output #0: loss = 1.00933 (* 1 = 1.00933 loss)
I0520 12:53:40.937947 25059 sgd_solver.cpp:106] Iteration 36000, lr = 0.0025
I0520 12:53:50.822312 25059 solver.cpp:237] Iteration 36375, loss = 1.75259
I0520 12:53:50.822358 25059 solver.cpp:253]     Train net output #0: loss = 1.75259 (* 1 = 1.75259 loss)
I0520 12:53:50.822374 25059 sgd_solver.cpp:106] Iteration 36375, lr = 0.0025
I0520 12:54:00.708672 25059 solver.cpp:237] Iteration 36750, loss = 1.02035
I0520 12:54:00.708813 25059 solver.cpp:253]     Train net output #0: loss = 1.02035 (* 1 = 1.02035 loss)
I0520 12:54:00.708827 25059 sgd_solver.cpp:106] Iteration 36750, lr = 0.0025
I0520 12:54:10.600968 25059 solver.cpp:237] Iteration 37125, loss = 1.33287
I0520 12:54:10.601003 25059 solver.cpp:253]     Train net output #0: loss = 1.33287 (* 1 = 1.33287 loss)
I0520 12:54:10.601021 25059 sgd_solver.cpp:106] Iteration 37125, lr = 0.0025
I0520 12:54:20.460784 25059 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_37500.caffemodel
I0520 12:54:20.523746 25059 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583_iter_37500.solverstate
I0520 12:54:41.470748 25059 solver.cpp:321] Iteration 37500, loss = 1.07514
I0520 12:54:41.470918 25059 solver.cpp:341] Iteration 37500, Testing net (#0)
I0520 12:55:29.949476 25059 solver.cpp:409]     Test net output #0: accuracy = 0.875487
I0520 12:55:29.949638 25059 solver.cpp:409]     Test net output #1: loss = 0.409506 (* 1 = 0.409506 loss)
I0520 12:55:29.949653 25059 solver.cpp:326] Optimization Done.
I0520 12:55:29.949664 25059 caffe.cpp:215] Optimization Done.
Application 11232101 resources: utime ~1448s, stime ~245s, Rss ~5332392, inblocks ~3744348, outblocks ~179816
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_40_2016-05-20T11.20.34.161583.solver"
	User time (seconds): 0.56
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 28:16.14
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15075
	Voluntary context switches: 2884
	Involuntary context switches: 75
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
