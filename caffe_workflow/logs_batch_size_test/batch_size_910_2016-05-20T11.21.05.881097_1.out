2806400
I0521 09:32:15.729027 31223 caffe.cpp:184] Using GPUs 0
I0521 09:32:16.151209 31223 solver.cpp:48] Initializing solver from parameters: 
test_iter: 164
test_interval: 329
base_lr: 0.0025
display: 16
max_iter: 1648
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 164
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097.prototxt"
I0521 09:32:16.152762 31223 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097.prototxt
I0521 09:32:16.175981 31223 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 09:32:16.176041 31223 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 09:32:16.176385 31223 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 910
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:32:16.176568 31223 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:32:16.176591 31223 net.cpp:106] Creating Layer data_hdf5
I0521 09:32:16.176606 31223 net.cpp:411] data_hdf5 -> data
I0521 09:32:16.176640 31223 net.cpp:411] data_hdf5 -> label
I0521 09:32:16.176672 31223 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 09:32:16.178043 31223 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 09:32:16.180271 31223 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 09:32:37.762953 31223 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 09:32:37.768059 31223 net.cpp:150] Setting up data_hdf5
I0521 09:32:37.768103 31223 net.cpp:157] Top shape: 910 1 127 50 (5778500)
I0521 09:32:37.768118 31223 net.cpp:157] Top shape: 910 (910)
I0521 09:32:37.768128 31223 net.cpp:165] Memory required for data: 23117640
I0521 09:32:37.768142 31223 layer_factory.hpp:77] Creating layer conv1
I0521 09:32:37.768177 31223 net.cpp:106] Creating Layer conv1
I0521 09:32:37.768188 31223 net.cpp:454] conv1 <- data
I0521 09:32:37.768213 31223 net.cpp:411] conv1 -> conv1
I0521 09:32:38.131836 31223 net.cpp:150] Setting up conv1
I0521 09:32:38.131883 31223 net.cpp:157] Top shape: 910 12 120 48 (62899200)
I0521 09:32:38.131894 31223 net.cpp:165] Memory required for data: 274714440
I0521 09:32:38.131922 31223 layer_factory.hpp:77] Creating layer relu1
I0521 09:32:38.131943 31223 net.cpp:106] Creating Layer relu1
I0521 09:32:38.131954 31223 net.cpp:454] relu1 <- conv1
I0521 09:32:38.131968 31223 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:32:38.132488 31223 net.cpp:150] Setting up relu1
I0521 09:32:38.132505 31223 net.cpp:157] Top shape: 910 12 120 48 (62899200)
I0521 09:32:38.132516 31223 net.cpp:165] Memory required for data: 526311240
I0521 09:32:38.132527 31223 layer_factory.hpp:77] Creating layer pool1
I0521 09:32:38.132545 31223 net.cpp:106] Creating Layer pool1
I0521 09:32:38.132555 31223 net.cpp:454] pool1 <- conv1
I0521 09:32:38.132568 31223 net.cpp:411] pool1 -> pool1
I0521 09:32:38.132648 31223 net.cpp:150] Setting up pool1
I0521 09:32:38.132663 31223 net.cpp:157] Top shape: 910 12 60 48 (31449600)
I0521 09:32:38.132673 31223 net.cpp:165] Memory required for data: 652109640
I0521 09:32:38.132680 31223 layer_factory.hpp:77] Creating layer conv2
I0521 09:32:38.132704 31223 net.cpp:106] Creating Layer conv2
I0521 09:32:38.132714 31223 net.cpp:454] conv2 <- pool1
I0521 09:32:38.132725 31223 net.cpp:411] conv2 -> conv2
I0521 09:32:38.135424 31223 net.cpp:150] Setting up conv2
I0521 09:32:38.135453 31223 net.cpp:157] Top shape: 910 20 54 46 (45208800)
I0521 09:32:38.135463 31223 net.cpp:165] Memory required for data: 832944840
I0521 09:32:38.135483 31223 layer_factory.hpp:77] Creating layer relu2
I0521 09:32:38.135498 31223 net.cpp:106] Creating Layer relu2
I0521 09:32:38.135507 31223 net.cpp:454] relu2 <- conv2
I0521 09:32:38.135520 31223 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:32:38.135850 31223 net.cpp:150] Setting up relu2
I0521 09:32:38.135865 31223 net.cpp:157] Top shape: 910 20 54 46 (45208800)
I0521 09:32:38.135875 31223 net.cpp:165] Memory required for data: 1013780040
I0521 09:32:38.135885 31223 layer_factory.hpp:77] Creating layer pool2
I0521 09:32:38.135898 31223 net.cpp:106] Creating Layer pool2
I0521 09:32:38.135910 31223 net.cpp:454] pool2 <- conv2
I0521 09:32:38.135934 31223 net.cpp:411] pool2 -> pool2
I0521 09:32:38.136004 31223 net.cpp:150] Setting up pool2
I0521 09:32:38.136018 31223 net.cpp:157] Top shape: 910 20 27 46 (22604400)
I0521 09:32:38.136028 31223 net.cpp:165] Memory required for data: 1104197640
I0521 09:32:38.136036 31223 layer_factory.hpp:77] Creating layer conv3
I0521 09:32:38.136055 31223 net.cpp:106] Creating Layer conv3
I0521 09:32:38.136065 31223 net.cpp:454] conv3 <- pool2
I0521 09:32:38.136078 31223 net.cpp:411] conv3 -> conv3
I0521 09:32:38.138015 31223 net.cpp:150] Setting up conv3
I0521 09:32:38.138038 31223 net.cpp:157] Top shape: 910 28 22 44 (24664640)
I0521 09:32:38.138051 31223 net.cpp:165] Memory required for data: 1202856200
I0521 09:32:38.138069 31223 layer_factory.hpp:77] Creating layer relu3
I0521 09:32:38.138085 31223 net.cpp:106] Creating Layer relu3
I0521 09:32:38.138095 31223 net.cpp:454] relu3 <- conv3
I0521 09:32:38.138108 31223 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:32:38.138586 31223 net.cpp:150] Setting up relu3
I0521 09:32:38.138603 31223 net.cpp:157] Top shape: 910 28 22 44 (24664640)
I0521 09:32:38.138613 31223 net.cpp:165] Memory required for data: 1301514760
I0521 09:32:38.138624 31223 layer_factory.hpp:77] Creating layer pool3
I0521 09:32:38.138636 31223 net.cpp:106] Creating Layer pool3
I0521 09:32:38.138646 31223 net.cpp:454] pool3 <- conv3
I0521 09:32:38.138659 31223 net.cpp:411] pool3 -> pool3
I0521 09:32:38.138726 31223 net.cpp:150] Setting up pool3
I0521 09:32:38.138741 31223 net.cpp:157] Top shape: 910 28 11 44 (12332320)
I0521 09:32:38.138751 31223 net.cpp:165] Memory required for data: 1350844040
I0521 09:32:38.138761 31223 layer_factory.hpp:77] Creating layer conv4
I0521 09:32:38.138777 31223 net.cpp:106] Creating Layer conv4
I0521 09:32:38.138787 31223 net.cpp:454] conv4 <- pool3
I0521 09:32:38.138800 31223 net.cpp:411] conv4 -> conv4
I0521 09:32:38.141527 31223 net.cpp:150] Setting up conv4
I0521 09:32:38.141556 31223 net.cpp:157] Top shape: 910 36 6 42 (8255520)
I0521 09:32:38.141566 31223 net.cpp:165] Memory required for data: 1383866120
I0521 09:32:38.141582 31223 layer_factory.hpp:77] Creating layer relu4
I0521 09:32:38.141597 31223 net.cpp:106] Creating Layer relu4
I0521 09:32:38.141607 31223 net.cpp:454] relu4 <- conv4
I0521 09:32:38.141619 31223 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:32:38.142082 31223 net.cpp:150] Setting up relu4
I0521 09:32:38.142099 31223 net.cpp:157] Top shape: 910 36 6 42 (8255520)
I0521 09:32:38.142110 31223 net.cpp:165] Memory required for data: 1416888200
I0521 09:32:38.142120 31223 layer_factory.hpp:77] Creating layer pool4
I0521 09:32:38.142133 31223 net.cpp:106] Creating Layer pool4
I0521 09:32:38.142144 31223 net.cpp:454] pool4 <- conv4
I0521 09:32:38.142158 31223 net.cpp:411] pool4 -> pool4
I0521 09:32:38.142225 31223 net.cpp:150] Setting up pool4
I0521 09:32:38.142237 31223 net.cpp:157] Top shape: 910 36 3 42 (4127760)
I0521 09:32:38.142248 31223 net.cpp:165] Memory required for data: 1433399240
I0521 09:32:38.142258 31223 layer_factory.hpp:77] Creating layer ip1
I0521 09:32:38.142279 31223 net.cpp:106] Creating Layer ip1
I0521 09:32:38.142289 31223 net.cpp:454] ip1 <- pool4
I0521 09:32:38.142302 31223 net.cpp:411] ip1 -> ip1
I0521 09:32:38.157749 31223 net.cpp:150] Setting up ip1
I0521 09:32:38.157773 31223 net.cpp:157] Top shape: 910 196 (178360)
I0521 09:32:38.157789 31223 net.cpp:165] Memory required for data: 1434112680
I0521 09:32:38.157812 31223 layer_factory.hpp:77] Creating layer relu5
I0521 09:32:38.157827 31223 net.cpp:106] Creating Layer relu5
I0521 09:32:38.157837 31223 net.cpp:454] relu5 <- ip1
I0521 09:32:38.157851 31223 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:32:38.158195 31223 net.cpp:150] Setting up relu5
I0521 09:32:38.158210 31223 net.cpp:157] Top shape: 910 196 (178360)
I0521 09:32:38.158220 31223 net.cpp:165] Memory required for data: 1434826120
I0521 09:32:38.158229 31223 layer_factory.hpp:77] Creating layer drop1
I0521 09:32:38.158252 31223 net.cpp:106] Creating Layer drop1
I0521 09:32:38.158262 31223 net.cpp:454] drop1 <- ip1
I0521 09:32:38.158288 31223 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:32:38.158334 31223 net.cpp:150] Setting up drop1
I0521 09:32:38.158347 31223 net.cpp:157] Top shape: 910 196 (178360)
I0521 09:32:38.158359 31223 net.cpp:165] Memory required for data: 1435539560
I0521 09:32:38.158370 31223 layer_factory.hpp:77] Creating layer ip2
I0521 09:32:38.158390 31223 net.cpp:106] Creating Layer ip2
I0521 09:32:38.158399 31223 net.cpp:454] ip2 <- ip1
I0521 09:32:38.158412 31223 net.cpp:411] ip2 -> ip2
I0521 09:32:38.158879 31223 net.cpp:150] Setting up ip2
I0521 09:32:38.158892 31223 net.cpp:157] Top shape: 910 98 (89180)
I0521 09:32:38.158901 31223 net.cpp:165] Memory required for data: 1435896280
I0521 09:32:38.158917 31223 layer_factory.hpp:77] Creating layer relu6
I0521 09:32:38.158929 31223 net.cpp:106] Creating Layer relu6
I0521 09:32:38.158939 31223 net.cpp:454] relu6 <- ip2
I0521 09:32:38.158951 31223 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:32:38.159466 31223 net.cpp:150] Setting up relu6
I0521 09:32:38.159482 31223 net.cpp:157] Top shape: 910 98 (89180)
I0521 09:32:38.159492 31223 net.cpp:165] Memory required for data: 1436253000
I0521 09:32:38.159502 31223 layer_factory.hpp:77] Creating layer drop2
I0521 09:32:38.159515 31223 net.cpp:106] Creating Layer drop2
I0521 09:32:38.159525 31223 net.cpp:454] drop2 <- ip2
I0521 09:32:38.159538 31223 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:32:38.159580 31223 net.cpp:150] Setting up drop2
I0521 09:32:38.159593 31223 net.cpp:157] Top shape: 910 98 (89180)
I0521 09:32:38.159605 31223 net.cpp:165] Memory required for data: 1436609720
I0521 09:32:38.159613 31223 layer_factory.hpp:77] Creating layer ip3
I0521 09:32:38.159627 31223 net.cpp:106] Creating Layer ip3
I0521 09:32:38.159637 31223 net.cpp:454] ip3 <- ip2
I0521 09:32:38.159651 31223 net.cpp:411] ip3 -> ip3
I0521 09:32:38.159860 31223 net.cpp:150] Setting up ip3
I0521 09:32:38.159873 31223 net.cpp:157] Top shape: 910 11 (10010)
I0521 09:32:38.159883 31223 net.cpp:165] Memory required for data: 1436649760
I0521 09:32:38.159898 31223 layer_factory.hpp:77] Creating layer drop3
I0521 09:32:38.159911 31223 net.cpp:106] Creating Layer drop3
I0521 09:32:38.159921 31223 net.cpp:454] drop3 <- ip3
I0521 09:32:38.159932 31223 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:32:38.159972 31223 net.cpp:150] Setting up drop3
I0521 09:32:38.159984 31223 net.cpp:157] Top shape: 910 11 (10010)
I0521 09:32:38.159996 31223 net.cpp:165] Memory required for data: 1436689800
I0521 09:32:38.160006 31223 layer_factory.hpp:77] Creating layer loss
I0521 09:32:38.160023 31223 net.cpp:106] Creating Layer loss
I0521 09:32:38.160034 31223 net.cpp:454] loss <- ip3
I0521 09:32:38.160045 31223 net.cpp:454] loss <- label
I0521 09:32:38.160058 31223 net.cpp:411] loss -> loss
I0521 09:32:38.160075 31223 layer_factory.hpp:77] Creating layer loss
I0521 09:32:38.160728 31223 net.cpp:150] Setting up loss
I0521 09:32:38.160749 31223 net.cpp:157] Top shape: (1)
I0521 09:32:38.160763 31223 net.cpp:160]     with loss weight 1
I0521 09:32:38.160805 31223 net.cpp:165] Memory required for data: 1436689804
I0521 09:32:38.160816 31223 net.cpp:226] loss needs backward computation.
I0521 09:32:38.160827 31223 net.cpp:226] drop3 needs backward computation.
I0521 09:32:38.160835 31223 net.cpp:226] ip3 needs backward computation.
I0521 09:32:38.160846 31223 net.cpp:226] drop2 needs backward computation.
I0521 09:32:38.160863 31223 net.cpp:226] relu6 needs backward computation.
I0521 09:32:38.160873 31223 net.cpp:226] ip2 needs backward computation.
I0521 09:32:38.160883 31223 net.cpp:226] drop1 needs backward computation.
I0521 09:32:38.160892 31223 net.cpp:226] relu5 needs backward computation.
I0521 09:32:38.160902 31223 net.cpp:226] ip1 needs backward computation.
I0521 09:32:38.160912 31223 net.cpp:226] pool4 needs backward computation.
I0521 09:32:38.160923 31223 net.cpp:226] relu4 needs backward computation.
I0521 09:32:38.160933 31223 net.cpp:226] conv4 needs backward computation.
I0521 09:32:38.160943 31223 net.cpp:226] pool3 needs backward computation.
I0521 09:32:38.160964 31223 net.cpp:226] relu3 needs backward computation.
I0521 09:32:38.160974 31223 net.cpp:226] conv3 needs backward computation.
I0521 09:32:38.160985 31223 net.cpp:226] pool2 needs backward computation.
I0521 09:32:38.160996 31223 net.cpp:226] relu2 needs backward computation.
I0521 09:32:38.161006 31223 net.cpp:226] conv2 needs backward computation.
I0521 09:32:38.161016 31223 net.cpp:226] pool1 needs backward computation.
I0521 09:32:38.161026 31223 net.cpp:226] relu1 needs backward computation.
I0521 09:32:38.161036 31223 net.cpp:226] conv1 needs backward computation.
I0521 09:32:38.161048 31223 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:32:38.161057 31223 net.cpp:270] This network produces output loss
I0521 09:32:38.161082 31223 net.cpp:283] Network initialization done.
I0521 09:32:38.162703 31223 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097.prototxt
I0521 09:32:38.162773 31223 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 09:32:38.163130 31223 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 910
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 09:32:38.163319 31223 layer_factory.hpp:77] Creating layer data_hdf5
I0521 09:32:38.163334 31223 net.cpp:106] Creating Layer data_hdf5
I0521 09:32:38.163347 31223 net.cpp:411] data_hdf5 -> data
I0521 09:32:38.163362 31223 net.cpp:411] data_hdf5 -> label
I0521 09:32:38.163378 31223 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 09:32:38.164649 31223 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 09:32:59.496423 31223 net.cpp:150] Setting up data_hdf5
I0521 09:32:59.496590 31223 net.cpp:157] Top shape: 910 1 127 50 (5778500)
I0521 09:32:59.496605 31223 net.cpp:157] Top shape: 910 (910)
I0521 09:32:59.496618 31223 net.cpp:165] Memory required for data: 23117640
I0521 09:32:59.496630 31223 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 09:32:59.496659 31223 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 09:32:59.496670 31223 net.cpp:454] label_data_hdf5_1_split <- label
I0521 09:32:59.496685 31223 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 09:32:59.496706 31223 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 09:32:59.496780 31223 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 09:32:59.496794 31223 net.cpp:157] Top shape: 910 (910)
I0521 09:32:59.496806 31223 net.cpp:157] Top shape: 910 (910)
I0521 09:32:59.496815 31223 net.cpp:165] Memory required for data: 23124920
I0521 09:32:59.496825 31223 layer_factory.hpp:77] Creating layer conv1
I0521 09:32:59.496847 31223 net.cpp:106] Creating Layer conv1
I0521 09:32:59.496867 31223 net.cpp:454] conv1 <- data
I0521 09:32:59.496883 31223 net.cpp:411] conv1 -> conv1
I0521 09:32:59.498800 31223 net.cpp:150] Setting up conv1
I0521 09:32:59.498821 31223 net.cpp:157] Top shape: 910 12 120 48 (62899200)
I0521 09:32:59.498831 31223 net.cpp:165] Memory required for data: 274721720
I0521 09:32:59.498850 31223 layer_factory.hpp:77] Creating layer relu1
I0521 09:32:59.498865 31223 net.cpp:106] Creating Layer relu1
I0521 09:32:59.498875 31223 net.cpp:454] relu1 <- conv1
I0521 09:32:59.498888 31223 net.cpp:397] relu1 -> conv1 (in-place)
I0521 09:32:59.499384 31223 net.cpp:150] Setting up relu1
I0521 09:32:59.499402 31223 net.cpp:157] Top shape: 910 12 120 48 (62899200)
I0521 09:32:59.499411 31223 net.cpp:165] Memory required for data: 526318520
I0521 09:32:59.499421 31223 layer_factory.hpp:77] Creating layer pool1
I0521 09:32:59.499439 31223 net.cpp:106] Creating Layer pool1
I0521 09:32:59.499449 31223 net.cpp:454] pool1 <- conv1
I0521 09:32:59.499461 31223 net.cpp:411] pool1 -> pool1
I0521 09:32:59.499536 31223 net.cpp:150] Setting up pool1
I0521 09:32:59.499549 31223 net.cpp:157] Top shape: 910 12 60 48 (31449600)
I0521 09:32:59.499559 31223 net.cpp:165] Memory required for data: 652116920
I0521 09:32:59.499570 31223 layer_factory.hpp:77] Creating layer conv2
I0521 09:32:59.499588 31223 net.cpp:106] Creating Layer conv2
I0521 09:32:59.499598 31223 net.cpp:454] conv2 <- pool1
I0521 09:32:59.499613 31223 net.cpp:411] conv2 -> conv2
I0521 09:32:59.501529 31223 net.cpp:150] Setting up conv2
I0521 09:32:59.501551 31223 net.cpp:157] Top shape: 910 20 54 46 (45208800)
I0521 09:32:59.501564 31223 net.cpp:165] Memory required for data: 832952120
I0521 09:32:59.501581 31223 layer_factory.hpp:77] Creating layer relu2
I0521 09:32:59.501595 31223 net.cpp:106] Creating Layer relu2
I0521 09:32:59.501605 31223 net.cpp:454] relu2 <- conv2
I0521 09:32:59.501617 31223 net.cpp:397] relu2 -> conv2 (in-place)
I0521 09:32:59.501952 31223 net.cpp:150] Setting up relu2
I0521 09:32:59.501966 31223 net.cpp:157] Top shape: 910 20 54 46 (45208800)
I0521 09:32:59.501977 31223 net.cpp:165] Memory required for data: 1013787320
I0521 09:32:59.501987 31223 layer_factory.hpp:77] Creating layer pool2
I0521 09:32:59.502001 31223 net.cpp:106] Creating Layer pool2
I0521 09:32:59.502010 31223 net.cpp:454] pool2 <- conv2
I0521 09:32:59.502023 31223 net.cpp:411] pool2 -> pool2
I0521 09:32:59.502094 31223 net.cpp:150] Setting up pool2
I0521 09:32:59.502107 31223 net.cpp:157] Top shape: 910 20 27 46 (22604400)
I0521 09:32:59.502117 31223 net.cpp:165] Memory required for data: 1104204920
I0521 09:32:59.502127 31223 layer_factory.hpp:77] Creating layer conv3
I0521 09:32:59.502146 31223 net.cpp:106] Creating Layer conv3
I0521 09:32:59.502156 31223 net.cpp:454] conv3 <- pool2
I0521 09:32:59.502171 31223 net.cpp:411] conv3 -> conv3
I0521 09:32:59.504145 31223 net.cpp:150] Setting up conv3
I0521 09:32:59.504169 31223 net.cpp:157] Top shape: 910 28 22 44 (24664640)
I0521 09:32:59.504180 31223 net.cpp:165] Memory required for data: 1202863480
I0521 09:32:59.504214 31223 layer_factory.hpp:77] Creating layer relu3
I0521 09:32:59.504226 31223 net.cpp:106] Creating Layer relu3
I0521 09:32:59.504236 31223 net.cpp:454] relu3 <- conv3
I0521 09:32:59.504250 31223 net.cpp:397] relu3 -> conv3 (in-place)
I0521 09:32:59.504719 31223 net.cpp:150] Setting up relu3
I0521 09:32:59.504735 31223 net.cpp:157] Top shape: 910 28 22 44 (24664640)
I0521 09:32:59.504746 31223 net.cpp:165] Memory required for data: 1301522040
I0521 09:32:59.504756 31223 layer_factory.hpp:77] Creating layer pool3
I0521 09:32:59.504770 31223 net.cpp:106] Creating Layer pool3
I0521 09:32:59.504779 31223 net.cpp:454] pool3 <- conv3
I0521 09:32:59.504792 31223 net.cpp:411] pool3 -> pool3
I0521 09:32:59.504871 31223 net.cpp:150] Setting up pool3
I0521 09:32:59.504884 31223 net.cpp:157] Top shape: 910 28 11 44 (12332320)
I0521 09:32:59.504894 31223 net.cpp:165] Memory required for data: 1350851320
I0521 09:32:59.504907 31223 layer_factory.hpp:77] Creating layer conv4
I0521 09:32:59.504925 31223 net.cpp:106] Creating Layer conv4
I0521 09:32:59.504935 31223 net.cpp:454] conv4 <- pool3
I0521 09:32:59.504950 31223 net.cpp:411] conv4 -> conv4
I0521 09:32:59.507001 31223 net.cpp:150] Setting up conv4
I0521 09:32:59.507019 31223 net.cpp:157] Top shape: 910 36 6 42 (8255520)
I0521 09:32:59.507030 31223 net.cpp:165] Memory required for data: 1383873400
I0521 09:32:59.507046 31223 layer_factory.hpp:77] Creating layer relu4
I0521 09:32:59.507061 31223 net.cpp:106] Creating Layer relu4
I0521 09:32:59.507071 31223 net.cpp:454] relu4 <- conv4
I0521 09:32:59.507083 31223 net.cpp:397] relu4 -> conv4 (in-place)
I0521 09:32:59.507550 31223 net.cpp:150] Setting up relu4
I0521 09:32:59.507566 31223 net.cpp:157] Top shape: 910 36 6 42 (8255520)
I0521 09:32:59.507577 31223 net.cpp:165] Memory required for data: 1416895480
I0521 09:32:59.507587 31223 layer_factory.hpp:77] Creating layer pool4
I0521 09:32:59.507601 31223 net.cpp:106] Creating Layer pool4
I0521 09:32:59.507611 31223 net.cpp:454] pool4 <- conv4
I0521 09:32:59.507624 31223 net.cpp:411] pool4 -> pool4
I0521 09:32:59.507694 31223 net.cpp:150] Setting up pool4
I0521 09:32:59.507707 31223 net.cpp:157] Top shape: 910 36 3 42 (4127760)
I0521 09:32:59.507717 31223 net.cpp:165] Memory required for data: 1433406520
I0521 09:32:59.507727 31223 layer_factory.hpp:77] Creating layer ip1
I0521 09:32:59.507742 31223 net.cpp:106] Creating Layer ip1
I0521 09:32:59.507753 31223 net.cpp:454] ip1 <- pool4
I0521 09:32:59.507767 31223 net.cpp:411] ip1 -> ip1
I0521 09:32:59.523296 31223 net.cpp:150] Setting up ip1
I0521 09:32:59.523325 31223 net.cpp:157] Top shape: 910 196 (178360)
I0521 09:32:59.523337 31223 net.cpp:165] Memory required for data: 1434119960
I0521 09:32:59.523360 31223 layer_factory.hpp:77] Creating layer relu5
I0521 09:32:59.523375 31223 net.cpp:106] Creating Layer relu5
I0521 09:32:59.523386 31223 net.cpp:454] relu5 <- ip1
I0521 09:32:59.523399 31223 net.cpp:397] relu5 -> ip1 (in-place)
I0521 09:32:59.523743 31223 net.cpp:150] Setting up relu5
I0521 09:32:59.523757 31223 net.cpp:157] Top shape: 910 196 (178360)
I0521 09:32:59.523767 31223 net.cpp:165] Memory required for data: 1434833400
I0521 09:32:59.523778 31223 layer_factory.hpp:77] Creating layer drop1
I0521 09:32:59.523797 31223 net.cpp:106] Creating Layer drop1
I0521 09:32:59.523808 31223 net.cpp:454] drop1 <- ip1
I0521 09:32:59.523820 31223 net.cpp:397] drop1 -> ip1 (in-place)
I0521 09:32:59.523864 31223 net.cpp:150] Setting up drop1
I0521 09:32:59.523877 31223 net.cpp:157] Top shape: 910 196 (178360)
I0521 09:32:59.523887 31223 net.cpp:165] Memory required for data: 1435546840
I0521 09:32:59.523897 31223 layer_factory.hpp:77] Creating layer ip2
I0521 09:32:59.523911 31223 net.cpp:106] Creating Layer ip2
I0521 09:32:59.523921 31223 net.cpp:454] ip2 <- ip1
I0521 09:32:59.523936 31223 net.cpp:411] ip2 -> ip2
I0521 09:32:59.524415 31223 net.cpp:150] Setting up ip2
I0521 09:32:59.524427 31223 net.cpp:157] Top shape: 910 98 (89180)
I0521 09:32:59.524438 31223 net.cpp:165] Memory required for data: 1435903560
I0521 09:32:59.524466 31223 layer_factory.hpp:77] Creating layer relu6
I0521 09:32:59.524479 31223 net.cpp:106] Creating Layer relu6
I0521 09:32:59.524489 31223 net.cpp:454] relu6 <- ip2
I0521 09:32:59.524502 31223 net.cpp:397] relu6 -> ip2 (in-place)
I0521 09:32:59.525046 31223 net.cpp:150] Setting up relu6
I0521 09:32:59.525068 31223 net.cpp:157] Top shape: 910 98 (89180)
I0521 09:32:59.525079 31223 net.cpp:165] Memory required for data: 1436260280
I0521 09:32:59.525089 31223 layer_factory.hpp:77] Creating layer drop2
I0521 09:32:59.525102 31223 net.cpp:106] Creating Layer drop2
I0521 09:32:59.525112 31223 net.cpp:454] drop2 <- ip2
I0521 09:32:59.525125 31223 net.cpp:397] drop2 -> ip2 (in-place)
I0521 09:32:59.525169 31223 net.cpp:150] Setting up drop2
I0521 09:32:59.525182 31223 net.cpp:157] Top shape: 910 98 (89180)
I0521 09:32:59.525193 31223 net.cpp:165] Memory required for data: 1436617000
I0521 09:32:59.525202 31223 layer_factory.hpp:77] Creating layer ip3
I0521 09:32:59.525218 31223 net.cpp:106] Creating Layer ip3
I0521 09:32:59.525228 31223 net.cpp:454] ip3 <- ip2
I0521 09:32:59.525241 31223 net.cpp:411] ip3 -> ip3
I0521 09:32:59.525467 31223 net.cpp:150] Setting up ip3
I0521 09:32:59.525480 31223 net.cpp:157] Top shape: 910 11 (10010)
I0521 09:32:59.525490 31223 net.cpp:165] Memory required for data: 1436657040
I0521 09:32:59.525506 31223 layer_factory.hpp:77] Creating layer drop3
I0521 09:32:59.525519 31223 net.cpp:106] Creating Layer drop3
I0521 09:32:59.525529 31223 net.cpp:454] drop3 <- ip3
I0521 09:32:59.525542 31223 net.cpp:397] drop3 -> ip3 (in-place)
I0521 09:32:59.525583 31223 net.cpp:150] Setting up drop3
I0521 09:32:59.525596 31223 net.cpp:157] Top shape: 910 11 (10010)
I0521 09:32:59.525606 31223 net.cpp:165] Memory required for data: 1436697080
I0521 09:32:59.525616 31223 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 09:32:59.525629 31223 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 09:32:59.525640 31223 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 09:32:59.525652 31223 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 09:32:59.525667 31223 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 09:32:59.525740 31223 net.cpp:150] Setting up ip3_drop3_0_split
I0521 09:32:59.525753 31223 net.cpp:157] Top shape: 910 11 (10010)
I0521 09:32:59.525766 31223 net.cpp:157] Top shape: 910 11 (10010)
I0521 09:32:59.525776 31223 net.cpp:165] Memory required for data: 1436777160
I0521 09:32:59.525784 31223 layer_factory.hpp:77] Creating layer accuracy
I0521 09:32:59.525806 31223 net.cpp:106] Creating Layer accuracy
I0521 09:32:59.525816 31223 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 09:32:59.525827 31223 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 09:32:59.525841 31223 net.cpp:411] accuracy -> accuracy
I0521 09:32:59.525864 31223 net.cpp:150] Setting up accuracy
I0521 09:32:59.525877 31223 net.cpp:157] Top shape: (1)
I0521 09:32:59.525888 31223 net.cpp:165] Memory required for data: 1436777164
I0521 09:32:59.525897 31223 layer_factory.hpp:77] Creating layer loss
I0521 09:32:59.525912 31223 net.cpp:106] Creating Layer loss
I0521 09:32:59.525921 31223 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 09:32:59.525933 31223 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 09:32:59.525946 31223 net.cpp:411] loss -> loss
I0521 09:32:59.525964 31223 layer_factory.hpp:77] Creating layer loss
I0521 09:32:59.526459 31223 net.cpp:150] Setting up loss
I0521 09:32:59.526473 31223 net.cpp:157] Top shape: (1)
I0521 09:32:59.526484 31223 net.cpp:160]     with loss weight 1
I0521 09:32:59.526502 31223 net.cpp:165] Memory required for data: 1436777168
I0521 09:32:59.526512 31223 net.cpp:226] loss needs backward computation.
I0521 09:32:59.526523 31223 net.cpp:228] accuracy does not need backward computation.
I0521 09:32:59.526535 31223 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 09:32:59.526546 31223 net.cpp:226] drop3 needs backward computation.
I0521 09:32:59.526554 31223 net.cpp:226] ip3 needs backward computation.
I0521 09:32:59.526574 31223 net.cpp:226] drop2 needs backward computation.
I0521 09:32:59.526585 31223 net.cpp:226] relu6 needs backward computation.
I0521 09:32:59.526594 31223 net.cpp:226] ip2 needs backward computation.
I0521 09:32:59.526604 31223 net.cpp:226] drop1 needs backward computation.
I0521 09:32:59.526614 31223 net.cpp:226] relu5 needs backward computation.
I0521 09:32:59.526624 31223 net.cpp:226] ip1 needs backward computation.
I0521 09:32:59.526633 31223 net.cpp:226] pool4 needs backward computation.
I0521 09:32:59.526644 31223 net.cpp:226] relu4 needs backward computation.
I0521 09:32:59.526654 31223 net.cpp:226] conv4 needs backward computation.
I0521 09:32:59.526664 31223 net.cpp:226] pool3 needs backward computation.
I0521 09:32:59.526674 31223 net.cpp:226] relu3 needs backward computation.
I0521 09:32:59.526685 31223 net.cpp:226] conv3 needs backward computation.
I0521 09:32:59.526695 31223 net.cpp:226] pool2 needs backward computation.
I0521 09:32:59.526705 31223 net.cpp:226] relu2 needs backward computation.
I0521 09:32:59.526715 31223 net.cpp:226] conv2 needs backward computation.
I0521 09:32:59.526726 31223 net.cpp:226] pool1 needs backward computation.
I0521 09:32:59.526736 31223 net.cpp:226] relu1 needs backward computation.
I0521 09:32:59.526746 31223 net.cpp:226] conv1 needs backward computation.
I0521 09:32:59.526757 31223 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 09:32:59.526769 31223 net.cpp:228] data_hdf5 does not need backward computation.
I0521 09:32:59.526778 31223 net.cpp:270] This network produces output accuracy
I0521 09:32:59.526789 31223 net.cpp:270] This network produces output loss
I0521 09:32:59.526818 31223 net.cpp:283] Network initialization done.
I0521 09:32:59.526949 31223 solver.cpp:60] Solver scaffolding done.
I0521 09:32:59.528076 31223 caffe.cpp:212] Starting Optimization
I0521 09:32:59.528095 31223 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 09:32:59.528108 31223 solver.cpp:289] Learning Rate Policy: fixed
I0521 09:32:59.529419 31223 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 09:33:45.487366 31223 solver.cpp:409]     Test net output #0: accuracy = 0.0785983
I0521 09:33:45.487536 31223 solver.cpp:409]     Test net output #1: loss = 2.39889 (* 1 = 2.39889 loss)
I0521 09:33:45.654486 31223 solver.cpp:237] Iteration 0, loss = 2.39902
I0521 09:33:45.654523 31223 solver.cpp:253]     Train net output #0: loss = 2.39902 (* 1 = 2.39902 loss)
I0521 09:33:45.654542 31223 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 09:33:53.473160 31223 solver.cpp:237] Iteration 16, loss = 2.39098
I0521 09:33:53.473196 31223 solver.cpp:253]     Train net output #0: loss = 2.39098 (* 1 = 2.39098 loss)
I0521 09:33:53.473212 31223 sgd_solver.cpp:106] Iteration 16, lr = 0.0025
I0521 09:34:01.291085 31223 solver.cpp:237] Iteration 32, loss = 2.38118
I0521 09:34:01.291116 31223 solver.cpp:253]     Train net output #0: loss = 2.38118 (* 1 = 2.38118 loss)
I0521 09:34:01.291132 31223 sgd_solver.cpp:106] Iteration 32, lr = 0.0025
I0521 09:34:09.108405 31223 solver.cpp:237] Iteration 48, loss = 2.36606
I0521 09:34:09.108433 31223 solver.cpp:253]     Train net output #0: loss = 2.36606 (* 1 = 2.36606 loss)
I0521 09:34:09.108451 31223 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0521 09:34:16.917876 31223 solver.cpp:237] Iteration 64, loss = 2.3564
I0521 09:34:16.918015 31223 solver.cpp:253]     Train net output #0: loss = 2.3564 (* 1 = 2.3564 loss)
I0521 09:34:16.918028 31223 sgd_solver.cpp:106] Iteration 64, lr = 0.0025
I0521 09:34:24.735132 31223 solver.cpp:237] Iteration 80, loss = 2.35324
I0521 09:34:24.735163 31223 solver.cpp:253]     Train net output #0: loss = 2.35324 (* 1 = 2.35324 loss)
I0521 09:34:24.735182 31223 sgd_solver.cpp:106] Iteration 80, lr = 0.0025
I0521 09:34:32.555340 31223 solver.cpp:237] Iteration 96, loss = 2.33962
I0521 09:34:32.555369 31223 solver.cpp:253]     Train net output #0: loss = 2.33962 (* 1 = 2.33962 loss)
I0521 09:34:32.555392 31223 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 09:35:02.554524 31223 solver.cpp:237] Iteration 112, loss = 2.33725
I0521 09:35:02.554687 31223 solver.cpp:253]     Train net output #0: loss = 2.33725 (* 1 = 2.33725 loss)
I0521 09:35:02.554700 31223 sgd_solver.cpp:106] Iteration 112, lr = 0.0025
I0521 09:35:10.368083 31223 solver.cpp:237] Iteration 128, loss = 2.32842
I0521 09:35:10.368116 31223 solver.cpp:253]     Train net output #0: loss = 2.32842 (* 1 = 2.32842 loss)
I0521 09:35:10.368130 31223 sgd_solver.cpp:106] Iteration 128, lr = 0.0025
I0521 09:35:18.188405 31223 solver.cpp:237] Iteration 144, loss = 2.33126
I0521 09:35:18.188439 31223 solver.cpp:253]     Train net output #0: loss = 2.33126 (* 1 = 2.33126 loss)
I0521 09:35:18.188455 31223 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 09:35:26.009387 31223 solver.cpp:237] Iteration 160, loss = 2.32314
I0521 09:35:26.009425 31223 solver.cpp:253]     Train net output #0: loss = 2.32314 (* 1 = 2.32314 loss)
I0521 09:35:26.009441 31223 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 09:35:27.474480 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_164.caffemodel
I0521 09:35:27.859601 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_164.solverstate
I0521 09:35:33.897011 31223 solver.cpp:237] Iteration 176, loss = 2.31136
I0521 09:35:33.897162 31223 solver.cpp:253]     Train net output #0: loss = 2.31136 (* 1 = 2.31136 loss)
I0521 09:35:33.897176 31223 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 09:35:41.713850 31223 solver.cpp:237] Iteration 192, loss = 2.31587
I0521 09:35:41.713881 31223 solver.cpp:253]     Train net output #0: loss = 2.31587 (* 1 = 2.31587 loss)
I0521 09:35:41.713899 31223 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 09:35:49.533710 31223 solver.cpp:237] Iteration 208, loss = 2.3143
I0521 09:35:49.533751 31223 solver.cpp:253]     Train net output #0: loss = 2.3143 (* 1 = 2.3143 loss)
I0521 09:35:49.533771 31223 sgd_solver.cpp:106] Iteration 208, lr = 0.0025
I0521 09:36:19.505067 31223 solver.cpp:237] Iteration 224, loss = 2.3038
I0521 09:36:19.505229 31223 solver.cpp:253]     Train net output #0: loss = 2.3038 (* 1 = 2.3038 loss)
I0521 09:36:19.505244 31223 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 09:36:27.326000 31223 solver.cpp:237] Iteration 240, loss = 2.31549
I0521 09:36:27.326033 31223 solver.cpp:253]     Train net output #0: loss = 2.31549 (* 1 = 2.31549 loss)
I0521 09:36:27.326050 31223 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 09:36:35.150166 31223 solver.cpp:237] Iteration 256, loss = 2.31659
I0521 09:36:35.150210 31223 solver.cpp:253]     Train net output #0: loss = 2.31659 (* 1 = 2.31659 loss)
I0521 09:36:35.150226 31223 sgd_solver.cpp:106] Iteration 256, lr = 0.0025
I0521 09:36:42.979240 31223 solver.cpp:237] Iteration 272, loss = 2.29031
I0521 09:36:42.979274 31223 solver.cpp:253]     Train net output #0: loss = 2.29031 (* 1 = 2.29031 loss)
I0521 09:36:42.979290 31223 sgd_solver.cpp:106] Iteration 272, lr = 0.0025
I0521 09:36:50.799330 31223 solver.cpp:237] Iteration 288, loss = 2.29315
I0521 09:36:50.799466 31223 solver.cpp:253]     Train net output #0: loss = 2.29315 (* 1 = 2.29315 loss)
I0521 09:36:50.799479 31223 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 09:36:58.622750 31223 solver.cpp:237] Iteration 304, loss = 2.28299
I0521 09:36:58.622781 31223 solver.cpp:253]     Train net output #0: loss = 2.28299 (* 1 = 2.28299 loss)
I0521 09:36:58.622799 31223 sgd_solver.cpp:106] Iteration 304, lr = 0.0025
I0521 09:37:06.453384 31223 solver.cpp:237] Iteration 320, loss = 2.26719
I0521 09:37:06.453425 31223 solver.cpp:253]     Train net output #0: loss = 2.26719 (* 1 = 2.26719 loss)
I0521 09:37:06.453441 31223 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 09:37:09.875087 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_328.caffemodel
I0521 09:37:10.257608 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_328.solverstate
I0521 09:37:10.428424 31223 solver.cpp:341] Iteration 329, Testing net (#0)
I0521 09:37:55.818390 31223 solver.cpp:409]     Test net output #0: accuracy = 0.303437
I0521 09:37:55.818547 31223 solver.cpp:409]     Test net output #1: loss = 2.22533 (* 1 = 2.22533 loss)
I0521 09:38:21.522954 31223 solver.cpp:237] Iteration 336, loss = 2.27833
I0521 09:38:21.523007 31223 solver.cpp:253]     Train net output #0: loss = 2.27833 (* 1 = 2.27833 loss)
I0521 09:38:21.523022 31223 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 09:38:29.334060 31223 solver.cpp:237] Iteration 352, loss = 2.26294
I0521 09:38:29.334200 31223 solver.cpp:253]     Train net output #0: loss = 2.26294 (* 1 = 2.26294 loss)
I0521 09:38:29.334213 31223 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 09:38:37.153456 31223 solver.cpp:237] Iteration 368, loss = 2.23308
I0521 09:38:37.153489 31223 solver.cpp:253]     Train net output #0: loss = 2.23308 (* 1 = 2.23308 loss)
I0521 09:38:37.153503 31223 sgd_solver.cpp:106] Iteration 368, lr = 0.0025
I0521 09:38:44.963402 31223 solver.cpp:237] Iteration 384, loss = 2.21621
I0521 09:38:44.963440 31223 solver.cpp:253]     Train net output #0: loss = 2.21621 (* 1 = 2.21621 loss)
I0521 09:38:44.963462 31223 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 09:38:52.781035 31223 solver.cpp:237] Iteration 400, loss = 2.15742
I0521 09:38:52.781067 31223 solver.cpp:253]     Train net output #0: loss = 2.15742 (* 1 = 2.15742 loss)
I0521 09:38:52.781081 31223 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0521 09:39:00.597262 31223 solver.cpp:237] Iteration 416, loss = 2.18948
I0521 09:39:00.597396 31223 solver.cpp:253]     Train net output #0: loss = 2.18948 (* 1 = 2.18948 loss)
I0521 09:39:00.597410 31223 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 09:39:08.411121 31223 solver.cpp:237] Iteration 432, loss = 2.12547
I0521 09:39:08.411150 31223 solver.cpp:253]     Train net output #0: loss = 2.12547 (* 1 = 2.12547 loss)
I0521 09:39:08.411169 31223 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 09:39:38.398661 31223 solver.cpp:237] Iteration 448, loss = 2.14058
I0521 09:39:38.398828 31223 solver.cpp:253]     Train net output #0: loss = 2.14058 (* 1 = 2.14058 loss)
I0521 09:39:38.398844 31223 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 09:39:46.218168 31223 solver.cpp:237] Iteration 464, loss = 2.06857
I0521 09:39:46.218201 31223 solver.cpp:253]     Train net output #0: loss = 2.06857 (* 1 = 2.06857 loss)
I0521 09:39:46.218219 31223 sgd_solver.cpp:106] Iteration 464, lr = 0.0025
I0521 09:39:54.037477 31223 solver.cpp:237] Iteration 480, loss = 2.08789
I0521 09:39:54.037509 31223 solver.cpp:253]     Train net output #0: loss = 2.08789 (* 1 = 2.08789 loss)
I0521 09:39:54.037526 31223 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 09:39:59.416704 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_492.caffemodel
I0521 09:39:59.799600 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_492.solverstate
I0521 09:40:01.929780 31223 solver.cpp:237] Iteration 496, loss = 2.05883
I0521 09:40:01.929831 31223 solver.cpp:253]     Train net output #0: loss = 2.05883 (* 1 = 2.05883 loss)
I0521 09:40:01.929847 31223 sgd_solver.cpp:106] Iteration 496, lr = 0.0025
I0521 09:40:09.743016 31223 solver.cpp:237] Iteration 512, loss = 2.02656
I0521 09:40:09.743155 31223 solver.cpp:253]     Train net output #0: loss = 2.02656 (* 1 = 2.02656 loss)
I0521 09:40:09.743167 31223 sgd_solver.cpp:106] Iteration 512, lr = 0.0025
I0521 09:40:17.558086 31223 solver.cpp:237] Iteration 528, loss = 2.04997
I0521 09:40:17.558120 31223 solver.cpp:253]     Train net output #0: loss = 2.04997 (* 1 = 2.04997 loss)
I0521 09:40:17.558136 31223 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 09:40:25.379503 31223 solver.cpp:237] Iteration 544, loss = 2.05688
I0521 09:40:25.379549 31223 solver.cpp:253]     Train net output #0: loss = 2.05688 (* 1 = 2.05688 loss)
I0521 09:40:25.379566 31223 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 09:40:55.386935 31223 solver.cpp:237] Iteration 560, loss = 2.00886
I0521 09:40:55.387100 31223 solver.cpp:253]     Train net output #0: loss = 2.00886 (* 1 = 2.00886 loss)
I0521 09:40:55.387115 31223 sgd_solver.cpp:106] Iteration 560, lr = 0.0025
I0521 09:41:03.204484 31223 solver.cpp:237] Iteration 576, loss = 1.96327
I0521 09:41:03.204516 31223 solver.cpp:253]     Train net output #0: loss = 1.96327 (* 1 = 1.96327 loss)
I0521 09:41:03.204535 31223 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 09:41:11.018051 31223 solver.cpp:237] Iteration 592, loss = 2.0271
I0521 09:41:11.018093 31223 solver.cpp:253]     Train net output #0: loss = 2.0271 (* 1 = 2.0271 loss)
I0521 09:41:11.018115 31223 sgd_solver.cpp:106] Iteration 592, lr = 0.0025
I0521 09:41:18.833600 31223 solver.cpp:237] Iteration 608, loss = 1.9684
I0521 09:41:18.833633 31223 solver.cpp:253]     Train net output #0: loss = 1.9684 (* 1 = 1.9684 loss)
I0521 09:41:18.833650 31223 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 09:41:26.651465 31223 solver.cpp:237] Iteration 624, loss = 1.94948
I0521 09:41:26.651602 31223 solver.cpp:253]     Train net output #0: loss = 1.94948 (* 1 = 1.94948 loss)
I0521 09:41:26.651617 31223 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 09:41:34.471416 31223 solver.cpp:237] Iteration 640, loss = 1.98723
I0521 09:41:34.471457 31223 solver.cpp:253]     Train net output #0: loss = 1.98723 (* 1 = 1.98723 loss)
I0521 09:41:34.471474 31223 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 09:41:41.791465 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_656.caffemodel
I0521 09:41:42.174979 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_656.solverstate
I0521 09:41:42.348743 31223 solver.cpp:237] Iteration 656, loss = 1.93685
I0521 09:41:42.348791 31223 solver.cpp:253]     Train net output #0: loss = 1.93685 (* 1 = 1.93685 loss)
I0521 09:41:42.348809 31223 sgd_solver.cpp:106] Iteration 656, lr = 0.0025
I0521 09:41:42.837628 31223 solver.cpp:341] Iteration 658, Testing net (#0)
I0521 09:42:49.162184 31223 solver.cpp:409]     Test net output #0: accuracy = 0.564695
I0521 09:42:49.162358 31223 solver.cpp:409]     Test net output #1: loss = 1.60203 (* 1 = 1.60203 loss)
I0521 09:43:18.312463 31223 solver.cpp:237] Iteration 672, loss = 1.9487
I0521 09:43:18.312512 31223 solver.cpp:253]     Train net output #0: loss = 1.9487 (* 1 = 1.9487 loss)
I0521 09:43:18.312530 31223 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 09:43:26.125031 31223 solver.cpp:237] Iteration 688, loss = 1.9301
I0521 09:43:26.125176 31223 solver.cpp:253]     Train net output #0: loss = 1.9301 (* 1 = 1.9301 loss)
I0521 09:43:26.125190 31223 sgd_solver.cpp:106] Iteration 688, lr = 0.0025
I0521 09:43:33.936230 31223 solver.cpp:237] Iteration 704, loss = 1.89251
I0521 09:43:33.936262 31223 solver.cpp:253]     Train net output #0: loss = 1.89251 (* 1 = 1.89251 loss)
I0521 09:43:33.936277 31223 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 09:43:41.753983 31223 solver.cpp:237] Iteration 720, loss = 1.90042
I0521 09:43:41.754026 31223 solver.cpp:253]     Train net output #0: loss = 1.90042 (* 1 = 1.90042 loss)
I0521 09:43:41.754043 31223 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 09:43:49.563089 31223 solver.cpp:237] Iteration 736, loss = 1.86629
I0521 09:43:49.563122 31223 solver.cpp:253]     Train net output #0: loss = 1.86629 (* 1 = 1.86629 loss)
I0521 09:43:49.563138 31223 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 09:43:57.377724 31223 solver.cpp:237] Iteration 752, loss = 1.88744
I0521 09:43:57.377863 31223 solver.cpp:253]     Train net output #0: loss = 1.88744 (* 1 = 1.88744 loss)
I0521 09:43:57.377877 31223 sgd_solver.cpp:106] Iteration 752, lr = 0.0025
I0521 09:44:05.191489 31223 solver.cpp:237] Iteration 768, loss = 1.83049
I0521 09:44:05.191529 31223 solver.cpp:253]     Train net output #0: loss = 1.83049 (* 1 = 1.83049 loss)
I0521 09:44:05.191550 31223 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 09:44:35.171874 31223 solver.cpp:237] Iteration 784, loss = 1.85537
I0521 09:44:35.172039 31223 solver.cpp:253]     Train net output #0: loss = 1.85537 (* 1 = 1.85537 loss)
I0521 09:44:35.172054 31223 sgd_solver.cpp:106] Iteration 784, lr = 0.0025
I0521 09:44:42.991236 31223 solver.cpp:237] Iteration 800, loss = 1.84712
I0521 09:44:42.991269 31223 solver.cpp:253]     Train net output #0: loss = 1.84712 (* 1 = 1.84712 loss)
I0521 09:44:42.991287 31223 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 09:44:50.802577 31223 solver.cpp:237] Iteration 816, loss = 1.83113
I0521 09:44:50.802610 31223 solver.cpp:253]     Train net output #0: loss = 1.83113 (* 1 = 1.83113 loss)
I0521 09:44:50.802626 31223 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 09:44:52.267503 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_820.caffemodel
I0521 09:44:52.652031 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_820.solverstate
I0521 09:44:58.681655 31223 solver.cpp:237] Iteration 832, loss = 1.89731
I0521 09:44:58.681704 31223 solver.cpp:253]     Train net output #0: loss = 1.89731 (* 1 = 1.89731 loss)
I0521 09:44:58.681722 31223 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 09:45:06.491029 31223 solver.cpp:237] Iteration 848, loss = 1.89675
I0521 09:45:06.491181 31223 solver.cpp:253]     Train net output #0: loss = 1.89675 (* 1 = 1.89675 loss)
I0521 09:45:06.491195 31223 sgd_solver.cpp:106] Iteration 848, lr = 0.0025
I0521 09:45:14.298142 31223 solver.cpp:237] Iteration 864, loss = 1.79027
I0521 09:45:14.298173 31223 solver.cpp:253]     Train net output #0: loss = 1.79027 (* 1 = 1.79027 loss)
I0521 09:45:14.298192 31223 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 09:45:44.251029 31223 solver.cpp:237] Iteration 880, loss = 1.80827
I0521 09:45:44.251199 31223 solver.cpp:253]     Train net output #0: loss = 1.80827 (* 1 = 1.80827 loss)
I0521 09:45:44.251212 31223 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 09:45:52.061781 31223 solver.cpp:237] Iteration 896, loss = 1.80846
I0521 09:45:52.061820 31223 solver.cpp:253]     Train net output #0: loss = 1.80846 (* 1 = 1.80846 loss)
I0521 09:45:52.061839 31223 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 09:45:59.874681 31223 solver.cpp:237] Iteration 912, loss = 1.80138
I0521 09:45:59.874716 31223 solver.cpp:253]     Train net output #0: loss = 1.80138 (* 1 = 1.80138 loss)
I0521 09:45:59.874732 31223 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 09:46:07.687036 31223 solver.cpp:237] Iteration 928, loss = 1.84545
I0521 09:46:07.687068 31223 solver.cpp:253]     Train net output #0: loss = 1.84545 (* 1 = 1.84545 loss)
I0521 09:46:07.687084 31223 sgd_solver.cpp:106] Iteration 928, lr = 0.0025
I0521 09:46:15.499336 31223 solver.cpp:237] Iteration 944, loss = 1.86861
I0521 09:46:15.499487 31223 solver.cpp:253]     Train net output #0: loss = 1.86861 (* 1 = 1.86861 loss)
I0521 09:46:15.499502 31223 sgd_solver.cpp:106] Iteration 944, lr = 0.0025
I0521 09:46:23.306753 31223 solver.cpp:237] Iteration 960, loss = 1.86552
I0521 09:46:23.306784 31223 solver.cpp:253]     Train net output #0: loss = 1.86552 (* 1 = 1.86552 loss)
I0521 09:46:23.306802 31223 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 09:46:31.118122 31223 solver.cpp:237] Iteration 976, loss = 1.85638
I0521 09:46:31.118155 31223 solver.cpp:253]     Train net output #0: loss = 1.85638 (* 1 = 1.85638 loss)
I0521 09:46:31.118171 31223 sgd_solver.cpp:106] Iteration 976, lr = 0.0025
I0521 09:46:34.538431 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_984.caffemodel
I0521 09:46:34.919616 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_984.solverstate
I0521 09:46:36.068473 31223 solver.cpp:341] Iteration 987, Testing net (#0)
I0521 09:47:21.172322 31223 solver.cpp:409]     Test net output #0: accuracy = 0.609441
I0521 09:47:21.172479 31223 solver.cpp:409]     Test net output #1: loss = 1.34683 (* 1 = 1.34683 loss)
I0521 09:47:45.962879 31223 solver.cpp:237] Iteration 992, loss = 1.77779
I0521 09:47:45.962930 31223 solver.cpp:253]     Train net output #0: loss = 1.77779 (* 1 = 1.77779 loss)
I0521 09:47:45.962946 31223 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0521 09:47:53.784891 31223 solver.cpp:237] Iteration 1008, loss = 1.88372
I0521 09:47:53.785046 31223 solver.cpp:253]     Train net output #0: loss = 1.88372 (* 1 = 1.88372 loss)
I0521 09:47:53.785061 31223 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 09:48:01.603564 31223 solver.cpp:237] Iteration 1024, loss = 1.83388
I0521 09:48:01.603596 31223 solver.cpp:253]     Train net output #0: loss = 1.83388 (* 1 = 1.83388 loss)
I0521 09:48:01.603610 31223 sgd_solver.cpp:106] Iteration 1024, lr = 0.0025
I0521 09:48:09.420563 31223 solver.cpp:237] Iteration 1040, loss = 1.72811
I0521 09:48:09.420598 31223 solver.cpp:253]     Train net output #0: loss = 1.72811 (* 1 = 1.72811 loss)
I0521 09:48:09.420611 31223 sgd_solver.cpp:106] Iteration 1040, lr = 0.0025
I0521 09:48:17.239616 31223 solver.cpp:237] Iteration 1056, loss = 1.78358
I0521 09:48:17.239662 31223 solver.cpp:253]     Train net output #0: loss = 1.78358 (* 1 = 1.78358 loss)
I0521 09:48:17.239678 31223 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 09:48:25.056087 31223 solver.cpp:237] Iteration 1072, loss = 1.80399
I0521 09:48:25.056236 31223 solver.cpp:253]     Train net output #0: loss = 1.80399 (* 1 = 1.80399 loss)
I0521 09:48:25.056249 31223 sgd_solver.cpp:106] Iteration 1072, lr = 0.0025
I0521 09:48:32.878275 31223 solver.cpp:237] Iteration 1088, loss = 1.74708
I0521 09:48:32.878306 31223 solver.cpp:253]     Train net output #0: loss = 1.74708 (* 1 = 1.74708 loss)
I0521 09:48:32.878324 31223 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 09:49:02.881427 31223 solver.cpp:237] Iteration 1104, loss = 1.72878
I0521 09:49:02.881597 31223 solver.cpp:253]     Train net output #0: loss = 1.72878 (* 1 = 1.72878 loss)
I0521 09:49:02.881611 31223 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 09:49:10.706064 31223 solver.cpp:237] Iteration 1120, loss = 1.8183
I0521 09:49:10.706102 31223 solver.cpp:253]     Train net output #0: loss = 1.8183 (* 1 = 1.8183 loss)
I0521 09:49:10.706120 31223 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 09:49:18.526274 31223 solver.cpp:237] Iteration 1136, loss = 1.79182
I0521 09:49:18.526309 31223 solver.cpp:253]     Train net output #0: loss = 1.79182 (* 1 = 1.79182 loss)
I0521 09:49:18.526324 31223 sgd_solver.cpp:106] Iteration 1136, lr = 0.0025
I0521 09:49:23.904723 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1148.caffemodel
I0521 09:49:24.285192 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1148.solverstate
I0521 09:49:26.414907 31223 solver.cpp:237] Iteration 1152, loss = 1.76421
I0521 09:49:26.414954 31223 solver.cpp:253]     Train net output #0: loss = 1.76421 (* 1 = 1.76421 loss)
I0521 09:49:26.414969 31223 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 09:49:34.229261 31223 solver.cpp:237] Iteration 1168, loss = 1.82856
I0521 09:49:34.229416 31223 solver.cpp:253]     Train net output #0: loss = 1.82856 (* 1 = 1.82856 loss)
I0521 09:49:34.229431 31223 sgd_solver.cpp:106] Iteration 1168, lr = 0.0025
I0521 09:49:42.048748 31223 solver.cpp:237] Iteration 1184, loss = 1.7024
I0521 09:49:42.048779 31223 solver.cpp:253]     Train net output #0: loss = 1.7024 (* 1 = 1.7024 loss)
I0521 09:49:42.048797 31223 sgd_solver.cpp:106] Iteration 1184, lr = 0.0025
I0521 09:49:49.865916 31223 solver.cpp:237] Iteration 1200, loss = 1.8205
I0521 09:49:49.865950 31223 solver.cpp:253]     Train net output #0: loss = 1.8205 (* 1 = 1.8205 loss)
I0521 09:49:49.865967 31223 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 09:50:19.833755 31223 solver.cpp:237] Iteration 1216, loss = 1.72587
I0521 09:50:19.833911 31223 solver.cpp:253]     Train net output #0: loss = 1.72587 (* 1 = 1.72587 loss)
I0521 09:50:19.833926 31223 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 09:50:27.656622 31223 solver.cpp:237] Iteration 1232, loss = 1.78556
I0521 09:50:27.656656 31223 solver.cpp:253]     Train net output #0: loss = 1.78556 (* 1 = 1.78556 loss)
I0521 09:50:27.656673 31223 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 09:50:35.472815 31223 solver.cpp:237] Iteration 1248, loss = 1.77928
I0521 09:50:35.472848 31223 solver.cpp:253]     Train net output #0: loss = 1.77928 (* 1 = 1.77928 loss)
I0521 09:50:35.472868 31223 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 09:50:43.293465 31223 solver.cpp:237] Iteration 1264, loss = 1.79337
I0521 09:50:43.293498 31223 solver.cpp:253]     Train net output #0: loss = 1.79337 (* 1 = 1.79337 loss)
I0521 09:50:43.293514 31223 sgd_solver.cpp:106] Iteration 1264, lr = 0.0025
I0521 09:50:51.112426 31223 solver.cpp:237] Iteration 1280, loss = 1.72691
I0521 09:50:51.112582 31223 solver.cpp:253]     Train net output #0: loss = 1.72691 (* 1 = 1.72691 loss)
I0521 09:50:51.112596 31223 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 09:50:58.929749 31223 solver.cpp:237] Iteration 1296, loss = 1.73748
I0521 09:50:58.929780 31223 solver.cpp:253]     Train net output #0: loss = 1.73748 (* 1 = 1.73748 loss)
I0521 09:50:58.929798 31223 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 09:51:06.254808 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1312.caffemodel
I0521 09:51:06.635692 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1312.solverstate
I0521 09:51:06.807548 31223 solver.cpp:237] Iteration 1312, loss = 1.71144
I0521 09:51:06.807593 31223 solver.cpp:253]     Train net output #0: loss = 1.71144 (* 1 = 1.71144 loss)
I0521 09:51:06.807610 31223 sgd_solver.cpp:106] Iteration 1312, lr = 0.0025
I0521 09:51:08.272459 31223 solver.cpp:341] Iteration 1316, Testing net (#0)
I0521 09:52:14.547170 31223 solver.cpp:409]     Test net output #0: accuracy = 0.643655
I0521 09:52:14.547348 31223 solver.cpp:409]     Test net output #1: loss = 1.2433 (* 1 = 1.2433 loss)
I0521 09:52:42.704819 31223 solver.cpp:237] Iteration 1328, loss = 1.74128
I0521 09:52:42.704875 31223 solver.cpp:253]     Train net output #0: loss = 1.74128 (* 1 = 1.74128 loss)
I0521 09:52:42.704890 31223 sgd_solver.cpp:106] Iteration 1328, lr = 0.0025
I0521 09:52:50.524351 31223 solver.cpp:237] Iteration 1344, loss = 1.7096
I0521 09:52:50.524513 31223 solver.cpp:253]     Train net output #0: loss = 1.7096 (* 1 = 1.7096 loss)
I0521 09:52:50.524528 31223 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 09:52:58.351702 31223 solver.cpp:237] Iteration 1360, loss = 1.78555
I0521 09:52:58.351735 31223 solver.cpp:253]     Train net output #0: loss = 1.78555 (* 1 = 1.78555 loss)
I0521 09:52:58.351752 31223 sgd_solver.cpp:106] Iteration 1360, lr = 0.0025
I0521 09:53:06.169693 31223 solver.cpp:237] Iteration 1376, loss = 1.78413
I0521 09:53:06.169726 31223 solver.cpp:253]     Train net output #0: loss = 1.78413 (* 1 = 1.78413 loss)
I0521 09:53:06.169742 31223 sgd_solver.cpp:106] Iteration 1376, lr = 0.0025
I0521 09:53:13.990984 31223 solver.cpp:237] Iteration 1392, loss = 1.69622
I0521 09:53:13.991016 31223 solver.cpp:253]     Train net output #0: loss = 1.69622 (* 1 = 1.69622 loss)
I0521 09:53:13.991032 31223 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 09:53:21.806355 31223 solver.cpp:237] Iteration 1408, loss = 1.75386
I0521 09:53:21.806507 31223 solver.cpp:253]     Train net output #0: loss = 1.75386 (* 1 = 1.75386 loss)
I0521 09:53:21.806520 31223 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 09:53:29.620218 31223 solver.cpp:237] Iteration 1424, loss = 1.61122
I0521 09:53:29.620249 31223 solver.cpp:253]     Train net output #0: loss = 1.61122 (* 1 = 1.61122 loss)
I0521 09:53:29.620266 31223 sgd_solver.cpp:106] Iteration 1424, lr = 0.0025
I0521 09:53:59.639955 31223 solver.cpp:237] Iteration 1440, loss = 1.78267
I0521 09:53:59.640120 31223 solver.cpp:253]     Train net output #0: loss = 1.78267 (* 1 = 1.78267 loss)
I0521 09:53:59.640135 31223 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 09:54:07.467365 31223 solver.cpp:237] Iteration 1456, loss = 1.71738
I0521 09:54:07.467411 31223 solver.cpp:253]     Train net output #0: loss = 1.71738 (* 1 = 1.71738 loss)
I0521 09:54:07.467427 31223 sgd_solver.cpp:106] Iteration 1456, lr = 0.0025
I0521 09:54:15.288406 31223 solver.cpp:237] Iteration 1472, loss = 1.78232
I0521 09:54:15.288440 31223 solver.cpp:253]     Train net output #0: loss = 1.78232 (* 1 = 1.78232 loss)
I0521 09:54:15.288456 31223 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 09:54:16.755887 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1476.caffemodel
I0521 09:54:17.138746 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1476.solverstate
I0521 09:54:23.176604 31223 solver.cpp:237] Iteration 1488, loss = 1.74142
I0521 09:54:23.176653 31223 solver.cpp:253]     Train net output #0: loss = 1.74142 (* 1 = 1.74142 loss)
I0521 09:54:23.176666 31223 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 09:54:31.000736 31223 solver.cpp:237] Iteration 1504, loss = 1.77041
I0521 09:54:31.000918 31223 solver.cpp:253]     Train net output #0: loss = 1.77041 (* 1 = 1.77041 loss)
I0521 09:54:31.000932 31223 sgd_solver.cpp:106] Iteration 1504, lr = 0.0025
I0521 09:54:38.825242 31223 solver.cpp:237] Iteration 1520, loss = 1.77693
I0521 09:54:38.825274 31223 solver.cpp:253]     Train net output #0: loss = 1.77693 (* 1 = 1.77693 loss)
I0521 09:54:38.825292 31223 sgd_solver.cpp:106] Iteration 1520, lr = 0.0025
I0521 09:54:46.647686 31223 solver.cpp:237] Iteration 1536, loss = 1.66948
I0521 09:54:46.647719 31223 solver.cpp:253]     Train net output #0: loss = 1.66948 (* 1 = 1.66948 loss)
I0521 09:54:46.647735 31223 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 09:55:16.616593 31223 solver.cpp:237] Iteration 1552, loss = 1.68838
I0521 09:55:16.616767 31223 solver.cpp:253]     Train net output #0: loss = 1.68838 (* 1 = 1.68838 loss)
I0521 09:55:16.616781 31223 sgd_solver.cpp:106] Iteration 1552, lr = 0.0025
I0521 09:55:24.440054 31223 solver.cpp:237] Iteration 1568, loss = 1.75772
I0521 09:55:24.440099 31223 solver.cpp:253]     Train net output #0: loss = 1.75772 (* 1 = 1.75772 loss)
I0521 09:55:24.440116 31223 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 09:55:32.258993 31223 solver.cpp:237] Iteration 1584, loss = 1.67789
I0521 09:55:32.259027 31223 solver.cpp:253]     Train net output #0: loss = 1.67789 (* 1 = 1.67789 loss)
I0521 09:55:32.259043 31223 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 09:55:40.080813 31223 solver.cpp:237] Iteration 1600, loss = 1.76671
I0521 09:55:40.080847 31223 solver.cpp:253]     Train net output #0: loss = 1.76671 (* 1 = 1.76671 loss)
I0521 09:55:40.080867 31223 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 09:55:47.899931 31223 solver.cpp:237] Iteration 1616, loss = 1.69774
I0521 09:55:47.900086 31223 solver.cpp:253]     Train net output #0: loss = 1.69774 (* 1 = 1.69774 loss)
I0521 09:55:47.900100 31223 sgd_solver.cpp:106] Iteration 1616, lr = 0.0025
I0521 09:55:55.713084 31223 solver.cpp:237] Iteration 1632, loss = 1.7136
I0521 09:55:55.713116 31223 solver.cpp:253]     Train net output #0: loss = 1.7136 (* 1 = 1.7136 loss)
I0521 09:55:55.713135 31223 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 09:55:59.133633 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1640.caffemodel
I0521 09:55:59.517587 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1640.solverstate
I0521 09:56:01.647151 31223 solver.cpp:341] Iteration 1645, Testing net (#0)
I0521 09:56:47.011121 31223 solver.cpp:409]     Test net output #0: accuracy = 0.647956
I0521 09:56:47.011282 31223 solver.cpp:409]     Test net output #1: loss = 1.19874 (* 1 = 1.19874 loss)
I0521 09:56:48.131512 31223 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1648.caffemodel
I0521 09:56:48.514411 31223 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097_iter_1648.solverstate
I0521 09:57:09.664902 31223 solver.cpp:321] Iteration 1648, loss = 1.66553
I0521 09:57:09.664945 31223 solver.cpp:326] Optimization Done.
I0521 09:57:09.664954 31223 caffe.cpp:215] Optimization Done.
Application 11237536 resources: utime ~1268s, stime ~228s, Rss ~5329296, inblocks ~3744348, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_910_2016-05-20T11.21.05.881097.solver"
	User time (seconds): 0.60
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:59.74
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 2
	Minor (reclaiming a frame) page faults: 15077
	Voluntary context switches: 2772
	Involuntary context switches: 90
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
