2805185
I0520 14:16:53.484244 24538 caffe.cpp:184] Using GPUs 0
I0520 14:16:53.908738 24538 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1250
test_interval: 2500
base_lr: 0.0025
display: 125
max_iter: 12500
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 1250
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866.prototxt"
I0520 14:16:53.910296 24538 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866.prototxt
I0520 14:16:53.923120 24538 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 14:16:53.923179 24538 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 14:16:53.923522 24538 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 120
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:16:53.923699 24538 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:16:53.923723 24538 net.cpp:106] Creating Layer data_hdf5
I0520 14:16:53.923738 24538 net.cpp:411] data_hdf5 -> data
I0520 14:16:53.923771 24538 net.cpp:411] data_hdf5 -> label
I0520 14:16:53.923804 24538 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 14:16:53.925218 24538 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 14:16:53.927438 24538 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 14:17:15.517673 24538 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 14:17:15.522835 24538 net.cpp:150] Setting up data_hdf5
I0520 14:17:15.522874 24538 net.cpp:157] Top shape: 120 1 127 50 (762000)
I0520 14:17:15.522888 24538 net.cpp:157] Top shape: 120 (120)
I0520 14:17:15.522902 24538 net.cpp:165] Memory required for data: 3048480
I0520 14:17:15.522915 24538 layer_factory.hpp:77] Creating layer conv1
I0520 14:17:15.522948 24538 net.cpp:106] Creating Layer conv1
I0520 14:17:15.522959 24538 net.cpp:454] conv1 <- data
I0520 14:17:15.522979 24538 net.cpp:411] conv1 -> conv1
I0520 14:17:15.893064 24538 net.cpp:150] Setting up conv1
I0520 14:17:15.893111 24538 net.cpp:157] Top shape: 120 12 120 48 (8294400)
I0520 14:17:15.893122 24538 net.cpp:165] Memory required for data: 36226080
I0520 14:17:15.893151 24538 layer_factory.hpp:77] Creating layer relu1
I0520 14:17:15.893172 24538 net.cpp:106] Creating Layer relu1
I0520 14:17:15.893183 24538 net.cpp:454] relu1 <- conv1
I0520 14:17:15.893198 24538 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:17:15.893717 24538 net.cpp:150] Setting up relu1
I0520 14:17:15.893733 24538 net.cpp:157] Top shape: 120 12 120 48 (8294400)
I0520 14:17:15.893743 24538 net.cpp:165] Memory required for data: 69403680
I0520 14:17:15.893754 24538 layer_factory.hpp:77] Creating layer pool1
I0520 14:17:15.893770 24538 net.cpp:106] Creating Layer pool1
I0520 14:17:15.893780 24538 net.cpp:454] pool1 <- conv1
I0520 14:17:15.893795 24538 net.cpp:411] pool1 -> pool1
I0520 14:17:15.893874 24538 net.cpp:150] Setting up pool1
I0520 14:17:15.893888 24538 net.cpp:157] Top shape: 120 12 60 48 (4147200)
I0520 14:17:15.893899 24538 net.cpp:165] Memory required for data: 85992480
I0520 14:17:15.893910 24538 layer_factory.hpp:77] Creating layer conv2
I0520 14:17:15.893932 24538 net.cpp:106] Creating Layer conv2
I0520 14:17:15.893942 24538 net.cpp:454] conv2 <- pool1
I0520 14:17:15.893955 24538 net.cpp:411] conv2 -> conv2
I0520 14:17:15.896697 24538 net.cpp:150] Setting up conv2
I0520 14:17:15.896720 24538 net.cpp:157] Top shape: 120 20 54 46 (5961600)
I0520 14:17:15.896735 24538 net.cpp:165] Memory required for data: 109838880
I0520 14:17:15.896755 24538 layer_factory.hpp:77] Creating layer relu2
I0520 14:17:15.896770 24538 net.cpp:106] Creating Layer relu2
I0520 14:17:15.896780 24538 net.cpp:454] relu2 <- conv2
I0520 14:17:15.896792 24538 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:17:15.897135 24538 net.cpp:150] Setting up relu2
I0520 14:17:15.897148 24538 net.cpp:157] Top shape: 120 20 54 46 (5961600)
I0520 14:17:15.897159 24538 net.cpp:165] Memory required for data: 133685280
I0520 14:17:15.897169 24538 layer_factory.hpp:77] Creating layer pool2
I0520 14:17:15.897181 24538 net.cpp:106] Creating Layer pool2
I0520 14:17:15.897192 24538 net.cpp:454] pool2 <- conv2
I0520 14:17:15.897217 24538 net.cpp:411] pool2 -> pool2
I0520 14:17:15.897286 24538 net.cpp:150] Setting up pool2
I0520 14:17:15.897300 24538 net.cpp:157] Top shape: 120 20 27 46 (2980800)
I0520 14:17:15.897310 24538 net.cpp:165] Memory required for data: 145608480
I0520 14:17:15.897318 24538 layer_factory.hpp:77] Creating layer conv3
I0520 14:17:15.897337 24538 net.cpp:106] Creating Layer conv3
I0520 14:17:15.897347 24538 net.cpp:454] conv3 <- pool2
I0520 14:17:15.897361 24538 net.cpp:411] conv3 -> conv3
I0520 14:17:15.899289 24538 net.cpp:150] Setting up conv3
I0520 14:17:15.899313 24538 net.cpp:157] Top shape: 120 28 22 44 (3252480)
I0520 14:17:15.899325 24538 net.cpp:165] Memory required for data: 158618400
I0520 14:17:15.899343 24538 layer_factory.hpp:77] Creating layer relu3
I0520 14:17:15.899359 24538 net.cpp:106] Creating Layer relu3
I0520 14:17:15.899369 24538 net.cpp:454] relu3 <- conv3
I0520 14:17:15.899382 24538 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:17:15.899852 24538 net.cpp:150] Setting up relu3
I0520 14:17:15.899869 24538 net.cpp:157] Top shape: 120 28 22 44 (3252480)
I0520 14:17:15.899879 24538 net.cpp:165] Memory required for data: 171628320
I0520 14:17:15.899889 24538 layer_factory.hpp:77] Creating layer pool3
I0520 14:17:15.899904 24538 net.cpp:106] Creating Layer pool3
I0520 14:17:15.899912 24538 net.cpp:454] pool3 <- conv3
I0520 14:17:15.899925 24538 net.cpp:411] pool3 -> pool3
I0520 14:17:15.899993 24538 net.cpp:150] Setting up pool3
I0520 14:17:15.900007 24538 net.cpp:157] Top shape: 120 28 11 44 (1626240)
I0520 14:17:15.900017 24538 net.cpp:165] Memory required for data: 178133280
I0520 14:17:15.900024 24538 layer_factory.hpp:77] Creating layer conv4
I0520 14:17:15.900043 24538 net.cpp:106] Creating Layer conv4
I0520 14:17:15.900053 24538 net.cpp:454] conv4 <- pool3
I0520 14:17:15.900065 24538 net.cpp:411] conv4 -> conv4
I0520 14:17:15.903046 24538 net.cpp:150] Setting up conv4
I0520 14:17:15.903070 24538 net.cpp:157] Top shape: 120 36 6 42 (1088640)
I0520 14:17:15.903080 24538 net.cpp:165] Memory required for data: 182487840
I0520 14:17:15.903096 24538 layer_factory.hpp:77] Creating layer relu4
I0520 14:17:15.903110 24538 net.cpp:106] Creating Layer relu4
I0520 14:17:15.903121 24538 net.cpp:454] relu4 <- conv4
I0520 14:17:15.903133 24538 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:17:15.903600 24538 net.cpp:150] Setting up relu4
I0520 14:17:15.903616 24538 net.cpp:157] Top shape: 120 36 6 42 (1088640)
I0520 14:17:15.903627 24538 net.cpp:165] Memory required for data: 186842400
I0520 14:17:15.903637 24538 layer_factory.hpp:77] Creating layer pool4
I0520 14:17:15.903650 24538 net.cpp:106] Creating Layer pool4
I0520 14:17:15.903661 24538 net.cpp:454] pool4 <- conv4
I0520 14:17:15.903672 24538 net.cpp:411] pool4 -> pool4
I0520 14:17:15.903741 24538 net.cpp:150] Setting up pool4
I0520 14:17:15.903754 24538 net.cpp:157] Top shape: 120 36 3 42 (544320)
I0520 14:17:15.903764 24538 net.cpp:165] Memory required for data: 189019680
I0520 14:17:15.903774 24538 layer_factory.hpp:77] Creating layer ip1
I0520 14:17:15.903794 24538 net.cpp:106] Creating Layer ip1
I0520 14:17:15.903805 24538 net.cpp:454] ip1 <- pool4
I0520 14:17:15.903817 24538 net.cpp:411] ip1 -> ip1
I0520 14:17:15.919281 24538 net.cpp:150] Setting up ip1
I0520 14:17:15.919309 24538 net.cpp:157] Top shape: 120 196 (23520)
I0520 14:17:15.919322 24538 net.cpp:165] Memory required for data: 189113760
I0520 14:17:15.919343 24538 layer_factory.hpp:77] Creating layer relu5
I0520 14:17:15.919358 24538 net.cpp:106] Creating Layer relu5
I0520 14:17:15.919368 24538 net.cpp:454] relu5 <- ip1
I0520 14:17:15.919381 24538 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:17:15.919723 24538 net.cpp:150] Setting up relu5
I0520 14:17:15.919736 24538 net.cpp:157] Top shape: 120 196 (23520)
I0520 14:17:15.919746 24538 net.cpp:165] Memory required for data: 189207840
I0520 14:17:15.919756 24538 layer_factory.hpp:77] Creating layer drop1
I0520 14:17:15.919777 24538 net.cpp:106] Creating Layer drop1
I0520 14:17:15.919787 24538 net.cpp:454] drop1 <- ip1
I0520 14:17:15.919812 24538 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:17:15.919859 24538 net.cpp:150] Setting up drop1
I0520 14:17:15.919872 24538 net.cpp:157] Top shape: 120 196 (23520)
I0520 14:17:15.919883 24538 net.cpp:165] Memory required for data: 189301920
I0520 14:17:15.919891 24538 layer_factory.hpp:77] Creating layer ip2
I0520 14:17:15.919910 24538 net.cpp:106] Creating Layer ip2
I0520 14:17:15.919920 24538 net.cpp:454] ip2 <- ip1
I0520 14:17:15.919934 24538 net.cpp:411] ip2 -> ip2
I0520 14:17:15.920398 24538 net.cpp:150] Setting up ip2
I0520 14:17:15.920413 24538 net.cpp:157] Top shape: 120 98 (11760)
I0520 14:17:15.920423 24538 net.cpp:165] Memory required for data: 189348960
I0520 14:17:15.920438 24538 layer_factory.hpp:77] Creating layer relu6
I0520 14:17:15.920450 24538 net.cpp:106] Creating Layer relu6
I0520 14:17:15.920459 24538 net.cpp:454] relu6 <- ip2
I0520 14:17:15.920471 24538 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:17:15.921003 24538 net.cpp:150] Setting up relu6
I0520 14:17:15.921020 24538 net.cpp:157] Top shape: 120 98 (11760)
I0520 14:17:15.921030 24538 net.cpp:165] Memory required for data: 189396000
I0520 14:17:15.921039 24538 layer_factory.hpp:77] Creating layer drop2
I0520 14:17:15.921052 24538 net.cpp:106] Creating Layer drop2
I0520 14:17:15.921062 24538 net.cpp:454] drop2 <- ip2
I0520 14:17:15.921074 24538 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:17:15.921116 24538 net.cpp:150] Setting up drop2
I0520 14:17:15.921129 24538 net.cpp:157] Top shape: 120 98 (11760)
I0520 14:17:15.921140 24538 net.cpp:165] Memory required for data: 189443040
I0520 14:17:15.921150 24538 layer_factory.hpp:77] Creating layer ip3
I0520 14:17:15.921164 24538 net.cpp:106] Creating Layer ip3
I0520 14:17:15.921174 24538 net.cpp:454] ip3 <- ip2
I0520 14:17:15.921186 24538 net.cpp:411] ip3 -> ip3
I0520 14:17:15.921396 24538 net.cpp:150] Setting up ip3
I0520 14:17:15.921409 24538 net.cpp:157] Top shape: 120 11 (1320)
I0520 14:17:15.921419 24538 net.cpp:165] Memory required for data: 189448320
I0520 14:17:15.921434 24538 layer_factory.hpp:77] Creating layer drop3
I0520 14:17:15.921447 24538 net.cpp:106] Creating Layer drop3
I0520 14:17:15.921457 24538 net.cpp:454] drop3 <- ip3
I0520 14:17:15.921468 24538 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:17:15.921507 24538 net.cpp:150] Setting up drop3
I0520 14:17:15.921520 24538 net.cpp:157] Top shape: 120 11 (1320)
I0520 14:17:15.921530 24538 net.cpp:165] Memory required for data: 189453600
I0520 14:17:15.921540 24538 layer_factory.hpp:77] Creating layer loss
I0520 14:17:15.921560 24538 net.cpp:106] Creating Layer loss
I0520 14:17:15.921569 24538 net.cpp:454] loss <- ip3
I0520 14:17:15.921581 24538 net.cpp:454] loss <- label
I0520 14:17:15.921592 24538 net.cpp:411] loss -> loss
I0520 14:17:15.921608 24538 layer_factory.hpp:77] Creating layer loss
I0520 14:17:15.922255 24538 net.cpp:150] Setting up loss
I0520 14:17:15.922276 24538 net.cpp:157] Top shape: (1)
I0520 14:17:15.922289 24538 net.cpp:160]     with loss weight 1
I0520 14:17:15.922330 24538 net.cpp:165] Memory required for data: 189453604
I0520 14:17:15.922341 24538 net.cpp:226] loss needs backward computation.
I0520 14:17:15.922353 24538 net.cpp:226] drop3 needs backward computation.
I0520 14:17:15.922363 24538 net.cpp:226] ip3 needs backward computation.
I0520 14:17:15.922371 24538 net.cpp:226] drop2 needs backward computation.
I0520 14:17:15.922381 24538 net.cpp:226] relu6 needs backward computation.
I0520 14:17:15.922391 24538 net.cpp:226] ip2 needs backward computation.
I0520 14:17:15.922401 24538 net.cpp:226] drop1 needs backward computation.
I0520 14:17:15.922411 24538 net.cpp:226] relu5 needs backward computation.
I0520 14:17:15.922420 24538 net.cpp:226] ip1 needs backward computation.
I0520 14:17:15.922430 24538 net.cpp:226] pool4 needs backward computation.
I0520 14:17:15.922441 24538 net.cpp:226] relu4 needs backward computation.
I0520 14:17:15.922451 24538 net.cpp:226] conv4 needs backward computation.
I0520 14:17:15.922461 24538 net.cpp:226] pool3 needs backward computation.
I0520 14:17:15.922471 24538 net.cpp:226] relu3 needs backward computation.
I0520 14:17:15.922489 24538 net.cpp:226] conv3 needs backward computation.
I0520 14:17:15.922500 24538 net.cpp:226] pool2 needs backward computation.
I0520 14:17:15.922508 24538 net.cpp:226] relu2 needs backward computation.
I0520 14:17:15.922519 24538 net.cpp:226] conv2 needs backward computation.
I0520 14:17:15.922530 24538 net.cpp:226] pool1 needs backward computation.
I0520 14:17:15.922540 24538 net.cpp:226] relu1 needs backward computation.
I0520 14:17:15.922550 24538 net.cpp:226] conv1 needs backward computation.
I0520 14:17:15.922560 24538 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:17:15.922570 24538 net.cpp:270] This network produces output loss
I0520 14:17:15.922593 24538 net.cpp:283] Network initialization done.
I0520 14:17:15.924477 24538 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866.prototxt
I0520 14:17:15.924549 24538 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 14:17:15.924913 24538 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 120
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 14:17:15.925104 24538 layer_factory.hpp:77] Creating layer data_hdf5
I0520 14:17:15.925119 24538 net.cpp:106] Creating Layer data_hdf5
I0520 14:17:15.925132 24538 net.cpp:411] data_hdf5 -> data
I0520 14:17:15.925148 24538 net.cpp:411] data_hdf5 -> label
I0520 14:17:15.925164 24538 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 14:17:15.926411 24538 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 14:17:37.304813 24538 net.cpp:150] Setting up data_hdf5
I0520 14:17:37.304987 24538 net.cpp:157] Top shape: 120 1 127 50 (762000)
I0520 14:17:37.305002 24538 net.cpp:157] Top shape: 120 (120)
I0520 14:17:37.305014 24538 net.cpp:165] Memory required for data: 3048480
I0520 14:17:37.305028 24538 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 14:17:37.305057 24538 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 14:17:37.305068 24538 net.cpp:454] label_data_hdf5_1_split <- label
I0520 14:17:37.305083 24538 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 14:17:37.305104 24538 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 14:17:37.305177 24538 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 14:17:37.305191 24538 net.cpp:157] Top shape: 120 (120)
I0520 14:17:37.305202 24538 net.cpp:157] Top shape: 120 (120)
I0520 14:17:37.305212 24538 net.cpp:165] Memory required for data: 3049440
I0520 14:17:37.305222 24538 layer_factory.hpp:77] Creating layer conv1
I0520 14:17:37.305243 24538 net.cpp:106] Creating Layer conv1
I0520 14:17:37.305253 24538 net.cpp:454] conv1 <- data
I0520 14:17:37.305269 24538 net.cpp:411] conv1 -> conv1
I0520 14:17:37.307194 24538 net.cpp:150] Setting up conv1
I0520 14:17:37.307219 24538 net.cpp:157] Top shape: 120 12 120 48 (8294400)
I0520 14:17:37.307230 24538 net.cpp:165] Memory required for data: 36227040
I0520 14:17:37.307250 24538 layer_factory.hpp:77] Creating layer relu1
I0520 14:17:37.307265 24538 net.cpp:106] Creating Layer relu1
I0520 14:17:37.307276 24538 net.cpp:454] relu1 <- conv1
I0520 14:17:37.307288 24538 net.cpp:397] relu1 -> conv1 (in-place)
I0520 14:17:37.307785 24538 net.cpp:150] Setting up relu1
I0520 14:17:37.307799 24538 net.cpp:157] Top shape: 120 12 120 48 (8294400)
I0520 14:17:37.307811 24538 net.cpp:165] Memory required for data: 69404640
I0520 14:17:37.307821 24538 layer_factory.hpp:77] Creating layer pool1
I0520 14:17:37.307835 24538 net.cpp:106] Creating Layer pool1
I0520 14:17:37.307845 24538 net.cpp:454] pool1 <- conv1
I0520 14:17:37.307858 24538 net.cpp:411] pool1 -> pool1
I0520 14:17:37.307934 24538 net.cpp:150] Setting up pool1
I0520 14:17:37.307947 24538 net.cpp:157] Top shape: 120 12 60 48 (4147200)
I0520 14:17:37.307957 24538 net.cpp:165] Memory required for data: 85993440
I0520 14:17:37.307967 24538 layer_factory.hpp:77] Creating layer conv2
I0520 14:17:37.307986 24538 net.cpp:106] Creating Layer conv2
I0520 14:17:37.307996 24538 net.cpp:454] conv2 <- pool1
I0520 14:17:37.308009 24538 net.cpp:411] conv2 -> conv2
I0520 14:17:37.309937 24538 net.cpp:150] Setting up conv2
I0520 14:17:37.309959 24538 net.cpp:157] Top shape: 120 20 54 46 (5961600)
I0520 14:17:37.309973 24538 net.cpp:165] Memory required for data: 109839840
I0520 14:17:37.309989 24538 layer_factory.hpp:77] Creating layer relu2
I0520 14:17:37.310003 24538 net.cpp:106] Creating Layer relu2
I0520 14:17:37.310014 24538 net.cpp:454] relu2 <- conv2
I0520 14:17:37.310025 24538 net.cpp:397] relu2 -> conv2 (in-place)
I0520 14:17:37.310361 24538 net.cpp:150] Setting up relu2
I0520 14:17:37.310375 24538 net.cpp:157] Top shape: 120 20 54 46 (5961600)
I0520 14:17:37.310385 24538 net.cpp:165] Memory required for data: 133686240
I0520 14:17:37.310395 24538 layer_factory.hpp:77] Creating layer pool2
I0520 14:17:37.310408 24538 net.cpp:106] Creating Layer pool2
I0520 14:17:37.310418 24538 net.cpp:454] pool2 <- conv2
I0520 14:17:37.310431 24538 net.cpp:411] pool2 -> pool2
I0520 14:17:37.310503 24538 net.cpp:150] Setting up pool2
I0520 14:17:37.310515 24538 net.cpp:157] Top shape: 120 20 27 46 (2980800)
I0520 14:17:37.310525 24538 net.cpp:165] Memory required for data: 145609440
I0520 14:17:37.310535 24538 layer_factory.hpp:77] Creating layer conv3
I0520 14:17:37.310554 24538 net.cpp:106] Creating Layer conv3
I0520 14:17:37.310564 24538 net.cpp:454] conv3 <- pool2
I0520 14:17:37.310577 24538 net.cpp:411] conv3 -> conv3
I0520 14:17:37.312572 24538 net.cpp:150] Setting up conv3
I0520 14:17:37.312595 24538 net.cpp:157] Top shape: 120 28 22 44 (3252480)
I0520 14:17:37.312608 24538 net.cpp:165] Memory required for data: 158619360
I0520 14:17:37.312640 24538 layer_factory.hpp:77] Creating layer relu3
I0520 14:17:37.312654 24538 net.cpp:106] Creating Layer relu3
I0520 14:17:37.312664 24538 net.cpp:454] relu3 <- conv3
I0520 14:17:37.312677 24538 net.cpp:397] relu3 -> conv3 (in-place)
I0520 14:17:37.313160 24538 net.cpp:150] Setting up relu3
I0520 14:17:37.313176 24538 net.cpp:157] Top shape: 120 28 22 44 (3252480)
I0520 14:17:37.313187 24538 net.cpp:165] Memory required for data: 171629280
I0520 14:17:37.313197 24538 layer_factory.hpp:77] Creating layer pool3
I0520 14:17:37.313210 24538 net.cpp:106] Creating Layer pool3
I0520 14:17:37.313220 24538 net.cpp:454] pool3 <- conv3
I0520 14:17:37.313233 24538 net.cpp:411] pool3 -> pool3
I0520 14:17:37.313304 24538 net.cpp:150] Setting up pool3
I0520 14:17:37.313318 24538 net.cpp:157] Top shape: 120 28 11 44 (1626240)
I0520 14:17:37.313328 24538 net.cpp:165] Memory required for data: 178134240
I0520 14:17:37.313335 24538 layer_factory.hpp:77] Creating layer conv4
I0520 14:17:37.313354 24538 net.cpp:106] Creating Layer conv4
I0520 14:17:37.313364 24538 net.cpp:454] conv4 <- pool3
I0520 14:17:37.313379 24538 net.cpp:411] conv4 -> conv4
I0520 14:17:37.315462 24538 net.cpp:150] Setting up conv4
I0520 14:17:37.315485 24538 net.cpp:157] Top shape: 120 36 6 42 (1088640)
I0520 14:17:37.315497 24538 net.cpp:165] Memory required for data: 182488800
I0520 14:17:37.315512 24538 layer_factory.hpp:77] Creating layer relu4
I0520 14:17:37.315526 24538 net.cpp:106] Creating Layer relu4
I0520 14:17:37.315536 24538 net.cpp:454] relu4 <- conv4
I0520 14:17:37.315549 24538 net.cpp:397] relu4 -> conv4 (in-place)
I0520 14:17:37.316021 24538 net.cpp:150] Setting up relu4
I0520 14:17:37.316037 24538 net.cpp:157] Top shape: 120 36 6 42 (1088640)
I0520 14:17:37.316047 24538 net.cpp:165] Memory required for data: 186843360
I0520 14:17:37.316057 24538 layer_factory.hpp:77] Creating layer pool4
I0520 14:17:37.316071 24538 net.cpp:106] Creating Layer pool4
I0520 14:17:37.316081 24538 net.cpp:454] pool4 <- conv4
I0520 14:17:37.316093 24538 net.cpp:411] pool4 -> pool4
I0520 14:17:37.316165 24538 net.cpp:150] Setting up pool4
I0520 14:17:37.316179 24538 net.cpp:157] Top shape: 120 36 3 42 (544320)
I0520 14:17:37.316189 24538 net.cpp:165] Memory required for data: 189020640
I0520 14:17:37.316196 24538 layer_factory.hpp:77] Creating layer ip1
I0520 14:17:37.316212 24538 net.cpp:106] Creating Layer ip1
I0520 14:17:37.316223 24538 net.cpp:454] ip1 <- pool4
I0520 14:17:37.316237 24538 net.cpp:411] ip1 -> ip1
I0520 14:17:37.331768 24538 net.cpp:150] Setting up ip1
I0520 14:17:37.331797 24538 net.cpp:157] Top shape: 120 196 (23520)
I0520 14:17:37.331807 24538 net.cpp:165] Memory required for data: 189114720
I0520 14:17:37.331830 24538 layer_factory.hpp:77] Creating layer relu5
I0520 14:17:37.331845 24538 net.cpp:106] Creating Layer relu5
I0520 14:17:37.331856 24538 net.cpp:454] relu5 <- ip1
I0520 14:17:37.331869 24538 net.cpp:397] relu5 -> ip1 (in-place)
I0520 14:17:37.332216 24538 net.cpp:150] Setting up relu5
I0520 14:17:37.332231 24538 net.cpp:157] Top shape: 120 196 (23520)
I0520 14:17:37.332240 24538 net.cpp:165] Memory required for data: 189208800
I0520 14:17:37.332250 24538 layer_factory.hpp:77] Creating layer drop1
I0520 14:17:37.332269 24538 net.cpp:106] Creating Layer drop1
I0520 14:17:37.332279 24538 net.cpp:454] drop1 <- ip1
I0520 14:17:37.332293 24538 net.cpp:397] drop1 -> ip1 (in-place)
I0520 14:17:37.332337 24538 net.cpp:150] Setting up drop1
I0520 14:17:37.332350 24538 net.cpp:157] Top shape: 120 196 (23520)
I0520 14:17:37.332360 24538 net.cpp:165] Memory required for data: 189302880
I0520 14:17:37.332370 24538 layer_factory.hpp:77] Creating layer ip2
I0520 14:17:37.332384 24538 net.cpp:106] Creating Layer ip2
I0520 14:17:37.332393 24538 net.cpp:454] ip2 <- ip1
I0520 14:17:37.332407 24538 net.cpp:411] ip2 -> ip2
I0520 14:17:37.332892 24538 net.cpp:150] Setting up ip2
I0520 14:17:37.332906 24538 net.cpp:157] Top shape: 120 98 (11760)
I0520 14:17:37.332916 24538 net.cpp:165] Memory required for data: 189349920
I0520 14:17:37.332943 24538 layer_factory.hpp:77] Creating layer relu6
I0520 14:17:37.332957 24538 net.cpp:106] Creating Layer relu6
I0520 14:17:37.332967 24538 net.cpp:454] relu6 <- ip2
I0520 14:17:37.332978 24538 net.cpp:397] relu6 -> ip2 (in-place)
I0520 14:17:37.333513 24538 net.cpp:150] Setting up relu6
I0520 14:17:37.333534 24538 net.cpp:157] Top shape: 120 98 (11760)
I0520 14:17:37.333544 24538 net.cpp:165] Memory required for data: 189396960
I0520 14:17:37.333554 24538 layer_factory.hpp:77] Creating layer drop2
I0520 14:17:37.333567 24538 net.cpp:106] Creating Layer drop2
I0520 14:17:37.333577 24538 net.cpp:454] drop2 <- ip2
I0520 14:17:37.333590 24538 net.cpp:397] drop2 -> ip2 (in-place)
I0520 14:17:37.333634 24538 net.cpp:150] Setting up drop2
I0520 14:17:37.333647 24538 net.cpp:157] Top shape: 120 98 (11760)
I0520 14:17:37.333657 24538 net.cpp:165] Memory required for data: 189444000
I0520 14:17:37.333667 24538 layer_factory.hpp:77] Creating layer ip3
I0520 14:17:37.333681 24538 net.cpp:106] Creating Layer ip3
I0520 14:17:37.333691 24538 net.cpp:454] ip3 <- ip2
I0520 14:17:37.333705 24538 net.cpp:411] ip3 -> ip3
I0520 14:17:37.333930 24538 net.cpp:150] Setting up ip3
I0520 14:17:37.333942 24538 net.cpp:157] Top shape: 120 11 (1320)
I0520 14:17:37.333952 24538 net.cpp:165] Memory required for data: 189449280
I0520 14:17:37.333967 24538 layer_factory.hpp:77] Creating layer drop3
I0520 14:17:37.333981 24538 net.cpp:106] Creating Layer drop3
I0520 14:17:37.333991 24538 net.cpp:454] drop3 <- ip3
I0520 14:17:37.334003 24538 net.cpp:397] drop3 -> ip3 (in-place)
I0520 14:17:37.334044 24538 net.cpp:150] Setting up drop3
I0520 14:17:37.334056 24538 net.cpp:157] Top shape: 120 11 (1320)
I0520 14:17:37.334066 24538 net.cpp:165] Memory required for data: 189454560
I0520 14:17:37.334075 24538 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 14:17:37.334089 24538 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 14:17:37.334098 24538 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 14:17:37.334111 24538 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 14:17:37.334126 24538 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 14:17:37.334200 24538 net.cpp:150] Setting up ip3_drop3_0_split
I0520 14:17:37.334213 24538 net.cpp:157] Top shape: 120 11 (1320)
I0520 14:17:37.334225 24538 net.cpp:157] Top shape: 120 11 (1320)
I0520 14:17:37.334235 24538 net.cpp:165] Memory required for data: 189465120
I0520 14:17:37.334245 24538 layer_factory.hpp:77] Creating layer accuracy
I0520 14:17:37.334267 24538 net.cpp:106] Creating Layer accuracy
I0520 14:17:37.334277 24538 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 14:17:37.334290 24538 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 14:17:37.334302 24538 net.cpp:411] accuracy -> accuracy
I0520 14:17:37.334326 24538 net.cpp:150] Setting up accuracy
I0520 14:17:37.334338 24538 net.cpp:157] Top shape: (1)
I0520 14:17:37.334348 24538 net.cpp:165] Memory required for data: 189465124
I0520 14:17:37.334358 24538 layer_factory.hpp:77] Creating layer loss
I0520 14:17:37.334372 24538 net.cpp:106] Creating Layer loss
I0520 14:17:37.334383 24538 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 14:17:37.334393 24538 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 14:17:37.334406 24538 net.cpp:411] loss -> loss
I0520 14:17:37.334424 24538 layer_factory.hpp:77] Creating layer loss
I0520 14:17:37.334909 24538 net.cpp:150] Setting up loss
I0520 14:17:37.334923 24538 net.cpp:157] Top shape: (1)
I0520 14:17:37.334933 24538 net.cpp:160]     with loss weight 1
I0520 14:17:37.334951 24538 net.cpp:165] Memory required for data: 189465128
I0520 14:17:37.334961 24538 net.cpp:226] loss needs backward computation.
I0520 14:17:37.334972 24538 net.cpp:228] accuracy does not need backward computation.
I0520 14:17:37.334985 24538 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 14:17:37.334995 24538 net.cpp:226] drop3 needs backward computation.
I0520 14:17:37.335005 24538 net.cpp:226] ip3 needs backward computation.
I0520 14:17:37.335014 24538 net.cpp:226] drop2 needs backward computation.
I0520 14:17:37.335033 24538 net.cpp:226] relu6 needs backward computation.
I0520 14:17:37.335043 24538 net.cpp:226] ip2 needs backward computation.
I0520 14:17:37.335053 24538 net.cpp:226] drop1 needs backward computation.
I0520 14:17:37.335063 24538 net.cpp:226] relu5 needs backward computation.
I0520 14:17:37.335072 24538 net.cpp:226] ip1 needs backward computation.
I0520 14:17:37.335083 24538 net.cpp:226] pool4 needs backward computation.
I0520 14:17:37.335093 24538 net.cpp:226] relu4 needs backward computation.
I0520 14:17:37.335103 24538 net.cpp:226] conv4 needs backward computation.
I0520 14:17:37.335114 24538 net.cpp:226] pool3 needs backward computation.
I0520 14:17:37.335124 24538 net.cpp:226] relu3 needs backward computation.
I0520 14:17:37.335134 24538 net.cpp:226] conv3 needs backward computation.
I0520 14:17:37.335144 24538 net.cpp:226] pool2 needs backward computation.
I0520 14:17:37.335155 24538 net.cpp:226] relu2 needs backward computation.
I0520 14:17:37.335165 24538 net.cpp:226] conv2 needs backward computation.
I0520 14:17:37.335175 24538 net.cpp:226] pool1 needs backward computation.
I0520 14:17:37.335186 24538 net.cpp:226] relu1 needs backward computation.
I0520 14:17:37.335196 24538 net.cpp:226] conv1 needs backward computation.
I0520 14:17:37.335207 24538 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 14:17:37.335218 24538 net.cpp:228] data_hdf5 does not need backward computation.
I0520 14:17:37.335228 24538 net.cpp:270] This network produces output accuracy
I0520 14:17:37.335238 24538 net.cpp:270] This network produces output loss
I0520 14:17:37.335264 24538 net.cpp:283] Network initialization done.
I0520 14:17:37.335399 24538 solver.cpp:60] Solver scaffolding done.
I0520 14:17:37.336537 24538 caffe.cpp:212] Starting Optimization
I0520 14:17:37.336556 24538 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 14:17:37.336570 24538 solver.cpp:289] Learning Rate Policy: fixed
I0520 14:17:37.337652 24538 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 14:18:24.687377 24538 solver.cpp:409]     Test net output #0: accuracy = 0.0633666
I0520 14:18:24.687538 24538 solver.cpp:409]     Test net output #1: loss = 2.39857 (* 1 = 2.39857 loss)
I0520 14:18:24.723619 24538 solver.cpp:237] Iteration 0, loss = 2.3983
I0520 14:18:24.723655 24538 solver.cpp:253]     Train net output #0: loss = 2.3983 (* 1 = 2.3983 loss)
I0520 14:18:24.723673 24538 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 14:18:33.344975 24538 solver.cpp:237] Iteration 125, loss = 2.30452
I0520 14:18:33.345011 24538 solver.cpp:253]     Train net output #0: loss = 2.30452 (* 1 = 2.30452 loss)
I0520 14:18:33.345027 24538 sgd_solver.cpp:106] Iteration 125, lr = 0.0025
I0520 14:18:41.961107 24538 solver.cpp:237] Iteration 250, loss = 2.28833
I0520 14:18:41.961155 24538 solver.cpp:253]     Train net output #0: loss = 2.28833 (* 1 = 2.28833 loss)
I0520 14:18:41.961171 24538 sgd_solver.cpp:106] Iteration 250, lr = 0.0025
I0520 14:18:50.586052 24538 solver.cpp:237] Iteration 375, loss = 2.06134
I0520 14:18:50.586088 24538 solver.cpp:253]     Train net output #0: loss = 2.06134 (* 1 = 2.06134 loss)
I0520 14:18:50.586104 24538 sgd_solver.cpp:106] Iteration 375, lr = 0.0025
I0520 14:18:59.200963 24538 solver.cpp:237] Iteration 500, loss = 2.03605
I0520 14:18:59.201109 24538 solver.cpp:253]     Train net output #0: loss = 2.03605 (* 1 = 2.03605 loss)
I0520 14:18:59.201122 24538 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0520 14:19:07.813961 24538 solver.cpp:237] Iteration 625, loss = 2.03001
I0520 14:19:07.814008 24538 solver.cpp:253]     Train net output #0: loss = 2.03001 (* 1 = 2.03001 loss)
I0520 14:19:07.814024 24538 sgd_solver.cpp:106] Iteration 625, lr = 0.0025
I0520 14:19:16.430688 24538 solver.cpp:237] Iteration 750, loss = 1.94387
I0520 14:19:16.430724 24538 solver.cpp:253]     Train net output #0: loss = 1.94387 (* 1 = 1.94387 loss)
I0520 14:19:16.430740 24538 sgd_solver.cpp:106] Iteration 750, lr = 0.0025
I0520 14:19:47.186964 24538 solver.cpp:237] Iteration 875, loss = 1.91992
I0520 14:19:47.187126 24538 solver.cpp:253]     Train net output #0: loss = 1.91992 (* 1 = 1.91992 loss)
I0520 14:19:47.187140 24538 sgd_solver.cpp:106] Iteration 875, lr = 0.0025
I0520 14:19:55.811720 24538 solver.cpp:237] Iteration 1000, loss = 1.89994
I0520 14:19:55.811758 24538 solver.cpp:253]     Train net output #0: loss = 1.89994 (* 1 = 1.89994 loss)
I0520 14:19:55.811776 24538 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0520 14:20:04.435163 24538 solver.cpp:237] Iteration 1125, loss = 1.89796
I0520 14:20:04.435199 24538 solver.cpp:253]     Train net output #0: loss = 1.89796 (* 1 = 1.89796 loss)
I0520 14:20:04.435211 24538 sgd_solver.cpp:106] Iteration 1125, lr = 0.0025
I0520 14:20:12.992720 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_1250.caffemodel
I0520 14:20:13.082101 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_1250.solverstate
I0520 14:20:13.129751 24538 solver.cpp:237] Iteration 1250, loss = 1.82838
I0520 14:20:13.129792 24538 solver.cpp:253]     Train net output #0: loss = 1.82838 (* 1 = 1.82838 loss)
I0520 14:20:13.129813 24538 sgd_solver.cpp:106] Iteration 1250, lr = 0.0025
I0520 14:20:21.752599 24538 solver.cpp:237] Iteration 1375, loss = 1.92619
I0520 14:20:21.752748 24538 solver.cpp:253]     Train net output #0: loss = 1.92619 (* 1 = 1.92619 loss)
I0520 14:20:21.752763 24538 sgd_solver.cpp:106] Iteration 1375, lr = 0.0025
I0520 14:20:30.377020 24538 solver.cpp:237] Iteration 1500, loss = 1.7358
I0520 14:20:30.377055 24538 solver.cpp:253]     Train net output #0: loss = 1.7358 (* 1 = 1.7358 loss)
I0520 14:20:30.377073 24538 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0520 14:20:38.998059 24538 solver.cpp:237] Iteration 1625, loss = 1.79821
I0520 14:20:38.998095 24538 solver.cpp:253]     Train net output #0: loss = 1.79821 (* 1 = 1.79821 loss)
I0520 14:20:38.998108 24538 sgd_solver.cpp:106] Iteration 1625, lr = 0.0025
I0520 14:21:09.817582 24538 solver.cpp:237] Iteration 1750, loss = 1.74032
I0520 14:21:09.817734 24538 solver.cpp:253]     Train net output #0: loss = 1.74032 (* 1 = 1.74032 loss)
I0520 14:21:09.817749 24538 sgd_solver.cpp:106] Iteration 1750, lr = 0.0025
I0520 14:21:18.437206 24538 solver.cpp:237] Iteration 1875, loss = 1.79674
I0520 14:21:18.437242 24538 solver.cpp:253]     Train net output #0: loss = 1.79674 (* 1 = 1.79674 loss)
I0520 14:21:18.437257 24538 sgd_solver.cpp:106] Iteration 1875, lr = 0.0025
I0520 14:21:27.055768 24538 solver.cpp:237] Iteration 2000, loss = 1.84222
I0520 14:21:27.055804 24538 solver.cpp:253]     Train net output #0: loss = 1.84222 (* 1 = 1.84222 loss)
I0520 14:21:27.055820 24538 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0520 14:21:35.677351 24538 solver.cpp:237] Iteration 2125, loss = 1.69097
I0520 14:21:35.677393 24538 solver.cpp:253]     Train net output #0: loss = 1.69097 (* 1 = 1.69097 loss)
I0520 14:21:35.677412 24538 sgd_solver.cpp:106] Iteration 2125, lr = 0.0025
I0520 14:21:44.301543 24538 solver.cpp:237] Iteration 2250, loss = 1.68685
I0520 14:21:44.301692 24538 solver.cpp:253]     Train net output #0: loss = 1.68685 (* 1 = 1.68685 loss)
I0520 14:21:44.301705 24538 sgd_solver.cpp:106] Iteration 2250, lr = 0.0025
I0520 14:21:52.928710 24538 solver.cpp:237] Iteration 2375, loss = 1.697
I0520 14:21:52.928745 24538 solver.cpp:253]     Train net output #0: loss = 1.697 (* 1 = 1.697 loss)
I0520 14:21:52.928762 24538 sgd_solver.cpp:106] Iteration 2375, lr = 0.0025
I0520 14:22:01.482878 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_2500.caffemodel
I0520 14:22:01.568941 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_2500.solverstate
I0520 14:22:01.595291 24538 solver.cpp:341] Iteration 2500, Testing net (#0)
I0520 14:22:48.051373 24538 solver.cpp:409]     Test net output #0: accuracy = 0.675006
I0520 14:22:48.051534 24538 solver.cpp:409]     Test net output #1: loss = 1.10739 (* 1 = 1.10739 loss)
I0520 14:23:10.210383 24538 solver.cpp:237] Iteration 2500, loss = 1.65902
I0520 14:23:10.210435 24538 solver.cpp:253]     Train net output #0: loss = 1.65902 (* 1 = 1.65902 loss)
I0520 14:23:10.210450 24538 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0520 14:23:18.833130 24538 solver.cpp:237] Iteration 2625, loss = 1.83276
I0520 14:23:18.833282 24538 solver.cpp:253]     Train net output #0: loss = 1.83276 (* 1 = 1.83276 loss)
I0520 14:23:18.833297 24538 sgd_solver.cpp:106] Iteration 2625, lr = 0.0025
I0520 14:23:27.459202 24538 solver.cpp:237] Iteration 2750, loss = 1.74322
I0520 14:23:27.459236 24538 solver.cpp:253]     Train net output #0: loss = 1.74322 (* 1 = 1.74322 loss)
I0520 14:23:27.459250 24538 sgd_solver.cpp:106] Iteration 2750, lr = 0.0025
I0520 14:23:36.083559 24538 solver.cpp:237] Iteration 2875, loss = 1.78136
I0520 14:23:36.083595 24538 solver.cpp:253]     Train net output #0: loss = 1.78136 (* 1 = 1.78136 loss)
I0520 14:23:36.083611 24538 sgd_solver.cpp:106] Iteration 2875, lr = 0.0025
I0520 14:23:44.709470 24538 solver.cpp:237] Iteration 3000, loss = 1.61273
I0520 14:23:44.709514 24538 solver.cpp:253]     Train net output #0: loss = 1.61273 (* 1 = 1.61273 loss)
I0520 14:23:44.709532 24538 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0520 14:23:53.332458 24538 solver.cpp:237] Iteration 3125, loss = 1.57211
I0520 14:23:53.332597 24538 solver.cpp:253]     Train net output #0: loss = 1.57211 (* 1 = 1.57211 loss)
I0520 14:23:53.332609 24538 sgd_solver.cpp:106] Iteration 3125, lr = 0.0025
I0520 14:24:01.953759 24538 solver.cpp:237] Iteration 3250, loss = 1.4256
I0520 14:24:01.953793 24538 solver.cpp:253]     Train net output #0: loss = 1.4256 (* 1 = 1.4256 loss)
I0520 14:24:01.953809 24538 sgd_solver.cpp:106] Iteration 3250, lr = 0.0025
I0520 14:24:32.757586 24538 solver.cpp:237] Iteration 3375, loss = 1.65484
I0520 14:24:32.757751 24538 solver.cpp:253]     Train net output #0: loss = 1.65484 (* 1 = 1.65484 loss)
I0520 14:24:32.757767 24538 sgd_solver.cpp:106] Iteration 3375, lr = 0.0025
I0520 14:24:41.378254 24538 solver.cpp:237] Iteration 3500, loss = 1.40723
I0520 14:24:41.378289 24538 solver.cpp:253]     Train net output #0: loss = 1.40723 (* 1 = 1.40723 loss)
I0520 14:24:41.378304 24538 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0520 14:24:49.995031 24538 solver.cpp:237] Iteration 3625, loss = 1.66367
I0520 14:24:49.995066 24538 solver.cpp:253]     Train net output #0: loss = 1.66367 (* 1 = 1.66367 loss)
I0520 14:24:49.995084 24538 sgd_solver.cpp:106] Iteration 3625, lr = 0.0025
I0520 14:24:58.545325 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_3750.caffemodel
I0520 14:24:58.634338 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_3750.solverstate
I0520 14:24:58.683936 24538 solver.cpp:237] Iteration 3750, loss = 1.55358
I0520 14:24:58.683986 24538 solver.cpp:253]     Train net output #0: loss = 1.55358 (* 1 = 1.55358 loss)
I0520 14:24:58.684002 24538 sgd_solver.cpp:106] Iteration 3750, lr = 0.0025
I0520 14:25:07.306900 24538 solver.cpp:237] Iteration 3875, loss = 1.5996
I0520 14:25:07.307054 24538 solver.cpp:253]     Train net output #0: loss = 1.5996 (* 1 = 1.5996 loss)
I0520 14:25:07.307067 24538 sgd_solver.cpp:106] Iteration 3875, lr = 0.0025
I0520 14:25:15.927110 24538 solver.cpp:237] Iteration 4000, loss = 1.62426
I0520 14:25:15.927145 24538 solver.cpp:253]     Train net output #0: loss = 1.62426 (* 1 = 1.62426 loss)
I0520 14:25:15.927162 24538 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0520 14:25:24.540148 24538 solver.cpp:237] Iteration 4125, loss = 1.46398
I0520 14:25:24.540196 24538 solver.cpp:253]     Train net output #0: loss = 1.46398 (* 1 = 1.46398 loss)
I0520 14:25:24.540210 24538 sgd_solver.cpp:106] Iteration 4125, lr = 0.0025
I0520 14:25:55.362352 24538 solver.cpp:237] Iteration 4250, loss = 1.46329
I0520 14:25:55.362511 24538 solver.cpp:253]     Train net output #0: loss = 1.46329 (* 1 = 1.46329 loss)
I0520 14:25:55.362527 24538 sgd_solver.cpp:106] Iteration 4250, lr = 0.0025
I0520 14:26:03.981478 24538 solver.cpp:237] Iteration 4375, loss = 1.34
I0520 14:26:03.981513 24538 solver.cpp:253]     Train net output #0: loss = 1.34 (* 1 = 1.34 loss)
I0520 14:26:03.981529 24538 sgd_solver.cpp:106] Iteration 4375, lr = 0.0025
I0520 14:26:12.599123 24538 solver.cpp:237] Iteration 4500, loss = 1.39059
I0520 14:26:12.599165 24538 solver.cpp:253]     Train net output #0: loss = 1.39059 (* 1 = 1.39059 loss)
I0520 14:26:12.599181 24538 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0520 14:26:21.223650 24538 solver.cpp:237] Iteration 4625, loss = 1.44941
I0520 14:26:21.223685 24538 solver.cpp:253]     Train net output #0: loss = 1.44941 (* 1 = 1.44941 loss)
I0520 14:26:21.223701 24538 sgd_solver.cpp:106] Iteration 4625, lr = 0.0025
I0520 14:26:29.847146 24538 solver.cpp:237] Iteration 4750, loss = 1.34479
I0520 14:26:29.847287 24538 solver.cpp:253]     Train net output #0: loss = 1.34479 (* 1 = 1.34479 loss)
I0520 14:26:29.847301 24538 sgd_solver.cpp:106] Iteration 4750, lr = 0.0025
I0520 14:26:38.474213 24538 solver.cpp:237] Iteration 4875, loss = 1.44987
I0520 14:26:38.474251 24538 solver.cpp:253]     Train net output #0: loss = 1.44987 (* 1 = 1.44987 loss)
I0520 14:26:38.474268 24538 sgd_solver.cpp:106] Iteration 4875, lr = 0.0025
I0520 14:26:47.030638 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_5000.caffemodel
I0520 14:26:47.119415 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_5000.solverstate
I0520 14:26:47.147994 24538 solver.cpp:341] Iteration 5000, Testing net (#0)
I0520 14:27:54.507068 24538 solver.cpp:409]     Test net output #0: accuracy = 0.781334
I0520 14:27:54.507236 24538 solver.cpp:409]     Test net output #1: loss = 0.765431 (* 1 = 0.765431 loss)
I0520 14:28:16.715566 24538 solver.cpp:237] Iteration 5000, loss = 1.35611
I0520 14:28:16.715620 24538 solver.cpp:253]     Train net output #0: loss = 1.35611 (* 1 = 1.35611 loss)
I0520 14:28:16.715638 24538 sgd_solver.cpp:106] Iteration 5000, lr = 0.0025
I0520 14:28:25.328044 24538 solver.cpp:237] Iteration 5125, loss = 1.38696
I0520 14:28:25.328197 24538 solver.cpp:253]     Train net output #0: loss = 1.38696 (* 1 = 1.38696 loss)
I0520 14:28:25.328212 24538 sgd_solver.cpp:106] Iteration 5125, lr = 0.0025
I0520 14:28:33.936494 24538 solver.cpp:237] Iteration 5250, loss = 1.60337
I0520 14:28:33.936528 24538 solver.cpp:253]     Train net output #0: loss = 1.60337 (* 1 = 1.60337 loss)
I0520 14:28:33.936545 24538 sgd_solver.cpp:106] Iteration 5250, lr = 0.0025
I0520 14:28:42.552762 24538 solver.cpp:237] Iteration 5375, loss = 1.58308
I0520 14:28:42.552799 24538 solver.cpp:253]     Train net output #0: loss = 1.58308 (* 1 = 1.58308 loss)
I0520 14:28:42.552820 24538 sgd_solver.cpp:106] Iteration 5375, lr = 0.0025
I0520 14:28:51.158535 24538 solver.cpp:237] Iteration 5500, loss = 1.58178
I0520 14:28:51.158570 24538 solver.cpp:253]     Train net output #0: loss = 1.58178 (* 1 = 1.58178 loss)
I0520 14:28:51.158586 24538 sgd_solver.cpp:106] Iteration 5500, lr = 0.0025
I0520 14:28:59.774747 24538 solver.cpp:237] Iteration 5625, loss = 1.42747
I0520 14:28:59.774886 24538 solver.cpp:253]     Train net output #0: loss = 1.42747 (* 1 = 1.42747 loss)
I0520 14:28:59.774900 24538 sgd_solver.cpp:106] Iteration 5625, lr = 0.0025
I0520 14:29:08.386242 24538 solver.cpp:237] Iteration 5750, loss = 1.44867
I0520 14:29:08.386288 24538 solver.cpp:253]     Train net output #0: loss = 1.44867 (* 1 = 1.44867 loss)
I0520 14:29:08.386303 24538 sgd_solver.cpp:106] Iteration 5750, lr = 0.0025
I0520 14:29:39.181056 24538 solver.cpp:237] Iteration 5875, loss = 1.38731
I0520 14:29:39.181218 24538 solver.cpp:253]     Train net output #0: loss = 1.38731 (* 1 = 1.38731 loss)
I0520 14:29:39.181236 24538 sgd_solver.cpp:106] Iteration 5875, lr = 0.0025
I0520 14:29:47.803032 24538 solver.cpp:237] Iteration 6000, loss = 1.41499
I0520 14:29:47.803067 24538 solver.cpp:253]     Train net output #0: loss = 1.41499 (* 1 = 1.41499 loss)
I0520 14:29:47.803082 24538 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0520 14:29:56.439021 24538 solver.cpp:237] Iteration 6125, loss = 1.49191
I0520 14:29:56.439067 24538 solver.cpp:253]     Train net output #0: loss = 1.49191 (* 1 = 1.49191 loss)
I0520 14:29:56.439083 24538 sgd_solver.cpp:106] Iteration 6125, lr = 0.0025
I0520 14:30:04.990442 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_6250.caffemodel
I0520 14:30:05.078691 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_6250.solverstate
I0520 14:30:05.127921 24538 solver.cpp:237] Iteration 6250, loss = 1.36213
I0520 14:30:05.127972 24538 solver.cpp:253]     Train net output #0: loss = 1.36213 (* 1 = 1.36213 loss)
I0520 14:30:05.127986 24538 sgd_solver.cpp:106] Iteration 6250, lr = 0.0025
I0520 14:30:13.743499 24538 solver.cpp:237] Iteration 6375, loss = 1.3315
I0520 14:30:13.743646 24538 solver.cpp:253]     Train net output #0: loss = 1.3315 (* 1 = 1.3315 loss)
I0520 14:30:13.743660 24538 sgd_solver.cpp:106] Iteration 6375, lr = 0.0025
I0520 14:30:22.353052 24538 solver.cpp:237] Iteration 6500, loss = 1.53284
I0520 14:30:22.353101 24538 solver.cpp:253]     Train net output #0: loss = 1.53284 (* 1 = 1.53284 loss)
I0520 14:30:22.353116 24538 sgd_solver.cpp:106] Iteration 6500, lr = 0.0025
I0520 14:30:30.963471 24538 solver.cpp:237] Iteration 6625, loss = 1.58158
I0520 14:30:30.963506 24538 solver.cpp:253]     Train net output #0: loss = 1.58158 (* 1 = 1.58158 loss)
I0520 14:30:30.963521 24538 sgd_solver.cpp:106] Iteration 6625, lr = 0.0025
I0520 14:31:01.755292 24538 solver.cpp:237] Iteration 6750, loss = 1.32153
I0520 14:31:01.755460 24538 solver.cpp:253]     Train net output #0: loss = 1.32153 (* 1 = 1.32153 loss)
I0520 14:31:01.755475 24538 sgd_solver.cpp:106] Iteration 6750, lr = 0.0025
I0520 14:31:10.369981 24538 solver.cpp:237] Iteration 6875, loss = 1.46645
I0520 14:31:10.370026 24538 solver.cpp:253]     Train net output #0: loss = 1.46645 (* 1 = 1.46645 loss)
I0520 14:31:10.370041 24538 sgd_solver.cpp:106] Iteration 6875, lr = 0.0025
I0520 14:31:18.977877 24538 solver.cpp:237] Iteration 7000, loss = 1.47134
I0520 14:31:18.977913 24538 solver.cpp:253]     Train net output #0: loss = 1.47134 (* 1 = 1.47134 loss)
I0520 14:31:18.977927 24538 sgd_solver.cpp:106] Iteration 7000, lr = 0.0025
I0520 14:31:27.592735 24538 solver.cpp:237] Iteration 7125, loss = 1.46974
I0520 14:31:27.592772 24538 solver.cpp:253]     Train net output #0: loss = 1.46974 (* 1 = 1.46974 loss)
I0520 14:31:27.592785 24538 sgd_solver.cpp:106] Iteration 7125, lr = 0.0025
I0520 14:31:36.210767 24538 solver.cpp:237] Iteration 7250, loss = 1.50367
I0520 14:31:36.210917 24538 solver.cpp:253]     Train net output #0: loss = 1.50367 (* 1 = 1.50367 loss)
I0520 14:31:36.210930 24538 sgd_solver.cpp:106] Iteration 7250, lr = 0.0025
I0520 14:31:44.826047 24538 solver.cpp:237] Iteration 7375, loss = 1.06655
I0520 14:31:44.826081 24538 solver.cpp:253]     Train net output #0: loss = 1.06655 (* 1 = 1.06655 loss)
I0520 14:31:44.826097 24538 sgd_solver.cpp:106] Iteration 7375, lr = 0.0025
I0520 14:31:53.371660 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_7500.caffemodel
I0520 14:31:53.458457 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_7500.solverstate
I0520 14:31:53.485185 24538 solver.cpp:341] Iteration 7500, Testing net (#0)
I0520 14:32:39.633329 24538 solver.cpp:409]     Test net output #0: accuracy = 0.816267
I0520 14:32:39.633489 24538 solver.cpp:409]     Test net output #1: loss = 0.639191 (* 1 = 0.639191 loss)
I0520 14:33:01.818733 24538 solver.cpp:237] Iteration 7500, loss = 1.35444
I0520 14:33:01.818788 24538 solver.cpp:253]     Train net output #0: loss = 1.35444 (* 1 = 1.35444 loss)
I0520 14:33:01.818802 24538 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0520 14:33:10.440387 24538 solver.cpp:237] Iteration 7625, loss = 1.23647
I0520 14:33:10.440539 24538 solver.cpp:253]     Train net output #0: loss = 1.23647 (* 1 = 1.23647 loss)
I0520 14:33:10.440552 24538 sgd_solver.cpp:106] Iteration 7625, lr = 0.0025
I0520 14:33:19.059933 24538 solver.cpp:237] Iteration 7750, loss = 1.34619
I0520 14:33:19.059981 24538 solver.cpp:253]     Train net output #0: loss = 1.34619 (* 1 = 1.34619 loss)
I0520 14:33:19.059996 24538 sgd_solver.cpp:106] Iteration 7750, lr = 0.0025
I0520 14:33:27.688663 24538 solver.cpp:237] Iteration 7875, loss = 1.4988
I0520 14:33:27.688699 24538 solver.cpp:253]     Train net output #0: loss = 1.4988 (* 1 = 1.4988 loss)
I0520 14:33:27.688714 24538 sgd_solver.cpp:106] Iteration 7875, lr = 0.0025
I0520 14:33:36.312778 24538 solver.cpp:237] Iteration 8000, loss = 1.37715
I0520 14:33:36.312814 24538 solver.cpp:253]     Train net output #0: loss = 1.37715 (* 1 = 1.37715 loss)
I0520 14:33:36.312829 24538 sgd_solver.cpp:106] Iteration 8000, lr = 0.0025
I0520 14:33:44.940434 24538 solver.cpp:237] Iteration 8125, loss = 1.45769
I0520 14:33:44.940598 24538 solver.cpp:253]     Train net output #0: loss = 1.45769 (* 1 = 1.45769 loss)
I0520 14:33:44.940613 24538 sgd_solver.cpp:106] Iteration 8125, lr = 0.0025
I0520 14:33:53.567116 24538 solver.cpp:237] Iteration 8250, loss = 1.48112
I0520 14:33:53.567150 24538 solver.cpp:253]     Train net output #0: loss = 1.48112 (* 1 = 1.48112 loss)
I0520 14:33:53.567165 24538 sgd_solver.cpp:106] Iteration 8250, lr = 0.0025
I0520 14:34:24.431221 24538 solver.cpp:237] Iteration 8375, loss = 1.3529
I0520 14:34:24.431401 24538 solver.cpp:253]     Train net output #0: loss = 1.3529 (* 1 = 1.3529 loss)
I0520 14:34:24.431417 24538 sgd_solver.cpp:106] Iteration 8375, lr = 0.0025
I0520 14:34:33.047480 24538 solver.cpp:237] Iteration 8500, loss = 1.28831
I0520 14:34:33.047519 24538 solver.cpp:253]     Train net output #0: loss = 1.28831 (* 1 = 1.28831 loss)
I0520 14:34:33.047536 24538 sgd_solver.cpp:106] Iteration 8500, lr = 0.0025
I0520 14:34:41.672068 24538 solver.cpp:237] Iteration 8625, loss = 1.24397
I0520 14:34:41.672103 24538 solver.cpp:253]     Train net output #0: loss = 1.24397 (* 1 = 1.24397 loss)
I0520 14:34:41.672116 24538 sgd_solver.cpp:106] Iteration 8625, lr = 0.0025
I0520 14:34:50.223950 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_8750.caffemodel
I0520 14:34:50.309612 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_8750.solverstate
I0520 14:34:50.356822 24538 solver.cpp:237] Iteration 8750, loss = 1.53407
I0520 14:34:50.356871 24538 solver.cpp:253]     Train net output #0: loss = 1.53407 (* 1 = 1.53407 loss)
I0520 14:34:50.356885 24538 sgd_solver.cpp:106] Iteration 8750, lr = 0.0025
I0520 14:34:58.978334 24538 solver.cpp:237] Iteration 8875, loss = 1.33935
I0520 14:34:58.978492 24538 solver.cpp:253]     Train net output #0: loss = 1.33935 (* 1 = 1.33935 loss)
I0520 14:34:58.978507 24538 sgd_solver.cpp:106] Iteration 8875, lr = 0.0025
I0520 14:35:07.609771 24538 solver.cpp:237] Iteration 9000, loss = 1.5655
I0520 14:35:07.609805 24538 solver.cpp:253]     Train net output #0: loss = 1.5655 (* 1 = 1.5655 loss)
I0520 14:35:07.609820 24538 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0520 14:35:16.235671 24538 solver.cpp:237] Iteration 9125, loss = 1.33412
I0520 14:35:16.235707 24538 solver.cpp:253]     Train net output #0: loss = 1.33412 (* 1 = 1.33412 loss)
I0520 14:35:16.235720 24538 sgd_solver.cpp:106] Iteration 9125, lr = 0.0025
I0520 14:35:47.081959 24538 solver.cpp:237] Iteration 9250, loss = 1.08215
I0520 14:35:47.082129 24538 solver.cpp:253]     Train net output #0: loss = 1.08215 (* 1 = 1.08215 loss)
I0520 14:35:47.082142 24538 sgd_solver.cpp:106] Iteration 9250, lr = 0.0025
I0520 14:35:55.710446 24538 solver.cpp:237] Iteration 9375, loss = 1.35242
I0520 14:35:55.710481 24538 solver.cpp:253]     Train net output #0: loss = 1.35242 (* 1 = 1.35242 loss)
I0520 14:35:55.710496 24538 sgd_solver.cpp:106] Iteration 9375, lr = 0.0025
I0520 14:36:04.333313 24538 solver.cpp:237] Iteration 9500, loss = 1.28505
I0520 14:36:04.333350 24538 solver.cpp:253]     Train net output #0: loss = 1.28505 (* 1 = 1.28505 loss)
I0520 14:36:04.333364 24538 sgd_solver.cpp:106] Iteration 9500, lr = 0.0025
I0520 14:36:12.952504 24538 solver.cpp:237] Iteration 9625, loss = 1.22359
I0520 14:36:12.952546 24538 solver.cpp:253]     Train net output #0: loss = 1.22359 (* 1 = 1.22359 loss)
I0520 14:36:12.952561 24538 sgd_solver.cpp:106] Iteration 9625, lr = 0.0025
I0520 14:36:21.574659 24538 solver.cpp:237] Iteration 9750, loss = 1.42362
I0520 14:36:21.574800 24538 solver.cpp:253]     Train net output #0: loss = 1.42362 (* 1 = 1.42362 loss)
I0520 14:36:21.574813 24538 sgd_solver.cpp:106] Iteration 9750, lr = 0.0025
I0520 14:36:30.201251 24538 solver.cpp:237] Iteration 9875, loss = 1.45691
I0520 14:36:30.201284 24538 solver.cpp:253]     Train net output #0: loss = 1.45691 (* 1 = 1.45691 loss)
I0520 14:36:30.201300 24538 sgd_solver.cpp:106] Iteration 9875, lr = 0.0025
I0520 14:36:38.754726 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_10000.caffemodel
I0520 14:36:38.841820 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_10000.solverstate
I0520 14:36:38.867008 24538 solver.cpp:341] Iteration 10000, Testing net (#0)
I0520 14:37:46.240721 24538 solver.cpp:409]     Test net output #0: accuracy = 0.8334
I0520 14:37:46.240905 24538 solver.cpp:409]     Test net output #1: loss = 0.597608 (* 1 = 0.597608 loss)
I0520 14:38:08.507187 24538 solver.cpp:237] Iteration 10000, loss = 1.20951
I0520 14:38:08.507241 24538 solver.cpp:253]     Train net output #0: loss = 1.20951 (* 1 = 1.20951 loss)
I0520 14:38:08.507256 24538 sgd_solver.cpp:106] Iteration 10000, lr = 0.0025
I0520 14:38:17.134308 24538 solver.cpp:237] Iteration 10125, loss = 1.32957
I0520 14:38:17.134474 24538 solver.cpp:253]     Train net output #0: loss = 1.32957 (* 1 = 1.32957 loss)
I0520 14:38:17.134487 24538 sgd_solver.cpp:106] Iteration 10125, lr = 0.0025
I0520 14:38:25.766577 24538 solver.cpp:237] Iteration 10250, loss = 1.26004
I0520 14:38:25.766612 24538 solver.cpp:253]     Train net output #0: loss = 1.26004 (* 1 = 1.26004 loss)
I0520 14:38:25.766626 24538 sgd_solver.cpp:106] Iteration 10250, lr = 0.0025
I0520 14:38:34.396967 24538 solver.cpp:237] Iteration 10375, loss = 1.354
I0520 14:38:34.397001 24538 solver.cpp:253]     Train net output #0: loss = 1.354 (* 1 = 1.354 loss)
I0520 14:38:34.397017 24538 sgd_solver.cpp:106] Iteration 10375, lr = 0.0025
I0520 14:38:43.025730 24538 solver.cpp:237] Iteration 10500, loss = 1.38551
I0520 14:38:43.025773 24538 solver.cpp:253]     Train net output #0: loss = 1.38551 (* 1 = 1.38551 loss)
I0520 14:38:43.025789 24538 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0520 14:38:51.659230 24538 solver.cpp:237] Iteration 10625, loss = 1.33098
I0520 14:38:51.659374 24538 solver.cpp:253]     Train net output #0: loss = 1.33098 (* 1 = 1.33098 loss)
I0520 14:38:51.659387 24538 sgd_solver.cpp:106] Iteration 10625, lr = 0.0025
I0520 14:39:00.291508 24538 solver.cpp:237] Iteration 10750, loss = 1.15269
I0520 14:39:00.291543 24538 solver.cpp:253]     Train net output #0: loss = 1.15269 (* 1 = 1.15269 loss)
I0520 14:39:00.291558 24538 sgd_solver.cpp:106] Iteration 10750, lr = 0.0025
I0520 14:39:31.141754 24538 solver.cpp:237] Iteration 10875, loss = 1.47738
I0520 14:39:31.141924 24538 solver.cpp:253]     Train net output #0: loss = 1.47738 (* 1 = 1.47738 loss)
I0520 14:39:31.141942 24538 sgd_solver.cpp:106] Iteration 10875, lr = 0.0025
I0520 14:39:39.767962 24538 solver.cpp:237] Iteration 11000, loss = 1.22955
I0520 14:39:39.767997 24538 solver.cpp:253]     Train net output #0: loss = 1.22955 (* 1 = 1.22955 loss)
I0520 14:39:39.768012 24538 sgd_solver.cpp:106] Iteration 11000, lr = 0.0025
I0520 14:39:48.391696 24538 solver.cpp:237] Iteration 11125, loss = 1.28766
I0520 14:39:48.391733 24538 solver.cpp:253]     Train net output #0: loss = 1.28766 (* 1 = 1.28766 loss)
I0520 14:39:48.391748 24538 sgd_solver.cpp:106] Iteration 11125, lr = 0.0025
I0520 14:39:56.957221 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_11250.caffemodel
I0520 14:39:57.045349 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_11250.solverstate
I0520 14:39:57.100081 24538 solver.cpp:237] Iteration 11250, loss = 1.25364
I0520 14:39:57.100132 24538 solver.cpp:253]     Train net output #0: loss = 1.25364 (* 1 = 1.25364 loss)
I0520 14:39:57.100147 24538 sgd_solver.cpp:106] Iteration 11250, lr = 0.0025
I0520 14:40:05.727794 24538 solver.cpp:237] Iteration 11375, loss = 1.28809
I0520 14:40:05.727946 24538 solver.cpp:253]     Train net output #0: loss = 1.28809 (* 1 = 1.28809 loss)
I0520 14:40:05.727959 24538 sgd_solver.cpp:106] Iteration 11375, lr = 0.0025
I0520 14:40:14.359729 24538 solver.cpp:237] Iteration 11500, loss = 1.43601
I0520 14:40:14.359762 24538 solver.cpp:253]     Train net output #0: loss = 1.43601 (* 1 = 1.43601 loss)
I0520 14:40:14.359778 24538 sgd_solver.cpp:106] Iteration 11500, lr = 0.0025
I0520 14:40:22.996505 24538 solver.cpp:237] Iteration 11625, loss = 1.39659
I0520 14:40:22.996546 24538 solver.cpp:253]     Train net output #0: loss = 1.39659 (* 1 = 1.39659 loss)
I0520 14:40:22.996564 24538 sgd_solver.cpp:106] Iteration 11625, lr = 0.0025
I0520 14:40:53.855882 24538 solver.cpp:237] Iteration 11750, loss = 1.10503
I0520 14:40:53.856063 24538 solver.cpp:253]     Train net output #0: loss = 1.10503 (* 1 = 1.10503 loss)
I0520 14:40:53.856081 24538 sgd_solver.cpp:106] Iteration 11750, lr = 0.0025
I0520 14:41:02.482734 24538 solver.cpp:237] Iteration 11875, loss = 1.38461
I0520 14:41:02.482769 24538 solver.cpp:253]     Train net output #0: loss = 1.38461 (* 1 = 1.38461 loss)
I0520 14:41:02.482784 24538 sgd_solver.cpp:106] Iteration 11875, lr = 0.0025
I0520 14:41:11.116852 24538 solver.cpp:237] Iteration 12000, loss = 1.1868
I0520 14:41:11.116905 24538 solver.cpp:253]     Train net output #0: loss = 1.1868 (* 1 = 1.1868 loss)
I0520 14:41:11.116919 24538 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0520 14:41:19.750455 24538 solver.cpp:237] Iteration 12125, loss = 1.33688
I0520 14:41:19.750490 24538 solver.cpp:253]     Train net output #0: loss = 1.33688 (* 1 = 1.33688 loss)
I0520 14:41:19.750505 24538 sgd_solver.cpp:106] Iteration 12125, lr = 0.0025
I0520 14:41:28.379976 24538 solver.cpp:237] Iteration 12250, loss = 1.45137
I0520 14:41:28.380122 24538 solver.cpp:253]     Train net output #0: loss = 1.45137 (* 1 = 1.45137 loss)
I0520 14:41:28.380136 24538 sgd_solver.cpp:106] Iteration 12250, lr = 0.0025
I0520 14:41:37.011828 24538 solver.cpp:237] Iteration 12375, loss = 1.46522
I0520 14:41:37.011867 24538 solver.cpp:253]     Train net output #0: loss = 1.46522 (* 1 = 1.46522 loss)
I0520 14:41:37.011880 24538 sgd_solver.cpp:106] Iteration 12375, lr = 0.0025
I0520 14:41:45.571408 24538 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_12500.caffemodel
I0520 14:41:45.660373 24538 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866_iter_12500.solverstate
I0520 14:42:06.683248 24538 solver.cpp:321] Iteration 12500, loss = 1.26782
I0520 14:42:06.683418 24538 solver.cpp:341] Iteration 12500, Testing net (#0)
I0520 14:42:53.134749 24538 solver.cpp:409]     Test net output #0: accuracy = 0.843392
I0520 14:42:53.134914 24538 solver.cpp:409]     Test net output #1: loss = 0.51611 (* 1 = 0.51611 loss)
I0520 14:42:53.134928 24538 solver.cpp:326] Optimization Done.
I0520 14:42:53.134938 24538 caffe.cpp:215] Optimization Done.
Application 11232495 resources: utime ~1327s, stime ~234s, Rss ~5329632, inblocks ~3744348, outblocks ~179816
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_120_2016-05-20T11.20.37.036866.solver"
	User time (seconds): 0.56
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 26:05.35
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15080
	Voluntary context switches: 2774
	Involuntary context switches: 80
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
