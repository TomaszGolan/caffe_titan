2806092
I0521 00:02:04.215019  9747 caffe.cpp:184] Using GPUs 0
I0521 00:02:04.684275  9747 solver.cpp:48] Initializing solver from parameters: 
test_iter: 333
test_interval: 666
base_lr: 0.0025
display: 33
max_iter: 3333
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 333
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868.prototxt"
I0521 00:02:04.686056  9747 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868.prototxt
I0521 00:02:04.699350  9747 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 00:02:04.699410  9747 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 00:02:04.699755  9747 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 450
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:02:04.699934  9747 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:02:04.699959  9747 net.cpp:106] Creating Layer data_hdf5
I0521 00:02:04.699973  9747 net.cpp:411] data_hdf5 -> data
I0521 00:02:04.700006  9747 net.cpp:411] data_hdf5 -> label
I0521 00:02:04.700038  9747 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 00:02:04.701261  9747 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 00:02:04.703445  9747 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 00:02:26.257730  9747 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 00:02:26.262832  9747 net.cpp:150] Setting up data_hdf5
I0521 00:02:26.262873  9747 net.cpp:157] Top shape: 450 1 127 50 (2857500)
I0521 00:02:26.262888  9747 net.cpp:157] Top shape: 450 (450)
I0521 00:02:26.262900  9747 net.cpp:165] Memory required for data: 11431800
I0521 00:02:26.262914  9747 layer_factory.hpp:77] Creating layer conv1
I0521 00:02:26.262948  9747 net.cpp:106] Creating Layer conv1
I0521 00:02:26.262959  9747 net.cpp:454] conv1 <- data
I0521 00:02:26.262982  9747 net.cpp:411] conv1 -> conv1
I0521 00:02:26.628368  9747 net.cpp:150] Setting up conv1
I0521 00:02:26.628413  9747 net.cpp:157] Top shape: 450 12 120 48 (31104000)
I0521 00:02:26.628424  9747 net.cpp:165] Memory required for data: 135847800
I0521 00:02:26.628456  9747 layer_factory.hpp:77] Creating layer relu1
I0521 00:02:26.628479  9747 net.cpp:106] Creating Layer relu1
I0521 00:02:26.628489  9747 net.cpp:454] relu1 <- conv1
I0521 00:02:26.628502  9747 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:02:26.629030  9747 net.cpp:150] Setting up relu1
I0521 00:02:26.629047  9747 net.cpp:157] Top shape: 450 12 120 48 (31104000)
I0521 00:02:26.629058  9747 net.cpp:165] Memory required for data: 260263800
I0521 00:02:26.629068  9747 layer_factory.hpp:77] Creating layer pool1
I0521 00:02:26.629086  9747 net.cpp:106] Creating Layer pool1
I0521 00:02:26.629096  9747 net.cpp:454] pool1 <- conv1
I0521 00:02:26.629109  9747 net.cpp:411] pool1 -> pool1
I0521 00:02:26.629190  9747 net.cpp:150] Setting up pool1
I0521 00:02:26.629204  9747 net.cpp:157] Top shape: 450 12 60 48 (15552000)
I0521 00:02:26.629215  9747 net.cpp:165] Memory required for data: 322471800
I0521 00:02:26.629226  9747 layer_factory.hpp:77] Creating layer conv2
I0521 00:02:26.629248  9747 net.cpp:106] Creating Layer conv2
I0521 00:02:26.629258  9747 net.cpp:454] conv2 <- pool1
I0521 00:02:26.629271  9747 net.cpp:411] conv2 -> conv2
I0521 00:02:26.631947  9747 net.cpp:150] Setting up conv2
I0521 00:02:26.631974  9747 net.cpp:157] Top shape: 450 20 54 46 (22356000)
I0521 00:02:26.631985  9747 net.cpp:165] Memory required for data: 411895800
I0521 00:02:26.632005  9747 layer_factory.hpp:77] Creating layer relu2
I0521 00:02:26.632020  9747 net.cpp:106] Creating Layer relu2
I0521 00:02:26.632030  9747 net.cpp:454] relu2 <- conv2
I0521 00:02:26.632042  9747 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:02:26.632372  9747 net.cpp:150] Setting up relu2
I0521 00:02:26.632387  9747 net.cpp:157] Top shape: 450 20 54 46 (22356000)
I0521 00:02:26.632398  9747 net.cpp:165] Memory required for data: 501319800
I0521 00:02:26.632408  9747 layer_factory.hpp:77] Creating layer pool2
I0521 00:02:26.632421  9747 net.cpp:106] Creating Layer pool2
I0521 00:02:26.632431  9747 net.cpp:454] pool2 <- conv2
I0521 00:02:26.632457  9747 net.cpp:411] pool2 -> pool2
I0521 00:02:26.632536  9747 net.cpp:150] Setting up pool2
I0521 00:02:26.632550  9747 net.cpp:157] Top shape: 450 20 27 46 (11178000)
I0521 00:02:26.632560  9747 net.cpp:165] Memory required for data: 546031800
I0521 00:02:26.632570  9747 layer_factory.hpp:77] Creating layer conv3
I0521 00:02:26.632589  9747 net.cpp:106] Creating Layer conv3
I0521 00:02:26.632601  9747 net.cpp:454] conv3 <- pool2
I0521 00:02:26.632614  9747 net.cpp:411] conv3 -> conv3
I0521 00:02:26.634533  9747 net.cpp:150] Setting up conv3
I0521 00:02:26.634552  9747 net.cpp:157] Top shape: 450 28 22 44 (12196800)
I0521 00:02:26.634563  9747 net.cpp:165] Memory required for data: 594819000
I0521 00:02:26.634582  9747 layer_factory.hpp:77] Creating layer relu3
I0521 00:02:26.634598  9747 net.cpp:106] Creating Layer relu3
I0521 00:02:26.634608  9747 net.cpp:454] relu3 <- conv3
I0521 00:02:26.634619  9747 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:02:26.635092  9747 net.cpp:150] Setting up relu3
I0521 00:02:26.635110  9747 net.cpp:157] Top shape: 450 28 22 44 (12196800)
I0521 00:02:26.635120  9747 net.cpp:165] Memory required for data: 643606200
I0521 00:02:26.635131  9747 layer_factory.hpp:77] Creating layer pool3
I0521 00:02:26.635144  9747 net.cpp:106] Creating Layer pool3
I0521 00:02:26.635154  9747 net.cpp:454] pool3 <- conv3
I0521 00:02:26.635166  9747 net.cpp:411] pool3 -> pool3
I0521 00:02:26.635234  9747 net.cpp:150] Setting up pool3
I0521 00:02:26.635247  9747 net.cpp:157] Top shape: 450 28 11 44 (6098400)
I0521 00:02:26.635257  9747 net.cpp:165] Memory required for data: 667999800
I0521 00:02:26.635265  9747 layer_factory.hpp:77] Creating layer conv4
I0521 00:02:26.635282  9747 net.cpp:106] Creating Layer conv4
I0521 00:02:26.635293  9747 net.cpp:454] conv4 <- pool3
I0521 00:02:26.635306  9747 net.cpp:411] conv4 -> conv4
I0521 00:02:26.638082  9747 net.cpp:150] Setting up conv4
I0521 00:02:26.638110  9747 net.cpp:157] Top shape: 450 36 6 42 (4082400)
I0521 00:02:26.638121  9747 net.cpp:165] Memory required for data: 684329400
I0521 00:02:26.638139  9747 layer_factory.hpp:77] Creating layer relu4
I0521 00:02:26.638152  9747 net.cpp:106] Creating Layer relu4
I0521 00:02:26.638164  9747 net.cpp:454] relu4 <- conv4
I0521 00:02:26.638176  9747 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:02:26.638648  9747 net.cpp:150] Setting up relu4
I0521 00:02:26.638664  9747 net.cpp:157] Top shape: 450 36 6 42 (4082400)
I0521 00:02:26.638674  9747 net.cpp:165] Memory required for data: 700659000
I0521 00:02:26.638684  9747 layer_factory.hpp:77] Creating layer pool4
I0521 00:02:26.638697  9747 net.cpp:106] Creating Layer pool4
I0521 00:02:26.638707  9747 net.cpp:454] pool4 <- conv4
I0521 00:02:26.638720  9747 net.cpp:411] pool4 -> pool4
I0521 00:02:26.638788  9747 net.cpp:150] Setting up pool4
I0521 00:02:26.638803  9747 net.cpp:157] Top shape: 450 36 3 42 (2041200)
I0521 00:02:26.638813  9747 net.cpp:165] Memory required for data: 708823800
I0521 00:02:26.638823  9747 layer_factory.hpp:77] Creating layer ip1
I0521 00:02:26.638844  9747 net.cpp:106] Creating Layer ip1
I0521 00:02:26.638854  9747 net.cpp:454] ip1 <- pool4
I0521 00:02:26.638867  9747 net.cpp:411] ip1 -> ip1
I0521 00:02:26.654289  9747 net.cpp:150] Setting up ip1
I0521 00:02:26.654316  9747 net.cpp:157] Top shape: 450 196 (88200)
I0521 00:02:26.654330  9747 net.cpp:165] Memory required for data: 709176600
I0521 00:02:26.654353  9747 layer_factory.hpp:77] Creating layer relu5
I0521 00:02:26.654368  9747 net.cpp:106] Creating Layer relu5
I0521 00:02:26.654378  9747 net.cpp:454] relu5 <- ip1
I0521 00:02:26.654392  9747 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:02:26.654733  9747 net.cpp:150] Setting up relu5
I0521 00:02:26.654747  9747 net.cpp:157] Top shape: 450 196 (88200)
I0521 00:02:26.654758  9747 net.cpp:165] Memory required for data: 709529400
I0521 00:02:26.654769  9747 layer_factory.hpp:77] Creating layer drop1
I0521 00:02:26.654790  9747 net.cpp:106] Creating Layer drop1
I0521 00:02:26.654800  9747 net.cpp:454] drop1 <- ip1
I0521 00:02:26.654825  9747 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:02:26.654872  9747 net.cpp:150] Setting up drop1
I0521 00:02:26.654886  9747 net.cpp:157] Top shape: 450 196 (88200)
I0521 00:02:26.654896  9747 net.cpp:165] Memory required for data: 709882200
I0521 00:02:26.654906  9747 layer_factory.hpp:77] Creating layer ip2
I0521 00:02:26.654924  9747 net.cpp:106] Creating Layer ip2
I0521 00:02:26.654934  9747 net.cpp:454] ip2 <- ip1
I0521 00:02:26.654947  9747 net.cpp:411] ip2 -> ip2
I0521 00:02:26.655416  9747 net.cpp:150] Setting up ip2
I0521 00:02:26.655428  9747 net.cpp:157] Top shape: 450 98 (44100)
I0521 00:02:26.655438  9747 net.cpp:165] Memory required for data: 710058600
I0521 00:02:26.655453  9747 layer_factory.hpp:77] Creating layer relu6
I0521 00:02:26.655467  9747 net.cpp:106] Creating Layer relu6
I0521 00:02:26.655475  9747 net.cpp:454] relu6 <- ip2
I0521 00:02:26.655488  9747 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:02:26.656008  9747 net.cpp:150] Setting up relu6
I0521 00:02:26.656024  9747 net.cpp:157] Top shape: 450 98 (44100)
I0521 00:02:26.656034  9747 net.cpp:165] Memory required for data: 710235000
I0521 00:02:26.656045  9747 layer_factory.hpp:77] Creating layer drop2
I0521 00:02:26.656059  9747 net.cpp:106] Creating Layer drop2
I0521 00:02:26.656069  9747 net.cpp:454] drop2 <- ip2
I0521 00:02:26.656080  9747 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:02:26.656122  9747 net.cpp:150] Setting up drop2
I0521 00:02:26.656136  9747 net.cpp:157] Top shape: 450 98 (44100)
I0521 00:02:26.656147  9747 net.cpp:165] Memory required for data: 710411400
I0521 00:02:26.656157  9747 layer_factory.hpp:77] Creating layer ip3
I0521 00:02:26.656169  9747 net.cpp:106] Creating Layer ip3
I0521 00:02:26.656180  9747 net.cpp:454] ip3 <- ip2
I0521 00:02:26.656193  9747 net.cpp:411] ip3 -> ip3
I0521 00:02:26.656404  9747 net.cpp:150] Setting up ip3
I0521 00:02:26.656417  9747 net.cpp:157] Top shape: 450 11 (4950)
I0521 00:02:26.656427  9747 net.cpp:165] Memory required for data: 710431200
I0521 00:02:26.656442  9747 layer_factory.hpp:77] Creating layer drop3
I0521 00:02:26.656455  9747 net.cpp:106] Creating Layer drop3
I0521 00:02:26.656464  9747 net.cpp:454] drop3 <- ip3
I0521 00:02:26.656476  9747 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:02:26.656522  9747 net.cpp:150] Setting up drop3
I0521 00:02:26.656535  9747 net.cpp:157] Top shape: 450 11 (4950)
I0521 00:02:26.656545  9747 net.cpp:165] Memory required for data: 710451000
I0521 00:02:26.656555  9747 layer_factory.hpp:77] Creating layer loss
I0521 00:02:26.656574  9747 net.cpp:106] Creating Layer loss
I0521 00:02:26.656584  9747 net.cpp:454] loss <- ip3
I0521 00:02:26.656595  9747 net.cpp:454] loss <- label
I0521 00:02:26.656607  9747 net.cpp:411] loss -> loss
I0521 00:02:26.656625  9747 layer_factory.hpp:77] Creating layer loss
I0521 00:02:26.657272  9747 net.cpp:150] Setting up loss
I0521 00:02:26.657294  9747 net.cpp:157] Top shape: (1)
I0521 00:02:26.657306  9747 net.cpp:160]     with loss weight 1
I0521 00:02:26.657348  9747 net.cpp:165] Memory required for data: 710451004
I0521 00:02:26.657359  9747 net.cpp:226] loss needs backward computation.
I0521 00:02:26.657371  9747 net.cpp:226] drop3 needs backward computation.
I0521 00:02:26.657378  9747 net.cpp:226] ip3 needs backward computation.
I0521 00:02:26.657389  9747 net.cpp:226] drop2 needs backward computation.
I0521 00:02:26.657398  9747 net.cpp:226] relu6 needs backward computation.
I0521 00:02:26.657408  9747 net.cpp:226] ip2 needs backward computation.
I0521 00:02:26.657419  9747 net.cpp:226] drop1 needs backward computation.
I0521 00:02:26.657428  9747 net.cpp:226] relu5 needs backward computation.
I0521 00:02:26.657438  9747 net.cpp:226] ip1 needs backward computation.
I0521 00:02:26.657449  9747 net.cpp:226] pool4 needs backward computation.
I0521 00:02:26.657459  9747 net.cpp:226] relu4 needs backward computation.
I0521 00:02:26.657469  9747 net.cpp:226] conv4 needs backward computation.
I0521 00:02:26.657480  9747 net.cpp:226] pool3 needs backward computation.
I0521 00:02:26.657498  9747 net.cpp:226] relu3 needs backward computation.
I0521 00:02:26.657506  9747 net.cpp:226] conv3 needs backward computation.
I0521 00:02:26.657518  9747 net.cpp:226] pool2 needs backward computation.
I0521 00:02:26.657528  9747 net.cpp:226] relu2 needs backward computation.
I0521 00:02:26.657539  9747 net.cpp:226] conv2 needs backward computation.
I0521 00:02:26.657548  9747 net.cpp:226] pool1 needs backward computation.
I0521 00:02:26.657559  9747 net.cpp:226] relu1 needs backward computation.
I0521 00:02:26.657569  9747 net.cpp:226] conv1 needs backward computation.
I0521 00:02:26.657580  9747 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:02:26.657590  9747 net.cpp:270] This network produces output loss
I0521 00:02:26.657614  9747 net.cpp:283] Network initialization done.
I0521 00:02:26.659169  9747 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868.prototxt
I0521 00:02:26.659241  9747 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 00:02:26.659600  9747 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 450
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:02:26.659788  9747 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:02:26.659804  9747 net.cpp:106] Creating Layer data_hdf5
I0521 00:02:26.659816  9747 net.cpp:411] data_hdf5 -> data
I0521 00:02:26.659832  9747 net.cpp:411] data_hdf5 -> label
I0521 00:02:26.659848  9747 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 00:02:26.661129  9747 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 00:02:48.014529  9747 net.cpp:150] Setting up data_hdf5
I0521 00:02:48.014693  9747 net.cpp:157] Top shape: 450 1 127 50 (2857500)
I0521 00:02:48.014708  9747 net.cpp:157] Top shape: 450 (450)
I0521 00:02:48.014719  9747 net.cpp:165] Memory required for data: 11431800
I0521 00:02:48.014731  9747 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 00:02:48.014760  9747 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 00:02:48.014770  9747 net.cpp:454] label_data_hdf5_1_split <- label
I0521 00:02:48.014786  9747 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 00:02:48.014807  9747 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 00:02:48.014881  9747 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 00:02:48.014895  9747 net.cpp:157] Top shape: 450 (450)
I0521 00:02:48.014907  9747 net.cpp:157] Top shape: 450 (450)
I0521 00:02:48.014916  9747 net.cpp:165] Memory required for data: 11435400
I0521 00:02:48.014927  9747 layer_factory.hpp:77] Creating layer conv1
I0521 00:02:48.014950  9747 net.cpp:106] Creating Layer conv1
I0521 00:02:48.014960  9747 net.cpp:454] conv1 <- data
I0521 00:02:48.014974  9747 net.cpp:411] conv1 -> conv1
I0521 00:02:48.016939  9747 net.cpp:150] Setting up conv1
I0521 00:02:48.016963  9747 net.cpp:157] Top shape: 450 12 120 48 (31104000)
I0521 00:02:48.016975  9747 net.cpp:165] Memory required for data: 135851400
I0521 00:02:48.016998  9747 layer_factory.hpp:77] Creating layer relu1
I0521 00:02:48.017012  9747 net.cpp:106] Creating Layer relu1
I0521 00:02:48.017022  9747 net.cpp:454] relu1 <- conv1
I0521 00:02:48.017035  9747 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:02:48.017535  9747 net.cpp:150] Setting up relu1
I0521 00:02:48.017551  9747 net.cpp:157] Top shape: 450 12 120 48 (31104000)
I0521 00:02:48.017561  9747 net.cpp:165] Memory required for data: 260267400
I0521 00:02:48.017571  9747 layer_factory.hpp:77] Creating layer pool1
I0521 00:02:48.017588  9747 net.cpp:106] Creating Layer pool1
I0521 00:02:48.017598  9747 net.cpp:454] pool1 <- conv1
I0521 00:02:48.017611  9747 net.cpp:411] pool1 -> pool1
I0521 00:02:48.017686  9747 net.cpp:150] Setting up pool1
I0521 00:02:48.017700  9747 net.cpp:157] Top shape: 450 12 60 48 (15552000)
I0521 00:02:48.017709  9747 net.cpp:165] Memory required for data: 322475400
I0521 00:02:48.017720  9747 layer_factory.hpp:77] Creating layer conv2
I0521 00:02:48.017737  9747 net.cpp:106] Creating Layer conv2
I0521 00:02:48.017748  9747 net.cpp:454] conv2 <- pool1
I0521 00:02:48.017762  9747 net.cpp:411] conv2 -> conv2
I0521 00:02:48.019676  9747 net.cpp:150] Setting up conv2
I0521 00:02:48.019698  9747 net.cpp:157] Top shape: 450 20 54 46 (22356000)
I0521 00:02:48.019711  9747 net.cpp:165] Memory required for data: 411899400
I0521 00:02:48.019729  9747 layer_factory.hpp:77] Creating layer relu2
I0521 00:02:48.019743  9747 net.cpp:106] Creating Layer relu2
I0521 00:02:48.019753  9747 net.cpp:454] relu2 <- conv2
I0521 00:02:48.019765  9747 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:02:48.020097  9747 net.cpp:150] Setting up relu2
I0521 00:02:48.020110  9747 net.cpp:157] Top shape: 450 20 54 46 (22356000)
I0521 00:02:48.020120  9747 net.cpp:165] Memory required for data: 501323400
I0521 00:02:48.020130  9747 layer_factory.hpp:77] Creating layer pool2
I0521 00:02:48.020144  9747 net.cpp:106] Creating Layer pool2
I0521 00:02:48.020154  9747 net.cpp:454] pool2 <- conv2
I0521 00:02:48.020166  9747 net.cpp:411] pool2 -> pool2
I0521 00:02:48.020238  9747 net.cpp:150] Setting up pool2
I0521 00:02:48.020251  9747 net.cpp:157] Top shape: 450 20 27 46 (11178000)
I0521 00:02:48.020262  9747 net.cpp:165] Memory required for data: 546035400
I0521 00:02:48.020270  9747 layer_factory.hpp:77] Creating layer conv3
I0521 00:02:48.020290  9747 net.cpp:106] Creating Layer conv3
I0521 00:02:48.020300  9747 net.cpp:454] conv3 <- pool2
I0521 00:02:48.020314  9747 net.cpp:411] conv3 -> conv3
I0521 00:02:48.022292  9747 net.cpp:150] Setting up conv3
I0521 00:02:48.022315  9747 net.cpp:157] Top shape: 450 28 22 44 (12196800)
I0521 00:02:48.022326  9747 net.cpp:165] Memory required for data: 594822600
I0521 00:02:48.022358  9747 layer_factory.hpp:77] Creating layer relu3
I0521 00:02:48.022372  9747 net.cpp:106] Creating Layer relu3
I0521 00:02:48.022382  9747 net.cpp:454] relu3 <- conv3
I0521 00:02:48.022395  9747 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:02:48.022867  9747 net.cpp:150] Setting up relu3
I0521 00:02:48.022883  9747 net.cpp:157] Top shape: 450 28 22 44 (12196800)
I0521 00:02:48.022893  9747 net.cpp:165] Memory required for data: 643609800
I0521 00:02:48.022903  9747 layer_factory.hpp:77] Creating layer pool3
I0521 00:02:48.022917  9747 net.cpp:106] Creating Layer pool3
I0521 00:02:48.022927  9747 net.cpp:454] pool3 <- conv3
I0521 00:02:48.022939  9747 net.cpp:411] pool3 -> pool3
I0521 00:02:48.023011  9747 net.cpp:150] Setting up pool3
I0521 00:02:48.023025  9747 net.cpp:157] Top shape: 450 28 11 44 (6098400)
I0521 00:02:48.023035  9747 net.cpp:165] Memory required for data: 668003400
I0521 00:02:48.023042  9747 layer_factory.hpp:77] Creating layer conv4
I0521 00:02:48.023061  9747 net.cpp:106] Creating Layer conv4
I0521 00:02:48.023071  9747 net.cpp:454] conv4 <- pool3
I0521 00:02:48.023085  9747 net.cpp:411] conv4 -> conv4
I0521 00:02:48.025148  9747 net.cpp:150] Setting up conv4
I0521 00:02:48.025171  9747 net.cpp:157] Top shape: 450 36 6 42 (4082400)
I0521 00:02:48.025183  9747 net.cpp:165] Memory required for data: 684333000
I0521 00:02:48.025199  9747 layer_factory.hpp:77] Creating layer relu4
I0521 00:02:48.025213  9747 net.cpp:106] Creating Layer relu4
I0521 00:02:48.025223  9747 net.cpp:454] relu4 <- conv4
I0521 00:02:48.025235  9747 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:02:48.025706  9747 net.cpp:150] Setting up relu4
I0521 00:02:48.025722  9747 net.cpp:157] Top shape: 450 36 6 42 (4082400)
I0521 00:02:48.025732  9747 net.cpp:165] Memory required for data: 700662600
I0521 00:02:48.025743  9747 layer_factory.hpp:77] Creating layer pool4
I0521 00:02:48.025755  9747 net.cpp:106] Creating Layer pool4
I0521 00:02:48.025765  9747 net.cpp:454] pool4 <- conv4
I0521 00:02:48.025779  9747 net.cpp:411] pool4 -> pool4
I0521 00:02:48.025851  9747 net.cpp:150] Setting up pool4
I0521 00:02:48.025863  9747 net.cpp:157] Top shape: 450 36 3 42 (2041200)
I0521 00:02:48.025873  9747 net.cpp:165] Memory required for data: 708827400
I0521 00:02:48.025884  9747 layer_factory.hpp:77] Creating layer ip1
I0521 00:02:48.025899  9747 net.cpp:106] Creating Layer ip1
I0521 00:02:48.025910  9747 net.cpp:454] ip1 <- pool4
I0521 00:02:48.025923  9747 net.cpp:411] ip1 -> ip1
I0521 00:02:48.041398  9747 net.cpp:150] Setting up ip1
I0521 00:02:48.041426  9747 net.cpp:157] Top shape: 450 196 (88200)
I0521 00:02:48.041437  9747 net.cpp:165] Memory required for data: 709180200
I0521 00:02:48.041460  9747 layer_factory.hpp:77] Creating layer relu5
I0521 00:02:48.041474  9747 net.cpp:106] Creating Layer relu5
I0521 00:02:48.041484  9747 net.cpp:454] relu5 <- ip1
I0521 00:02:48.041498  9747 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:02:48.041846  9747 net.cpp:150] Setting up relu5
I0521 00:02:48.041859  9747 net.cpp:157] Top shape: 450 196 (88200)
I0521 00:02:48.041870  9747 net.cpp:165] Memory required for data: 709533000
I0521 00:02:48.041880  9747 layer_factory.hpp:77] Creating layer drop1
I0521 00:02:48.041899  9747 net.cpp:106] Creating Layer drop1
I0521 00:02:48.041909  9747 net.cpp:454] drop1 <- ip1
I0521 00:02:48.041923  9747 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:02:48.041967  9747 net.cpp:150] Setting up drop1
I0521 00:02:48.041980  9747 net.cpp:157] Top shape: 450 196 (88200)
I0521 00:02:48.041988  9747 net.cpp:165] Memory required for data: 709885800
I0521 00:02:48.041999  9747 layer_factory.hpp:77] Creating layer ip2
I0521 00:02:48.042013  9747 net.cpp:106] Creating Layer ip2
I0521 00:02:48.042027  9747 net.cpp:454] ip2 <- ip1
I0521 00:02:48.042042  9747 net.cpp:411] ip2 -> ip2
I0521 00:02:48.042524  9747 net.cpp:150] Setting up ip2
I0521 00:02:48.042538  9747 net.cpp:157] Top shape: 450 98 (44100)
I0521 00:02:48.042548  9747 net.cpp:165] Memory required for data: 710062200
I0521 00:02:48.042577  9747 layer_factory.hpp:77] Creating layer relu6
I0521 00:02:48.042590  9747 net.cpp:106] Creating Layer relu6
I0521 00:02:48.042600  9747 net.cpp:454] relu6 <- ip2
I0521 00:02:48.042613  9747 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:02:48.043149  9747 net.cpp:150] Setting up relu6
I0521 00:02:48.043170  9747 net.cpp:157] Top shape: 450 98 (44100)
I0521 00:02:48.043180  9747 net.cpp:165] Memory required for data: 710238600
I0521 00:02:48.043191  9747 layer_factory.hpp:77] Creating layer drop2
I0521 00:02:48.043205  9747 net.cpp:106] Creating Layer drop2
I0521 00:02:48.043215  9747 net.cpp:454] drop2 <- ip2
I0521 00:02:48.043228  9747 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:02:48.043272  9747 net.cpp:150] Setting up drop2
I0521 00:02:48.043285  9747 net.cpp:157] Top shape: 450 98 (44100)
I0521 00:02:48.043297  9747 net.cpp:165] Memory required for data: 710415000
I0521 00:02:48.043305  9747 layer_factory.hpp:77] Creating layer ip3
I0521 00:02:48.043320  9747 net.cpp:106] Creating Layer ip3
I0521 00:02:48.043330  9747 net.cpp:454] ip3 <- ip2
I0521 00:02:48.043344  9747 net.cpp:411] ip3 -> ip3
I0521 00:02:48.043566  9747 net.cpp:150] Setting up ip3
I0521 00:02:48.043579  9747 net.cpp:157] Top shape: 450 11 (4950)
I0521 00:02:48.043589  9747 net.cpp:165] Memory required for data: 710434800
I0521 00:02:48.043606  9747 layer_factory.hpp:77] Creating layer drop3
I0521 00:02:48.043618  9747 net.cpp:106] Creating Layer drop3
I0521 00:02:48.043628  9747 net.cpp:454] drop3 <- ip3
I0521 00:02:48.043642  9747 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:02:48.043683  9747 net.cpp:150] Setting up drop3
I0521 00:02:48.043695  9747 net.cpp:157] Top shape: 450 11 (4950)
I0521 00:02:48.043705  9747 net.cpp:165] Memory required for data: 710454600
I0521 00:02:48.043715  9747 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 00:02:48.043728  9747 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 00:02:48.043738  9747 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 00:02:48.043751  9747 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 00:02:48.043766  9747 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 00:02:48.043839  9747 net.cpp:150] Setting up ip3_drop3_0_split
I0521 00:02:48.043853  9747 net.cpp:157] Top shape: 450 11 (4950)
I0521 00:02:48.043864  9747 net.cpp:157] Top shape: 450 11 (4950)
I0521 00:02:48.043874  9747 net.cpp:165] Memory required for data: 710494200
I0521 00:02:48.043885  9747 layer_factory.hpp:77] Creating layer accuracy
I0521 00:02:48.043906  9747 net.cpp:106] Creating Layer accuracy
I0521 00:02:48.043917  9747 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 00:02:48.043928  9747 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 00:02:48.043942  9747 net.cpp:411] accuracy -> accuracy
I0521 00:02:48.043967  9747 net.cpp:150] Setting up accuracy
I0521 00:02:48.043978  9747 net.cpp:157] Top shape: (1)
I0521 00:02:48.043988  9747 net.cpp:165] Memory required for data: 710494204
I0521 00:02:48.043998  9747 layer_factory.hpp:77] Creating layer loss
I0521 00:02:48.044013  9747 net.cpp:106] Creating Layer loss
I0521 00:02:48.044023  9747 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 00:02:48.044034  9747 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 00:02:48.044047  9747 net.cpp:411] loss -> loss
I0521 00:02:48.044064  9747 layer_factory.hpp:77] Creating layer loss
I0521 00:02:48.044560  9747 net.cpp:150] Setting up loss
I0521 00:02:48.044574  9747 net.cpp:157] Top shape: (1)
I0521 00:02:48.044585  9747 net.cpp:160]     with loss weight 1
I0521 00:02:48.044605  9747 net.cpp:165] Memory required for data: 710494208
I0521 00:02:48.044615  9747 net.cpp:226] loss needs backward computation.
I0521 00:02:48.044625  9747 net.cpp:228] accuracy does not need backward computation.
I0521 00:02:48.044637  9747 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 00:02:48.044647  9747 net.cpp:226] drop3 needs backward computation.
I0521 00:02:48.044658  9747 net.cpp:226] ip3 needs backward computation.
I0521 00:02:48.044668  9747 net.cpp:226] drop2 needs backward computation.
I0521 00:02:48.044688  9747 net.cpp:226] relu6 needs backward computation.
I0521 00:02:48.044697  9747 net.cpp:226] ip2 needs backward computation.
I0521 00:02:48.044708  9747 net.cpp:226] drop1 needs backward computation.
I0521 00:02:48.044718  9747 net.cpp:226] relu5 needs backward computation.
I0521 00:02:48.044726  9747 net.cpp:226] ip1 needs backward computation.
I0521 00:02:48.044739  9747 net.cpp:226] pool4 needs backward computation.
I0521 00:02:48.044749  9747 net.cpp:226] relu4 needs backward computation.
I0521 00:02:48.044756  9747 net.cpp:226] conv4 needs backward computation.
I0521 00:02:48.044767  9747 net.cpp:226] pool3 needs backward computation.
I0521 00:02:48.044778  9747 net.cpp:226] relu3 needs backward computation.
I0521 00:02:48.044788  9747 net.cpp:226] conv3 needs backward computation.
I0521 00:02:48.044798  9747 net.cpp:226] pool2 needs backward computation.
I0521 00:02:48.044808  9747 net.cpp:226] relu2 needs backward computation.
I0521 00:02:48.044818  9747 net.cpp:226] conv2 needs backward computation.
I0521 00:02:48.044828  9747 net.cpp:226] pool1 needs backward computation.
I0521 00:02:48.044838  9747 net.cpp:226] relu1 needs backward computation.
I0521 00:02:48.044848  9747 net.cpp:226] conv1 needs backward computation.
I0521 00:02:48.044860  9747 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 00:02:48.044872  9747 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:02:48.044883  9747 net.cpp:270] This network produces output accuracy
I0521 00:02:48.044891  9747 net.cpp:270] This network produces output loss
I0521 00:02:48.044920  9747 net.cpp:283] Network initialization done.
I0521 00:02:48.045054  9747 solver.cpp:60] Solver scaffolding done.
I0521 00:02:48.046185  9747 caffe.cpp:212] Starting Optimization
I0521 00:02:48.046205  9747 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 00:02:48.046214  9747 solver.cpp:289] Learning Rate Policy: fixed
I0521 00:02:48.047435  9747 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 00:03:34.109272  9747 solver.cpp:409]     Test net output #0: accuracy = 0.0717851
I0521 00:03:34.109432  9747 solver.cpp:409]     Test net output #1: loss = 2.3976 (* 1 = 2.3976 loss)
I0521 00:03:34.199648  9747 solver.cpp:237] Iteration 0, loss = 2.39601
I0521 00:03:34.199686  9747 solver.cpp:253]     Train net output #0: loss = 2.39601 (* 1 = 2.39601 loss)
I0521 00:03:34.199703  9747 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 00:03:42.205673  9747 solver.cpp:237] Iteration 33, loss = 2.38026
I0521 00:03:42.205711  9747 solver.cpp:253]     Train net output #0: loss = 2.38026 (* 1 = 2.38026 loss)
I0521 00:03:42.205732  9747 sgd_solver.cpp:106] Iteration 33, lr = 0.0025
I0521 00:03:50.218838  9747 solver.cpp:237] Iteration 66, loss = 2.36215
I0521 00:03:50.218871  9747 solver.cpp:253]     Train net output #0: loss = 2.36215 (* 1 = 2.36215 loss)
I0521 00:03:50.218884  9747 sgd_solver.cpp:106] Iteration 66, lr = 0.0025
I0521 00:03:58.229374  9747 solver.cpp:237] Iteration 99, loss = 2.33205
I0521 00:03:58.229408  9747 solver.cpp:253]     Train net output #0: loss = 2.33205 (* 1 = 2.33205 loss)
I0521 00:03:58.229423  9747 sgd_solver.cpp:106] Iteration 99, lr = 0.0025
I0521 00:04:06.238867  9747 solver.cpp:237] Iteration 132, loss = 2.34027
I0521 00:04:06.239022  9747 solver.cpp:253]     Train net output #0: loss = 2.34027 (* 1 = 2.34027 loss)
I0521 00:04:06.239037  9747 sgd_solver.cpp:106] Iteration 132, lr = 0.0025
I0521 00:04:14.250943  9747 solver.cpp:237] Iteration 165, loss = 2.32801
I0521 00:04:14.250977  9747 solver.cpp:253]     Train net output #0: loss = 2.32801 (* 1 = 2.32801 loss)
I0521 00:04:14.250994  9747 sgd_solver.cpp:106] Iteration 165, lr = 0.0025
I0521 00:04:22.260423  9747 solver.cpp:237] Iteration 198, loss = 2.30468
I0521 00:04:22.260455  9747 solver.cpp:253]     Train net output #0: loss = 2.30468 (* 1 = 2.30468 loss)
I0521 00:04:22.260473  9747 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 00:04:52.387408  9747 solver.cpp:237] Iteration 231, loss = 2.28299
I0521 00:04:52.387570  9747 solver.cpp:253]     Train net output #0: loss = 2.28299 (* 1 = 2.28299 loss)
I0521 00:04:52.387584  9747 sgd_solver.cpp:106] Iteration 231, lr = 0.0025
I0521 00:05:00.406668  9747 solver.cpp:237] Iteration 264, loss = 2.22042
I0521 00:05:00.406708  9747 solver.cpp:253]     Train net output #0: loss = 2.22042 (* 1 = 2.22042 loss)
I0521 00:05:00.406729  9747 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0521 00:05:08.415442  9747 solver.cpp:237] Iteration 297, loss = 2.23639
I0521 00:05:08.415475  9747 solver.cpp:253]     Train net output #0: loss = 2.23639 (* 1 = 2.23639 loss)
I0521 00:05:08.415493  9747 sgd_solver.cpp:106] Iteration 297, lr = 0.0025
I0521 00:05:16.424932  9747 solver.cpp:237] Iteration 330, loss = 2.17786
I0521 00:05:16.424965  9747 solver.cpp:253]     Train net output #0: loss = 2.17786 (* 1 = 2.17786 loss)
I0521 00:05:16.424981  9747 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 00:05:16.911660  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_333.caffemodel
I0521 00:05:17.123258  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_333.solverstate
I0521 00:05:24.504809  9747 solver.cpp:237] Iteration 363, loss = 2.08703
I0521 00:05:24.504968  9747 solver.cpp:253]     Train net output #0: loss = 2.08703 (* 1 = 2.08703 loss)
I0521 00:05:24.504982  9747 sgd_solver.cpp:106] Iteration 363, lr = 0.0025
I0521 00:05:32.516013  9747 solver.cpp:237] Iteration 396, loss = 2.1149
I0521 00:05:32.516046  9747 solver.cpp:253]     Train net output #0: loss = 2.1149 (* 1 = 2.1149 loss)
I0521 00:05:32.516062  9747 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 00:05:40.520982  9747 solver.cpp:237] Iteration 429, loss = 1.97847
I0521 00:05:40.521015  9747 solver.cpp:253]     Train net output #0: loss = 1.97847 (* 1 = 1.97847 loss)
I0521 00:05:40.521034  9747 sgd_solver.cpp:106] Iteration 429, lr = 0.0025
I0521 00:06:10.685297  9747 solver.cpp:237] Iteration 462, loss = 1.95729
I0521 00:06:10.685451  9747 solver.cpp:253]     Train net output #0: loss = 1.95729 (* 1 = 1.95729 loss)
I0521 00:06:10.685467  9747 sgd_solver.cpp:106] Iteration 462, lr = 0.0025
I0521 00:06:18.700269  9747 solver.cpp:237] Iteration 495, loss = 1.99797
I0521 00:06:18.700304  9747 solver.cpp:253]     Train net output #0: loss = 1.99797 (* 1 = 1.99797 loss)
I0521 00:06:18.700321  9747 sgd_solver.cpp:106] Iteration 495, lr = 0.0025
I0521 00:06:26.708367  9747 solver.cpp:237] Iteration 528, loss = 1.94645
I0521 00:06:26.708402  9747 solver.cpp:253]     Train net output #0: loss = 1.94645 (* 1 = 1.94645 loss)
I0521 00:06:26.708420  9747 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 00:06:34.720859  9747 solver.cpp:237] Iteration 561, loss = 1.91298
I0521 00:06:34.720901  9747 solver.cpp:253]     Train net output #0: loss = 1.91298 (* 1 = 1.91298 loss)
I0521 00:06:34.720918  9747 sgd_solver.cpp:106] Iteration 561, lr = 0.0025
I0521 00:06:42.732260  9747 solver.cpp:237] Iteration 594, loss = 1.92221
I0521 00:06:42.732417  9747 solver.cpp:253]     Train net output #0: loss = 1.92221 (* 1 = 1.92221 loss)
I0521 00:06:42.732432  9747 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 00:06:50.747421  9747 solver.cpp:237] Iteration 627, loss = 1.90142
I0521 00:06:50.747453  9747 solver.cpp:253]     Train net output #0: loss = 1.90142 (* 1 = 1.90142 loss)
I0521 00:06:50.747472  9747 sgd_solver.cpp:106] Iteration 627, lr = 0.0025
I0521 00:06:58.760803  9747 solver.cpp:237] Iteration 660, loss = 1.91151
I0521 00:06:58.760838  9747 solver.cpp:253]     Train net output #0: loss = 1.91151 (* 1 = 1.91151 loss)
I0521 00:06:58.760854  9747 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 00:06:59.975433  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_666.caffemodel
I0521 00:07:00.182904  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_666.solverstate
I0521 00:07:00.208205  9747 solver.cpp:341] Iteration 666, Testing net (#0)
I0521 00:07:45.300946  9747 solver.cpp:409]     Test net output #0: accuracy = 0.59996
I0521 00:07:45.301106  9747 solver.cpp:409]     Test net output #1: loss = 1.48616 (* 1 = 1.48616 loss)
I0521 00:08:14.057862  9747 solver.cpp:237] Iteration 693, loss = 1.82164
I0521 00:08:14.057914  9747 solver.cpp:253]     Train net output #0: loss = 1.82164 (* 1 = 1.82164 loss)
I0521 00:08:14.057929  9747 sgd_solver.cpp:106] Iteration 693, lr = 0.0025
I0521 00:08:22.061871  9747 solver.cpp:237] Iteration 726, loss = 1.79755
I0521 00:08:22.062029  9747 solver.cpp:253]     Train net output #0: loss = 1.79755 (* 1 = 1.79755 loss)
I0521 00:08:22.062043  9747 sgd_solver.cpp:106] Iteration 726, lr = 0.0025
I0521 00:08:30.063593  9747 solver.cpp:237] Iteration 759, loss = 1.88087
I0521 00:08:30.063627  9747 solver.cpp:253]     Train net output #0: loss = 1.88087 (* 1 = 1.88087 loss)
I0521 00:08:30.063644  9747 sgd_solver.cpp:106] Iteration 759, lr = 0.0025
I0521 00:08:38.060551  9747 solver.cpp:237] Iteration 792, loss = 1.83188
I0521 00:08:38.060585  9747 solver.cpp:253]     Train net output #0: loss = 1.83188 (* 1 = 1.83188 loss)
I0521 00:08:38.060601  9747 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 00:08:46.060552  9747 solver.cpp:237] Iteration 825, loss = 1.79058
I0521 00:08:46.060600  9747 solver.cpp:253]     Train net output #0: loss = 1.79058 (* 1 = 1.79058 loss)
I0521 00:08:46.060616  9747 sgd_solver.cpp:106] Iteration 825, lr = 0.0025
I0521 00:08:54.059028  9747 solver.cpp:237] Iteration 858, loss = 1.79785
I0521 00:08:54.059168  9747 solver.cpp:253]     Train net output #0: loss = 1.79785 (* 1 = 1.79785 loss)
I0521 00:08:54.059182  9747 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0521 00:09:24.199103  9747 solver.cpp:237] Iteration 891, loss = 1.77591
I0521 00:09:24.199267  9747 solver.cpp:253]     Train net output #0: loss = 1.77591 (* 1 = 1.77591 loss)
I0521 00:09:24.199282  9747 sgd_solver.cpp:106] Iteration 891, lr = 0.0025
I0521 00:09:32.199971  9747 solver.cpp:237] Iteration 924, loss = 1.7523
I0521 00:09:32.200011  9747 solver.cpp:253]     Train net output #0: loss = 1.7523 (* 1 = 1.7523 loss)
I0521 00:09:32.200031  9747 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 00:09:40.207377  9747 solver.cpp:237] Iteration 957, loss = 1.77092
I0521 00:09:40.207412  9747 solver.cpp:253]     Train net output #0: loss = 1.77092 (* 1 = 1.77092 loss)
I0521 00:09:40.207429  9747 sgd_solver.cpp:106] Iteration 957, lr = 0.0025
I0521 00:09:48.203740  9747 solver.cpp:237] Iteration 990, loss = 1.82902
I0521 00:09:48.203774  9747 solver.cpp:253]     Train net output #0: loss = 1.82902 (* 1 = 1.82902 loss)
I0521 00:09:48.203791  9747 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 00:09:50.143301  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_999.caffemodel
I0521 00:09:50.353387  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_999.solverstate
I0521 00:09:56.270601  9747 solver.cpp:237] Iteration 1023, loss = 1.92344
I0521 00:09:56.270768  9747 solver.cpp:253]     Train net output #0: loss = 1.92344 (* 1 = 1.92344 loss)
I0521 00:09:56.270782  9747 sgd_solver.cpp:106] Iteration 1023, lr = 0.0025
I0521 00:10:04.269582  9747 solver.cpp:237] Iteration 1056, loss = 1.74425
I0521 00:10:04.269615  9747 solver.cpp:253]     Train net output #0: loss = 1.74425 (* 1 = 1.74425 loss)
I0521 00:10:04.269632  9747 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 00:10:12.267478  9747 solver.cpp:237] Iteration 1089, loss = 1.74284
I0521 00:10:12.267511  9747 solver.cpp:253]     Train net output #0: loss = 1.74284 (* 1 = 1.74284 loss)
I0521 00:10:12.267529  9747 sgd_solver.cpp:106] Iteration 1089, lr = 0.0025
I0521 00:10:42.442818  9747 solver.cpp:237] Iteration 1122, loss = 1.69271
I0521 00:10:42.442982  9747 solver.cpp:253]     Train net output #0: loss = 1.69271 (* 1 = 1.69271 loss)
I0521 00:10:42.442998  9747 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 00:10:50.438141  9747 solver.cpp:237] Iteration 1155, loss = 1.7247
I0521 00:10:50.438179  9747 solver.cpp:253]     Train net output #0: loss = 1.7247 (* 1 = 1.7247 loss)
I0521 00:10:50.438197  9747 sgd_solver.cpp:106] Iteration 1155, lr = 0.0025
I0521 00:10:58.433512  9747 solver.cpp:237] Iteration 1188, loss = 1.78595
I0521 00:10:58.433547  9747 solver.cpp:253]     Train net output #0: loss = 1.78595 (* 1 = 1.78595 loss)
I0521 00:10:58.433564  9747 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 00:11:06.430258  9747 solver.cpp:237] Iteration 1221, loss = 1.71647
I0521 00:11:06.430291  9747 solver.cpp:253]     Train net output #0: loss = 1.71647 (* 1 = 1.71647 loss)
I0521 00:11:06.430310  9747 sgd_solver.cpp:106] Iteration 1221, lr = 0.0025
I0521 00:11:14.429425  9747 solver.cpp:237] Iteration 1254, loss = 1.65747
I0521 00:11:14.429580  9747 solver.cpp:253]     Train net output #0: loss = 1.65747 (* 1 = 1.65747 loss)
I0521 00:11:14.429594  9747 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0521 00:11:22.430840  9747 solver.cpp:237] Iteration 1287, loss = 1.73947
I0521 00:11:22.430871  9747 solver.cpp:253]     Train net output #0: loss = 1.73947 (* 1 = 1.73947 loss)
I0521 00:11:22.430889  9747 sgd_solver.cpp:106] Iteration 1287, lr = 0.0025
I0521 00:11:30.432265  9747 solver.cpp:237] Iteration 1320, loss = 1.69201
I0521 00:11:30.432299  9747 solver.cpp:253]     Train net output #0: loss = 1.69201 (* 1 = 1.69201 loss)
I0521 00:11:30.432315  9747 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 00:11:33.097599  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_1332.caffemodel
I0521 00:11:33.307322  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_1332.solverstate
I0521 00:11:33.335533  9747 solver.cpp:341] Iteration 1332, Testing net (#0)
I0521 00:12:39.293221  9747 solver.cpp:409]     Test net output #0: accuracy = 0.655142
I0521 00:12:39.293393  9747 solver.cpp:409]     Test net output #1: loss = 1.21729 (* 1 = 1.21729 loss)
I0521 00:13:06.587937  9747 solver.cpp:237] Iteration 1353, loss = 1.83183
I0521 00:13:06.587990  9747 solver.cpp:253]     Train net output #0: loss = 1.83183 (* 1 = 1.83183 loss)
I0521 00:13:06.588004  9747 sgd_solver.cpp:106] Iteration 1353, lr = 0.0025
I0521 00:13:14.578192  9747 solver.cpp:237] Iteration 1386, loss = 1.75998
I0521 00:13:14.578358  9747 solver.cpp:253]     Train net output #0: loss = 1.75998 (* 1 = 1.75998 loss)
I0521 00:13:14.578372  9747 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 00:13:22.569864  9747 solver.cpp:237] Iteration 1419, loss = 1.75294
I0521 00:13:22.569897  9747 solver.cpp:253]     Train net output #0: loss = 1.75294 (* 1 = 1.75294 loss)
I0521 00:13:22.569914  9747 sgd_solver.cpp:106] Iteration 1419, lr = 0.0025
I0521 00:13:30.567100  9747 solver.cpp:237] Iteration 1452, loss = 1.81249
I0521 00:13:30.567133  9747 solver.cpp:253]     Train net output #0: loss = 1.81249 (* 1 = 1.81249 loss)
I0521 00:13:30.567149  9747 sgd_solver.cpp:106] Iteration 1452, lr = 0.0025
I0521 00:13:38.555794  9747 solver.cpp:237] Iteration 1485, loss = 1.75158
I0521 00:13:38.555829  9747 solver.cpp:253]     Train net output #0: loss = 1.75158 (* 1 = 1.75158 loss)
I0521 00:13:38.555845  9747 sgd_solver.cpp:106] Iteration 1485, lr = 0.0025
I0521 00:13:46.547616  9747 solver.cpp:237] Iteration 1518, loss = 1.66864
I0521 00:13:46.547755  9747 solver.cpp:253]     Train net output #0: loss = 1.66864 (* 1 = 1.66864 loss)
I0521 00:13:46.547770  9747 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0521 00:13:54.542554  9747 solver.cpp:237] Iteration 1551, loss = 1.71437
I0521 00:13:54.542587  9747 solver.cpp:253]     Train net output #0: loss = 1.71437 (* 1 = 1.71437 loss)
I0521 00:13:54.542605  9747 sgd_solver.cpp:106] Iteration 1551, lr = 0.0025
I0521 00:14:24.643489  9747 solver.cpp:237] Iteration 1584, loss = 1.71717
I0521 00:14:24.643657  9747 solver.cpp:253]     Train net output #0: loss = 1.71717 (* 1 = 1.71717 loss)
I0521 00:14:24.643671  9747 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 00:14:32.633718  9747 solver.cpp:237] Iteration 1617, loss = 1.6905
I0521 00:14:32.633754  9747 solver.cpp:253]     Train net output #0: loss = 1.6905 (* 1 = 1.6905 loss)
I0521 00:14:32.633772  9747 sgd_solver.cpp:106] Iteration 1617, lr = 0.0025
I0521 00:14:40.621543  9747 solver.cpp:237] Iteration 1650, loss = 1.57282
I0521 00:14:40.621577  9747 solver.cpp:253]     Train net output #0: loss = 1.57282 (* 1 = 1.57282 loss)
I0521 00:14:40.621593  9747 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 00:14:44.014192  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_1665.caffemodel
I0521 00:14:44.224437  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_1665.solverstate
I0521 00:14:48.689494  9747 solver.cpp:237] Iteration 1683, loss = 1.68205
I0521 00:14:48.689543  9747 solver.cpp:253]     Train net output #0: loss = 1.68205 (* 1 = 1.68205 loss)
I0521 00:14:48.689558  9747 sgd_solver.cpp:106] Iteration 1683, lr = 0.0025
I0521 00:14:56.677666  9747 solver.cpp:237] Iteration 1716, loss = 1.68513
I0521 00:14:56.677822  9747 solver.cpp:253]     Train net output #0: loss = 1.68513 (* 1 = 1.68513 loss)
I0521 00:14:56.677836  9747 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0521 00:15:04.670867  9747 solver.cpp:237] Iteration 1749, loss = 1.70809
I0521 00:15:04.670899  9747 solver.cpp:253]     Train net output #0: loss = 1.70809 (* 1 = 1.70809 loss)
I0521 00:15:04.670917  9747 sgd_solver.cpp:106] Iteration 1749, lr = 0.0025
I0521 00:15:34.814584  9747 solver.cpp:237] Iteration 1782, loss = 1.67757
I0521 00:15:34.814752  9747 solver.cpp:253]     Train net output #0: loss = 1.67757 (* 1 = 1.67757 loss)
I0521 00:15:34.814767  9747 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 00:15:42.809010  9747 solver.cpp:237] Iteration 1815, loss = 1.73195
I0521 00:15:42.809043  9747 solver.cpp:253]     Train net output #0: loss = 1.73195 (* 1 = 1.73195 loss)
I0521 00:15:42.809061  9747 sgd_solver.cpp:106] Iteration 1815, lr = 0.0025
I0521 00:15:50.801574  9747 solver.cpp:237] Iteration 1848, loss = 1.63782
I0521 00:15:50.801614  9747 solver.cpp:253]     Train net output #0: loss = 1.63782 (* 1 = 1.63782 loss)
I0521 00:15:50.801630  9747 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 00:15:58.793179  9747 solver.cpp:237] Iteration 1881, loss = 1.67504
I0521 00:15:58.793212  9747 solver.cpp:253]     Train net output #0: loss = 1.67504 (* 1 = 1.67504 loss)
I0521 00:15:58.793228  9747 sgd_solver.cpp:106] Iteration 1881, lr = 0.0025
I0521 00:16:06.785495  9747 solver.cpp:237] Iteration 1914, loss = 1.71863
I0521 00:16:06.785631  9747 solver.cpp:253]     Train net output #0: loss = 1.71863 (* 1 = 1.71863 loss)
I0521 00:16:06.785645  9747 sgd_solver.cpp:106] Iteration 1914, lr = 0.0025
I0521 00:16:14.786803  9747 solver.cpp:237] Iteration 1947, loss = 1.64778
I0521 00:16:14.786847  9747 solver.cpp:253]     Train net output #0: loss = 1.64778 (* 1 = 1.64778 loss)
I0521 00:16:14.786865  9747 sgd_solver.cpp:106] Iteration 1947, lr = 0.0025
I0521 00:16:22.774514  9747 solver.cpp:237] Iteration 1980, loss = 1.67185
I0521 00:16:22.774546  9747 solver.cpp:253]     Train net output #0: loss = 1.67185 (* 1 = 1.67185 loss)
I0521 00:16:22.774564  9747 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 00:16:26.890599  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_1998.caffemodel
I0521 00:16:27.099324  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_1998.solverstate
I0521 00:16:27.125727  9747 solver.cpp:341] Iteration 1998, Testing net (#0)
I0521 00:17:11.971952  9747 solver.cpp:409]     Test net output #0: accuracy = 0.681815
I0521 00:17:11.972115  9747 solver.cpp:409]     Test net output #1: loss = 1.10135 (* 1 = 1.10135 loss)
I0521 00:17:37.821138  9747 solver.cpp:237] Iteration 2013, loss = 1.70742
I0521 00:17:37.821189  9747 solver.cpp:253]     Train net output #0: loss = 1.70742 (* 1 = 1.70742 loss)
I0521 00:17:37.821203  9747 sgd_solver.cpp:106] Iteration 2013, lr = 0.0025
I0521 00:17:45.819222  9747 solver.cpp:237] Iteration 2046, loss = 1.67596
I0521 00:17:45.819366  9747 solver.cpp:253]     Train net output #0: loss = 1.67596 (* 1 = 1.67596 loss)
I0521 00:17:45.819380  9747 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0521 00:17:53.816813  9747 solver.cpp:237] Iteration 2079, loss = 1.6875
I0521 00:17:53.816850  9747 solver.cpp:253]     Train net output #0: loss = 1.6875 (* 1 = 1.6875 loss)
I0521 00:17:53.816869  9747 sgd_solver.cpp:106] Iteration 2079, lr = 0.0025
I0521 00:18:01.814417  9747 solver.cpp:237] Iteration 2112, loss = 1.65371
I0521 00:18:01.814450  9747 solver.cpp:253]     Train net output #0: loss = 1.65371 (* 1 = 1.65371 loss)
I0521 00:18:01.814466  9747 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 00:18:09.813458  9747 solver.cpp:237] Iteration 2145, loss = 1.63772
I0521 00:18:09.813493  9747 solver.cpp:253]     Train net output #0: loss = 1.63772 (* 1 = 1.63772 loss)
I0521 00:18:09.813508  9747 sgd_solver.cpp:106] Iteration 2145, lr = 0.0025
I0521 00:18:17.811306  9747 solver.cpp:237] Iteration 2178, loss = 1.60683
I0521 00:18:17.811481  9747 solver.cpp:253]     Train net output #0: loss = 1.60683 (* 1 = 1.60683 loss)
I0521 00:18:17.811496  9747 sgd_solver.cpp:106] Iteration 2178, lr = 0.0025
I0521 00:18:25.810802  9747 solver.cpp:237] Iteration 2211, loss = 1.62293
I0521 00:18:25.810834  9747 solver.cpp:253]     Train net output #0: loss = 1.62293 (* 1 = 1.62293 loss)
I0521 00:18:25.810853  9747 sgd_solver.cpp:106] Iteration 2211, lr = 0.0025
I0521 00:18:55.958545  9747 solver.cpp:237] Iteration 2244, loss = 1.65859
I0521 00:18:55.958709  9747 solver.cpp:253]     Train net output #0: loss = 1.65859 (* 1 = 1.65859 loss)
I0521 00:18:55.958724  9747 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0521 00:19:03.952093  9747 solver.cpp:237] Iteration 2277, loss = 1.72386
I0521 00:19:03.952126  9747 solver.cpp:253]     Train net output #0: loss = 1.72386 (* 1 = 1.72386 loss)
I0521 00:19:03.952143  9747 sgd_solver.cpp:106] Iteration 2277, lr = 0.0025
I0521 00:19:11.950402  9747 solver.cpp:237] Iteration 2310, loss = 1.56725
I0521 00:19:11.950434  9747 solver.cpp:253]     Train net output #0: loss = 1.56725 (* 1 = 1.56725 loss)
I0521 00:19:11.950453  9747 sgd_solver.cpp:106] Iteration 2310, lr = 0.0025
I0521 00:19:16.796183  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_2331.caffemodel
I0521 00:19:17.010671  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_2331.solverstate
I0521 00:19:20.019450  9747 solver.cpp:237] Iteration 2343, loss = 1.58564
I0521 00:19:20.019496  9747 solver.cpp:253]     Train net output #0: loss = 1.58564 (* 1 = 1.58564 loss)
I0521 00:19:20.019510  9747 sgd_solver.cpp:106] Iteration 2343, lr = 0.0025
I0521 00:19:28.013507  9747 solver.cpp:237] Iteration 2376, loss = 1.58292
I0521 00:19:28.013654  9747 solver.cpp:253]     Train net output #0: loss = 1.58292 (* 1 = 1.58292 loss)
I0521 00:19:28.013669  9747 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0521 00:19:36.014317  9747 solver.cpp:237] Iteration 2409, loss = 1.56928
I0521 00:19:36.014360  9747 solver.cpp:253]     Train net output #0: loss = 1.56928 (* 1 = 1.56928 loss)
I0521 00:19:36.014376  9747 sgd_solver.cpp:106] Iteration 2409, lr = 0.0025
I0521 00:19:44.014013  9747 solver.cpp:237] Iteration 2442, loss = 1.53904
I0521 00:19:44.014046  9747 solver.cpp:253]     Train net output #0: loss = 1.53904 (* 1 = 1.53904 loss)
I0521 00:19:44.014062  9747 sgd_solver.cpp:106] Iteration 2442, lr = 0.0025
I0521 00:20:14.137950  9747 solver.cpp:237] Iteration 2475, loss = 1.54592
I0521 00:20:14.138121  9747 solver.cpp:253]     Train net output #0: loss = 1.54592 (* 1 = 1.54592 loss)
I0521 00:20:14.138136  9747 sgd_solver.cpp:106] Iteration 2475, lr = 0.0025
I0521 00:20:22.141028  9747 solver.cpp:237] Iteration 2508, loss = 1.50995
I0521 00:20:22.141070  9747 solver.cpp:253]     Train net output #0: loss = 1.50995 (* 1 = 1.50995 loss)
I0521 00:20:22.141089  9747 sgd_solver.cpp:106] Iteration 2508, lr = 0.0025
I0521 00:20:30.132130  9747 solver.cpp:237] Iteration 2541, loss = 1.62966
I0521 00:20:30.132165  9747 solver.cpp:253]     Train net output #0: loss = 1.62966 (* 1 = 1.62966 loss)
I0521 00:20:30.132179  9747 sgd_solver.cpp:106] Iteration 2541, lr = 0.0025
I0521 00:20:38.131464  9747 solver.cpp:237] Iteration 2574, loss = 1.66497
I0521 00:20:38.131499  9747 solver.cpp:253]     Train net output #0: loss = 1.66497 (* 1 = 1.66497 loss)
I0521 00:20:38.131515  9747 sgd_solver.cpp:106] Iteration 2574, lr = 0.0025
I0521 00:20:46.134030  9747 solver.cpp:237] Iteration 2607, loss = 1.66991
I0521 00:20:46.134188  9747 solver.cpp:253]     Train net output #0: loss = 1.66991 (* 1 = 1.66991 loss)
I0521 00:20:46.134203  9747 sgd_solver.cpp:106] Iteration 2607, lr = 0.0025
I0521 00:20:54.131541  9747 solver.cpp:237] Iteration 2640, loss = 1.57034
I0521 00:20:54.131574  9747 solver.cpp:253]     Train net output #0: loss = 1.57034 (* 1 = 1.57034 loss)
I0521 00:20:54.131592  9747 sgd_solver.cpp:106] Iteration 2640, lr = 0.0025
I0521 00:20:59.704008  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_2664.caffemodel
I0521 00:20:59.912592  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_2664.solverstate
I0521 00:20:59.938822  9747 solver.cpp:341] Iteration 2664, Testing net (#0)
I0521 00:22:05.898103  9747 solver.cpp:409]     Test net output #0: accuracy = 0.696209
I0521 00:22:05.898274  9747 solver.cpp:409]     Test net output #1: loss = 1.04233 (* 1 = 1.04233 loss)
I0521 00:22:30.303385  9747 solver.cpp:237] Iteration 2673, loss = 1.60268
I0521 00:22:30.303436  9747 solver.cpp:253]     Train net output #0: loss = 1.60268 (* 1 = 1.60268 loss)
I0521 00:22:30.303452  9747 sgd_solver.cpp:106] Iteration 2673, lr = 0.0025
I0521 00:22:38.302633  9747 solver.cpp:237] Iteration 2706, loss = 1.56562
I0521 00:22:38.302785  9747 solver.cpp:253]     Train net output #0: loss = 1.56562 (* 1 = 1.56562 loss)
I0521 00:22:38.302799  9747 sgd_solver.cpp:106] Iteration 2706, lr = 0.0025
I0521 00:22:46.306998  9747 solver.cpp:237] Iteration 2739, loss = 1.57151
I0521 00:22:46.307031  9747 solver.cpp:253]     Train net output #0: loss = 1.57151 (* 1 = 1.57151 loss)
I0521 00:22:46.307047  9747 sgd_solver.cpp:106] Iteration 2739, lr = 0.0025
I0521 00:22:54.303745  9747 solver.cpp:237] Iteration 2772, loss = 1.70086
I0521 00:22:54.303783  9747 solver.cpp:253]     Train net output #0: loss = 1.70086 (* 1 = 1.70086 loss)
I0521 00:22:54.303803  9747 sgd_solver.cpp:106] Iteration 2772, lr = 0.0025
I0521 00:23:02.307095  9747 solver.cpp:237] Iteration 2805, loss = 1.59267
I0521 00:23:02.307128  9747 solver.cpp:253]     Train net output #0: loss = 1.59267 (* 1 = 1.59267 loss)
I0521 00:23:02.307144  9747 sgd_solver.cpp:106] Iteration 2805, lr = 0.0025
I0521 00:23:10.301796  9747 solver.cpp:237] Iteration 2838, loss = 1.63091
I0521 00:23:10.301950  9747 solver.cpp:253]     Train net output #0: loss = 1.63091 (* 1 = 1.63091 loss)
I0521 00:23:10.301964  9747 sgd_solver.cpp:106] Iteration 2838, lr = 0.0025
I0521 00:23:18.304357  9747 solver.cpp:237] Iteration 2871, loss = 1.59532
I0521 00:23:18.304401  9747 solver.cpp:253]     Train net output #0: loss = 1.59532 (* 1 = 1.59532 loss)
I0521 00:23:18.304419  9747 sgd_solver.cpp:106] Iteration 2871, lr = 0.0025
I0521 00:23:48.430485  9747 solver.cpp:237] Iteration 2904, loss = 1.67428
I0521 00:23:48.430656  9747 solver.cpp:253]     Train net output #0: loss = 1.67428 (* 1 = 1.67428 loss)
I0521 00:23:48.430672  9747 sgd_solver.cpp:106] Iteration 2904, lr = 0.0025
I0521 00:23:56.431066  9747 solver.cpp:237] Iteration 2937, loss = 1.5159
I0521 00:23:56.431097  9747 solver.cpp:253]     Train net output #0: loss = 1.5159 (* 1 = 1.5159 loss)
I0521 00:23:56.431115  9747 sgd_solver.cpp:106] Iteration 2937, lr = 0.0025
I0521 00:24:04.428480  9747 solver.cpp:237] Iteration 2970, loss = 1.51798
I0521 00:24:04.428529  9747 solver.cpp:253]     Train net output #0: loss = 1.51798 (* 1 = 1.51798 loss)
I0521 00:24:04.428544  9747 sgd_solver.cpp:106] Iteration 2970, lr = 0.0025
I0521 00:24:10.730036  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_2997.caffemodel
I0521 00:24:10.939249  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_2997.solverstate
I0521 00:24:12.496340  9747 solver.cpp:237] Iteration 3003, loss = 1.60134
I0521 00:24:12.496387  9747 solver.cpp:253]     Train net output #0: loss = 1.60134 (* 1 = 1.60134 loss)
I0521 00:24:12.496403  9747 sgd_solver.cpp:106] Iteration 3003, lr = 0.0025
I0521 00:24:20.495158  9747 solver.cpp:237] Iteration 3036, loss = 1.51838
I0521 00:24:20.495318  9747 solver.cpp:253]     Train net output #0: loss = 1.51838 (* 1 = 1.51838 loss)
I0521 00:24:20.495332  9747 sgd_solver.cpp:106] Iteration 3036, lr = 0.0025
I0521 00:24:28.501114  9747 solver.cpp:237] Iteration 3069, loss = 1.52075
I0521 00:24:28.501148  9747 solver.cpp:253]     Train net output #0: loss = 1.52075 (* 1 = 1.52075 loss)
I0521 00:24:28.501165  9747 sgd_solver.cpp:106] Iteration 3069, lr = 0.0025
I0521 00:24:36.500771  9747 solver.cpp:237] Iteration 3102, loss = 1.52514
I0521 00:24:36.500819  9747 solver.cpp:253]     Train net output #0: loss = 1.52514 (* 1 = 1.52514 loss)
I0521 00:24:36.500836  9747 sgd_solver.cpp:106] Iteration 3102, lr = 0.0025
I0521 00:25:06.663445  9747 solver.cpp:237] Iteration 3135, loss = 1.56735
I0521 00:25:06.663617  9747 solver.cpp:253]     Train net output #0: loss = 1.56735 (* 1 = 1.56735 loss)
I0521 00:25:06.663632  9747 sgd_solver.cpp:106] Iteration 3135, lr = 0.0025
I0521 00:25:14.674602  9747 solver.cpp:237] Iteration 3168, loss = 1.55094
I0521 00:25:14.674635  9747 solver.cpp:253]     Train net output #0: loss = 1.55094 (* 1 = 1.55094 loss)
I0521 00:25:14.674652  9747 sgd_solver.cpp:106] Iteration 3168, lr = 0.0025
I0521 00:25:22.675351  9747 solver.cpp:237] Iteration 3201, loss = 1.5731
I0521 00:25:22.675389  9747 solver.cpp:253]     Train net output #0: loss = 1.5731 (* 1 = 1.5731 loss)
I0521 00:25:22.675405  9747 sgd_solver.cpp:106] Iteration 3201, lr = 0.0025
I0521 00:25:30.673388  9747 solver.cpp:237] Iteration 3234, loss = 1.54831
I0521 00:25:30.673421  9747 solver.cpp:253]     Train net output #0: loss = 1.54831 (* 1 = 1.54831 loss)
I0521 00:25:30.673439  9747 sgd_solver.cpp:106] Iteration 3234, lr = 0.0025
I0521 00:25:38.673635  9747 solver.cpp:237] Iteration 3267, loss = 1.53213
I0521 00:25:38.673782  9747 solver.cpp:253]     Train net output #0: loss = 1.53213 (* 1 = 1.53213 loss)
I0521 00:25:38.673796  9747 sgd_solver.cpp:106] Iteration 3267, lr = 0.0025
I0521 00:25:46.671339  9747 solver.cpp:237] Iteration 3300, loss = 1.62749
I0521 00:25:46.671380  9747 solver.cpp:253]     Train net output #0: loss = 1.62749 (* 1 = 1.62749 loss)
I0521 00:25:46.671397  9747 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0521 00:25:53.708142  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_3330.caffemodel
I0521 00:25:53.919086  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_3330.solverstate
I0521 00:25:53.947501  9747 solver.cpp:341] Iteration 3330, Testing net (#0)
I0521 00:26:39.082273  9747 solver.cpp:409]     Test net output #0: accuracy = 0.731071
I0521 00:26:39.082445  9747 solver.cpp:409]     Test net output #1: loss = 0.948728 (* 1 = 0.948728 loss)
I0521 00:26:39.640244  9747 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_3333.caffemodel
I0521 00:26:39.851143  9747 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868_iter_3333.solverstate
I0521 00:27:00.831341  9747 solver.cpp:321] Iteration 3333, loss = 1.54863
I0521 00:27:00.831383  9747 solver.cpp:326] Optimization Done.
I0521 00:27:00.831394  9747 caffe.cpp:215] Optimization Done.
Application 11236132 resources: utime ~1270s, stime ~228s, Rss ~5330592, inblocks ~3744348, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_450_2016-05-20T11.20.48.986868.solver"
	User time (seconds): 0.57
	System time (seconds): 0.11
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:02.29
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15074
	Voluntary context switches: 2757
	Involuntary context switches: 71
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
