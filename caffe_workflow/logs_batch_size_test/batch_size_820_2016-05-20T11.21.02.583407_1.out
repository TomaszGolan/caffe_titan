2806347
I0521 07:50:10.639330 17623 caffe.cpp:184] Using GPUs 0
I0521 07:50:11.065647 17623 solver.cpp:48] Initializing solver from parameters: 
test_iter: 182
test_interval: 365
base_lr: 0.0025
display: 18
max_iter: 1829
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 182
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407.prototxt"
I0521 07:50:11.067317 17623 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407.prototxt
I0521 07:50:11.081960 17623 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 07:50:11.082020 17623 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 07:50:11.082367 17623 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 820
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:50:11.082543 17623 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:50:11.082567 17623 net.cpp:106] Creating Layer data_hdf5
I0521 07:50:11.082582 17623 net.cpp:411] data_hdf5 -> data
I0521 07:50:11.082617 17623 net.cpp:411] data_hdf5 -> label
I0521 07:50:11.082648 17623 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 07:50:11.083968 17623 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 07:50:11.086192 17623 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 07:50:32.591505 17623 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 07:50:32.596711 17623 net.cpp:150] Setting up data_hdf5
I0521 07:50:32.596752 17623 net.cpp:157] Top shape: 820 1 127 50 (5207000)
I0521 07:50:32.596766 17623 net.cpp:157] Top shape: 820 (820)
I0521 07:50:32.596778 17623 net.cpp:165] Memory required for data: 20831280
I0521 07:50:32.596792 17623 layer_factory.hpp:77] Creating layer conv1
I0521 07:50:32.596825 17623 net.cpp:106] Creating Layer conv1
I0521 07:50:32.596837 17623 net.cpp:454] conv1 <- data
I0521 07:50:32.596858 17623 net.cpp:411] conv1 -> conv1
I0521 07:50:33.629634 17623 net.cpp:150] Setting up conv1
I0521 07:50:33.629680 17623 net.cpp:157] Top shape: 820 12 120 48 (56678400)
I0521 07:50:33.629691 17623 net.cpp:165] Memory required for data: 247544880
I0521 07:50:33.629721 17623 layer_factory.hpp:77] Creating layer relu1
I0521 07:50:33.629744 17623 net.cpp:106] Creating Layer relu1
I0521 07:50:33.629755 17623 net.cpp:454] relu1 <- conv1
I0521 07:50:33.629767 17623 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:50:33.630290 17623 net.cpp:150] Setting up relu1
I0521 07:50:33.630306 17623 net.cpp:157] Top shape: 820 12 120 48 (56678400)
I0521 07:50:33.630317 17623 net.cpp:165] Memory required for data: 474258480
I0521 07:50:33.630327 17623 layer_factory.hpp:77] Creating layer pool1
I0521 07:50:33.630343 17623 net.cpp:106] Creating Layer pool1
I0521 07:50:33.630353 17623 net.cpp:454] pool1 <- conv1
I0521 07:50:33.630367 17623 net.cpp:411] pool1 -> pool1
I0521 07:50:33.630447 17623 net.cpp:150] Setting up pool1
I0521 07:50:33.630461 17623 net.cpp:157] Top shape: 820 12 60 48 (28339200)
I0521 07:50:33.630471 17623 net.cpp:165] Memory required for data: 587615280
I0521 07:50:33.630482 17623 layer_factory.hpp:77] Creating layer conv2
I0521 07:50:33.630504 17623 net.cpp:106] Creating Layer conv2
I0521 07:50:33.630514 17623 net.cpp:454] conv2 <- pool1
I0521 07:50:33.630527 17623 net.cpp:411] conv2 -> conv2
I0521 07:50:33.633195 17623 net.cpp:150] Setting up conv2
I0521 07:50:33.633224 17623 net.cpp:157] Top shape: 820 20 54 46 (40737600)
I0521 07:50:33.633234 17623 net.cpp:165] Memory required for data: 750565680
I0521 07:50:33.633252 17623 layer_factory.hpp:77] Creating layer relu2
I0521 07:50:33.633267 17623 net.cpp:106] Creating Layer relu2
I0521 07:50:33.633277 17623 net.cpp:454] relu2 <- conv2
I0521 07:50:33.633290 17623 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:50:33.633630 17623 net.cpp:150] Setting up relu2
I0521 07:50:33.633643 17623 net.cpp:157] Top shape: 820 20 54 46 (40737600)
I0521 07:50:33.633654 17623 net.cpp:165] Memory required for data: 913516080
I0521 07:50:33.633664 17623 layer_factory.hpp:77] Creating layer pool2
I0521 07:50:33.633677 17623 net.cpp:106] Creating Layer pool2
I0521 07:50:33.633687 17623 net.cpp:454] pool2 <- conv2
I0521 07:50:33.633711 17623 net.cpp:411] pool2 -> pool2
I0521 07:50:33.633780 17623 net.cpp:150] Setting up pool2
I0521 07:50:33.633793 17623 net.cpp:157] Top shape: 820 20 27 46 (20368800)
I0521 07:50:33.633802 17623 net.cpp:165] Memory required for data: 994991280
I0521 07:50:33.633812 17623 layer_factory.hpp:77] Creating layer conv3
I0521 07:50:33.633831 17623 net.cpp:106] Creating Layer conv3
I0521 07:50:33.633841 17623 net.cpp:454] conv3 <- pool2
I0521 07:50:33.633854 17623 net.cpp:411] conv3 -> conv3
I0521 07:50:33.635767 17623 net.cpp:150] Setting up conv3
I0521 07:50:33.635785 17623 net.cpp:157] Top shape: 820 28 22 44 (22225280)
I0521 07:50:33.635797 17623 net.cpp:165] Memory required for data: 1083892400
I0521 07:50:33.635814 17623 layer_factory.hpp:77] Creating layer relu3
I0521 07:50:33.635830 17623 net.cpp:106] Creating Layer relu3
I0521 07:50:33.635840 17623 net.cpp:454] relu3 <- conv3
I0521 07:50:33.635853 17623 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:50:33.636322 17623 net.cpp:150] Setting up relu3
I0521 07:50:33.636338 17623 net.cpp:157] Top shape: 820 28 22 44 (22225280)
I0521 07:50:33.636349 17623 net.cpp:165] Memory required for data: 1172793520
I0521 07:50:33.636359 17623 layer_factory.hpp:77] Creating layer pool3
I0521 07:50:33.636373 17623 net.cpp:106] Creating Layer pool3
I0521 07:50:33.636381 17623 net.cpp:454] pool3 <- conv3
I0521 07:50:33.636394 17623 net.cpp:411] pool3 -> pool3
I0521 07:50:33.636461 17623 net.cpp:150] Setting up pool3
I0521 07:50:33.636474 17623 net.cpp:157] Top shape: 820 28 11 44 (11112640)
I0521 07:50:33.636484 17623 net.cpp:165] Memory required for data: 1217244080
I0521 07:50:33.636493 17623 layer_factory.hpp:77] Creating layer conv4
I0521 07:50:33.636512 17623 net.cpp:106] Creating Layer conv4
I0521 07:50:33.636523 17623 net.cpp:454] conv4 <- pool3
I0521 07:50:33.636535 17623 net.cpp:411] conv4 -> conv4
I0521 07:50:33.639253 17623 net.cpp:150] Setting up conv4
I0521 07:50:33.639281 17623 net.cpp:157] Top shape: 820 36 6 42 (7439040)
I0521 07:50:33.639292 17623 net.cpp:165] Memory required for data: 1247000240
I0521 07:50:33.639307 17623 layer_factory.hpp:77] Creating layer relu4
I0521 07:50:33.639322 17623 net.cpp:106] Creating Layer relu4
I0521 07:50:33.639331 17623 net.cpp:454] relu4 <- conv4
I0521 07:50:33.639343 17623 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:50:33.639816 17623 net.cpp:150] Setting up relu4
I0521 07:50:33.639832 17623 net.cpp:157] Top shape: 820 36 6 42 (7439040)
I0521 07:50:33.639843 17623 net.cpp:165] Memory required for data: 1276756400
I0521 07:50:33.639853 17623 layer_factory.hpp:77] Creating layer pool4
I0521 07:50:33.639866 17623 net.cpp:106] Creating Layer pool4
I0521 07:50:33.639876 17623 net.cpp:454] pool4 <- conv4
I0521 07:50:33.639889 17623 net.cpp:411] pool4 -> pool4
I0521 07:50:33.639957 17623 net.cpp:150] Setting up pool4
I0521 07:50:33.639971 17623 net.cpp:157] Top shape: 820 36 3 42 (3719520)
I0521 07:50:33.639981 17623 net.cpp:165] Memory required for data: 1291634480
I0521 07:50:33.639991 17623 layer_factory.hpp:77] Creating layer ip1
I0521 07:50:33.640012 17623 net.cpp:106] Creating Layer ip1
I0521 07:50:33.640023 17623 net.cpp:454] ip1 <- pool4
I0521 07:50:33.640035 17623 net.cpp:411] ip1 -> ip1
I0521 07:50:33.655408 17623 net.cpp:150] Setting up ip1
I0521 07:50:33.655437 17623 net.cpp:157] Top shape: 820 196 (160720)
I0521 07:50:33.655450 17623 net.cpp:165] Memory required for data: 1292277360
I0521 07:50:33.655477 17623 layer_factory.hpp:77] Creating layer relu5
I0521 07:50:33.655490 17623 net.cpp:106] Creating Layer relu5
I0521 07:50:33.655500 17623 net.cpp:454] relu5 <- ip1
I0521 07:50:33.655514 17623 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:50:33.655855 17623 net.cpp:150] Setting up relu5
I0521 07:50:33.655869 17623 net.cpp:157] Top shape: 820 196 (160720)
I0521 07:50:33.655880 17623 net.cpp:165] Memory required for data: 1292920240
I0521 07:50:33.655890 17623 layer_factory.hpp:77] Creating layer drop1
I0521 07:50:33.655911 17623 net.cpp:106] Creating Layer drop1
I0521 07:50:33.655921 17623 net.cpp:454] drop1 <- ip1
I0521 07:50:33.655946 17623 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:50:33.655993 17623 net.cpp:150] Setting up drop1
I0521 07:50:33.656007 17623 net.cpp:157] Top shape: 820 196 (160720)
I0521 07:50:33.656015 17623 net.cpp:165] Memory required for data: 1293563120
I0521 07:50:33.656025 17623 layer_factory.hpp:77] Creating layer ip2
I0521 07:50:33.656044 17623 net.cpp:106] Creating Layer ip2
I0521 07:50:33.656054 17623 net.cpp:454] ip2 <- ip1
I0521 07:50:33.656067 17623 net.cpp:411] ip2 -> ip2
I0521 07:50:33.656530 17623 net.cpp:150] Setting up ip2
I0521 07:50:33.656543 17623 net.cpp:157] Top shape: 820 98 (80360)
I0521 07:50:33.656553 17623 net.cpp:165] Memory required for data: 1293884560
I0521 07:50:33.656569 17623 layer_factory.hpp:77] Creating layer relu6
I0521 07:50:33.656580 17623 net.cpp:106] Creating Layer relu6
I0521 07:50:33.656590 17623 net.cpp:454] relu6 <- ip2
I0521 07:50:33.656602 17623 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:50:33.657119 17623 net.cpp:150] Setting up relu6
I0521 07:50:33.657135 17623 net.cpp:157] Top shape: 820 98 (80360)
I0521 07:50:33.657145 17623 net.cpp:165] Memory required for data: 1294206000
I0521 07:50:33.657155 17623 layer_factory.hpp:77] Creating layer drop2
I0521 07:50:33.657167 17623 net.cpp:106] Creating Layer drop2
I0521 07:50:33.657178 17623 net.cpp:454] drop2 <- ip2
I0521 07:50:33.657191 17623 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:50:33.657232 17623 net.cpp:150] Setting up drop2
I0521 07:50:33.657245 17623 net.cpp:157] Top shape: 820 98 (80360)
I0521 07:50:33.657255 17623 net.cpp:165] Memory required for data: 1294527440
I0521 07:50:33.657265 17623 layer_factory.hpp:77] Creating layer ip3
I0521 07:50:33.657279 17623 net.cpp:106] Creating Layer ip3
I0521 07:50:33.657289 17623 net.cpp:454] ip3 <- ip2
I0521 07:50:33.657300 17623 net.cpp:411] ip3 -> ip3
I0521 07:50:33.657512 17623 net.cpp:150] Setting up ip3
I0521 07:50:33.657526 17623 net.cpp:157] Top shape: 820 11 (9020)
I0521 07:50:33.657536 17623 net.cpp:165] Memory required for data: 1294563520
I0521 07:50:33.657559 17623 layer_factory.hpp:77] Creating layer drop3
I0521 07:50:33.657572 17623 net.cpp:106] Creating Layer drop3
I0521 07:50:33.657582 17623 net.cpp:454] drop3 <- ip3
I0521 07:50:33.657593 17623 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:50:33.657634 17623 net.cpp:150] Setting up drop3
I0521 07:50:33.657647 17623 net.cpp:157] Top shape: 820 11 (9020)
I0521 07:50:33.657657 17623 net.cpp:165] Memory required for data: 1294599600
I0521 07:50:33.657667 17623 layer_factory.hpp:77] Creating layer loss
I0521 07:50:33.657686 17623 net.cpp:106] Creating Layer loss
I0521 07:50:33.657697 17623 net.cpp:454] loss <- ip3
I0521 07:50:33.657708 17623 net.cpp:454] loss <- label
I0521 07:50:33.657721 17623 net.cpp:411] loss -> loss
I0521 07:50:33.657738 17623 layer_factory.hpp:77] Creating layer loss
I0521 07:50:33.658390 17623 net.cpp:150] Setting up loss
I0521 07:50:33.658406 17623 net.cpp:157] Top shape: (1)
I0521 07:50:33.658419 17623 net.cpp:160]     with loss weight 1
I0521 07:50:33.658463 17623 net.cpp:165] Memory required for data: 1294599604
I0521 07:50:33.658473 17623 net.cpp:226] loss needs backward computation.
I0521 07:50:33.658484 17623 net.cpp:226] drop3 needs backward computation.
I0521 07:50:33.658493 17623 net.cpp:226] ip3 needs backward computation.
I0521 07:50:33.658504 17623 net.cpp:226] drop2 needs backward computation.
I0521 07:50:33.658514 17623 net.cpp:226] relu6 needs backward computation.
I0521 07:50:33.658524 17623 net.cpp:226] ip2 needs backward computation.
I0521 07:50:33.658534 17623 net.cpp:226] drop1 needs backward computation.
I0521 07:50:33.658543 17623 net.cpp:226] relu5 needs backward computation.
I0521 07:50:33.658552 17623 net.cpp:226] ip1 needs backward computation.
I0521 07:50:33.658562 17623 net.cpp:226] pool4 needs backward computation.
I0521 07:50:33.658572 17623 net.cpp:226] relu4 needs backward computation.
I0521 07:50:33.658582 17623 net.cpp:226] conv4 needs backward computation.
I0521 07:50:33.658592 17623 net.cpp:226] pool3 needs backward computation.
I0521 07:50:33.658612 17623 net.cpp:226] relu3 needs backward computation.
I0521 07:50:33.658622 17623 net.cpp:226] conv3 needs backward computation.
I0521 07:50:33.658633 17623 net.cpp:226] pool2 needs backward computation.
I0521 07:50:33.658643 17623 net.cpp:226] relu2 needs backward computation.
I0521 07:50:33.658653 17623 net.cpp:226] conv2 needs backward computation.
I0521 07:50:33.658664 17623 net.cpp:226] pool1 needs backward computation.
I0521 07:50:33.658674 17623 net.cpp:226] relu1 needs backward computation.
I0521 07:50:33.658684 17623 net.cpp:226] conv1 needs backward computation.
I0521 07:50:33.658695 17623 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:50:33.658704 17623 net.cpp:270] This network produces output loss
I0521 07:50:33.658728 17623 net.cpp:283] Network initialization done.
I0521 07:50:33.660398 17623 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407.prototxt
I0521 07:50:33.660470 17623 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 07:50:33.660825 17623 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 820
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 07:50:33.661015 17623 layer_factory.hpp:77] Creating layer data_hdf5
I0521 07:50:33.661031 17623 net.cpp:106] Creating Layer data_hdf5
I0521 07:50:33.661042 17623 net.cpp:411] data_hdf5 -> data
I0521 07:50:33.661059 17623 net.cpp:411] data_hdf5 -> label
I0521 07:50:33.661074 17623 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 07:50:33.662245 17623 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 07:50:54.920850 17623 net.cpp:150] Setting up data_hdf5
I0521 07:50:54.921013 17623 net.cpp:157] Top shape: 820 1 127 50 (5207000)
I0521 07:50:54.921028 17623 net.cpp:157] Top shape: 820 (820)
I0521 07:50:54.921041 17623 net.cpp:165] Memory required for data: 20831280
I0521 07:50:54.921056 17623 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 07:50:54.921082 17623 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 07:50:54.921093 17623 net.cpp:454] label_data_hdf5_1_split <- label
I0521 07:50:54.921108 17623 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 07:50:54.921129 17623 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 07:50:54.921203 17623 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 07:50:54.921217 17623 net.cpp:157] Top shape: 820 (820)
I0521 07:50:54.921229 17623 net.cpp:157] Top shape: 820 (820)
I0521 07:50:54.921238 17623 net.cpp:165] Memory required for data: 20837840
I0521 07:50:54.921249 17623 layer_factory.hpp:77] Creating layer conv1
I0521 07:50:54.921272 17623 net.cpp:106] Creating Layer conv1
I0521 07:50:54.921281 17623 net.cpp:454] conv1 <- data
I0521 07:50:54.921296 17623 net.cpp:411] conv1 -> conv1
I0521 07:50:54.923265 17623 net.cpp:150] Setting up conv1
I0521 07:50:54.923290 17623 net.cpp:157] Top shape: 820 12 120 48 (56678400)
I0521 07:50:54.923300 17623 net.cpp:165] Memory required for data: 247551440
I0521 07:50:54.923321 17623 layer_factory.hpp:77] Creating layer relu1
I0521 07:50:54.923336 17623 net.cpp:106] Creating Layer relu1
I0521 07:50:54.923346 17623 net.cpp:454] relu1 <- conv1
I0521 07:50:54.923359 17623 net.cpp:397] relu1 -> conv1 (in-place)
I0521 07:50:54.923854 17623 net.cpp:150] Setting up relu1
I0521 07:50:54.923871 17623 net.cpp:157] Top shape: 820 12 120 48 (56678400)
I0521 07:50:54.923880 17623 net.cpp:165] Memory required for data: 474265040
I0521 07:50:54.923890 17623 layer_factory.hpp:77] Creating layer pool1
I0521 07:50:54.923907 17623 net.cpp:106] Creating Layer pool1
I0521 07:50:54.923916 17623 net.cpp:454] pool1 <- conv1
I0521 07:50:54.923929 17623 net.cpp:411] pool1 -> pool1
I0521 07:50:54.924005 17623 net.cpp:150] Setting up pool1
I0521 07:50:54.924017 17623 net.cpp:157] Top shape: 820 12 60 48 (28339200)
I0521 07:50:54.924027 17623 net.cpp:165] Memory required for data: 587621840
I0521 07:50:54.924038 17623 layer_factory.hpp:77] Creating layer conv2
I0521 07:50:54.924057 17623 net.cpp:106] Creating Layer conv2
I0521 07:50:54.924067 17623 net.cpp:454] conv2 <- pool1
I0521 07:50:54.924079 17623 net.cpp:411] conv2 -> conv2
I0521 07:50:54.926002 17623 net.cpp:150] Setting up conv2
I0521 07:50:54.926023 17623 net.cpp:157] Top shape: 820 20 54 46 (40737600)
I0521 07:50:54.926036 17623 net.cpp:165] Memory required for data: 750572240
I0521 07:50:54.926054 17623 layer_factory.hpp:77] Creating layer relu2
I0521 07:50:54.926067 17623 net.cpp:106] Creating Layer relu2
I0521 07:50:54.926077 17623 net.cpp:454] relu2 <- conv2
I0521 07:50:54.926090 17623 net.cpp:397] relu2 -> conv2 (in-place)
I0521 07:50:54.926424 17623 net.cpp:150] Setting up relu2
I0521 07:50:54.926439 17623 net.cpp:157] Top shape: 820 20 54 46 (40737600)
I0521 07:50:54.926448 17623 net.cpp:165] Memory required for data: 913522640
I0521 07:50:54.926458 17623 layer_factory.hpp:77] Creating layer pool2
I0521 07:50:54.926471 17623 net.cpp:106] Creating Layer pool2
I0521 07:50:54.926481 17623 net.cpp:454] pool2 <- conv2
I0521 07:50:54.926494 17623 net.cpp:411] pool2 -> pool2
I0521 07:50:54.926564 17623 net.cpp:150] Setting up pool2
I0521 07:50:54.926578 17623 net.cpp:157] Top shape: 820 20 27 46 (20368800)
I0521 07:50:54.926587 17623 net.cpp:165] Memory required for data: 994997840
I0521 07:50:54.926597 17623 layer_factory.hpp:77] Creating layer conv3
I0521 07:50:54.926617 17623 net.cpp:106] Creating Layer conv3
I0521 07:50:54.926628 17623 net.cpp:454] conv3 <- pool2
I0521 07:50:54.926641 17623 net.cpp:411] conv3 -> conv3
I0521 07:50:54.928616 17623 net.cpp:150] Setting up conv3
I0521 07:50:54.928638 17623 net.cpp:157] Top shape: 820 28 22 44 (22225280)
I0521 07:50:54.928649 17623 net.cpp:165] Memory required for data: 1083898960
I0521 07:50:54.928683 17623 layer_factory.hpp:77] Creating layer relu3
I0521 07:50:54.928696 17623 net.cpp:106] Creating Layer relu3
I0521 07:50:54.928706 17623 net.cpp:454] relu3 <- conv3
I0521 07:50:54.928719 17623 net.cpp:397] relu3 -> conv3 (in-place)
I0521 07:50:54.929189 17623 net.cpp:150] Setting up relu3
I0521 07:50:54.929206 17623 net.cpp:157] Top shape: 820 28 22 44 (22225280)
I0521 07:50:54.929215 17623 net.cpp:165] Memory required for data: 1172800080
I0521 07:50:54.929225 17623 layer_factory.hpp:77] Creating layer pool3
I0521 07:50:54.929239 17623 net.cpp:106] Creating Layer pool3
I0521 07:50:54.929250 17623 net.cpp:454] pool3 <- conv3
I0521 07:50:54.929262 17623 net.cpp:411] pool3 -> pool3
I0521 07:50:54.929333 17623 net.cpp:150] Setting up pool3
I0521 07:50:54.929347 17623 net.cpp:157] Top shape: 820 28 11 44 (11112640)
I0521 07:50:54.929357 17623 net.cpp:165] Memory required for data: 1217250640
I0521 07:50:54.929364 17623 layer_factory.hpp:77] Creating layer conv4
I0521 07:50:54.929383 17623 net.cpp:106] Creating Layer conv4
I0521 07:50:54.929394 17623 net.cpp:454] conv4 <- pool3
I0521 07:50:54.929407 17623 net.cpp:411] conv4 -> conv4
I0521 07:50:54.931478 17623 net.cpp:150] Setting up conv4
I0521 07:50:54.931496 17623 net.cpp:157] Top shape: 820 36 6 42 (7439040)
I0521 07:50:54.931506 17623 net.cpp:165] Memory required for data: 1247006800
I0521 07:50:54.931521 17623 layer_factory.hpp:77] Creating layer relu4
I0521 07:50:54.931535 17623 net.cpp:106] Creating Layer relu4
I0521 07:50:54.931545 17623 net.cpp:454] relu4 <- conv4
I0521 07:50:54.931557 17623 net.cpp:397] relu4 -> conv4 (in-place)
I0521 07:50:54.932030 17623 net.cpp:150] Setting up relu4
I0521 07:50:54.932046 17623 net.cpp:157] Top shape: 820 36 6 42 (7439040)
I0521 07:50:54.932056 17623 net.cpp:165] Memory required for data: 1276762960
I0521 07:50:54.932066 17623 layer_factory.hpp:77] Creating layer pool4
I0521 07:50:54.932080 17623 net.cpp:106] Creating Layer pool4
I0521 07:50:54.932090 17623 net.cpp:454] pool4 <- conv4
I0521 07:50:54.932102 17623 net.cpp:411] pool4 -> pool4
I0521 07:50:54.932173 17623 net.cpp:150] Setting up pool4
I0521 07:50:54.932188 17623 net.cpp:157] Top shape: 820 36 3 42 (3719520)
I0521 07:50:54.932198 17623 net.cpp:165] Memory required for data: 1291641040
I0521 07:50:54.932209 17623 layer_factory.hpp:77] Creating layer ip1
I0521 07:50:54.932224 17623 net.cpp:106] Creating Layer ip1
I0521 07:50:54.932234 17623 net.cpp:454] ip1 <- pool4
I0521 07:50:54.932246 17623 net.cpp:411] ip1 -> ip1
I0521 07:50:54.947749 17623 net.cpp:150] Setting up ip1
I0521 07:50:54.947772 17623 net.cpp:157] Top shape: 820 196 (160720)
I0521 07:50:54.947783 17623 net.cpp:165] Memory required for data: 1292283920
I0521 07:50:54.947804 17623 layer_factory.hpp:77] Creating layer relu5
I0521 07:50:54.947819 17623 net.cpp:106] Creating Layer relu5
I0521 07:50:54.947830 17623 net.cpp:454] relu5 <- ip1
I0521 07:50:54.947844 17623 net.cpp:397] relu5 -> ip1 (in-place)
I0521 07:50:54.948189 17623 net.cpp:150] Setting up relu5
I0521 07:50:54.948202 17623 net.cpp:157] Top shape: 820 196 (160720)
I0521 07:50:54.948212 17623 net.cpp:165] Memory required for data: 1292926800
I0521 07:50:54.948222 17623 layer_factory.hpp:77] Creating layer drop1
I0521 07:50:54.948241 17623 net.cpp:106] Creating Layer drop1
I0521 07:50:54.948251 17623 net.cpp:454] drop1 <- ip1
I0521 07:50:54.948264 17623 net.cpp:397] drop1 -> ip1 (in-place)
I0521 07:50:54.948308 17623 net.cpp:150] Setting up drop1
I0521 07:50:54.948321 17623 net.cpp:157] Top shape: 820 196 (160720)
I0521 07:50:54.948331 17623 net.cpp:165] Memory required for data: 1293569680
I0521 07:50:54.948340 17623 layer_factory.hpp:77] Creating layer ip2
I0521 07:50:54.948355 17623 net.cpp:106] Creating Layer ip2
I0521 07:50:54.948365 17623 net.cpp:454] ip2 <- ip1
I0521 07:50:54.948379 17623 net.cpp:411] ip2 -> ip2
I0521 07:50:54.948859 17623 net.cpp:150] Setting up ip2
I0521 07:50:54.948873 17623 net.cpp:157] Top shape: 820 98 (80360)
I0521 07:50:54.948882 17623 net.cpp:165] Memory required for data: 1293891120
I0521 07:50:54.948910 17623 layer_factory.hpp:77] Creating layer relu6
I0521 07:50:54.948923 17623 net.cpp:106] Creating Layer relu6
I0521 07:50:54.948933 17623 net.cpp:454] relu6 <- ip2
I0521 07:50:54.948945 17623 net.cpp:397] relu6 -> ip2 (in-place)
I0521 07:50:54.949483 17623 net.cpp:150] Setting up relu6
I0521 07:50:54.949504 17623 net.cpp:157] Top shape: 820 98 (80360)
I0521 07:50:54.949514 17623 net.cpp:165] Memory required for data: 1294212560
I0521 07:50:54.949523 17623 layer_factory.hpp:77] Creating layer drop2
I0521 07:50:54.949537 17623 net.cpp:106] Creating Layer drop2
I0521 07:50:54.949553 17623 net.cpp:454] drop2 <- ip2
I0521 07:50:54.949568 17623 net.cpp:397] drop2 -> ip2 (in-place)
I0521 07:50:54.949611 17623 net.cpp:150] Setting up drop2
I0521 07:50:54.949625 17623 net.cpp:157] Top shape: 820 98 (80360)
I0521 07:50:54.949635 17623 net.cpp:165] Memory required for data: 1294534000
I0521 07:50:54.949645 17623 layer_factory.hpp:77] Creating layer ip3
I0521 07:50:54.949658 17623 net.cpp:106] Creating Layer ip3
I0521 07:50:54.949668 17623 net.cpp:454] ip3 <- ip2
I0521 07:50:54.949681 17623 net.cpp:411] ip3 -> ip3
I0521 07:50:54.949905 17623 net.cpp:150] Setting up ip3
I0521 07:50:54.949918 17623 net.cpp:157] Top shape: 820 11 (9020)
I0521 07:50:54.949928 17623 net.cpp:165] Memory required for data: 1294570080
I0521 07:50:54.949944 17623 layer_factory.hpp:77] Creating layer drop3
I0521 07:50:54.949956 17623 net.cpp:106] Creating Layer drop3
I0521 07:50:54.949966 17623 net.cpp:454] drop3 <- ip3
I0521 07:50:54.949978 17623 net.cpp:397] drop3 -> ip3 (in-place)
I0521 07:50:54.950021 17623 net.cpp:150] Setting up drop3
I0521 07:50:54.950033 17623 net.cpp:157] Top shape: 820 11 (9020)
I0521 07:50:54.950042 17623 net.cpp:165] Memory required for data: 1294606160
I0521 07:50:54.950052 17623 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 07:50:54.950065 17623 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 07:50:54.950075 17623 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 07:50:54.950088 17623 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 07:50:54.950103 17623 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 07:50:54.950176 17623 net.cpp:150] Setting up ip3_drop3_0_split
I0521 07:50:54.950189 17623 net.cpp:157] Top shape: 820 11 (9020)
I0521 07:50:54.950201 17623 net.cpp:157] Top shape: 820 11 (9020)
I0521 07:50:54.950211 17623 net.cpp:165] Memory required for data: 1294678320
I0521 07:50:54.950218 17623 layer_factory.hpp:77] Creating layer accuracy
I0521 07:50:54.950239 17623 net.cpp:106] Creating Layer accuracy
I0521 07:50:54.950250 17623 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 07:50:54.950261 17623 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 07:50:54.950275 17623 net.cpp:411] accuracy -> accuracy
I0521 07:50:54.950299 17623 net.cpp:150] Setting up accuracy
I0521 07:50:54.950311 17623 net.cpp:157] Top shape: (1)
I0521 07:50:54.950321 17623 net.cpp:165] Memory required for data: 1294678324
I0521 07:50:54.950331 17623 layer_factory.hpp:77] Creating layer loss
I0521 07:50:54.950346 17623 net.cpp:106] Creating Layer loss
I0521 07:50:54.950356 17623 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 07:50:54.950367 17623 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 07:50:54.950381 17623 net.cpp:411] loss -> loss
I0521 07:50:54.950397 17623 layer_factory.hpp:77] Creating layer loss
I0521 07:50:54.950894 17623 net.cpp:150] Setting up loss
I0521 07:50:54.950907 17623 net.cpp:157] Top shape: (1)
I0521 07:50:54.950917 17623 net.cpp:160]     with loss weight 1
I0521 07:50:54.950935 17623 net.cpp:165] Memory required for data: 1294678328
I0521 07:50:54.950945 17623 net.cpp:226] loss needs backward computation.
I0521 07:50:54.950956 17623 net.cpp:228] accuracy does not need backward computation.
I0521 07:50:54.950968 17623 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 07:50:54.950978 17623 net.cpp:226] drop3 needs backward computation.
I0521 07:50:54.950986 17623 net.cpp:226] ip3 needs backward computation.
I0521 07:50:54.950996 17623 net.cpp:226] drop2 needs backward computation.
I0521 07:50:54.951015 17623 net.cpp:226] relu6 needs backward computation.
I0521 07:50:54.951025 17623 net.cpp:226] ip2 needs backward computation.
I0521 07:50:54.951035 17623 net.cpp:226] drop1 needs backward computation.
I0521 07:50:54.951045 17623 net.cpp:226] relu5 needs backward computation.
I0521 07:50:54.951055 17623 net.cpp:226] ip1 needs backward computation.
I0521 07:50:54.951064 17623 net.cpp:226] pool4 needs backward computation.
I0521 07:50:54.951074 17623 net.cpp:226] relu4 needs backward computation.
I0521 07:50:54.951083 17623 net.cpp:226] conv4 needs backward computation.
I0521 07:50:54.951094 17623 net.cpp:226] pool3 needs backward computation.
I0521 07:50:54.951104 17623 net.cpp:226] relu3 needs backward computation.
I0521 07:50:54.951112 17623 net.cpp:226] conv3 needs backward computation.
I0521 07:50:54.951123 17623 net.cpp:226] pool2 needs backward computation.
I0521 07:50:54.951133 17623 net.cpp:226] relu2 needs backward computation.
I0521 07:50:54.951144 17623 net.cpp:226] conv2 needs backward computation.
I0521 07:50:54.951154 17623 net.cpp:226] pool1 needs backward computation.
I0521 07:50:54.951164 17623 net.cpp:226] relu1 needs backward computation.
I0521 07:50:54.951174 17623 net.cpp:226] conv1 needs backward computation.
I0521 07:50:54.951185 17623 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 07:50:54.951197 17623 net.cpp:228] data_hdf5 does not need backward computation.
I0521 07:50:54.951207 17623 net.cpp:270] This network produces output accuracy
I0521 07:50:54.951218 17623 net.cpp:270] This network produces output loss
I0521 07:50:54.951246 17623 net.cpp:283] Network initialization done.
I0521 07:50:54.951380 17623 solver.cpp:60] Solver scaffolding done.
I0521 07:50:54.952507 17623 caffe.cpp:212] Starting Optimization
I0521 07:50:54.952527 17623 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 07:50:54.952539 17623 solver.cpp:289] Learning Rate Policy: fixed
I0521 07:50:54.953780 17623 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 07:51:40.820143 17623 solver.cpp:409]     Test net output #0: accuracy = 0.0615586
I0521 07:51:40.820305 17623 solver.cpp:409]     Test net output #1: loss = 2.39874 (* 1 = 2.39874 loss)
I0521 07:51:40.972360 17623 solver.cpp:237] Iteration 0, loss = 2.39832
I0521 07:51:40.972395 17623 solver.cpp:253]     Train net output #0: loss = 2.39832 (* 1 = 2.39832 loss)
I0521 07:51:40.972417 17623 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 07:51:48.899083 17623 solver.cpp:237] Iteration 18, loss = 2.38871
I0521 07:51:48.899118 17623 solver.cpp:253]     Train net output #0: loss = 2.38871 (* 1 = 2.38871 loss)
I0521 07:51:48.899133 17623 sgd_solver.cpp:106] Iteration 18, lr = 0.0025
I0521 07:51:56.825528 17623 solver.cpp:237] Iteration 36, loss = 2.37635
I0521 07:51:56.825562 17623 solver.cpp:253]     Train net output #0: loss = 2.37635 (* 1 = 2.37635 loss)
I0521 07:51:56.825579 17623 sgd_solver.cpp:106] Iteration 36, lr = 0.0025
I0521 07:52:04.751571 17623 solver.cpp:237] Iteration 54, loss = 2.36292
I0521 07:52:04.751603 17623 solver.cpp:253]     Train net output #0: loss = 2.36292 (* 1 = 2.36292 loss)
I0521 07:52:04.751616 17623 sgd_solver.cpp:106] Iteration 54, lr = 0.0025
I0521 07:52:12.674479 17623 solver.cpp:237] Iteration 72, loss = 2.35695
I0521 07:52:12.674623 17623 solver.cpp:253]     Train net output #0: loss = 2.35695 (* 1 = 2.35695 loss)
I0521 07:52:12.674638 17623 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0521 07:52:20.596670 17623 solver.cpp:237] Iteration 90, loss = 2.33987
I0521 07:52:20.596707 17623 solver.cpp:253]     Train net output #0: loss = 2.33987 (* 1 = 2.33987 loss)
I0521 07:52:20.596724 17623 sgd_solver.cpp:106] Iteration 90, lr = 0.0025
I0521 07:52:28.526322 17623 solver.cpp:237] Iteration 108, loss = 2.33569
I0521 07:52:28.526355 17623 solver.cpp:253]     Train net output #0: loss = 2.33569 (* 1 = 2.33569 loss)
I0521 07:52:28.526371 17623 sgd_solver.cpp:106] Iteration 108, lr = 0.0025
I0521 07:52:58.491969 17623 solver.cpp:237] Iteration 126, loss = 2.32754
I0521 07:52:58.492130 17623 solver.cpp:253]     Train net output #0: loss = 2.32754 (* 1 = 2.32754 loss)
I0521 07:52:58.492144 17623 sgd_solver.cpp:106] Iteration 126, lr = 0.0025
I0521 07:53:06.424553 17623 solver.cpp:237] Iteration 144, loss = 2.32438
I0521 07:53:06.424599 17623 solver.cpp:253]     Train net output #0: loss = 2.32438 (* 1 = 2.32438 loss)
I0521 07:53:06.424615 17623 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 07:53:14.352008 17623 solver.cpp:237] Iteration 162, loss = 2.3538
I0521 07:53:14.352041 17623 solver.cpp:253]     Train net output #0: loss = 2.3538 (* 1 = 2.3538 loss)
I0521 07:53:14.352057 17623 sgd_solver.cpp:106] Iteration 162, lr = 0.0025
I0521 07:53:22.274796 17623 solver.cpp:237] Iteration 180, loss = 2.33411
I0521 07:53:22.274829 17623 solver.cpp:253]     Train net output #0: loss = 2.33411 (* 1 = 2.33411 loss)
I0521 07:53:22.274847 17623 sgd_solver.cpp:106] Iteration 180, lr = 0.0025
I0521 07:53:22.716469 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_182.caffemodel
I0521 07:53:23.067775 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_182.solverstate
I0521 07:53:30.268702 17623 solver.cpp:237] Iteration 198, loss = 2.30619
I0521 07:53:30.268857 17623 solver.cpp:253]     Train net output #0: loss = 2.30619 (* 1 = 2.30619 loss)
I0521 07:53:30.268870 17623 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 07:53:38.193603 17623 solver.cpp:237] Iteration 216, loss = 2.29282
I0521 07:53:38.193635 17623 solver.cpp:253]     Train net output #0: loss = 2.29282 (* 1 = 2.29282 loss)
I0521 07:53:38.193653 17623 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 07:53:46.117638 17623 solver.cpp:237] Iteration 234, loss = 2.3065
I0521 07:53:46.117671 17623 solver.cpp:253]     Train net output #0: loss = 2.3065 (* 1 = 2.3065 loss)
I0521 07:53:46.117686 17623 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0521 07:54:16.101579 17623 solver.cpp:237] Iteration 252, loss = 2.2814
I0521 07:54:16.101734 17623 solver.cpp:253]     Train net output #0: loss = 2.2814 (* 1 = 2.2814 loss)
I0521 07:54:16.101748 17623 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0521 07:54:24.031575 17623 solver.cpp:237] Iteration 270, loss = 2.27288
I0521 07:54:24.031618 17623 solver.cpp:253]     Train net output #0: loss = 2.27288 (* 1 = 2.27288 loss)
I0521 07:54:24.031636 17623 sgd_solver.cpp:106] Iteration 270, lr = 0.0025
I0521 07:54:31.954887 17623 solver.cpp:237] Iteration 288, loss = 2.27164
I0521 07:54:31.954921 17623 solver.cpp:253]     Train net output #0: loss = 2.27164 (* 1 = 2.27164 loss)
I0521 07:54:31.954937 17623 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 07:54:39.883643 17623 solver.cpp:237] Iteration 306, loss = 2.23284
I0521 07:54:39.883677 17623 solver.cpp:253]     Train net output #0: loss = 2.23284 (* 1 = 2.23284 loss)
I0521 07:54:39.883694 17623 sgd_solver.cpp:106] Iteration 306, lr = 0.0025
I0521 07:54:47.811208 17623 solver.cpp:237] Iteration 324, loss = 2.21838
I0521 07:54:47.811372 17623 solver.cpp:253]     Train net output #0: loss = 2.21838 (* 1 = 2.21838 loss)
I0521 07:54:47.811388 17623 sgd_solver.cpp:106] Iteration 324, lr = 0.0025
I0521 07:54:55.742657 17623 solver.cpp:237] Iteration 342, loss = 2.19916
I0521 07:54:55.742691 17623 solver.cpp:253]     Train net output #0: loss = 2.19916 (* 1 = 2.19916 loss)
I0521 07:54:55.742707 17623 sgd_solver.cpp:106] Iteration 342, lr = 0.0025
I0521 07:55:03.672471 17623 solver.cpp:237] Iteration 360, loss = 2.17218
I0521 07:55:03.672502 17623 solver.cpp:253]     Train net output #0: loss = 2.17218 (* 1 = 2.17218 loss)
I0521 07:55:03.672519 17623 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 07:55:04.990708 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_364.caffemodel
I0521 07:55:05.339148 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_364.solverstate
I0521 07:55:05.496229 17623 solver.cpp:341] Iteration 365, Testing net (#0)
I0521 07:55:50.731802 17623 solver.cpp:409]     Test net output #0: accuracy = 0.441269
I0521 07:55:50.731976 17623 solver.cpp:409]     Test net output #1: loss = 2.00714 (* 1 = 2.00714 loss)
I0521 07:56:18.672291 17623 solver.cpp:237] Iteration 378, loss = 2.11221
I0521 07:56:18.672343 17623 solver.cpp:253]     Train net output #0: loss = 2.11221 (* 1 = 2.11221 loss)
I0521 07:56:18.672356 17623 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0521 07:56:26.593580 17623 solver.cpp:237] Iteration 396, loss = 2.09393
I0521 07:56:26.593739 17623 solver.cpp:253]     Train net output #0: loss = 2.09393 (* 1 = 2.09393 loss)
I0521 07:56:26.593751 17623 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 07:56:34.513792 17623 solver.cpp:237] Iteration 414, loss = 2.09391
I0521 07:56:34.513823 17623 solver.cpp:253]     Train net output #0: loss = 2.09391 (* 1 = 2.09391 loss)
I0521 07:56:34.513840 17623 sgd_solver.cpp:106] Iteration 414, lr = 0.0025
I0521 07:56:42.432629 17623 solver.cpp:237] Iteration 432, loss = 2.08578
I0521 07:56:42.432662 17623 solver.cpp:253]     Train net output #0: loss = 2.08578 (* 1 = 2.08578 loss)
I0521 07:56:42.432677 17623 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 07:56:50.358225 17623 solver.cpp:237] Iteration 450, loss = 2.06154
I0521 07:56:50.358273 17623 solver.cpp:253]     Train net output #0: loss = 2.06154 (* 1 = 2.06154 loss)
I0521 07:56:50.358288 17623 sgd_solver.cpp:106] Iteration 450, lr = 0.0025
I0521 07:56:58.287626 17623 solver.cpp:237] Iteration 468, loss = 2.05528
I0521 07:56:58.287761 17623 solver.cpp:253]     Train net output #0: loss = 2.05528 (* 1 = 2.05528 loss)
I0521 07:56:58.287775 17623 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0521 07:57:06.211896 17623 solver.cpp:237] Iteration 486, loss = 2.03706
I0521 07:57:06.211927 17623 solver.cpp:253]     Train net output #0: loss = 2.03706 (* 1 = 2.03706 loss)
I0521 07:57:06.211946 17623 sgd_solver.cpp:106] Iteration 486, lr = 0.0025
I0521 07:57:36.262087 17623 solver.cpp:237] Iteration 504, loss = 2.02779
I0521 07:57:36.262260 17623 solver.cpp:253]     Train net output #0: loss = 2.02779 (* 1 = 2.02779 loss)
I0521 07:57:36.262275 17623 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 07:57:44.188571 17623 solver.cpp:237] Iteration 522, loss = 1.97334
I0521 07:57:44.188607 17623 solver.cpp:253]     Train net output #0: loss = 1.97334 (* 1 = 1.97334 loss)
I0521 07:57:44.188626 17623 sgd_solver.cpp:106] Iteration 522, lr = 0.0025
I0521 07:57:52.113427 17623 solver.cpp:237] Iteration 540, loss = 1.89308
I0521 07:57:52.113459 17623 solver.cpp:253]     Train net output #0: loss = 1.89308 (* 1 = 1.89308 loss)
I0521 07:57:52.113476 17623 sgd_solver.cpp:106] Iteration 540, lr = 0.0025
I0521 07:57:54.312970 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_546.caffemodel
I0521 07:57:54.661458 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_546.solverstate
I0521 07:58:00.103750 17623 solver.cpp:237] Iteration 558, loss = 1.93607
I0521 07:58:00.103796 17623 solver.cpp:253]     Train net output #0: loss = 1.93607 (* 1 = 1.93607 loss)
I0521 07:58:00.103814 17623 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0521 07:58:08.022585 17623 solver.cpp:237] Iteration 576, loss = 1.93535
I0521 07:58:08.022740 17623 solver.cpp:253]     Train net output #0: loss = 1.93535 (* 1 = 1.93535 loss)
I0521 07:58:08.022754 17623 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 07:58:15.951385 17623 solver.cpp:237] Iteration 594, loss = 1.88422
I0521 07:58:15.951417 17623 solver.cpp:253]     Train net output #0: loss = 1.88422 (* 1 = 1.88422 loss)
I0521 07:58:15.951434 17623 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 07:58:46.028995 17623 solver.cpp:237] Iteration 612, loss = 1.94214
I0521 07:58:46.029155 17623 solver.cpp:253]     Train net output #0: loss = 1.94214 (* 1 = 1.94214 loss)
I0521 07:58:46.029173 17623 sgd_solver.cpp:106] Iteration 612, lr = 0.0025
I0521 07:58:53.948207 17623 solver.cpp:237] Iteration 630, loss = 1.91131
I0521 07:58:53.948240 17623 solver.cpp:253]     Train net output #0: loss = 1.91131 (* 1 = 1.91131 loss)
I0521 07:58:53.948256 17623 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0521 07:59:01.870790 17623 solver.cpp:237] Iteration 648, loss = 1.86288
I0521 07:59:01.870825 17623 solver.cpp:253]     Train net output #0: loss = 1.86288 (* 1 = 1.86288 loss)
I0521 07:59:01.870841 17623 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 07:59:09.791589 17623 solver.cpp:237] Iteration 666, loss = 1.83442
I0521 07:59:09.791616 17623 solver.cpp:253]     Train net output #0: loss = 1.83442 (* 1 = 1.83442 loss)
I0521 07:59:09.791630 17623 sgd_solver.cpp:106] Iteration 666, lr = 0.0025
I0521 07:59:17.710733 17623 solver.cpp:237] Iteration 684, loss = 1.91158
I0521 07:59:17.710865 17623 solver.cpp:253]     Train net output #0: loss = 1.91158 (* 1 = 1.91158 loss)
I0521 07:59:17.710878 17623 sgd_solver.cpp:106] Iteration 684, lr = 0.0025
I0521 07:59:25.632802 17623 solver.cpp:237] Iteration 702, loss = 1.86626
I0521 07:59:25.632833 17623 solver.cpp:253]     Train net output #0: loss = 1.86626 (* 1 = 1.86626 loss)
I0521 07:59:25.632844 17623 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0521 07:59:33.551009 17623 solver.cpp:237] Iteration 720, loss = 1.86677
I0521 07:59:33.551043 17623 solver.cpp:253]     Train net output #0: loss = 1.86677 (* 1 = 1.86677 loss)
I0521 07:59:33.551059 17623 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 07:59:36.630133 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_728.caffemodel
I0521 07:59:36.979401 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_728.solverstate
I0521 07:59:37.578915 17623 solver.cpp:341] Iteration 730, Testing net (#0)
I0521 08:00:43.634374 17623 solver.cpp:409]     Test net output #0: accuracy = 0.597983
I0521 08:00:43.634546 17623 solver.cpp:409]     Test net output #1: loss = 1.46058 (* 1 = 1.46058 loss)
I0521 08:01:09.384196 17623 solver.cpp:237] Iteration 738, loss = 1.82976
I0521 08:01:09.384248 17623 solver.cpp:253]     Train net output #0: loss = 1.82976 (* 1 = 1.82976 loss)
I0521 08:01:09.384263 17623 sgd_solver.cpp:106] Iteration 738, lr = 0.0025
I0521 08:01:17.297438 17623 solver.cpp:237] Iteration 756, loss = 1.8408
I0521 08:01:17.297588 17623 solver.cpp:253]     Train net output #0: loss = 1.8408 (* 1 = 1.8408 loss)
I0521 08:01:17.297602 17623 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0521 08:01:25.209962 17623 solver.cpp:237] Iteration 774, loss = 1.8558
I0521 08:01:25.210000 17623 solver.cpp:253]     Train net output #0: loss = 1.8558 (* 1 = 1.8558 loss)
I0521 08:01:25.210016 17623 sgd_solver.cpp:106] Iteration 774, lr = 0.0025
I0521 08:01:33.127074 17623 solver.cpp:237] Iteration 792, loss = 1.82871
I0521 08:01:33.127106 17623 solver.cpp:253]     Train net output #0: loss = 1.82871 (* 1 = 1.82871 loss)
I0521 08:01:33.127121 17623 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 08:01:41.044703 17623 solver.cpp:237] Iteration 810, loss = 1.83402
I0521 08:01:41.044735 17623 solver.cpp:253]     Train net output #0: loss = 1.83402 (* 1 = 1.83402 loss)
I0521 08:01:41.044751 17623 sgd_solver.cpp:106] Iteration 810, lr = 0.0025
I0521 08:01:48.957895 17623 solver.cpp:237] Iteration 828, loss = 1.81778
I0521 08:01:48.958034 17623 solver.cpp:253]     Train net output #0: loss = 1.81778 (* 1 = 1.81778 loss)
I0521 08:01:48.958048 17623 sgd_solver.cpp:106] Iteration 828, lr = 0.0025
I0521 08:01:56.871222 17623 solver.cpp:237] Iteration 846, loss = 1.79979
I0521 08:01:56.871254 17623 solver.cpp:253]     Train net output #0: loss = 1.79979 (* 1 = 1.79979 loss)
I0521 08:01:56.871273 17623 sgd_solver.cpp:106] Iteration 846, lr = 0.0025
I0521 08:02:26.876466 17623 solver.cpp:237] Iteration 864, loss = 1.84643
I0521 08:02:26.876636 17623 solver.cpp:253]     Train net output #0: loss = 1.84643 (* 1 = 1.84643 loss)
I0521 08:02:26.876649 17623 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 08:02:34.786433 17623 solver.cpp:237] Iteration 882, loss = 1.79637
I0521 08:02:34.786466 17623 solver.cpp:253]     Train net output #0: loss = 1.79637 (* 1 = 1.79637 loss)
I0521 08:02:34.786483 17623 sgd_solver.cpp:106] Iteration 882, lr = 0.0025
I0521 08:02:42.700721 17623 solver.cpp:237] Iteration 900, loss = 1.86663
I0521 08:02:42.700763 17623 solver.cpp:253]     Train net output #0: loss = 1.86663 (* 1 = 1.86663 loss)
I0521 08:02:42.700779 17623 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0521 08:02:46.658674 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_910.caffemodel
I0521 08:02:47.008167 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_910.solverstate
I0521 08:02:50.686172 17623 solver.cpp:237] Iteration 918, loss = 1.8302
I0521 08:02:50.686219 17623 solver.cpp:253]     Train net output #0: loss = 1.8302 (* 1 = 1.8302 loss)
I0521 08:02:50.686234 17623 sgd_solver.cpp:106] Iteration 918, lr = 0.0025
I0521 08:02:58.605582 17623 solver.cpp:237] Iteration 936, loss = 1.78127
I0521 08:02:58.605725 17623 solver.cpp:253]     Train net output #0: loss = 1.78127 (* 1 = 1.78127 loss)
I0521 08:02:58.605737 17623 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 08:03:06.521430 17623 solver.cpp:237] Iteration 954, loss = 1.75454
I0521 08:03:06.521570 17623 solver.cpp:253]     Train net output #0: loss = 1.75454 (* 1 = 1.75454 loss)
I0521 08:03:06.521586 17623 sgd_solver.cpp:106] Iteration 954, lr = 0.0025
I0521 08:03:14.437634 17623 solver.cpp:237] Iteration 972, loss = 1.83265
I0521 08:03:14.437666 17623 solver.cpp:253]     Train net output #0: loss = 1.83265 (* 1 = 1.83265 loss)
I0521 08:03:14.437681 17623 sgd_solver.cpp:106] Iteration 972, lr = 0.0025
I0521 08:03:44.476886 17623 solver.cpp:237] Iteration 990, loss = 1.77902
I0521 08:03:44.477051 17623 solver.cpp:253]     Train net output #0: loss = 1.77902 (* 1 = 1.77902 loss)
I0521 08:03:44.477067 17623 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 08:03:52.394984 17623 solver.cpp:237] Iteration 1008, loss = 1.78041
I0521 08:03:52.395016 17623 solver.cpp:253]     Train net output #0: loss = 1.78041 (* 1 = 1.78041 loss)
I0521 08:03:52.395031 17623 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 08:04:00.312563 17623 solver.cpp:237] Iteration 1026, loss = 1.80821
I0521 08:04:00.312605 17623 solver.cpp:253]     Train net output #0: loss = 1.80821 (* 1 = 1.80821 loss)
I0521 08:04:00.312623 17623 sgd_solver.cpp:106] Iteration 1026, lr = 0.0025
I0521 08:04:08.227741 17623 solver.cpp:237] Iteration 1044, loss = 1.78111
I0521 08:04:08.227771 17623 solver.cpp:253]     Train net output #0: loss = 1.78111 (* 1 = 1.78111 loss)
I0521 08:04:08.227787 17623 sgd_solver.cpp:106] Iteration 1044, lr = 0.0025
I0521 08:04:16.143720 17623 solver.cpp:237] Iteration 1062, loss = 1.81656
I0521 08:04:16.143867 17623 solver.cpp:253]     Train net output #0: loss = 1.81656 (* 1 = 1.81656 loss)
I0521 08:04:16.143882 17623 sgd_solver.cpp:106] Iteration 1062, lr = 0.0025
I0521 08:04:24.065924 17623 solver.cpp:237] Iteration 1080, loss = 1.76857
I0521 08:04:24.065971 17623 solver.cpp:253]     Train net output #0: loss = 1.76857 (* 1 = 1.76857 loss)
I0521 08:04:24.065986 17623 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 08:04:28.901010 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1092.caffemodel
I0521 08:04:29.247318 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1092.solverstate
I0521 08:04:30.286964 17623 solver.cpp:341] Iteration 1095, Testing net (#0)
I0521 08:05:15.245710 17623 solver.cpp:409]     Test net output #0: accuracy = 0.643145
I0521 08:05:15.245882 17623 solver.cpp:409]     Test net output #1: loss = 1.30198 (* 1 = 1.30198 loss)
I0521 08:05:38.864413 17623 solver.cpp:237] Iteration 1098, loss = 1.76123
I0521 08:05:38.864462 17623 solver.cpp:253]     Train net output #0: loss = 1.76123 (* 1 = 1.76123 loss)
I0521 08:05:38.864477 17623 sgd_solver.cpp:106] Iteration 1098, lr = 0.0025
I0521 08:05:46.782101 17623 solver.cpp:237] Iteration 1116, loss = 1.78611
I0521 08:05:46.782249 17623 solver.cpp:253]     Train net output #0: loss = 1.78611 (* 1 = 1.78611 loss)
I0521 08:05:46.782263 17623 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0521 08:05:54.703837 17623 solver.cpp:237] Iteration 1134, loss = 1.76539
I0521 08:05:54.703871 17623 solver.cpp:253]     Train net output #0: loss = 1.76539 (* 1 = 1.76539 loss)
I0521 08:05:54.703886 17623 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0521 08:06:02.627414 17623 solver.cpp:237] Iteration 1152, loss = 1.74959
I0521 08:06:02.627457 17623 solver.cpp:253]     Train net output #0: loss = 1.74959 (* 1 = 1.74959 loss)
I0521 08:06:02.627473 17623 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 08:06:10.544420 17623 solver.cpp:237] Iteration 1170, loss = 1.79391
I0521 08:06:10.544453 17623 solver.cpp:253]     Train net output #0: loss = 1.79391 (* 1 = 1.79391 loss)
I0521 08:06:10.544467 17623 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0521 08:06:18.468389 17623 solver.cpp:237] Iteration 1188, loss = 1.71345
I0521 08:06:18.468528 17623 solver.cpp:253]     Train net output #0: loss = 1.71345 (* 1 = 1.71345 loss)
I0521 08:06:18.468540 17623 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 08:06:26.391765 17623 solver.cpp:237] Iteration 1206, loss = 1.74499
I0521 08:06:26.391800 17623 solver.cpp:253]     Train net output #0: loss = 1.74499 (* 1 = 1.74499 loss)
I0521 08:06:26.391816 17623 sgd_solver.cpp:106] Iteration 1206, lr = 0.0025
I0521 08:06:56.469071 17623 solver.cpp:237] Iteration 1224, loss = 1.77229
I0521 08:06:56.469239 17623 solver.cpp:253]     Train net output #0: loss = 1.77229 (* 1 = 1.77229 loss)
I0521 08:06:56.469254 17623 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 08:07:04.388222 17623 solver.cpp:237] Iteration 1242, loss = 1.72292
I0521 08:07:04.388254 17623 solver.cpp:253]     Train net output #0: loss = 1.72292 (* 1 = 1.72292 loss)
I0521 08:07:04.388270 17623 sgd_solver.cpp:106] Iteration 1242, lr = 0.0025
I0521 08:07:12.308341 17623 solver.cpp:237] Iteration 1260, loss = 1.7029
I0521 08:07:12.308374 17623 solver.cpp:253]     Train net output #0: loss = 1.7029 (* 1 = 1.7029 loss)
I0521 08:07:12.308389 17623 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0521 08:07:18.029947 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1274.caffemodel
I0521 08:07:18.375921 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1274.solverstate
I0521 08:07:20.292393 17623 solver.cpp:237] Iteration 1278, loss = 1.70891
I0521 08:07:20.292438 17623 solver.cpp:253]     Train net output #0: loss = 1.70891 (* 1 = 1.70891 loss)
I0521 08:07:20.292454 17623 sgd_solver.cpp:106] Iteration 1278, lr = 0.0025
I0521 08:07:28.212671 17623 solver.cpp:237] Iteration 1296, loss = 1.7948
I0521 08:07:28.212816 17623 solver.cpp:253]     Train net output #0: loss = 1.7948 (* 1 = 1.7948 loss)
I0521 08:07:28.212829 17623 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 08:07:36.134853 17623 solver.cpp:237] Iteration 1314, loss = 1.65689
I0521 08:07:36.134884 17623 solver.cpp:253]     Train net output #0: loss = 1.65689 (* 1 = 1.65689 loss)
I0521 08:07:36.134901 17623 sgd_solver.cpp:106] Iteration 1314, lr = 0.0025
I0521 08:07:44.050472 17623 solver.cpp:237] Iteration 1332, loss = 1.64613
I0521 08:07:44.050518 17623 solver.cpp:253]     Train net output #0: loss = 1.64613 (* 1 = 1.64613 loss)
I0521 08:07:44.050532 17623 sgd_solver.cpp:106] Iteration 1332, lr = 0.0025
I0521 08:08:14.095839 17623 solver.cpp:237] Iteration 1350, loss = 1.74399
I0521 08:08:14.096009 17623 solver.cpp:253]     Train net output #0: loss = 1.74399 (* 1 = 1.74399 loss)
I0521 08:08:14.096024 17623 sgd_solver.cpp:106] Iteration 1350, lr = 0.0025
I0521 08:08:22.014334 17623 solver.cpp:237] Iteration 1368, loss = 1.76714
I0521 08:08:22.014366 17623 solver.cpp:253]     Train net output #0: loss = 1.76714 (* 1 = 1.76714 loss)
I0521 08:08:22.014381 17623 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 08:08:29.932220 17623 solver.cpp:237] Iteration 1386, loss = 1.66167
I0521 08:08:29.932265 17623 solver.cpp:253]     Train net output #0: loss = 1.66167 (* 1 = 1.66167 loss)
I0521 08:08:29.932279 17623 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 08:08:37.851713 17623 solver.cpp:237] Iteration 1404, loss = 1.6739
I0521 08:08:37.851747 17623 solver.cpp:253]     Train net output #0: loss = 1.6739 (* 1 = 1.6739 loss)
I0521 08:08:37.851761 17623 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0521 08:08:45.773632 17623 solver.cpp:237] Iteration 1422, loss = 1.68305
I0521 08:08:45.773777 17623 solver.cpp:253]     Train net output #0: loss = 1.68305 (* 1 = 1.68305 loss)
I0521 08:08:45.773790 17623 sgd_solver.cpp:106] Iteration 1422, lr = 0.0025
I0521 08:08:53.697814 17623 solver.cpp:237] Iteration 1440, loss = 1.73594
I0521 08:08:53.697846 17623 solver.cpp:253]     Train net output #0: loss = 1.73594 (* 1 = 1.73594 loss)
I0521 08:08:53.697861 17623 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 08:09:00.298518 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1456.caffemodel
I0521 08:09:00.647328 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1456.solverstate
I0521 08:09:01.684681 17623 solver.cpp:237] Iteration 1458, loss = 1.77955
I0521 08:09:01.684722 17623 solver.cpp:253]     Train net output #0: loss = 1.77955 (* 1 = 1.77955 loss)
I0521 08:09:01.684741 17623 sgd_solver.cpp:106] Iteration 1458, lr = 0.0025
I0521 08:09:02.124783 17623 solver.cpp:341] Iteration 1460, Testing net (#0)
I0521 08:10:08.165065 17623 solver.cpp:409]     Test net output #0: accuracy = 0.658295
I0521 08:10:08.165237 17623 solver.cpp:409]     Test net output #1: loss = 1.19799 (* 1 = 1.19799 loss)
I0521 08:10:37.462083 17623 solver.cpp:237] Iteration 1476, loss = 1.79658
I0521 08:10:37.462133 17623 solver.cpp:253]     Train net output #0: loss = 1.79658 (* 1 = 1.79658 loss)
I0521 08:10:37.462147 17623 sgd_solver.cpp:106] Iteration 1476, lr = 0.0025
I0521 08:10:45.383441 17623 solver.cpp:237] Iteration 1494, loss = 1.7804
I0521 08:10:45.383585 17623 solver.cpp:253]     Train net output #0: loss = 1.7804 (* 1 = 1.7804 loss)
I0521 08:10:45.383599 17623 sgd_solver.cpp:106] Iteration 1494, lr = 0.0025
I0521 08:10:53.307183 17623 solver.cpp:237] Iteration 1512, loss = 1.72164
I0521 08:10:53.307214 17623 solver.cpp:253]     Train net output #0: loss = 1.72164 (* 1 = 1.72164 loss)
I0521 08:10:53.307231 17623 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 08:11:01.226642 17623 solver.cpp:237] Iteration 1530, loss = 1.71205
I0521 08:11:01.226686 17623 solver.cpp:253]     Train net output #0: loss = 1.71205 (* 1 = 1.71205 loss)
I0521 08:11:01.226701 17623 sgd_solver.cpp:106] Iteration 1530, lr = 0.0025
I0521 08:11:09.148720 17623 solver.cpp:237] Iteration 1548, loss = 1.70853
I0521 08:11:09.148752 17623 solver.cpp:253]     Train net output #0: loss = 1.70853 (* 1 = 1.70853 loss)
I0521 08:11:09.148767 17623 sgd_solver.cpp:106] Iteration 1548, lr = 0.0025
I0521 08:11:17.065968 17623 solver.cpp:237] Iteration 1566, loss = 1.70302
I0521 08:11:17.066108 17623 solver.cpp:253]     Train net output #0: loss = 1.70302 (* 1 = 1.70302 loss)
I0521 08:11:17.066123 17623 sgd_solver.cpp:106] Iteration 1566, lr = 0.0025
I0521 08:11:24.989171 17623 solver.cpp:237] Iteration 1584, loss = 1.70348
I0521 08:11:24.989215 17623 solver.cpp:253]     Train net output #0: loss = 1.70348 (* 1 = 1.70348 loss)
I0521 08:11:24.989234 17623 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 08:11:54.998116 17623 solver.cpp:237] Iteration 1602, loss = 1.64103
I0521 08:11:54.998281 17623 solver.cpp:253]     Train net output #0: loss = 1.64103 (* 1 = 1.64103 loss)
I0521 08:11:54.998296 17623 sgd_solver.cpp:106] Iteration 1602, lr = 0.0025
I0521 08:12:02.918525 17623 solver.cpp:237] Iteration 1620, loss = 1.72673
I0521 08:12:02.918557 17623 solver.cpp:253]     Train net output #0: loss = 1.72673 (* 1 = 1.72673 loss)
I0521 08:12:02.918573 17623 sgd_solver.cpp:106] Iteration 1620, lr = 0.0025
I0521 08:12:10.400463 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1638.caffemodel
I0521 08:12:10.748620 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1638.solverstate
I0521 08:12:10.908104 17623 solver.cpp:237] Iteration 1638, loss = 1.73452
I0521 08:12:10.908152 17623 solver.cpp:253]     Train net output #0: loss = 1.73452 (* 1 = 1.73452 loss)
I0521 08:12:10.908166 17623 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0521 08:12:18.831220 17623 solver.cpp:237] Iteration 1656, loss = 1.64233
I0521 08:12:18.831266 17623 solver.cpp:253]     Train net output #0: loss = 1.64233 (* 1 = 1.64233 loss)
I0521 08:12:18.831281 17623 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 08:12:26.749549 17623 solver.cpp:237] Iteration 1674, loss = 1.64223
I0521 08:12:26.749699 17623 solver.cpp:253]     Train net output #0: loss = 1.64223 (* 1 = 1.64223 loss)
I0521 08:12:26.749712 17623 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 08:12:34.669380 17623 solver.cpp:237] Iteration 1692, loss = 1.77692
I0521 08:12:34.669415 17623 solver.cpp:253]     Train net output #0: loss = 1.77692 (* 1 = 1.77692 loss)
I0521 08:12:34.669427 17623 sgd_solver.cpp:106] Iteration 1692, lr = 0.0025
I0521 08:13:04.697989 17623 solver.cpp:237] Iteration 1710, loss = 1.72948
I0521 08:13:04.698159 17623 solver.cpp:253]     Train net output #0: loss = 1.72948 (* 1 = 1.72948 loss)
I0521 08:13:04.698173 17623 sgd_solver.cpp:106] Iteration 1710, lr = 0.0025
I0521 08:13:12.619464 17623 solver.cpp:237] Iteration 1728, loss = 1.72403
I0521 08:13:12.619506 17623 solver.cpp:253]     Train net output #0: loss = 1.72403 (* 1 = 1.72403 loss)
I0521 08:13:12.619523 17623 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 08:13:20.541738 17623 solver.cpp:237] Iteration 1746, loss = 1.63556
I0521 08:13:20.541772 17623 solver.cpp:253]     Train net output #0: loss = 1.63556 (* 1 = 1.63556 loss)
I0521 08:13:20.541786 17623 sgd_solver.cpp:106] Iteration 1746, lr = 0.0025
I0521 08:13:28.455828 17623 solver.cpp:237] Iteration 1764, loss = 1.71152
I0521 08:13:28.455860 17623 solver.cpp:253]     Train net output #0: loss = 1.71152 (* 1 = 1.71152 loss)
I0521 08:13:28.455875 17623 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0521 08:13:36.383257 17623 solver.cpp:237] Iteration 1782, loss = 1.67189
I0521 08:13:36.383414 17623 solver.cpp:253]     Train net output #0: loss = 1.67189 (* 1 = 1.67189 loss)
I0521 08:13:36.383426 17623 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 08:13:44.305337 17623 solver.cpp:237] Iteration 1800, loss = 1.66525
I0521 08:13:44.305369 17623 solver.cpp:253]     Train net output #0: loss = 1.66525 (* 1 = 1.66525 loss)
I0521 08:13:44.305384 17623 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 08:13:52.230700 17623 solver.cpp:237] Iteration 1818, loss = 1.65893
I0521 08:13:52.230732 17623 solver.cpp:253]     Train net output #0: loss = 1.65893 (* 1 = 1.65893 loss)
I0521 08:13:52.230747 17623 sgd_solver.cpp:106] Iteration 1818, lr = 0.0025
I0521 08:13:52.670596 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1820.caffemodel
I0521 08:13:53.018892 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1820.solverstate
I0521 08:13:54.938202 17623 solver.cpp:341] Iteration 1825, Testing net (#0)
I0521 08:14:40.145467 17623 solver.cpp:409]     Test net output #0: accuracy = 0.66834
I0521 08:14:40.145632 17623 solver.cpp:409]     Test net output #1: loss = 1.13092 (* 1 = 1.13092 loss)
I0521 08:14:41.597757 17623 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1829.caffemodel
I0521 08:14:41.946009 17623 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407_iter_1829.solverstate
I0521 08:14:41.973830 17623 solver.cpp:326] Optimization Done.
I0521 08:14:41.973857 17623 caffe.cpp:215] Optimization Done.
Application 11237266 resources: utime ~1248s, stime ~225s, Rss ~5329440, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_820_2016-05-20T11.21.02.583407.solver"
	User time (seconds): 0.56
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:37.51
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15085
	Voluntary context switches: 2693
	Involuntary context switches: 82
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
