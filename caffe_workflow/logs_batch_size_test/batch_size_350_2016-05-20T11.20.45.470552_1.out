2805983
I0520 21:56:01.776279 24185 caffe.cpp:184] Using GPUs 0
I0520 21:56:02.202666 24185 solver.cpp:48] Initializing solver from parameters: 
test_iter: 428
test_interval: 857
base_lr: 0.0025
display: 42
max_iter: 4285
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 428
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552.prototxt"
I0520 21:56:02.204469 24185 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552.prototxt
I0520 21:56:02.218338 24185 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 21:56:02.218397 24185 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 21:56:02.218741 24185 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 350
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:56:02.218919 24185 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:56:02.218943 24185 net.cpp:106] Creating Layer data_hdf5
I0520 21:56:02.218957 24185 net.cpp:411] data_hdf5 -> data
I0520 21:56:02.218994 24185 net.cpp:411] data_hdf5 -> label
I0520 21:56:02.219029 24185 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 21:56:02.220499 24185 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 21:56:02.233533 24185 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 21:56:23.894260 24185 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 21:56:23.899353 24185 net.cpp:150] Setting up data_hdf5
I0520 21:56:23.899389 24185 net.cpp:157] Top shape: 350 1 127 50 (2222500)
I0520 21:56:23.899405 24185 net.cpp:157] Top shape: 350 (350)
I0520 21:56:23.899415 24185 net.cpp:165] Memory required for data: 8891400
I0520 21:56:23.899428 24185 layer_factory.hpp:77] Creating layer conv1
I0520 21:56:23.899462 24185 net.cpp:106] Creating Layer conv1
I0520 21:56:23.899474 24185 net.cpp:454] conv1 <- data
I0520 21:56:23.899497 24185 net.cpp:411] conv1 -> conv1
I0520 21:56:24.263273 24185 net.cpp:150] Setting up conv1
I0520 21:56:24.263315 24185 net.cpp:157] Top shape: 350 12 120 48 (24192000)
I0520 21:56:24.263326 24185 net.cpp:165] Memory required for data: 105659400
I0520 21:56:24.263355 24185 layer_factory.hpp:77] Creating layer relu1
I0520 21:56:24.263376 24185 net.cpp:106] Creating Layer relu1
I0520 21:56:24.263387 24185 net.cpp:454] relu1 <- conv1
I0520 21:56:24.263401 24185 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:56:24.263916 24185 net.cpp:150] Setting up relu1
I0520 21:56:24.263933 24185 net.cpp:157] Top shape: 350 12 120 48 (24192000)
I0520 21:56:24.263944 24185 net.cpp:165] Memory required for data: 202427400
I0520 21:56:24.263955 24185 layer_factory.hpp:77] Creating layer pool1
I0520 21:56:24.263972 24185 net.cpp:106] Creating Layer pool1
I0520 21:56:24.263983 24185 net.cpp:454] pool1 <- conv1
I0520 21:56:24.263996 24185 net.cpp:411] pool1 -> pool1
I0520 21:56:24.264075 24185 net.cpp:150] Setting up pool1
I0520 21:56:24.264089 24185 net.cpp:157] Top shape: 350 12 60 48 (12096000)
I0520 21:56:24.264099 24185 net.cpp:165] Memory required for data: 250811400
I0520 21:56:24.264111 24185 layer_factory.hpp:77] Creating layer conv2
I0520 21:56:24.264132 24185 net.cpp:106] Creating Layer conv2
I0520 21:56:24.264142 24185 net.cpp:454] conv2 <- pool1
I0520 21:56:24.264155 24185 net.cpp:411] conv2 -> conv2
I0520 21:56:24.266834 24185 net.cpp:150] Setting up conv2
I0520 21:56:24.266861 24185 net.cpp:157] Top shape: 350 20 54 46 (17388000)
I0520 21:56:24.266872 24185 net.cpp:165] Memory required for data: 320363400
I0520 21:56:24.266892 24185 layer_factory.hpp:77] Creating layer relu2
I0520 21:56:24.266906 24185 net.cpp:106] Creating Layer relu2
I0520 21:56:24.266916 24185 net.cpp:454] relu2 <- conv2
I0520 21:56:24.266932 24185 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:56:24.267258 24185 net.cpp:150] Setting up relu2
I0520 21:56:24.267273 24185 net.cpp:157] Top shape: 350 20 54 46 (17388000)
I0520 21:56:24.267284 24185 net.cpp:165] Memory required for data: 389915400
I0520 21:56:24.267294 24185 layer_factory.hpp:77] Creating layer pool2
I0520 21:56:24.267307 24185 net.cpp:106] Creating Layer pool2
I0520 21:56:24.267316 24185 net.cpp:454] pool2 <- conv2
I0520 21:56:24.267341 24185 net.cpp:411] pool2 -> pool2
I0520 21:56:24.267410 24185 net.cpp:150] Setting up pool2
I0520 21:56:24.267422 24185 net.cpp:157] Top shape: 350 20 27 46 (8694000)
I0520 21:56:24.267432 24185 net.cpp:165] Memory required for data: 424691400
I0520 21:56:24.267443 24185 layer_factory.hpp:77] Creating layer conv3
I0520 21:56:24.267462 24185 net.cpp:106] Creating Layer conv3
I0520 21:56:24.267472 24185 net.cpp:454] conv3 <- pool2
I0520 21:56:24.267487 24185 net.cpp:411] conv3 -> conv3
I0520 21:56:24.269433 24185 net.cpp:150] Setting up conv3
I0520 21:56:24.269456 24185 net.cpp:157] Top shape: 350 28 22 44 (9486400)
I0520 21:56:24.269469 24185 net.cpp:165] Memory required for data: 462637000
I0520 21:56:24.269489 24185 layer_factory.hpp:77] Creating layer relu3
I0520 21:56:24.269505 24185 net.cpp:106] Creating Layer relu3
I0520 21:56:24.269515 24185 net.cpp:454] relu3 <- conv3
I0520 21:56:24.269526 24185 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:56:24.269997 24185 net.cpp:150] Setting up relu3
I0520 21:56:24.270015 24185 net.cpp:157] Top shape: 350 28 22 44 (9486400)
I0520 21:56:24.270025 24185 net.cpp:165] Memory required for data: 500582600
I0520 21:56:24.270035 24185 layer_factory.hpp:77] Creating layer pool3
I0520 21:56:24.270048 24185 net.cpp:106] Creating Layer pool3
I0520 21:56:24.270058 24185 net.cpp:454] pool3 <- conv3
I0520 21:56:24.270071 24185 net.cpp:411] pool3 -> pool3
I0520 21:56:24.270138 24185 net.cpp:150] Setting up pool3
I0520 21:56:24.270151 24185 net.cpp:157] Top shape: 350 28 11 44 (4743200)
I0520 21:56:24.270161 24185 net.cpp:165] Memory required for data: 519555400
I0520 21:56:24.270170 24185 layer_factory.hpp:77] Creating layer conv4
I0520 21:56:24.270189 24185 net.cpp:106] Creating Layer conv4
I0520 21:56:24.270200 24185 net.cpp:454] conv4 <- pool3
I0520 21:56:24.270213 24185 net.cpp:411] conv4 -> conv4
I0520 21:56:24.272982 24185 net.cpp:150] Setting up conv4
I0520 21:56:24.273011 24185 net.cpp:157] Top shape: 350 36 6 42 (3175200)
I0520 21:56:24.273023 24185 net.cpp:165] Memory required for data: 532256200
I0520 21:56:24.273041 24185 layer_factory.hpp:77] Creating layer relu4
I0520 21:56:24.273054 24185 net.cpp:106] Creating Layer relu4
I0520 21:56:24.273064 24185 net.cpp:454] relu4 <- conv4
I0520 21:56:24.273077 24185 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:56:24.273560 24185 net.cpp:150] Setting up relu4
I0520 21:56:24.273576 24185 net.cpp:157] Top shape: 350 36 6 42 (3175200)
I0520 21:56:24.273587 24185 net.cpp:165] Memory required for data: 544957000
I0520 21:56:24.273597 24185 layer_factory.hpp:77] Creating layer pool4
I0520 21:56:24.273610 24185 net.cpp:106] Creating Layer pool4
I0520 21:56:24.273620 24185 net.cpp:454] pool4 <- conv4
I0520 21:56:24.273633 24185 net.cpp:411] pool4 -> pool4
I0520 21:56:24.273701 24185 net.cpp:150] Setting up pool4
I0520 21:56:24.273715 24185 net.cpp:157] Top shape: 350 36 3 42 (1587600)
I0520 21:56:24.273725 24185 net.cpp:165] Memory required for data: 551307400
I0520 21:56:24.273736 24185 layer_factory.hpp:77] Creating layer ip1
I0520 21:56:24.273756 24185 net.cpp:106] Creating Layer ip1
I0520 21:56:24.273766 24185 net.cpp:454] ip1 <- pool4
I0520 21:56:24.273778 24185 net.cpp:411] ip1 -> ip1
I0520 21:56:24.289189 24185 net.cpp:150] Setting up ip1
I0520 21:56:24.289217 24185 net.cpp:157] Top shape: 350 196 (68600)
I0520 21:56:24.289230 24185 net.cpp:165] Memory required for data: 551581800
I0520 21:56:24.289252 24185 layer_factory.hpp:77] Creating layer relu5
I0520 21:56:24.289266 24185 net.cpp:106] Creating Layer relu5
I0520 21:56:24.289276 24185 net.cpp:454] relu5 <- ip1
I0520 21:56:24.289290 24185 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:56:24.289633 24185 net.cpp:150] Setting up relu5
I0520 21:56:24.289646 24185 net.cpp:157] Top shape: 350 196 (68600)
I0520 21:56:24.289657 24185 net.cpp:165] Memory required for data: 551856200
I0520 21:56:24.289667 24185 layer_factory.hpp:77] Creating layer drop1
I0520 21:56:24.289688 24185 net.cpp:106] Creating Layer drop1
I0520 21:56:24.289698 24185 net.cpp:454] drop1 <- ip1
I0520 21:56:24.289723 24185 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:56:24.289770 24185 net.cpp:150] Setting up drop1
I0520 21:56:24.289783 24185 net.cpp:157] Top shape: 350 196 (68600)
I0520 21:56:24.289793 24185 net.cpp:165] Memory required for data: 552130600
I0520 21:56:24.289803 24185 layer_factory.hpp:77] Creating layer ip2
I0520 21:56:24.289822 24185 net.cpp:106] Creating Layer ip2
I0520 21:56:24.289832 24185 net.cpp:454] ip2 <- ip1
I0520 21:56:24.289845 24185 net.cpp:411] ip2 -> ip2
I0520 21:56:24.290313 24185 net.cpp:150] Setting up ip2
I0520 21:56:24.290326 24185 net.cpp:157] Top shape: 350 98 (34300)
I0520 21:56:24.290336 24185 net.cpp:165] Memory required for data: 552267800
I0520 21:56:24.290351 24185 layer_factory.hpp:77] Creating layer relu6
I0520 21:56:24.290364 24185 net.cpp:106] Creating Layer relu6
I0520 21:56:24.290374 24185 net.cpp:454] relu6 <- ip2
I0520 21:56:24.290385 24185 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:56:24.290904 24185 net.cpp:150] Setting up relu6
I0520 21:56:24.290920 24185 net.cpp:157] Top shape: 350 98 (34300)
I0520 21:56:24.290930 24185 net.cpp:165] Memory required for data: 552405000
I0520 21:56:24.290940 24185 layer_factory.hpp:77] Creating layer drop2
I0520 21:56:24.290953 24185 net.cpp:106] Creating Layer drop2
I0520 21:56:24.290963 24185 net.cpp:454] drop2 <- ip2
I0520 21:56:24.290976 24185 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:56:24.291018 24185 net.cpp:150] Setting up drop2
I0520 21:56:24.291031 24185 net.cpp:157] Top shape: 350 98 (34300)
I0520 21:56:24.291043 24185 net.cpp:165] Memory required for data: 552542200
I0520 21:56:24.291052 24185 layer_factory.hpp:77] Creating layer ip3
I0520 21:56:24.291065 24185 net.cpp:106] Creating Layer ip3
I0520 21:56:24.291075 24185 net.cpp:454] ip3 <- ip2
I0520 21:56:24.291088 24185 net.cpp:411] ip3 -> ip3
I0520 21:56:24.291297 24185 net.cpp:150] Setting up ip3
I0520 21:56:24.291311 24185 net.cpp:157] Top shape: 350 11 (3850)
I0520 21:56:24.291321 24185 net.cpp:165] Memory required for data: 552557600
I0520 21:56:24.291337 24185 layer_factory.hpp:77] Creating layer drop3
I0520 21:56:24.291349 24185 net.cpp:106] Creating Layer drop3
I0520 21:56:24.291358 24185 net.cpp:454] drop3 <- ip3
I0520 21:56:24.291370 24185 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:56:24.291409 24185 net.cpp:150] Setting up drop3
I0520 21:56:24.291422 24185 net.cpp:157] Top shape: 350 11 (3850)
I0520 21:56:24.291432 24185 net.cpp:165] Memory required for data: 552573000
I0520 21:56:24.291442 24185 layer_factory.hpp:77] Creating layer loss
I0520 21:56:24.291461 24185 net.cpp:106] Creating Layer loss
I0520 21:56:24.291471 24185 net.cpp:454] loss <- ip3
I0520 21:56:24.291482 24185 net.cpp:454] loss <- label
I0520 21:56:24.291496 24185 net.cpp:411] loss -> loss
I0520 21:56:24.291512 24185 layer_factory.hpp:77] Creating layer loss
I0520 21:56:24.292151 24185 net.cpp:150] Setting up loss
I0520 21:56:24.292172 24185 net.cpp:157] Top shape: (1)
I0520 21:56:24.292186 24185 net.cpp:160]     with loss weight 1
I0520 21:56:24.292227 24185 net.cpp:165] Memory required for data: 552573004
I0520 21:56:24.292238 24185 net.cpp:226] loss needs backward computation.
I0520 21:56:24.292249 24185 net.cpp:226] drop3 needs backward computation.
I0520 21:56:24.292259 24185 net.cpp:226] ip3 needs backward computation.
I0520 21:56:24.292270 24185 net.cpp:226] drop2 needs backward computation.
I0520 21:56:24.292279 24185 net.cpp:226] relu6 needs backward computation.
I0520 21:56:24.292289 24185 net.cpp:226] ip2 needs backward computation.
I0520 21:56:24.292300 24185 net.cpp:226] drop1 needs backward computation.
I0520 21:56:24.292310 24185 net.cpp:226] relu5 needs backward computation.
I0520 21:56:24.292320 24185 net.cpp:226] ip1 needs backward computation.
I0520 21:56:24.292330 24185 net.cpp:226] pool4 needs backward computation.
I0520 21:56:24.292340 24185 net.cpp:226] relu4 needs backward computation.
I0520 21:56:24.292349 24185 net.cpp:226] conv4 needs backward computation.
I0520 21:56:24.292361 24185 net.cpp:226] pool3 needs backward computation.
I0520 21:56:24.292378 24185 net.cpp:226] relu3 needs backward computation.
I0520 21:56:24.292387 24185 net.cpp:226] conv3 needs backward computation.
I0520 21:56:24.292398 24185 net.cpp:226] pool2 needs backward computation.
I0520 21:56:24.292408 24185 net.cpp:226] relu2 needs backward computation.
I0520 21:56:24.292418 24185 net.cpp:226] conv2 needs backward computation.
I0520 21:56:24.292428 24185 net.cpp:226] pool1 needs backward computation.
I0520 21:56:24.292439 24185 net.cpp:226] relu1 needs backward computation.
I0520 21:56:24.292448 24185 net.cpp:226] conv1 needs backward computation.
I0520 21:56:24.292460 24185 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:56:24.292469 24185 net.cpp:270] This network produces output loss
I0520 21:56:24.292493 24185 net.cpp:283] Network initialization done.
I0520 21:56:24.294095 24185 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552.prototxt
I0520 21:56:24.294165 24185 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 21:56:24.294522 24185 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 350
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 21:56:24.294710 24185 layer_factory.hpp:77] Creating layer data_hdf5
I0520 21:56:24.294725 24185 net.cpp:106] Creating Layer data_hdf5
I0520 21:56:24.294737 24185 net.cpp:411] data_hdf5 -> data
I0520 21:56:24.294754 24185 net.cpp:411] data_hdf5 -> label
I0520 21:56:24.294770 24185 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 21:56:24.296120 24185 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 21:56:45.576128 24185 net.cpp:150] Setting up data_hdf5
I0520 21:56:45.576288 24185 net.cpp:157] Top shape: 350 1 127 50 (2222500)
I0520 21:56:45.576303 24185 net.cpp:157] Top shape: 350 (350)
I0520 21:56:45.576315 24185 net.cpp:165] Memory required for data: 8891400
I0520 21:56:45.576329 24185 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 21:56:45.576356 24185 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 21:56:45.576367 24185 net.cpp:454] label_data_hdf5_1_split <- label
I0520 21:56:45.576382 24185 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 21:56:45.576405 24185 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 21:56:45.576477 24185 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 21:56:45.576490 24185 net.cpp:157] Top shape: 350 (350)
I0520 21:56:45.576501 24185 net.cpp:157] Top shape: 350 (350)
I0520 21:56:45.576511 24185 net.cpp:165] Memory required for data: 8894200
I0520 21:56:45.576521 24185 layer_factory.hpp:77] Creating layer conv1
I0520 21:56:45.576541 24185 net.cpp:106] Creating Layer conv1
I0520 21:56:45.576551 24185 net.cpp:454] conv1 <- data
I0520 21:56:45.576565 24185 net.cpp:411] conv1 -> conv1
I0520 21:56:45.578526 24185 net.cpp:150] Setting up conv1
I0520 21:56:45.578550 24185 net.cpp:157] Top shape: 350 12 120 48 (24192000)
I0520 21:56:45.578562 24185 net.cpp:165] Memory required for data: 105662200
I0520 21:56:45.578583 24185 layer_factory.hpp:77] Creating layer relu1
I0520 21:56:45.578598 24185 net.cpp:106] Creating Layer relu1
I0520 21:56:45.578608 24185 net.cpp:454] relu1 <- conv1
I0520 21:56:45.578621 24185 net.cpp:397] relu1 -> conv1 (in-place)
I0520 21:56:45.579118 24185 net.cpp:150] Setting up relu1
I0520 21:56:45.579133 24185 net.cpp:157] Top shape: 350 12 120 48 (24192000)
I0520 21:56:45.579144 24185 net.cpp:165] Memory required for data: 202430200
I0520 21:56:45.579154 24185 layer_factory.hpp:77] Creating layer pool1
I0520 21:56:45.579170 24185 net.cpp:106] Creating Layer pool1
I0520 21:56:45.579180 24185 net.cpp:454] pool1 <- conv1
I0520 21:56:45.579193 24185 net.cpp:411] pool1 -> pool1
I0520 21:56:45.579269 24185 net.cpp:150] Setting up pool1
I0520 21:56:45.579282 24185 net.cpp:157] Top shape: 350 12 60 48 (12096000)
I0520 21:56:45.579291 24185 net.cpp:165] Memory required for data: 250814200
I0520 21:56:45.579299 24185 layer_factory.hpp:77] Creating layer conv2
I0520 21:56:45.579318 24185 net.cpp:106] Creating Layer conv2
I0520 21:56:45.579329 24185 net.cpp:454] conv2 <- pool1
I0520 21:56:45.579341 24185 net.cpp:411] conv2 -> conv2
I0520 21:56:45.581264 24185 net.cpp:150] Setting up conv2
I0520 21:56:45.581285 24185 net.cpp:157] Top shape: 350 20 54 46 (17388000)
I0520 21:56:45.581298 24185 net.cpp:165] Memory required for data: 320366200
I0520 21:56:45.581316 24185 layer_factory.hpp:77] Creating layer relu2
I0520 21:56:45.581331 24185 net.cpp:106] Creating Layer relu2
I0520 21:56:45.581341 24185 net.cpp:454] relu2 <- conv2
I0520 21:56:45.581352 24185 net.cpp:397] relu2 -> conv2 (in-place)
I0520 21:56:45.581687 24185 net.cpp:150] Setting up relu2
I0520 21:56:45.581701 24185 net.cpp:157] Top shape: 350 20 54 46 (17388000)
I0520 21:56:45.581710 24185 net.cpp:165] Memory required for data: 389918200
I0520 21:56:45.581720 24185 layer_factory.hpp:77] Creating layer pool2
I0520 21:56:45.581734 24185 net.cpp:106] Creating Layer pool2
I0520 21:56:45.581744 24185 net.cpp:454] pool2 <- conv2
I0520 21:56:45.581758 24185 net.cpp:411] pool2 -> pool2
I0520 21:56:45.581828 24185 net.cpp:150] Setting up pool2
I0520 21:56:45.581842 24185 net.cpp:157] Top shape: 350 20 27 46 (8694000)
I0520 21:56:45.581851 24185 net.cpp:165] Memory required for data: 424694200
I0520 21:56:45.581861 24185 layer_factory.hpp:77] Creating layer conv3
I0520 21:56:45.581879 24185 net.cpp:106] Creating Layer conv3
I0520 21:56:45.581890 24185 net.cpp:454] conv3 <- pool2
I0520 21:56:45.581903 24185 net.cpp:411] conv3 -> conv3
I0520 21:56:45.583874 24185 net.cpp:150] Setting up conv3
I0520 21:56:45.583897 24185 net.cpp:157] Top shape: 350 28 22 44 (9486400)
I0520 21:56:45.583909 24185 net.cpp:165] Memory required for data: 462639800
I0520 21:56:45.583940 24185 layer_factory.hpp:77] Creating layer relu3
I0520 21:56:45.583955 24185 net.cpp:106] Creating Layer relu3
I0520 21:56:45.583964 24185 net.cpp:454] relu3 <- conv3
I0520 21:56:45.583977 24185 net.cpp:397] relu3 -> conv3 (in-place)
I0520 21:56:45.584506 24185 net.cpp:150] Setting up relu3
I0520 21:56:45.584522 24185 net.cpp:157] Top shape: 350 28 22 44 (9486400)
I0520 21:56:45.584530 24185 net.cpp:165] Memory required for data: 500585400
I0520 21:56:45.584542 24185 layer_factory.hpp:77] Creating layer pool3
I0520 21:56:45.584554 24185 net.cpp:106] Creating Layer pool3
I0520 21:56:45.584565 24185 net.cpp:454] pool3 <- conv3
I0520 21:56:45.584578 24185 net.cpp:411] pool3 -> pool3
I0520 21:56:45.584650 24185 net.cpp:150] Setting up pool3
I0520 21:56:45.584662 24185 net.cpp:157] Top shape: 350 28 11 44 (4743200)
I0520 21:56:45.584672 24185 net.cpp:165] Memory required for data: 519558200
I0520 21:56:45.584682 24185 layer_factory.hpp:77] Creating layer conv4
I0520 21:56:45.584699 24185 net.cpp:106] Creating Layer conv4
I0520 21:56:45.584710 24185 net.cpp:454] conv4 <- pool3
I0520 21:56:45.584724 24185 net.cpp:411] conv4 -> conv4
I0520 21:56:45.586791 24185 net.cpp:150] Setting up conv4
I0520 21:56:45.586814 24185 net.cpp:157] Top shape: 350 36 6 42 (3175200)
I0520 21:56:45.586824 24185 net.cpp:165] Memory required for data: 532259000
I0520 21:56:45.586839 24185 layer_factory.hpp:77] Creating layer relu4
I0520 21:56:45.586853 24185 net.cpp:106] Creating Layer relu4
I0520 21:56:45.586863 24185 net.cpp:454] relu4 <- conv4
I0520 21:56:45.586875 24185 net.cpp:397] relu4 -> conv4 (in-place)
I0520 21:56:45.587345 24185 net.cpp:150] Setting up relu4
I0520 21:56:45.587362 24185 net.cpp:157] Top shape: 350 36 6 42 (3175200)
I0520 21:56:45.587371 24185 net.cpp:165] Memory required for data: 544959800
I0520 21:56:45.587381 24185 layer_factory.hpp:77] Creating layer pool4
I0520 21:56:45.587394 24185 net.cpp:106] Creating Layer pool4
I0520 21:56:45.587404 24185 net.cpp:454] pool4 <- conv4
I0520 21:56:45.587419 24185 net.cpp:411] pool4 -> pool4
I0520 21:56:45.587489 24185 net.cpp:150] Setting up pool4
I0520 21:56:45.587503 24185 net.cpp:157] Top shape: 350 36 3 42 (1587600)
I0520 21:56:45.587512 24185 net.cpp:165] Memory required for data: 551310200
I0520 21:56:45.587523 24185 layer_factory.hpp:77] Creating layer ip1
I0520 21:56:45.587538 24185 net.cpp:106] Creating Layer ip1
I0520 21:56:45.587548 24185 net.cpp:454] ip1 <- pool4
I0520 21:56:45.587563 24185 net.cpp:411] ip1 -> ip1
I0520 21:56:45.603142 24185 net.cpp:150] Setting up ip1
I0520 21:56:45.603170 24185 net.cpp:157] Top shape: 350 196 (68600)
I0520 21:56:45.603188 24185 net.cpp:165] Memory required for data: 551584600
I0520 21:56:45.603209 24185 layer_factory.hpp:77] Creating layer relu5
I0520 21:56:45.603224 24185 net.cpp:106] Creating Layer relu5
I0520 21:56:45.603235 24185 net.cpp:454] relu5 <- ip1
I0520 21:56:45.603252 24185 net.cpp:397] relu5 -> ip1 (in-place)
I0520 21:56:45.603598 24185 net.cpp:150] Setting up relu5
I0520 21:56:45.603612 24185 net.cpp:157] Top shape: 350 196 (68600)
I0520 21:56:45.603622 24185 net.cpp:165] Memory required for data: 551859000
I0520 21:56:45.603632 24185 layer_factory.hpp:77] Creating layer drop1
I0520 21:56:45.603652 24185 net.cpp:106] Creating Layer drop1
I0520 21:56:45.603662 24185 net.cpp:454] drop1 <- ip1
I0520 21:56:45.603674 24185 net.cpp:397] drop1 -> ip1 (in-place)
I0520 21:56:45.603718 24185 net.cpp:150] Setting up drop1
I0520 21:56:45.603731 24185 net.cpp:157] Top shape: 350 196 (68600)
I0520 21:56:45.603740 24185 net.cpp:165] Memory required for data: 552133400
I0520 21:56:45.603751 24185 layer_factory.hpp:77] Creating layer ip2
I0520 21:56:45.603766 24185 net.cpp:106] Creating Layer ip2
I0520 21:56:45.603775 24185 net.cpp:454] ip2 <- ip1
I0520 21:56:45.603790 24185 net.cpp:411] ip2 -> ip2
I0520 21:56:45.604272 24185 net.cpp:150] Setting up ip2
I0520 21:56:45.604286 24185 net.cpp:157] Top shape: 350 98 (34300)
I0520 21:56:45.604296 24185 net.cpp:165] Memory required for data: 552270600
I0520 21:56:45.604324 24185 layer_factory.hpp:77] Creating layer relu6
I0520 21:56:45.604337 24185 net.cpp:106] Creating Layer relu6
I0520 21:56:45.604347 24185 net.cpp:454] relu6 <- ip2
I0520 21:56:45.604359 24185 net.cpp:397] relu6 -> ip2 (in-place)
I0520 21:56:45.604897 24185 net.cpp:150] Setting up relu6
I0520 21:56:45.604919 24185 net.cpp:157] Top shape: 350 98 (34300)
I0520 21:56:45.604928 24185 net.cpp:165] Memory required for data: 552407800
I0520 21:56:45.604939 24185 layer_factory.hpp:77] Creating layer drop2
I0520 21:56:45.604954 24185 net.cpp:106] Creating Layer drop2
I0520 21:56:45.604964 24185 net.cpp:454] drop2 <- ip2
I0520 21:56:45.604976 24185 net.cpp:397] drop2 -> ip2 (in-place)
I0520 21:56:45.605020 24185 net.cpp:150] Setting up drop2
I0520 21:56:45.605032 24185 net.cpp:157] Top shape: 350 98 (34300)
I0520 21:56:45.605042 24185 net.cpp:165] Memory required for data: 552545000
I0520 21:56:45.605052 24185 layer_factory.hpp:77] Creating layer ip3
I0520 21:56:45.605067 24185 net.cpp:106] Creating Layer ip3
I0520 21:56:45.605077 24185 net.cpp:454] ip3 <- ip2
I0520 21:56:45.605090 24185 net.cpp:411] ip3 -> ip3
I0520 21:56:45.605322 24185 net.cpp:150] Setting up ip3
I0520 21:56:45.605335 24185 net.cpp:157] Top shape: 350 11 (3850)
I0520 21:56:45.605345 24185 net.cpp:165] Memory required for data: 552560400
I0520 21:56:45.605361 24185 layer_factory.hpp:77] Creating layer drop3
I0520 21:56:45.605375 24185 net.cpp:106] Creating Layer drop3
I0520 21:56:45.605383 24185 net.cpp:454] drop3 <- ip3
I0520 21:56:45.605396 24185 net.cpp:397] drop3 -> ip3 (in-place)
I0520 21:56:45.605437 24185 net.cpp:150] Setting up drop3
I0520 21:56:45.605450 24185 net.cpp:157] Top shape: 350 11 (3850)
I0520 21:56:45.605460 24185 net.cpp:165] Memory required for data: 552575800
I0520 21:56:45.605469 24185 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 21:56:45.605482 24185 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 21:56:45.605492 24185 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 21:56:45.605505 24185 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 21:56:45.605520 24185 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 21:56:45.605593 24185 net.cpp:150] Setting up ip3_drop3_0_split
I0520 21:56:45.605607 24185 net.cpp:157] Top shape: 350 11 (3850)
I0520 21:56:45.605618 24185 net.cpp:157] Top shape: 350 11 (3850)
I0520 21:56:45.605628 24185 net.cpp:165] Memory required for data: 552606600
I0520 21:56:45.605639 24185 layer_factory.hpp:77] Creating layer accuracy
I0520 21:56:45.605660 24185 net.cpp:106] Creating Layer accuracy
I0520 21:56:45.605670 24185 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 21:56:45.605681 24185 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 21:56:45.605695 24185 net.cpp:411] accuracy -> accuracy
I0520 21:56:45.605718 24185 net.cpp:150] Setting up accuracy
I0520 21:56:45.605731 24185 net.cpp:157] Top shape: (1)
I0520 21:56:45.605741 24185 net.cpp:165] Memory required for data: 552606604
I0520 21:56:45.605751 24185 layer_factory.hpp:77] Creating layer loss
I0520 21:56:45.605765 24185 net.cpp:106] Creating Layer loss
I0520 21:56:45.605777 24185 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 21:56:45.605787 24185 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 21:56:45.605800 24185 net.cpp:411] loss -> loss
I0520 21:56:45.605818 24185 layer_factory.hpp:77] Creating layer loss
I0520 21:56:45.606302 24185 net.cpp:150] Setting up loss
I0520 21:56:45.606317 24185 net.cpp:157] Top shape: (1)
I0520 21:56:45.606326 24185 net.cpp:160]     with loss weight 1
I0520 21:56:45.606344 24185 net.cpp:165] Memory required for data: 552606608
I0520 21:56:45.606354 24185 net.cpp:226] loss needs backward computation.
I0520 21:56:45.606365 24185 net.cpp:228] accuracy does not need backward computation.
I0520 21:56:45.606376 24185 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 21:56:45.606386 24185 net.cpp:226] drop3 needs backward computation.
I0520 21:56:45.606395 24185 net.cpp:226] ip3 needs backward computation.
I0520 21:56:45.606405 24185 net.cpp:226] drop2 needs backward computation.
I0520 21:56:45.606425 24185 net.cpp:226] relu6 needs backward computation.
I0520 21:56:45.606434 24185 net.cpp:226] ip2 needs backward computation.
I0520 21:56:45.606444 24185 net.cpp:226] drop1 needs backward computation.
I0520 21:56:45.606454 24185 net.cpp:226] relu5 needs backward computation.
I0520 21:56:45.606464 24185 net.cpp:226] ip1 needs backward computation.
I0520 21:56:45.606474 24185 net.cpp:226] pool4 needs backward computation.
I0520 21:56:45.606484 24185 net.cpp:226] relu4 needs backward computation.
I0520 21:56:45.606494 24185 net.cpp:226] conv4 needs backward computation.
I0520 21:56:45.606504 24185 net.cpp:226] pool3 needs backward computation.
I0520 21:56:45.606514 24185 net.cpp:226] relu3 needs backward computation.
I0520 21:56:45.606528 24185 net.cpp:226] conv3 needs backward computation.
I0520 21:56:45.606537 24185 net.cpp:226] pool2 needs backward computation.
I0520 21:56:45.606549 24185 net.cpp:226] relu2 needs backward computation.
I0520 21:56:45.606559 24185 net.cpp:226] conv2 needs backward computation.
I0520 21:56:45.606569 24185 net.cpp:226] pool1 needs backward computation.
I0520 21:56:45.606578 24185 net.cpp:226] relu1 needs backward computation.
I0520 21:56:45.606588 24185 net.cpp:226] conv1 needs backward computation.
I0520 21:56:45.606600 24185 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 21:56:45.606611 24185 net.cpp:228] data_hdf5 does not need backward computation.
I0520 21:56:45.606621 24185 net.cpp:270] This network produces output accuracy
I0520 21:56:45.606632 24185 net.cpp:270] This network produces output loss
I0520 21:56:45.606662 24185 net.cpp:283] Network initialization done.
I0520 21:56:45.606796 24185 solver.cpp:60] Solver scaffolding done.
I0520 21:56:45.607920 24185 caffe.cpp:212] Starting Optimization
I0520 21:56:45.607939 24185 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 21:56:45.607951 24185 solver.cpp:289] Learning Rate Policy: fixed
I0520 21:56:45.609180 24185 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 21:57:31.780671 24185 solver.cpp:409]     Test net output #0: accuracy = 0.0516688
I0520 21:57:31.780833 24185 solver.cpp:409]     Test net output #1: loss = 2.39941 (* 1 = 2.39941 loss)
I0520 21:57:31.854849 24185 solver.cpp:237] Iteration 0, loss = 2.39961
I0520 21:57:31.854884 24185 solver.cpp:253]     Train net output #0: loss = 2.39961 (* 1 = 2.39961 loss)
I0520 21:57:31.854903 24185 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 21:57:39.817611 24185 solver.cpp:237] Iteration 42, loss = 2.36284
I0520 21:57:39.817659 24185 solver.cpp:253]     Train net output #0: loss = 2.36284 (* 1 = 2.36284 loss)
I0520 21:57:39.817677 24185 sgd_solver.cpp:106] Iteration 42, lr = 0.0025
I0520 21:57:47.780628 24185 solver.cpp:237] Iteration 84, loss = 2.34598
I0520 21:57:47.780661 24185 solver.cpp:253]     Train net output #0: loss = 2.34598 (* 1 = 2.34598 loss)
I0520 21:57:47.780678 24185 sgd_solver.cpp:106] Iteration 84, lr = 0.0025
I0520 21:57:55.742982 24185 solver.cpp:237] Iteration 126, loss = 2.34731
I0520 21:57:55.743016 24185 solver.cpp:253]     Train net output #0: loss = 2.34731 (* 1 = 2.34731 loss)
I0520 21:57:55.743028 24185 sgd_solver.cpp:106] Iteration 126, lr = 0.0025
I0520 21:58:03.706681 24185 solver.cpp:237] Iteration 168, loss = 2.31227
I0520 21:58:03.706836 24185 solver.cpp:253]     Train net output #0: loss = 2.31227 (* 1 = 2.31227 loss)
I0520 21:58:03.706851 24185 sgd_solver.cpp:106] Iteration 168, lr = 0.0025
I0520 21:58:11.666966 24185 solver.cpp:237] Iteration 210, loss = 2.30205
I0520 21:58:11.666996 24185 solver.cpp:253]     Train net output #0: loss = 2.30205 (* 1 = 2.30205 loss)
I0520 21:58:11.667009 24185 sgd_solver.cpp:106] Iteration 210, lr = 0.0025
I0520 21:58:19.632138 24185 solver.cpp:237] Iteration 252, loss = 2.29777
I0520 21:58:19.632171 24185 solver.cpp:253]     Train net output #0: loss = 2.29777 (* 1 = 2.29777 loss)
I0520 21:58:19.632187 24185 sgd_solver.cpp:106] Iteration 252, lr = 0.0025
I0520 21:58:49.760104 24185 solver.cpp:237] Iteration 294, loss = 2.30304
I0520 21:58:49.760264 24185 solver.cpp:253]     Train net output #0: loss = 2.30304 (* 1 = 2.30304 loss)
I0520 21:58:49.760279 24185 sgd_solver.cpp:106] Iteration 294, lr = 0.0025
I0520 21:58:57.727386 24185 solver.cpp:237] Iteration 336, loss = 2.28224
I0520 21:58:57.727432 24185 solver.cpp:253]     Train net output #0: loss = 2.28224 (* 1 = 2.28224 loss)
I0520 21:58:57.727447 24185 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0520 21:59:05.700594 24185 solver.cpp:237] Iteration 378, loss = 2.27164
I0520 21:59:05.700628 24185 solver.cpp:253]     Train net output #0: loss = 2.27164 (* 1 = 2.27164 loss)
I0520 21:59:05.700645 24185 sgd_solver.cpp:106] Iteration 378, lr = 0.0025
I0520 21:59:13.671077 24185 solver.cpp:237] Iteration 420, loss = 2.15403
I0520 21:59:13.671110 24185 solver.cpp:253]     Train net output #0: loss = 2.15403 (* 1 = 2.15403 loss)
I0520 21:59:13.671128 24185 sgd_solver.cpp:106] Iteration 420, lr = 0.0025
I0520 21:59:15.000788 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_428.caffemodel
I0520 21:59:15.184572 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_428.solverstate
I0520 21:59:21.726781 24185 solver.cpp:237] Iteration 462, loss = 2.11731
I0520 21:59:21.726938 24185 solver.cpp:253]     Train net output #0: loss = 2.11731 (* 1 = 2.11731 loss)
I0520 21:59:21.726953 24185 sgd_solver.cpp:106] Iteration 462, lr = 0.0025
I0520 21:59:29.690989 24185 solver.cpp:237] Iteration 504, loss = 2.15398
I0520 21:59:29.691021 24185 solver.cpp:253]     Train net output #0: loss = 2.15398 (* 1 = 2.15398 loss)
I0520 21:59:29.691035 24185 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0520 21:59:37.656682 24185 solver.cpp:237] Iteration 546, loss = 2.04063
I0520 21:59:37.656716 24185 solver.cpp:253]     Train net output #0: loss = 2.04063 (* 1 = 2.04063 loss)
I0520 21:59:37.656729 24185 sgd_solver.cpp:106] Iteration 546, lr = 0.0025
I0520 22:00:07.842180 24185 solver.cpp:237] Iteration 588, loss = 2.05291
I0520 22:00:07.842330 24185 solver.cpp:253]     Train net output #0: loss = 2.05291 (* 1 = 2.05291 loss)
I0520 22:00:07.842345 24185 sgd_solver.cpp:106] Iteration 588, lr = 0.0025
I0520 22:00:15.803655 24185 solver.cpp:237] Iteration 630, loss = 1.84126
I0520 22:00:15.803689 24185 solver.cpp:253]     Train net output #0: loss = 1.84126 (* 1 = 1.84126 loss)
I0520 22:00:15.803704 24185 sgd_solver.cpp:106] Iteration 630, lr = 0.0025
I0520 22:00:23.766006 24185 solver.cpp:237] Iteration 672, loss = 1.95645
I0520 22:00:23.766039 24185 solver.cpp:253]     Train net output #0: loss = 1.95645 (* 1 = 1.95645 loss)
I0520 22:00:23.766053 24185 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0520 22:00:31.724661 24185 solver.cpp:237] Iteration 714, loss = 1.96182
I0520 22:00:31.724695 24185 solver.cpp:253]     Train net output #0: loss = 1.96182 (* 1 = 1.96182 loss)
I0520 22:00:31.724711 24185 sgd_solver.cpp:106] Iteration 714, lr = 0.0025
I0520 22:00:39.681140 24185 solver.cpp:237] Iteration 756, loss = 1.9309
I0520 22:00:39.681287 24185 solver.cpp:253]     Train net output #0: loss = 1.9309 (* 1 = 1.9309 loss)
I0520 22:00:39.681300 24185 sgd_solver.cpp:106] Iteration 756, lr = 0.0025
I0520 22:00:47.643741 24185 solver.cpp:237] Iteration 798, loss = 1.96672
I0520 22:00:47.643774 24185 solver.cpp:253]     Train net output #0: loss = 1.96672 (* 1 = 1.96672 loss)
I0520 22:00:47.643790 24185 sgd_solver.cpp:106] Iteration 798, lr = 0.0025
I0520 22:00:55.608140 24185 solver.cpp:237] Iteration 840, loss = 1.90237
I0520 22:00:55.608173 24185 solver.cpp:253]     Train net output #0: loss = 1.90237 (* 1 = 1.90237 loss)
I0520 22:00:55.608191 24185 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0520 22:00:58.451969 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_856.caffemodel
I0520 22:00:58.632947 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_856.solverstate
I0520 22:00:58.722127 24185 solver.cpp:341] Iteration 857, Testing net (#0)
I0520 22:01:44.138664 24185 solver.cpp:409]     Test net output #0: accuracy = 0.581649
I0520 22:01:44.138826 24185 solver.cpp:409]     Test net output #1: loss = 1.46486 (* 1 = 1.46486 loss)
I0520 22:02:11.029850 24185 solver.cpp:237] Iteration 882, loss = 1.91906
I0520 22:02:11.029899 24185 solver.cpp:253]     Train net output #0: loss = 1.91906 (* 1 = 1.91906 loss)
I0520 22:02:11.029916 24185 sgd_solver.cpp:106] Iteration 882, lr = 0.0025
I0520 22:02:18.997506 24185 solver.cpp:237] Iteration 924, loss = 1.79076
I0520 22:02:18.997655 24185 solver.cpp:253]     Train net output #0: loss = 1.79076 (* 1 = 1.79076 loss)
I0520 22:02:18.997670 24185 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0520 22:02:26.970312 24185 solver.cpp:237] Iteration 966, loss = 1.90273
I0520 22:02:26.970345 24185 solver.cpp:253]     Train net output #0: loss = 1.90273 (* 1 = 1.90273 loss)
I0520 22:02:26.970362 24185 sgd_solver.cpp:106] Iteration 966, lr = 0.0025
I0520 22:02:34.943843 24185 solver.cpp:237] Iteration 1008, loss = 1.92929
I0520 22:02:34.943876 24185 solver.cpp:253]     Train net output #0: loss = 1.92929 (* 1 = 1.92929 loss)
I0520 22:02:34.943893 24185 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0520 22:02:42.913820 24185 solver.cpp:237] Iteration 1050, loss = 1.77216
I0520 22:02:42.913857 24185 solver.cpp:253]     Train net output #0: loss = 1.77216 (* 1 = 1.77216 loss)
I0520 22:02:42.913878 24185 sgd_solver.cpp:106] Iteration 1050, lr = 0.0025
I0520 22:02:50.885740 24185 solver.cpp:237] Iteration 1092, loss = 1.7197
I0520 22:02:50.885874 24185 solver.cpp:253]     Train net output #0: loss = 1.7197 (* 1 = 1.7197 loss)
I0520 22:02:50.885888 24185 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0520 22:02:58.857928 24185 solver.cpp:237] Iteration 1134, loss = 1.83668
I0520 22:02:58.857960 24185 solver.cpp:253]     Train net output #0: loss = 1.83668 (* 1 = 1.83668 loss)
I0520 22:02:58.857977 24185 sgd_solver.cpp:106] Iteration 1134, lr = 0.0025
I0520 22:03:29.034437 24185 solver.cpp:237] Iteration 1176, loss = 2.03249
I0520 22:03:29.034602 24185 solver.cpp:253]     Train net output #0: loss = 2.03249 (* 1 = 2.03249 loss)
I0520 22:03:29.034620 24185 sgd_solver.cpp:106] Iteration 1176, lr = 0.0025
I0520 22:03:37.005007 24185 solver.cpp:237] Iteration 1218, loss = 1.81084
I0520 22:03:37.005043 24185 solver.cpp:253]     Train net output #0: loss = 1.81084 (* 1 = 1.81084 loss)
I0520 22:03:37.005059 24185 sgd_solver.cpp:106] Iteration 1218, lr = 0.0025
I0520 22:03:44.979097 24185 solver.cpp:237] Iteration 1260, loss = 1.68753
I0520 22:03:44.979131 24185 solver.cpp:253]     Train net output #0: loss = 1.68753 (* 1 = 1.68753 loss)
I0520 22:03:44.979147 24185 sgd_solver.cpp:106] Iteration 1260, lr = 0.0025
I0520 22:03:49.346642 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_1284.caffemodel
I0520 22:03:49.525224 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_1284.solverstate
I0520 22:03:53.033465 24185 solver.cpp:237] Iteration 1302, loss = 1.79977
I0520 22:03:53.033516 24185 solver.cpp:253]     Train net output #0: loss = 1.79977 (* 1 = 1.79977 loss)
I0520 22:03:53.033532 24185 sgd_solver.cpp:106] Iteration 1302, lr = 0.0025
I0520 22:04:01.004700 24185 solver.cpp:237] Iteration 1344, loss = 1.68085
I0520 22:04:01.004858 24185 solver.cpp:253]     Train net output #0: loss = 1.68085 (* 1 = 1.68085 loss)
I0520 22:04:01.004873 24185 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0520 22:04:08.973745 24185 solver.cpp:237] Iteration 1386, loss = 1.71123
I0520 22:04:08.973778 24185 solver.cpp:253]     Train net output #0: loss = 1.71123 (* 1 = 1.71123 loss)
I0520 22:04:08.973795 24185 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0520 22:04:39.092125 24185 solver.cpp:237] Iteration 1428, loss = 1.8231
I0520 22:04:39.092298 24185 solver.cpp:253]     Train net output #0: loss = 1.8231 (* 1 = 1.8231 loss)
I0520 22:04:39.092315 24185 sgd_solver.cpp:106] Iteration 1428, lr = 0.0025
I0520 22:04:47.059388 24185 solver.cpp:237] Iteration 1470, loss = 1.8111
I0520 22:04:47.059428 24185 solver.cpp:253]     Train net output #0: loss = 1.8111 (* 1 = 1.8111 loss)
I0520 22:04:47.059449 24185 sgd_solver.cpp:106] Iteration 1470, lr = 0.0025
I0520 22:04:55.030069 24185 solver.cpp:237] Iteration 1512, loss = 1.77939
I0520 22:04:55.030103 24185 solver.cpp:253]     Train net output #0: loss = 1.77939 (* 1 = 1.77939 loss)
I0520 22:04:55.030119 24185 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0520 22:05:03.000111 24185 solver.cpp:237] Iteration 1554, loss = 1.71998
I0520 22:05:03.000144 24185 solver.cpp:253]     Train net output #0: loss = 1.71998 (* 1 = 1.71998 loss)
I0520 22:05:03.000161 24185 sgd_solver.cpp:106] Iteration 1554, lr = 0.0025
I0520 22:05:10.973536 24185 solver.cpp:237] Iteration 1596, loss = 1.72431
I0520 22:05:10.973678 24185 solver.cpp:253]     Train net output #0: loss = 1.72431 (* 1 = 1.72431 loss)
I0520 22:05:10.973692 24185 sgd_solver.cpp:106] Iteration 1596, lr = 0.0025
I0520 22:05:18.946470 24185 solver.cpp:237] Iteration 1638, loss = 1.76241
I0520 22:05:18.946502 24185 solver.cpp:253]     Train net output #0: loss = 1.76241 (* 1 = 1.76241 loss)
I0520 22:05:18.946519 24185 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0520 22:05:26.914418 24185 solver.cpp:237] Iteration 1680, loss = 1.90582
I0520 22:05:26.914453 24185 solver.cpp:253]     Train net output #0: loss = 1.90582 (* 1 = 1.90582 loss)
I0520 22:05:26.914469 24185 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0520 22:05:32.799068 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_1712.caffemodel
I0520 22:05:32.984693 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_1712.solverstate
I0520 22:05:33.267098 24185 solver.cpp:341] Iteration 1714, Testing net (#0)
I0520 22:06:39.538672 24185 solver.cpp:409]     Test net output #0: accuracy = 0.659166
I0520 22:06:39.538859 24185 solver.cpp:409]     Test net output #1: loss = 1.23461 (* 1 = 1.23461 loss)
I0520 22:07:03.285276 24185 solver.cpp:237] Iteration 1722, loss = 1.66703
I0520 22:07:03.285327 24185 solver.cpp:253]     Train net output #0: loss = 1.66703 (* 1 = 1.66703 loss)
I0520 22:07:03.285342 24185 sgd_solver.cpp:106] Iteration 1722, lr = 0.0025
I0520 22:07:11.267537 24185 solver.cpp:237] Iteration 1764, loss = 1.66792
I0520 22:07:11.267686 24185 solver.cpp:253]     Train net output #0: loss = 1.66792 (* 1 = 1.66792 loss)
I0520 22:07:11.267699 24185 sgd_solver.cpp:106] Iteration 1764, lr = 0.0025
I0520 22:07:19.244722 24185 solver.cpp:237] Iteration 1806, loss = 1.70443
I0520 22:07:19.244763 24185 solver.cpp:253]     Train net output #0: loss = 1.70443 (* 1 = 1.70443 loss)
I0520 22:07:19.244781 24185 sgd_solver.cpp:106] Iteration 1806, lr = 0.0025
I0520 22:07:27.222792 24185 solver.cpp:237] Iteration 1848, loss = 1.63723
I0520 22:07:27.222825 24185 solver.cpp:253]     Train net output #0: loss = 1.63723 (* 1 = 1.63723 loss)
I0520 22:07:27.222841 24185 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0520 22:07:35.201983 24185 solver.cpp:237] Iteration 1890, loss = 1.76401
I0520 22:07:35.202016 24185 solver.cpp:253]     Train net output #0: loss = 1.76401 (* 1 = 1.76401 loss)
I0520 22:07:35.202033 24185 sgd_solver.cpp:106] Iteration 1890, lr = 0.0025
I0520 22:07:43.176821 24185 solver.cpp:237] Iteration 1932, loss = 1.67588
I0520 22:07:43.176976 24185 solver.cpp:253]     Train net output #0: loss = 1.67588 (* 1 = 1.67588 loss)
I0520 22:07:43.176990 24185 sgd_solver.cpp:106] Iteration 1932, lr = 0.0025
I0520 22:07:51.155323 24185 solver.cpp:237] Iteration 1974, loss = 1.82241
I0520 22:07:51.155354 24185 solver.cpp:253]     Train net output #0: loss = 1.82241 (* 1 = 1.82241 loss)
I0520 22:07:51.155371 24185 sgd_solver.cpp:106] Iteration 1974, lr = 0.0025
I0520 22:08:21.265103 24185 solver.cpp:237] Iteration 2016, loss = 1.63497
I0520 22:08:21.265261 24185 solver.cpp:253]     Train net output #0: loss = 1.63497 (* 1 = 1.63497 loss)
I0520 22:08:21.265276 24185 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0520 22:08:29.243883 24185 solver.cpp:237] Iteration 2058, loss = 1.72397
I0520 22:08:29.243930 24185 solver.cpp:253]     Train net output #0: loss = 1.72397 (* 1 = 1.72397 loss)
I0520 22:08:29.243947 24185 sgd_solver.cpp:106] Iteration 2058, lr = 0.0025
I0520 22:08:37.222682 24185 solver.cpp:237] Iteration 2100, loss = 1.68015
I0520 22:08:37.222717 24185 solver.cpp:253]     Train net output #0: loss = 1.68015 (* 1 = 1.68015 loss)
I0520 22:08:37.222733 24185 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0520 22:08:44.630306 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_2140.caffemodel
I0520 22:08:44.812911 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_2140.solverstate
I0520 22:08:45.284757 24185 solver.cpp:237] Iteration 2142, loss = 1.60802
I0520 22:08:45.284806 24185 solver.cpp:253]     Train net output #0: loss = 1.60802 (* 1 = 1.60802 loss)
I0520 22:08:45.284821 24185 sgd_solver.cpp:106] Iteration 2142, lr = 0.0025
I0520 22:08:53.258074 24185 solver.cpp:237] Iteration 2184, loss = 1.70721
I0520 22:08:53.258222 24185 solver.cpp:253]     Train net output #0: loss = 1.70721 (* 1 = 1.70721 loss)
I0520 22:08:53.258236 24185 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0520 22:09:01.234267 24185 solver.cpp:237] Iteration 2226, loss = 1.66647
I0520 22:09:01.234300 24185 solver.cpp:253]     Train net output #0: loss = 1.66647 (* 1 = 1.66647 loss)
I0520 22:09:01.234318 24185 sgd_solver.cpp:106] Iteration 2226, lr = 0.0025
I0520 22:09:09.214354 24185 solver.cpp:237] Iteration 2268, loss = 1.62421
I0520 22:09:09.214388 24185 solver.cpp:253]     Train net output #0: loss = 1.62421 (* 1 = 1.62421 loss)
I0520 22:09:09.214406 24185 sgd_solver.cpp:106] Iteration 2268, lr = 0.0025
I0520 22:09:39.338114 24185 solver.cpp:237] Iteration 2310, loss = 1.6669
I0520 22:09:39.338290 24185 solver.cpp:253]     Train net output #0: loss = 1.6669 (* 1 = 1.6669 loss)
I0520 22:09:39.338305 24185 sgd_solver.cpp:106] Iteration 2310, lr = 0.0025
I0520 22:09:47.318250 24185 solver.cpp:237] Iteration 2352, loss = 1.61291
I0520 22:09:47.318290 24185 solver.cpp:253]     Train net output #0: loss = 1.61291 (* 1 = 1.61291 loss)
I0520 22:09:47.318307 24185 sgd_solver.cpp:106] Iteration 2352, lr = 0.0025
I0520 22:09:55.294396 24185 solver.cpp:237] Iteration 2394, loss = 1.64759
I0520 22:09:55.294430 24185 solver.cpp:253]     Train net output #0: loss = 1.64759 (* 1 = 1.64759 loss)
I0520 22:09:55.294446 24185 sgd_solver.cpp:106] Iteration 2394, lr = 0.0025
I0520 22:10:03.273986 24185 solver.cpp:237] Iteration 2436, loss = 1.64008
I0520 22:10:03.274019 24185 solver.cpp:253]     Train net output #0: loss = 1.64008 (* 1 = 1.64008 loss)
I0520 22:10:03.274036 24185 sgd_solver.cpp:106] Iteration 2436, lr = 0.0025
I0520 22:10:11.250821 24185 solver.cpp:237] Iteration 2478, loss = 1.57806
I0520 22:10:11.250993 24185 solver.cpp:253]     Train net output #0: loss = 1.57806 (* 1 = 1.57806 loss)
I0520 22:10:11.251006 24185 sgd_solver.cpp:106] Iteration 2478, lr = 0.0025
I0520 22:10:19.221201 24185 solver.cpp:237] Iteration 2520, loss = 1.62655
I0520 22:10:19.221235 24185 solver.cpp:253]     Train net output #0: loss = 1.62655 (* 1 = 1.62655 loss)
I0520 22:10:19.221252 24185 sgd_solver.cpp:106] Iteration 2520, lr = 0.0025
I0520 22:10:27.195416 24185 solver.cpp:237] Iteration 2562, loss = 1.68839
I0520 22:10:27.195449 24185 solver.cpp:253]     Train net output #0: loss = 1.68839 (* 1 = 1.68839 loss)
I0520 22:10:27.195462 24185 sgd_solver.cpp:106] Iteration 2562, lr = 0.0025
I0520 22:10:28.144019 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_2568.caffemodel
I0520 22:10:28.323470 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_2568.solverstate
I0520 22:10:28.802181 24185 solver.cpp:341] Iteration 2571, Testing net (#0)
I0520 22:11:13.906853 24185 solver.cpp:409]     Test net output #0: accuracy = 0.688344
I0520 22:11:13.907014 24185 solver.cpp:409]     Test net output #1: loss = 1.06003 (* 1 = 1.06003 loss)
I0520 22:11:42.427865 24185 solver.cpp:237] Iteration 2604, loss = 1.70155
I0520 22:11:42.427917 24185 solver.cpp:253]     Train net output #0: loss = 1.70155 (* 1 = 1.70155 loss)
I0520 22:11:42.427932 24185 sgd_solver.cpp:106] Iteration 2604, lr = 0.0025
I0520 22:11:50.395059 24185 solver.cpp:237] Iteration 2646, loss = 1.75766
I0520 22:11:50.395205 24185 solver.cpp:253]     Train net output #0: loss = 1.75766 (* 1 = 1.75766 loss)
I0520 22:11:50.395220 24185 sgd_solver.cpp:106] Iteration 2646, lr = 0.0025
I0520 22:11:58.362964 24185 solver.cpp:237] Iteration 2688, loss = 1.58086
I0520 22:11:58.362998 24185 solver.cpp:253]     Train net output #0: loss = 1.58086 (* 1 = 1.58086 loss)
I0520 22:11:58.363014 24185 sgd_solver.cpp:106] Iteration 2688, lr = 0.0025
I0520 22:12:06.331585 24185 solver.cpp:237] Iteration 2730, loss = 1.6285
I0520 22:12:06.331619 24185 solver.cpp:253]     Train net output #0: loss = 1.6285 (* 1 = 1.6285 loss)
I0520 22:12:06.331634 24185 sgd_solver.cpp:106] Iteration 2730, lr = 0.0025
I0520 22:12:14.302778 24185 solver.cpp:237] Iteration 2772, loss = 1.56808
I0520 22:12:14.302819 24185 solver.cpp:253]     Train net output #0: loss = 1.56808 (* 1 = 1.56808 loss)
I0520 22:12:14.302836 24185 sgd_solver.cpp:106] Iteration 2772, lr = 0.0025
I0520 22:12:22.267668 24185 solver.cpp:237] Iteration 2814, loss = 1.55709
I0520 22:12:22.267819 24185 solver.cpp:253]     Train net output #0: loss = 1.55709 (* 1 = 1.55709 loss)
I0520 22:12:22.267833 24185 sgd_solver.cpp:106] Iteration 2814, lr = 0.0025
I0520 22:12:30.238143 24185 solver.cpp:237] Iteration 2856, loss = 1.63173
I0520 22:12:30.238176 24185 solver.cpp:253]     Train net output #0: loss = 1.63173 (* 1 = 1.63173 loss)
I0520 22:12:30.238193 24185 sgd_solver.cpp:106] Iteration 2856, lr = 0.0025
I0520 22:13:00.410694 24185 solver.cpp:237] Iteration 2898, loss = 1.57307
I0520 22:13:00.410879 24185 solver.cpp:253]     Train net output #0: loss = 1.57307 (* 1 = 1.57307 loss)
I0520 22:13:00.410895 24185 sgd_solver.cpp:106] Iteration 2898, lr = 0.0025
I0520 22:13:08.380594 24185 solver.cpp:237] Iteration 2940, loss = 1.57491
I0520 22:13:08.380635 24185 solver.cpp:253]     Train net output #0: loss = 1.57491 (* 1 = 1.57491 loss)
I0520 22:13:08.380653 24185 sgd_solver.cpp:106] Iteration 2940, lr = 0.0025
I0520 22:13:16.351102 24185 solver.cpp:237] Iteration 2982, loss = 1.69257
I0520 22:13:16.351135 24185 solver.cpp:253]     Train net output #0: loss = 1.69257 (* 1 = 1.69257 loss)
I0520 22:13:16.351151 24185 sgd_solver.cpp:106] Iteration 2982, lr = 0.0025
I0520 22:13:18.817999 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_2996.caffemodel
I0520 22:13:18.997282 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_2996.solverstate
I0520 22:13:24.399806 24185 solver.cpp:237] Iteration 3024, loss = 1.52955
I0520 22:13:24.399853 24185 solver.cpp:253]     Train net output #0: loss = 1.52955 (* 1 = 1.52955 loss)
I0520 22:13:24.399866 24185 sgd_solver.cpp:106] Iteration 3024, lr = 0.0025
I0520 22:13:32.371405 24185 solver.cpp:237] Iteration 3066, loss = 1.61148
I0520 22:13:32.371554 24185 solver.cpp:253]     Train net output #0: loss = 1.61148 (* 1 = 1.61148 loss)
I0520 22:13:32.371568 24185 sgd_solver.cpp:106] Iteration 3066, lr = 0.0025
I0520 22:13:40.340703 24185 solver.cpp:237] Iteration 3108, loss = 1.54856
I0520 22:13:40.340735 24185 solver.cpp:253]     Train net output #0: loss = 1.54856 (* 1 = 1.54856 loss)
I0520 22:13:40.340752 24185 sgd_solver.cpp:106] Iteration 3108, lr = 0.0025
I0520 22:14:10.463469 24185 solver.cpp:237] Iteration 3150, loss = 1.58247
I0520 22:14:10.463637 24185 solver.cpp:253]     Train net output #0: loss = 1.58247 (* 1 = 1.58247 loss)
I0520 22:14:10.463654 24185 sgd_solver.cpp:106] Iteration 3150, lr = 0.0025
I0520 22:14:18.434089 24185 solver.cpp:237] Iteration 3192, loss = 1.50734
I0520 22:14:18.434136 24185 solver.cpp:253]     Train net output #0: loss = 1.50734 (* 1 = 1.50734 loss)
I0520 22:14:18.434152 24185 sgd_solver.cpp:106] Iteration 3192, lr = 0.0025
I0520 22:14:26.404656 24185 solver.cpp:237] Iteration 3234, loss = 1.59469
I0520 22:14:26.404691 24185 solver.cpp:253]     Train net output #0: loss = 1.59469 (* 1 = 1.59469 loss)
I0520 22:14:26.404708 24185 sgd_solver.cpp:106] Iteration 3234, lr = 0.0025
I0520 22:14:34.374465 24185 solver.cpp:237] Iteration 3276, loss = 1.49553
I0520 22:14:34.374498 24185 solver.cpp:253]     Train net output #0: loss = 1.49553 (* 1 = 1.49553 loss)
I0520 22:14:34.374512 24185 sgd_solver.cpp:106] Iteration 3276, lr = 0.0025
I0520 22:14:42.344696 24185 solver.cpp:237] Iteration 3318, loss = 1.62734
I0520 22:14:42.344835 24185 solver.cpp:253]     Train net output #0: loss = 1.62734 (* 1 = 1.62734 loss)
I0520 22:14:42.344848 24185 sgd_solver.cpp:106] Iteration 3318, lr = 0.0025
I0520 22:14:50.317600 24185 solver.cpp:237] Iteration 3360, loss = 1.60546
I0520 22:14:50.317644 24185 solver.cpp:253]     Train net output #0: loss = 1.60546 (* 1 = 1.60546 loss)
I0520 22:14:50.317661 24185 sgd_solver.cpp:106] Iteration 3360, lr = 0.0025
I0520 22:14:58.287516 24185 solver.cpp:237] Iteration 3402, loss = 1.56372
I0520 22:14:58.287549 24185 solver.cpp:253]     Train net output #0: loss = 1.56372 (* 1 = 1.56372 loss)
I0520 22:14:58.287566 24185 sgd_solver.cpp:106] Iteration 3402, lr = 0.0025
I0520 22:15:02.268230 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_3424.caffemodel
I0520 22:15:02.447969 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_3424.solverstate
I0520 22:15:03.109383 24185 solver.cpp:341] Iteration 3428, Testing net (#0)
I0520 22:16:09.389951 24185 solver.cpp:409]     Test net output #0: accuracy = 0.737062
I0520 22:16:09.390120 24185 solver.cpp:409]     Test net output #1: loss = 0.93474 (* 1 = 0.93474 loss)
I0520 22:16:34.615283 24185 solver.cpp:237] Iteration 3444, loss = 1.48714
I0520 22:16:34.615332 24185 solver.cpp:253]     Train net output #0: loss = 1.48714 (* 1 = 1.48714 loss)
I0520 22:16:34.615350 24185 sgd_solver.cpp:106] Iteration 3444, lr = 0.0025
I0520 22:16:42.581646 24185 solver.cpp:237] Iteration 3486, loss = 1.56792
I0520 22:16:42.581789 24185 solver.cpp:253]     Train net output #0: loss = 1.56792 (* 1 = 1.56792 loss)
I0520 22:16:42.581801 24185 sgd_solver.cpp:106] Iteration 3486, lr = 0.0025
I0520 22:16:50.552016 24185 solver.cpp:237] Iteration 3528, loss = 1.54353
I0520 22:16:50.552060 24185 solver.cpp:253]     Train net output #0: loss = 1.54353 (* 1 = 1.54353 loss)
I0520 22:16:50.552076 24185 sgd_solver.cpp:106] Iteration 3528, lr = 0.0025
I0520 22:16:58.523177 24185 solver.cpp:237] Iteration 3570, loss = 1.55517
I0520 22:16:58.523211 24185 solver.cpp:253]     Train net output #0: loss = 1.55517 (* 1 = 1.55517 loss)
I0520 22:16:58.523227 24185 sgd_solver.cpp:106] Iteration 3570, lr = 0.0025
I0520 22:17:06.493959 24185 solver.cpp:237] Iteration 3612, loss = 1.57082
I0520 22:17:06.493993 24185 solver.cpp:253]     Train net output #0: loss = 1.57082 (* 1 = 1.57082 loss)
I0520 22:17:06.494009 24185 sgd_solver.cpp:106] Iteration 3612, lr = 0.0025
I0520 22:17:14.464946 24185 solver.cpp:237] Iteration 3654, loss = 1.52585
I0520 22:17:14.465122 24185 solver.cpp:253]     Train net output #0: loss = 1.52585 (* 1 = 1.52585 loss)
I0520 22:17:14.465137 24185 sgd_solver.cpp:106] Iteration 3654, lr = 0.0025
I0520 22:17:22.436614 24185 solver.cpp:237] Iteration 3696, loss = 1.49125
I0520 22:17:22.436645 24185 solver.cpp:253]     Train net output #0: loss = 1.49125 (* 1 = 1.49125 loss)
I0520 22:17:22.436663 24185 sgd_solver.cpp:106] Iteration 3696, lr = 0.0025
I0520 22:17:52.613435 24185 solver.cpp:237] Iteration 3738, loss = 1.43658
I0520 22:17:52.613620 24185 solver.cpp:253]     Train net output #0: loss = 1.43658 (* 1 = 1.43658 loss)
I0520 22:17:52.613636 24185 sgd_solver.cpp:106] Iteration 3738, lr = 0.0025
I0520 22:18:00.589383 24185 solver.cpp:237] Iteration 3780, loss = 1.49584
I0520 22:18:00.589417 24185 solver.cpp:253]     Train net output #0: loss = 1.49584 (* 1 = 1.49584 loss)
I0520 22:18:00.589434 24185 sgd_solver.cpp:106] Iteration 3780, lr = 0.0025
I0520 22:18:08.562914 24185 solver.cpp:237] Iteration 3822, loss = 1.53239
I0520 22:18:08.562953 24185 solver.cpp:253]     Train net output #0: loss = 1.53239 (* 1 = 1.53239 loss)
I0520 22:18:08.562973 24185 sgd_solver.cpp:106] Iteration 3822, lr = 0.0025
I0520 22:18:14.066014 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_3852.caffemodel
I0520 22:18:14.246487 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_3852.solverstate
I0520 22:18:16.617981 24185 solver.cpp:237] Iteration 3864, loss = 1.51227
I0520 22:18:16.618024 24185 solver.cpp:253]     Train net output #0: loss = 1.51227 (* 1 = 1.51227 loss)
I0520 22:18:16.618046 24185 sgd_solver.cpp:106] Iteration 3864, lr = 0.0025
I0520 22:18:24.589429 24185 solver.cpp:237] Iteration 3906, loss = 1.60199
I0520 22:18:24.589583 24185 solver.cpp:253]     Train net output #0: loss = 1.60199 (* 1 = 1.60199 loss)
I0520 22:18:24.589596 24185 sgd_solver.cpp:106] Iteration 3906, lr = 0.0025
I0520 22:18:32.561800 24185 solver.cpp:237] Iteration 3948, loss = 1.57034
I0520 22:18:32.561842 24185 solver.cpp:253]     Train net output #0: loss = 1.57034 (* 1 = 1.57034 loss)
I0520 22:18:32.561862 24185 sgd_solver.cpp:106] Iteration 3948, lr = 0.0025
I0520 22:18:40.533838 24185 solver.cpp:237] Iteration 3990, loss = 1.47232
I0520 22:18:40.533871 24185 solver.cpp:253]     Train net output #0: loss = 1.47232 (* 1 = 1.47232 loss)
I0520 22:18:40.533887 24185 sgd_solver.cpp:106] Iteration 3990, lr = 0.0025
I0520 22:19:10.737962 24185 solver.cpp:237] Iteration 4032, loss = 1.539
I0520 22:19:10.738133 24185 solver.cpp:253]     Train net output #0: loss = 1.539 (* 1 = 1.539 loss)
I0520 22:19:10.738148 24185 sgd_solver.cpp:106] Iteration 4032, lr = 0.0025
I0520 22:19:18.706562 24185 solver.cpp:237] Iteration 4074, loss = 1.60446
I0520 22:19:18.706609 24185 solver.cpp:253]     Train net output #0: loss = 1.60446 (* 1 = 1.60446 loss)
I0520 22:19:18.706625 24185 sgd_solver.cpp:106] Iteration 4074, lr = 0.0025
I0520 22:19:26.677090 24185 solver.cpp:237] Iteration 4116, loss = 1.51151
I0520 22:19:26.677129 24185 solver.cpp:253]     Train net output #0: loss = 1.51151 (* 1 = 1.51151 loss)
I0520 22:19:26.677145 24185 sgd_solver.cpp:106] Iteration 4116, lr = 0.0025
I0520 22:19:34.649233 24185 solver.cpp:237] Iteration 4158, loss = 1.50227
I0520 22:19:34.649266 24185 solver.cpp:253]     Train net output #0: loss = 1.50227 (* 1 = 1.50227 loss)
I0520 22:19:34.649282 24185 sgd_solver.cpp:106] Iteration 4158, lr = 0.0025
I0520 22:19:42.621269 24185 solver.cpp:237] Iteration 4200, loss = 1.46731
I0520 22:19:42.621412 24185 solver.cpp:253]     Train net output #0: loss = 1.46731 (* 1 = 1.46731 loss)
I0520 22:19:42.621426 24185 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0520 22:19:50.592072 24185 solver.cpp:237] Iteration 4242, loss = 1.55692
I0520 22:19:50.592108 24185 solver.cpp:253]     Train net output #0: loss = 1.55692 (* 1 = 1.55692 loss)
I0520 22:19:50.592128 24185 sgd_solver.cpp:106] Iteration 4242, lr = 0.0025
I0520 22:19:57.614974 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_4280.caffemodel
I0520 22:19:57.798249 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_4280.solverstate
I0520 22:19:58.699203 24185 solver.cpp:237] Iteration 4284, loss = 1.54738
I0520 22:19:58.699251 24185 solver.cpp:253]     Train net output #0: loss = 1.54738 (* 1 = 1.54738 loss)
I0520 22:19:58.699268 24185 sgd_solver.cpp:106] Iteration 4284, lr = 0.0025
I0520 22:19:58.699640 24185 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_4285.caffemodel
I0520 22:19:58.883263 24185 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552_iter_4285.solverstate
I0520 22:19:58.920298 24185 solver.cpp:341] Iteration 4285, Testing net (#0)
I0520 22:20:44.213878 24185 solver.cpp:409]     Test net output #0: accuracy = 0.754419
I0520 22:20:44.214056 24185 solver.cpp:409]     Test net output #1: loss = 0.854741 (* 1 = 0.854741 loss)
I0520 22:20:44.214071 24185 solver.cpp:326] Optimization Done.
I0520 22:20:44.214082 24185 caffe.cpp:215] Optimization Done.
Application 11235569 resources: utime ~1257s, stime ~227s, Rss ~5329328, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_350_2016-05-20T11.20.45.470552.solver"
	User time (seconds): 0.54
	System time (seconds): 0.22
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:48.91
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15072
	Voluntary context switches: 3068
	Involuntary context switches: 202
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
