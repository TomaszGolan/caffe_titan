2806213
I0521 03:42:39.171636 30122 caffe.cpp:184] Using GPUs 0
I0521 03:42:39.597344 30122 solver.cpp:48] Initializing solver from parameters: 
test_iter: 241
test_interval: 483
base_lr: 0.0025
display: 24
max_iter: 2419
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 241
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016.prototxt"
I0521 03:42:39.598935 30122 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016.prototxt
I0521 03:42:39.620292 30122 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 03:42:39.620352 30122 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 03:42:39.620694 30122 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 620
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 03:42:39.620872 30122 layer_factory.hpp:77] Creating layer data_hdf5
I0521 03:42:39.620895 30122 net.cpp:106] Creating Layer data_hdf5
I0521 03:42:39.620909 30122 net.cpp:411] data_hdf5 -> data
I0521 03:42:39.620942 30122 net.cpp:411] data_hdf5 -> label
I0521 03:42:39.620975 30122 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 03:42:39.622217 30122 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 03:42:39.624548 30122 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 03:43:01.172705 30122 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 03:43:01.177903 30122 net.cpp:150] Setting up data_hdf5
I0521 03:43:01.177943 30122 net.cpp:157] Top shape: 620 1 127 50 (3937000)
I0521 03:43:01.177958 30122 net.cpp:157] Top shape: 620 (620)
I0521 03:43:01.177969 30122 net.cpp:165] Memory required for data: 15750480
I0521 03:43:01.177983 30122 layer_factory.hpp:77] Creating layer conv1
I0521 03:43:01.178017 30122 net.cpp:106] Creating Layer conv1
I0521 03:43:01.178028 30122 net.cpp:454] conv1 <- data
I0521 03:43:01.178051 30122 net.cpp:411] conv1 -> conv1
I0521 03:43:01.538179 30122 net.cpp:150] Setting up conv1
I0521 03:43:01.538226 30122 net.cpp:157] Top shape: 620 12 120 48 (42854400)
I0521 03:43:01.538238 30122 net.cpp:165] Memory required for data: 187168080
I0521 03:43:01.538266 30122 layer_factory.hpp:77] Creating layer relu1
I0521 03:43:01.538287 30122 net.cpp:106] Creating Layer relu1
I0521 03:43:01.538298 30122 net.cpp:454] relu1 <- conv1
I0521 03:43:01.538312 30122 net.cpp:397] relu1 -> conv1 (in-place)
I0521 03:43:01.538839 30122 net.cpp:150] Setting up relu1
I0521 03:43:01.538857 30122 net.cpp:157] Top shape: 620 12 120 48 (42854400)
I0521 03:43:01.538867 30122 net.cpp:165] Memory required for data: 358585680
I0521 03:43:01.538877 30122 layer_factory.hpp:77] Creating layer pool1
I0521 03:43:01.538894 30122 net.cpp:106] Creating Layer pool1
I0521 03:43:01.538904 30122 net.cpp:454] pool1 <- conv1
I0521 03:43:01.538918 30122 net.cpp:411] pool1 -> pool1
I0521 03:43:01.538997 30122 net.cpp:150] Setting up pool1
I0521 03:43:01.539011 30122 net.cpp:157] Top shape: 620 12 60 48 (21427200)
I0521 03:43:01.539021 30122 net.cpp:165] Memory required for data: 444294480
I0521 03:43:01.539031 30122 layer_factory.hpp:77] Creating layer conv2
I0521 03:43:01.539055 30122 net.cpp:106] Creating Layer conv2
I0521 03:43:01.539065 30122 net.cpp:454] conv2 <- pool1
I0521 03:43:01.539079 30122 net.cpp:411] conv2 -> conv2
I0521 03:43:01.541744 30122 net.cpp:150] Setting up conv2
I0521 03:43:01.541771 30122 net.cpp:157] Top shape: 620 20 54 46 (30801600)
I0521 03:43:01.541782 30122 net.cpp:165] Memory required for data: 567500880
I0521 03:43:01.541801 30122 layer_factory.hpp:77] Creating layer relu2
I0521 03:43:01.541815 30122 net.cpp:106] Creating Layer relu2
I0521 03:43:01.541826 30122 net.cpp:454] relu2 <- conv2
I0521 03:43:01.541838 30122 net.cpp:397] relu2 -> conv2 (in-place)
I0521 03:43:01.542171 30122 net.cpp:150] Setting up relu2
I0521 03:43:01.542186 30122 net.cpp:157] Top shape: 620 20 54 46 (30801600)
I0521 03:43:01.542196 30122 net.cpp:165] Memory required for data: 690707280
I0521 03:43:01.542206 30122 layer_factory.hpp:77] Creating layer pool2
I0521 03:43:01.542217 30122 net.cpp:106] Creating Layer pool2
I0521 03:43:01.542227 30122 net.cpp:454] pool2 <- conv2
I0521 03:43:01.542251 30122 net.cpp:411] pool2 -> pool2
I0521 03:43:01.542321 30122 net.cpp:150] Setting up pool2
I0521 03:43:01.542335 30122 net.cpp:157] Top shape: 620 20 27 46 (15400800)
I0521 03:43:01.542345 30122 net.cpp:165] Memory required for data: 752310480
I0521 03:43:01.542352 30122 layer_factory.hpp:77] Creating layer conv3
I0521 03:43:01.542371 30122 net.cpp:106] Creating Layer conv3
I0521 03:43:01.542382 30122 net.cpp:454] conv3 <- pool2
I0521 03:43:01.542397 30122 net.cpp:411] conv3 -> conv3
I0521 03:43:01.544328 30122 net.cpp:150] Setting up conv3
I0521 03:43:01.544348 30122 net.cpp:157] Top shape: 620 28 22 44 (16804480)
I0521 03:43:01.544358 30122 net.cpp:165] Memory required for data: 819528400
I0521 03:43:01.544375 30122 layer_factory.hpp:77] Creating layer relu3
I0521 03:43:01.544391 30122 net.cpp:106] Creating Layer relu3
I0521 03:43:01.544401 30122 net.cpp:454] relu3 <- conv3
I0521 03:43:01.544414 30122 net.cpp:397] relu3 -> conv3 (in-place)
I0521 03:43:01.544883 30122 net.cpp:150] Setting up relu3
I0521 03:43:01.544900 30122 net.cpp:157] Top shape: 620 28 22 44 (16804480)
I0521 03:43:01.544910 30122 net.cpp:165] Memory required for data: 886746320
I0521 03:43:01.544920 30122 layer_factory.hpp:77] Creating layer pool3
I0521 03:43:01.544934 30122 net.cpp:106] Creating Layer pool3
I0521 03:43:01.544944 30122 net.cpp:454] pool3 <- conv3
I0521 03:43:01.544956 30122 net.cpp:411] pool3 -> pool3
I0521 03:43:01.545023 30122 net.cpp:150] Setting up pool3
I0521 03:43:01.545037 30122 net.cpp:157] Top shape: 620 28 11 44 (8402240)
I0521 03:43:01.545047 30122 net.cpp:165] Memory required for data: 920355280
I0521 03:43:01.545056 30122 layer_factory.hpp:77] Creating layer conv4
I0521 03:43:01.545073 30122 net.cpp:106] Creating Layer conv4
I0521 03:43:01.545083 30122 net.cpp:454] conv4 <- pool3
I0521 03:43:01.545097 30122 net.cpp:411] conv4 -> conv4
I0521 03:43:01.547951 30122 net.cpp:150] Setting up conv4
I0521 03:43:01.547979 30122 net.cpp:157] Top shape: 620 36 6 42 (5624640)
I0521 03:43:01.547991 30122 net.cpp:165] Memory required for data: 942853840
I0521 03:43:01.548007 30122 layer_factory.hpp:77] Creating layer relu4
I0521 03:43:01.548020 30122 net.cpp:106] Creating Layer relu4
I0521 03:43:01.548032 30122 net.cpp:454] relu4 <- conv4
I0521 03:43:01.548044 30122 net.cpp:397] relu4 -> conv4 (in-place)
I0521 03:43:01.548516 30122 net.cpp:150] Setting up relu4
I0521 03:43:01.548532 30122 net.cpp:157] Top shape: 620 36 6 42 (5624640)
I0521 03:43:01.548542 30122 net.cpp:165] Memory required for data: 965352400
I0521 03:43:01.548552 30122 layer_factory.hpp:77] Creating layer pool4
I0521 03:43:01.548565 30122 net.cpp:106] Creating Layer pool4
I0521 03:43:01.548575 30122 net.cpp:454] pool4 <- conv4
I0521 03:43:01.548589 30122 net.cpp:411] pool4 -> pool4
I0521 03:43:01.548657 30122 net.cpp:150] Setting up pool4
I0521 03:43:01.548671 30122 net.cpp:157] Top shape: 620 36 3 42 (2812320)
I0521 03:43:01.548681 30122 net.cpp:165] Memory required for data: 976601680
I0521 03:43:01.548691 30122 layer_factory.hpp:77] Creating layer ip1
I0521 03:43:01.548710 30122 net.cpp:106] Creating Layer ip1
I0521 03:43:01.548720 30122 net.cpp:454] ip1 <- pool4
I0521 03:43:01.548734 30122 net.cpp:411] ip1 -> ip1
I0521 03:43:01.564160 30122 net.cpp:150] Setting up ip1
I0521 03:43:01.564189 30122 net.cpp:157] Top shape: 620 196 (121520)
I0521 03:43:01.564201 30122 net.cpp:165] Memory required for data: 977087760
I0521 03:43:01.564229 30122 layer_factory.hpp:77] Creating layer relu5
I0521 03:43:01.564244 30122 net.cpp:106] Creating Layer relu5
I0521 03:43:01.564254 30122 net.cpp:454] relu5 <- ip1
I0521 03:43:01.564268 30122 net.cpp:397] relu5 -> ip1 (in-place)
I0521 03:43:01.564612 30122 net.cpp:150] Setting up relu5
I0521 03:43:01.564626 30122 net.cpp:157] Top shape: 620 196 (121520)
I0521 03:43:01.564637 30122 net.cpp:165] Memory required for data: 977573840
I0521 03:43:01.564647 30122 layer_factory.hpp:77] Creating layer drop1
I0521 03:43:01.564668 30122 net.cpp:106] Creating Layer drop1
I0521 03:43:01.564678 30122 net.cpp:454] drop1 <- ip1
I0521 03:43:01.564703 30122 net.cpp:397] drop1 -> ip1 (in-place)
I0521 03:43:01.564749 30122 net.cpp:150] Setting up drop1
I0521 03:43:01.564762 30122 net.cpp:157] Top shape: 620 196 (121520)
I0521 03:43:01.564772 30122 net.cpp:165] Memory required for data: 978059920
I0521 03:43:01.564782 30122 layer_factory.hpp:77] Creating layer ip2
I0521 03:43:01.564800 30122 net.cpp:106] Creating Layer ip2
I0521 03:43:01.564810 30122 net.cpp:454] ip2 <- ip1
I0521 03:43:01.564823 30122 net.cpp:411] ip2 -> ip2
I0521 03:43:01.565289 30122 net.cpp:150] Setting up ip2
I0521 03:43:01.565302 30122 net.cpp:157] Top shape: 620 98 (60760)
I0521 03:43:01.565312 30122 net.cpp:165] Memory required for data: 978302960
I0521 03:43:01.565327 30122 layer_factory.hpp:77] Creating layer relu6
I0521 03:43:01.565340 30122 net.cpp:106] Creating Layer relu6
I0521 03:43:01.565349 30122 net.cpp:454] relu6 <- ip2
I0521 03:43:01.565361 30122 net.cpp:397] relu6 -> ip2 (in-place)
I0521 03:43:01.565872 30122 net.cpp:150] Setting up relu6
I0521 03:43:01.565887 30122 net.cpp:157] Top shape: 620 98 (60760)
I0521 03:43:01.565898 30122 net.cpp:165] Memory required for data: 978546000
I0521 03:43:01.565908 30122 layer_factory.hpp:77] Creating layer drop2
I0521 03:43:01.565922 30122 net.cpp:106] Creating Layer drop2
I0521 03:43:01.565932 30122 net.cpp:454] drop2 <- ip2
I0521 03:43:01.565943 30122 net.cpp:397] drop2 -> ip2 (in-place)
I0521 03:43:01.565985 30122 net.cpp:150] Setting up drop2
I0521 03:43:01.565999 30122 net.cpp:157] Top shape: 620 98 (60760)
I0521 03:43:01.566009 30122 net.cpp:165] Memory required for data: 978789040
I0521 03:43:01.566020 30122 layer_factory.hpp:77] Creating layer ip3
I0521 03:43:01.566032 30122 net.cpp:106] Creating Layer ip3
I0521 03:43:01.566042 30122 net.cpp:454] ip3 <- ip2
I0521 03:43:01.566054 30122 net.cpp:411] ip3 -> ip3
I0521 03:43:01.566265 30122 net.cpp:150] Setting up ip3
I0521 03:43:01.566278 30122 net.cpp:157] Top shape: 620 11 (6820)
I0521 03:43:01.566288 30122 net.cpp:165] Memory required for data: 978816320
I0521 03:43:01.566303 30122 layer_factory.hpp:77] Creating layer drop3
I0521 03:43:01.566315 30122 net.cpp:106] Creating Layer drop3
I0521 03:43:01.566324 30122 net.cpp:454] drop3 <- ip3
I0521 03:43:01.566336 30122 net.cpp:397] drop3 -> ip3 (in-place)
I0521 03:43:01.566375 30122 net.cpp:150] Setting up drop3
I0521 03:43:01.566387 30122 net.cpp:157] Top shape: 620 11 (6820)
I0521 03:43:01.566397 30122 net.cpp:165] Memory required for data: 978843600
I0521 03:43:01.566407 30122 layer_factory.hpp:77] Creating layer loss
I0521 03:43:01.566433 30122 net.cpp:106] Creating Layer loss
I0521 03:43:01.566443 30122 net.cpp:454] loss <- ip3
I0521 03:43:01.566454 30122 net.cpp:454] loss <- label
I0521 03:43:01.566467 30122 net.cpp:411] loss -> loss
I0521 03:43:01.566483 30122 layer_factory.hpp:77] Creating layer loss
I0521 03:43:01.567132 30122 net.cpp:150] Setting up loss
I0521 03:43:01.567153 30122 net.cpp:157] Top shape: (1)
I0521 03:43:01.567167 30122 net.cpp:160]     with loss weight 1
I0521 03:43:01.567209 30122 net.cpp:165] Memory required for data: 978843604
I0521 03:43:01.567219 30122 net.cpp:226] loss needs backward computation.
I0521 03:43:01.567230 30122 net.cpp:226] drop3 needs backward computation.
I0521 03:43:01.567239 30122 net.cpp:226] ip3 needs backward computation.
I0521 03:43:01.567250 30122 net.cpp:226] drop2 needs backward computation.
I0521 03:43:01.567260 30122 net.cpp:226] relu6 needs backward computation.
I0521 03:43:01.567270 30122 net.cpp:226] ip2 needs backward computation.
I0521 03:43:01.567279 30122 net.cpp:226] drop1 needs backward computation.
I0521 03:43:01.567288 30122 net.cpp:226] relu5 needs backward computation.
I0521 03:43:01.567298 30122 net.cpp:226] ip1 needs backward computation.
I0521 03:43:01.567308 30122 net.cpp:226] pool4 needs backward computation.
I0521 03:43:01.567318 30122 net.cpp:226] relu4 needs backward computation.
I0521 03:43:01.567328 30122 net.cpp:226] conv4 needs backward computation.
I0521 03:43:01.567339 30122 net.cpp:226] pool3 needs backward computation.
I0521 03:43:01.567358 30122 net.cpp:226] relu3 needs backward computation.
I0521 03:43:01.567368 30122 net.cpp:226] conv3 needs backward computation.
I0521 03:43:01.567378 30122 net.cpp:226] pool2 needs backward computation.
I0521 03:43:01.567389 30122 net.cpp:226] relu2 needs backward computation.
I0521 03:43:01.567399 30122 net.cpp:226] conv2 needs backward computation.
I0521 03:43:01.567409 30122 net.cpp:226] pool1 needs backward computation.
I0521 03:43:01.567420 30122 net.cpp:226] relu1 needs backward computation.
I0521 03:43:01.567430 30122 net.cpp:226] conv1 needs backward computation.
I0521 03:43:01.567440 30122 net.cpp:228] data_hdf5 does not need backward computation.
I0521 03:43:01.567451 30122 net.cpp:270] This network produces output loss
I0521 03:43:01.567474 30122 net.cpp:283] Network initialization done.
I0521 03:43:01.569072 30122 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016.prototxt
I0521 03:43:01.569141 30122 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 03:43:01.569497 30122 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 620
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 03:43:01.569687 30122 layer_factory.hpp:77] Creating layer data_hdf5
I0521 03:43:01.569702 30122 net.cpp:106] Creating Layer data_hdf5
I0521 03:43:01.569715 30122 net.cpp:411] data_hdf5 -> data
I0521 03:43:01.569730 30122 net.cpp:411] data_hdf5 -> label
I0521 03:43:01.569746 30122 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 03:43:01.570910 30122 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 03:43:22.906378 30122 net.cpp:150] Setting up data_hdf5
I0521 03:43:22.906549 30122 net.cpp:157] Top shape: 620 1 127 50 (3937000)
I0521 03:43:22.906561 30122 net.cpp:157] Top shape: 620 (620)
I0521 03:43:22.906570 30122 net.cpp:165] Memory required for data: 15750480
I0521 03:43:22.906589 30122 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 03:43:22.906620 30122 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 03:43:22.906631 30122 net.cpp:454] label_data_hdf5_1_split <- label
I0521 03:43:22.906649 30122 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 03:43:22.906671 30122 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 03:43:22.906744 30122 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 03:43:22.906757 30122 net.cpp:157] Top shape: 620 (620)
I0521 03:43:22.906767 30122 net.cpp:157] Top shape: 620 (620)
I0521 03:43:22.906777 30122 net.cpp:165] Memory required for data: 15755440
I0521 03:43:22.906788 30122 layer_factory.hpp:77] Creating layer conv1
I0521 03:43:22.906810 30122 net.cpp:106] Creating Layer conv1
I0521 03:43:22.906821 30122 net.cpp:454] conv1 <- data
I0521 03:43:22.906834 30122 net.cpp:411] conv1 -> conv1
I0521 03:43:22.908754 30122 net.cpp:150] Setting up conv1
I0521 03:43:22.908778 30122 net.cpp:157] Top shape: 620 12 120 48 (42854400)
I0521 03:43:22.908789 30122 net.cpp:165] Memory required for data: 187173040
I0521 03:43:22.908809 30122 layer_factory.hpp:77] Creating layer relu1
I0521 03:43:22.908824 30122 net.cpp:106] Creating Layer relu1
I0521 03:43:22.908834 30122 net.cpp:454] relu1 <- conv1
I0521 03:43:22.908846 30122 net.cpp:397] relu1 -> conv1 (in-place)
I0521 03:43:22.909343 30122 net.cpp:150] Setting up relu1
I0521 03:43:22.909358 30122 net.cpp:157] Top shape: 620 12 120 48 (42854400)
I0521 03:43:22.909368 30122 net.cpp:165] Memory required for data: 358590640
I0521 03:43:22.909378 30122 layer_factory.hpp:77] Creating layer pool1
I0521 03:43:22.909394 30122 net.cpp:106] Creating Layer pool1
I0521 03:43:22.909404 30122 net.cpp:454] pool1 <- conv1
I0521 03:43:22.909416 30122 net.cpp:411] pool1 -> pool1
I0521 03:43:22.909492 30122 net.cpp:150] Setting up pool1
I0521 03:43:22.909504 30122 net.cpp:157] Top shape: 620 12 60 48 (21427200)
I0521 03:43:22.909514 30122 net.cpp:165] Memory required for data: 444299440
I0521 03:43:22.909524 30122 layer_factory.hpp:77] Creating layer conv2
I0521 03:43:22.909544 30122 net.cpp:106] Creating Layer conv2
I0521 03:43:22.909554 30122 net.cpp:454] conv2 <- pool1
I0521 03:43:22.909569 30122 net.cpp:411] conv2 -> conv2
I0521 03:43:22.911484 30122 net.cpp:150] Setting up conv2
I0521 03:43:22.911507 30122 net.cpp:157] Top shape: 620 20 54 46 (30801600)
I0521 03:43:22.911519 30122 net.cpp:165] Memory required for data: 567505840
I0521 03:43:22.911535 30122 layer_factory.hpp:77] Creating layer relu2
I0521 03:43:22.911548 30122 net.cpp:106] Creating Layer relu2
I0521 03:43:22.911558 30122 net.cpp:454] relu2 <- conv2
I0521 03:43:22.911571 30122 net.cpp:397] relu2 -> conv2 (in-place)
I0521 03:43:22.911903 30122 net.cpp:150] Setting up relu2
I0521 03:43:22.911917 30122 net.cpp:157] Top shape: 620 20 54 46 (30801600)
I0521 03:43:22.911928 30122 net.cpp:165] Memory required for data: 690712240
I0521 03:43:22.911937 30122 layer_factory.hpp:77] Creating layer pool2
I0521 03:43:22.911950 30122 net.cpp:106] Creating Layer pool2
I0521 03:43:22.911960 30122 net.cpp:454] pool2 <- conv2
I0521 03:43:22.911972 30122 net.cpp:411] pool2 -> pool2
I0521 03:43:22.912042 30122 net.cpp:150] Setting up pool2
I0521 03:43:22.912056 30122 net.cpp:157] Top shape: 620 20 27 46 (15400800)
I0521 03:43:22.912065 30122 net.cpp:165] Memory required for data: 752315440
I0521 03:43:22.912075 30122 layer_factory.hpp:77] Creating layer conv3
I0521 03:43:22.912094 30122 net.cpp:106] Creating Layer conv3
I0521 03:43:22.912104 30122 net.cpp:454] conv3 <- pool2
I0521 03:43:22.912120 30122 net.cpp:411] conv3 -> conv3
I0521 03:43:22.914078 30122 net.cpp:150] Setting up conv3
I0521 03:43:22.914101 30122 net.cpp:157] Top shape: 620 28 22 44 (16804480)
I0521 03:43:22.914111 30122 net.cpp:165] Memory required for data: 819533360
I0521 03:43:22.914144 30122 layer_factory.hpp:77] Creating layer relu3
I0521 03:43:22.914156 30122 net.cpp:106] Creating Layer relu3
I0521 03:43:22.914166 30122 net.cpp:454] relu3 <- conv3
I0521 03:43:22.914180 30122 net.cpp:397] relu3 -> conv3 (in-place)
I0521 03:43:22.914660 30122 net.cpp:150] Setting up relu3
I0521 03:43:22.914675 30122 net.cpp:157] Top shape: 620 28 22 44 (16804480)
I0521 03:43:22.914686 30122 net.cpp:165] Memory required for data: 886751280
I0521 03:43:22.914695 30122 layer_factory.hpp:77] Creating layer pool3
I0521 03:43:22.914708 30122 net.cpp:106] Creating Layer pool3
I0521 03:43:22.914718 30122 net.cpp:454] pool3 <- conv3
I0521 03:43:22.914731 30122 net.cpp:411] pool3 -> pool3
I0521 03:43:22.914803 30122 net.cpp:150] Setting up pool3
I0521 03:43:22.914816 30122 net.cpp:157] Top shape: 620 28 11 44 (8402240)
I0521 03:43:22.914826 30122 net.cpp:165] Memory required for data: 920360240
I0521 03:43:22.914835 30122 layer_factory.hpp:77] Creating layer conv4
I0521 03:43:22.914854 30122 net.cpp:106] Creating Layer conv4
I0521 03:43:22.914863 30122 net.cpp:454] conv4 <- pool3
I0521 03:43:22.914875 30122 net.cpp:411] conv4 -> conv4
I0521 03:43:22.916921 30122 net.cpp:150] Setting up conv4
I0521 03:43:22.916939 30122 net.cpp:157] Top shape: 620 36 6 42 (5624640)
I0521 03:43:22.916949 30122 net.cpp:165] Memory required for data: 942858800
I0521 03:43:22.916963 30122 layer_factory.hpp:77] Creating layer relu4
I0521 03:43:22.916977 30122 net.cpp:106] Creating Layer relu4
I0521 03:43:22.916987 30122 net.cpp:454] relu4 <- conv4
I0521 03:43:22.916999 30122 net.cpp:397] relu4 -> conv4 (in-place)
I0521 03:43:22.917469 30122 net.cpp:150] Setting up relu4
I0521 03:43:22.917484 30122 net.cpp:157] Top shape: 620 36 6 42 (5624640)
I0521 03:43:22.917493 30122 net.cpp:165] Memory required for data: 965357360
I0521 03:43:22.917503 30122 layer_factory.hpp:77] Creating layer pool4
I0521 03:43:22.917516 30122 net.cpp:106] Creating Layer pool4
I0521 03:43:22.917526 30122 net.cpp:454] pool4 <- conv4
I0521 03:43:22.917539 30122 net.cpp:411] pool4 -> pool4
I0521 03:43:22.917610 30122 net.cpp:150] Setting up pool4
I0521 03:43:22.917623 30122 net.cpp:157] Top shape: 620 36 3 42 (2812320)
I0521 03:43:22.917632 30122 net.cpp:165] Memory required for data: 976606640
I0521 03:43:22.917642 30122 layer_factory.hpp:77] Creating layer ip1
I0521 03:43:22.917657 30122 net.cpp:106] Creating Layer ip1
I0521 03:43:22.917667 30122 net.cpp:454] ip1 <- pool4
I0521 03:43:22.917680 30122 net.cpp:411] ip1 -> ip1
I0521 03:43:22.933177 30122 net.cpp:150] Setting up ip1
I0521 03:43:22.933207 30122 net.cpp:157] Top shape: 620 196 (121520)
I0521 03:43:22.933220 30122 net.cpp:165] Memory required for data: 977092720
I0521 03:43:22.933243 30122 layer_factory.hpp:77] Creating layer relu5
I0521 03:43:22.933259 30122 net.cpp:106] Creating Layer relu5
I0521 03:43:22.933269 30122 net.cpp:454] relu5 <- ip1
I0521 03:43:22.933282 30122 net.cpp:397] relu5 -> ip1 (in-place)
I0521 03:43:22.933629 30122 net.cpp:150] Setting up relu5
I0521 03:43:22.933643 30122 net.cpp:157] Top shape: 620 196 (121520)
I0521 03:43:22.933653 30122 net.cpp:165] Memory required for data: 977578800
I0521 03:43:22.933662 30122 layer_factory.hpp:77] Creating layer drop1
I0521 03:43:22.933682 30122 net.cpp:106] Creating Layer drop1
I0521 03:43:22.933692 30122 net.cpp:454] drop1 <- ip1
I0521 03:43:22.933704 30122 net.cpp:397] drop1 -> ip1 (in-place)
I0521 03:43:22.933748 30122 net.cpp:150] Setting up drop1
I0521 03:43:22.933760 30122 net.cpp:157] Top shape: 620 196 (121520)
I0521 03:43:22.933770 30122 net.cpp:165] Memory required for data: 978064880
I0521 03:43:22.933779 30122 layer_factory.hpp:77] Creating layer ip2
I0521 03:43:22.933794 30122 net.cpp:106] Creating Layer ip2
I0521 03:43:22.933804 30122 net.cpp:454] ip2 <- ip1
I0521 03:43:22.933816 30122 net.cpp:411] ip2 -> ip2
I0521 03:43:22.934296 30122 net.cpp:150] Setting up ip2
I0521 03:43:22.934310 30122 net.cpp:157] Top shape: 620 98 (60760)
I0521 03:43:22.934320 30122 net.cpp:165] Memory required for data: 978307920
I0521 03:43:22.934348 30122 layer_factory.hpp:77] Creating layer relu6
I0521 03:43:22.934361 30122 net.cpp:106] Creating Layer relu6
I0521 03:43:22.934371 30122 net.cpp:454] relu6 <- ip2
I0521 03:43:22.934383 30122 net.cpp:397] relu6 -> ip2 (in-place)
I0521 03:43:22.934919 30122 net.cpp:150] Setting up relu6
I0521 03:43:22.934936 30122 net.cpp:157] Top shape: 620 98 (60760)
I0521 03:43:22.934947 30122 net.cpp:165] Memory required for data: 978550960
I0521 03:43:22.934957 30122 layer_factory.hpp:77] Creating layer drop2
I0521 03:43:22.934970 30122 net.cpp:106] Creating Layer drop2
I0521 03:43:22.934980 30122 net.cpp:454] drop2 <- ip2
I0521 03:43:22.934993 30122 net.cpp:397] drop2 -> ip2 (in-place)
I0521 03:43:22.935037 30122 net.cpp:150] Setting up drop2
I0521 03:43:22.935050 30122 net.cpp:157] Top shape: 620 98 (60760)
I0521 03:43:22.935060 30122 net.cpp:165] Memory required for data: 978794000
I0521 03:43:22.935070 30122 layer_factory.hpp:77] Creating layer ip3
I0521 03:43:22.935083 30122 net.cpp:106] Creating Layer ip3
I0521 03:43:22.935093 30122 net.cpp:454] ip3 <- ip2
I0521 03:43:22.935107 30122 net.cpp:411] ip3 -> ip3
I0521 03:43:22.935330 30122 net.cpp:150] Setting up ip3
I0521 03:43:22.935343 30122 net.cpp:157] Top shape: 620 11 (6820)
I0521 03:43:22.935353 30122 net.cpp:165] Memory required for data: 978821280
I0521 03:43:22.935367 30122 layer_factory.hpp:77] Creating layer drop3
I0521 03:43:22.935381 30122 net.cpp:106] Creating Layer drop3
I0521 03:43:22.935390 30122 net.cpp:454] drop3 <- ip3
I0521 03:43:22.935403 30122 net.cpp:397] drop3 -> ip3 (in-place)
I0521 03:43:22.935444 30122 net.cpp:150] Setting up drop3
I0521 03:43:22.935457 30122 net.cpp:157] Top shape: 620 11 (6820)
I0521 03:43:22.935467 30122 net.cpp:165] Memory required for data: 978848560
I0521 03:43:22.935477 30122 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 03:43:22.935492 30122 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 03:43:22.935502 30122 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 03:43:22.935514 30122 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 03:43:22.935528 30122 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 03:43:22.935601 30122 net.cpp:150] Setting up ip3_drop3_0_split
I0521 03:43:22.935613 30122 net.cpp:157] Top shape: 620 11 (6820)
I0521 03:43:22.935626 30122 net.cpp:157] Top shape: 620 11 (6820)
I0521 03:43:22.935636 30122 net.cpp:165] Memory required for data: 978903120
I0521 03:43:22.935645 30122 layer_factory.hpp:77] Creating layer accuracy
I0521 03:43:22.935668 30122 net.cpp:106] Creating Layer accuracy
I0521 03:43:22.935678 30122 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 03:43:22.935690 30122 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 03:43:22.935704 30122 net.cpp:411] accuracy -> accuracy
I0521 03:43:22.935729 30122 net.cpp:150] Setting up accuracy
I0521 03:43:22.935740 30122 net.cpp:157] Top shape: (1)
I0521 03:43:22.935750 30122 net.cpp:165] Memory required for data: 978903124
I0521 03:43:22.935760 30122 layer_factory.hpp:77] Creating layer loss
I0521 03:43:22.935772 30122 net.cpp:106] Creating Layer loss
I0521 03:43:22.935781 30122 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 03:43:22.935791 30122 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 03:43:22.935806 30122 net.cpp:411] loss -> loss
I0521 03:43:22.935822 30122 layer_factory.hpp:77] Creating layer loss
I0521 03:43:22.936313 30122 net.cpp:150] Setting up loss
I0521 03:43:22.936326 30122 net.cpp:157] Top shape: (1)
I0521 03:43:22.936336 30122 net.cpp:160]     with loss weight 1
I0521 03:43:22.936353 30122 net.cpp:165] Memory required for data: 978903128
I0521 03:43:22.936364 30122 net.cpp:226] loss needs backward computation.
I0521 03:43:22.936374 30122 net.cpp:228] accuracy does not need backward computation.
I0521 03:43:22.936385 30122 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 03:43:22.936395 30122 net.cpp:226] drop3 needs backward computation.
I0521 03:43:22.936405 30122 net.cpp:226] ip3 needs backward computation.
I0521 03:43:22.936414 30122 net.cpp:226] drop2 needs backward computation.
I0521 03:43:22.936432 30122 net.cpp:226] relu6 needs backward computation.
I0521 03:43:22.936442 30122 net.cpp:226] ip2 needs backward computation.
I0521 03:43:22.936452 30122 net.cpp:226] drop1 needs backward computation.
I0521 03:43:22.936461 30122 net.cpp:226] relu5 needs backward computation.
I0521 03:43:22.936471 30122 net.cpp:226] ip1 needs backward computation.
I0521 03:43:22.936480 30122 net.cpp:226] pool4 needs backward computation.
I0521 03:43:22.936491 30122 net.cpp:226] relu4 needs backward computation.
I0521 03:43:22.936501 30122 net.cpp:226] conv4 needs backward computation.
I0521 03:43:22.936511 30122 net.cpp:226] pool3 needs backward computation.
I0521 03:43:22.936522 30122 net.cpp:226] relu3 needs backward computation.
I0521 03:43:22.936530 30122 net.cpp:226] conv3 needs backward computation.
I0521 03:43:22.936540 30122 net.cpp:226] pool2 needs backward computation.
I0521 03:43:22.936550 30122 net.cpp:226] relu2 needs backward computation.
I0521 03:43:22.936560 30122 net.cpp:226] conv2 needs backward computation.
I0521 03:43:22.936570 30122 net.cpp:226] pool1 needs backward computation.
I0521 03:43:22.936578 30122 net.cpp:226] relu1 needs backward computation.
I0521 03:43:22.936589 30122 net.cpp:226] conv1 needs backward computation.
I0521 03:43:22.936600 30122 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 03:43:22.936612 30122 net.cpp:228] data_hdf5 does not need backward computation.
I0521 03:43:22.936621 30122 net.cpp:270] This network produces output accuracy
I0521 03:43:22.936632 30122 net.cpp:270] This network produces output loss
I0521 03:43:22.936661 30122 net.cpp:283] Network initialization done.
I0521 03:43:22.936795 30122 solver.cpp:60] Solver scaffolding done.
I0521 03:43:22.937932 30122 caffe.cpp:212] Starting Optimization
I0521 03:43:22.937949 30122 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 03:43:22.937963 30122 solver.cpp:289] Learning Rate Policy: fixed
I0521 03:43:22.939187 30122 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 03:44:08.792640 30122 solver.cpp:409]     Test net output #0: accuracy = 0.0594298
I0521 03:44:08.792803 30122 solver.cpp:409]     Test net output #1: loss = 2.39851 (* 1 = 2.39851 loss)
I0521 03:44:08.911232 30122 solver.cpp:237] Iteration 0, loss = 2.39807
I0521 03:44:08.911268 30122 solver.cpp:253]     Train net output #0: loss = 2.39807 (* 1 = 2.39807 loss)
I0521 03:44:08.911288 30122 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 03:44:16.921537 30122 solver.cpp:237] Iteration 24, loss = 2.38071
I0521 03:44:16.921573 30122 solver.cpp:253]     Train net output #0: loss = 2.38071 (* 1 = 2.38071 loss)
I0521 03:44:16.921587 30122 sgd_solver.cpp:106] Iteration 24, lr = 0.0025
I0521 03:44:24.927744 30122 solver.cpp:237] Iteration 48, loss = 2.36596
I0521 03:44:24.927783 30122 solver.cpp:253]     Train net output #0: loss = 2.36596 (* 1 = 2.36596 loss)
I0521 03:44:24.927803 30122 sgd_solver.cpp:106] Iteration 48, lr = 0.0025
I0521 03:44:32.937728 30122 solver.cpp:237] Iteration 72, loss = 2.34643
I0521 03:44:32.937760 30122 solver.cpp:253]     Train net output #0: loss = 2.34643 (* 1 = 2.34643 loss)
I0521 03:44:32.937775 30122 sgd_solver.cpp:106] Iteration 72, lr = 0.0025
I0521 03:44:40.947383 30122 solver.cpp:237] Iteration 96, loss = 2.33949
I0521 03:44:40.947528 30122 solver.cpp:253]     Train net output #0: loss = 2.33949 (* 1 = 2.33949 loss)
I0521 03:44:40.947541 30122 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 03:44:48.956269 30122 solver.cpp:237] Iteration 120, loss = 2.33455
I0521 03:44:48.956307 30122 solver.cpp:253]     Train net output #0: loss = 2.33455 (* 1 = 2.33455 loss)
I0521 03:44:48.956327 30122 sgd_solver.cpp:106] Iteration 120, lr = 0.0025
I0521 03:44:56.960983 30122 solver.cpp:237] Iteration 144, loss = 2.32806
I0521 03:44:56.961014 30122 solver.cpp:253]     Train net output #0: loss = 2.32806 (* 1 = 2.32806 loss)
I0521 03:44:56.961030 30122 sgd_solver.cpp:106] Iteration 144, lr = 0.0025
I0521 03:45:27.062347 30122 solver.cpp:237] Iteration 168, loss = 2.30187
I0521 03:45:27.062515 30122 solver.cpp:253]     Train net output #0: loss = 2.30187 (* 1 = 2.30187 loss)
I0521 03:45:27.062530 30122 sgd_solver.cpp:106] Iteration 168, lr = 0.0025
I0521 03:45:35.072430 30122 solver.cpp:237] Iteration 192, loss = 2.30424
I0521 03:45:35.072471 30122 solver.cpp:253]     Train net output #0: loss = 2.30424 (* 1 = 2.30424 loss)
I0521 03:45:35.072487 30122 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 03:45:43.086040 30122 solver.cpp:237] Iteration 216, loss = 2.31589
I0521 03:45:43.086073 30122 solver.cpp:253]     Train net output #0: loss = 2.31589 (* 1 = 2.31589 loss)
I0521 03:45:43.086087 30122 sgd_solver.cpp:106] Iteration 216, lr = 0.0025
I0521 03:45:51.099912 30122 solver.cpp:237] Iteration 240, loss = 2.29245
I0521 03:45:51.099946 30122 solver.cpp:253]     Train net output #0: loss = 2.29245 (* 1 = 2.29245 loss)
I0521 03:45:51.099961 30122 sgd_solver.cpp:106] Iteration 240, lr = 0.0025
I0521 03:45:51.100340 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_241.caffemodel
I0521 03:45:51.376392 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_241.solverstate
I0521 03:45:59.178308 30122 solver.cpp:237] Iteration 264, loss = 2.26478
I0521 03:45:59.178478 30122 solver.cpp:253]     Train net output #0: loss = 2.26478 (* 1 = 2.26478 loss)
I0521 03:45:59.178491 30122 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0521 03:46:07.188158 30122 solver.cpp:237] Iteration 288, loss = 2.25923
I0521 03:46:07.188190 30122 solver.cpp:253]     Train net output #0: loss = 2.25923 (* 1 = 2.25923 loss)
I0521 03:46:07.188206 30122 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 03:46:15.197234 30122 solver.cpp:237] Iteration 312, loss = 2.20527
I0521 03:46:15.197266 30122 solver.cpp:253]     Train net output #0: loss = 2.20527 (* 1 = 2.20527 loss)
I0521 03:46:15.197283 30122 sgd_solver.cpp:106] Iteration 312, lr = 0.0025
I0521 03:46:45.316812 30122 solver.cpp:237] Iteration 336, loss = 2.15509
I0521 03:46:45.316967 30122 solver.cpp:253]     Train net output #0: loss = 2.15509 (* 1 = 2.15509 loss)
I0521 03:46:45.316980 30122 sgd_solver.cpp:106] Iteration 336, lr = 0.0025
I0521 03:46:53.330602 30122 solver.cpp:237] Iteration 360, loss = 2.12879
I0521 03:46:53.330641 30122 solver.cpp:253]     Train net output #0: loss = 2.12879 (* 1 = 2.12879 loss)
I0521 03:46:53.330662 30122 sgd_solver.cpp:106] Iteration 360, lr = 0.0025
I0521 03:47:01.335168 30122 solver.cpp:237] Iteration 384, loss = 2.12276
I0521 03:47:01.335201 30122 solver.cpp:253]     Train net output #0: loss = 2.12276 (* 1 = 2.12276 loss)
I0521 03:47:01.335216 30122 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 03:47:09.340942 30122 solver.cpp:237] Iteration 408, loss = 2.06569
I0521 03:47:09.340976 30122 solver.cpp:253]     Train net output #0: loss = 2.06569 (* 1 = 2.06569 loss)
I0521 03:47:09.340991 30122 sgd_solver.cpp:106] Iteration 408, lr = 0.0025
I0521 03:47:17.351142 30122 solver.cpp:237] Iteration 432, loss = 2.08069
I0521 03:47:17.351297 30122 solver.cpp:253]     Train net output #0: loss = 2.08069 (* 1 = 2.08069 loss)
I0521 03:47:17.351311 30122 sgd_solver.cpp:106] Iteration 432, lr = 0.0025
I0521 03:47:25.359333 30122 solver.cpp:237] Iteration 456, loss = 2.03065
I0521 03:47:25.359364 30122 solver.cpp:253]     Train net output #0: loss = 2.03065 (* 1 = 2.03065 loss)
I0521 03:47:25.359380 30122 sgd_solver.cpp:106] Iteration 456, lr = 0.0025
I0521 03:47:33.371878 30122 solver.cpp:237] Iteration 480, loss = 1.99985
I0521 03:47:33.371912 30122 solver.cpp:253]     Train net output #0: loss = 1.99985 (* 1 = 1.99985 loss)
I0521 03:47:33.371927 30122 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 03:47:33.705482 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_482.caffemodel
I0521 03:47:33.978723 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_482.solverstate
I0521 03:47:34.103374 30122 solver.cpp:341] Iteration 483, Testing net (#0)
I0521 03:48:19.294415 30122 solver.cpp:409]     Test net output #0: accuracy = 0.540871
I0521 03:48:19.294572 30122 solver.cpp:409]     Test net output #1: loss = 1.73523 (* 1 = 1.73523 loss)
I0521 03:48:48.511195 30122 solver.cpp:237] Iteration 504, loss = 2.00579
I0521 03:48:48.511245 30122 solver.cpp:253]     Train net output #0: loss = 2.00579 (* 1 = 2.00579 loss)
I0521 03:48:48.511260 30122 sgd_solver.cpp:106] Iteration 504, lr = 0.0025
I0521 03:48:56.508968 30122 solver.cpp:237] Iteration 528, loss = 1.98578
I0521 03:48:56.509121 30122 solver.cpp:253]     Train net output #0: loss = 1.98578 (* 1 = 1.98578 loss)
I0521 03:48:56.509135 30122 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 03:49:04.514696 30122 solver.cpp:237] Iteration 552, loss = 2.01484
I0521 03:49:04.514729 30122 solver.cpp:253]     Train net output #0: loss = 2.01484 (* 1 = 2.01484 loss)
I0521 03:49:04.514744 30122 sgd_solver.cpp:106] Iteration 552, lr = 0.0025
I0521 03:49:12.516446 30122 solver.cpp:237] Iteration 576, loss = 1.98144
I0521 03:49:12.516479 30122 solver.cpp:253]     Train net output #0: loss = 1.98144 (* 1 = 1.98144 loss)
I0521 03:49:12.516494 30122 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 03:49:20.520689 30122 solver.cpp:237] Iteration 600, loss = 1.87769
I0521 03:49:20.520730 30122 solver.cpp:253]     Train net output #0: loss = 1.87769 (* 1 = 1.87769 loss)
I0521 03:49:20.520745 30122 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0521 03:49:28.520369 30122 solver.cpp:237] Iteration 624, loss = 1.9144
I0521 03:49:28.520499 30122 solver.cpp:253]     Train net output #0: loss = 1.9144 (* 1 = 1.9144 loss)
I0521 03:49:28.520512 30122 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0521 03:49:58.651433 30122 solver.cpp:237] Iteration 648, loss = 1.96617
I0521 03:49:58.651599 30122 solver.cpp:253]     Train net output #0: loss = 1.96617 (* 1 = 1.96617 loss)
I0521 03:49:58.651614 30122 sgd_solver.cpp:106] Iteration 648, lr = 0.0025
I0521 03:50:06.653930 30122 solver.cpp:237] Iteration 672, loss = 1.88305
I0521 03:50:06.653962 30122 solver.cpp:253]     Train net output #0: loss = 1.88305 (* 1 = 1.88305 loss)
I0521 03:50:06.653977 30122 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 03:50:14.651847 30122 solver.cpp:237] Iteration 696, loss = 1.92935
I0521 03:50:14.651891 30122 solver.cpp:253]     Train net output #0: loss = 1.92935 (* 1 = 1.92935 loss)
I0521 03:50:14.651906 30122 sgd_solver.cpp:106] Iteration 696, lr = 0.0025
I0521 03:50:22.653772 30122 solver.cpp:237] Iteration 720, loss = 1.87945
I0521 03:50:22.653805 30122 solver.cpp:253]     Train net output #0: loss = 1.87945 (* 1 = 1.87945 loss)
I0521 03:50:22.653820 30122 sgd_solver.cpp:106] Iteration 720, lr = 0.0025
I0521 03:50:23.319414 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_723.caffemodel
I0521 03:50:23.594388 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_723.solverstate
I0521 03:50:30.722046 30122 solver.cpp:237] Iteration 744, loss = 1.89348
I0521 03:50:30.722214 30122 solver.cpp:253]     Train net output #0: loss = 1.89348 (* 1 = 1.89348 loss)
I0521 03:50:30.722228 30122 sgd_solver.cpp:106] Iteration 744, lr = 0.0025
I0521 03:50:38.721580 30122 solver.cpp:237] Iteration 768, loss = 1.78888
I0521 03:50:38.721616 30122 solver.cpp:253]     Train net output #0: loss = 1.78888 (* 1 = 1.78888 loss)
I0521 03:50:38.721637 30122 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 03:50:46.724329 30122 solver.cpp:237] Iteration 792, loss = 1.74519
I0521 03:50:46.724362 30122 solver.cpp:253]     Train net output #0: loss = 1.74519 (* 1 = 1.74519 loss)
I0521 03:50:46.724378 30122 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 03:51:16.862615 30122 solver.cpp:237] Iteration 816, loss = 1.87096
I0521 03:51:16.862776 30122 solver.cpp:253]     Train net output #0: loss = 1.87096 (* 1 = 1.87096 loss)
I0521 03:51:16.862793 30122 sgd_solver.cpp:106] Iteration 816, lr = 0.0025
I0521 03:51:24.861742 30122 solver.cpp:237] Iteration 840, loss = 1.76748
I0521 03:51:24.861776 30122 solver.cpp:253]     Train net output #0: loss = 1.76748 (* 1 = 1.76748 loss)
I0521 03:51:24.861795 30122 sgd_solver.cpp:106] Iteration 840, lr = 0.0025
I0521 03:51:32.862243 30122 solver.cpp:237] Iteration 864, loss = 1.86612
I0521 03:51:32.862277 30122 solver.cpp:253]     Train net output #0: loss = 1.86612 (* 1 = 1.86612 loss)
I0521 03:51:32.862293 30122 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 03:51:40.869408 30122 solver.cpp:237] Iteration 888, loss = 1.79026
I0521 03:51:40.869441 30122 solver.cpp:253]     Train net output #0: loss = 1.79026 (* 1 = 1.79026 loss)
I0521 03:51:40.869457 30122 sgd_solver.cpp:106] Iteration 888, lr = 0.0025
I0521 03:51:48.868998 30122 solver.cpp:237] Iteration 912, loss = 1.85022
I0521 03:51:48.869140 30122 solver.cpp:253]     Train net output #0: loss = 1.85022 (* 1 = 1.85022 loss)
I0521 03:51:48.869154 30122 sgd_solver.cpp:106] Iteration 912, lr = 0.0025
I0521 03:51:56.868525 30122 solver.cpp:237] Iteration 936, loss = 1.95097
I0521 03:51:56.868556 30122 solver.cpp:253]     Train net output #0: loss = 1.95097 (* 1 = 1.95097 loss)
I0521 03:51:56.868572 30122 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0521 03:52:04.870956 30122 solver.cpp:237] Iteration 960, loss = 1.8137
I0521 03:52:04.870990 30122 solver.cpp:253]     Train net output #0: loss = 1.8137 (* 1 = 1.8137 loss)
I0521 03:52:04.871004 30122 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 03:52:05.872565 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_964.caffemodel
I0521 03:52:06.148787 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_964.solverstate
I0521 03:52:06.609364 30122 solver.cpp:341] Iteration 966, Testing net (#0)
I0521 03:53:12.611838 30122 solver.cpp:409]     Test net output #0: accuracy = 0.622922
I0521 03:53:12.612009 30122 solver.cpp:409]     Test net output #1: loss = 1.3159 (* 1 = 1.3159 loss)
I0521 03:53:40.819996 30122 solver.cpp:237] Iteration 984, loss = 1.87278
I0521 03:53:40.820046 30122 solver.cpp:253]     Train net output #0: loss = 1.87278 (* 1 = 1.87278 loss)
I0521 03:53:40.820060 30122 sgd_solver.cpp:106] Iteration 984, lr = 0.0025
I0521 03:53:48.816164 30122 solver.cpp:237] Iteration 1008, loss = 1.83662
I0521 03:53:48.816334 30122 solver.cpp:253]     Train net output #0: loss = 1.83662 (* 1 = 1.83662 loss)
I0521 03:53:48.816347 30122 sgd_solver.cpp:106] Iteration 1008, lr = 0.0025
I0521 03:53:56.814153 30122 solver.cpp:237] Iteration 1032, loss = 1.79949
I0521 03:53:56.814193 30122 solver.cpp:253]     Train net output #0: loss = 1.79949 (* 1 = 1.79949 loss)
I0521 03:53:56.814213 30122 sgd_solver.cpp:106] Iteration 1032, lr = 0.0025
I0521 03:54:04.810176 30122 solver.cpp:237] Iteration 1056, loss = 1.67022
I0521 03:54:04.810209 30122 solver.cpp:253]     Train net output #0: loss = 1.67022 (* 1 = 1.67022 loss)
I0521 03:54:04.810223 30122 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 03:54:12.809144 30122 solver.cpp:237] Iteration 1080, loss = 1.74358
I0521 03:54:12.809176 30122 solver.cpp:253]     Train net output #0: loss = 1.74358 (* 1 = 1.74358 loss)
I0521 03:54:12.809190 30122 sgd_solver.cpp:106] Iteration 1080, lr = 0.0025
I0521 03:54:20.800721 30122 solver.cpp:237] Iteration 1104, loss = 1.78001
I0521 03:54:20.800879 30122 solver.cpp:253]     Train net output #0: loss = 1.78001 (* 1 = 1.78001 loss)
I0521 03:54:20.800892 30122 sgd_solver.cpp:106] Iteration 1104, lr = 0.0025
I0521 03:54:28.796211 30122 solver.cpp:237] Iteration 1128, loss = 1.84746
I0521 03:54:28.796243 30122 solver.cpp:253]     Train net output #0: loss = 1.84746 (* 1 = 1.84746 loss)
I0521 03:54:28.796259 30122 sgd_solver.cpp:106] Iteration 1128, lr = 0.0025
I0521 03:54:58.918967 30122 solver.cpp:237] Iteration 1152, loss = 1.78297
I0521 03:54:58.919132 30122 solver.cpp:253]     Train net output #0: loss = 1.78297 (* 1 = 1.78297 loss)
I0521 03:54:58.919147 30122 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 03:55:06.913393 30122 solver.cpp:237] Iteration 1176, loss = 1.73802
I0521 03:55:06.913424 30122 solver.cpp:253]     Train net output #0: loss = 1.73802 (* 1 = 1.73802 loss)
I0521 03:55:06.913441 30122 sgd_solver.cpp:106] Iteration 1176, lr = 0.0025
I0521 03:55:14.906554 30122 solver.cpp:237] Iteration 1200, loss = 1.71025
I0521 03:55:14.906587 30122 solver.cpp:253]     Train net output #0: loss = 1.71025 (* 1 = 1.71025 loss)
I0521 03:55:14.906608 30122 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0521 03:55:16.241016 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1205.caffemodel
I0521 03:55:16.515141 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1205.solverstate
I0521 03:55:22.974181 30122 solver.cpp:237] Iteration 1224, loss = 1.74265
I0521 03:55:22.974230 30122 solver.cpp:253]     Train net output #0: loss = 1.74265 (* 1 = 1.74265 loss)
I0521 03:55:22.974244 30122 sgd_solver.cpp:106] Iteration 1224, lr = 0.0025
I0521 03:55:30.973383 30122 solver.cpp:237] Iteration 1248, loss = 1.72855
I0521 03:55:30.973536 30122 solver.cpp:253]     Train net output #0: loss = 1.72855 (* 1 = 1.72855 loss)
I0521 03:55:30.973549 30122 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 03:55:38.973114 30122 solver.cpp:237] Iteration 1272, loss = 1.73128
I0521 03:55:38.973140 30122 solver.cpp:253]     Train net output #0: loss = 1.73128 (* 1 = 1.73128 loss)
I0521 03:55:38.973152 30122 sgd_solver.cpp:106] Iteration 1272, lr = 0.0025
I0521 03:56:09.174222 30122 solver.cpp:237] Iteration 1296, loss = 1.75405
I0521 03:56:09.174398 30122 solver.cpp:253]     Train net output #0: loss = 1.75405 (* 1 = 1.75405 loss)
I0521 03:56:09.174419 30122 sgd_solver.cpp:106] Iteration 1296, lr = 0.0025
I0521 03:56:17.173027 30122 solver.cpp:237] Iteration 1320, loss = 1.7802
I0521 03:56:17.173059 30122 solver.cpp:253]     Train net output #0: loss = 1.7802 (* 1 = 1.7802 loss)
I0521 03:56:17.173074 30122 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 03:56:25.169718 30122 solver.cpp:237] Iteration 1344, loss = 1.75527
I0521 03:56:25.169757 30122 solver.cpp:253]     Train net output #0: loss = 1.75527 (* 1 = 1.75527 loss)
I0521 03:56:25.169777 30122 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 03:56:33.166527 30122 solver.cpp:237] Iteration 1368, loss = 1.83179
I0521 03:56:33.166559 30122 solver.cpp:253]     Train net output #0: loss = 1.83179 (* 1 = 1.83179 loss)
I0521 03:56:33.166574 30122 sgd_solver.cpp:106] Iteration 1368, lr = 0.0025
I0521 03:56:41.162205 30122 solver.cpp:237] Iteration 1392, loss = 1.7427
I0521 03:56:41.162343 30122 solver.cpp:253]     Train net output #0: loss = 1.7427 (* 1 = 1.7427 loss)
I0521 03:56:41.162356 30122 sgd_solver.cpp:106] Iteration 1392, lr = 0.0025
I0521 03:56:49.163276 30122 solver.cpp:237] Iteration 1416, loss = 1.7604
I0521 03:56:49.163321 30122 solver.cpp:253]     Train net output #0: loss = 1.7604 (* 1 = 1.7604 loss)
I0521 03:56:49.163336 30122 sgd_solver.cpp:106] Iteration 1416, lr = 0.0025
I0521 03:56:57.158776 30122 solver.cpp:237] Iteration 1440, loss = 1.75199
I0521 03:56:57.158809 30122 solver.cpp:253]     Train net output #0: loss = 1.75199 (* 1 = 1.75199 loss)
I0521 03:56:57.158824 30122 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 03:56:58.827569 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1446.caffemodel
I0521 03:56:59.100857 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1446.solverstate
I0521 03:56:59.893903 30122 solver.cpp:341] Iteration 1449, Testing net (#0)
I0521 03:57:44.762912 30122 solver.cpp:409]     Test net output #0: accuracy = 0.64039
I0521 03:57:44.763072 30122 solver.cpp:409]     Test net output #1: loss = 1.2315 (* 1 = 1.2315 loss)
I0521 03:58:12.003417 30122 solver.cpp:237] Iteration 1464, loss = 1.72878
I0521 03:58:12.003465 30122 solver.cpp:253]     Train net output #0: loss = 1.72878 (* 1 = 1.72878 loss)
I0521 03:58:12.003479 30122 sgd_solver.cpp:106] Iteration 1464, lr = 0.0025
I0521 03:58:20.005455 30122 solver.cpp:237] Iteration 1488, loss = 1.70711
I0521 03:58:20.005604 30122 solver.cpp:253]     Train net output #0: loss = 1.70711 (* 1 = 1.70711 loss)
I0521 03:58:20.005616 30122 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 03:58:28.012053 30122 solver.cpp:237] Iteration 1512, loss = 1.71513
I0521 03:58:28.012096 30122 solver.cpp:253]     Train net output #0: loss = 1.71513 (* 1 = 1.71513 loss)
I0521 03:58:28.012111 30122 sgd_solver.cpp:106] Iteration 1512, lr = 0.0025
I0521 03:58:36.016288 30122 solver.cpp:237] Iteration 1536, loss = 1.73021
I0521 03:58:36.016320 30122 solver.cpp:253]     Train net output #0: loss = 1.73021 (* 1 = 1.73021 loss)
I0521 03:58:36.016335 30122 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 03:58:44.020078 30122 solver.cpp:237] Iteration 1560, loss = 1.6725
I0521 03:58:44.020112 30122 solver.cpp:253]     Train net output #0: loss = 1.6725 (* 1 = 1.6725 loss)
I0521 03:58:44.020125 30122 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0521 03:58:52.022935 30122 solver.cpp:237] Iteration 1584, loss = 1.71834
I0521 03:58:52.023082 30122 solver.cpp:253]     Train net output #0: loss = 1.71834 (* 1 = 1.71834 loss)
I0521 03:58:52.023095 30122 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 03:59:00.024998 30122 solver.cpp:237] Iteration 1608, loss = 1.72887
I0521 03:59:00.025037 30122 solver.cpp:253]     Train net output #0: loss = 1.72887 (* 1 = 1.72887 loss)
I0521 03:59:00.025055 30122 sgd_solver.cpp:106] Iteration 1608, lr = 0.0025
I0521 03:59:30.161027 30122 solver.cpp:237] Iteration 1632, loss = 1.73535
I0521 03:59:30.161196 30122 solver.cpp:253]     Train net output #0: loss = 1.73535 (* 1 = 1.73535 loss)
I0521 03:59:30.161212 30122 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 03:59:38.165324 30122 solver.cpp:237] Iteration 1656, loss = 1.75249
I0521 03:59:38.165356 30122 solver.cpp:253]     Train net output #0: loss = 1.75249 (* 1 = 1.75249 loss)
I0521 03:59:38.165372 30122 sgd_solver.cpp:106] Iteration 1656, lr = 0.0025
I0521 03:59:46.171990 30122 solver.cpp:237] Iteration 1680, loss = 1.69552
I0521 03:59:46.172034 30122 solver.cpp:253]     Train net output #0: loss = 1.69552 (* 1 = 1.69552 loss)
I0521 03:59:46.172046 30122 sgd_solver.cpp:106] Iteration 1680, lr = 0.0025
I0521 03:59:48.177866 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1687.caffemodel
I0521 03:59:48.450198 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1687.solverstate
I0521 03:59:54.244485 30122 solver.cpp:237] Iteration 1704, loss = 1.70258
I0521 03:59:54.244526 30122 solver.cpp:253]     Train net output #0: loss = 1.70258 (* 1 = 1.70258 loss)
I0521 03:59:54.244546 30122 sgd_solver.cpp:106] Iteration 1704, lr = 0.0025
I0521 04:00:02.250264 30122 solver.cpp:237] Iteration 1728, loss = 1.72896
I0521 04:00:02.250407 30122 solver.cpp:253]     Train net output #0: loss = 1.72896 (* 1 = 1.72896 loss)
I0521 04:00:02.250427 30122 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 04:00:10.254675 30122 solver.cpp:237] Iteration 1752, loss = 1.69944
I0521 04:00:10.254714 30122 solver.cpp:253]     Train net output #0: loss = 1.69944 (* 1 = 1.69944 loss)
I0521 04:00:10.254735 30122 sgd_solver.cpp:106] Iteration 1752, lr = 0.0025
I0521 04:00:40.453665 30122 solver.cpp:237] Iteration 1776, loss = 1.69053
I0521 04:00:40.453825 30122 solver.cpp:253]     Train net output #0: loss = 1.69053 (* 1 = 1.69053 loss)
I0521 04:00:40.453840 30122 sgd_solver.cpp:106] Iteration 1776, lr = 0.0025
I0521 04:00:48.455173 30122 solver.cpp:237] Iteration 1800, loss = 1.628
I0521 04:00:48.455205 30122 solver.cpp:253]     Train net output #0: loss = 1.628 (* 1 = 1.628 loss)
I0521 04:00:48.455222 30122 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0521 04:00:56.460408 30122 solver.cpp:237] Iteration 1824, loss = 1.73919
I0521 04:00:56.460441 30122 solver.cpp:253]     Train net output #0: loss = 1.73919 (* 1 = 1.73919 loss)
I0521 04:00:56.460456 30122 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0521 04:01:04.468696 30122 solver.cpp:237] Iteration 1848, loss = 1.6157
I0521 04:01:04.468739 30122 solver.cpp:253]     Train net output #0: loss = 1.6157 (* 1 = 1.6157 loss)
I0521 04:01:04.468755 30122 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 04:01:12.471249 30122 solver.cpp:237] Iteration 1872, loss = 1.62445
I0521 04:01:12.471388 30122 solver.cpp:253]     Train net output #0: loss = 1.62445 (* 1 = 1.62445 loss)
I0521 04:01:12.471402 30122 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0521 04:01:20.472853 30122 solver.cpp:237] Iteration 1896, loss = 1.69573
I0521 04:01:20.472885 30122 solver.cpp:253]     Train net output #0: loss = 1.69573 (* 1 = 1.69573 loss)
I0521 04:01:20.472900 30122 sgd_solver.cpp:106] Iteration 1896, lr = 0.0025
I0521 04:01:28.477649 30122 solver.cpp:237] Iteration 1920, loss = 1.6638
I0521 04:01:28.477692 30122 solver.cpp:253]     Train net output #0: loss = 1.6638 (* 1 = 1.6638 loss)
I0521 04:01:28.477708 30122 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0521 04:01:30.811264 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1928.caffemodel
I0521 04:01:31.084692 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_1928.solverstate
I0521 04:01:32.210180 30122 solver.cpp:341] Iteration 1932, Testing net (#0)
I0521 04:02:38.193634 30122 solver.cpp:409]     Test net output #0: accuracy = 0.666169
I0521 04:02:38.193805 30122 solver.cpp:409]     Test net output #1: loss = 1.12732 (* 1 = 1.12732 loss)
I0521 04:03:04.449666 30122 solver.cpp:237] Iteration 1944, loss = 1.65133
I0521 04:03:04.449714 30122 solver.cpp:253]     Train net output #0: loss = 1.65133 (* 1 = 1.65133 loss)
I0521 04:03:04.449730 30122 sgd_solver.cpp:106] Iteration 1944, lr = 0.0025
I0521 04:03:12.458549 30122 solver.cpp:237] Iteration 1968, loss = 1.6438
I0521 04:03:12.458698 30122 solver.cpp:253]     Train net output #0: loss = 1.6438 (* 1 = 1.6438 loss)
I0521 04:03:12.458710 30122 sgd_solver.cpp:106] Iteration 1968, lr = 0.0025
I0521 04:03:20.468686 30122 solver.cpp:237] Iteration 1992, loss = 1.68352
I0521 04:03:20.468719 30122 solver.cpp:253]     Train net output #0: loss = 1.68352 (* 1 = 1.68352 loss)
I0521 04:03:20.468734 30122 sgd_solver.cpp:106] Iteration 1992, lr = 0.0025
I0521 04:03:28.476665 30122 solver.cpp:237] Iteration 2016, loss = 1.64584
I0521 04:03:28.476707 30122 solver.cpp:253]     Train net output #0: loss = 1.64584 (* 1 = 1.64584 loss)
I0521 04:03:28.476725 30122 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0521 04:03:36.486837 30122 solver.cpp:237] Iteration 2040, loss = 1.71495
I0521 04:03:36.486871 30122 solver.cpp:253]     Train net output #0: loss = 1.71495 (* 1 = 1.71495 loss)
I0521 04:03:36.486884 30122 sgd_solver.cpp:106] Iteration 2040, lr = 0.0025
I0521 04:03:44.502176 30122 solver.cpp:237] Iteration 2064, loss = 1.69011
I0521 04:03:44.502320 30122 solver.cpp:253]     Train net output #0: loss = 1.69011 (* 1 = 1.69011 loss)
I0521 04:03:44.502332 30122 sgd_solver.cpp:106] Iteration 2064, lr = 0.0025
I0521 04:03:52.508033 30122 solver.cpp:237] Iteration 2088, loss = 1.64964
I0521 04:03:52.508067 30122 solver.cpp:253]     Train net output #0: loss = 1.64964 (* 1 = 1.64964 loss)
I0521 04:03:52.508080 30122 sgd_solver.cpp:106] Iteration 2088, lr = 0.0025
I0521 04:04:22.667901 30122 solver.cpp:237] Iteration 2112, loss = 1.60599
I0521 04:04:22.668066 30122 solver.cpp:253]     Train net output #0: loss = 1.60599 (* 1 = 1.60599 loss)
I0521 04:04:22.668081 30122 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 04:04:30.681995 30122 solver.cpp:237] Iteration 2136, loss = 1.6229
I0521 04:04:30.682027 30122 solver.cpp:253]     Train net output #0: loss = 1.6229 (* 1 = 1.6229 loss)
I0521 04:04:30.682045 30122 sgd_solver.cpp:106] Iteration 2136, lr = 0.0025
I0521 04:04:38.688618 30122 solver.cpp:237] Iteration 2160, loss = 1.69817
I0521 04:04:38.688653 30122 solver.cpp:253]     Train net output #0: loss = 1.69817 (* 1 = 1.69817 loss)
I0521 04:04:38.688668 30122 sgd_solver.cpp:106] Iteration 2160, lr = 0.0025
I0521 04:04:41.359488 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_2169.caffemodel
I0521 04:04:41.633280 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_2169.solverstate
I0521 04:04:46.764359 30122 solver.cpp:237] Iteration 2184, loss = 1.64266
I0521 04:04:46.764407 30122 solver.cpp:253]     Train net output #0: loss = 1.64266 (* 1 = 1.64266 loss)
I0521 04:04:46.764425 30122 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0521 04:04:54.779671 30122 solver.cpp:237] Iteration 2208, loss = 1.72128
I0521 04:04:54.779836 30122 solver.cpp:253]     Train net output #0: loss = 1.72128 (* 1 = 1.72128 loss)
I0521 04:04:54.779850 30122 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0521 04:05:02.790865 30122 solver.cpp:237] Iteration 2232, loss = 1.69502
I0521 04:05:02.790897 30122 solver.cpp:253]     Train net output #0: loss = 1.69502 (* 1 = 1.69502 loss)
I0521 04:05:02.790915 30122 sgd_solver.cpp:106] Iteration 2232, lr = 0.0025
I0521 04:05:10.798490 30122 solver.cpp:237] Iteration 2256, loss = 1.71562
I0521 04:05:10.798533 30122 solver.cpp:253]     Train net output #0: loss = 1.71562 (* 1 = 1.71562 loss)
I0521 04:05:10.798553 30122 sgd_solver.cpp:106] Iteration 2256, lr = 0.0025
I0521 04:05:40.957658 30122 solver.cpp:237] Iteration 2280, loss = 1.66556
I0521 04:05:40.957828 30122 solver.cpp:253]     Train net output #0: loss = 1.66556 (* 1 = 1.66556 loss)
I0521 04:05:40.957842 30122 sgd_solver.cpp:106] Iteration 2280, lr = 0.0025
I0521 04:05:48.968611 30122 solver.cpp:237] Iteration 2304, loss = 1.62923
I0521 04:05:48.968644 30122 solver.cpp:253]     Train net output #0: loss = 1.62923 (* 1 = 1.62923 loss)
I0521 04:05:48.968662 30122 sgd_solver.cpp:106] Iteration 2304, lr = 0.0025
I0521 04:05:56.980146 30122 solver.cpp:237] Iteration 2328, loss = 1.62046
I0521 04:05:56.980180 30122 solver.cpp:253]     Train net output #0: loss = 1.62046 (* 1 = 1.62046 loss)
I0521 04:05:56.980197 30122 sgd_solver.cpp:106] Iteration 2328, lr = 0.0025
I0521 04:06:04.991000 30122 solver.cpp:237] Iteration 2352, loss = 1.68008
I0521 04:06:04.991035 30122 solver.cpp:253]     Train net output #0: loss = 1.68008 (* 1 = 1.68008 loss)
I0521 04:06:04.991053 30122 sgd_solver.cpp:106] Iteration 2352, lr = 0.0025
I0521 04:06:12.995501 30122 solver.cpp:237] Iteration 2376, loss = 1.67889
I0521 04:06:12.995643 30122 solver.cpp:253]     Train net output #0: loss = 1.67889 (* 1 = 1.67889 loss)
I0521 04:06:12.995656 30122 sgd_solver.cpp:106] Iteration 2376, lr = 0.0025
I0521 04:06:21.005846 30122 solver.cpp:237] Iteration 2400, loss = 1.6809
I0521 04:06:21.005878 30122 solver.cpp:253]     Train net output #0: loss = 1.6809 (* 1 = 1.6809 loss)
I0521 04:06:21.005897 30122 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0521 04:06:24.010118 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_2410.caffemodel
I0521 04:06:24.285152 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_2410.solverstate
I0521 04:06:25.747989 30122 solver.cpp:341] Iteration 2415, Testing net (#0)
I0521 04:07:10.950804 30122 solver.cpp:409]     Test net output #0: accuracy = 0.681448
I0521 04:07:10.950970 30122 solver.cpp:409]     Test net output #1: loss = 1.12132 (* 1 = 1.12132 loss)
I0521 04:07:12.051389 30122 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_2419.caffemodel
I0521 04:07:12.327551 30122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016_iter_2419.solverstate
I0521 04:07:12.355888 30122 solver.cpp:326] Optimization Done.
I0521 04:07:12.355919 30122 caffe.cpp:215] Optimization Done.
Application 11236734 resources: utime ~1250s, stime ~225s, Rss ~5329736, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_620_2016-05-20T11.20.55.153016.solver"
	User time (seconds): 0.55
	System time (seconds): 0.15
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:38.98
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15081
	Voluntary context switches: 2748
	Involuntary context switches: 75
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
