2806098
I0521 00:08:26.516206  9137 caffe.cpp:184] Using GPUs 0
I0521 00:08:26.960578  9137 solver.cpp:48] Initializing solver from parameters: 
test_iter: 326
test_interval: 652
base_lr: 0.0025
display: 32
max_iter: 3260
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 326
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925.prototxt"
I0521 00:08:26.962623  9137 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925.prototxt
I0521 00:08:26.982017  9137 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 00:08:26.982076  9137 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 00:08:26.982419  9137 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 460
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:08:26.982599  9137 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:08:26.982623  9137 net.cpp:106] Creating Layer data_hdf5
I0521 00:08:26.982638  9137 net.cpp:411] data_hdf5 -> data
I0521 00:08:26.982671  9137 net.cpp:411] data_hdf5 -> label
I0521 00:08:26.982703  9137 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 00:08:26.984040  9137 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 00:08:26.986289  9137 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 00:08:48.530920  9137 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 00:08:48.536093  9137 net.cpp:150] Setting up data_hdf5
I0521 00:08:48.536139  9137 net.cpp:157] Top shape: 460 1 127 50 (2921000)
I0521 00:08:48.536154  9137 net.cpp:157] Top shape: 460 (460)
I0521 00:08:48.536164  9137 net.cpp:165] Memory required for data: 11685840
I0521 00:08:48.536176  9137 layer_factory.hpp:77] Creating layer conv1
I0521 00:08:48.536211  9137 net.cpp:106] Creating Layer conv1
I0521 00:08:48.536222  9137 net.cpp:454] conv1 <- data
I0521 00:08:48.536247  9137 net.cpp:411] conv1 -> conv1
I0521 00:08:48.903818  9137 net.cpp:150] Setting up conv1
I0521 00:08:48.903864  9137 net.cpp:157] Top shape: 460 12 120 48 (31795200)
I0521 00:08:48.903877  9137 net.cpp:165] Memory required for data: 138866640
I0521 00:08:48.903904  9137 layer_factory.hpp:77] Creating layer relu1
I0521 00:08:48.903925  9137 net.cpp:106] Creating Layer relu1
I0521 00:08:48.903936  9137 net.cpp:454] relu1 <- conv1
I0521 00:08:48.903950  9137 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:08:48.904465  9137 net.cpp:150] Setting up relu1
I0521 00:08:48.904481  9137 net.cpp:157] Top shape: 460 12 120 48 (31795200)
I0521 00:08:48.904492  9137 net.cpp:165] Memory required for data: 266047440
I0521 00:08:48.904502  9137 layer_factory.hpp:77] Creating layer pool1
I0521 00:08:48.904518  9137 net.cpp:106] Creating Layer pool1
I0521 00:08:48.904528  9137 net.cpp:454] pool1 <- conv1
I0521 00:08:48.904541  9137 net.cpp:411] pool1 -> pool1
I0521 00:08:48.904621  9137 net.cpp:150] Setting up pool1
I0521 00:08:48.904635  9137 net.cpp:157] Top shape: 460 12 60 48 (15897600)
I0521 00:08:48.904644  9137 net.cpp:165] Memory required for data: 329637840
I0521 00:08:48.904655  9137 layer_factory.hpp:77] Creating layer conv2
I0521 00:08:48.904678  9137 net.cpp:106] Creating Layer conv2
I0521 00:08:48.904688  9137 net.cpp:454] conv2 <- pool1
I0521 00:08:48.904701  9137 net.cpp:411] conv2 -> conv2
I0521 00:08:48.907376  9137 net.cpp:150] Setting up conv2
I0521 00:08:48.907403  9137 net.cpp:157] Top shape: 460 20 54 46 (22852800)
I0521 00:08:48.907413  9137 net.cpp:165] Memory required for data: 421049040
I0521 00:08:48.907433  9137 layer_factory.hpp:77] Creating layer relu2
I0521 00:08:48.907447  9137 net.cpp:106] Creating Layer relu2
I0521 00:08:48.907457  9137 net.cpp:454] relu2 <- conv2
I0521 00:08:48.907483  9137 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:08:48.907815  9137 net.cpp:150] Setting up relu2
I0521 00:08:48.907829  9137 net.cpp:157] Top shape: 460 20 54 46 (22852800)
I0521 00:08:48.907840  9137 net.cpp:165] Memory required for data: 512460240
I0521 00:08:48.907850  9137 layer_factory.hpp:77] Creating layer pool2
I0521 00:08:48.907863  9137 net.cpp:106] Creating Layer pool2
I0521 00:08:48.907873  9137 net.cpp:454] pool2 <- conv2
I0521 00:08:48.907898  9137 net.cpp:411] pool2 -> pool2
I0521 00:08:48.907968  9137 net.cpp:150] Setting up pool2
I0521 00:08:48.907980  9137 net.cpp:157] Top shape: 460 20 27 46 (11426400)
I0521 00:08:48.907990  9137 net.cpp:165] Memory required for data: 558165840
I0521 00:08:48.908001  9137 layer_factory.hpp:77] Creating layer conv3
I0521 00:08:48.908018  9137 net.cpp:106] Creating Layer conv3
I0521 00:08:48.908030  9137 net.cpp:454] conv3 <- pool2
I0521 00:08:48.908042  9137 net.cpp:411] conv3 -> conv3
I0521 00:08:48.909955  9137 net.cpp:150] Setting up conv3
I0521 00:08:48.909979  9137 net.cpp:157] Top shape: 460 28 22 44 (12467840)
I0521 00:08:48.909991  9137 net.cpp:165] Memory required for data: 608037200
I0521 00:08:48.910009  9137 layer_factory.hpp:77] Creating layer relu3
I0521 00:08:48.910025  9137 net.cpp:106] Creating Layer relu3
I0521 00:08:48.910035  9137 net.cpp:454] relu3 <- conv3
I0521 00:08:48.910048  9137 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:08:48.910517  9137 net.cpp:150] Setting up relu3
I0521 00:08:48.910534  9137 net.cpp:157] Top shape: 460 28 22 44 (12467840)
I0521 00:08:48.910544  9137 net.cpp:165] Memory required for data: 657908560
I0521 00:08:48.910554  9137 layer_factory.hpp:77] Creating layer pool3
I0521 00:08:48.910567  9137 net.cpp:106] Creating Layer pool3
I0521 00:08:48.910578  9137 net.cpp:454] pool3 <- conv3
I0521 00:08:48.910589  9137 net.cpp:411] pool3 -> pool3
I0521 00:08:48.910657  9137 net.cpp:150] Setting up pool3
I0521 00:08:48.910670  9137 net.cpp:157] Top shape: 460 28 11 44 (6233920)
I0521 00:08:48.910681  9137 net.cpp:165] Memory required for data: 682844240
I0521 00:08:48.910688  9137 layer_factory.hpp:77] Creating layer conv4
I0521 00:08:48.910706  9137 net.cpp:106] Creating Layer conv4
I0521 00:08:48.910717  9137 net.cpp:454] conv4 <- pool3
I0521 00:08:48.910730  9137 net.cpp:411] conv4 -> conv4
I0521 00:08:48.913519  9137 net.cpp:150] Setting up conv4
I0521 00:08:48.913543  9137 net.cpp:157] Top shape: 460 36 6 42 (4173120)
I0521 00:08:48.913554  9137 net.cpp:165] Memory required for data: 699536720
I0521 00:08:48.913569  9137 layer_factory.hpp:77] Creating layer relu4
I0521 00:08:48.913583  9137 net.cpp:106] Creating Layer relu4
I0521 00:08:48.913594  9137 net.cpp:454] relu4 <- conv4
I0521 00:08:48.913607  9137 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:08:48.914077  9137 net.cpp:150] Setting up relu4
I0521 00:08:48.914093  9137 net.cpp:157] Top shape: 460 36 6 42 (4173120)
I0521 00:08:48.914103  9137 net.cpp:165] Memory required for data: 716229200
I0521 00:08:48.914113  9137 layer_factory.hpp:77] Creating layer pool4
I0521 00:08:48.914126  9137 net.cpp:106] Creating Layer pool4
I0521 00:08:48.914136  9137 net.cpp:454] pool4 <- conv4
I0521 00:08:48.914149  9137 net.cpp:411] pool4 -> pool4
I0521 00:08:48.914217  9137 net.cpp:150] Setting up pool4
I0521 00:08:48.914232  9137 net.cpp:157] Top shape: 460 36 3 42 (2086560)
I0521 00:08:48.914242  9137 net.cpp:165] Memory required for data: 724575440
I0521 00:08:48.914252  9137 layer_factory.hpp:77] Creating layer ip1
I0521 00:08:48.914271  9137 net.cpp:106] Creating Layer ip1
I0521 00:08:48.914283  9137 net.cpp:454] ip1 <- pool4
I0521 00:08:48.914294  9137 net.cpp:411] ip1 -> ip1
I0521 00:08:48.929817  9137 net.cpp:150] Setting up ip1
I0521 00:08:48.929846  9137 net.cpp:157] Top shape: 460 196 (90160)
I0521 00:08:48.929862  9137 net.cpp:165] Memory required for data: 724936080
I0521 00:08:48.929889  9137 layer_factory.hpp:77] Creating layer relu5
I0521 00:08:48.929904  9137 net.cpp:106] Creating Layer relu5
I0521 00:08:48.929915  9137 net.cpp:454] relu5 <- ip1
I0521 00:08:48.929929  9137 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:08:48.930270  9137 net.cpp:150] Setting up relu5
I0521 00:08:48.930284  9137 net.cpp:157] Top shape: 460 196 (90160)
I0521 00:08:48.930294  9137 net.cpp:165] Memory required for data: 725296720
I0521 00:08:48.930305  9137 layer_factory.hpp:77] Creating layer drop1
I0521 00:08:48.930326  9137 net.cpp:106] Creating Layer drop1
I0521 00:08:48.930336  9137 net.cpp:454] drop1 <- ip1
I0521 00:08:48.930363  9137 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:08:48.930410  9137 net.cpp:150] Setting up drop1
I0521 00:08:48.930424  9137 net.cpp:157] Top shape: 460 196 (90160)
I0521 00:08:48.930434  9137 net.cpp:165] Memory required for data: 725657360
I0521 00:08:48.930444  9137 layer_factory.hpp:77] Creating layer ip2
I0521 00:08:48.930462  9137 net.cpp:106] Creating Layer ip2
I0521 00:08:48.930474  9137 net.cpp:454] ip2 <- ip1
I0521 00:08:48.930485  9137 net.cpp:411] ip2 -> ip2
I0521 00:08:48.930953  9137 net.cpp:150] Setting up ip2
I0521 00:08:48.930966  9137 net.cpp:157] Top shape: 460 98 (45080)
I0521 00:08:48.930976  9137 net.cpp:165] Memory required for data: 725837680
I0521 00:08:48.930991  9137 layer_factory.hpp:77] Creating layer relu6
I0521 00:08:48.931005  9137 net.cpp:106] Creating Layer relu6
I0521 00:08:48.931015  9137 net.cpp:454] relu6 <- ip2
I0521 00:08:48.931026  9137 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:08:48.931556  9137 net.cpp:150] Setting up relu6
I0521 00:08:48.931572  9137 net.cpp:157] Top shape: 460 98 (45080)
I0521 00:08:48.931583  9137 net.cpp:165] Memory required for data: 726018000
I0521 00:08:48.931593  9137 layer_factory.hpp:77] Creating layer drop2
I0521 00:08:48.931607  9137 net.cpp:106] Creating Layer drop2
I0521 00:08:48.931617  9137 net.cpp:454] drop2 <- ip2
I0521 00:08:48.931628  9137 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:08:48.931670  9137 net.cpp:150] Setting up drop2
I0521 00:08:48.931684  9137 net.cpp:157] Top shape: 460 98 (45080)
I0521 00:08:48.931694  9137 net.cpp:165] Memory required for data: 726198320
I0521 00:08:48.931704  9137 layer_factory.hpp:77] Creating layer ip3
I0521 00:08:48.931717  9137 net.cpp:106] Creating Layer ip3
I0521 00:08:48.931727  9137 net.cpp:454] ip3 <- ip2
I0521 00:08:48.931740  9137 net.cpp:411] ip3 -> ip3
I0521 00:08:48.931951  9137 net.cpp:150] Setting up ip3
I0521 00:08:48.931965  9137 net.cpp:157] Top shape: 460 11 (5060)
I0521 00:08:48.931974  9137 net.cpp:165] Memory required for data: 726218560
I0521 00:08:48.931990  9137 layer_factory.hpp:77] Creating layer drop3
I0521 00:08:48.932003  9137 net.cpp:106] Creating Layer drop3
I0521 00:08:48.932013  9137 net.cpp:454] drop3 <- ip3
I0521 00:08:48.932024  9137 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:08:48.932062  9137 net.cpp:150] Setting up drop3
I0521 00:08:48.932075  9137 net.cpp:157] Top shape: 460 11 (5060)
I0521 00:08:48.932085  9137 net.cpp:165] Memory required for data: 726238800
I0521 00:08:48.932096  9137 layer_factory.hpp:77] Creating layer loss
I0521 00:08:48.932114  9137 net.cpp:106] Creating Layer loss
I0521 00:08:48.932126  9137 net.cpp:454] loss <- ip3
I0521 00:08:48.932137  9137 net.cpp:454] loss <- label
I0521 00:08:48.932148  9137 net.cpp:411] loss -> loss
I0521 00:08:48.932165  9137 layer_factory.hpp:77] Creating layer loss
I0521 00:08:48.932813  9137 net.cpp:150] Setting up loss
I0521 00:08:48.932834  9137 net.cpp:157] Top shape: (1)
I0521 00:08:48.932847  9137 net.cpp:160]     with loss weight 1
I0521 00:08:48.932889  9137 net.cpp:165] Memory required for data: 726238804
I0521 00:08:48.932900  9137 net.cpp:226] loss needs backward computation.
I0521 00:08:48.932911  9137 net.cpp:226] drop3 needs backward computation.
I0521 00:08:48.932920  9137 net.cpp:226] ip3 needs backward computation.
I0521 00:08:48.932931  9137 net.cpp:226] drop2 needs backward computation.
I0521 00:08:48.932941  9137 net.cpp:226] relu6 needs backward computation.
I0521 00:08:48.932950  9137 net.cpp:226] ip2 needs backward computation.
I0521 00:08:48.932960  9137 net.cpp:226] drop1 needs backward computation.
I0521 00:08:48.932971  9137 net.cpp:226] relu5 needs backward computation.
I0521 00:08:48.932981  9137 net.cpp:226] ip1 needs backward computation.
I0521 00:08:48.932991  9137 net.cpp:226] pool4 needs backward computation.
I0521 00:08:48.933001  9137 net.cpp:226] relu4 needs backward computation.
I0521 00:08:48.933012  9137 net.cpp:226] conv4 needs backward computation.
I0521 00:08:48.933022  9137 net.cpp:226] pool3 needs backward computation.
I0521 00:08:48.933040  9137 net.cpp:226] relu3 needs backward computation.
I0521 00:08:48.933050  9137 net.cpp:226] conv3 needs backward computation.
I0521 00:08:48.933061  9137 net.cpp:226] pool2 needs backward computation.
I0521 00:08:48.933073  9137 net.cpp:226] relu2 needs backward computation.
I0521 00:08:48.933079  9137 net.cpp:226] conv2 needs backward computation.
I0521 00:08:48.933090  9137 net.cpp:226] pool1 needs backward computation.
I0521 00:08:48.933101  9137 net.cpp:226] relu1 needs backward computation.
I0521 00:08:48.933110  9137 net.cpp:226] conv1 needs backward computation.
I0521 00:08:48.933121  9137 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:08:48.933131  9137 net.cpp:270] This network produces output loss
I0521 00:08:48.933154  9137 net.cpp:283] Network initialization done.
I0521 00:08:48.934782  9137 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925.prototxt
I0521 00:08:48.934854  9137 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 00:08:48.935209  9137 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 460
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:08:48.935400  9137 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:08:48.935415  9137 net.cpp:106] Creating Layer data_hdf5
I0521 00:08:48.935427  9137 net.cpp:411] data_hdf5 -> data
I0521 00:08:48.935443  9137 net.cpp:411] data_hdf5 -> label
I0521 00:08:48.935461  9137 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 00:08:48.936658  9137 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 00:09:10.257526  9137 net.cpp:150] Setting up data_hdf5
I0521 00:09:10.257691  9137 net.cpp:157] Top shape: 460 1 127 50 (2921000)
I0521 00:09:10.257705  9137 net.cpp:157] Top shape: 460 (460)
I0521 00:09:10.257716  9137 net.cpp:165] Memory required for data: 11685840
I0521 00:09:10.257730  9137 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 00:09:10.257758  9137 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 00:09:10.257769  9137 net.cpp:454] label_data_hdf5_1_split <- label
I0521 00:09:10.257784  9137 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 00:09:10.257807  9137 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 00:09:10.257879  9137 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 00:09:10.257892  9137 net.cpp:157] Top shape: 460 (460)
I0521 00:09:10.257905  9137 net.cpp:157] Top shape: 460 (460)
I0521 00:09:10.257913  9137 net.cpp:165] Memory required for data: 11689520
I0521 00:09:10.257923  9137 layer_factory.hpp:77] Creating layer conv1
I0521 00:09:10.257946  9137 net.cpp:106] Creating Layer conv1
I0521 00:09:10.257956  9137 net.cpp:454] conv1 <- data
I0521 00:09:10.257971  9137 net.cpp:411] conv1 -> conv1
I0521 00:09:10.259930  9137 net.cpp:150] Setting up conv1
I0521 00:09:10.259954  9137 net.cpp:157] Top shape: 460 12 120 48 (31795200)
I0521 00:09:10.259966  9137 net.cpp:165] Memory required for data: 138870320
I0521 00:09:10.259986  9137 layer_factory.hpp:77] Creating layer relu1
I0521 00:09:10.260001  9137 net.cpp:106] Creating Layer relu1
I0521 00:09:10.260012  9137 net.cpp:454] relu1 <- conv1
I0521 00:09:10.260025  9137 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:09:10.260524  9137 net.cpp:150] Setting up relu1
I0521 00:09:10.260540  9137 net.cpp:157] Top shape: 460 12 120 48 (31795200)
I0521 00:09:10.260550  9137 net.cpp:165] Memory required for data: 266051120
I0521 00:09:10.260560  9137 layer_factory.hpp:77] Creating layer pool1
I0521 00:09:10.260577  9137 net.cpp:106] Creating Layer pool1
I0521 00:09:10.260587  9137 net.cpp:454] pool1 <- conv1
I0521 00:09:10.260601  9137 net.cpp:411] pool1 -> pool1
I0521 00:09:10.260675  9137 net.cpp:150] Setting up pool1
I0521 00:09:10.260689  9137 net.cpp:157] Top shape: 460 12 60 48 (15897600)
I0521 00:09:10.260699  9137 net.cpp:165] Memory required for data: 329641520
I0521 00:09:10.260710  9137 layer_factory.hpp:77] Creating layer conv2
I0521 00:09:10.260726  9137 net.cpp:106] Creating Layer conv2
I0521 00:09:10.260738  9137 net.cpp:454] conv2 <- pool1
I0521 00:09:10.260752  9137 net.cpp:411] conv2 -> conv2
I0521 00:09:10.262678  9137 net.cpp:150] Setting up conv2
I0521 00:09:10.262701  9137 net.cpp:157] Top shape: 460 20 54 46 (22852800)
I0521 00:09:10.262713  9137 net.cpp:165] Memory required for data: 421052720
I0521 00:09:10.262732  9137 layer_factory.hpp:77] Creating layer relu2
I0521 00:09:10.262745  9137 net.cpp:106] Creating Layer relu2
I0521 00:09:10.262755  9137 net.cpp:454] relu2 <- conv2
I0521 00:09:10.262768  9137 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:09:10.263103  9137 net.cpp:150] Setting up relu2
I0521 00:09:10.263116  9137 net.cpp:157] Top shape: 460 20 54 46 (22852800)
I0521 00:09:10.263126  9137 net.cpp:165] Memory required for data: 512463920
I0521 00:09:10.263136  9137 layer_factory.hpp:77] Creating layer pool2
I0521 00:09:10.263150  9137 net.cpp:106] Creating Layer pool2
I0521 00:09:10.263160  9137 net.cpp:454] pool2 <- conv2
I0521 00:09:10.263172  9137 net.cpp:411] pool2 -> pool2
I0521 00:09:10.263243  9137 net.cpp:150] Setting up pool2
I0521 00:09:10.263257  9137 net.cpp:157] Top shape: 460 20 27 46 (11426400)
I0521 00:09:10.263267  9137 net.cpp:165] Memory required for data: 558169520
I0521 00:09:10.263275  9137 layer_factory.hpp:77] Creating layer conv3
I0521 00:09:10.263295  9137 net.cpp:106] Creating Layer conv3
I0521 00:09:10.263306  9137 net.cpp:454] conv3 <- pool2
I0521 00:09:10.263319  9137 net.cpp:411] conv3 -> conv3
I0521 00:09:10.265297  9137 net.cpp:150] Setting up conv3
I0521 00:09:10.265321  9137 net.cpp:157] Top shape: 460 28 22 44 (12467840)
I0521 00:09:10.265331  9137 net.cpp:165] Memory required for data: 608040880
I0521 00:09:10.265362  9137 layer_factory.hpp:77] Creating layer relu3
I0521 00:09:10.265377  9137 net.cpp:106] Creating Layer relu3
I0521 00:09:10.265386  9137 net.cpp:454] relu3 <- conv3
I0521 00:09:10.265398  9137 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:09:10.265873  9137 net.cpp:150] Setting up relu3
I0521 00:09:10.265889  9137 net.cpp:157] Top shape: 460 28 22 44 (12467840)
I0521 00:09:10.265900  9137 net.cpp:165] Memory required for data: 657912240
I0521 00:09:10.265910  9137 layer_factory.hpp:77] Creating layer pool3
I0521 00:09:10.265923  9137 net.cpp:106] Creating Layer pool3
I0521 00:09:10.265933  9137 net.cpp:454] pool3 <- conv3
I0521 00:09:10.265946  9137 net.cpp:411] pool3 -> pool3
I0521 00:09:10.266017  9137 net.cpp:150] Setting up pool3
I0521 00:09:10.266031  9137 net.cpp:157] Top shape: 460 28 11 44 (6233920)
I0521 00:09:10.266041  9137 net.cpp:165] Memory required for data: 682847920
I0521 00:09:10.266049  9137 layer_factory.hpp:77] Creating layer conv4
I0521 00:09:10.266067  9137 net.cpp:106] Creating Layer conv4
I0521 00:09:10.266077  9137 net.cpp:454] conv4 <- pool3
I0521 00:09:10.266093  9137 net.cpp:411] conv4 -> conv4
I0521 00:09:10.268148  9137 net.cpp:150] Setting up conv4
I0521 00:09:10.268170  9137 net.cpp:157] Top shape: 460 36 6 42 (4173120)
I0521 00:09:10.268183  9137 net.cpp:165] Memory required for data: 699540400
I0521 00:09:10.268198  9137 layer_factory.hpp:77] Creating layer relu4
I0521 00:09:10.268213  9137 net.cpp:106] Creating Layer relu4
I0521 00:09:10.268223  9137 net.cpp:454] relu4 <- conv4
I0521 00:09:10.268234  9137 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:09:10.268708  9137 net.cpp:150] Setting up relu4
I0521 00:09:10.268725  9137 net.cpp:157] Top shape: 460 36 6 42 (4173120)
I0521 00:09:10.268735  9137 net.cpp:165] Memory required for data: 716232880
I0521 00:09:10.268745  9137 layer_factory.hpp:77] Creating layer pool4
I0521 00:09:10.268759  9137 net.cpp:106] Creating Layer pool4
I0521 00:09:10.268769  9137 net.cpp:454] pool4 <- conv4
I0521 00:09:10.268781  9137 net.cpp:411] pool4 -> pool4
I0521 00:09:10.268852  9137 net.cpp:150] Setting up pool4
I0521 00:09:10.268867  9137 net.cpp:157] Top shape: 460 36 3 42 (2086560)
I0521 00:09:10.268877  9137 net.cpp:165] Memory required for data: 724579120
I0521 00:09:10.268887  9137 layer_factory.hpp:77] Creating layer ip1
I0521 00:09:10.268903  9137 net.cpp:106] Creating Layer ip1
I0521 00:09:10.268913  9137 net.cpp:454] ip1 <- pool4
I0521 00:09:10.268928  9137 net.cpp:411] ip1 -> ip1
I0521 00:09:10.284461  9137 net.cpp:150] Setting up ip1
I0521 00:09:10.284488  9137 net.cpp:157] Top shape: 460 196 (90160)
I0521 00:09:10.284504  9137 net.cpp:165] Memory required for data: 724939760
I0521 00:09:10.284526  9137 layer_factory.hpp:77] Creating layer relu5
I0521 00:09:10.284543  9137 net.cpp:106] Creating Layer relu5
I0521 00:09:10.284553  9137 net.cpp:454] relu5 <- ip1
I0521 00:09:10.284565  9137 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:09:10.284914  9137 net.cpp:150] Setting up relu5
I0521 00:09:10.284927  9137 net.cpp:157] Top shape: 460 196 (90160)
I0521 00:09:10.284936  9137 net.cpp:165] Memory required for data: 725300400
I0521 00:09:10.284947  9137 layer_factory.hpp:77] Creating layer drop1
I0521 00:09:10.284965  9137 net.cpp:106] Creating Layer drop1
I0521 00:09:10.284978  9137 net.cpp:454] drop1 <- ip1
I0521 00:09:10.284991  9137 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:09:10.285034  9137 net.cpp:150] Setting up drop1
I0521 00:09:10.285048  9137 net.cpp:157] Top shape: 460 196 (90160)
I0521 00:09:10.285058  9137 net.cpp:165] Memory required for data: 725661040
I0521 00:09:10.285066  9137 layer_factory.hpp:77] Creating layer ip2
I0521 00:09:10.285082  9137 net.cpp:106] Creating Layer ip2
I0521 00:09:10.285092  9137 net.cpp:454] ip2 <- ip1
I0521 00:09:10.285105  9137 net.cpp:411] ip2 -> ip2
I0521 00:09:10.285585  9137 net.cpp:150] Setting up ip2
I0521 00:09:10.285598  9137 net.cpp:157] Top shape: 460 98 (45080)
I0521 00:09:10.285609  9137 net.cpp:165] Memory required for data: 725841360
I0521 00:09:10.285639  9137 layer_factory.hpp:77] Creating layer relu6
I0521 00:09:10.285651  9137 net.cpp:106] Creating Layer relu6
I0521 00:09:10.285661  9137 net.cpp:454] relu6 <- ip2
I0521 00:09:10.285673  9137 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:09:10.286211  9137 net.cpp:150] Setting up relu6
I0521 00:09:10.286231  9137 net.cpp:157] Top shape: 460 98 (45080)
I0521 00:09:10.286242  9137 net.cpp:165] Memory required for data: 726021680
I0521 00:09:10.286252  9137 layer_factory.hpp:77] Creating layer drop2
I0521 00:09:10.286264  9137 net.cpp:106] Creating Layer drop2
I0521 00:09:10.286275  9137 net.cpp:454] drop2 <- ip2
I0521 00:09:10.286288  9137 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:09:10.286332  9137 net.cpp:150] Setting up drop2
I0521 00:09:10.286345  9137 net.cpp:157] Top shape: 460 98 (45080)
I0521 00:09:10.286355  9137 net.cpp:165] Memory required for data: 726202000
I0521 00:09:10.286365  9137 layer_factory.hpp:77] Creating layer ip3
I0521 00:09:10.286381  9137 net.cpp:106] Creating Layer ip3
I0521 00:09:10.286391  9137 net.cpp:454] ip3 <- ip2
I0521 00:09:10.286403  9137 net.cpp:411] ip3 -> ip3
I0521 00:09:10.286626  9137 net.cpp:150] Setting up ip3
I0521 00:09:10.286639  9137 net.cpp:157] Top shape: 460 11 (5060)
I0521 00:09:10.286649  9137 net.cpp:165] Memory required for data: 726222240
I0521 00:09:10.286665  9137 layer_factory.hpp:77] Creating layer drop3
I0521 00:09:10.286679  9137 net.cpp:106] Creating Layer drop3
I0521 00:09:10.286689  9137 net.cpp:454] drop3 <- ip3
I0521 00:09:10.286700  9137 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:09:10.286742  9137 net.cpp:150] Setting up drop3
I0521 00:09:10.286754  9137 net.cpp:157] Top shape: 460 11 (5060)
I0521 00:09:10.286763  9137 net.cpp:165] Memory required for data: 726242480
I0521 00:09:10.286773  9137 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 00:09:10.286787  9137 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 00:09:10.286797  9137 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 00:09:10.286808  9137 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 00:09:10.286823  9137 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 00:09:10.286895  9137 net.cpp:150] Setting up ip3_drop3_0_split
I0521 00:09:10.286909  9137 net.cpp:157] Top shape: 460 11 (5060)
I0521 00:09:10.286921  9137 net.cpp:157] Top shape: 460 11 (5060)
I0521 00:09:10.286931  9137 net.cpp:165] Memory required for data: 726282960
I0521 00:09:10.286942  9137 layer_factory.hpp:77] Creating layer accuracy
I0521 00:09:10.286963  9137 net.cpp:106] Creating Layer accuracy
I0521 00:09:10.286974  9137 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 00:09:10.286985  9137 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 00:09:10.286998  9137 net.cpp:411] accuracy -> accuracy
I0521 00:09:10.287022  9137 net.cpp:150] Setting up accuracy
I0521 00:09:10.287035  9137 net.cpp:157] Top shape: (1)
I0521 00:09:10.287045  9137 net.cpp:165] Memory required for data: 726282964
I0521 00:09:10.287056  9137 layer_factory.hpp:77] Creating layer loss
I0521 00:09:10.287070  9137 net.cpp:106] Creating Layer loss
I0521 00:09:10.287081  9137 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 00:09:10.287091  9137 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 00:09:10.287103  9137 net.cpp:411] loss -> loss
I0521 00:09:10.287122  9137 layer_factory.hpp:77] Creating layer loss
I0521 00:09:10.287619  9137 net.cpp:150] Setting up loss
I0521 00:09:10.287632  9137 net.cpp:157] Top shape: (1)
I0521 00:09:10.287642  9137 net.cpp:160]     with loss weight 1
I0521 00:09:10.287660  9137 net.cpp:165] Memory required for data: 726282968
I0521 00:09:10.287672  9137 net.cpp:226] loss needs backward computation.
I0521 00:09:10.287683  9137 net.cpp:228] accuracy does not need backward computation.
I0521 00:09:10.287693  9137 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 00:09:10.287704  9137 net.cpp:226] drop3 needs backward computation.
I0521 00:09:10.287713  9137 net.cpp:226] ip3 needs backward computation.
I0521 00:09:10.287722  9137 net.cpp:226] drop2 needs backward computation.
I0521 00:09:10.287741  9137 net.cpp:226] relu6 needs backward computation.
I0521 00:09:10.287751  9137 net.cpp:226] ip2 needs backward computation.
I0521 00:09:10.287761  9137 net.cpp:226] drop1 needs backward computation.
I0521 00:09:10.287770  9137 net.cpp:226] relu5 needs backward computation.
I0521 00:09:10.287781  9137 net.cpp:226] ip1 needs backward computation.
I0521 00:09:10.287789  9137 net.cpp:226] pool4 needs backward computation.
I0521 00:09:10.287799  9137 net.cpp:226] relu4 needs backward computation.
I0521 00:09:10.287811  9137 net.cpp:226] conv4 needs backward computation.
I0521 00:09:10.287820  9137 net.cpp:226] pool3 needs backward computation.
I0521 00:09:10.287830  9137 net.cpp:226] relu3 needs backward computation.
I0521 00:09:10.287838  9137 net.cpp:226] conv3 needs backward computation.
I0521 00:09:10.287848  9137 net.cpp:226] pool2 needs backward computation.
I0521 00:09:10.287859  9137 net.cpp:226] relu2 needs backward computation.
I0521 00:09:10.287869  9137 net.cpp:226] conv2 needs backward computation.
I0521 00:09:10.287880  9137 net.cpp:226] pool1 needs backward computation.
I0521 00:09:10.287890  9137 net.cpp:226] relu1 needs backward computation.
I0521 00:09:10.287899  9137 net.cpp:226] conv1 needs backward computation.
I0521 00:09:10.287910  9137 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 00:09:10.287922  9137 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:09:10.287931  9137 net.cpp:270] This network produces output accuracy
I0521 00:09:10.287942  9137 net.cpp:270] This network produces output loss
I0521 00:09:10.287971  9137 net.cpp:283] Network initialization done.
I0521 00:09:10.288105  9137 solver.cpp:60] Solver scaffolding done.
I0521 00:09:10.289232  9137 caffe.cpp:212] Starting Optimization
I0521 00:09:10.289249  9137 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 00:09:10.289259  9137 solver.cpp:289] Learning Rate Policy: fixed
I0521 00:09:10.290475  9137 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 00:09:56.355075  9137 solver.cpp:409]     Test net output #0: accuracy = 0.13811
I0521 00:09:56.355233  9137 solver.cpp:409]     Test net output #1: loss = 2.39695 (* 1 = 2.39695 loss)
I0521 00:09:56.447315  9137 solver.cpp:237] Iteration 0, loss = 2.39889
I0521 00:09:56.447351  9137 solver.cpp:253]     Train net output #0: loss = 2.39889 (* 1 = 2.39889 loss)
I0521 00:09:56.447370  9137 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 00:10:04.392760  9137 solver.cpp:237] Iteration 32, loss = 2.37619
I0521 00:10:04.392794  9137 solver.cpp:253]     Train net output #0: loss = 2.37619 (* 1 = 2.37619 loss)
I0521 00:10:04.392812  9137 sgd_solver.cpp:106] Iteration 32, lr = 0.0025
I0521 00:10:12.333246  9137 solver.cpp:237] Iteration 64, loss = 2.35331
I0521 00:10:12.333292  9137 solver.cpp:253]     Train net output #0: loss = 2.35331 (* 1 = 2.35331 loss)
I0521 00:10:12.333309  9137 sgd_solver.cpp:106] Iteration 64, lr = 0.0025
I0521 00:10:20.270946  9137 solver.cpp:237] Iteration 96, loss = 2.33655
I0521 00:10:20.270977  9137 solver.cpp:253]     Train net output #0: loss = 2.33655 (* 1 = 2.33655 loss)
I0521 00:10:20.270992  9137 sgd_solver.cpp:106] Iteration 96, lr = 0.0025
I0521 00:10:28.203387  9137 solver.cpp:237] Iteration 128, loss = 2.32732
I0521 00:10:28.203536  9137 solver.cpp:253]     Train net output #0: loss = 2.32732 (* 1 = 2.32732 loss)
I0521 00:10:28.203550  9137 sgd_solver.cpp:106] Iteration 128, lr = 0.0025
I0521 00:10:36.148769  9137 solver.cpp:237] Iteration 160, loss = 2.30608
I0521 00:10:36.148814  9137 solver.cpp:253]     Train net output #0: loss = 2.30608 (* 1 = 2.30608 loss)
I0521 00:10:36.148830  9137 sgd_solver.cpp:106] Iteration 160, lr = 0.0025
I0521 00:10:44.088595  9137 solver.cpp:237] Iteration 192, loss = 2.31478
I0521 00:10:44.088629  9137 solver.cpp:253]     Train net output #0: loss = 2.31478 (* 1 = 2.31478 loss)
I0521 00:10:44.088642  9137 sgd_solver.cpp:106] Iteration 192, lr = 0.0025
I0521 00:11:14.128836  9137 solver.cpp:237] Iteration 224, loss = 2.29694
I0521 00:11:14.128998  9137 solver.cpp:253]     Train net output #0: loss = 2.29694 (* 1 = 2.29694 loss)
I0521 00:11:14.129014  9137 sgd_solver.cpp:106] Iteration 224, lr = 0.0025
I0521 00:11:22.071655  9137 solver.cpp:237] Iteration 256, loss = 2.30037
I0521 00:11:22.071687  9137 solver.cpp:253]     Train net output #0: loss = 2.30037 (* 1 = 2.30037 loss)
I0521 00:11:22.071702  9137 sgd_solver.cpp:106] Iteration 256, lr = 0.0025
I0521 00:11:30.017551  9137 solver.cpp:237] Iteration 288, loss = 2.20922
I0521 00:11:30.017590  9137 solver.cpp:253]     Train net output #0: loss = 2.20922 (* 1 = 2.20922 loss)
I0521 00:11:30.017611  9137 sgd_solver.cpp:106] Iteration 288, lr = 0.0025
I0521 00:11:37.963860  9137 solver.cpp:237] Iteration 320, loss = 2.16157
I0521 00:11:37.963893  9137 solver.cpp:253]     Train net output #0: loss = 2.16157 (* 1 = 2.16157 loss)
I0521 00:11:37.963909  9137 sgd_solver.cpp:106] Iteration 320, lr = 0.0025
I0521 00:11:39.203320  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_326.caffemodel
I0521 00:11:39.421255  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_326.solverstate
I0521 00:11:45.972661  9137 solver.cpp:237] Iteration 352, loss = 2.11061
I0521 00:11:45.972812  9137 solver.cpp:253]     Train net output #0: loss = 2.11061 (* 1 = 2.11061 loss)
I0521 00:11:45.972826  9137 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 00:11:53.921872  9137 solver.cpp:237] Iteration 384, loss = 2.14543
I0521 00:11:53.921917  9137 solver.cpp:253]     Train net output #0: loss = 2.14543 (* 1 = 2.14543 loss)
I0521 00:11:53.921936  9137 sgd_solver.cpp:106] Iteration 384, lr = 0.0025
I0521 00:12:01.860893  9137 solver.cpp:237] Iteration 416, loss = 2.12197
I0521 00:12:01.860925  9137 solver.cpp:253]     Train net output #0: loss = 2.12197 (* 1 = 2.12197 loss)
I0521 00:12:01.860942  9137 sgd_solver.cpp:106] Iteration 416, lr = 0.0025
I0521 00:12:31.939267  9137 solver.cpp:237] Iteration 448, loss = 2.02492
I0521 00:12:31.939424  9137 solver.cpp:253]     Train net output #0: loss = 2.02492 (* 1 = 2.02492 loss)
I0521 00:12:31.939438  9137 sgd_solver.cpp:106] Iteration 448, lr = 0.0025
I0521 00:12:39.884850  9137 solver.cpp:237] Iteration 480, loss = 2.02106
I0521 00:12:39.884894  9137 solver.cpp:253]     Train net output #0: loss = 2.02106 (* 1 = 2.02106 loss)
I0521 00:12:39.884912  9137 sgd_solver.cpp:106] Iteration 480, lr = 0.0025
I0521 00:12:47.827908  9137 solver.cpp:237] Iteration 512, loss = 1.95844
I0521 00:12:47.827944  9137 solver.cpp:253]     Train net output #0: loss = 1.95844 (* 1 = 1.95844 loss)
I0521 00:12:47.827960  9137 sgd_solver.cpp:106] Iteration 512, lr = 0.0025
I0521 00:12:55.760846  9137 solver.cpp:237] Iteration 544, loss = 1.90555
I0521 00:12:55.760879  9137 solver.cpp:253]     Train net output #0: loss = 1.90555 (* 1 = 1.90555 loss)
I0521 00:12:55.760892  9137 sgd_solver.cpp:106] Iteration 544, lr = 0.0025
I0521 00:13:03.703064  9137 solver.cpp:237] Iteration 576, loss = 1.96649
I0521 00:13:03.703224  9137 solver.cpp:253]     Train net output #0: loss = 1.96649 (* 1 = 1.96649 loss)
I0521 00:13:03.703238  9137 sgd_solver.cpp:106] Iteration 576, lr = 0.0025
I0521 00:13:11.649096  9137 solver.cpp:237] Iteration 608, loss = 1.98982
I0521 00:13:11.649128  9137 solver.cpp:253]     Train net output #0: loss = 1.98982 (* 1 = 1.98982 loss)
I0521 00:13:11.649147  9137 sgd_solver.cpp:106] Iteration 608, lr = 0.0025
I0521 00:13:19.587559  9137 solver.cpp:237] Iteration 640, loss = 1.90317
I0521 00:13:19.587592  9137 solver.cpp:253]     Train net output #0: loss = 1.90317 (* 1 = 1.90317 loss)
I0521 00:13:19.587609  9137 sgd_solver.cpp:106] Iteration 640, lr = 0.0025
I0521 00:13:22.320158  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_652.caffemodel
I0521 00:13:22.532295  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_652.solverstate
I0521 00:13:22.557354  9137 solver.cpp:341] Iteration 652, Testing net (#0)
I0521 00:14:07.748386  9137 solver.cpp:409]     Test net output #0: accuracy = 0.579428
I0521 00:14:07.748544  9137 solver.cpp:409]     Test net output #1: loss = 1.52072 (* 1 = 1.52072 loss)
I0521 00:14:34.930280  9137 solver.cpp:237] Iteration 672, loss = 1.87171
I0521 00:14:34.930330  9137 solver.cpp:253]     Train net output #0: loss = 1.87171 (* 1 = 1.87171 loss)
I0521 00:14:34.930346  9137 sgd_solver.cpp:106] Iteration 672, lr = 0.0025
I0521 00:14:42.866138  9137 solver.cpp:237] Iteration 704, loss = 1.83546
I0521 00:14:42.866291  9137 solver.cpp:253]     Train net output #0: loss = 1.83546 (* 1 = 1.83546 loss)
I0521 00:14:42.866307  9137 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 00:14:50.800643  9137 solver.cpp:237] Iteration 736, loss = 1.83364
I0521 00:14:50.800675  9137 solver.cpp:253]     Train net output #0: loss = 1.83364 (* 1 = 1.83364 loss)
I0521 00:14:50.800693  9137 sgd_solver.cpp:106] Iteration 736, lr = 0.0025
I0521 00:14:58.735237  9137 solver.cpp:237] Iteration 768, loss = 1.91067
I0521 00:14:58.735270  9137 solver.cpp:253]     Train net output #0: loss = 1.91067 (* 1 = 1.91067 loss)
I0521 00:14:58.735286  9137 sgd_solver.cpp:106] Iteration 768, lr = 0.0025
I0521 00:15:06.669142  9137 solver.cpp:237] Iteration 800, loss = 1.85348
I0521 00:15:06.669173  9137 solver.cpp:253]     Train net output #0: loss = 1.85348 (* 1 = 1.85348 loss)
I0521 00:15:06.669190  9137 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0521 00:15:14.611472  9137 solver.cpp:237] Iteration 832, loss = 1.77818
I0521 00:15:14.611603  9137 solver.cpp:253]     Train net output #0: loss = 1.77818 (* 1 = 1.77818 loss)
I0521 00:15:14.611618  9137 sgd_solver.cpp:106] Iteration 832, lr = 0.0025
I0521 00:15:22.545167  9137 solver.cpp:237] Iteration 864, loss = 1.82476
I0521 00:15:22.545199  9137 solver.cpp:253]     Train net output #0: loss = 1.82476 (* 1 = 1.82476 loss)
I0521 00:15:22.545217  9137 sgd_solver.cpp:106] Iteration 864, lr = 0.0025
I0521 00:15:52.645313  9137 solver.cpp:237] Iteration 896, loss = 1.85939
I0521 00:15:52.645486  9137 solver.cpp:253]     Train net output #0: loss = 1.85939 (* 1 = 1.85939 loss)
I0521 00:15:52.645501  9137 sgd_solver.cpp:106] Iteration 896, lr = 0.0025
I0521 00:16:00.581697  9137 solver.cpp:237] Iteration 928, loss = 1.83412
I0521 00:16:00.581737  9137 solver.cpp:253]     Train net output #0: loss = 1.83412 (* 1 = 1.83412 loss)
I0521 00:16:00.581754  9137 sgd_solver.cpp:106] Iteration 928, lr = 0.0025
I0521 00:16:08.521173  9137 solver.cpp:237] Iteration 960, loss = 1.80796
I0521 00:16:08.521206  9137 solver.cpp:253]     Train net output #0: loss = 1.80796 (* 1 = 1.80796 loss)
I0521 00:16:08.521222  9137 sgd_solver.cpp:106] Iteration 960, lr = 0.0025
I0521 00:16:12.739369  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_978.caffemodel
I0521 00:16:12.965859  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_978.solverstate
I0521 00:16:16.537853  9137 solver.cpp:237] Iteration 992, loss = 1.73152
I0521 00:16:16.537900  9137 solver.cpp:253]     Train net output #0: loss = 1.73152 (* 1 = 1.73152 loss)
I0521 00:16:16.537916  9137 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0521 00:16:24.480448  9137 solver.cpp:237] Iteration 1024, loss = 1.84805
I0521 00:16:24.480614  9137 solver.cpp:253]     Train net output #0: loss = 1.84805 (* 1 = 1.84805 loss)
I0521 00:16:24.480628  9137 sgd_solver.cpp:106] Iteration 1024, lr = 0.0025
I0521 00:16:32.422880  9137 solver.cpp:237] Iteration 1056, loss = 1.90268
I0521 00:16:32.422912  9137 solver.cpp:253]     Train net output #0: loss = 1.90268 (* 1 = 1.90268 loss)
I0521 00:16:32.422926  9137 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 00:17:02.499004  9137 solver.cpp:237] Iteration 1088, loss = 1.79951
I0521 00:17:02.499166  9137 solver.cpp:253]     Train net output #0: loss = 1.79951 (* 1 = 1.79951 loss)
I0521 00:17:02.499181  9137 sgd_solver.cpp:106] Iteration 1088, lr = 0.0025
I0521 00:17:10.436847  9137 solver.cpp:237] Iteration 1120, loss = 1.75725
I0521 00:17:10.436879  9137 solver.cpp:253]     Train net output #0: loss = 1.75725 (* 1 = 1.75725 loss)
I0521 00:17:10.436899  9137 sgd_solver.cpp:106] Iteration 1120, lr = 0.0025
I0521 00:17:18.371209  9137 solver.cpp:237] Iteration 1152, loss = 1.86492
I0521 00:17:18.371254  9137 solver.cpp:253]     Train net output #0: loss = 1.86492 (* 1 = 1.86492 loss)
I0521 00:17:18.371269  9137 sgd_solver.cpp:106] Iteration 1152, lr = 0.0025
I0521 00:17:26.301455  9137 solver.cpp:237] Iteration 1184, loss = 1.81032
I0521 00:17:26.301488  9137 solver.cpp:253]     Train net output #0: loss = 1.81032 (* 1 = 1.81032 loss)
I0521 00:17:26.301506  9137 sgd_solver.cpp:106] Iteration 1184, lr = 0.0025
I0521 00:17:34.236362  9137 solver.cpp:237] Iteration 1216, loss = 1.75565
I0521 00:17:34.236511  9137 solver.cpp:253]     Train net output #0: loss = 1.75565 (* 1 = 1.75565 loss)
I0521 00:17:34.236524  9137 sgd_solver.cpp:106] Iteration 1216, lr = 0.0025
I0521 00:17:42.173991  9137 solver.cpp:237] Iteration 1248, loss = 1.76915
I0521 00:17:42.174032  9137 solver.cpp:253]     Train net output #0: loss = 1.76915 (* 1 = 1.76915 loss)
I0521 00:17:42.174049  9137 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0521 00:17:50.113396  9137 solver.cpp:237] Iteration 1280, loss = 1.66672
I0521 00:17:50.113430  9137 solver.cpp:253]     Train net output #0: loss = 1.66672 (* 1 = 1.66672 loss)
I0521 00:17:50.113446  9137 sgd_solver.cpp:106] Iteration 1280, lr = 0.0025
I0521 00:17:55.817539  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_1304.caffemodel
I0521 00:17:56.031478  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_1304.solverstate
I0521 00:17:56.059950  9137 solver.cpp:341] Iteration 1304, Testing net (#0)
I0521 00:19:02.118916  9137 solver.cpp:409]     Test net output #0: accuracy = 0.648313
I0521 00:19:02.119083  9137 solver.cpp:409]     Test net output #1: loss = 1.21131 (* 1 = 1.21131 loss)
I0521 00:19:26.332973  9137 solver.cpp:237] Iteration 1312, loss = 1.81829
I0521 00:19:26.333019  9137 solver.cpp:253]     Train net output #0: loss = 1.81829 (* 1 = 1.81829 loss)
I0521 00:19:26.333037  9137 sgd_solver.cpp:106] Iteration 1312, lr = 0.0025
I0521 00:19:34.264916  9137 solver.cpp:237] Iteration 1344, loss = 1.76538
I0521 00:19:34.265065  9137 solver.cpp:253]     Train net output #0: loss = 1.76538 (* 1 = 1.76538 loss)
I0521 00:19:34.265079  9137 sgd_solver.cpp:106] Iteration 1344, lr = 0.0025
I0521 00:19:42.196126  9137 solver.cpp:237] Iteration 1376, loss = 1.6751
I0521 00:19:42.196158  9137 solver.cpp:253]     Train net output #0: loss = 1.6751 (* 1 = 1.6751 loss)
I0521 00:19:42.196176  9137 sgd_solver.cpp:106] Iteration 1376, lr = 0.0025
I0521 00:19:50.126328  9137 solver.cpp:237] Iteration 1408, loss = 1.80416
I0521 00:19:50.126363  9137 solver.cpp:253]     Train net output #0: loss = 1.80416 (* 1 = 1.80416 loss)
I0521 00:19:50.126384  9137 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 00:19:58.055639  9137 solver.cpp:237] Iteration 1440, loss = 1.71492
I0521 00:19:58.055671  9137 solver.cpp:253]     Train net output #0: loss = 1.71492 (* 1 = 1.71492 loss)
I0521 00:19:58.055690  9137 sgd_solver.cpp:106] Iteration 1440, lr = 0.0025
I0521 00:20:05.990697  9137 solver.cpp:237] Iteration 1472, loss = 1.77435
I0521 00:20:05.990833  9137 solver.cpp:253]     Train net output #0: loss = 1.77435 (* 1 = 1.77435 loss)
I0521 00:20:05.990847  9137 sgd_solver.cpp:106] Iteration 1472, lr = 0.0025
I0521 00:20:13.921036  9137 solver.cpp:237] Iteration 1504, loss = 1.70764
I0521 00:20:13.921073  9137 solver.cpp:253]     Train net output #0: loss = 1.70764 (* 1 = 1.70764 loss)
I0521 00:20:13.921097  9137 sgd_solver.cpp:106] Iteration 1504, lr = 0.0025
I0521 00:20:43.995383  9137 solver.cpp:237] Iteration 1536, loss = 1.65787
I0521 00:20:43.995553  9137 solver.cpp:253]     Train net output #0: loss = 1.65787 (* 1 = 1.65787 loss)
I0521 00:20:43.995568  9137 sgd_solver.cpp:106] Iteration 1536, lr = 0.0025
I0521 00:20:51.929478  9137 solver.cpp:237] Iteration 1568, loss = 1.71541
I0521 00:20:51.929510  9137 solver.cpp:253]     Train net output #0: loss = 1.71541 (* 1 = 1.71541 loss)
I0521 00:20:51.929527  9137 sgd_solver.cpp:106] Iteration 1568, lr = 0.0025
I0521 00:20:59.858222  9137 solver.cpp:237] Iteration 1600, loss = 1.8252
I0521 00:20:59.858263  9137 solver.cpp:253]     Train net output #0: loss = 1.8252 (* 1 = 1.8252 loss)
I0521 00:20:59.858281  9137 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0521 00:21:07.046288  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_1630.caffemodel
I0521 00:21:07.260324  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_1630.solverstate
I0521 00:21:07.858072  9137 solver.cpp:237] Iteration 1632, loss = 1.73327
I0521 00:21:07.858117  9137 solver.cpp:253]     Train net output #0: loss = 1.73327 (* 1 = 1.73327 loss)
I0521 00:21:07.858137  9137 sgd_solver.cpp:106] Iteration 1632, lr = 0.0025
I0521 00:21:15.787478  9137 solver.cpp:237] Iteration 1664, loss = 1.64599
I0521 00:21:15.787631  9137 solver.cpp:253]     Train net output #0: loss = 1.64599 (* 1 = 1.64599 loss)
I0521 00:21:15.787644  9137 sgd_solver.cpp:106] Iteration 1664, lr = 0.0025
I0521 00:21:23.715755  9137 solver.cpp:237] Iteration 1696, loss = 1.70596
I0521 00:21:23.715796  9137 solver.cpp:253]     Train net output #0: loss = 1.70596 (* 1 = 1.70596 loss)
I0521 00:21:23.715814  9137 sgd_solver.cpp:106] Iteration 1696, lr = 0.0025
I0521 00:21:31.645125  9137 solver.cpp:237] Iteration 1728, loss = 1.69096
I0521 00:21:31.645158  9137 solver.cpp:253]     Train net output #0: loss = 1.69096 (* 1 = 1.69096 loss)
I0521 00:21:31.645171  9137 sgd_solver.cpp:106] Iteration 1728, lr = 0.0025
I0521 00:22:01.743923  9137 solver.cpp:237] Iteration 1760, loss = 1.71975
I0521 00:22:01.744098  9137 solver.cpp:253]     Train net output #0: loss = 1.71975 (* 1 = 1.71975 loss)
I0521 00:22:01.744114  9137 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0521 00:22:09.672399  9137 solver.cpp:237] Iteration 1792, loss = 1.64708
I0521 00:22:09.672431  9137 solver.cpp:253]     Train net output #0: loss = 1.64708 (* 1 = 1.64708 loss)
I0521 00:22:09.672449  9137 sgd_solver.cpp:106] Iteration 1792, lr = 0.0025
I0521 00:22:17.600934  9137 solver.cpp:237] Iteration 1824, loss = 1.64932
I0521 00:22:17.600975  9137 solver.cpp:253]     Train net output #0: loss = 1.64932 (* 1 = 1.64932 loss)
I0521 00:22:17.600996  9137 sgd_solver.cpp:106] Iteration 1824, lr = 0.0025
I0521 00:22:25.525534  9137 solver.cpp:237] Iteration 1856, loss = 1.666
I0521 00:22:25.525568  9137 solver.cpp:253]     Train net output #0: loss = 1.666 (* 1 = 1.666 loss)
I0521 00:22:25.525583  9137 sgd_solver.cpp:106] Iteration 1856, lr = 0.0025
I0521 00:22:33.448688  9137 solver.cpp:237] Iteration 1888, loss = 1.64918
I0521 00:22:33.448827  9137 solver.cpp:253]     Train net output #0: loss = 1.64918 (* 1 = 1.64918 loss)
I0521 00:22:33.448842  9137 sgd_solver.cpp:106] Iteration 1888, lr = 0.0025
I0521 00:22:41.370564  9137 solver.cpp:237] Iteration 1920, loss = 1.66929
I0521 00:22:41.370602  9137 solver.cpp:253]     Train net output #0: loss = 1.66929 (* 1 = 1.66929 loss)
I0521 00:22:41.370620  9137 sgd_solver.cpp:106] Iteration 1920, lr = 0.0025
I0521 00:22:49.296911  9137 solver.cpp:237] Iteration 1952, loss = 1.71808
I0521 00:22:49.296944  9137 solver.cpp:253]     Train net output #0: loss = 1.71808 (* 1 = 1.71808 loss)
I0521 00:22:49.296962  9137 sgd_solver.cpp:106] Iteration 1952, lr = 0.0025
I0521 00:22:50.040902  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_1956.caffemodel
I0521 00:22:50.251850  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_1956.solverstate
I0521 00:22:50.278062  9137 solver.cpp:341] Iteration 1956, Testing net (#0)
I0521 00:23:35.134630  9137 solver.cpp:409]     Test net output #0: accuracy = 0.681195
I0521 00:23:35.134790  9137 solver.cpp:409]     Test net output #1: loss = 1.09817 (* 1 = 1.09817 loss)
I0521 00:24:04.294080  9137 solver.cpp:237] Iteration 1984, loss = 1.66117
I0521 00:24:04.294128  9137 solver.cpp:253]     Train net output #0: loss = 1.66117 (* 1 = 1.66117 loss)
I0521 00:24:04.294144  9137 sgd_solver.cpp:106] Iteration 1984, lr = 0.0025
I0521 00:24:12.225039  9137 solver.cpp:237] Iteration 2016, loss = 1.64107
I0521 00:24:12.225186  9137 solver.cpp:253]     Train net output #0: loss = 1.64107 (* 1 = 1.64107 loss)
I0521 00:24:12.225200  9137 sgd_solver.cpp:106] Iteration 2016, lr = 0.0025
I0521 00:24:20.160830  9137 solver.cpp:237] Iteration 2048, loss = 1.65773
I0521 00:24:20.160858  9137 solver.cpp:253]     Train net output #0: loss = 1.65773 (* 1 = 1.65773 loss)
I0521 00:24:20.160883  9137 sgd_solver.cpp:106] Iteration 2048, lr = 0.0025
I0521 00:24:28.092564  9137 solver.cpp:237] Iteration 2080, loss = 1.6725
I0521 00:24:28.092597  9137 solver.cpp:253]     Train net output #0: loss = 1.6725 (* 1 = 1.6725 loss)
I0521 00:24:28.092614  9137 sgd_solver.cpp:106] Iteration 2080, lr = 0.0025
I0521 00:24:36.028421  9137 solver.cpp:237] Iteration 2112, loss = 1.6907
I0521 00:24:36.028453  9137 solver.cpp:253]     Train net output #0: loss = 1.6907 (* 1 = 1.6907 loss)
I0521 00:24:36.028470  9137 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 00:24:43.960254  9137 solver.cpp:237] Iteration 2144, loss = 1.64465
I0521 00:24:43.960408  9137 solver.cpp:253]     Train net output #0: loss = 1.64465 (* 1 = 1.64465 loss)
I0521 00:24:43.960422  9137 sgd_solver.cpp:106] Iteration 2144, lr = 0.0025
I0521 00:25:14.021106  9137 solver.cpp:237] Iteration 2176, loss = 1.60247
I0521 00:25:14.021268  9137 solver.cpp:253]     Train net output #0: loss = 1.60247 (* 1 = 1.60247 loss)
I0521 00:25:14.021284  9137 sgd_solver.cpp:106] Iteration 2176, lr = 0.0025
I0521 00:25:21.956605  9137 solver.cpp:237] Iteration 2208, loss = 1.61113
I0521 00:25:21.956639  9137 solver.cpp:253]     Train net output #0: loss = 1.61113 (* 1 = 1.61113 loss)
I0521 00:25:21.956655  9137 sgd_solver.cpp:106] Iteration 2208, lr = 0.0025
I0521 00:25:29.891211  9137 solver.cpp:237] Iteration 2240, loss = 1.62489
I0521 00:25:29.891244  9137 solver.cpp:253]     Train net output #0: loss = 1.62489 (* 1 = 1.62489 loss)
I0521 00:25:29.891261  9137 sgd_solver.cpp:106] Iteration 2240, lr = 0.0025
I0521 00:25:37.827754  9137 solver.cpp:237] Iteration 2272, loss = 1.62872
I0521 00:25:37.827797  9137 solver.cpp:253]     Train net output #0: loss = 1.62872 (* 1 = 1.62872 loss)
I0521 00:25:37.827816  9137 sgd_solver.cpp:106] Iteration 2272, lr = 0.0025
I0521 00:25:40.060719  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_2282.caffemodel
I0521 00:25:40.271795  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_2282.solverstate
I0521 00:25:45.823004  9137 solver.cpp:237] Iteration 2304, loss = 1.59124
I0521 00:25:45.823171  9137 solver.cpp:253]     Train net output #0: loss = 1.59124 (* 1 = 1.59124 loss)
I0521 00:25:45.823185  9137 sgd_solver.cpp:106] Iteration 2304, lr = 0.0025
I0521 00:25:53.756652  9137 solver.cpp:237] Iteration 2336, loss = 1.63066
I0521 00:25:53.756685  9137 solver.cpp:253]     Train net output #0: loss = 1.63066 (* 1 = 1.63066 loss)
I0521 00:25:53.756702  9137 sgd_solver.cpp:106] Iteration 2336, lr = 0.0025
I0521 00:26:01.693388  9137 solver.cpp:237] Iteration 2368, loss = 1.62934
I0521 00:26:01.693431  9137 solver.cpp:253]     Train net output #0: loss = 1.62934 (* 1 = 1.62934 loss)
I0521 00:26:01.693449  9137 sgd_solver.cpp:106] Iteration 2368, lr = 0.0025
I0521 00:26:31.803340  9137 solver.cpp:237] Iteration 2400, loss = 1.56627
I0521 00:26:31.803529  9137 solver.cpp:253]     Train net output #0: loss = 1.56627 (* 1 = 1.56627 loss)
I0521 00:26:31.803544  9137 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0521 00:26:39.733711  9137 solver.cpp:237] Iteration 2432, loss = 1.5647
I0521 00:26:39.733743  9137 solver.cpp:253]     Train net output #0: loss = 1.5647 (* 1 = 1.5647 loss)
I0521 00:26:39.733775  9137 sgd_solver.cpp:106] Iteration 2432, lr = 0.0025
I0521 00:26:47.669354  9137 solver.cpp:237] Iteration 2464, loss = 1.64518
I0521 00:26:47.669404  9137 solver.cpp:253]     Train net output #0: loss = 1.64518 (* 1 = 1.64518 loss)
I0521 00:26:47.669420  9137 sgd_solver.cpp:106] Iteration 2464, lr = 0.0025
I0521 00:26:55.602620  9137 solver.cpp:237] Iteration 2496, loss = 1.55381
I0521 00:26:55.602654  9137 solver.cpp:253]     Train net output #0: loss = 1.55381 (* 1 = 1.55381 loss)
I0521 00:26:55.602670  9137 sgd_solver.cpp:106] Iteration 2496, lr = 0.0025
I0521 00:27:03.536551  9137 solver.cpp:237] Iteration 2528, loss = 1.69513
I0521 00:27:03.536695  9137 solver.cpp:253]     Train net output #0: loss = 1.69513 (* 1 = 1.69513 loss)
I0521 00:27:03.536707  9137 sgd_solver.cpp:106] Iteration 2528, lr = 0.0025
I0521 00:27:11.470276  9137 solver.cpp:237] Iteration 2560, loss = 1.63678
I0521 00:27:11.470309  9137 solver.cpp:253]     Train net output #0: loss = 1.63678 (* 1 = 1.63678 loss)
I0521 00:27:11.470326  9137 sgd_solver.cpp:106] Iteration 2560, lr = 0.0025
I0521 00:27:19.410593  9137 solver.cpp:237] Iteration 2592, loss = 1.51462
I0521 00:27:19.410640  9137 solver.cpp:253]     Train net output #0: loss = 1.51462 (* 1 = 1.51462 loss)
I0521 00:27:19.410653  9137 sgd_solver.cpp:106] Iteration 2592, lr = 0.0025
I0521 00:27:23.131234  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_2608.caffemodel
I0521 00:27:23.347580  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_2608.solverstate
I0521 00:27:23.373786  9137 solver.cpp:341] Iteration 2608, Testing net (#0)
I0521 00:28:29.462126  9137 solver.cpp:409]     Test net output #0: accuracy = 0.695759
I0521 00:28:29.462296  9137 solver.cpp:409]     Test net output #1: loss = 1.0365 (* 1 = 1.0365 loss)
I0521 00:28:55.678097  9137 solver.cpp:237] Iteration 2624, loss = 1.6171
I0521 00:28:55.678144  9137 solver.cpp:253]     Train net output #0: loss = 1.6171 (* 1 = 1.6171 loss)
I0521 00:28:55.678164  9137 sgd_solver.cpp:106] Iteration 2624, lr = 0.0025
I0521 00:29:03.613509  9137 solver.cpp:237] Iteration 2656, loss = 1.59814
I0521 00:29:03.613658  9137 solver.cpp:253]     Train net output #0: loss = 1.59814 (* 1 = 1.59814 loss)
I0521 00:29:03.613673  9137 sgd_solver.cpp:106] Iteration 2656, lr = 0.0025
I0521 00:29:11.547850  9137 solver.cpp:237] Iteration 2688, loss = 1.60898
I0521 00:29:11.547883  9137 solver.cpp:253]     Train net output #0: loss = 1.60898 (* 1 = 1.60898 loss)
I0521 00:29:11.547900  9137 sgd_solver.cpp:106] Iteration 2688, lr = 0.0025
I0521 00:29:19.482848  9137 solver.cpp:237] Iteration 2720, loss = 1.68092
I0521 00:29:19.482890  9137 solver.cpp:253]     Train net output #0: loss = 1.68092 (* 1 = 1.68092 loss)
I0521 00:29:19.482911  9137 sgd_solver.cpp:106] Iteration 2720, lr = 0.0025
I0521 00:29:27.423899  9137 solver.cpp:237] Iteration 2752, loss = 1.56108
I0521 00:29:27.423934  9137 solver.cpp:253]     Train net output #0: loss = 1.56108 (* 1 = 1.56108 loss)
I0521 00:29:27.423951  9137 sgd_solver.cpp:106] Iteration 2752, lr = 0.0025
I0521 00:29:35.364260  9137 solver.cpp:237] Iteration 2784, loss = 1.68385
I0521 00:29:35.364403  9137 solver.cpp:253]     Train net output #0: loss = 1.68385 (* 1 = 1.68385 loss)
I0521 00:29:35.364416  9137 sgd_solver.cpp:106] Iteration 2784, lr = 0.0025
I0521 00:29:43.299075  9137 solver.cpp:237] Iteration 2816, loss = 1.55704
I0521 00:29:43.299124  9137 solver.cpp:253]     Train net output #0: loss = 1.55704 (* 1 = 1.55704 loss)
I0521 00:29:43.299137  9137 sgd_solver.cpp:106] Iteration 2816, lr = 0.0025
I0521 00:30:13.426610  9137 solver.cpp:237] Iteration 2848, loss = 1.65487
I0521 00:30:13.426777  9137 solver.cpp:253]     Train net output #0: loss = 1.65487 (* 1 = 1.65487 loss)
I0521 00:30:13.426794  9137 sgd_solver.cpp:106] Iteration 2848, lr = 0.0025
I0521 00:30:21.363910  9137 solver.cpp:237] Iteration 2880, loss = 1.54443
I0521 00:30:21.363942  9137 solver.cpp:253]     Train net output #0: loss = 1.54443 (* 1 = 1.54443 loss)
I0521 00:30:21.363957  9137 sgd_solver.cpp:106] Iteration 2880, lr = 0.0025
I0521 00:30:29.298954  9137 solver.cpp:237] Iteration 2912, loss = 1.57684
I0521 00:30:29.298987  9137 solver.cpp:253]     Train net output #0: loss = 1.57684 (* 1 = 1.57684 loss)
I0521 00:30:29.299000  9137 sgd_solver.cpp:106] Iteration 2912, lr = 0.0025
I0521 00:30:34.510017  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_2934.caffemodel
I0521 00:30:34.722805  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_2934.solverstate
I0521 00:30:37.304008  9137 solver.cpp:237] Iteration 2944, loss = 1.68814
I0521 00:30:37.304056  9137 solver.cpp:253]     Train net output #0: loss = 1.68814 (* 1 = 1.68814 loss)
I0521 00:30:37.304072  9137 sgd_solver.cpp:106] Iteration 2944, lr = 0.0025
I0521 00:30:45.241508  9137 solver.cpp:237] Iteration 2976, loss = 1.66333
I0521 00:30:45.241664  9137 solver.cpp:253]     Train net output #0: loss = 1.66333 (* 1 = 1.66333 loss)
I0521 00:30:45.241677  9137 sgd_solver.cpp:106] Iteration 2976, lr = 0.0025
I0521 00:30:53.171844  9137 solver.cpp:237] Iteration 3008, loss = 1.5561
I0521 00:30:53.171875  9137 solver.cpp:253]     Train net output #0: loss = 1.5561 (* 1 = 1.5561 loss)
I0521 00:30:53.171892  9137 sgd_solver.cpp:106] Iteration 3008, lr = 0.0025
I0521 00:31:01.109390  9137 solver.cpp:237] Iteration 3040, loss = 1.55861
I0521 00:31:01.109421  9137 solver.cpp:253]     Train net output #0: loss = 1.55861 (* 1 = 1.55861 loss)
I0521 00:31:01.109439  9137 sgd_solver.cpp:106] Iteration 3040, lr = 0.0025
I0521 00:31:31.183580  9137 solver.cpp:237] Iteration 3072, loss = 1.52909
I0521 00:31:31.183754  9137 solver.cpp:253]     Train net output #0: loss = 1.52909 (* 1 = 1.52909 loss)
I0521 00:31:31.183770  9137 sgd_solver.cpp:106] Iteration 3072, lr = 0.0025
I0521 00:31:39.123410  9137 solver.cpp:237] Iteration 3104, loss = 1.71334
I0521 00:31:39.123443  9137 solver.cpp:253]     Train net output #0: loss = 1.71334 (* 1 = 1.71334 loss)
I0521 00:31:39.123461  9137 sgd_solver.cpp:106] Iteration 3104, lr = 0.0025
I0521 00:31:47.060468  9137 solver.cpp:237] Iteration 3136, loss = 1.57543
I0521 00:31:47.060502  9137 solver.cpp:253]     Train net output #0: loss = 1.57543 (* 1 = 1.57543 loss)
I0521 00:31:47.060514  9137 sgd_solver.cpp:106] Iteration 3136, lr = 0.0025
I0521 00:31:54.996775  9137 solver.cpp:237] Iteration 3168, loss = 1.5283
I0521 00:31:54.996809  9137 solver.cpp:253]     Train net output #0: loss = 1.5283 (* 1 = 1.5283 loss)
I0521 00:31:54.996827  9137 sgd_solver.cpp:106] Iteration 3168, lr = 0.0025
I0521 00:32:02.929620  9137 solver.cpp:237] Iteration 3200, loss = 1.63041
I0521 00:32:02.929761  9137 solver.cpp:253]     Train net output #0: loss = 1.63041 (* 1 = 1.63041 loss)
I0521 00:32:02.929774  9137 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0521 00:32:10.864168  9137 solver.cpp:237] Iteration 3232, loss = 1.53826
I0521 00:32:10.864199  9137 solver.cpp:253]     Train net output #0: loss = 1.53826 (* 1 = 1.53826 loss)
I0521 00:32:10.864217  9137 sgd_solver.cpp:106] Iteration 3232, lr = 0.0025
I0521 00:32:17.564164  9137 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_3260.caffemodel
I0521 00:32:18.185819  9137 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925_iter_3260.solverstate
I0521 00:32:18.215107  9137 solver.cpp:341] Iteration 3260, Testing net (#0)
I0521 00:33:03.408386  9137 solver.cpp:409]     Test net output #0: accuracy = 0.683129
I0521 00:33:03.408551  9137 solver.cpp:409]     Test net output #1: loss = 1.28738 (* 1 = 1.28738 loss)
I0521 00:33:03.408566  9137 solver.cpp:326] Optimization Done.
I0521 00:33:03.408574  9137 caffe.cpp:215] Optimization Done.
Application 11236179 resources: utime ~1252s, stime ~227s, Rss ~5332988, inblocks ~3594475, outblocks ~179818
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_460_2016-05-20T11.20.49.329925.solver"
	User time (seconds): 0.55
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:42.71
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15082
	Voluntary context switches: 2720
	Involuntary context switches: 123
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
