2806109
I0521 00:27:22.370214  9372 caffe.cpp:184] Using GPUs 0
I0521 00:27:22.792779  9372 solver.cpp:48] Initializing solver from parameters: 
test_iter: 319
test_interval: 638
base_lr: 0.0025
display: 31
max_iter: 3191
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 319
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464.prototxt"
I0521 00:27:22.794289  9372 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464.prototxt
I0521 00:27:22.812172  9372 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 00:27:22.812233  9372 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 00:27:22.812587  9372 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 470
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:27:22.812762  9372 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:27:22.812785  9372 net.cpp:106] Creating Layer data_hdf5
I0521 00:27:22.812800  9372 net.cpp:411] data_hdf5 -> data
I0521 00:27:22.812834  9372 net.cpp:411] data_hdf5 -> label
I0521 00:27:22.812865  9372 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 00:27:22.814056  9372 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 00:27:22.816252  9372 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 00:27:44.322484  9372 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 00:27:44.327587  9372 net.cpp:150] Setting up data_hdf5
I0521 00:27:44.327630  9372 net.cpp:157] Top shape: 470 1 127 50 (2984500)
I0521 00:27:44.327644  9372 net.cpp:157] Top shape: 470 (470)
I0521 00:27:44.327656  9372 net.cpp:165] Memory required for data: 11939880
I0521 00:27:44.327668  9372 layer_factory.hpp:77] Creating layer conv1
I0521 00:27:44.327702  9372 net.cpp:106] Creating Layer conv1
I0521 00:27:44.327714  9372 net.cpp:454] conv1 <- data
I0521 00:27:44.327738  9372 net.cpp:411] conv1 -> conv1
I0521 00:27:44.697335  9372 net.cpp:150] Setting up conv1
I0521 00:27:44.697381  9372 net.cpp:157] Top shape: 470 12 120 48 (32486400)
I0521 00:27:44.697392  9372 net.cpp:165] Memory required for data: 141885480
I0521 00:27:44.697422  9372 layer_factory.hpp:77] Creating layer relu1
I0521 00:27:44.697443  9372 net.cpp:106] Creating Layer relu1
I0521 00:27:44.697454  9372 net.cpp:454] relu1 <- conv1
I0521 00:27:44.697468  9372 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:27:44.697991  9372 net.cpp:150] Setting up relu1
I0521 00:27:44.698009  9372 net.cpp:157] Top shape: 470 12 120 48 (32486400)
I0521 00:27:44.698019  9372 net.cpp:165] Memory required for data: 271831080
I0521 00:27:44.698029  9372 layer_factory.hpp:77] Creating layer pool1
I0521 00:27:44.698045  9372 net.cpp:106] Creating Layer pool1
I0521 00:27:44.698056  9372 net.cpp:454] pool1 <- conv1
I0521 00:27:44.698070  9372 net.cpp:411] pool1 -> pool1
I0521 00:27:44.698149  9372 net.cpp:150] Setting up pool1
I0521 00:27:44.698163  9372 net.cpp:157] Top shape: 470 12 60 48 (16243200)
I0521 00:27:44.698173  9372 net.cpp:165] Memory required for data: 336803880
I0521 00:27:44.698182  9372 layer_factory.hpp:77] Creating layer conv2
I0521 00:27:44.698204  9372 net.cpp:106] Creating Layer conv2
I0521 00:27:44.698215  9372 net.cpp:454] conv2 <- pool1
I0521 00:27:44.698228  9372 net.cpp:411] conv2 -> conv2
I0521 00:27:44.700920  9372 net.cpp:150] Setting up conv2
I0521 00:27:44.700948  9372 net.cpp:157] Top shape: 470 20 54 46 (23349600)
I0521 00:27:44.700958  9372 net.cpp:165] Memory required for data: 430202280
I0521 00:27:44.700978  9372 layer_factory.hpp:77] Creating layer relu2
I0521 00:27:44.700991  9372 net.cpp:106] Creating Layer relu2
I0521 00:27:44.701001  9372 net.cpp:454] relu2 <- conv2
I0521 00:27:44.701014  9372 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:27:44.701346  9372 net.cpp:150] Setting up relu2
I0521 00:27:44.701360  9372 net.cpp:157] Top shape: 470 20 54 46 (23349600)
I0521 00:27:44.701370  9372 net.cpp:165] Memory required for data: 523600680
I0521 00:27:44.701380  9372 layer_factory.hpp:77] Creating layer pool2
I0521 00:27:44.701393  9372 net.cpp:106] Creating Layer pool2
I0521 00:27:44.701403  9372 net.cpp:454] pool2 <- conv2
I0521 00:27:44.701427  9372 net.cpp:411] pool2 -> pool2
I0521 00:27:44.701496  9372 net.cpp:150] Setting up pool2
I0521 00:27:44.701509  9372 net.cpp:157] Top shape: 470 20 27 46 (11674800)
I0521 00:27:44.701519  9372 net.cpp:165] Memory required for data: 570299880
I0521 00:27:44.701529  9372 layer_factory.hpp:77] Creating layer conv3
I0521 00:27:44.701548  9372 net.cpp:106] Creating Layer conv3
I0521 00:27:44.701558  9372 net.cpp:454] conv3 <- pool2
I0521 00:27:44.701571  9372 net.cpp:411] conv3 -> conv3
I0521 00:27:44.703495  9372 net.cpp:150] Setting up conv3
I0521 00:27:44.703518  9372 net.cpp:157] Top shape: 470 28 22 44 (12738880)
I0521 00:27:44.703531  9372 net.cpp:165] Memory required for data: 621255400
I0521 00:27:44.703548  9372 layer_factory.hpp:77] Creating layer relu3
I0521 00:27:44.703564  9372 net.cpp:106] Creating Layer relu3
I0521 00:27:44.703575  9372 net.cpp:454] relu3 <- conv3
I0521 00:27:44.703588  9372 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:27:44.704058  9372 net.cpp:150] Setting up relu3
I0521 00:27:44.704077  9372 net.cpp:157] Top shape: 470 28 22 44 (12738880)
I0521 00:27:44.704087  9372 net.cpp:165] Memory required for data: 672210920
I0521 00:27:44.704097  9372 layer_factory.hpp:77] Creating layer pool3
I0521 00:27:44.704110  9372 net.cpp:106] Creating Layer pool3
I0521 00:27:44.704119  9372 net.cpp:454] pool3 <- conv3
I0521 00:27:44.704133  9372 net.cpp:411] pool3 -> pool3
I0521 00:27:44.704200  9372 net.cpp:150] Setting up pool3
I0521 00:27:44.704213  9372 net.cpp:157] Top shape: 470 28 11 44 (6369440)
I0521 00:27:44.704223  9372 net.cpp:165] Memory required for data: 697688680
I0521 00:27:44.704232  9372 layer_factory.hpp:77] Creating layer conv4
I0521 00:27:44.704247  9372 net.cpp:106] Creating Layer conv4
I0521 00:27:44.704259  9372 net.cpp:454] conv4 <- pool3
I0521 00:27:44.704272  9372 net.cpp:411] conv4 -> conv4
I0521 00:27:44.707039  9372 net.cpp:150] Setting up conv4
I0521 00:27:44.707067  9372 net.cpp:157] Top shape: 470 36 6 42 (4263840)
I0521 00:27:44.707077  9372 net.cpp:165] Memory required for data: 714744040
I0521 00:27:44.707093  9372 layer_factory.hpp:77] Creating layer relu4
I0521 00:27:44.707108  9372 net.cpp:106] Creating Layer relu4
I0521 00:27:44.707118  9372 net.cpp:454] relu4 <- conv4
I0521 00:27:44.707131  9372 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:27:44.707607  9372 net.cpp:150] Setting up relu4
I0521 00:27:44.707623  9372 net.cpp:157] Top shape: 470 36 6 42 (4263840)
I0521 00:27:44.707634  9372 net.cpp:165] Memory required for data: 731799400
I0521 00:27:44.707644  9372 layer_factory.hpp:77] Creating layer pool4
I0521 00:27:44.707658  9372 net.cpp:106] Creating Layer pool4
I0521 00:27:44.707667  9372 net.cpp:454] pool4 <- conv4
I0521 00:27:44.707680  9372 net.cpp:411] pool4 -> pool4
I0521 00:27:44.707748  9372 net.cpp:150] Setting up pool4
I0521 00:27:44.707762  9372 net.cpp:157] Top shape: 470 36 3 42 (2131920)
I0521 00:27:44.707772  9372 net.cpp:165] Memory required for data: 740327080
I0521 00:27:44.707782  9372 layer_factory.hpp:77] Creating layer ip1
I0521 00:27:44.707803  9372 net.cpp:106] Creating Layer ip1
I0521 00:27:44.707813  9372 net.cpp:454] ip1 <- pool4
I0521 00:27:44.707826  9372 net.cpp:411] ip1 -> ip1
I0521 00:27:44.723359  9372 net.cpp:150] Setting up ip1
I0521 00:27:44.723388  9372 net.cpp:157] Top shape: 470 196 (92120)
I0521 00:27:44.723404  9372 net.cpp:165] Memory required for data: 740695560
I0521 00:27:44.723430  9372 layer_factory.hpp:77] Creating layer relu5
I0521 00:27:44.723445  9372 net.cpp:106] Creating Layer relu5
I0521 00:27:44.723455  9372 net.cpp:454] relu5 <- ip1
I0521 00:27:44.723469  9372 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:27:44.723812  9372 net.cpp:150] Setting up relu5
I0521 00:27:44.723827  9372 net.cpp:157] Top shape: 470 196 (92120)
I0521 00:27:44.723837  9372 net.cpp:165] Memory required for data: 741064040
I0521 00:27:44.723847  9372 layer_factory.hpp:77] Creating layer drop1
I0521 00:27:44.723870  9372 net.cpp:106] Creating Layer drop1
I0521 00:27:44.723881  9372 net.cpp:454] drop1 <- ip1
I0521 00:27:44.723906  9372 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:27:44.723953  9372 net.cpp:150] Setting up drop1
I0521 00:27:44.723965  9372 net.cpp:157] Top shape: 470 196 (92120)
I0521 00:27:44.723976  9372 net.cpp:165] Memory required for data: 741432520
I0521 00:27:44.723984  9372 layer_factory.hpp:77] Creating layer ip2
I0521 00:27:44.724004  9372 net.cpp:106] Creating Layer ip2
I0521 00:27:44.724014  9372 net.cpp:454] ip2 <- ip1
I0521 00:27:44.724028  9372 net.cpp:411] ip2 -> ip2
I0521 00:27:44.724504  9372 net.cpp:150] Setting up ip2
I0521 00:27:44.724517  9372 net.cpp:157] Top shape: 470 98 (46060)
I0521 00:27:44.724526  9372 net.cpp:165] Memory required for data: 741616760
I0521 00:27:44.724542  9372 layer_factory.hpp:77] Creating layer relu6
I0521 00:27:44.724555  9372 net.cpp:106] Creating Layer relu6
I0521 00:27:44.724565  9372 net.cpp:454] relu6 <- ip2
I0521 00:27:44.724577  9372 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:27:44.725098  9372 net.cpp:150] Setting up relu6
I0521 00:27:44.725114  9372 net.cpp:157] Top shape: 470 98 (46060)
I0521 00:27:44.725126  9372 net.cpp:165] Memory required for data: 741801000
I0521 00:27:44.725134  9372 layer_factory.hpp:77] Creating layer drop2
I0521 00:27:44.725148  9372 net.cpp:106] Creating Layer drop2
I0521 00:27:44.725157  9372 net.cpp:454] drop2 <- ip2
I0521 00:27:44.725170  9372 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:27:44.725213  9372 net.cpp:150] Setting up drop2
I0521 00:27:44.725226  9372 net.cpp:157] Top shape: 470 98 (46060)
I0521 00:27:44.725236  9372 net.cpp:165] Memory required for data: 741985240
I0521 00:27:44.725246  9372 layer_factory.hpp:77] Creating layer ip3
I0521 00:27:44.725260  9372 net.cpp:106] Creating Layer ip3
I0521 00:27:44.725270  9372 net.cpp:454] ip3 <- ip2
I0521 00:27:44.725281  9372 net.cpp:411] ip3 -> ip3
I0521 00:27:44.725493  9372 net.cpp:150] Setting up ip3
I0521 00:27:44.725507  9372 net.cpp:157] Top shape: 470 11 (5170)
I0521 00:27:44.725517  9372 net.cpp:165] Memory required for data: 742005920
I0521 00:27:44.725533  9372 layer_factory.hpp:77] Creating layer drop3
I0521 00:27:44.725544  9372 net.cpp:106] Creating Layer drop3
I0521 00:27:44.725554  9372 net.cpp:454] drop3 <- ip3
I0521 00:27:44.725566  9372 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:27:44.725605  9372 net.cpp:150] Setting up drop3
I0521 00:27:44.725618  9372 net.cpp:157] Top shape: 470 11 (5170)
I0521 00:27:44.725628  9372 net.cpp:165] Memory required for data: 742026600
I0521 00:27:44.725638  9372 layer_factory.hpp:77] Creating layer loss
I0521 00:27:44.725657  9372 net.cpp:106] Creating Layer loss
I0521 00:27:44.725667  9372 net.cpp:454] loss <- ip3
I0521 00:27:44.725677  9372 net.cpp:454] loss <- label
I0521 00:27:44.725689  9372 net.cpp:411] loss -> loss
I0521 00:27:44.725708  9372 layer_factory.hpp:77] Creating layer loss
I0521 00:27:44.726361  9372 net.cpp:150] Setting up loss
I0521 00:27:44.726377  9372 net.cpp:157] Top shape: (1)
I0521 00:27:44.726387  9372 net.cpp:160]     with loss weight 1
I0521 00:27:44.726429  9372 net.cpp:165] Memory required for data: 742026604
I0521 00:27:44.726439  9372 net.cpp:226] loss needs backward computation.
I0521 00:27:44.726449  9372 net.cpp:226] drop3 needs backward computation.
I0521 00:27:44.726460  9372 net.cpp:226] ip3 needs backward computation.
I0521 00:27:44.726471  9372 net.cpp:226] drop2 needs backward computation.
I0521 00:27:44.726480  9372 net.cpp:226] relu6 needs backward computation.
I0521 00:27:44.726490  9372 net.cpp:226] ip2 needs backward computation.
I0521 00:27:44.726501  9372 net.cpp:226] drop1 needs backward computation.
I0521 00:27:44.726511  9372 net.cpp:226] relu5 needs backward computation.
I0521 00:27:44.726521  9372 net.cpp:226] ip1 needs backward computation.
I0521 00:27:44.726531  9372 net.cpp:226] pool4 needs backward computation.
I0521 00:27:44.726541  9372 net.cpp:226] relu4 needs backward computation.
I0521 00:27:44.726550  9372 net.cpp:226] conv4 needs backward computation.
I0521 00:27:44.726562  9372 net.cpp:226] pool3 needs backward computation.
I0521 00:27:44.726580  9372 net.cpp:226] relu3 needs backward computation.
I0521 00:27:44.726590  9372 net.cpp:226] conv3 needs backward computation.
I0521 00:27:44.726601  9372 net.cpp:226] pool2 needs backward computation.
I0521 00:27:44.726613  9372 net.cpp:226] relu2 needs backward computation.
I0521 00:27:44.726622  9372 net.cpp:226] conv2 needs backward computation.
I0521 00:27:44.726632  9372 net.cpp:226] pool1 needs backward computation.
I0521 00:27:44.726642  9372 net.cpp:226] relu1 needs backward computation.
I0521 00:27:44.726652  9372 net.cpp:226] conv1 needs backward computation.
I0521 00:27:44.726663  9372 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:27:44.726673  9372 net.cpp:270] This network produces output loss
I0521 00:27:44.726697  9372 net.cpp:283] Network initialization done.
I0521 00:27:44.728287  9372 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464.prototxt
I0521 00:27:44.728358  9372 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 00:27:44.728726  9372 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 470
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 00:27:44.728914  9372 layer_factory.hpp:77] Creating layer data_hdf5
I0521 00:27:44.728929  9372 net.cpp:106] Creating Layer data_hdf5
I0521 00:27:44.728941  9372 net.cpp:411] data_hdf5 -> data
I0521 00:27:44.728957  9372 net.cpp:411] data_hdf5 -> label
I0521 00:27:44.728972  9372 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 00:27:44.730222  9372 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 00:28:06.018491  9372 net.cpp:150] Setting up data_hdf5
I0521 00:28:06.018656  9372 net.cpp:157] Top shape: 470 1 127 50 (2984500)
I0521 00:28:06.018671  9372 net.cpp:157] Top shape: 470 (470)
I0521 00:28:06.018683  9372 net.cpp:165] Memory required for data: 11939880
I0521 00:28:06.018697  9372 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 00:28:06.018726  9372 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 00:28:06.018736  9372 net.cpp:454] label_data_hdf5_1_split <- label
I0521 00:28:06.018751  9372 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 00:28:06.018774  9372 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 00:28:06.018847  9372 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 00:28:06.018860  9372 net.cpp:157] Top shape: 470 (470)
I0521 00:28:06.018872  9372 net.cpp:157] Top shape: 470 (470)
I0521 00:28:06.018882  9372 net.cpp:165] Memory required for data: 11943640
I0521 00:28:06.018892  9372 layer_factory.hpp:77] Creating layer conv1
I0521 00:28:06.018911  9372 net.cpp:106] Creating Layer conv1
I0521 00:28:06.018923  9372 net.cpp:454] conv1 <- data
I0521 00:28:06.018937  9372 net.cpp:411] conv1 -> conv1
I0521 00:28:06.020877  9372 net.cpp:150] Setting up conv1
I0521 00:28:06.020901  9372 net.cpp:157] Top shape: 470 12 120 48 (32486400)
I0521 00:28:06.020913  9372 net.cpp:165] Memory required for data: 141889240
I0521 00:28:06.020934  9372 layer_factory.hpp:77] Creating layer relu1
I0521 00:28:06.020951  9372 net.cpp:106] Creating Layer relu1
I0521 00:28:06.020961  9372 net.cpp:454] relu1 <- conv1
I0521 00:28:06.020973  9372 net.cpp:397] relu1 -> conv1 (in-place)
I0521 00:28:06.021476  9372 net.cpp:150] Setting up relu1
I0521 00:28:06.021492  9372 net.cpp:157] Top shape: 470 12 120 48 (32486400)
I0521 00:28:06.021502  9372 net.cpp:165] Memory required for data: 271834840
I0521 00:28:06.021513  9372 layer_factory.hpp:77] Creating layer pool1
I0521 00:28:06.021529  9372 net.cpp:106] Creating Layer pool1
I0521 00:28:06.021539  9372 net.cpp:454] pool1 <- conv1
I0521 00:28:06.021553  9372 net.cpp:411] pool1 -> pool1
I0521 00:28:06.021626  9372 net.cpp:150] Setting up pool1
I0521 00:28:06.021641  9372 net.cpp:157] Top shape: 470 12 60 48 (16243200)
I0521 00:28:06.021651  9372 net.cpp:165] Memory required for data: 336807640
I0521 00:28:06.021661  9372 layer_factory.hpp:77] Creating layer conv2
I0521 00:28:06.021677  9372 net.cpp:106] Creating Layer conv2
I0521 00:28:06.021688  9372 net.cpp:454] conv2 <- pool1
I0521 00:28:06.021703  9372 net.cpp:411] conv2 -> conv2
I0521 00:28:06.023617  9372 net.cpp:150] Setting up conv2
I0521 00:28:06.023639  9372 net.cpp:157] Top shape: 470 20 54 46 (23349600)
I0521 00:28:06.023653  9372 net.cpp:165] Memory required for data: 430206040
I0521 00:28:06.023669  9372 layer_factory.hpp:77] Creating layer relu2
I0521 00:28:06.023684  9372 net.cpp:106] Creating Layer relu2
I0521 00:28:06.023694  9372 net.cpp:454] relu2 <- conv2
I0521 00:28:06.023705  9372 net.cpp:397] relu2 -> conv2 (in-place)
I0521 00:28:06.024039  9372 net.cpp:150] Setting up relu2
I0521 00:28:06.024054  9372 net.cpp:157] Top shape: 470 20 54 46 (23349600)
I0521 00:28:06.024063  9372 net.cpp:165] Memory required for data: 523604440
I0521 00:28:06.024073  9372 layer_factory.hpp:77] Creating layer pool2
I0521 00:28:06.024086  9372 net.cpp:106] Creating Layer pool2
I0521 00:28:06.024096  9372 net.cpp:454] pool2 <- conv2
I0521 00:28:06.024109  9372 net.cpp:411] pool2 -> pool2
I0521 00:28:06.024180  9372 net.cpp:150] Setting up pool2
I0521 00:28:06.024194  9372 net.cpp:157] Top shape: 470 20 27 46 (11674800)
I0521 00:28:06.024204  9372 net.cpp:165] Memory required for data: 570303640
I0521 00:28:06.024214  9372 layer_factory.hpp:77] Creating layer conv3
I0521 00:28:06.024235  9372 net.cpp:106] Creating Layer conv3
I0521 00:28:06.024245  9372 net.cpp:454] conv3 <- pool2
I0521 00:28:06.024260  9372 net.cpp:411] conv3 -> conv3
I0521 00:28:06.026238  9372 net.cpp:150] Setting up conv3
I0521 00:28:06.026262  9372 net.cpp:157] Top shape: 470 28 22 44 (12738880)
I0521 00:28:06.026273  9372 net.cpp:165] Memory required for data: 621259160
I0521 00:28:06.026304  9372 layer_factory.hpp:77] Creating layer relu3
I0521 00:28:06.026317  9372 net.cpp:106] Creating Layer relu3
I0521 00:28:06.026329  9372 net.cpp:454] relu3 <- conv3
I0521 00:28:06.026341  9372 net.cpp:397] relu3 -> conv3 (in-place)
I0521 00:28:06.026813  9372 net.cpp:150] Setting up relu3
I0521 00:28:06.026829  9372 net.cpp:157] Top shape: 470 28 22 44 (12738880)
I0521 00:28:06.026839  9372 net.cpp:165] Memory required for data: 672214680
I0521 00:28:06.026850  9372 layer_factory.hpp:77] Creating layer pool3
I0521 00:28:06.026864  9372 net.cpp:106] Creating Layer pool3
I0521 00:28:06.026873  9372 net.cpp:454] pool3 <- conv3
I0521 00:28:06.026886  9372 net.cpp:411] pool3 -> pool3
I0521 00:28:06.026957  9372 net.cpp:150] Setting up pool3
I0521 00:28:06.026971  9372 net.cpp:157] Top shape: 470 28 11 44 (6369440)
I0521 00:28:06.026981  9372 net.cpp:165] Memory required for data: 697692440
I0521 00:28:06.026989  9372 layer_factory.hpp:77] Creating layer conv4
I0521 00:28:06.027007  9372 net.cpp:106] Creating Layer conv4
I0521 00:28:06.027017  9372 net.cpp:454] conv4 <- pool3
I0521 00:28:06.027031  9372 net.cpp:411] conv4 -> conv4
I0521 00:28:06.029091  9372 net.cpp:150] Setting up conv4
I0521 00:28:06.029114  9372 net.cpp:157] Top shape: 470 36 6 42 (4263840)
I0521 00:28:06.029127  9372 net.cpp:165] Memory required for data: 714747800
I0521 00:28:06.029142  9372 layer_factory.hpp:77] Creating layer relu4
I0521 00:28:06.029155  9372 net.cpp:106] Creating Layer relu4
I0521 00:28:06.029165  9372 net.cpp:454] relu4 <- conv4
I0521 00:28:06.029180  9372 net.cpp:397] relu4 -> conv4 (in-place)
I0521 00:28:06.029654  9372 net.cpp:150] Setting up relu4
I0521 00:28:06.029670  9372 net.cpp:157] Top shape: 470 36 6 42 (4263840)
I0521 00:28:06.029680  9372 net.cpp:165] Memory required for data: 731803160
I0521 00:28:06.029690  9372 layer_factory.hpp:77] Creating layer pool4
I0521 00:28:06.029703  9372 net.cpp:106] Creating Layer pool4
I0521 00:28:06.029713  9372 net.cpp:454] pool4 <- conv4
I0521 00:28:06.029726  9372 net.cpp:411] pool4 -> pool4
I0521 00:28:06.029798  9372 net.cpp:150] Setting up pool4
I0521 00:28:06.029811  9372 net.cpp:157] Top shape: 470 36 3 42 (2131920)
I0521 00:28:06.029820  9372 net.cpp:165] Memory required for data: 740330840
I0521 00:28:06.029829  9372 layer_factory.hpp:77] Creating layer ip1
I0521 00:28:06.029844  9372 net.cpp:106] Creating Layer ip1
I0521 00:28:06.029855  9372 net.cpp:454] ip1 <- pool4
I0521 00:28:06.029868  9372 net.cpp:411] ip1 -> ip1
I0521 00:28:06.045336  9372 net.cpp:150] Setting up ip1
I0521 00:28:06.045367  9372 net.cpp:157] Top shape: 470 196 (92120)
I0521 00:28:06.045382  9372 net.cpp:165] Memory required for data: 740699320
I0521 00:28:06.045403  9372 layer_factory.hpp:77] Creating layer relu5
I0521 00:28:06.045419  9372 net.cpp:106] Creating Layer relu5
I0521 00:28:06.045429  9372 net.cpp:454] relu5 <- ip1
I0521 00:28:06.045442  9372 net.cpp:397] relu5 -> ip1 (in-place)
I0521 00:28:06.045788  9372 net.cpp:150] Setting up relu5
I0521 00:28:06.045801  9372 net.cpp:157] Top shape: 470 196 (92120)
I0521 00:28:06.045811  9372 net.cpp:165] Memory required for data: 741067800
I0521 00:28:06.045821  9372 layer_factory.hpp:77] Creating layer drop1
I0521 00:28:06.045840  9372 net.cpp:106] Creating Layer drop1
I0521 00:28:06.045850  9372 net.cpp:454] drop1 <- ip1
I0521 00:28:06.045864  9372 net.cpp:397] drop1 -> ip1 (in-place)
I0521 00:28:06.045909  9372 net.cpp:150] Setting up drop1
I0521 00:28:06.045922  9372 net.cpp:157] Top shape: 470 196 (92120)
I0521 00:28:06.045931  9372 net.cpp:165] Memory required for data: 741436280
I0521 00:28:06.045941  9372 layer_factory.hpp:77] Creating layer ip2
I0521 00:28:06.045956  9372 net.cpp:106] Creating Layer ip2
I0521 00:28:06.045967  9372 net.cpp:454] ip2 <- ip1
I0521 00:28:06.045981  9372 net.cpp:411] ip2 -> ip2
I0521 00:28:06.046479  9372 net.cpp:150] Setting up ip2
I0521 00:28:06.046492  9372 net.cpp:157] Top shape: 470 98 (46060)
I0521 00:28:06.046502  9372 net.cpp:165] Memory required for data: 741620520
I0521 00:28:06.046530  9372 layer_factory.hpp:77] Creating layer relu6
I0521 00:28:06.046543  9372 net.cpp:106] Creating Layer relu6
I0521 00:28:06.046553  9372 net.cpp:454] relu6 <- ip2
I0521 00:28:06.046566  9372 net.cpp:397] relu6 -> ip2 (in-place)
I0521 00:28:06.047101  9372 net.cpp:150] Setting up relu6
I0521 00:28:06.047122  9372 net.cpp:157] Top shape: 470 98 (46060)
I0521 00:28:06.047132  9372 net.cpp:165] Memory required for data: 741804760
I0521 00:28:06.047142  9372 layer_factory.hpp:77] Creating layer drop2
I0521 00:28:06.047155  9372 net.cpp:106] Creating Layer drop2
I0521 00:28:06.047165  9372 net.cpp:454] drop2 <- ip2
I0521 00:28:06.047178  9372 net.cpp:397] drop2 -> ip2 (in-place)
I0521 00:28:06.047224  9372 net.cpp:150] Setting up drop2
I0521 00:28:06.047236  9372 net.cpp:157] Top shape: 470 98 (46060)
I0521 00:28:06.047246  9372 net.cpp:165] Memory required for data: 741989000
I0521 00:28:06.047255  9372 layer_factory.hpp:77] Creating layer ip3
I0521 00:28:06.047269  9372 net.cpp:106] Creating Layer ip3
I0521 00:28:06.047279  9372 net.cpp:454] ip3 <- ip2
I0521 00:28:06.047293  9372 net.cpp:411] ip3 -> ip3
I0521 00:28:06.047518  9372 net.cpp:150] Setting up ip3
I0521 00:28:06.047531  9372 net.cpp:157] Top shape: 470 11 (5170)
I0521 00:28:06.047541  9372 net.cpp:165] Memory required for data: 742009680
I0521 00:28:06.047556  9372 layer_factory.hpp:77] Creating layer drop3
I0521 00:28:06.047569  9372 net.cpp:106] Creating Layer drop3
I0521 00:28:06.047580  9372 net.cpp:454] drop3 <- ip3
I0521 00:28:06.047591  9372 net.cpp:397] drop3 -> ip3 (in-place)
I0521 00:28:06.047633  9372 net.cpp:150] Setting up drop3
I0521 00:28:06.047646  9372 net.cpp:157] Top shape: 470 11 (5170)
I0521 00:28:06.047655  9372 net.cpp:165] Memory required for data: 742030360
I0521 00:28:06.047665  9372 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 00:28:06.047678  9372 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 00:28:06.047688  9372 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 00:28:06.047700  9372 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 00:28:06.047715  9372 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 00:28:06.047790  9372 net.cpp:150] Setting up ip3_drop3_0_split
I0521 00:28:06.047802  9372 net.cpp:157] Top shape: 470 11 (5170)
I0521 00:28:06.047814  9372 net.cpp:157] Top shape: 470 11 (5170)
I0521 00:28:06.047824  9372 net.cpp:165] Memory required for data: 742071720
I0521 00:28:06.047835  9372 layer_factory.hpp:77] Creating layer accuracy
I0521 00:28:06.047857  9372 net.cpp:106] Creating Layer accuracy
I0521 00:28:06.047868  9372 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 00:28:06.047878  9372 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 00:28:06.047891  9372 net.cpp:411] accuracy -> accuracy
I0521 00:28:06.047915  9372 net.cpp:150] Setting up accuracy
I0521 00:28:06.047929  9372 net.cpp:157] Top shape: (1)
I0521 00:28:06.047937  9372 net.cpp:165] Memory required for data: 742071724
I0521 00:28:06.047947  9372 layer_factory.hpp:77] Creating layer loss
I0521 00:28:06.047961  9372 net.cpp:106] Creating Layer loss
I0521 00:28:06.047971  9372 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 00:28:06.047982  9372 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 00:28:06.047996  9372 net.cpp:411] loss -> loss
I0521 00:28:06.048013  9372 layer_factory.hpp:77] Creating layer loss
I0521 00:28:06.048509  9372 net.cpp:150] Setting up loss
I0521 00:28:06.048523  9372 net.cpp:157] Top shape: (1)
I0521 00:28:06.048533  9372 net.cpp:160]     with loss weight 1
I0521 00:28:06.048552  9372 net.cpp:165] Memory required for data: 742071728
I0521 00:28:06.048562  9372 net.cpp:226] loss needs backward computation.
I0521 00:28:06.048573  9372 net.cpp:228] accuracy does not need backward computation.
I0521 00:28:06.048584  9372 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 00:28:06.048594  9372 net.cpp:226] drop3 needs backward computation.
I0521 00:28:06.048605  9372 net.cpp:226] ip3 needs backward computation.
I0521 00:28:06.048616  9372 net.cpp:226] drop2 needs backward computation.
I0521 00:28:06.048635  9372 net.cpp:226] relu6 needs backward computation.
I0521 00:28:06.048645  9372 net.cpp:226] ip2 needs backward computation.
I0521 00:28:06.048655  9372 net.cpp:226] drop1 needs backward computation.
I0521 00:28:06.048717  9372 net.cpp:226] relu5 needs backward computation.
I0521 00:28:06.048727  9372 net.cpp:226] ip1 needs backward computation.
I0521 00:28:06.048738  9372 net.cpp:226] pool4 needs backward computation.
I0521 00:28:06.048748  9372 net.cpp:226] relu4 needs backward computation.
I0521 00:28:06.048758  9372 net.cpp:226] conv4 needs backward computation.
I0521 00:28:06.048768  9372 net.cpp:226] pool3 needs backward computation.
I0521 00:28:06.048779  9372 net.cpp:226] relu3 needs backward computation.
I0521 00:28:06.048787  9372 net.cpp:226] conv3 needs backward computation.
I0521 00:28:06.048799  9372 net.cpp:226] pool2 needs backward computation.
I0521 00:28:06.048809  9372 net.cpp:226] relu2 needs backward computation.
I0521 00:28:06.048817  9372 net.cpp:226] conv2 needs backward computation.
I0521 00:28:06.048828  9372 net.cpp:226] pool1 needs backward computation.
I0521 00:28:06.048838  9372 net.cpp:226] relu1 needs backward computation.
I0521 00:28:06.048848  9372 net.cpp:226] conv1 needs backward computation.
I0521 00:28:06.048861  9372 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 00:28:06.048872  9372 net.cpp:228] data_hdf5 does not need backward computation.
I0521 00:28:06.048882  9372 net.cpp:270] This network produces output accuracy
I0521 00:28:06.048893  9372 net.cpp:270] This network produces output loss
I0521 00:28:06.048921  9372 net.cpp:283] Network initialization done.
I0521 00:28:06.049055  9372 solver.cpp:60] Solver scaffolding done.
I0521 00:28:06.050180  9372 caffe.cpp:212] Starting Optimization
I0521 00:28:06.050199  9372 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 00:28:06.050209  9372 solver.cpp:289] Learning Rate Policy: fixed
I0521 00:28:06.051436  9372 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 00:28:52.147145  9372 solver.cpp:409]     Test net output #0: accuracy = 0.0481957
I0521 00:28:52.147306  9372 solver.cpp:409]     Test net output #1: loss = 2.39967 (* 1 = 2.39967 loss)
I0521 00:28:52.241194  9372 solver.cpp:237] Iteration 0, loss = 2.3986
I0521 00:28:52.241231  9372 solver.cpp:253]     Train net output #0: loss = 2.3986 (* 1 = 2.3986 loss)
I0521 00:28:52.241250  9372 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 00:29:00.108445  9372 solver.cpp:237] Iteration 31, loss = 2.37857
I0521 00:29:00.108486  9372 solver.cpp:253]     Train net output #0: loss = 2.37857 (* 1 = 2.37857 loss)
I0521 00:29:00.108500  9372 sgd_solver.cpp:106] Iteration 31, lr = 0.0025
I0521 00:29:07.982185  9372 solver.cpp:237] Iteration 62, loss = 2.35686
I0521 00:29:07.982228  9372 solver.cpp:253]     Train net output #0: loss = 2.35686 (* 1 = 2.35686 loss)
I0521 00:29:07.982245  9372 sgd_solver.cpp:106] Iteration 62, lr = 0.0025
I0521 00:29:15.848580  9372 solver.cpp:237] Iteration 93, loss = 2.32646
I0521 00:29:15.848613  9372 solver.cpp:253]     Train net output #0: loss = 2.32646 (* 1 = 2.32646 loss)
I0521 00:29:15.848629  9372 sgd_solver.cpp:106] Iteration 93, lr = 0.0025
I0521 00:29:23.719743  9372 solver.cpp:237] Iteration 124, loss = 2.31025
I0521 00:29:23.719894  9372 solver.cpp:253]     Train net output #0: loss = 2.31025 (* 1 = 2.31025 loss)
I0521 00:29:23.719908  9372 sgd_solver.cpp:106] Iteration 124, lr = 0.0025
I0521 00:29:31.589310  9372 solver.cpp:237] Iteration 155, loss = 2.27707
I0521 00:29:31.589352  9372 solver.cpp:253]     Train net output #0: loss = 2.27707 (* 1 = 2.27707 loss)
I0521 00:29:31.589370  9372 sgd_solver.cpp:106] Iteration 155, lr = 0.0025
I0521 00:29:39.459053  9372 solver.cpp:237] Iteration 186, loss = 2.24218
I0521 00:29:39.459085  9372 solver.cpp:253]     Train net output #0: loss = 2.24218 (* 1 = 2.24218 loss)
I0521 00:29:39.459103  9372 sgd_solver.cpp:106] Iteration 186, lr = 0.0025
I0521 00:30:09.416235  9372 solver.cpp:237] Iteration 217, loss = 2.22592
I0521 00:30:09.416398  9372 solver.cpp:253]     Train net output #0: loss = 2.22592 (* 1 = 2.22592 loss)
I0521 00:30:09.416411  9372 sgd_solver.cpp:106] Iteration 217, lr = 0.0025
I0521 00:30:17.288300  9372 solver.cpp:237] Iteration 248, loss = 2.20665
I0521 00:30:17.288347  9372 solver.cpp:253]     Train net output #0: loss = 2.20665 (* 1 = 2.20665 loss)
I0521 00:30:17.288363  9372 sgd_solver.cpp:106] Iteration 248, lr = 0.0025
I0521 00:30:25.160303  9372 solver.cpp:237] Iteration 279, loss = 2.13573
I0521 00:30:25.160336  9372 solver.cpp:253]     Train net output #0: loss = 2.13573 (* 1 = 2.13573 loss)
I0521 00:30:25.160353  9372 sgd_solver.cpp:106] Iteration 279, lr = 0.0025
I0521 00:30:33.030627  9372 solver.cpp:237] Iteration 310, loss = 2.10647
I0521 00:30:33.030655  9372 solver.cpp:253]     Train net output #0: loss = 2.10647 (* 1 = 2.10647 loss)
I0521 00:30:33.030668  9372 sgd_solver.cpp:106] Iteration 310, lr = 0.0025
I0521 00:30:35.064318  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_319.caffemodel
I0521 00:30:35.283839  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_319.solverstate
I0521 00:30:40.970890  9372 solver.cpp:237] Iteration 341, loss = 2.06469
I0521 00:30:40.971050  9372 solver.cpp:253]     Train net output #0: loss = 2.06469 (* 1 = 2.06469 loss)
I0521 00:30:40.971065  9372 sgd_solver.cpp:106] Iteration 341, lr = 0.0025
I0521 00:30:48.841733  9372 solver.cpp:237] Iteration 372, loss = 1.9942
I0521 00:30:48.841768  9372 solver.cpp:253]     Train net output #0: loss = 1.9942 (* 1 = 1.9942 loss)
I0521 00:30:48.841784  9372 sgd_solver.cpp:106] Iteration 372, lr = 0.0025
I0521 00:30:56.711375  9372 solver.cpp:237] Iteration 403, loss = 2.04904
I0521 00:30:56.711407  9372 solver.cpp:253]     Train net output #0: loss = 2.04904 (* 1 = 2.04904 loss)
I0521 00:30:56.711424  9372 sgd_solver.cpp:106] Iteration 403, lr = 0.0025
I0521 00:31:26.693011  9372 solver.cpp:237] Iteration 434, loss = 1.95471
I0521 00:31:26.693166  9372 solver.cpp:253]     Train net output #0: loss = 1.95471 (* 1 = 1.95471 loss)
I0521 00:31:26.693179  9372 sgd_solver.cpp:106] Iteration 434, lr = 0.0025
I0521 00:31:34.563057  9372 solver.cpp:237] Iteration 465, loss = 1.90247
I0521 00:31:34.563096  9372 solver.cpp:253]     Train net output #0: loss = 1.90247 (* 1 = 1.90247 loss)
I0521 00:31:34.563109  9372 sgd_solver.cpp:106] Iteration 465, lr = 0.0025
I0521 00:31:42.433897  9372 solver.cpp:237] Iteration 496, loss = 1.94786
I0521 00:31:42.433930  9372 solver.cpp:253]     Train net output #0: loss = 1.94786 (* 1 = 1.94786 loss)
I0521 00:31:42.433946  9372 sgd_solver.cpp:106] Iteration 496, lr = 0.0025
I0521 00:31:50.308893  9372 solver.cpp:237] Iteration 527, loss = 1.91115
I0521 00:31:50.308928  9372 solver.cpp:253]     Train net output #0: loss = 1.91115 (* 1 = 1.91115 loss)
I0521 00:31:50.308943  9372 sgd_solver.cpp:106] Iteration 527, lr = 0.0025
I0521 00:31:58.179225  9372 solver.cpp:237] Iteration 558, loss = 1.91276
I0521 00:31:58.179381  9372 solver.cpp:253]     Train net output #0: loss = 1.91276 (* 1 = 1.91276 loss)
I0521 00:31:58.179394  9372 sgd_solver.cpp:106] Iteration 558, lr = 0.0025
I0521 00:32:06.052755  9372 solver.cpp:237] Iteration 589, loss = 1.82271
I0521 00:32:06.052788  9372 solver.cpp:253]     Train net output #0: loss = 1.82271 (* 1 = 1.82271 loss)
I0521 00:32:06.052805  9372 sgd_solver.cpp:106] Iteration 589, lr = 0.0025
I0521 00:32:13.925602  9372 solver.cpp:237] Iteration 620, loss = 1.86228
I0521 00:32:13.925637  9372 solver.cpp:253]     Train net output #0: loss = 1.86228 (* 1 = 1.86228 loss)
I0521 00:32:13.925652  9372 sgd_solver.cpp:106] Iteration 620, lr = 0.0025
I0521 00:32:18.245481  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_638.caffemodel
I0521 00:32:18.468204  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_638.solverstate
I0521 00:32:18.493937  9372 solver.cpp:341] Iteration 638, Testing net (#0)
I0521 00:33:03.670790  9372 solver.cpp:409]     Test net output #0: accuracy = 0.600714
I0521 00:33:03.670950  9372 solver.cpp:409]     Test net output #1: loss = 1.47577 (* 1 = 1.47577 loss)
I0521 00:33:29.172118  9372 solver.cpp:237] Iteration 651, loss = 1.87201
I0521 00:33:29.172169  9372 solver.cpp:253]     Train net output #0: loss = 1.87201 (* 1 = 1.87201 loss)
I0521 00:33:29.172184  9372 sgd_solver.cpp:106] Iteration 651, lr = 0.0025
I0521 00:33:37.037061  9372 solver.cpp:237] Iteration 682, loss = 1.88601
I0521 00:33:37.037206  9372 solver.cpp:253]     Train net output #0: loss = 1.88601 (* 1 = 1.88601 loss)
I0521 00:33:37.037220  9372 sgd_solver.cpp:106] Iteration 682, lr = 0.0025
I0521 00:33:44.905427  9372 solver.cpp:237] Iteration 713, loss = 1.82506
I0521 00:33:44.905460  9372 solver.cpp:253]     Train net output #0: loss = 1.82506 (* 1 = 1.82506 loss)
I0521 00:33:44.905478  9372 sgd_solver.cpp:106] Iteration 713, lr = 0.0025
I0521 00:33:52.774487  9372 solver.cpp:237] Iteration 744, loss = 1.77412
I0521 00:33:52.774520  9372 solver.cpp:253]     Train net output #0: loss = 1.77412 (* 1 = 1.77412 loss)
I0521 00:33:52.774538  9372 sgd_solver.cpp:106] Iteration 744, lr = 0.0025
I0521 00:34:00.638571  9372 solver.cpp:237] Iteration 775, loss = 1.86241
I0521 00:34:00.638603  9372 solver.cpp:253]     Train net output #0: loss = 1.86241 (* 1 = 1.86241 loss)
I0521 00:34:00.638619  9372 sgd_solver.cpp:106] Iteration 775, lr = 0.0025
I0521 00:34:08.506770  9372 solver.cpp:237] Iteration 806, loss = 1.83268
I0521 00:34:08.506914  9372 solver.cpp:253]     Train net output #0: loss = 1.83268 (* 1 = 1.83268 loss)
I0521 00:34:08.506928  9372 sgd_solver.cpp:106] Iteration 806, lr = 0.0025
I0521 00:34:16.373890  9372 solver.cpp:237] Iteration 837, loss = 1.76793
I0521 00:34:16.373924  9372 solver.cpp:253]     Train net output #0: loss = 1.76793 (* 1 = 1.76793 loss)
I0521 00:34:16.373937  9372 sgd_solver.cpp:106] Iteration 837, lr = 0.0025
I0521 00:34:46.410373  9372 solver.cpp:237] Iteration 868, loss = 1.79446
I0521 00:34:46.410543  9372 solver.cpp:253]     Train net output #0: loss = 1.79446 (* 1 = 1.79446 loss)
I0521 00:34:46.410559  9372 sgd_solver.cpp:106] Iteration 868, lr = 0.0025
I0521 00:34:54.276124  9372 solver.cpp:237] Iteration 899, loss = 1.72734
I0521 00:34:54.276167  9372 solver.cpp:253]     Train net output #0: loss = 1.72734 (* 1 = 1.72734 loss)
I0521 00:34:54.276183  9372 sgd_solver.cpp:106] Iteration 899, lr = 0.0025
I0521 00:35:02.150979  9372 solver.cpp:237] Iteration 930, loss = 1.77852
I0521 00:35:02.151013  9372 solver.cpp:253]     Train net output #0: loss = 1.77852 (* 1 = 1.77852 loss)
I0521 00:35:02.151031  9372 sgd_solver.cpp:106] Iteration 930, lr = 0.0025
I0521 00:35:08.750548  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_957.caffemodel
I0521 00:35:08.968719  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_957.solverstate
I0521 00:35:10.087446  9372 solver.cpp:237] Iteration 961, loss = 1.76496
I0521 00:35:10.087496  9372 solver.cpp:253]     Train net output #0: loss = 1.76496 (* 1 = 1.76496 loss)
I0521 00:35:10.087512  9372 sgd_solver.cpp:106] Iteration 961, lr = 0.0025
I0521 00:35:17.954836  9372 solver.cpp:237] Iteration 992, loss = 1.77159
I0521 00:35:17.954994  9372 solver.cpp:253]     Train net output #0: loss = 1.77159 (* 1 = 1.77159 loss)
I0521 00:35:17.955008  9372 sgd_solver.cpp:106] Iteration 992, lr = 0.0025
I0521 00:35:25.820359  9372 solver.cpp:237] Iteration 1023, loss = 1.90158
I0521 00:35:25.820391  9372 solver.cpp:253]     Train net output #0: loss = 1.90158 (* 1 = 1.90158 loss)
I0521 00:35:25.820408  9372 sgd_solver.cpp:106] Iteration 1023, lr = 0.0025
I0521 00:35:33.691723  9372 solver.cpp:237] Iteration 1054, loss = 1.7209
I0521 00:35:33.691757  9372 solver.cpp:253]     Train net output #0: loss = 1.7209 (* 1 = 1.7209 loss)
I0521 00:35:33.691771  9372 sgd_solver.cpp:106] Iteration 1054, lr = 0.0025
I0521 00:36:03.745997  9372 solver.cpp:237] Iteration 1085, loss = 1.77978
I0521 00:36:03.746153  9372 solver.cpp:253]     Train net output #0: loss = 1.77978 (* 1 = 1.77978 loss)
I0521 00:36:03.746168  9372 sgd_solver.cpp:106] Iteration 1085, lr = 0.0025
I0521 00:36:11.612941  9372 solver.cpp:237] Iteration 1116, loss = 1.73065
I0521 00:36:11.612990  9372 solver.cpp:253]     Train net output #0: loss = 1.73065 (* 1 = 1.73065 loss)
I0521 00:36:11.613005  9372 sgd_solver.cpp:106] Iteration 1116, lr = 0.0025
I0521 00:36:19.480216  9372 solver.cpp:237] Iteration 1147, loss = 1.86976
I0521 00:36:19.480248  9372 solver.cpp:253]     Train net output #0: loss = 1.86976 (* 1 = 1.86976 loss)
I0521 00:36:19.480262  9372 sgd_solver.cpp:106] Iteration 1147, lr = 0.0025
I0521 00:36:27.345628  9372 solver.cpp:237] Iteration 1178, loss = 1.78702
I0521 00:36:27.345659  9372 solver.cpp:253]     Train net output #0: loss = 1.78702 (* 1 = 1.78702 loss)
I0521 00:36:27.345676  9372 sgd_solver.cpp:106] Iteration 1178, lr = 0.0025
I0521 00:36:35.214778  9372 solver.cpp:237] Iteration 1209, loss = 1.71346
I0521 00:36:35.214921  9372 solver.cpp:253]     Train net output #0: loss = 1.71346 (* 1 = 1.71346 loss)
I0521 00:36:35.214934  9372 sgd_solver.cpp:106] Iteration 1209, lr = 0.0025
I0521 00:36:43.080396  9372 solver.cpp:237] Iteration 1240, loss = 1.76019
I0521 00:36:43.080430  9372 solver.cpp:253]     Train net output #0: loss = 1.76019 (* 1 = 1.76019 loss)
I0521 00:36:43.080443  9372 sgd_solver.cpp:106] Iteration 1240, lr = 0.0025
I0521 00:36:50.946748  9372 solver.cpp:237] Iteration 1271, loss = 1.77452
I0521 00:36:50.946781  9372 solver.cpp:253]     Train net output #0: loss = 1.77452 (* 1 = 1.77452 loss)
I0521 00:36:50.946799  9372 sgd_solver.cpp:106] Iteration 1271, lr = 0.0025
I0521 00:36:51.962214  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_1276.caffemodel
I0521 00:36:52.180981  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_1276.solverstate
I0521 00:36:52.209681  9372 solver.cpp:341] Iteration 1276, Testing net (#0)
I0521 00:37:58.265578  9372 solver.cpp:409]     Test net output #0: accuracy = 0.649043
I0521 00:37:58.265748  9372 solver.cpp:409]     Test net output #1: loss = 1.23938 (* 1 = 1.23938 loss)
I0521 00:38:27.051100  9372 solver.cpp:237] Iteration 1302, loss = 1.71784
I0521 00:38:27.051151  9372 solver.cpp:253]     Train net output #0: loss = 1.71784 (* 1 = 1.71784 loss)
I0521 00:38:27.051165  9372 sgd_solver.cpp:106] Iteration 1302, lr = 0.0025
I0521 00:38:34.908363  9372 solver.cpp:237] Iteration 1333, loss = 1.81243
I0521 00:38:34.908529  9372 solver.cpp:253]     Train net output #0: loss = 1.81243 (* 1 = 1.81243 loss)
I0521 00:38:34.908545  9372 sgd_solver.cpp:106] Iteration 1333, lr = 0.0025
I0521 00:38:42.773223  9372 solver.cpp:237] Iteration 1364, loss = 1.80525
I0521 00:38:42.773267  9372 solver.cpp:253]     Train net output #0: loss = 1.80525 (* 1 = 1.80525 loss)
I0521 00:38:42.773283  9372 sgd_solver.cpp:106] Iteration 1364, lr = 0.0025
I0521 00:38:50.633611  9372 solver.cpp:237] Iteration 1395, loss = 1.69369
I0521 00:38:50.633643  9372 solver.cpp:253]     Train net output #0: loss = 1.69369 (* 1 = 1.69369 loss)
I0521 00:38:50.633661  9372 sgd_solver.cpp:106] Iteration 1395, lr = 0.0025
I0521 00:38:58.487524  9372 solver.cpp:237] Iteration 1426, loss = 1.76753
I0521 00:38:58.487555  9372 solver.cpp:253]     Train net output #0: loss = 1.76753 (* 1 = 1.76753 loss)
I0521 00:38:58.487571  9372 sgd_solver.cpp:106] Iteration 1426, lr = 0.0025
I0521 00:39:06.347014  9372 solver.cpp:237] Iteration 1457, loss = 1.69483
I0521 00:39:06.347164  9372 solver.cpp:253]     Train net output #0: loss = 1.69483 (* 1 = 1.69483 loss)
I0521 00:39:06.347178  9372 sgd_solver.cpp:106] Iteration 1457, lr = 0.0025
I0521 00:39:14.205878  9372 solver.cpp:237] Iteration 1488, loss = 1.7801
I0521 00:39:14.205912  9372 solver.cpp:253]     Train net output #0: loss = 1.7801 (* 1 = 1.7801 loss)
I0521 00:39:14.205929  9372 sgd_solver.cpp:106] Iteration 1488, lr = 0.0025
I0521 00:39:44.244645  9372 solver.cpp:237] Iteration 1519, loss = 1.64799
I0521 00:39:44.244803  9372 solver.cpp:253]     Train net output #0: loss = 1.64799 (* 1 = 1.64799 loss)
I0521 00:39:44.244819  9372 sgd_solver.cpp:106] Iteration 1519, lr = 0.0025
I0521 00:39:52.101024  9372 solver.cpp:237] Iteration 1550, loss = 1.72251
I0521 00:39:52.101068  9372 solver.cpp:253]     Train net output #0: loss = 1.72251 (* 1 = 1.72251 loss)
I0521 00:39:52.101084  9372 sgd_solver.cpp:106] Iteration 1550, lr = 0.0025
I0521 00:39:59.959298  9372 solver.cpp:237] Iteration 1581, loss = 1.6521
I0521 00:39:59.959331  9372 solver.cpp:253]     Train net output #0: loss = 1.6521 (* 1 = 1.6521 loss)
I0521 00:39:59.959345  9372 sgd_solver.cpp:106] Iteration 1581, lr = 0.0025
I0521 00:40:03.257179  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_1595.caffemodel
I0521 00:40:03.475076  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_1595.solverstate
I0521 00:40:07.892774  9372 solver.cpp:237] Iteration 1612, loss = 1.73804
I0521 00:40:07.892823  9372 solver.cpp:253]     Train net output #0: loss = 1.73804 (* 1 = 1.73804 loss)
I0521 00:40:07.892839  9372 sgd_solver.cpp:106] Iteration 1612, lr = 0.0025
I0521 00:40:15.750326  9372 solver.cpp:237] Iteration 1643, loss = 1.75998
I0521 00:40:15.750481  9372 solver.cpp:253]     Train net output #0: loss = 1.75998 (* 1 = 1.75998 loss)
I0521 00:40:15.750494  9372 sgd_solver.cpp:106] Iteration 1643, lr = 0.0025
I0521 00:40:23.610771  9372 solver.cpp:237] Iteration 1674, loss = 1.63208
I0521 00:40:23.610815  9372 solver.cpp:253]     Train net output #0: loss = 1.63208 (* 1 = 1.63208 loss)
I0521 00:40:23.610828  9372 sgd_solver.cpp:106] Iteration 1674, lr = 0.0025
I0521 00:40:53.881203  9372 solver.cpp:237] Iteration 1705, loss = 1.71615
I0521 00:40:53.881373  9372 solver.cpp:253]     Train net output #0: loss = 1.71615 (* 1 = 1.71615 loss)
I0521 00:40:53.881388  9372 sgd_solver.cpp:106] Iteration 1705, lr = 0.0025
I0521 00:41:01.734812  9372 solver.cpp:237] Iteration 1736, loss = 1.7642
I0521 00:41:01.734845  9372 solver.cpp:253]     Train net output #0: loss = 1.7642 (* 1 = 1.7642 loss)
I0521 00:41:01.734863  9372 sgd_solver.cpp:106] Iteration 1736, lr = 0.0025
I0521 00:41:09.594713  9372 solver.cpp:237] Iteration 1767, loss = 1.7005
I0521 00:41:09.594753  9372 solver.cpp:253]     Train net output #0: loss = 1.7005 (* 1 = 1.7005 loss)
I0521 00:41:09.594769  9372 sgd_solver.cpp:106] Iteration 1767, lr = 0.0025
I0521 00:41:17.452258  9372 solver.cpp:237] Iteration 1798, loss = 1.62794
I0521 00:41:17.452291  9372 solver.cpp:253]     Train net output #0: loss = 1.62794 (* 1 = 1.62794 loss)
I0521 00:41:17.452308  9372 sgd_solver.cpp:106] Iteration 1798, lr = 0.0025
I0521 00:41:25.309540  9372 solver.cpp:237] Iteration 1829, loss = 1.62117
I0521 00:41:25.309677  9372 solver.cpp:253]     Train net output #0: loss = 1.62117 (* 1 = 1.62117 loss)
I0521 00:41:25.309690  9372 sgd_solver.cpp:106] Iteration 1829, lr = 0.0025
I0521 00:41:33.171672  9372 solver.cpp:237] Iteration 1860, loss = 1.66385
I0521 00:41:33.171721  9372 solver.cpp:253]     Train net output #0: loss = 1.66385 (* 1 = 1.66385 loss)
I0521 00:41:33.171736  9372 sgd_solver.cpp:106] Iteration 1860, lr = 0.0025
I0521 00:41:41.031280  9372 solver.cpp:237] Iteration 1891, loss = 1.74383
I0521 00:41:41.031312  9372 solver.cpp:253]     Train net output #0: loss = 1.74383 (* 1 = 1.74383 loss)
I0521 00:41:41.031329  9372 sgd_solver.cpp:106] Iteration 1891, lr = 0.0025
I0521 00:41:46.606788  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_1914.caffemodel
I0521 00:41:46.973798  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_1914.solverstate
I0521 00:41:47.142997  9372 solver.cpp:341] Iteration 1914, Testing net (#0)
I0521 00:42:32.071475  9372 solver.cpp:409]     Test net output #0: accuracy = 0.672908
I0521 00:42:32.071633  9372 solver.cpp:409]     Test net output #1: loss = 1.11258 (* 1 = 1.11258 loss)
I0521 00:42:56.313509  9372 solver.cpp:237] Iteration 1922, loss = 1.66577
I0521 00:42:56.313560  9372 solver.cpp:253]     Train net output #0: loss = 1.66577 (* 1 = 1.66577 loss)
I0521 00:42:56.313580  9372 sgd_solver.cpp:106] Iteration 1922, lr = 0.0025
I0521 00:43:04.177201  9372 solver.cpp:237] Iteration 1953, loss = 1.64462
I0521 00:43:04.177357  9372 solver.cpp:253]     Train net output #0: loss = 1.64462 (* 1 = 1.64462 loss)
I0521 00:43:04.177371  9372 sgd_solver.cpp:106] Iteration 1953, lr = 0.0025
I0521 00:43:12.036460  9372 solver.cpp:237] Iteration 1984, loss = 1.68067
I0521 00:43:12.036499  9372 solver.cpp:253]     Train net output #0: loss = 1.68067 (* 1 = 1.68067 loss)
I0521 00:43:12.036511  9372 sgd_solver.cpp:106] Iteration 1984, lr = 0.0025
I0521 00:43:19.899472  9372 solver.cpp:237] Iteration 2015, loss = 1.6467
I0521 00:43:19.899507  9372 solver.cpp:253]     Train net output #0: loss = 1.6467 (* 1 = 1.6467 loss)
I0521 00:43:19.899519  9372 sgd_solver.cpp:106] Iteration 2015, lr = 0.0025
I0521 00:43:27.759604  9372 solver.cpp:237] Iteration 2046, loss = 1.69278
I0521 00:43:27.759637  9372 solver.cpp:253]     Train net output #0: loss = 1.69278 (* 1 = 1.69278 loss)
I0521 00:43:27.759654  9372 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0521 00:43:35.621588  9372 solver.cpp:237] Iteration 2077, loss = 1.66066
I0521 00:43:35.621745  9372 solver.cpp:253]     Train net output #0: loss = 1.66066 (* 1 = 1.66066 loss)
I0521 00:43:35.621760  9372 sgd_solver.cpp:106] Iteration 2077, lr = 0.0025
I0521 00:43:43.482748  9372 solver.cpp:237] Iteration 2108, loss = 1.60441
I0521 00:43:43.482797  9372 solver.cpp:253]     Train net output #0: loss = 1.60441 (* 1 = 1.60441 loss)
I0521 00:43:43.482811  9372 sgd_solver.cpp:106] Iteration 2108, lr = 0.0025
I0521 00:44:13.571288  9372 solver.cpp:237] Iteration 2139, loss = 1.61786
I0521 00:44:13.571458  9372 solver.cpp:253]     Train net output #0: loss = 1.61786 (* 1 = 1.61786 loss)
I0521 00:44:13.571475  9372 sgd_solver.cpp:106] Iteration 2139, lr = 0.0025
I0521 00:44:21.431659  9372 solver.cpp:237] Iteration 2170, loss = 1.63769
I0521 00:44:21.431692  9372 solver.cpp:253]     Train net output #0: loss = 1.63769 (* 1 = 1.63769 loss)
I0521 00:44:21.431706  9372 sgd_solver.cpp:106] Iteration 2170, lr = 0.0025
I0521 00:44:29.294754  9372 solver.cpp:237] Iteration 2201, loss = 1.56948
I0521 00:44:29.294798  9372 solver.cpp:253]     Train net output #0: loss = 1.56948 (* 1 = 1.56948 loss)
I0521 00:44:29.294816  9372 sgd_solver.cpp:106] Iteration 2201, lr = 0.0025
I0521 00:44:37.157316  9372 solver.cpp:237] Iteration 2232, loss = 1.61827
I0521 00:44:37.157349  9372 solver.cpp:253]     Train net output #0: loss = 1.61827 (* 1 = 1.61827 loss)
I0521 00:44:37.157364  9372 sgd_solver.cpp:106] Iteration 2232, lr = 0.0025
I0521 00:44:37.157740  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_2233.caffemodel
I0521 00:44:37.372964  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_2233.solverstate
I0521 00:44:45.083184  9372 solver.cpp:237] Iteration 2263, loss = 1.63749
I0521 00:44:45.083339  9372 solver.cpp:253]     Train net output #0: loss = 1.63749 (* 1 = 1.63749 loss)
I0521 00:44:45.083353  9372 sgd_solver.cpp:106] Iteration 2263, lr = 0.0025
I0521 00:44:52.944324  9372 solver.cpp:237] Iteration 2294, loss = 1.59662
I0521 00:44:52.944372  9372 solver.cpp:253]     Train net output #0: loss = 1.59662 (* 1 = 1.59662 loss)
I0521 00:44:52.944387  9372 sgd_solver.cpp:106] Iteration 2294, lr = 0.0025
I0521 00:45:00.805783  9372 solver.cpp:237] Iteration 2325, loss = 1.72139
I0521 00:45:00.805816  9372 solver.cpp:253]     Train net output #0: loss = 1.72139 (* 1 = 1.72139 loss)
I0521 00:45:00.805833  9372 sgd_solver.cpp:106] Iteration 2325, lr = 0.0025
I0521 00:45:30.839053  9372 solver.cpp:237] Iteration 2356, loss = 1.62322
I0521 00:45:30.839222  9372 solver.cpp:253]     Train net output #0: loss = 1.62322 (* 1 = 1.62322 loss)
I0521 00:45:30.839237  9372 sgd_solver.cpp:106] Iteration 2356, lr = 0.0025
I0521 00:45:38.698925  9372 solver.cpp:237] Iteration 2387, loss = 1.58918
I0521 00:45:38.698958  9372 solver.cpp:253]     Train net output #0: loss = 1.58918 (* 1 = 1.58918 loss)
I0521 00:45:38.698976  9372 sgd_solver.cpp:106] Iteration 2387, lr = 0.0025
I0521 00:45:46.560791  9372 solver.cpp:237] Iteration 2418, loss = 1.58215
I0521 00:45:46.560827  9372 solver.cpp:253]     Train net output #0: loss = 1.58215 (* 1 = 1.58215 loss)
I0521 00:45:46.560843  9372 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0521 00:45:54.423511  9372 solver.cpp:237] Iteration 2449, loss = 1.58101
I0521 00:45:54.423544  9372 solver.cpp:253]     Train net output #0: loss = 1.58101 (* 1 = 1.58101 loss)
I0521 00:45:54.423562  9372 sgd_solver.cpp:106] Iteration 2449, lr = 0.0025
I0521 00:46:02.286943  9372 solver.cpp:237] Iteration 2480, loss = 1.61701
I0521 00:46:02.287088  9372 solver.cpp:253]     Train net output #0: loss = 1.61701 (* 1 = 1.61701 loss)
I0521 00:46:02.287103  9372 sgd_solver.cpp:106] Iteration 2480, lr = 0.0025
I0521 00:46:10.154423  9372 solver.cpp:237] Iteration 2511, loss = 1.66056
I0521 00:46:10.154463  9372 solver.cpp:253]     Train net output #0: loss = 1.66056 (* 1 = 1.66056 loss)
I0521 00:46:10.154479  9372 sgd_solver.cpp:106] Iteration 2511, lr = 0.0025
I0521 00:46:18.014600  9372 solver.cpp:237] Iteration 2542, loss = 1.66665
I0521 00:46:18.014633  9372 solver.cpp:253]     Train net output #0: loss = 1.66665 (* 1 = 1.66665 loss)
I0521 00:46:18.014649  9372 sgd_solver.cpp:106] Iteration 2542, lr = 0.0025
I0521 00:46:20.298185  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_2552.caffemodel
I0521 00:46:20.515431  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_2552.solverstate
I0521 00:46:20.541584  9372 solver.cpp:341] Iteration 2552, Testing net (#0)
I0521 00:47:26.531033  9372 solver.cpp:409]     Test net output #0: accuracy = 0.682492
I0521 00:47:26.531198  9372 solver.cpp:409]     Test net output #1: loss = 1.07446 (* 1 = 1.07446 loss)
I0521 00:47:54.091835  9372 solver.cpp:237] Iteration 2573, loss = 1.67336
I0521 00:47:54.091887  9372 solver.cpp:253]     Train net output #0: loss = 1.67336 (* 1 = 1.67336 loss)
I0521 00:47:54.091902  9372 sgd_solver.cpp:106] Iteration 2573, lr = 0.0025
I0521 00:48:01.957726  9372 solver.cpp:237] Iteration 2604, loss = 1.55474
I0521 00:48:01.957867  9372 solver.cpp:253]     Train net output #0: loss = 1.55474 (* 1 = 1.55474 loss)
I0521 00:48:01.957880  9372 sgd_solver.cpp:106] Iteration 2604, lr = 0.0025
I0521 00:48:09.822548  9372 solver.cpp:237] Iteration 2635, loss = 1.63434
I0521 00:48:09.822581  9372 solver.cpp:253]     Train net output #0: loss = 1.63434 (* 1 = 1.63434 loss)
I0521 00:48:09.822597  9372 sgd_solver.cpp:106] Iteration 2635, lr = 0.0025
I0521 00:48:17.689826  9372 solver.cpp:237] Iteration 2666, loss = 1.67729
I0521 00:48:17.689863  9372 solver.cpp:253]     Train net output #0: loss = 1.67729 (* 1 = 1.67729 loss)
I0521 00:48:17.689880  9372 sgd_solver.cpp:106] Iteration 2666, lr = 0.0025
I0521 00:48:25.553077  9372 solver.cpp:237] Iteration 2697, loss = 1.65199
I0521 00:48:25.553110  9372 solver.cpp:253]     Train net output #0: loss = 1.65199 (* 1 = 1.65199 loss)
I0521 00:48:25.553128  9372 sgd_solver.cpp:106] Iteration 2697, lr = 0.0025
I0521 00:48:33.417119  9372 solver.cpp:237] Iteration 2728, loss = 1.59658
I0521 00:48:33.417273  9372 solver.cpp:253]     Train net output #0: loss = 1.59658 (* 1 = 1.59658 loss)
I0521 00:48:33.417285  9372 sgd_solver.cpp:106] Iteration 2728, lr = 0.0025
I0521 00:48:41.281332  9372 solver.cpp:237] Iteration 2759, loss = 1.65326
I0521 00:48:41.281371  9372 solver.cpp:253]     Train net output #0: loss = 1.65326 (* 1 = 1.65326 loss)
I0521 00:48:41.281388  9372 sgd_solver.cpp:106] Iteration 2759, lr = 0.0025
I0521 00:49:11.345248  9372 solver.cpp:237] Iteration 2790, loss = 1.55864
I0521 00:49:11.345422  9372 solver.cpp:253]     Train net output #0: loss = 1.55864 (* 1 = 1.55864 loss)
I0521 00:49:11.345438  9372 sgd_solver.cpp:106] Iteration 2790, lr = 0.0025
I0521 00:49:19.213832  9372 solver.cpp:237] Iteration 2821, loss = 1.65924
I0521 00:49:19.213865  9372 solver.cpp:253]     Train net output #0: loss = 1.65924 (* 1 = 1.65924 loss)
I0521 00:49:19.213883  9372 sgd_solver.cpp:106] Iteration 2821, lr = 0.0025
I0521 00:49:27.076570  9372 solver.cpp:237] Iteration 2852, loss = 1.60149
I0521 00:49:27.076603  9372 solver.cpp:253]     Train net output #0: loss = 1.60149 (* 1 = 1.60149 loss)
I0521 00:49:27.076614  9372 sgd_solver.cpp:106] Iteration 2852, lr = 0.0025
I0521 00:49:31.644970  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_2871.caffemodel
I0521 00:49:31.863454  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_2871.solverstate
I0521 00:49:35.012037  9372 solver.cpp:237] Iteration 2883, loss = 1.56491
I0521 00:49:35.012086  9372 solver.cpp:253]     Train net output #0: loss = 1.56491 (* 1 = 1.56491 loss)
I0521 00:49:35.012101  9372 sgd_solver.cpp:106] Iteration 2883, lr = 0.0025
I0521 00:49:42.880237  9372 solver.cpp:237] Iteration 2914, loss = 1.65176
I0521 00:49:42.880394  9372 solver.cpp:253]     Train net output #0: loss = 1.65176 (* 1 = 1.65176 loss)
I0521 00:49:42.880409  9372 sgd_solver.cpp:106] Iteration 2914, lr = 0.0025
I0521 00:49:50.747691  9372 solver.cpp:237] Iteration 2945, loss = 1.63861
I0521 00:49:50.747723  9372 solver.cpp:253]     Train net output #0: loss = 1.63861 (* 1 = 1.63861 loss)
I0521 00:49:50.747740  9372 sgd_solver.cpp:106] Iteration 2945, lr = 0.0025
I0521 00:49:58.611063  9372 solver.cpp:237] Iteration 2976, loss = 1.62953
I0521 00:49:58.611115  9372 solver.cpp:253]     Train net output #0: loss = 1.62953 (* 1 = 1.62953 loss)
I0521 00:49:58.611129  9372 sgd_solver.cpp:106] Iteration 2976, lr = 0.0025
I0521 00:50:28.620957  9372 solver.cpp:237] Iteration 3007, loss = 1.59461
I0521 00:50:28.621122  9372 solver.cpp:253]     Train net output #0: loss = 1.59461 (* 1 = 1.59461 loss)
I0521 00:50:28.621136  9372 sgd_solver.cpp:106] Iteration 3007, lr = 0.0025
I0521 00:50:36.488294  9372 solver.cpp:237] Iteration 3038, loss = 1.60318
I0521 00:50:36.488328  9372 solver.cpp:253]     Train net output #0: loss = 1.60318 (* 1 = 1.60318 loss)
I0521 00:50:36.488345  9372 sgd_solver.cpp:106] Iteration 3038, lr = 0.0025
I0521 00:50:44.356660  9372 solver.cpp:237] Iteration 3069, loss = 1.58275
I0521 00:50:44.356695  9372 solver.cpp:253]     Train net output #0: loss = 1.58275 (* 1 = 1.58275 loss)
I0521 00:50:44.356710  9372 sgd_solver.cpp:106] Iteration 3069, lr = 0.0025
I0521 00:50:52.224628  9372 solver.cpp:237] Iteration 3100, loss = 1.62453
I0521 00:50:52.224661  9372 solver.cpp:253]     Train net output #0: loss = 1.62453 (* 1 = 1.62453 loss)
I0521 00:50:52.224680  9372 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0521 00:51:00.090744  9372 solver.cpp:237] Iteration 3131, loss = 1.60848
I0521 00:51:00.090893  9372 solver.cpp:253]     Train net output #0: loss = 1.60848 (* 1 = 1.60848 loss)
I0521 00:51:00.090908  9372 sgd_solver.cpp:106] Iteration 3131, lr = 0.0025
I0521 00:51:07.956269  9372 solver.cpp:237] Iteration 3162, loss = 1.56425
I0521 00:51:07.956305  9372 solver.cpp:253]     Train net output #0: loss = 1.56425 (* 1 = 1.56425 loss)
I0521 00:51:07.956322  9372 sgd_solver.cpp:106] Iteration 3162, lr = 0.0025
I0521 00:51:14.810865  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_3190.caffemodel
I0521 00:51:15.028703  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_3190.solverstate
I0521 00:51:15.057188  9372 solver.cpp:341] Iteration 3190, Testing net (#0)
I0521 00:52:00.233052  9372 solver.cpp:409]     Test net output #0: accuracy = 0.704762
I0521 00:52:00.233224  9372 solver.cpp:409]     Test net output #1: loss = 1.01576 (* 1 = 1.01576 loss)
I0521 00:52:00.309134  9372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_3191.caffemodel
I0521 00:52:00.527801  9372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464_iter_3191.solverstate
I0521 00:52:00.556057  9372 solver.cpp:326] Optimization Done.
I0521 00:52:00.556085  9372 caffe.cpp:215] Optimization Done.
Application 11236200 resources: utime ~1253s, stime ~227s, Rss ~5329448, inblocks ~3594475, outblocks ~194563
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_470_2016-05-20T11.20.49.690464.solver"
	User time (seconds): 0.55
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:43.87
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15078
	Voluntary context switches: 2716
	Involuntary context switches: 73
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
