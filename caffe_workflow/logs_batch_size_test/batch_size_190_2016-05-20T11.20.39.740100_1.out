2805409
I0520 15:47:15.853238  7516 caffe.cpp:184] Using GPUs 0
I0520 15:47:16.272825  7516 solver.cpp:48] Initializing solver from parameters: 
test_iter: 789
test_interval: 1578
base_lr: 0.0025
display: 78
max_iter: 7894
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 789
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100.prototxt"
I0520 15:47:16.274420  7516 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100.prototxt
I0520 15:47:16.287902  7516 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 15:47:16.287968  7516 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 15:47:16.288312  7516 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 190
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:47:16.288489  7516 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:47:16.288512  7516 net.cpp:106] Creating Layer data_hdf5
I0520 15:47:16.288527  7516 net.cpp:411] data_hdf5 -> data
I0520 15:47:16.288568  7516 net.cpp:411] data_hdf5 -> label
I0520 15:47:16.288601  7516 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 15:47:16.289991  7516 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 15:47:16.308789  7516 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 15:47:37.839921  7516 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 15:47:37.845019  7516 net.cpp:150] Setting up data_hdf5
I0520 15:47:37.845059  7516 net.cpp:157] Top shape: 190 1 127 50 (1206500)
I0520 15:47:37.845074  7516 net.cpp:157] Top shape: 190 (190)
I0520 15:47:37.845088  7516 net.cpp:165] Memory required for data: 4826760
I0520 15:47:37.845100  7516 layer_factory.hpp:77] Creating layer conv1
I0520 15:47:37.845134  7516 net.cpp:106] Creating Layer conv1
I0520 15:47:37.845145  7516 net.cpp:454] conv1 <- data
I0520 15:47:37.845167  7516 net.cpp:411] conv1 -> conv1
I0520 15:47:39.116127  7516 net.cpp:150] Setting up conv1
I0520 15:47:39.116174  7516 net.cpp:157] Top shape: 190 12 120 48 (13132800)
I0520 15:47:39.116185  7516 net.cpp:165] Memory required for data: 57357960
I0520 15:47:39.116215  7516 layer_factory.hpp:77] Creating layer relu1
I0520 15:47:39.116235  7516 net.cpp:106] Creating Layer relu1
I0520 15:47:39.116246  7516 net.cpp:454] relu1 <- conv1
I0520 15:47:39.116261  7516 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:47:39.116785  7516 net.cpp:150] Setting up relu1
I0520 15:47:39.116801  7516 net.cpp:157] Top shape: 190 12 120 48 (13132800)
I0520 15:47:39.116811  7516 net.cpp:165] Memory required for data: 109889160
I0520 15:47:39.116822  7516 layer_factory.hpp:77] Creating layer pool1
I0520 15:47:39.116838  7516 net.cpp:106] Creating Layer pool1
I0520 15:47:39.116848  7516 net.cpp:454] pool1 <- conv1
I0520 15:47:39.116861  7516 net.cpp:411] pool1 -> pool1
I0520 15:47:39.116942  7516 net.cpp:150] Setting up pool1
I0520 15:47:39.116956  7516 net.cpp:157] Top shape: 190 12 60 48 (6566400)
I0520 15:47:39.116966  7516 net.cpp:165] Memory required for data: 136154760
I0520 15:47:39.116976  7516 layer_factory.hpp:77] Creating layer conv2
I0520 15:47:39.116998  7516 net.cpp:106] Creating Layer conv2
I0520 15:47:39.117009  7516 net.cpp:454] conv2 <- pool1
I0520 15:47:39.117022  7516 net.cpp:411] conv2 -> conv2
I0520 15:47:39.119709  7516 net.cpp:150] Setting up conv2
I0520 15:47:39.119736  7516 net.cpp:157] Top shape: 190 20 54 46 (9439200)
I0520 15:47:39.119747  7516 net.cpp:165] Memory required for data: 173911560
I0520 15:47:39.119766  7516 layer_factory.hpp:77] Creating layer relu2
I0520 15:47:39.119781  7516 net.cpp:106] Creating Layer relu2
I0520 15:47:39.119791  7516 net.cpp:454] relu2 <- conv2
I0520 15:47:39.119803  7516 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:47:39.120132  7516 net.cpp:150] Setting up relu2
I0520 15:47:39.120147  7516 net.cpp:157] Top shape: 190 20 54 46 (9439200)
I0520 15:47:39.120157  7516 net.cpp:165] Memory required for data: 211668360
I0520 15:47:39.120167  7516 layer_factory.hpp:77] Creating layer pool2
I0520 15:47:39.120179  7516 net.cpp:106] Creating Layer pool2
I0520 15:47:39.120189  7516 net.cpp:454] pool2 <- conv2
I0520 15:47:39.120214  7516 net.cpp:411] pool2 -> pool2
I0520 15:47:39.120283  7516 net.cpp:150] Setting up pool2
I0520 15:47:39.120296  7516 net.cpp:157] Top shape: 190 20 27 46 (4719600)
I0520 15:47:39.120306  7516 net.cpp:165] Memory required for data: 230546760
I0520 15:47:39.120316  7516 layer_factory.hpp:77] Creating layer conv3
I0520 15:47:39.120332  7516 net.cpp:106] Creating Layer conv3
I0520 15:47:39.120342  7516 net.cpp:454] conv3 <- pool2
I0520 15:47:39.120357  7516 net.cpp:411] conv3 -> conv3
I0520 15:47:39.122318  7516 net.cpp:150] Setting up conv3
I0520 15:47:39.122340  7516 net.cpp:157] Top shape: 190 28 22 44 (5149760)
I0520 15:47:39.122351  7516 net.cpp:165] Memory required for data: 251145800
I0520 15:47:39.122370  7516 layer_factory.hpp:77] Creating layer relu3
I0520 15:47:39.122386  7516 net.cpp:106] Creating Layer relu3
I0520 15:47:39.122396  7516 net.cpp:454] relu3 <- conv3
I0520 15:47:39.122408  7516 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:47:39.122875  7516 net.cpp:150] Setting up relu3
I0520 15:47:39.122892  7516 net.cpp:157] Top shape: 190 28 22 44 (5149760)
I0520 15:47:39.122902  7516 net.cpp:165] Memory required for data: 271744840
I0520 15:47:39.122912  7516 layer_factory.hpp:77] Creating layer pool3
I0520 15:47:39.122925  7516 net.cpp:106] Creating Layer pool3
I0520 15:47:39.122936  7516 net.cpp:454] pool3 <- conv3
I0520 15:47:39.122948  7516 net.cpp:411] pool3 -> pool3
I0520 15:47:39.123015  7516 net.cpp:150] Setting up pool3
I0520 15:47:39.123028  7516 net.cpp:157] Top shape: 190 28 11 44 (2574880)
I0520 15:47:39.123039  7516 net.cpp:165] Memory required for data: 282044360
I0520 15:47:39.123047  7516 layer_factory.hpp:77] Creating layer conv4
I0520 15:47:39.123064  7516 net.cpp:106] Creating Layer conv4
I0520 15:47:39.123075  7516 net.cpp:454] conv4 <- pool3
I0520 15:47:39.123088  7516 net.cpp:411] conv4 -> conv4
I0520 15:47:39.125818  7516 net.cpp:150] Setting up conv4
I0520 15:47:39.125844  7516 net.cpp:157] Top shape: 190 36 6 42 (1723680)
I0520 15:47:39.125855  7516 net.cpp:165] Memory required for data: 288939080
I0520 15:47:39.125871  7516 layer_factory.hpp:77] Creating layer relu4
I0520 15:47:39.125885  7516 net.cpp:106] Creating Layer relu4
I0520 15:47:39.125895  7516 net.cpp:454] relu4 <- conv4
I0520 15:47:39.125908  7516 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:47:39.126369  7516 net.cpp:150] Setting up relu4
I0520 15:47:39.126386  7516 net.cpp:157] Top shape: 190 36 6 42 (1723680)
I0520 15:47:39.126396  7516 net.cpp:165] Memory required for data: 295833800
I0520 15:47:39.126406  7516 layer_factory.hpp:77] Creating layer pool4
I0520 15:47:39.126420  7516 net.cpp:106] Creating Layer pool4
I0520 15:47:39.126428  7516 net.cpp:454] pool4 <- conv4
I0520 15:47:39.126441  7516 net.cpp:411] pool4 -> pool4
I0520 15:47:39.126509  7516 net.cpp:150] Setting up pool4
I0520 15:47:39.126523  7516 net.cpp:157] Top shape: 190 36 3 42 (861840)
I0520 15:47:39.126533  7516 net.cpp:165] Memory required for data: 299281160
I0520 15:47:39.126543  7516 layer_factory.hpp:77] Creating layer ip1
I0520 15:47:39.126562  7516 net.cpp:106] Creating Layer ip1
I0520 15:47:39.126572  7516 net.cpp:454] ip1 <- pool4
I0520 15:47:39.126585  7516 net.cpp:411] ip1 -> ip1
I0520 15:47:39.142024  7516 net.cpp:150] Setting up ip1
I0520 15:47:39.142050  7516 net.cpp:157] Top shape: 190 196 (37240)
I0520 15:47:39.142067  7516 net.cpp:165] Memory required for data: 299430120
I0520 15:47:39.142094  7516 layer_factory.hpp:77] Creating layer relu5
I0520 15:47:39.142109  7516 net.cpp:106] Creating Layer relu5
I0520 15:47:39.142119  7516 net.cpp:454] relu5 <- ip1
I0520 15:47:39.142132  7516 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:47:39.142475  7516 net.cpp:150] Setting up relu5
I0520 15:47:39.142488  7516 net.cpp:157] Top shape: 190 196 (37240)
I0520 15:47:39.142499  7516 net.cpp:165] Memory required for data: 299579080
I0520 15:47:39.142509  7516 layer_factory.hpp:77] Creating layer drop1
I0520 15:47:39.142530  7516 net.cpp:106] Creating Layer drop1
I0520 15:47:39.142540  7516 net.cpp:454] drop1 <- ip1
I0520 15:47:39.142565  7516 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:47:39.142612  7516 net.cpp:150] Setting up drop1
I0520 15:47:39.142627  7516 net.cpp:157] Top shape: 190 196 (37240)
I0520 15:47:39.142637  7516 net.cpp:165] Memory required for data: 299728040
I0520 15:47:39.142647  7516 layer_factory.hpp:77] Creating layer ip2
I0520 15:47:39.142664  7516 net.cpp:106] Creating Layer ip2
I0520 15:47:39.142674  7516 net.cpp:454] ip2 <- ip1
I0520 15:47:39.142688  7516 net.cpp:411] ip2 -> ip2
I0520 15:47:39.143147  7516 net.cpp:150] Setting up ip2
I0520 15:47:39.143162  7516 net.cpp:157] Top shape: 190 98 (18620)
I0520 15:47:39.143170  7516 net.cpp:165] Memory required for data: 299802520
I0520 15:47:39.143185  7516 layer_factory.hpp:77] Creating layer relu6
I0520 15:47:39.143198  7516 net.cpp:106] Creating Layer relu6
I0520 15:47:39.143208  7516 net.cpp:454] relu6 <- ip2
I0520 15:47:39.143219  7516 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:47:39.143735  7516 net.cpp:150] Setting up relu6
I0520 15:47:39.143751  7516 net.cpp:157] Top shape: 190 98 (18620)
I0520 15:47:39.143762  7516 net.cpp:165] Memory required for data: 299877000
I0520 15:47:39.143772  7516 layer_factory.hpp:77] Creating layer drop2
I0520 15:47:39.143785  7516 net.cpp:106] Creating Layer drop2
I0520 15:47:39.143795  7516 net.cpp:454] drop2 <- ip2
I0520 15:47:39.143807  7516 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:47:39.143849  7516 net.cpp:150] Setting up drop2
I0520 15:47:39.143862  7516 net.cpp:157] Top shape: 190 98 (18620)
I0520 15:47:39.143873  7516 net.cpp:165] Memory required for data: 299951480
I0520 15:47:39.143882  7516 layer_factory.hpp:77] Creating layer ip3
I0520 15:47:39.143896  7516 net.cpp:106] Creating Layer ip3
I0520 15:47:39.143906  7516 net.cpp:454] ip3 <- ip2
I0520 15:47:39.143918  7516 net.cpp:411] ip3 -> ip3
I0520 15:47:39.144129  7516 net.cpp:150] Setting up ip3
I0520 15:47:39.144142  7516 net.cpp:157] Top shape: 190 11 (2090)
I0520 15:47:39.144152  7516 net.cpp:165] Memory required for data: 299959840
I0520 15:47:39.144166  7516 layer_factory.hpp:77] Creating layer drop3
I0520 15:47:39.144179  7516 net.cpp:106] Creating Layer drop3
I0520 15:47:39.144189  7516 net.cpp:454] drop3 <- ip3
I0520 15:47:39.144201  7516 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:47:39.144240  7516 net.cpp:150] Setting up drop3
I0520 15:47:39.144253  7516 net.cpp:157] Top shape: 190 11 (2090)
I0520 15:47:39.144263  7516 net.cpp:165] Memory required for data: 299968200
I0520 15:47:39.144273  7516 layer_factory.hpp:77] Creating layer loss
I0520 15:47:39.144292  7516 net.cpp:106] Creating Layer loss
I0520 15:47:39.144301  7516 net.cpp:454] loss <- ip3
I0520 15:47:39.144312  7516 net.cpp:454] loss <- label
I0520 15:47:39.144325  7516 net.cpp:411] loss -> loss
I0520 15:47:39.144341  7516 layer_factory.hpp:77] Creating layer loss
I0520 15:47:39.144982  7516 net.cpp:150] Setting up loss
I0520 15:47:39.145002  7516 net.cpp:157] Top shape: (1)
I0520 15:47:39.145016  7516 net.cpp:160]     with loss weight 1
I0520 15:47:39.145058  7516 net.cpp:165] Memory required for data: 299968204
I0520 15:47:39.145068  7516 net.cpp:226] loss needs backward computation.
I0520 15:47:39.145079  7516 net.cpp:226] drop3 needs backward computation.
I0520 15:47:39.145089  7516 net.cpp:226] ip3 needs backward computation.
I0520 15:47:39.145100  7516 net.cpp:226] drop2 needs backward computation.
I0520 15:47:39.145109  7516 net.cpp:226] relu6 needs backward computation.
I0520 15:47:39.145119  7516 net.cpp:226] ip2 needs backward computation.
I0520 15:47:39.145129  7516 net.cpp:226] drop1 needs backward computation.
I0520 15:47:39.145139  7516 net.cpp:226] relu5 needs backward computation.
I0520 15:47:39.145148  7516 net.cpp:226] ip1 needs backward computation.
I0520 15:47:39.145159  7516 net.cpp:226] pool4 needs backward computation.
I0520 15:47:39.145169  7516 net.cpp:226] relu4 needs backward computation.
I0520 15:47:39.145179  7516 net.cpp:226] conv4 needs backward computation.
I0520 15:47:39.145190  7516 net.cpp:226] pool3 needs backward computation.
I0520 15:47:39.145200  7516 net.cpp:226] relu3 needs backward computation.
I0520 15:47:39.145218  7516 net.cpp:226] conv3 needs backward computation.
I0520 15:47:39.145231  7516 net.cpp:226] pool2 needs backward computation.
I0520 15:47:39.145241  7516 net.cpp:226] relu2 needs backward computation.
I0520 15:47:39.145251  7516 net.cpp:226] conv2 needs backward computation.
I0520 15:47:39.145262  7516 net.cpp:226] pool1 needs backward computation.
I0520 15:47:39.145272  7516 net.cpp:226] relu1 needs backward computation.
I0520 15:47:39.145282  7516 net.cpp:226] conv1 needs backward computation.
I0520 15:47:39.145292  7516 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:47:39.145301  7516 net.cpp:270] This network produces output loss
I0520 15:47:39.145325  7516 net.cpp:283] Network initialization done.
I0520 15:47:39.146920  7516 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100.prototxt
I0520 15:47:39.146991  7516 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 15:47:39.147346  7516 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 190
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 15:47:39.147534  7516 layer_factory.hpp:77] Creating layer data_hdf5
I0520 15:47:39.147550  7516 net.cpp:106] Creating Layer data_hdf5
I0520 15:47:39.147563  7516 net.cpp:411] data_hdf5 -> data
I0520 15:47:39.147578  7516 net.cpp:411] data_hdf5 -> label
I0520 15:47:39.147594  7516 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 15:47:39.168182  7516 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 15:48:00.434682  7516 net.cpp:150] Setting up data_hdf5
I0520 15:48:00.434849  7516 net.cpp:157] Top shape: 190 1 127 50 (1206500)
I0520 15:48:00.434862  7516 net.cpp:157] Top shape: 190 (190)
I0520 15:48:00.434875  7516 net.cpp:165] Memory required for data: 4826760
I0520 15:48:00.434888  7516 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 15:48:00.434916  7516 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 15:48:00.434927  7516 net.cpp:454] label_data_hdf5_1_split <- label
I0520 15:48:00.434942  7516 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 15:48:00.434963  7516 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 15:48:00.435036  7516 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 15:48:00.435050  7516 net.cpp:157] Top shape: 190 (190)
I0520 15:48:00.435061  7516 net.cpp:157] Top shape: 190 (190)
I0520 15:48:00.435071  7516 net.cpp:165] Memory required for data: 4828280
I0520 15:48:00.435081  7516 layer_factory.hpp:77] Creating layer conv1
I0520 15:48:00.435101  7516 net.cpp:106] Creating Layer conv1
I0520 15:48:00.435112  7516 net.cpp:454] conv1 <- data
I0520 15:48:00.435127  7516 net.cpp:411] conv1 -> conv1
I0520 15:48:00.437077  7516 net.cpp:150] Setting up conv1
I0520 15:48:00.437099  7516 net.cpp:157] Top shape: 190 12 120 48 (13132800)
I0520 15:48:00.437113  7516 net.cpp:165] Memory required for data: 57359480
I0520 15:48:00.437132  7516 layer_factory.hpp:77] Creating layer relu1
I0520 15:48:00.437149  7516 net.cpp:106] Creating Layer relu1
I0520 15:48:00.437158  7516 net.cpp:454] relu1 <- conv1
I0520 15:48:00.437170  7516 net.cpp:397] relu1 -> conv1 (in-place)
I0520 15:48:00.437671  7516 net.cpp:150] Setting up relu1
I0520 15:48:00.437688  7516 net.cpp:157] Top shape: 190 12 120 48 (13132800)
I0520 15:48:00.437698  7516 net.cpp:165] Memory required for data: 109890680
I0520 15:48:00.437710  7516 layer_factory.hpp:77] Creating layer pool1
I0520 15:48:00.437726  7516 net.cpp:106] Creating Layer pool1
I0520 15:48:00.437734  7516 net.cpp:454] pool1 <- conv1
I0520 15:48:00.437747  7516 net.cpp:411] pool1 -> pool1
I0520 15:48:00.437822  7516 net.cpp:150] Setting up pool1
I0520 15:48:00.437835  7516 net.cpp:157] Top shape: 190 12 60 48 (6566400)
I0520 15:48:00.437845  7516 net.cpp:165] Memory required for data: 136156280
I0520 15:48:00.437857  7516 layer_factory.hpp:77] Creating layer conv2
I0520 15:48:00.437875  7516 net.cpp:106] Creating Layer conv2
I0520 15:48:00.437885  7516 net.cpp:454] conv2 <- pool1
I0520 15:48:00.437901  7516 net.cpp:411] conv2 -> conv2
I0520 15:48:00.439815  7516 net.cpp:150] Setting up conv2
I0520 15:48:00.439837  7516 net.cpp:157] Top shape: 190 20 54 46 (9439200)
I0520 15:48:00.439849  7516 net.cpp:165] Memory required for data: 173913080
I0520 15:48:00.439867  7516 layer_factory.hpp:77] Creating layer relu2
I0520 15:48:00.439880  7516 net.cpp:106] Creating Layer relu2
I0520 15:48:00.439890  7516 net.cpp:454] relu2 <- conv2
I0520 15:48:00.439903  7516 net.cpp:397] relu2 -> conv2 (in-place)
I0520 15:48:00.440234  7516 net.cpp:150] Setting up relu2
I0520 15:48:00.440248  7516 net.cpp:157] Top shape: 190 20 54 46 (9439200)
I0520 15:48:00.440258  7516 net.cpp:165] Memory required for data: 211669880
I0520 15:48:00.440268  7516 layer_factory.hpp:77] Creating layer pool2
I0520 15:48:00.440281  7516 net.cpp:106] Creating Layer pool2
I0520 15:48:00.440291  7516 net.cpp:454] pool2 <- conv2
I0520 15:48:00.440304  7516 net.cpp:411] pool2 -> pool2
I0520 15:48:00.440376  7516 net.cpp:150] Setting up pool2
I0520 15:48:00.440389  7516 net.cpp:157] Top shape: 190 20 27 46 (4719600)
I0520 15:48:00.440398  7516 net.cpp:165] Memory required for data: 230548280
I0520 15:48:00.440408  7516 layer_factory.hpp:77] Creating layer conv3
I0520 15:48:00.440425  7516 net.cpp:106] Creating Layer conv3
I0520 15:48:00.440436  7516 net.cpp:454] conv3 <- pool2
I0520 15:48:00.440450  7516 net.cpp:411] conv3 -> conv3
I0520 15:48:00.442505  7516 net.cpp:150] Setting up conv3
I0520 15:48:00.442524  7516 net.cpp:157] Top shape: 190 28 22 44 (5149760)
I0520 15:48:00.442534  7516 net.cpp:165] Memory required for data: 251147320
I0520 15:48:00.442567  7516 layer_factory.hpp:77] Creating layer relu3
I0520 15:48:00.442581  7516 net.cpp:106] Creating Layer relu3
I0520 15:48:00.442592  7516 net.cpp:454] relu3 <- conv3
I0520 15:48:00.442605  7516 net.cpp:397] relu3 -> conv3 (in-place)
I0520 15:48:00.443078  7516 net.cpp:150] Setting up relu3
I0520 15:48:00.443094  7516 net.cpp:157] Top shape: 190 28 22 44 (5149760)
I0520 15:48:00.443104  7516 net.cpp:165] Memory required for data: 271746360
I0520 15:48:00.443114  7516 layer_factory.hpp:77] Creating layer pool3
I0520 15:48:00.443126  7516 net.cpp:106] Creating Layer pool3
I0520 15:48:00.443136  7516 net.cpp:454] pool3 <- conv3
I0520 15:48:00.443150  7516 net.cpp:411] pool3 -> pool3
I0520 15:48:00.443222  7516 net.cpp:150] Setting up pool3
I0520 15:48:00.443234  7516 net.cpp:157] Top shape: 190 28 11 44 (2574880)
I0520 15:48:00.443244  7516 net.cpp:165] Memory required for data: 282045880
I0520 15:48:00.443254  7516 layer_factory.hpp:77] Creating layer conv4
I0520 15:48:00.443269  7516 net.cpp:106] Creating Layer conv4
I0520 15:48:00.443280  7516 net.cpp:454] conv4 <- pool3
I0520 15:48:00.443295  7516 net.cpp:411] conv4 -> conv4
I0520 15:48:00.445356  7516 net.cpp:150] Setting up conv4
I0520 15:48:00.445377  7516 net.cpp:157] Top shape: 190 36 6 42 (1723680)
I0520 15:48:00.445391  7516 net.cpp:165] Memory required for data: 288940600
I0520 15:48:00.445406  7516 layer_factory.hpp:77] Creating layer relu4
I0520 15:48:00.445420  7516 net.cpp:106] Creating Layer relu4
I0520 15:48:00.445430  7516 net.cpp:454] relu4 <- conv4
I0520 15:48:00.445442  7516 net.cpp:397] relu4 -> conv4 (in-place)
I0520 15:48:00.445912  7516 net.cpp:150] Setting up relu4
I0520 15:48:00.445929  7516 net.cpp:157] Top shape: 190 36 6 42 (1723680)
I0520 15:48:00.445938  7516 net.cpp:165] Memory required for data: 295835320
I0520 15:48:00.445948  7516 layer_factory.hpp:77] Creating layer pool4
I0520 15:48:00.445961  7516 net.cpp:106] Creating Layer pool4
I0520 15:48:00.445971  7516 net.cpp:454] pool4 <- conv4
I0520 15:48:00.445984  7516 net.cpp:411] pool4 -> pool4
I0520 15:48:00.446058  7516 net.cpp:150] Setting up pool4
I0520 15:48:00.446074  7516 net.cpp:157] Top shape: 190 36 3 42 (861840)
I0520 15:48:00.446085  7516 net.cpp:165] Memory required for data: 299282680
I0520 15:48:00.446096  7516 layer_factory.hpp:77] Creating layer ip1
I0520 15:48:00.446111  7516 net.cpp:106] Creating Layer ip1
I0520 15:48:00.446121  7516 net.cpp:454] ip1 <- pool4
I0520 15:48:00.446135  7516 net.cpp:411] ip1 -> ip1
I0520 15:48:00.461573  7516 net.cpp:150] Setting up ip1
I0520 15:48:00.461602  7516 net.cpp:157] Top shape: 190 196 (37240)
I0520 15:48:00.461613  7516 net.cpp:165] Memory required for data: 299431640
I0520 15:48:00.461635  7516 layer_factory.hpp:77] Creating layer relu5
I0520 15:48:00.461650  7516 net.cpp:106] Creating Layer relu5
I0520 15:48:00.461661  7516 net.cpp:454] relu5 <- ip1
I0520 15:48:00.461674  7516 net.cpp:397] relu5 -> ip1 (in-place)
I0520 15:48:00.462019  7516 net.cpp:150] Setting up relu5
I0520 15:48:00.462033  7516 net.cpp:157] Top shape: 190 196 (37240)
I0520 15:48:00.462044  7516 net.cpp:165] Memory required for data: 299580600
I0520 15:48:00.462057  7516 layer_factory.hpp:77] Creating layer drop1
I0520 15:48:00.462080  7516 net.cpp:106] Creating Layer drop1
I0520 15:48:00.462090  7516 net.cpp:454] drop1 <- ip1
I0520 15:48:00.462103  7516 net.cpp:397] drop1 -> ip1 (in-place)
I0520 15:48:00.462148  7516 net.cpp:150] Setting up drop1
I0520 15:48:00.462162  7516 net.cpp:157] Top shape: 190 196 (37240)
I0520 15:48:00.462172  7516 net.cpp:165] Memory required for data: 299729560
I0520 15:48:00.462182  7516 layer_factory.hpp:77] Creating layer ip2
I0520 15:48:00.462195  7516 net.cpp:106] Creating Layer ip2
I0520 15:48:00.462205  7516 net.cpp:454] ip2 <- ip1
I0520 15:48:00.462219  7516 net.cpp:411] ip2 -> ip2
I0520 15:48:00.462695  7516 net.cpp:150] Setting up ip2
I0520 15:48:00.462709  7516 net.cpp:157] Top shape: 190 98 (18620)
I0520 15:48:00.462719  7516 net.cpp:165] Memory required for data: 299804040
I0520 15:48:00.462749  7516 layer_factory.hpp:77] Creating layer relu6
I0520 15:48:00.462761  7516 net.cpp:106] Creating Layer relu6
I0520 15:48:00.462771  7516 net.cpp:454] relu6 <- ip2
I0520 15:48:00.462784  7516 net.cpp:397] relu6 -> ip2 (in-place)
I0520 15:48:00.463316  7516 net.cpp:150] Setting up relu6
I0520 15:48:00.463337  7516 net.cpp:157] Top shape: 190 98 (18620)
I0520 15:48:00.463347  7516 net.cpp:165] Memory required for data: 299878520
I0520 15:48:00.463358  7516 layer_factory.hpp:77] Creating layer drop2
I0520 15:48:00.463371  7516 net.cpp:106] Creating Layer drop2
I0520 15:48:00.463382  7516 net.cpp:454] drop2 <- ip2
I0520 15:48:00.463394  7516 net.cpp:397] drop2 -> ip2 (in-place)
I0520 15:48:00.463438  7516 net.cpp:150] Setting up drop2
I0520 15:48:00.463451  7516 net.cpp:157] Top shape: 190 98 (18620)
I0520 15:48:00.463461  7516 net.cpp:165] Memory required for data: 299953000
I0520 15:48:00.463471  7516 layer_factory.hpp:77] Creating layer ip3
I0520 15:48:00.463485  7516 net.cpp:106] Creating Layer ip3
I0520 15:48:00.463495  7516 net.cpp:454] ip3 <- ip2
I0520 15:48:00.463510  7516 net.cpp:411] ip3 -> ip3
I0520 15:48:00.463729  7516 net.cpp:150] Setting up ip3
I0520 15:48:00.463742  7516 net.cpp:157] Top shape: 190 11 (2090)
I0520 15:48:00.463752  7516 net.cpp:165] Memory required for data: 299961360
I0520 15:48:00.463768  7516 layer_factory.hpp:77] Creating layer drop3
I0520 15:48:00.463781  7516 net.cpp:106] Creating Layer drop3
I0520 15:48:00.463791  7516 net.cpp:454] drop3 <- ip3
I0520 15:48:00.463804  7516 net.cpp:397] drop3 -> ip3 (in-place)
I0520 15:48:00.463845  7516 net.cpp:150] Setting up drop3
I0520 15:48:00.463858  7516 net.cpp:157] Top shape: 190 11 (2090)
I0520 15:48:00.463867  7516 net.cpp:165] Memory required for data: 299969720
I0520 15:48:00.463877  7516 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 15:48:00.463891  7516 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 15:48:00.463901  7516 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 15:48:00.463914  7516 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 15:48:00.463929  7516 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 15:48:00.464001  7516 net.cpp:150] Setting up ip3_drop3_0_split
I0520 15:48:00.464015  7516 net.cpp:157] Top shape: 190 11 (2090)
I0520 15:48:00.464027  7516 net.cpp:157] Top shape: 190 11 (2090)
I0520 15:48:00.464037  7516 net.cpp:165] Memory required for data: 299986440
I0520 15:48:00.464047  7516 layer_factory.hpp:77] Creating layer accuracy
I0520 15:48:00.464068  7516 net.cpp:106] Creating Layer accuracy
I0520 15:48:00.464078  7516 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 15:48:00.464090  7516 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 15:48:00.464103  7516 net.cpp:411] accuracy -> accuracy
I0520 15:48:00.464126  7516 net.cpp:150] Setting up accuracy
I0520 15:48:00.464139  7516 net.cpp:157] Top shape: (1)
I0520 15:48:00.464149  7516 net.cpp:165] Memory required for data: 299986444
I0520 15:48:00.464159  7516 layer_factory.hpp:77] Creating layer loss
I0520 15:48:00.464174  7516 net.cpp:106] Creating Layer loss
I0520 15:48:00.464184  7516 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 15:48:00.464195  7516 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 15:48:00.464207  7516 net.cpp:411] loss -> loss
I0520 15:48:00.464226  7516 layer_factory.hpp:77] Creating layer loss
I0520 15:48:00.464715  7516 net.cpp:150] Setting up loss
I0520 15:48:00.464728  7516 net.cpp:157] Top shape: (1)
I0520 15:48:00.464738  7516 net.cpp:160]     with loss weight 1
I0520 15:48:00.464756  7516 net.cpp:165] Memory required for data: 299986448
I0520 15:48:00.464766  7516 net.cpp:226] loss needs backward computation.
I0520 15:48:00.464778  7516 net.cpp:228] accuracy does not need backward computation.
I0520 15:48:00.464788  7516 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 15:48:00.464798  7516 net.cpp:226] drop3 needs backward computation.
I0520 15:48:00.464809  7516 net.cpp:226] ip3 needs backward computation.
I0520 15:48:00.464819  7516 net.cpp:226] drop2 needs backward computation.
I0520 15:48:00.464838  7516 net.cpp:226] relu6 needs backward computation.
I0520 15:48:00.464848  7516 net.cpp:226] ip2 needs backward computation.
I0520 15:48:00.464857  7516 net.cpp:226] drop1 needs backward computation.
I0520 15:48:00.464867  7516 net.cpp:226] relu5 needs backward computation.
I0520 15:48:00.464876  7516 net.cpp:226] ip1 needs backward computation.
I0520 15:48:00.464886  7516 net.cpp:226] pool4 needs backward computation.
I0520 15:48:00.464896  7516 net.cpp:226] relu4 needs backward computation.
I0520 15:48:00.464906  7516 net.cpp:226] conv4 needs backward computation.
I0520 15:48:00.464917  7516 net.cpp:226] pool3 needs backward computation.
I0520 15:48:00.464927  7516 net.cpp:226] relu3 needs backward computation.
I0520 15:48:00.464938  7516 net.cpp:226] conv3 needs backward computation.
I0520 15:48:00.464948  7516 net.cpp:226] pool2 needs backward computation.
I0520 15:48:00.464959  7516 net.cpp:226] relu2 needs backward computation.
I0520 15:48:00.464968  7516 net.cpp:226] conv2 needs backward computation.
I0520 15:48:00.464979  7516 net.cpp:226] pool1 needs backward computation.
I0520 15:48:00.464989  7516 net.cpp:226] relu1 needs backward computation.
I0520 15:48:00.464998  7516 net.cpp:226] conv1 needs backward computation.
I0520 15:48:00.465010  7516 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 15:48:00.465021  7516 net.cpp:228] data_hdf5 does not need backward computation.
I0520 15:48:00.465031  7516 net.cpp:270] This network produces output accuracy
I0520 15:48:00.465041  7516 net.cpp:270] This network produces output loss
I0520 15:48:00.465068  7516 net.cpp:283] Network initialization done.
I0520 15:48:00.465203  7516 solver.cpp:60] Solver scaffolding done.
I0520 15:48:00.466331  7516 caffe.cpp:212] Starting Optimization
I0520 15:48:00.466349  7516 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 15:48:00.466359  7516 solver.cpp:289] Learning Rate Policy: fixed
I0520 15:48:00.467582  7516 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 15:48:47.234501  7516 solver.cpp:409]     Test net output #0: accuracy = 0.0902675
I0520 15:48:47.234660  7516 solver.cpp:409]     Test net output #1: loss = 2.39732 (* 1 = 2.39732 loss)
I0520 15:48:47.282479  7516 solver.cpp:237] Iteration 0, loss = 2.39843
I0520 15:48:47.282515  7516 solver.cpp:253]     Train net output #0: loss = 2.39843 (* 1 = 2.39843 loss)
I0520 15:48:47.282532  7516 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 15:48:55.475249  7516 solver.cpp:237] Iteration 78, loss = 2.33337
I0520 15:48:55.475284  7516 solver.cpp:253]     Train net output #0: loss = 2.33337 (* 1 = 2.33337 loss)
I0520 15:48:55.475298  7516 sgd_solver.cpp:106] Iteration 78, lr = 0.0025
I0520 15:49:03.666369  7516 solver.cpp:237] Iteration 156, loss = 2.31959
I0520 15:49:03.666404  7516 solver.cpp:253]     Train net output #0: loss = 2.31959 (* 1 = 2.31959 loss)
I0520 15:49:03.666419  7516 sgd_solver.cpp:106] Iteration 156, lr = 0.0025
I0520 15:49:11.860956  7516 solver.cpp:237] Iteration 234, loss = 2.29171
I0520 15:49:11.860993  7516 solver.cpp:253]     Train net output #0: loss = 2.29171 (* 1 = 2.29171 loss)
I0520 15:49:11.861016  7516 sgd_solver.cpp:106] Iteration 234, lr = 0.0025
I0520 15:49:20.050029  7516 solver.cpp:237] Iteration 312, loss = 2.26451
I0520 15:49:20.050174  7516 solver.cpp:253]     Train net output #0: loss = 2.26451 (* 1 = 2.26451 loss)
I0520 15:49:20.050189  7516 sgd_solver.cpp:106] Iteration 312, lr = 0.0025
I0520 15:49:28.243103  7516 solver.cpp:237] Iteration 390, loss = 2.10777
I0520 15:49:28.243137  7516 solver.cpp:253]     Train net output #0: loss = 2.10777 (* 1 = 2.10777 loss)
I0520 15:49:28.243155  7516 sgd_solver.cpp:106] Iteration 390, lr = 0.0025
I0520 15:49:36.430400  7516 solver.cpp:237] Iteration 468, loss = 2.08132
I0520 15:49:36.430433  7516 solver.cpp:253]     Train net output #0: loss = 2.08132 (* 1 = 2.08132 loss)
I0520 15:49:36.430451  7516 sgd_solver.cpp:106] Iteration 468, lr = 0.0025
I0520 15:50:06.741207  7516 solver.cpp:237] Iteration 546, loss = 1.96702
I0520 15:50:06.741370  7516 solver.cpp:253]     Train net output #0: loss = 1.96702 (* 1 = 1.96702 loss)
I0520 15:50:06.741386  7516 sgd_solver.cpp:106] Iteration 546, lr = 0.0025
I0520 15:50:14.938922  7516 solver.cpp:237] Iteration 624, loss = 1.98319
I0520 15:50:14.938956  7516 solver.cpp:253]     Train net output #0: loss = 1.98319 (* 1 = 1.98319 loss)
I0520 15:50:14.938973  7516 sgd_solver.cpp:106] Iteration 624, lr = 0.0025
I0520 15:50:23.122841  7516 solver.cpp:237] Iteration 702, loss = 1.92533
I0520 15:50:23.122876  7516 solver.cpp:253]     Train net output #0: loss = 1.92533 (* 1 = 1.92533 loss)
I0520 15:50:23.122892  7516 sgd_solver.cpp:106] Iteration 702, lr = 0.0025
I0520 15:50:31.312450  7516 solver.cpp:237] Iteration 780, loss = 1.82064
I0520 15:50:31.312489  7516 solver.cpp:253]     Train net output #0: loss = 1.82064 (* 1 = 1.82064 loss)
I0520 15:50:31.312506  7516 sgd_solver.cpp:106] Iteration 780, lr = 0.0025
I0520 15:50:32.152575  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_789.caffemodel
I0520 15:50:32.267256  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_789.solverstate
I0520 15:50:39.564007  7516 solver.cpp:237] Iteration 858, loss = 1.96979
I0520 15:50:39.564163  7516 solver.cpp:253]     Train net output #0: loss = 1.96979 (* 1 = 1.96979 loss)
I0520 15:50:39.564177  7516 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0520 15:50:47.753962  7516 solver.cpp:237] Iteration 936, loss = 1.76481
I0520 15:50:47.753996  7516 solver.cpp:253]     Train net output #0: loss = 1.76481 (* 1 = 1.76481 loss)
I0520 15:50:47.754009  7516 sgd_solver.cpp:106] Iteration 936, lr = 0.0025
I0520 15:50:55.953222  7516 solver.cpp:237] Iteration 1014, loss = 1.96136
I0520 15:50:55.953269  7516 solver.cpp:253]     Train net output #0: loss = 1.96136 (* 1 = 1.96136 loss)
I0520 15:50:55.953282  7516 sgd_solver.cpp:106] Iteration 1014, lr = 0.0025
I0520 15:51:26.250505  7516 solver.cpp:237] Iteration 1092, loss = 1.8345
I0520 15:51:26.250658  7516 solver.cpp:253]     Train net output #0: loss = 1.8345 (* 1 = 1.8345 loss)
I0520 15:51:26.250672  7516 sgd_solver.cpp:106] Iteration 1092, lr = 0.0025
I0520 15:51:34.440102  7516 solver.cpp:237] Iteration 1170, loss = 1.75426
I0520 15:51:34.440137  7516 solver.cpp:253]     Train net output #0: loss = 1.75426 (* 1 = 1.75426 loss)
I0520 15:51:34.440153  7516 sgd_solver.cpp:106] Iteration 1170, lr = 0.0025
I0520 15:51:42.632702  7516 solver.cpp:237] Iteration 1248, loss = 1.87538
I0520 15:51:42.632748  7516 solver.cpp:253]     Train net output #0: loss = 1.87538 (* 1 = 1.87538 loss)
I0520 15:51:42.632766  7516 sgd_solver.cpp:106] Iteration 1248, lr = 0.0025
I0520 15:51:50.832628  7516 solver.cpp:237] Iteration 1326, loss = 1.81695
I0520 15:51:50.832662  7516 solver.cpp:253]     Train net output #0: loss = 1.81695 (* 1 = 1.81695 loss)
I0520 15:51:50.832675  7516 sgd_solver.cpp:106] Iteration 1326, lr = 0.0025
I0520 15:51:59.030278  7516 solver.cpp:237] Iteration 1404, loss = 1.69224
I0520 15:51:59.030427  7516 solver.cpp:253]     Train net output #0: loss = 1.69224 (* 1 = 1.69224 loss)
I0520 15:51:59.030441  7516 sgd_solver.cpp:106] Iteration 1404, lr = 0.0025
I0520 15:52:07.225666  7516 solver.cpp:237] Iteration 1482, loss = 1.77374
I0520 15:52:07.225704  7516 solver.cpp:253]     Train net output #0: loss = 1.77374 (* 1 = 1.77374 loss)
I0520 15:52:07.225723  7516 sgd_solver.cpp:106] Iteration 1482, lr = 0.0025
I0520 15:52:15.419540  7516 solver.cpp:237] Iteration 1560, loss = 1.63301
I0520 15:52:15.419574  7516 solver.cpp:253]     Train net output #0: loss = 1.63301 (* 1 = 1.63301 loss)
I0520 15:52:15.419591  7516 sgd_solver.cpp:106] Iteration 1560, lr = 0.0025
I0520 15:52:17.204236  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_1578.caffemodel
I0520 15:52:17.316272  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_1578.solverstate
I0520 15:52:17.343572  7516 solver.cpp:341] Iteration 1578, Testing net (#0)
I0520 15:53:03.143767  7516 solver.cpp:409]     Test net output #0: accuracy = 0.644687
I0520 15:53:03.143926  7516 solver.cpp:409]     Test net output #1: loss = 1.22201 (* 1 = 1.22201 loss)
I0520 15:53:31.542503  7516 solver.cpp:237] Iteration 1638, loss = 1.78383
I0520 15:53:31.542559  7516 solver.cpp:253]     Train net output #0: loss = 1.78383 (* 1 = 1.78383 loss)
I0520 15:53:31.542573  7516 sgd_solver.cpp:106] Iteration 1638, lr = 0.0025
I0520 15:53:39.727849  7516 solver.cpp:237] Iteration 1716, loss = 1.76594
I0520 15:53:39.727985  7516 solver.cpp:253]     Train net output #0: loss = 1.76594 (* 1 = 1.76594 loss)
I0520 15:53:39.727999  7516 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0520 15:53:47.917186  7516 solver.cpp:237] Iteration 1794, loss = 1.70441
I0520 15:53:47.917228  7516 solver.cpp:253]     Train net output #0: loss = 1.70441 (* 1 = 1.70441 loss)
I0520 15:53:47.917244  7516 sgd_solver.cpp:106] Iteration 1794, lr = 0.0025
I0520 15:53:56.107710  7516 solver.cpp:237] Iteration 1872, loss = 1.71095
I0520 15:53:56.107744  7516 solver.cpp:253]     Train net output #0: loss = 1.71095 (* 1 = 1.71095 loss)
I0520 15:53:56.107761  7516 sgd_solver.cpp:106] Iteration 1872, lr = 0.0025
I0520 15:54:04.305148  7516 solver.cpp:237] Iteration 1950, loss = 1.5272
I0520 15:54:04.305182  7516 solver.cpp:253]     Train net output #0: loss = 1.5272 (* 1 = 1.5272 loss)
I0520 15:54:04.305198  7516 sgd_solver.cpp:106] Iteration 1950, lr = 0.0025
I0520 15:54:12.501598  7516 solver.cpp:237] Iteration 2028, loss = 1.68384
I0520 15:54:12.501741  7516 solver.cpp:253]     Train net output #0: loss = 1.68384 (* 1 = 1.68384 loss)
I0520 15:54:12.501755  7516 sgd_solver.cpp:106] Iteration 2028, lr = 0.0025
I0520 15:54:42.747833  7516 solver.cpp:237] Iteration 2106, loss = 1.71651
I0520 15:54:42.748003  7516 solver.cpp:253]     Train net output #0: loss = 1.71651 (* 1 = 1.71651 loss)
I0520 15:54:42.748018  7516 sgd_solver.cpp:106] Iteration 2106, lr = 0.0025
I0520 15:54:50.941581  7516 solver.cpp:237] Iteration 2184, loss = 1.78277
I0520 15:54:50.941617  7516 solver.cpp:253]     Train net output #0: loss = 1.78277 (* 1 = 1.78277 loss)
I0520 15:54:50.941633  7516 sgd_solver.cpp:106] Iteration 2184, lr = 0.0025
I0520 15:54:59.139120  7516 solver.cpp:237] Iteration 2262, loss = 1.69639
I0520 15:54:59.139155  7516 solver.cpp:253]     Train net output #0: loss = 1.69639 (* 1 = 1.69639 loss)
I0520 15:54:59.139168  7516 sgd_solver.cpp:106] Iteration 2262, lr = 0.0025
I0520 15:55:07.318058  7516 solver.cpp:237] Iteration 2340, loss = 1.68329
I0520 15:55:07.318099  7516 solver.cpp:253]     Train net output #0: loss = 1.68329 (* 1 = 1.68329 loss)
I0520 15:55:07.318114  7516 sgd_solver.cpp:106] Iteration 2340, lr = 0.0025
I0520 15:55:10.045837  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_2367.caffemodel
I0520 15:55:10.158236  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_2367.solverstate
I0520 15:55:15.566386  7516 solver.cpp:237] Iteration 2418, loss = 1.83429
I0520 15:55:15.566550  7516 solver.cpp:253]     Train net output #0: loss = 1.83429 (* 1 = 1.83429 loss)
I0520 15:55:15.566565  7516 sgd_solver.cpp:106] Iteration 2418, lr = 0.0025
I0520 15:55:23.756320  7516 solver.cpp:237] Iteration 2496, loss = 1.56696
I0520 15:55:23.756356  7516 solver.cpp:253]     Train net output #0: loss = 1.56696 (* 1 = 1.56696 loss)
I0520 15:55:23.756372  7516 sgd_solver.cpp:106] Iteration 2496, lr = 0.0025
I0520 15:55:31.945940  7516 solver.cpp:237] Iteration 2574, loss = 1.64479
I0520 15:55:31.945979  7516 solver.cpp:253]     Train net output #0: loss = 1.64479 (* 1 = 1.64479 loss)
I0520 15:55:31.945997  7516 sgd_solver.cpp:106] Iteration 2574, lr = 0.0025
I0520 15:56:02.218731  7516 solver.cpp:237] Iteration 2652, loss = 1.63833
I0520 15:56:02.218895  7516 solver.cpp:253]     Train net output #0: loss = 1.63833 (* 1 = 1.63833 loss)
I0520 15:56:02.218911  7516 sgd_solver.cpp:106] Iteration 2652, lr = 0.0025
I0520 15:56:10.410624  7516 solver.cpp:237] Iteration 2730, loss = 1.65342
I0520 15:56:10.410657  7516 solver.cpp:253]     Train net output #0: loss = 1.65342 (* 1 = 1.65342 loss)
I0520 15:56:10.410675  7516 sgd_solver.cpp:106] Iteration 2730, lr = 0.0025
I0520 15:56:18.608108  7516 solver.cpp:237] Iteration 2808, loss = 1.63744
I0520 15:56:18.608150  7516 solver.cpp:253]     Train net output #0: loss = 1.63744 (* 1 = 1.63744 loss)
I0520 15:56:18.608167  7516 sgd_solver.cpp:106] Iteration 2808, lr = 0.0025
I0520 15:56:26.806855  7516 solver.cpp:237] Iteration 2886, loss = 1.647
I0520 15:56:26.806888  7516 solver.cpp:253]     Train net output #0: loss = 1.647 (* 1 = 1.647 loss)
I0520 15:56:26.806905  7516 sgd_solver.cpp:106] Iteration 2886, lr = 0.0025
I0520 15:56:35.002023  7516 solver.cpp:237] Iteration 2964, loss = 1.49418
I0520 15:56:35.002161  7516 solver.cpp:253]     Train net output #0: loss = 1.49418 (* 1 = 1.49418 loss)
I0520 15:56:35.002177  7516 sgd_solver.cpp:106] Iteration 2964, lr = 0.0025
I0520 15:56:43.186022  7516 solver.cpp:237] Iteration 3042, loss = 1.49467
I0520 15:56:43.186060  7516 solver.cpp:253]     Train net output #0: loss = 1.49467 (* 1 = 1.49467 loss)
I0520 15:56:43.186077  7516 sgd_solver.cpp:106] Iteration 3042, lr = 0.0025
I0520 15:56:51.376485  7516 solver.cpp:237] Iteration 3120, loss = 1.56404
I0520 15:56:51.376520  7516 solver.cpp:253]     Train net output #0: loss = 1.56404 (* 1 = 1.56404 loss)
I0520 15:56:51.376538  7516 sgd_solver.cpp:106] Iteration 3120, lr = 0.0025
I0520 15:56:55.058975  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_3156.caffemodel
I0520 15:56:55.172411  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_3156.solverstate
I0520 15:56:55.200466  7516 solver.cpp:341] Iteration 3156, Testing net (#0)
I0520 15:58:01.850141  7516 solver.cpp:409]     Test net output #0: accuracy = 0.701988
I0520 15:58:01.850309  7516 solver.cpp:409]     Test net output #1: loss = 1.0177 (* 1 = 1.0177 loss)
I0520 15:58:28.413039  7516 solver.cpp:237] Iteration 3198, loss = 1.50564
I0520 15:58:28.413089  7516 solver.cpp:253]     Train net output #0: loss = 1.50564 (* 1 = 1.50564 loss)
I0520 15:58:28.413105  7516 sgd_solver.cpp:106] Iteration 3198, lr = 0.0025
I0520 15:58:36.599684  7516 solver.cpp:237] Iteration 3276, loss = 1.5825
I0520 15:58:36.599830  7516 solver.cpp:253]     Train net output #0: loss = 1.5825 (* 1 = 1.5825 loss)
I0520 15:58:36.599845  7516 sgd_solver.cpp:106] Iteration 3276, lr = 0.0025
I0520 15:58:44.794509  7516 solver.cpp:237] Iteration 3354, loss = 1.55575
I0520 15:58:44.794543  7516 solver.cpp:253]     Train net output #0: loss = 1.55575 (* 1 = 1.55575 loss)
I0520 15:58:44.794559  7516 sgd_solver.cpp:106] Iteration 3354, lr = 0.0025
I0520 15:58:52.980896  7516 solver.cpp:237] Iteration 3432, loss = 1.51135
I0520 15:58:52.980936  7516 solver.cpp:253]     Train net output #0: loss = 1.51135 (* 1 = 1.51135 loss)
I0520 15:58:52.980957  7516 sgd_solver.cpp:106] Iteration 3432, lr = 0.0025
I0520 15:59:01.169981  7516 solver.cpp:237] Iteration 3510, loss = 1.50515
I0520 15:59:01.170014  7516 solver.cpp:253]     Train net output #0: loss = 1.50515 (* 1 = 1.50515 loss)
I0520 15:59:01.170032  7516 sgd_solver.cpp:106] Iteration 3510, lr = 0.0025
I0520 15:59:09.356194  7516 solver.cpp:237] Iteration 3588, loss = 1.44851
I0520 15:59:09.356331  7516 solver.cpp:253]     Train net output #0: loss = 1.44851 (* 1 = 1.44851 loss)
I0520 15:59:09.356345  7516 sgd_solver.cpp:106] Iteration 3588, lr = 0.0025
I0520 15:59:17.552677  7516 solver.cpp:237] Iteration 3666, loss = 1.48172
I0520 15:59:17.552721  7516 solver.cpp:253]     Train net output #0: loss = 1.48172 (* 1 = 1.48172 loss)
I0520 15:59:17.552736  7516 sgd_solver.cpp:106] Iteration 3666, lr = 0.0025
I0520 15:59:47.876993  7516 solver.cpp:237] Iteration 3744, loss = 1.4081
I0520 15:59:47.877153  7516 solver.cpp:253]     Train net output #0: loss = 1.4081 (* 1 = 1.4081 loss)
I0520 15:59:47.877168  7516 sgd_solver.cpp:106] Iteration 3744, lr = 0.0025
I0520 15:59:56.069197  7516 solver.cpp:237] Iteration 3822, loss = 1.48847
I0520 15:59:56.069231  7516 solver.cpp:253]     Train net output #0: loss = 1.48847 (* 1 = 1.48847 loss)
I0520 15:59:56.069247  7516 sgd_solver.cpp:106] Iteration 3822, lr = 0.0025
I0520 16:00:04.268586  7516 solver.cpp:237] Iteration 3900, loss = 1.54676
I0520 16:00:04.268625  7516 solver.cpp:253]     Train net output #0: loss = 1.54676 (* 1 = 1.54676 loss)
I0520 16:00:04.268642  7516 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0520 16:00:08.893120  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_3945.caffemodel
I0520 16:00:09.006999  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_3945.solverstate
I0520 16:00:12.529703  7516 solver.cpp:237] Iteration 3978, loss = 1.55684
I0520 16:00:12.529748  7516 solver.cpp:253]     Train net output #0: loss = 1.55684 (* 1 = 1.55684 loss)
I0520 16:00:12.529767  7516 sgd_solver.cpp:106] Iteration 3978, lr = 0.0025
I0520 16:00:20.718916  7516 solver.cpp:237] Iteration 4056, loss = 1.47379
I0520 16:00:20.719060  7516 solver.cpp:253]     Train net output #0: loss = 1.47379 (* 1 = 1.47379 loss)
I0520 16:00:20.719074  7516 sgd_solver.cpp:106] Iteration 4056, lr = 0.0025
I0520 16:00:28.907733  7516 solver.cpp:237] Iteration 4134, loss = 1.50006
I0520 16:00:28.907768  7516 solver.cpp:253]     Train net output #0: loss = 1.50006 (* 1 = 1.50006 loss)
I0520 16:00:28.907789  7516 sgd_solver.cpp:106] Iteration 4134, lr = 0.0025
I0520 16:00:59.257740  7516 solver.cpp:237] Iteration 4212, loss = 1.51575
I0520 16:00:59.257913  7516 solver.cpp:253]     Train net output #0: loss = 1.51575 (* 1 = 1.51575 loss)
I0520 16:00:59.257927  7516 sgd_solver.cpp:106] Iteration 4212, lr = 0.0025
I0520 16:01:07.451493  7516 solver.cpp:237] Iteration 4290, loss = 1.43603
I0520 16:01:07.451526  7516 solver.cpp:253]     Train net output #0: loss = 1.43603 (* 1 = 1.43603 loss)
I0520 16:01:07.451544  7516 sgd_solver.cpp:106] Iteration 4290, lr = 0.0025
I0520 16:01:15.644821  7516 solver.cpp:237] Iteration 4368, loss = 1.49593
I0520 16:01:15.644855  7516 solver.cpp:253]     Train net output #0: loss = 1.49593 (* 1 = 1.49593 loss)
I0520 16:01:15.644871  7516 sgd_solver.cpp:106] Iteration 4368, lr = 0.0025
I0520 16:01:23.841701  7516 solver.cpp:237] Iteration 4446, loss = 1.52976
I0520 16:01:23.841743  7516 solver.cpp:253]     Train net output #0: loss = 1.52976 (* 1 = 1.52976 loss)
I0520 16:01:23.841761  7516 sgd_solver.cpp:106] Iteration 4446, lr = 0.0025
I0520 16:01:32.029819  7516 solver.cpp:237] Iteration 4524, loss = 1.57667
I0520 16:01:32.029960  7516 solver.cpp:253]     Train net output #0: loss = 1.57667 (* 1 = 1.57667 loss)
I0520 16:01:32.029974  7516 sgd_solver.cpp:106] Iteration 4524, lr = 0.0025
I0520 16:01:40.221164  7516 solver.cpp:237] Iteration 4602, loss = 1.38915
I0520 16:01:40.221199  7516 solver.cpp:253]     Train net output #0: loss = 1.38915 (* 1 = 1.38915 loss)
I0520 16:01:40.221216  7516 sgd_solver.cpp:106] Iteration 4602, lr = 0.0025
I0520 16:01:48.418251  7516 solver.cpp:237] Iteration 4680, loss = 1.55335
I0520 16:01:48.418297  7516 solver.cpp:253]     Train net output #0: loss = 1.55335 (* 1 = 1.55335 loss)
I0520 16:01:48.418313  7516 sgd_solver.cpp:106] Iteration 4680, lr = 0.0025
I0520 16:01:53.985710  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_4734.caffemodel
I0520 16:01:54.097254  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_4734.solverstate
I0520 16:01:54.123491  7516 solver.cpp:341] Iteration 4734, Testing net (#0)
I0520 16:02:39.659566  7516 solver.cpp:409]     Test net output #0: accuracy = 0.775159
I0520 16:02:39.659726  7516 solver.cpp:409]     Test net output #1: loss = 0.799157 (* 1 = 0.799157 loss)
I0520 16:03:04.323456  7516 solver.cpp:237] Iteration 4758, loss = 1.46407
I0520 16:03:04.323505  7516 solver.cpp:253]     Train net output #0: loss = 1.46407 (* 1 = 1.46407 loss)
I0520 16:03:04.323523  7516 sgd_solver.cpp:106] Iteration 4758, lr = 0.0025
I0520 16:03:12.512421  7516 solver.cpp:237] Iteration 4836, loss = 1.43844
I0520 16:03:12.512574  7516 solver.cpp:253]     Train net output #0: loss = 1.43844 (* 1 = 1.43844 loss)
I0520 16:03:12.512588  7516 sgd_solver.cpp:106] Iteration 4836, lr = 0.0025
I0520 16:03:20.703279  7516 solver.cpp:237] Iteration 4914, loss = 1.22244
I0520 16:03:20.703312  7516 solver.cpp:253]     Train net output #0: loss = 1.22244 (* 1 = 1.22244 loss)
I0520 16:03:20.703326  7516 sgd_solver.cpp:106] Iteration 4914, lr = 0.0025
I0520 16:03:28.889343  7516 solver.cpp:237] Iteration 4992, loss = 1.45375
I0520 16:03:28.889379  7516 solver.cpp:253]     Train net output #0: loss = 1.45375 (* 1 = 1.45375 loss)
I0520 16:03:28.889395  7516 sgd_solver.cpp:106] Iteration 4992, lr = 0.0025
I0520 16:03:37.078500  7516 solver.cpp:237] Iteration 5070, loss = 1.58269
I0520 16:03:37.078534  7516 solver.cpp:253]     Train net output #0: loss = 1.58269 (* 1 = 1.58269 loss)
I0520 16:03:37.078549  7516 sgd_solver.cpp:106] Iteration 5070, lr = 0.0025
I0520 16:03:45.262333  7516 solver.cpp:237] Iteration 5148, loss = 1.58532
I0520 16:03:45.262482  7516 solver.cpp:253]     Train net output #0: loss = 1.58532 (* 1 = 1.58532 loss)
I0520 16:03:45.262496  7516 sgd_solver.cpp:106] Iteration 5148, lr = 0.0025
I0520 16:03:53.444299  7516 solver.cpp:237] Iteration 5226, loss = 1.4165
I0520 16:03:53.444329  7516 solver.cpp:253]     Train net output #0: loss = 1.4165 (* 1 = 1.4165 loss)
I0520 16:03:53.444355  7516 sgd_solver.cpp:106] Iteration 5226, lr = 0.0025
I0520 16:04:23.737284  7516 solver.cpp:237] Iteration 5304, loss = 1.55827
I0520 16:04:23.737455  7516 solver.cpp:253]     Train net output #0: loss = 1.55827 (* 1 = 1.55827 loss)
I0520 16:04:23.737470  7516 sgd_solver.cpp:106] Iteration 5304, lr = 0.0025
I0520 16:04:31.917110  7516 solver.cpp:237] Iteration 5382, loss = 1.38036
I0520 16:04:31.917145  7516 solver.cpp:253]     Train net output #0: loss = 1.38036 (* 1 = 1.38036 loss)
I0520 16:04:31.917158  7516 sgd_solver.cpp:106] Iteration 5382, lr = 0.0025
I0520 16:04:40.111971  7516 solver.cpp:237] Iteration 5460, loss = 1.51283
I0520 16:04:40.112006  7516 solver.cpp:253]     Train net output #0: loss = 1.51283 (* 1 = 1.51283 loss)
I0520 16:04:40.112026  7516 sgd_solver.cpp:106] Iteration 5460, lr = 0.0025
I0520 16:04:46.620434  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_5523.caffemodel
I0520 16:04:46.730795  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_5523.solverstate
I0520 16:04:48.363782  7516 solver.cpp:237] Iteration 5538, loss = 1.45131
I0520 16:04:48.363829  7516 solver.cpp:253]     Train net output #0: loss = 1.45131 (* 1 = 1.45131 loss)
I0520 16:04:48.363845  7516 sgd_solver.cpp:106] Iteration 5538, lr = 0.0025
I0520 16:04:56.564841  7516 solver.cpp:237] Iteration 5616, loss = 1.31126
I0520 16:04:56.564985  7516 solver.cpp:253]     Train net output #0: loss = 1.31126 (* 1 = 1.31126 loss)
I0520 16:04:56.564999  7516 sgd_solver.cpp:106] Iteration 5616, lr = 0.0025
I0520 16:05:04.758092  7516 solver.cpp:237] Iteration 5694, loss = 1.40734
I0520 16:05:04.758132  7516 solver.cpp:253]     Train net output #0: loss = 1.40734 (* 1 = 1.40734 loss)
I0520 16:05:04.758149  7516 sgd_solver.cpp:106] Iteration 5694, lr = 0.0025
I0520 16:05:12.947520  7516 solver.cpp:237] Iteration 5772, loss = 1.30124
I0520 16:05:12.947554  7516 solver.cpp:253]     Train net output #0: loss = 1.30124 (* 1 = 1.30124 loss)
I0520 16:05:12.947567  7516 sgd_solver.cpp:106] Iteration 5772, lr = 0.0025
I0520 16:05:43.243970  7516 solver.cpp:237] Iteration 5850, loss = 1.37285
I0520 16:05:43.244138  7516 solver.cpp:253]     Train net output #0: loss = 1.37285 (* 1 = 1.37285 loss)
I0520 16:05:43.244154  7516 sgd_solver.cpp:106] Iteration 5850, lr = 0.0025
I0520 16:05:51.438352  7516 solver.cpp:237] Iteration 5928, loss = 1.36674
I0520 16:05:51.438387  7516 solver.cpp:253]     Train net output #0: loss = 1.36674 (* 1 = 1.36674 loss)
I0520 16:05:51.438405  7516 sgd_solver.cpp:106] Iteration 5928, lr = 0.0025
I0520 16:05:59.637380  7516 solver.cpp:237] Iteration 6006, loss = 1.40058
I0520 16:05:59.637414  7516 solver.cpp:253]     Train net output #0: loss = 1.40058 (* 1 = 1.40058 loss)
I0520 16:05:59.637434  7516 sgd_solver.cpp:106] Iteration 6006, lr = 0.0025
I0520 16:06:07.830458  7516 solver.cpp:237] Iteration 6084, loss = 1.34567
I0520 16:06:07.830493  7516 solver.cpp:253]     Train net output #0: loss = 1.34567 (* 1 = 1.34567 loss)
I0520 16:06:07.830510  7516 sgd_solver.cpp:106] Iteration 6084, lr = 0.0025
I0520 16:06:16.022503  7516 solver.cpp:237] Iteration 6162, loss = 1.27851
I0520 16:06:16.022645  7516 solver.cpp:253]     Train net output #0: loss = 1.27851 (* 1 = 1.27851 loss)
I0520 16:06:16.022660  7516 sgd_solver.cpp:106] Iteration 6162, lr = 0.0025
I0520 16:06:24.220242  7516 solver.cpp:237] Iteration 6240, loss = 1.39549
I0520 16:06:24.220276  7516 solver.cpp:253]     Train net output #0: loss = 1.39549 (* 1 = 1.39549 loss)
I0520 16:06:24.220295  7516 sgd_solver.cpp:106] Iteration 6240, lr = 0.0025
I0520 16:06:31.677208  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_6312.caffemodel
I0520 16:06:31.788017  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_6312.solverstate
I0520 16:06:31.814031  7516 solver.cpp:341] Iteration 6312, Testing net (#0)
I0520 16:07:38.413668  7516 solver.cpp:409]     Test net output #0: accuracy = 0.801201
I0520 16:07:38.413847  7516 solver.cpp:409]     Test net output #1: loss = 0.76542 (* 1 = 0.76542 loss)
I0520 16:08:01.234923  7516 solver.cpp:237] Iteration 6318, loss = 1.48538
I0520 16:08:01.234974  7516 solver.cpp:253]     Train net output #0: loss = 1.48538 (* 1 = 1.48538 loss)
I0520 16:08:01.234988  7516 sgd_solver.cpp:106] Iteration 6318, lr = 0.0025
I0520 16:08:09.423663  7516 solver.cpp:237] Iteration 6396, loss = 1.44434
I0520 16:08:09.423815  7516 solver.cpp:253]     Train net output #0: loss = 1.44434 (* 1 = 1.44434 loss)
I0520 16:08:09.423831  7516 sgd_solver.cpp:106] Iteration 6396, lr = 0.0025
I0520 16:08:17.615317  7516 solver.cpp:237] Iteration 6474, loss = 1.35433
I0520 16:08:17.615352  7516 solver.cpp:253]     Train net output #0: loss = 1.35433 (* 1 = 1.35433 loss)
I0520 16:08:17.615370  7516 sgd_solver.cpp:106] Iteration 6474, lr = 0.0025
I0520 16:08:25.814537  7516 solver.cpp:237] Iteration 6552, loss = 1.37878
I0520 16:08:25.814573  7516 solver.cpp:253]     Train net output #0: loss = 1.37878 (* 1 = 1.37878 loss)
I0520 16:08:25.814594  7516 sgd_solver.cpp:106] Iteration 6552, lr = 0.0025
I0520 16:08:34.011382  7516 solver.cpp:237] Iteration 6630, loss = 1.33801
I0520 16:08:34.011417  7516 solver.cpp:253]     Train net output #0: loss = 1.33801 (* 1 = 1.33801 loss)
I0520 16:08:34.011435  7516 sgd_solver.cpp:106] Iteration 6630, lr = 0.0025
I0520 16:08:42.201155  7516 solver.cpp:237] Iteration 6708, loss = 1.1691
I0520 16:08:42.201295  7516 solver.cpp:253]     Train net output #0: loss = 1.1691 (* 1 = 1.1691 loss)
I0520 16:08:42.201309  7516 sgd_solver.cpp:106] Iteration 6708, lr = 0.0025
I0520 16:08:50.384877  7516 solver.cpp:237] Iteration 6786, loss = 1.38747
I0520 16:08:50.384917  7516 solver.cpp:253]     Train net output #0: loss = 1.38747 (* 1 = 1.38747 loss)
I0520 16:08:50.384938  7516 sgd_solver.cpp:106] Iteration 6786, lr = 0.0025
I0520 16:09:20.681663  7516 solver.cpp:237] Iteration 6864, loss = 1.31313
I0520 16:09:20.681841  7516 solver.cpp:253]     Train net output #0: loss = 1.31313 (* 1 = 1.31313 loss)
I0520 16:09:20.681857  7516 sgd_solver.cpp:106] Iteration 6864, lr = 0.0025
I0520 16:09:28.872956  7516 solver.cpp:237] Iteration 6942, loss = 1.45204
I0520 16:09:28.872990  7516 solver.cpp:253]     Train net output #0: loss = 1.45204 (* 1 = 1.45204 loss)
I0520 16:09:28.873008  7516 sgd_solver.cpp:106] Iteration 6942, lr = 0.0025
I0520 16:09:37.067639  7516 solver.cpp:237] Iteration 7020, loss = 1.3352
I0520 16:09:37.067680  7516 solver.cpp:253]     Train net output #0: loss = 1.3352 (* 1 = 1.3352 loss)
I0520 16:09:37.067699  7516 sgd_solver.cpp:106] Iteration 7020, lr = 0.0025
I0520 16:09:45.254029  7516 solver.cpp:237] Iteration 7098, loss = 1.38614
I0520 16:09:45.254062  7516 solver.cpp:253]     Train net output #0: loss = 1.38614 (* 1 = 1.38614 loss)
I0520 16:09:45.254079  7516 sgd_solver.cpp:106] Iteration 7098, lr = 0.0025
I0520 16:09:45.463846  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_7101.caffemodel
I0520 16:09:45.576769  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_7101.solverstate
I0520 16:09:53.509099  7516 solver.cpp:237] Iteration 7176, loss = 1.33343
I0520 16:09:53.509275  7516 solver.cpp:253]     Train net output #0: loss = 1.33343 (* 1 = 1.33343 loss)
I0520 16:09:53.509289  7516 sgd_solver.cpp:106] Iteration 7176, lr = 0.0025
I0520 16:10:01.693516  7516 solver.cpp:237] Iteration 7254, loss = 1.45818
I0520 16:10:01.693564  7516 solver.cpp:253]     Train net output #0: loss = 1.45818 (* 1 = 1.45818 loss)
I0520 16:10:01.693583  7516 sgd_solver.cpp:106] Iteration 7254, lr = 0.0025
I0520 16:10:09.888875  7516 solver.cpp:237] Iteration 7332, loss = 1.46457
I0520 16:10:09.888911  7516 solver.cpp:253]     Train net output #0: loss = 1.46457 (* 1 = 1.46457 loss)
I0520 16:10:09.888926  7516 sgd_solver.cpp:106] Iteration 7332, lr = 0.0025
I0520 16:10:40.251144  7516 solver.cpp:237] Iteration 7410, loss = 1.43587
I0520 16:10:40.251314  7516 solver.cpp:253]     Train net output #0: loss = 1.43587 (* 1 = 1.43587 loss)
I0520 16:10:40.251330  7516 sgd_solver.cpp:106] Iteration 7410, lr = 0.0025
I0520 16:10:48.434625  7516 solver.cpp:237] Iteration 7488, loss = 1.51265
I0520 16:10:48.434660  7516 solver.cpp:253]     Train net output #0: loss = 1.51265 (* 1 = 1.51265 loss)
I0520 16:10:48.434677  7516 sgd_solver.cpp:106] Iteration 7488, lr = 0.0025
I0520 16:10:56.632949  7516 solver.cpp:237] Iteration 7566, loss = 1.35937
I0520 16:10:56.632988  7516 solver.cpp:253]     Train net output #0: loss = 1.35937 (* 1 = 1.35937 loss)
I0520 16:10:56.633004  7516 sgd_solver.cpp:106] Iteration 7566, lr = 0.0025
I0520 16:11:04.825413  7516 solver.cpp:237] Iteration 7644, loss = 1.35569
I0520 16:11:04.825449  7516 solver.cpp:253]     Train net output #0: loss = 1.35569 (* 1 = 1.35569 loss)
I0520 16:11:04.825464  7516 sgd_solver.cpp:106] Iteration 7644, lr = 0.0025
I0520 16:11:13.018188  7516 solver.cpp:237] Iteration 7722, loss = 1.33753
I0520 16:11:13.018332  7516 solver.cpp:253]     Train net output #0: loss = 1.33753 (* 1 = 1.33753 loss)
I0520 16:11:13.018345  7516 sgd_solver.cpp:106] Iteration 7722, lr = 0.0025
I0520 16:11:21.208147  7516 solver.cpp:237] Iteration 7800, loss = 1.28498
I0520 16:11:21.208191  7516 solver.cpp:253]     Train net output #0: loss = 1.28498 (* 1 = 1.28498 loss)
I0520 16:11:21.208207  7516 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0520 16:11:29.401813  7516 solver.cpp:237] Iteration 7878, loss = 1.38223
I0520 16:11:29.401849  7516 solver.cpp:253]     Train net output #0: loss = 1.38223 (* 1 = 1.38223 loss)
I0520 16:11:29.401864  7516 sgd_solver.cpp:106] Iteration 7878, lr = 0.0025
I0520 16:11:30.557166  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_7890.caffemodel
I0520 16:11:30.669554  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_7890.solverstate
I0520 16:11:30.697808  7516 solver.cpp:341] Iteration 7890, Testing net (#0)
I0520 16:12:16.540812  7516 solver.cpp:409]     Test net output #0: accuracy = 0.813335
I0520 16:12:16.540984  7516 solver.cpp:409]     Test net output #1: loss = 0.646419 (* 1 = 0.646419 loss)
I0520 16:12:16.888599  7516 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_7894.caffemodel
I0520 16:12:17.001422  7516 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100_iter_7894.solverstate
I0520 16:12:17.029233  7516 solver.cpp:326] Optimization Done.
I0520 16:12:17.029261  7516 caffe.cpp:215] Optimization Done.
Application 11233120 resources: utime ~1273s, stime ~229s, Rss ~5329408, inblocks ~3594475, outblocks ~194562
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_190_2016-05-20T11.20.39.740100.solver"
	User time (seconds): 0.56
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 25:07.48
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15072
	Voluntary context switches: 3004
	Involuntary context switches: 172
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
