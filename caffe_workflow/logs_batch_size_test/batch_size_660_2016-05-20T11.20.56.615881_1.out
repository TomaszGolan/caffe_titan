2806257
I0521 04:32:25.356987 13815 caffe.cpp:184] Using GPUs 0
I0521 04:32:25.787633 13815 solver.cpp:48] Initializing solver from parameters: 
test_iter: 227
test_interval: 454
base_lr: 0.0025
display: 22
max_iter: 2272
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 227
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881.prototxt"
I0521 04:32:25.789772 13815 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881.prototxt
I0521 04:32:25.802556 13815 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 04:32:25.802615 13815 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 04:32:25.802961 13815 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 660
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:32:25.803138 13815 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:32:25.803160 13815 net.cpp:106] Creating Layer data_hdf5
I0521 04:32:25.803175 13815 net.cpp:411] data_hdf5 -> data
I0521 04:32:25.803208 13815 net.cpp:411] data_hdf5 -> label
I0521 04:32:25.803241 13815 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 04:32:25.804436 13815 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 04:32:25.806593 13815 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 04:32:47.329776 13815 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 04:32:47.334916 13815 net.cpp:150] Setting up data_hdf5
I0521 04:32:47.334956 13815 net.cpp:157] Top shape: 660 1 127 50 (4191000)
I0521 04:32:47.334971 13815 net.cpp:157] Top shape: 660 (660)
I0521 04:32:47.334985 13815 net.cpp:165] Memory required for data: 16766640
I0521 04:32:47.334997 13815 layer_factory.hpp:77] Creating layer conv1
I0521 04:32:47.335031 13815 net.cpp:106] Creating Layer conv1
I0521 04:32:47.335042 13815 net.cpp:454] conv1 <- data
I0521 04:32:47.335064 13815 net.cpp:411] conv1 -> conv1
I0521 04:32:47.696430 13815 net.cpp:150] Setting up conv1
I0521 04:32:47.696478 13815 net.cpp:157] Top shape: 660 12 120 48 (45619200)
I0521 04:32:47.696488 13815 net.cpp:165] Memory required for data: 199243440
I0521 04:32:47.696516 13815 layer_factory.hpp:77] Creating layer relu1
I0521 04:32:47.696537 13815 net.cpp:106] Creating Layer relu1
I0521 04:32:47.696557 13815 net.cpp:454] relu1 <- conv1
I0521 04:32:47.696570 13815 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:32:47.697088 13815 net.cpp:150] Setting up relu1
I0521 04:32:47.697104 13815 net.cpp:157] Top shape: 660 12 120 48 (45619200)
I0521 04:32:47.697115 13815 net.cpp:165] Memory required for data: 381720240
I0521 04:32:47.697125 13815 layer_factory.hpp:77] Creating layer pool1
I0521 04:32:47.697141 13815 net.cpp:106] Creating Layer pool1
I0521 04:32:47.697151 13815 net.cpp:454] pool1 <- conv1
I0521 04:32:47.697165 13815 net.cpp:411] pool1 -> pool1
I0521 04:32:47.697244 13815 net.cpp:150] Setting up pool1
I0521 04:32:47.697258 13815 net.cpp:157] Top shape: 660 12 60 48 (22809600)
I0521 04:32:47.697268 13815 net.cpp:165] Memory required for data: 472958640
I0521 04:32:47.697278 13815 layer_factory.hpp:77] Creating layer conv2
I0521 04:32:47.697301 13815 net.cpp:106] Creating Layer conv2
I0521 04:32:47.697311 13815 net.cpp:454] conv2 <- pool1
I0521 04:32:47.697324 13815 net.cpp:411] conv2 -> conv2
I0521 04:32:47.699992 13815 net.cpp:150] Setting up conv2
I0521 04:32:47.700021 13815 net.cpp:157] Top shape: 660 20 54 46 (32788800)
I0521 04:32:47.700031 13815 net.cpp:165] Memory required for data: 604113840
I0521 04:32:47.700049 13815 layer_factory.hpp:77] Creating layer relu2
I0521 04:32:47.700064 13815 net.cpp:106] Creating Layer relu2
I0521 04:32:47.700074 13815 net.cpp:454] relu2 <- conv2
I0521 04:32:47.700088 13815 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:32:47.700417 13815 net.cpp:150] Setting up relu2
I0521 04:32:47.700431 13815 net.cpp:157] Top shape: 660 20 54 46 (32788800)
I0521 04:32:47.700441 13815 net.cpp:165] Memory required for data: 735269040
I0521 04:32:47.700453 13815 layer_factory.hpp:77] Creating layer pool2
I0521 04:32:47.700464 13815 net.cpp:106] Creating Layer pool2
I0521 04:32:47.700474 13815 net.cpp:454] pool2 <- conv2
I0521 04:32:47.700498 13815 net.cpp:411] pool2 -> pool2
I0521 04:32:47.700577 13815 net.cpp:150] Setting up pool2
I0521 04:32:47.700590 13815 net.cpp:157] Top shape: 660 20 27 46 (16394400)
I0521 04:32:47.700599 13815 net.cpp:165] Memory required for data: 800846640
I0521 04:32:47.700610 13815 layer_factory.hpp:77] Creating layer conv3
I0521 04:32:47.700628 13815 net.cpp:106] Creating Layer conv3
I0521 04:32:47.700639 13815 net.cpp:454] conv3 <- pool2
I0521 04:32:47.700651 13815 net.cpp:411] conv3 -> conv3
I0521 04:32:47.702560 13815 net.cpp:150] Setting up conv3
I0521 04:32:47.702579 13815 net.cpp:157] Top shape: 660 28 22 44 (17888640)
I0521 04:32:47.702589 13815 net.cpp:165] Memory required for data: 872401200
I0521 04:32:47.702607 13815 layer_factory.hpp:77] Creating layer relu3
I0521 04:32:47.702623 13815 net.cpp:106] Creating Layer relu3
I0521 04:32:47.702633 13815 net.cpp:454] relu3 <- conv3
I0521 04:32:47.702646 13815 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:32:47.703114 13815 net.cpp:150] Setting up relu3
I0521 04:32:47.703130 13815 net.cpp:157] Top shape: 660 28 22 44 (17888640)
I0521 04:32:47.703140 13815 net.cpp:165] Memory required for data: 943955760
I0521 04:32:47.703150 13815 layer_factory.hpp:77] Creating layer pool3
I0521 04:32:47.703163 13815 net.cpp:106] Creating Layer pool3
I0521 04:32:47.703172 13815 net.cpp:454] pool3 <- conv3
I0521 04:32:47.703186 13815 net.cpp:411] pool3 -> pool3
I0521 04:32:47.703253 13815 net.cpp:150] Setting up pool3
I0521 04:32:47.703265 13815 net.cpp:157] Top shape: 660 28 11 44 (8944320)
I0521 04:32:47.703275 13815 net.cpp:165] Memory required for data: 979733040
I0521 04:32:47.703282 13815 layer_factory.hpp:77] Creating layer conv4
I0521 04:32:47.703300 13815 net.cpp:106] Creating Layer conv4
I0521 04:32:47.703310 13815 net.cpp:454] conv4 <- pool3
I0521 04:32:47.703325 13815 net.cpp:411] conv4 -> conv4
I0521 04:32:47.706126 13815 net.cpp:150] Setting up conv4
I0521 04:32:47.706154 13815 net.cpp:157] Top shape: 660 36 6 42 (5987520)
I0521 04:32:47.706166 13815 net.cpp:165] Memory required for data: 1003683120
I0521 04:32:47.706181 13815 layer_factory.hpp:77] Creating layer relu4
I0521 04:32:47.706195 13815 net.cpp:106] Creating Layer relu4
I0521 04:32:47.706205 13815 net.cpp:454] relu4 <- conv4
I0521 04:32:47.706218 13815 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:32:47.706691 13815 net.cpp:150] Setting up relu4
I0521 04:32:47.706707 13815 net.cpp:157] Top shape: 660 36 6 42 (5987520)
I0521 04:32:47.706717 13815 net.cpp:165] Memory required for data: 1027633200
I0521 04:32:47.706727 13815 layer_factory.hpp:77] Creating layer pool4
I0521 04:32:47.706739 13815 net.cpp:106] Creating Layer pool4
I0521 04:32:47.706749 13815 net.cpp:454] pool4 <- conv4
I0521 04:32:47.706761 13815 net.cpp:411] pool4 -> pool4
I0521 04:32:47.706830 13815 net.cpp:150] Setting up pool4
I0521 04:32:47.706842 13815 net.cpp:157] Top shape: 660 36 3 42 (2993760)
I0521 04:32:47.706853 13815 net.cpp:165] Memory required for data: 1039608240
I0521 04:32:47.706863 13815 layer_factory.hpp:77] Creating layer ip1
I0521 04:32:47.706883 13815 net.cpp:106] Creating Layer ip1
I0521 04:32:47.706893 13815 net.cpp:454] ip1 <- pool4
I0521 04:32:47.706907 13815 net.cpp:411] ip1 -> ip1
I0521 04:32:47.722353 13815 net.cpp:150] Setting up ip1
I0521 04:32:47.722381 13815 net.cpp:157] Top shape: 660 196 (129360)
I0521 04:32:47.722394 13815 net.cpp:165] Memory required for data: 1040125680
I0521 04:32:47.722420 13815 layer_factory.hpp:77] Creating layer relu5
I0521 04:32:47.722434 13815 net.cpp:106] Creating Layer relu5
I0521 04:32:47.722445 13815 net.cpp:454] relu5 <- ip1
I0521 04:32:47.722457 13815 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:32:47.722800 13815 net.cpp:150] Setting up relu5
I0521 04:32:47.722813 13815 net.cpp:157] Top shape: 660 196 (129360)
I0521 04:32:47.722825 13815 net.cpp:165] Memory required for data: 1040643120
I0521 04:32:47.722833 13815 layer_factory.hpp:77] Creating layer drop1
I0521 04:32:47.722856 13815 net.cpp:106] Creating Layer drop1
I0521 04:32:47.722865 13815 net.cpp:454] drop1 <- ip1
I0521 04:32:47.722890 13815 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:32:47.722936 13815 net.cpp:150] Setting up drop1
I0521 04:32:47.722949 13815 net.cpp:157] Top shape: 660 196 (129360)
I0521 04:32:47.722959 13815 net.cpp:165] Memory required for data: 1041160560
I0521 04:32:47.722968 13815 layer_factory.hpp:77] Creating layer ip2
I0521 04:32:47.722988 13815 net.cpp:106] Creating Layer ip2
I0521 04:32:47.722998 13815 net.cpp:454] ip2 <- ip1
I0521 04:32:47.723011 13815 net.cpp:411] ip2 -> ip2
I0521 04:32:47.723474 13815 net.cpp:150] Setting up ip2
I0521 04:32:47.723487 13815 net.cpp:157] Top shape: 660 98 (64680)
I0521 04:32:47.723496 13815 net.cpp:165] Memory required for data: 1041419280
I0521 04:32:47.723511 13815 layer_factory.hpp:77] Creating layer relu6
I0521 04:32:47.723525 13815 net.cpp:106] Creating Layer relu6
I0521 04:32:47.723533 13815 net.cpp:454] relu6 <- ip2
I0521 04:32:47.723546 13815 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:32:47.724066 13815 net.cpp:150] Setting up relu6
I0521 04:32:47.724081 13815 net.cpp:157] Top shape: 660 98 (64680)
I0521 04:32:47.724092 13815 net.cpp:165] Memory required for data: 1041678000
I0521 04:32:47.724102 13815 layer_factory.hpp:77] Creating layer drop2
I0521 04:32:47.724114 13815 net.cpp:106] Creating Layer drop2
I0521 04:32:47.724124 13815 net.cpp:454] drop2 <- ip2
I0521 04:32:47.724136 13815 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:32:47.724179 13815 net.cpp:150] Setting up drop2
I0521 04:32:47.724191 13815 net.cpp:157] Top shape: 660 98 (64680)
I0521 04:32:47.724201 13815 net.cpp:165] Memory required for data: 1041936720
I0521 04:32:47.724210 13815 layer_factory.hpp:77] Creating layer ip3
I0521 04:32:47.724223 13815 net.cpp:106] Creating Layer ip3
I0521 04:32:47.724233 13815 net.cpp:454] ip3 <- ip2
I0521 04:32:47.724246 13815 net.cpp:411] ip3 -> ip3
I0521 04:32:47.724455 13815 net.cpp:150] Setting up ip3
I0521 04:32:47.724469 13815 net.cpp:157] Top shape: 660 11 (7260)
I0521 04:32:47.724479 13815 net.cpp:165] Memory required for data: 1041965760
I0521 04:32:47.724493 13815 layer_factory.hpp:77] Creating layer drop3
I0521 04:32:47.724505 13815 net.cpp:106] Creating Layer drop3
I0521 04:32:47.724515 13815 net.cpp:454] drop3 <- ip3
I0521 04:32:47.724527 13815 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:32:47.724575 13815 net.cpp:150] Setting up drop3
I0521 04:32:47.724588 13815 net.cpp:157] Top shape: 660 11 (7260)
I0521 04:32:47.724598 13815 net.cpp:165] Memory required for data: 1041994800
I0521 04:32:47.724607 13815 layer_factory.hpp:77] Creating layer loss
I0521 04:32:47.724627 13815 net.cpp:106] Creating Layer loss
I0521 04:32:47.724635 13815 net.cpp:454] loss <- ip3
I0521 04:32:47.724647 13815 net.cpp:454] loss <- label
I0521 04:32:47.724658 13815 net.cpp:411] loss -> loss
I0521 04:32:47.724675 13815 layer_factory.hpp:77] Creating layer loss
I0521 04:32:47.725325 13815 net.cpp:150] Setting up loss
I0521 04:32:47.725347 13815 net.cpp:157] Top shape: (1)
I0521 04:32:47.725359 13815 net.cpp:160]     with loss weight 1
I0521 04:32:47.725401 13815 net.cpp:165] Memory required for data: 1041994804
I0521 04:32:47.725412 13815 net.cpp:226] loss needs backward computation.
I0521 04:32:47.725424 13815 net.cpp:226] drop3 needs backward computation.
I0521 04:32:47.725432 13815 net.cpp:226] ip3 needs backward computation.
I0521 04:32:47.725443 13815 net.cpp:226] drop2 needs backward computation.
I0521 04:32:47.725453 13815 net.cpp:226] relu6 needs backward computation.
I0521 04:32:47.725463 13815 net.cpp:226] ip2 needs backward computation.
I0521 04:32:47.725473 13815 net.cpp:226] drop1 needs backward computation.
I0521 04:32:47.725483 13815 net.cpp:226] relu5 needs backward computation.
I0521 04:32:47.725492 13815 net.cpp:226] ip1 needs backward computation.
I0521 04:32:47.725503 13815 net.cpp:226] pool4 needs backward computation.
I0521 04:32:47.725513 13815 net.cpp:226] relu4 needs backward computation.
I0521 04:32:47.725522 13815 net.cpp:226] conv4 needs backward computation.
I0521 04:32:47.725533 13815 net.cpp:226] pool3 needs backward computation.
I0521 04:32:47.725553 13815 net.cpp:226] relu3 needs backward computation.
I0521 04:32:47.725563 13815 net.cpp:226] conv3 needs backward computation.
I0521 04:32:47.725574 13815 net.cpp:226] pool2 needs backward computation.
I0521 04:32:47.725584 13815 net.cpp:226] relu2 needs backward computation.
I0521 04:32:47.725594 13815 net.cpp:226] conv2 needs backward computation.
I0521 04:32:47.725605 13815 net.cpp:226] pool1 needs backward computation.
I0521 04:32:47.725615 13815 net.cpp:226] relu1 needs backward computation.
I0521 04:32:47.725623 13815 net.cpp:226] conv1 needs backward computation.
I0521 04:32:47.725635 13815 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:32:47.725643 13815 net.cpp:270] This network produces output loss
I0521 04:32:47.725667 13815 net.cpp:283] Network initialization done.
I0521 04:32:47.727246 13815 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881.prototxt
I0521 04:32:47.727318 13815 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 04:32:47.727672 13815 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 660
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:32:47.727860 13815 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:32:47.727875 13815 net.cpp:106] Creating Layer data_hdf5
I0521 04:32:47.727888 13815 net.cpp:411] data_hdf5 -> data
I0521 04:32:47.727905 13815 net.cpp:411] data_hdf5 -> label
I0521 04:32:47.727921 13815 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 04:32:47.729158 13815 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 04:33:09.022462 13815 net.cpp:150] Setting up data_hdf5
I0521 04:33:09.022626 13815 net.cpp:157] Top shape: 660 1 127 50 (4191000)
I0521 04:33:09.022641 13815 net.cpp:157] Top shape: 660 (660)
I0521 04:33:09.022652 13815 net.cpp:165] Memory required for data: 16766640
I0521 04:33:09.022666 13815 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 04:33:09.022694 13815 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 04:33:09.022704 13815 net.cpp:454] label_data_hdf5_1_split <- label
I0521 04:33:09.022719 13815 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 04:33:09.022740 13815 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 04:33:09.022814 13815 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 04:33:09.022827 13815 net.cpp:157] Top shape: 660 (660)
I0521 04:33:09.022840 13815 net.cpp:157] Top shape: 660 (660)
I0521 04:33:09.022848 13815 net.cpp:165] Memory required for data: 16771920
I0521 04:33:09.022858 13815 layer_factory.hpp:77] Creating layer conv1
I0521 04:33:09.022881 13815 net.cpp:106] Creating Layer conv1
I0521 04:33:09.022891 13815 net.cpp:454] conv1 <- data
I0521 04:33:09.022905 13815 net.cpp:411] conv1 -> conv1
I0521 04:33:09.024844 13815 net.cpp:150] Setting up conv1
I0521 04:33:09.024868 13815 net.cpp:157] Top shape: 660 12 120 48 (45619200)
I0521 04:33:09.024880 13815 net.cpp:165] Memory required for data: 199248720
I0521 04:33:09.024900 13815 layer_factory.hpp:77] Creating layer relu1
I0521 04:33:09.024915 13815 net.cpp:106] Creating Layer relu1
I0521 04:33:09.024925 13815 net.cpp:454] relu1 <- conv1
I0521 04:33:09.024938 13815 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:33:09.025434 13815 net.cpp:150] Setting up relu1
I0521 04:33:09.025449 13815 net.cpp:157] Top shape: 660 12 120 48 (45619200)
I0521 04:33:09.025460 13815 net.cpp:165] Memory required for data: 381725520
I0521 04:33:09.025470 13815 layer_factory.hpp:77] Creating layer pool1
I0521 04:33:09.025486 13815 net.cpp:106] Creating Layer pool1
I0521 04:33:09.025496 13815 net.cpp:454] pool1 <- conv1
I0521 04:33:09.025509 13815 net.cpp:411] pool1 -> pool1
I0521 04:33:09.025583 13815 net.cpp:150] Setting up pool1
I0521 04:33:09.025598 13815 net.cpp:157] Top shape: 660 12 60 48 (22809600)
I0521 04:33:09.025606 13815 net.cpp:165] Memory required for data: 472963920
I0521 04:33:09.025614 13815 layer_factory.hpp:77] Creating layer conv2
I0521 04:33:09.025634 13815 net.cpp:106] Creating Layer conv2
I0521 04:33:09.025643 13815 net.cpp:454] conv2 <- pool1
I0521 04:33:09.025660 13815 net.cpp:411] conv2 -> conv2
I0521 04:33:09.027565 13815 net.cpp:150] Setting up conv2
I0521 04:33:09.027587 13815 net.cpp:157] Top shape: 660 20 54 46 (32788800)
I0521 04:33:09.027601 13815 net.cpp:165] Memory required for data: 604119120
I0521 04:33:09.027617 13815 layer_factory.hpp:77] Creating layer relu2
I0521 04:33:09.027631 13815 net.cpp:106] Creating Layer relu2
I0521 04:33:09.027640 13815 net.cpp:454] relu2 <- conv2
I0521 04:33:09.027653 13815 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:33:09.027988 13815 net.cpp:150] Setting up relu2
I0521 04:33:09.028002 13815 net.cpp:157] Top shape: 660 20 54 46 (32788800)
I0521 04:33:09.028012 13815 net.cpp:165] Memory required for data: 735274320
I0521 04:33:09.028023 13815 layer_factory.hpp:77] Creating layer pool2
I0521 04:33:09.028035 13815 net.cpp:106] Creating Layer pool2
I0521 04:33:09.028045 13815 net.cpp:454] pool2 <- conv2
I0521 04:33:09.028059 13815 net.cpp:411] pool2 -> pool2
I0521 04:33:09.028128 13815 net.cpp:150] Setting up pool2
I0521 04:33:09.028141 13815 net.cpp:157] Top shape: 660 20 27 46 (16394400)
I0521 04:33:09.028151 13815 net.cpp:165] Memory required for data: 800851920
I0521 04:33:09.028162 13815 layer_factory.hpp:77] Creating layer conv3
I0521 04:33:09.028180 13815 net.cpp:106] Creating Layer conv3
I0521 04:33:09.028190 13815 net.cpp:454] conv3 <- pool2
I0521 04:33:09.028204 13815 net.cpp:411] conv3 -> conv3
I0521 04:33:09.030195 13815 net.cpp:150] Setting up conv3
I0521 04:33:09.030220 13815 net.cpp:157] Top shape: 660 28 22 44 (17888640)
I0521 04:33:09.030230 13815 net.cpp:165] Memory required for data: 872406480
I0521 04:33:09.030261 13815 layer_factory.hpp:77] Creating layer relu3
I0521 04:33:09.030275 13815 net.cpp:106] Creating Layer relu3
I0521 04:33:09.030285 13815 net.cpp:454] relu3 <- conv3
I0521 04:33:09.030298 13815 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:33:09.030771 13815 net.cpp:150] Setting up relu3
I0521 04:33:09.030786 13815 net.cpp:157] Top shape: 660 28 22 44 (17888640)
I0521 04:33:09.030797 13815 net.cpp:165] Memory required for data: 943961040
I0521 04:33:09.030807 13815 layer_factory.hpp:77] Creating layer pool3
I0521 04:33:09.030819 13815 net.cpp:106] Creating Layer pool3
I0521 04:33:09.030828 13815 net.cpp:454] pool3 <- conv3
I0521 04:33:09.030841 13815 net.cpp:411] pool3 -> pool3
I0521 04:33:09.030913 13815 net.cpp:150] Setting up pool3
I0521 04:33:09.030926 13815 net.cpp:157] Top shape: 660 28 11 44 (8944320)
I0521 04:33:09.030936 13815 net.cpp:165] Memory required for data: 979738320
I0521 04:33:09.030946 13815 layer_factory.hpp:77] Creating layer conv4
I0521 04:33:09.030963 13815 net.cpp:106] Creating Layer conv4
I0521 04:33:09.030973 13815 net.cpp:454] conv4 <- pool3
I0521 04:33:09.030987 13815 net.cpp:411] conv4 -> conv4
I0521 04:33:09.033041 13815 net.cpp:150] Setting up conv4
I0521 04:33:09.033064 13815 net.cpp:157] Top shape: 660 36 6 42 (5987520)
I0521 04:33:09.033077 13815 net.cpp:165] Memory required for data: 1003688400
I0521 04:33:09.033092 13815 layer_factory.hpp:77] Creating layer relu4
I0521 04:33:09.033104 13815 net.cpp:106] Creating Layer relu4
I0521 04:33:09.033114 13815 net.cpp:454] relu4 <- conv4
I0521 04:33:09.033128 13815 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:33:09.033596 13815 net.cpp:150] Setting up relu4
I0521 04:33:09.033612 13815 net.cpp:157] Top shape: 660 36 6 42 (5987520)
I0521 04:33:09.033622 13815 net.cpp:165] Memory required for data: 1027638480
I0521 04:33:09.033632 13815 layer_factory.hpp:77] Creating layer pool4
I0521 04:33:09.033645 13815 net.cpp:106] Creating Layer pool4
I0521 04:33:09.033655 13815 net.cpp:454] pool4 <- conv4
I0521 04:33:09.033668 13815 net.cpp:411] pool4 -> pool4
I0521 04:33:09.033740 13815 net.cpp:150] Setting up pool4
I0521 04:33:09.033753 13815 net.cpp:157] Top shape: 660 36 3 42 (2993760)
I0521 04:33:09.033762 13815 net.cpp:165] Memory required for data: 1039613520
I0521 04:33:09.033771 13815 layer_factory.hpp:77] Creating layer ip1
I0521 04:33:09.033787 13815 net.cpp:106] Creating Layer ip1
I0521 04:33:09.033797 13815 net.cpp:454] ip1 <- pool4
I0521 04:33:09.033812 13815 net.cpp:411] ip1 -> ip1
I0521 04:33:09.049257 13815 net.cpp:150] Setting up ip1
I0521 04:33:09.049285 13815 net.cpp:157] Top shape: 660 196 (129360)
I0521 04:33:09.049299 13815 net.cpp:165] Memory required for data: 1040130960
I0521 04:33:09.049321 13815 layer_factory.hpp:77] Creating layer relu5
I0521 04:33:09.049337 13815 net.cpp:106] Creating Layer relu5
I0521 04:33:09.049347 13815 net.cpp:454] relu5 <- ip1
I0521 04:33:09.049361 13815 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:33:09.049707 13815 net.cpp:150] Setting up relu5
I0521 04:33:09.049721 13815 net.cpp:157] Top shape: 660 196 (129360)
I0521 04:33:09.049731 13815 net.cpp:165] Memory required for data: 1040648400
I0521 04:33:09.049741 13815 layer_factory.hpp:77] Creating layer drop1
I0521 04:33:09.049760 13815 net.cpp:106] Creating Layer drop1
I0521 04:33:09.049770 13815 net.cpp:454] drop1 <- ip1
I0521 04:33:09.049783 13815 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:33:09.049828 13815 net.cpp:150] Setting up drop1
I0521 04:33:09.049840 13815 net.cpp:157] Top shape: 660 196 (129360)
I0521 04:33:09.049849 13815 net.cpp:165] Memory required for data: 1041165840
I0521 04:33:09.049860 13815 layer_factory.hpp:77] Creating layer ip2
I0521 04:33:09.049875 13815 net.cpp:106] Creating Layer ip2
I0521 04:33:09.049885 13815 net.cpp:454] ip2 <- ip1
I0521 04:33:09.049898 13815 net.cpp:411] ip2 -> ip2
I0521 04:33:09.050377 13815 net.cpp:150] Setting up ip2
I0521 04:33:09.050390 13815 net.cpp:157] Top shape: 660 98 (64680)
I0521 04:33:09.050401 13815 net.cpp:165] Memory required for data: 1041424560
I0521 04:33:09.050429 13815 layer_factory.hpp:77] Creating layer relu6
I0521 04:33:09.050442 13815 net.cpp:106] Creating Layer relu6
I0521 04:33:09.050452 13815 net.cpp:454] relu6 <- ip2
I0521 04:33:09.050465 13815 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:33:09.050992 13815 net.cpp:150] Setting up relu6
I0521 04:33:09.051008 13815 net.cpp:157] Top shape: 660 98 (64680)
I0521 04:33:09.051018 13815 net.cpp:165] Memory required for data: 1041683280
I0521 04:33:09.051028 13815 layer_factory.hpp:77] Creating layer drop2
I0521 04:33:09.051041 13815 net.cpp:106] Creating Layer drop2
I0521 04:33:09.051051 13815 net.cpp:454] drop2 <- ip2
I0521 04:33:09.051064 13815 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:33:09.051108 13815 net.cpp:150] Setting up drop2
I0521 04:33:09.051121 13815 net.cpp:157] Top shape: 660 98 (64680)
I0521 04:33:09.051132 13815 net.cpp:165] Memory required for data: 1041942000
I0521 04:33:09.051142 13815 layer_factory.hpp:77] Creating layer ip3
I0521 04:33:09.051156 13815 net.cpp:106] Creating Layer ip3
I0521 04:33:09.051167 13815 net.cpp:454] ip3 <- ip2
I0521 04:33:09.051180 13815 net.cpp:411] ip3 -> ip3
I0521 04:33:09.051400 13815 net.cpp:150] Setting up ip3
I0521 04:33:09.051414 13815 net.cpp:157] Top shape: 660 11 (7260)
I0521 04:33:09.051424 13815 net.cpp:165] Memory required for data: 1041971040
I0521 04:33:09.051440 13815 layer_factory.hpp:77] Creating layer drop3
I0521 04:33:09.051452 13815 net.cpp:106] Creating Layer drop3
I0521 04:33:09.051461 13815 net.cpp:454] drop3 <- ip3
I0521 04:33:09.051475 13815 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:33:09.051517 13815 net.cpp:150] Setting up drop3
I0521 04:33:09.051529 13815 net.cpp:157] Top shape: 660 11 (7260)
I0521 04:33:09.051538 13815 net.cpp:165] Memory required for data: 1042000080
I0521 04:33:09.051548 13815 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 04:33:09.051561 13815 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 04:33:09.051571 13815 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 04:33:09.051584 13815 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 04:33:09.051600 13815 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 04:33:09.051672 13815 net.cpp:150] Setting up ip3_drop3_0_split
I0521 04:33:09.051686 13815 net.cpp:157] Top shape: 660 11 (7260)
I0521 04:33:09.051698 13815 net.cpp:157] Top shape: 660 11 (7260)
I0521 04:33:09.051707 13815 net.cpp:165] Memory required for data: 1042058160
I0521 04:33:09.051717 13815 layer_factory.hpp:77] Creating layer accuracy
I0521 04:33:09.051738 13815 net.cpp:106] Creating Layer accuracy
I0521 04:33:09.051748 13815 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 04:33:09.051759 13815 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 04:33:09.051774 13815 net.cpp:411] accuracy -> accuracy
I0521 04:33:09.051796 13815 net.cpp:150] Setting up accuracy
I0521 04:33:09.051808 13815 net.cpp:157] Top shape: (1)
I0521 04:33:09.051817 13815 net.cpp:165] Memory required for data: 1042058164
I0521 04:33:09.051827 13815 layer_factory.hpp:77] Creating layer loss
I0521 04:33:09.051839 13815 net.cpp:106] Creating Layer loss
I0521 04:33:09.051849 13815 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 04:33:09.051861 13815 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 04:33:09.051873 13815 net.cpp:411] loss -> loss
I0521 04:33:09.051892 13815 layer_factory.hpp:77] Creating layer loss
I0521 04:33:09.052382 13815 net.cpp:150] Setting up loss
I0521 04:33:09.052397 13815 net.cpp:157] Top shape: (1)
I0521 04:33:09.052407 13815 net.cpp:160]     with loss weight 1
I0521 04:33:09.052424 13815 net.cpp:165] Memory required for data: 1042058168
I0521 04:33:09.052435 13815 net.cpp:226] loss needs backward computation.
I0521 04:33:09.052446 13815 net.cpp:228] accuracy does not need backward computation.
I0521 04:33:09.052458 13815 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 04:33:09.052469 13815 net.cpp:226] drop3 needs backward computation.
I0521 04:33:09.052479 13815 net.cpp:226] ip3 needs backward computation.
I0521 04:33:09.052486 13815 net.cpp:226] drop2 needs backward computation.
I0521 04:33:09.052505 13815 net.cpp:226] relu6 needs backward computation.
I0521 04:33:09.052515 13815 net.cpp:226] ip2 needs backward computation.
I0521 04:33:09.052525 13815 net.cpp:226] drop1 needs backward computation.
I0521 04:33:09.052534 13815 net.cpp:226] relu5 needs backward computation.
I0521 04:33:09.052551 13815 net.cpp:226] ip1 needs backward computation.
I0521 04:33:09.052561 13815 net.cpp:226] pool4 needs backward computation.
I0521 04:33:09.052572 13815 net.cpp:226] relu4 needs backward computation.
I0521 04:33:09.052582 13815 net.cpp:226] conv4 needs backward computation.
I0521 04:33:09.052592 13815 net.cpp:226] pool3 needs backward computation.
I0521 04:33:09.052603 13815 net.cpp:226] relu3 needs backward computation.
I0521 04:33:09.052610 13815 net.cpp:226] conv3 needs backward computation.
I0521 04:33:09.052621 13815 net.cpp:226] pool2 needs backward computation.
I0521 04:33:09.052631 13815 net.cpp:226] relu2 needs backward computation.
I0521 04:33:09.052641 13815 net.cpp:226] conv2 needs backward computation.
I0521 04:33:09.052651 13815 net.cpp:226] pool1 needs backward computation.
I0521 04:33:09.052662 13815 net.cpp:226] relu1 needs backward computation.
I0521 04:33:09.052671 13815 net.cpp:226] conv1 needs backward computation.
I0521 04:33:09.052682 13815 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 04:33:09.052695 13815 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:33:09.052703 13815 net.cpp:270] This network produces output accuracy
I0521 04:33:09.052714 13815 net.cpp:270] This network produces output loss
I0521 04:33:09.052742 13815 net.cpp:283] Network initialization done.
I0521 04:33:09.052875 13815 solver.cpp:60] Solver scaffolding done.
I0521 04:33:09.054006 13815 caffe.cpp:212] Starting Optimization
I0521 04:33:09.054024 13815 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 04:33:09.054038 13815 solver.cpp:289] Learning Rate Policy: fixed
I0521 04:33:09.055254 13815 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 04:33:54.968793 13815 solver.cpp:409]     Test net output #0: accuracy = 0.0921839
I0521 04:33:54.968964 13815 solver.cpp:409]     Test net output #1: loss = 2.39862 (* 1 = 2.39862 loss)
I0521 04:33:55.093917 13815 solver.cpp:237] Iteration 0, loss = 2.39908
I0521 04:33:55.093955 13815 solver.cpp:253]     Train net output #0: loss = 2.39908 (* 1 = 2.39908 loss)
I0521 04:33:55.093972 13815 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 04:34:02.882498 13815 solver.cpp:237] Iteration 22, loss = 2.38325
I0521 04:34:02.882534 13815 solver.cpp:253]     Train net output #0: loss = 2.38325 (* 1 = 2.38325 loss)
I0521 04:34:02.882550 13815 sgd_solver.cpp:106] Iteration 22, lr = 0.0025
I0521 04:34:10.664916 13815 solver.cpp:237] Iteration 44, loss = 2.37207
I0521 04:34:10.664948 13815 solver.cpp:253]     Train net output #0: loss = 2.37207 (* 1 = 2.37207 loss)
I0521 04:34:10.664966 13815 sgd_solver.cpp:106] Iteration 44, lr = 0.0025
I0521 04:34:18.444703 13815 solver.cpp:237] Iteration 66, loss = 2.34964
I0521 04:34:18.444744 13815 solver.cpp:253]     Train net output #0: loss = 2.34964 (* 1 = 2.34964 loss)
I0521 04:34:18.444757 13815 sgd_solver.cpp:106] Iteration 66, lr = 0.0025
I0521 04:34:26.226696 13815 solver.cpp:237] Iteration 88, loss = 2.32024
I0521 04:34:26.226838 13815 solver.cpp:253]     Train net output #0: loss = 2.32024 (* 1 = 2.32024 loss)
I0521 04:34:26.226852 13815 sgd_solver.cpp:106] Iteration 88, lr = 0.0025
I0521 04:34:34.009608 13815 solver.cpp:237] Iteration 110, loss = 2.33084
I0521 04:34:34.009641 13815 solver.cpp:253]     Train net output #0: loss = 2.33084 (* 1 = 2.33084 loss)
I0521 04:34:34.009659 13815 sgd_solver.cpp:106] Iteration 110, lr = 0.0025
I0521 04:34:41.788064 13815 solver.cpp:237] Iteration 132, loss = 2.33625
I0521 04:34:41.788110 13815 solver.cpp:253]     Train net output #0: loss = 2.33625 (* 1 = 2.33625 loss)
I0521 04:34:41.788126 13815 sgd_solver.cpp:106] Iteration 132, lr = 0.0025
I0521 04:35:11.681216 13815 solver.cpp:237] Iteration 154, loss = 2.33426
I0521 04:35:11.681376 13815 solver.cpp:253]     Train net output #0: loss = 2.33426 (* 1 = 2.33426 loss)
I0521 04:35:11.681391 13815 sgd_solver.cpp:106] Iteration 154, lr = 0.0025
I0521 04:35:19.467289 13815 solver.cpp:237] Iteration 176, loss = 2.30672
I0521 04:35:19.467322 13815 solver.cpp:253]     Train net output #0: loss = 2.30672 (* 1 = 2.30672 loss)
I0521 04:35:19.467340 13815 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 04:35:27.254405 13815 solver.cpp:237] Iteration 198, loss = 2.30897
I0521 04:35:27.254444 13815 solver.cpp:253]     Train net output #0: loss = 2.30897 (* 1 = 2.30897 loss)
I0521 04:35:27.254464 13815 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 04:35:35.031164 13815 solver.cpp:237] Iteration 220, loss = 2.2933
I0521 04:35:35.031198 13815 solver.cpp:253]     Train net output #0: loss = 2.2933 (* 1 = 2.2933 loss)
I0521 04:35:35.031214 13815 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0521 04:35:37.154152 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_227.caffemodel
I0521 04:35:37.443588 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_227.solverstate
I0521 04:35:42.885246 13815 solver.cpp:237] Iteration 242, loss = 2.26078
I0521 04:35:42.885403 13815 solver.cpp:253]     Train net output #0: loss = 2.26078 (* 1 = 2.26078 loss)
I0521 04:35:42.885417 13815 sgd_solver.cpp:106] Iteration 242, lr = 0.0025
I0521 04:35:50.671937 13815 solver.cpp:237] Iteration 264, loss = 2.24875
I0521 04:35:50.671969 13815 solver.cpp:253]     Train net output #0: loss = 2.24875 (* 1 = 2.24875 loss)
I0521 04:35:50.671986 13815 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0521 04:35:58.458757 13815 solver.cpp:237] Iteration 286, loss = 2.21495
I0521 04:35:58.458801 13815 solver.cpp:253]     Train net output #0: loss = 2.21495 (* 1 = 2.21495 loss)
I0521 04:35:58.458817 13815 sgd_solver.cpp:106] Iteration 286, lr = 0.0025
I0521 04:36:28.375633 13815 solver.cpp:237] Iteration 308, loss = 2.15368
I0521 04:36:28.375788 13815 solver.cpp:253]     Train net output #0: loss = 2.15368 (* 1 = 2.15368 loss)
I0521 04:36:28.375802 13815 sgd_solver.cpp:106] Iteration 308, lr = 0.0025
I0521 04:36:36.158310 13815 solver.cpp:237] Iteration 330, loss = 2.17003
I0521 04:36:36.158344 13815 solver.cpp:253]     Train net output #0: loss = 2.17003 (* 1 = 2.17003 loss)
I0521 04:36:36.158360 13815 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 04:36:43.946362 13815 solver.cpp:237] Iteration 352, loss = 2.104
I0521 04:36:43.946409 13815 solver.cpp:253]     Train net output #0: loss = 2.104 (* 1 = 2.104 loss)
I0521 04:36:43.946424 13815 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 04:36:51.728091 13815 solver.cpp:237] Iteration 374, loss = 2.11432
I0521 04:36:51.728124 13815 solver.cpp:253]     Train net output #0: loss = 2.11432 (* 1 = 2.11432 loss)
I0521 04:36:51.728142 13815 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 04:36:59.517745 13815 solver.cpp:237] Iteration 396, loss = 2.05387
I0521 04:36:59.517891 13815 solver.cpp:253]     Train net output #0: loss = 2.05387 (* 1 = 2.05387 loss)
I0521 04:36:59.517905 13815 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 04:37:07.301909 13815 solver.cpp:237] Iteration 418, loss = 2.06219
I0521 04:37:07.301954 13815 solver.cpp:253]     Train net output #0: loss = 2.06219 (* 1 = 2.06219 loss)
I0521 04:37:07.301970 13815 sgd_solver.cpp:106] Iteration 418, lr = 0.0025
I0521 04:37:15.086689 13815 solver.cpp:237] Iteration 440, loss = 2.03901
I0521 04:37:15.086724 13815 solver.cpp:253]     Train net output #0: loss = 2.03901 (* 1 = 2.03901 loss)
I0521 04:37:15.086740 13815 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0521 04:37:19.688266 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_454.caffemodel
I0521 04:37:19.974361 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_454.solverstate
I0521 04:37:20.000434 13815 solver.cpp:341] Iteration 454, Testing net (#0)
I0521 04:38:04.962942 13815 solver.cpp:409]     Test net output #0: accuracy = 0.51379
I0521 04:38:04.963110 13815 solver.cpp:409]     Test net output #1: loss = 1.77995 (* 1 = 1.77995 loss)
I0521 04:38:29.986263 13815 solver.cpp:237] Iteration 462, loss = 1.96588
I0521 04:38:29.986313 13815 solver.cpp:253]     Train net output #0: loss = 1.96588 (* 1 = 1.96588 loss)
I0521 04:38:29.986330 13815 sgd_solver.cpp:106] Iteration 462, lr = 0.0025
I0521 04:38:37.762768 13815 solver.cpp:237] Iteration 484, loss = 1.98296
I0521 04:38:37.762909 13815 solver.cpp:253]     Train net output #0: loss = 1.98296 (* 1 = 1.98296 loss)
I0521 04:38:37.762923 13815 sgd_solver.cpp:106] Iteration 484, lr = 0.0025
I0521 04:38:45.543730 13815 solver.cpp:237] Iteration 506, loss = 1.97039
I0521 04:38:45.543761 13815 solver.cpp:253]     Train net output #0: loss = 1.97039 (* 1 = 1.97039 loss)
I0521 04:38:45.543776 13815 sgd_solver.cpp:106] Iteration 506, lr = 0.0025
I0521 04:38:53.330456 13815 solver.cpp:237] Iteration 528, loss = 1.9887
I0521 04:38:53.330497 13815 solver.cpp:253]     Train net output #0: loss = 1.9887 (* 1 = 1.9887 loss)
I0521 04:38:53.330513 13815 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 04:39:01.111780 13815 solver.cpp:237] Iteration 550, loss = 1.89548
I0521 04:39:01.111814 13815 solver.cpp:253]     Train net output #0: loss = 1.89548 (* 1 = 1.89548 loss)
I0521 04:39:01.111827 13815 sgd_solver.cpp:106] Iteration 550, lr = 0.0025
I0521 04:39:08.890450 13815 solver.cpp:237] Iteration 572, loss = 1.96711
I0521 04:39:08.890589 13815 solver.cpp:253]     Train net output #0: loss = 1.96711 (* 1 = 1.96711 loss)
I0521 04:39:08.890604 13815 sgd_solver.cpp:106] Iteration 572, lr = 0.0025
I0521 04:39:16.671802 13815 solver.cpp:237] Iteration 594, loss = 1.92374
I0521 04:39:16.671833 13815 solver.cpp:253]     Train net output #0: loss = 1.92374 (* 1 = 1.92374 loss)
I0521 04:39:16.671854 13815 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 04:39:46.567914 13815 solver.cpp:237] Iteration 616, loss = 1.95832
I0521 04:39:46.568084 13815 solver.cpp:253]     Train net output #0: loss = 1.95832 (* 1 = 1.95832 loss)
I0521 04:39:46.568099 13815 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0521 04:39:54.351686 13815 solver.cpp:237] Iteration 638, loss = 1.9052
I0521 04:39:54.351717 13815 solver.cpp:253]     Train net output #0: loss = 1.9052 (* 1 = 1.9052 loss)
I0521 04:39:54.351732 13815 sgd_solver.cpp:106] Iteration 638, lr = 0.0025
I0521 04:40:02.136699 13815 solver.cpp:237] Iteration 660, loss = 1.87291
I0521 04:40:02.136739 13815 solver.cpp:253]     Train net output #0: loss = 1.87291 (* 1 = 1.87291 loss)
I0521 04:40:02.136751 13815 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 04:40:09.210374 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_681.caffemodel
I0521 04:40:09.499678 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_681.solverstate
I0521 04:40:09.986161 13815 solver.cpp:237] Iteration 682, loss = 1.85126
I0521 04:40:09.986208 13815 solver.cpp:253]     Train net output #0: loss = 1.85126 (* 1 = 1.85126 loss)
I0521 04:40:09.986223 13815 sgd_solver.cpp:106] Iteration 682, lr = 0.0025
I0521 04:40:17.765053 13815 solver.cpp:237] Iteration 704, loss = 1.82984
I0521 04:40:17.765194 13815 solver.cpp:253]     Train net output #0: loss = 1.82984 (* 1 = 1.82984 loss)
I0521 04:40:17.765208 13815 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 04:40:25.537087 13815 solver.cpp:237] Iteration 726, loss = 1.90522
I0521 04:40:25.537118 13815 solver.cpp:253]     Train net output #0: loss = 1.90522 (* 1 = 1.90522 loss)
I0521 04:40:25.537137 13815 sgd_solver.cpp:106] Iteration 726, lr = 0.0025
I0521 04:40:33.317486 13815 solver.cpp:237] Iteration 748, loss = 1.83074
I0521 04:40:33.317523 13815 solver.cpp:253]     Train net output #0: loss = 1.83074 (* 1 = 1.83074 loss)
I0521 04:40:33.317544 13815 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 04:41:03.218360 13815 solver.cpp:237] Iteration 770, loss = 1.87595
I0521 04:41:03.218529 13815 solver.cpp:253]     Train net output #0: loss = 1.87595 (* 1 = 1.87595 loss)
I0521 04:41:03.218544 13815 sgd_solver.cpp:106] Iteration 770, lr = 0.0025
I0521 04:41:11.002547 13815 solver.cpp:237] Iteration 792, loss = 1.86838
I0521 04:41:11.002578 13815 solver.cpp:253]     Train net output #0: loss = 1.86838 (* 1 = 1.86838 loss)
I0521 04:41:11.002596 13815 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 04:41:18.782253 13815 solver.cpp:237] Iteration 814, loss = 1.80194
I0521 04:41:18.782299 13815 solver.cpp:253]     Train net output #0: loss = 1.80194 (* 1 = 1.80194 loss)
I0521 04:41:18.782312 13815 sgd_solver.cpp:106] Iteration 814, lr = 0.0025
I0521 04:41:26.557610 13815 solver.cpp:237] Iteration 836, loss = 1.83801
I0521 04:41:26.557642 13815 solver.cpp:253]     Train net output #0: loss = 1.83801 (* 1 = 1.83801 loss)
I0521 04:41:26.557659 13815 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0521 04:41:34.335954 13815 solver.cpp:237] Iteration 858, loss = 1.80554
I0521 04:41:34.336087 13815 solver.cpp:253]     Train net output #0: loss = 1.80554 (* 1 = 1.80554 loss)
I0521 04:41:34.336100 13815 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0521 04:41:42.114750 13815 solver.cpp:237] Iteration 880, loss = 1.8098
I0521 04:41:42.114799 13815 solver.cpp:253]     Train net output #0: loss = 1.8098 (* 1 = 1.8098 loss)
I0521 04:41:42.114814 13815 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 04:41:49.897526 13815 solver.cpp:237] Iteration 902, loss = 1.80835
I0521 04:41:49.897559 13815 solver.cpp:253]     Train net output #0: loss = 1.80835 (* 1 = 1.80835 loss)
I0521 04:41:49.897572 13815 sgd_solver.cpp:106] Iteration 902, lr = 0.0025
I0521 04:41:51.663267 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_908.caffemodel
I0521 04:41:51.951247 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_908.solverstate
I0521 04:41:51.978296 13815 solver.cpp:341] Iteration 908, Testing net (#0)
I0521 04:42:57.791012 13815 solver.cpp:409]     Test net output #0: accuracy = 0.61407
I0521 04:42:57.791187 13815 solver.cpp:409]     Test net output #1: loss = 1.34888 (* 1 = 1.34888 loss)
I0521 04:43:25.684021 13815 solver.cpp:237] Iteration 924, loss = 1.84245
I0521 04:43:25.684072 13815 solver.cpp:253]     Train net output #0: loss = 1.84245 (* 1 = 1.84245 loss)
I0521 04:43:25.684089 13815 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 04:43:33.455359 13815 solver.cpp:237] Iteration 946, loss = 1.76627
I0521 04:43:33.455505 13815 solver.cpp:253]     Train net output #0: loss = 1.76627 (* 1 = 1.76627 loss)
I0521 04:43:33.455519 13815 sgd_solver.cpp:106] Iteration 946, lr = 0.0025
I0521 04:43:41.231467 13815 solver.cpp:237] Iteration 968, loss = 1.89988
I0521 04:43:41.231498 13815 solver.cpp:253]     Train net output #0: loss = 1.89988 (* 1 = 1.89988 loss)
I0521 04:43:41.231516 13815 sgd_solver.cpp:106] Iteration 968, lr = 0.0025
I0521 04:43:49.005290 13815 solver.cpp:237] Iteration 990, loss = 1.8779
I0521 04:43:49.005331 13815 solver.cpp:253]     Train net output #0: loss = 1.8779 (* 1 = 1.8779 loss)
I0521 04:43:49.005350 13815 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 04:43:56.773320 13815 solver.cpp:237] Iteration 1012, loss = 1.75762
I0521 04:43:56.773353 13815 solver.cpp:253]     Train net output #0: loss = 1.75762 (* 1 = 1.75762 loss)
I0521 04:43:56.773370 13815 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0521 04:44:04.543665 13815 solver.cpp:237] Iteration 1034, loss = 1.85604
I0521 04:44:04.543803 13815 solver.cpp:253]     Train net output #0: loss = 1.85604 (* 1 = 1.85604 loss)
I0521 04:44:04.543817 13815 sgd_solver.cpp:106] Iteration 1034, lr = 0.0025
I0521 04:44:12.311918 13815 solver.cpp:237] Iteration 1056, loss = 1.86395
I0521 04:44:12.311955 13815 solver.cpp:253]     Train net output #0: loss = 1.86395 (* 1 = 1.86395 loss)
I0521 04:44:12.311971 13815 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 04:44:42.196410 13815 solver.cpp:237] Iteration 1078, loss = 1.81582
I0521 04:44:42.196574 13815 solver.cpp:253]     Train net output #0: loss = 1.81582 (* 1 = 1.81582 loss)
I0521 04:44:42.196589 13815 sgd_solver.cpp:106] Iteration 1078, lr = 0.0025
I0521 04:44:49.967610 13815 solver.cpp:237] Iteration 1100, loss = 1.79054
I0521 04:44:49.967643 13815 solver.cpp:253]     Train net output #0: loss = 1.79054 (* 1 = 1.79054 loss)
I0521 04:44:49.967660 13815 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 04:44:57.739503 13815 solver.cpp:237] Iteration 1122, loss = 1.79223
I0521 04:44:57.739536 13815 solver.cpp:253]     Train net output #0: loss = 1.79223 (* 1 = 1.79223 loss)
I0521 04:44:57.739553 13815 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 04:45:01.981498 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1135.caffemodel
I0521 04:45:02.269459 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1135.solverstate
I0521 04:45:05.583076 13815 solver.cpp:237] Iteration 1144, loss = 1.78707
I0521 04:45:05.583123 13815 solver.cpp:253]     Train net output #0: loss = 1.78707 (* 1 = 1.78707 loss)
I0521 04:45:05.583139 13815 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0521 04:45:13.356498 13815 solver.cpp:237] Iteration 1166, loss = 1.83195
I0521 04:45:13.356667 13815 solver.cpp:253]     Train net output #0: loss = 1.83195 (* 1 = 1.83195 loss)
I0521 04:45:13.356679 13815 sgd_solver.cpp:106] Iteration 1166, lr = 0.0025
I0521 04:45:21.126498 13815 solver.cpp:237] Iteration 1188, loss = 1.73137
I0521 04:45:21.126530 13815 solver.cpp:253]     Train net output #0: loss = 1.73137 (* 1 = 1.73137 loss)
I0521 04:45:21.126548 13815 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 04:45:28.897858 13815 solver.cpp:237] Iteration 1210, loss = 1.70702
I0521 04:45:28.897902 13815 solver.cpp:253]     Train net output #0: loss = 1.70702 (* 1 = 1.70702 loss)
I0521 04:45:28.897915 13815 sgd_solver.cpp:106] Iteration 1210, lr = 0.0025
I0521 04:45:58.834174 13815 solver.cpp:237] Iteration 1232, loss = 1.75249
I0521 04:45:58.834336 13815 solver.cpp:253]     Train net output #0: loss = 1.75249 (* 1 = 1.75249 loss)
I0521 04:45:58.834349 13815 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 04:46:06.608868 13815 solver.cpp:237] Iteration 1254, loss = 1.71061
I0521 04:46:06.608901 13815 solver.cpp:253]     Train net output #0: loss = 1.71061 (* 1 = 1.71061 loss)
I0521 04:46:06.608919 13815 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0521 04:46:14.376914 13815 solver.cpp:237] Iteration 1276, loss = 1.73354
I0521 04:46:14.376947 13815 solver.cpp:253]     Train net output #0: loss = 1.73354 (* 1 = 1.73354 loss)
I0521 04:46:14.376963 13815 sgd_solver.cpp:106] Iteration 1276, lr = 0.0025
I0521 04:46:22.145897 13815 solver.cpp:237] Iteration 1298, loss = 1.73705
I0521 04:46:22.145941 13815 solver.cpp:253]     Train net output #0: loss = 1.73705 (* 1 = 1.73705 loss)
I0521 04:46:22.145956 13815 sgd_solver.cpp:106] Iteration 1298, lr = 0.0025
I0521 04:46:29.919574 13815 solver.cpp:237] Iteration 1320, loss = 1.82103
I0521 04:46:29.919711 13815 solver.cpp:253]     Train net output #0: loss = 1.82103 (* 1 = 1.82103 loss)
I0521 04:46:29.919724 13815 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 04:46:37.697016 13815 solver.cpp:237] Iteration 1342, loss = 1.75056
I0521 04:46:37.697046 13815 solver.cpp:253]     Train net output #0: loss = 1.75056 (* 1 = 1.75056 loss)
I0521 04:46:37.697065 13815 sgd_solver.cpp:106] Iteration 1342, lr = 0.0025
I0521 04:46:44.414778 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1362.caffemodel
I0521 04:46:44.700639 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1362.solverstate
I0521 04:46:44.727094 13815 solver.cpp:341] Iteration 1362, Testing net (#0)
I0521 04:47:29.392325 13815 solver.cpp:409]     Test net output #0: accuracy = 0.622287
I0521 04:47:29.392487 13815 solver.cpp:409]     Test net output #1: loss = 1.30188 (* 1 = 1.30188 loss)
I0521 04:47:52.326213 13815 solver.cpp:237] Iteration 1364, loss = 1.80501
I0521 04:47:52.326264 13815 solver.cpp:253]     Train net output #0: loss = 1.80501 (* 1 = 1.80501 loss)
I0521 04:47:52.326278 13815 sgd_solver.cpp:106] Iteration 1364, lr = 0.0025
I0521 04:48:00.096817 13815 solver.cpp:237] Iteration 1386, loss = 1.72119
I0521 04:48:00.096978 13815 solver.cpp:253]     Train net output #0: loss = 1.72119 (* 1 = 1.72119 loss)
I0521 04:48:00.096993 13815 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 04:48:07.862668 13815 solver.cpp:237] Iteration 1408, loss = 1.74544
I0521 04:48:07.862701 13815 solver.cpp:253]     Train net output #0: loss = 1.74544 (* 1 = 1.74544 loss)
I0521 04:48:07.862720 13815 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 04:48:15.627965 13815 solver.cpp:237] Iteration 1430, loss = 1.74712
I0521 04:48:15.627997 13815 solver.cpp:253]     Train net output #0: loss = 1.74712 (* 1 = 1.74712 loss)
I0521 04:48:15.628015 13815 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0521 04:48:23.392326 13815 solver.cpp:237] Iteration 1452, loss = 1.74625
I0521 04:48:23.392369 13815 solver.cpp:253]     Train net output #0: loss = 1.74625 (* 1 = 1.74625 loss)
I0521 04:48:23.392384 13815 sgd_solver.cpp:106] Iteration 1452, lr = 0.0025
I0521 04:48:31.159375 13815 solver.cpp:237] Iteration 1474, loss = 1.69595
I0521 04:48:31.159523 13815 solver.cpp:253]     Train net output #0: loss = 1.69595 (* 1 = 1.69595 loss)
I0521 04:48:31.159539 13815 sgd_solver.cpp:106] Iteration 1474, lr = 0.0025
I0521 04:48:38.929514 13815 solver.cpp:237] Iteration 1496, loss = 1.78649
I0521 04:48:38.929545 13815 solver.cpp:253]     Train net output #0: loss = 1.78649 (* 1 = 1.78649 loss)
I0521 04:48:38.929563 13815 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 04:49:08.858500 13815 solver.cpp:237] Iteration 1518, loss = 1.6941
I0521 04:49:08.858669 13815 solver.cpp:253]     Train net output #0: loss = 1.6941 (* 1 = 1.6941 loss)
I0521 04:49:08.858683 13815 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0521 04:49:16.631765 13815 solver.cpp:237] Iteration 1540, loss = 1.69853
I0521 04:49:16.631809 13815 solver.cpp:253]     Train net output #0: loss = 1.69853 (* 1 = 1.69853 loss)
I0521 04:49:16.631826 13815 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 04:49:24.401442 13815 solver.cpp:237] Iteration 1562, loss = 1.65659
I0521 04:49:24.401475 13815 solver.cpp:253]     Train net output #0: loss = 1.65659 (* 1 = 1.65659 loss)
I0521 04:49:24.401491 13815 sgd_solver.cpp:106] Iteration 1562, lr = 0.0025
I0521 04:49:32.178134 13815 solver.cpp:237] Iteration 1584, loss = 1.7094
I0521 04:49:32.178167 13815 solver.cpp:253]     Train net output #0: loss = 1.7094 (* 1 = 1.7094 loss)
I0521 04:49:32.178184 13815 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 04:49:33.590684 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1589.caffemodel
I0521 04:49:33.877724 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1589.solverstate
I0521 04:49:40.018266 13815 solver.cpp:237] Iteration 1606, loss = 1.65804
I0521 04:49:40.018427 13815 solver.cpp:253]     Train net output #0: loss = 1.65804 (* 1 = 1.65804 loss)
I0521 04:49:40.018442 13815 sgd_solver.cpp:106] Iteration 1606, lr = 0.0025
I0521 04:49:47.789705 13815 solver.cpp:237] Iteration 1628, loss = 1.64209
I0521 04:49:47.789736 13815 solver.cpp:253]     Train net output #0: loss = 1.64209 (* 1 = 1.64209 loss)
I0521 04:49:47.789752 13815 sgd_solver.cpp:106] Iteration 1628, lr = 0.0025
I0521 04:49:55.562582 13815 solver.cpp:237] Iteration 1650, loss = 1.6628
I0521 04:49:55.562614 13815 solver.cpp:253]     Train net output #0: loss = 1.6628 (* 1 = 1.6628 loss)
I0521 04:49:55.562631 13815 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 04:50:25.483515 13815 solver.cpp:237] Iteration 1672, loss = 1.70233
I0521 04:50:25.483681 13815 solver.cpp:253]     Train net output #0: loss = 1.70233 (* 1 = 1.70233 loss)
I0521 04:50:25.483696 13815 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0521 04:50:33.253015 13815 solver.cpp:237] Iteration 1694, loss = 1.68326
I0521 04:50:33.253056 13815 solver.cpp:253]     Train net output #0: loss = 1.68326 (* 1 = 1.68326 loss)
I0521 04:50:33.253073 13815 sgd_solver.cpp:106] Iteration 1694, lr = 0.0025
I0521 04:50:41.027158 13815 solver.cpp:237] Iteration 1716, loss = 1.69151
I0521 04:50:41.027191 13815 solver.cpp:253]     Train net output #0: loss = 1.69151 (* 1 = 1.69151 loss)
I0521 04:50:41.027206 13815 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0521 04:50:48.795017 13815 solver.cpp:237] Iteration 1738, loss = 1.71697
I0521 04:50:48.795049 13815 solver.cpp:253]     Train net output #0: loss = 1.71697 (* 1 = 1.71697 loss)
I0521 04:50:48.795068 13815 sgd_solver.cpp:106] Iteration 1738, lr = 0.0025
I0521 04:50:56.569423 13815 solver.cpp:237] Iteration 1760, loss = 1.75317
I0521 04:50:56.569576 13815 solver.cpp:253]     Train net output #0: loss = 1.75317 (* 1 = 1.75317 loss)
I0521 04:50:56.569591 13815 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0521 04:51:04.341838 13815 solver.cpp:237] Iteration 1782, loss = 1.67673
I0521 04:51:04.341869 13815 solver.cpp:253]     Train net output #0: loss = 1.67673 (* 1 = 1.67673 loss)
I0521 04:51:04.341887 13815 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 04:51:12.112890 13815 solver.cpp:237] Iteration 1804, loss = 1.73193
I0521 04:51:12.112922 13815 solver.cpp:253]     Train net output #0: loss = 1.73193 (* 1 = 1.73193 loss)
I0521 04:51:12.112937 13815 sgd_solver.cpp:106] Iteration 1804, lr = 0.0025
I0521 04:51:15.997789 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1816.caffemodel
I0521 04:51:16.287611 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_1816.solverstate
I0521 04:51:16.313730 13815 solver.cpp:341] Iteration 1816, Testing net (#0)
I0521 04:52:22.236672 13815 solver.cpp:409]     Test net output #0: accuracy = 0.656668
I0521 04:52:22.236847 13815 solver.cpp:409]     Test net output #1: loss = 1.18619 (* 1 = 1.18619 loss)
I0521 04:52:48.018324 13815 solver.cpp:237] Iteration 1826, loss = 1.62545
I0521 04:52:48.018374 13815 solver.cpp:253]     Train net output #0: loss = 1.62545 (* 1 = 1.62545 loss)
I0521 04:52:48.018388 13815 sgd_solver.cpp:106] Iteration 1826, lr = 0.0025
I0521 04:52:55.794723 13815 solver.cpp:237] Iteration 1848, loss = 1.65546
I0521 04:52:55.794873 13815 solver.cpp:253]     Train net output #0: loss = 1.65546 (* 1 = 1.65546 loss)
I0521 04:52:55.794886 13815 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 04:53:03.567541 13815 solver.cpp:237] Iteration 1870, loss = 1.65153
I0521 04:53:03.567587 13815 solver.cpp:253]     Train net output #0: loss = 1.65153 (* 1 = 1.65153 loss)
I0521 04:53:03.567600 13815 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0521 04:53:11.346089 13815 solver.cpp:237] Iteration 1892, loss = 1.63189
I0521 04:53:11.346122 13815 solver.cpp:253]     Train net output #0: loss = 1.63189 (* 1 = 1.63189 loss)
I0521 04:53:11.346138 13815 sgd_solver.cpp:106] Iteration 1892, lr = 0.0025
I0521 04:53:19.123476 13815 solver.cpp:237] Iteration 1914, loss = 1.67494
I0521 04:53:19.123509 13815 solver.cpp:253]     Train net output #0: loss = 1.67494 (* 1 = 1.67494 loss)
I0521 04:53:19.123524 13815 sgd_solver.cpp:106] Iteration 1914, lr = 0.0025
I0521 04:53:26.895417 13815 solver.cpp:237] Iteration 1936, loss = 1.66544
I0521 04:53:26.895568 13815 solver.cpp:253]     Train net output #0: loss = 1.66544 (* 1 = 1.66544 loss)
I0521 04:53:26.895581 13815 sgd_solver.cpp:106] Iteration 1936, lr = 0.0025
I0521 04:53:34.671665 13815 solver.cpp:237] Iteration 1958, loss = 1.69126
I0521 04:53:34.671697 13815 solver.cpp:253]     Train net output #0: loss = 1.69126 (* 1 = 1.69126 loss)
I0521 04:53:34.671715 13815 sgd_solver.cpp:106] Iteration 1958, lr = 0.0025
I0521 04:54:04.609403 13815 solver.cpp:237] Iteration 1980, loss = 1.72094
I0521 04:54:04.609581 13815 solver.cpp:253]     Train net output #0: loss = 1.72094 (* 1 = 1.72094 loss)
I0521 04:54:04.609596 13815 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 04:54:12.387729 13815 solver.cpp:237] Iteration 2002, loss = 1.6529
I0521 04:54:12.387771 13815 solver.cpp:253]     Train net output #0: loss = 1.6529 (* 1 = 1.6529 loss)
I0521 04:54:12.387790 13815 sgd_solver.cpp:106] Iteration 2002, lr = 0.0025
I0521 04:54:20.164453 13815 solver.cpp:237] Iteration 2024, loss = 1.7501
I0521 04:54:20.164486 13815 solver.cpp:253]     Train net output #0: loss = 1.7501 (* 1 = 1.7501 loss)
I0521 04:54:20.164502 13815 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0521 04:54:26.526103 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_2043.caffemodel
I0521 04:54:26.814092 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_2043.solverstate
I0521 04:54:28.008095 13815 solver.cpp:237] Iteration 2046, loss = 1.63003
I0521 04:54:28.008143 13815 solver.cpp:253]     Train net output #0: loss = 1.63003 (* 1 = 1.63003 loss)
I0521 04:54:28.008160 13815 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0521 04:54:35.784562 13815 solver.cpp:237] Iteration 2068, loss = 1.69021
I0521 04:54:35.784723 13815 solver.cpp:253]     Train net output #0: loss = 1.69021 (* 1 = 1.69021 loss)
I0521 04:54:35.784737 13815 sgd_solver.cpp:106] Iteration 2068, lr = 0.0025
I0521 04:54:43.559579 13815 solver.cpp:237] Iteration 2090, loss = 1.65013
I0521 04:54:43.559626 13815 solver.cpp:253]     Train net output #0: loss = 1.65013 (* 1 = 1.65013 loss)
I0521 04:54:43.559643 13815 sgd_solver.cpp:106] Iteration 2090, lr = 0.0025
I0521 04:54:51.333344 13815 solver.cpp:237] Iteration 2112, loss = 1.61077
I0521 04:54:51.333377 13815 solver.cpp:253]     Train net output #0: loss = 1.61077 (* 1 = 1.61077 loss)
I0521 04:54:51.333392 13815 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 04:55:21.246515 13815 solver.cpp:237] Iteration 2134, loss = 1.62814
I0521 04:55:21.246686 13815 solver.cpp:253]     Train net output #0: loss = 1.62814 (* 1 = 1.62814 loss)
I0521 04:55:21.246701 13815 sgd_solver.cpp:106] Iteration 2134, lr = 0.0025
I0521 04:55:29.022850 13815 solver.cpp:237] Iteration 2156, loss = 1.68185
I0521 04:55:29.022891 13815 solver.cpp:253]     Train net output #0: loss = 1.68185 (* 1 = 1.68185 loss)
I0521 04:55:29.022907 13815 sgd_solver.cpp:106] Iteration 2156, lr = 0.0025
I0521 04:55:36.794045 13815 solver.cpp:237] Iteration 2178, loss = 1.66833
I0521 04:55:36.794080 13815 solver.cpp:253]     Train net output #0: loss = 1.66833 (* 1 = 1.66833 loss)
I0521 04:55:36.794092 13815 sgd_solver.cpp:106] Iteration 2178, lr = 0.0025
I0521 04:55:44.571939 13815 solver.cpp:237] Iteration 2200, loss = 1.69736
I0521 04:55:44.571974 13815 solver.cpp:253]     Train net output #0: loss = 1.69736 (* 1 = 1.69736 loss)
I0521 04:55:44.571990 13815 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0521 04:55:52.346549 13815 solver.cpp:237] Iteration 2222, loss = 1.69421
I0521 04:55:52.346704 13815 solver.cpp:253]     Train net output #0: loss = 1.69421 (* 1 = 1.69421 loss)
I0521 04:55:52.346716 13815 sgd_solver.cpp:106] Iteration 2222, lr = 0.0025
I0521 04:56:00.124241 13815 solver.cpp:237] Iteration 2244, loss = 1.69432
I0521 04:56:00.124274 13815 solver.cpp:253]     Train net output #0: loss = 1.69432 (* 1 = 1.69432 loss)
I0521 04:56:00.124292 13815 sgd_solver.cpp:106] Iteration 2244, lr = 0.0025
I0521 04:56:07.901772 13815 solver.cpp:237] Iteration 2266, loss = 1.62421
I0521 04:56:07.901805 13815 solver.cpp:253]     Train net output #0: loss = 1.62421 (* 1 = 1.62421 loss)
I0521 04:56:07.901821 13815 sgd_solver.cpp:106] Iteration 2266, lr = 0.0025
I0521 04:56:08.960474 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_2270.caffemodel
I0521 04:56:09.249804 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_2270.solverstate
I0521 04:56:09.278079 13815 solver.cpp:341] Iteration 2270, Testing net (#0)
I0521 04:56:54.249246 13815 solver.cpp:409]     Test net output #0: accuracy = 0.678935
I0521 04:56:54.249408 13815 solver.cpp:409]     Test net output #1: loss = 1.10808 (* 1 = 1.10808 loss)
I0521 04:56:54.708878 13815 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_2272.caffemodel
I0521 04:56:54.997537 13815 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881_iter_2272.solverstate
I0521 04:56:55.025598 13815 solver.cpp:326] Optimization Done.
I0521 04:56:55.025624 13815 caffe.cpp:215] Optimization Done.
Application 11236808 resources: utime ~1247s, stime ~225s, Rss ~5333488, inblocks ~3594475, outblocks ~194564
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_660_2016-05-20T11.20.56.615881.solver"
	User time (seconds): 0.55
	System time (seconds): 0.12
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:35.34
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15075
	Voluntary context switches: 2721
	Involuntary context switches: 76
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
