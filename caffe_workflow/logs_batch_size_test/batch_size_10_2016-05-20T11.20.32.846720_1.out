2804901
I0520 11:20:59.975229 21902 caffe.cpp:184] Using GPUs 0
I0520 11:21:00.404322 21902 solver.cpp:48] Initializing solver from parameters: 
test_iter: 15000
test_interval: 30000
base_lr: 0.0025
display: 1500
max_iter: 150000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 15000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720.prototxt"
I0520 11:21:00.406175 21902 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720.prototxt
I0520 11:21:00.413126 21902 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0520 11:21:00.413187 21902 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0520 11:21:00.413535 21902 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 10
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 11:21:00.413714 21902 layer_factory.hpp:77] Creating layer data_hdf5
I0520 11:21:00.413738 21902 net.cpp:106] Creating Layer data_hdf5
I0520 11:21:00.413753 21902 net.cpp:411] data_hdf5 -> data
I0520 11:21:00.413784 21902 net.cpp:411] data_hdf5 -> label
I0520 11:21:00.413816 21902 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0520 11:21:00.415298 21902 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0520 11:21:00.417595 21902 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0520 11:21:21.976408 21902 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0520 11:21:21.981770 21902 net.cpp:150] Setting up data_hdf5
I0520 11:21:21.981811 21902 net.cpp:157] Top shape: 10 1 127 50 (63500)
I0520 11:21:21.981825 21902 net.cpp:157] Top shape: 10 (10)
I0520 11:21:21.981838 21902 net.cpp:165] Memory required for data: 254040
I0520 11:21:21.981851 21902 layer_factory.hpp:77] Creating layer conv1
I0520 11:21:21.981885 21902 net.cpp:106] Creating Layer conv1
I0520 11:21:21.981896 21902 net.cpp:454] conv1 <- data
I0520 11:21:21.981917 21902 net.cpp:411] conv1 -> conv1
I0520 11:21:23.979869 21902 net.cpp:150] Setting up conv1
I0520 11:21:23.979915 21902 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0520 11:21:23.979926 21902 net.cpp:165] Memory required for data: 3018840
I0520 11:21:23.979954 21902 layer_factory.hpp:77] Creating layer relu1
I0520 11:21:23.979975 21902 net.cpp:106] Creating Layer relu1
I0520 11:21:23.979986 21902 net.cpp:454] relu1 <- conv1
I0520 11:21:23.980000 21902 net.cpp:397] relu1 -> conv1 (in-place)
I0520 11:21:23.980523 21902 net.cpp:150] Setting up relu1
I0520 11:21:23.980540 21902 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0520 11:21:23.980551 21902 net.cpp:165] Memory required for data: 5783640
I0520 11:21:23.980561 21902 layer_factory.hpp:77] Creating layer pool1
I0520 11:21:23.980577 21902 net.cpp:106] Creating Layer pool1
I0520 11:21:23.980587 21902 net.cpp:454] pool1 <- conv1
I0520 11:21:23.980600 21902 net.cpp:411] pool1 -> pool1
I0520 11:21:23.980680 21902 net.cpp:150] Setting up pool1
I0520 11:21:23.980693 21902 net.cpp:157] Top shape: 10 12 60 48 (345600)
I0520 11:21:23.980703 21902 net.cpp:165] Memory required for data: 7166040
I0520 11:21:23.980711 21902 layer_factory.hpp:77] Creating layer conv2
I0520 11:21:23.980733 21902 net.cpp:106] Creating Layer conv2
I0520 11:21:23.980743 21902 net.cpp:454] conv2 <- pool1
I0520 11:21:23.980757 21902 net.cpp:411] conv2 -> conv2
I0520 11:21:23.983458 21902 net.cpp:150] Setting up conv2
I0520 11:21:23.983485 21902 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0520 11:21:23.983495 21902 net.cpp:165] Memory required for data: 9153240
I0520 11:21:23.983515 21902 layer_factory.hpp:77] Creating layer relu2
I0520 11:21:23.983530 21902 net.cpp:106] Creating Layer relu2
I0520 11:21:23.983538 21902 net.cpp:454] relu2 <- conv2
I0520 11:21:23.983551 21902 net.cpp:397] relu2 -> conv2 (in-place)
I0520 11:21:23.983883 21902 net.cpp:150] Setting up relu2
I0520 11:21:23.983898 21902 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0520 11:21:23.983907 21902 net.cpp:165] Memory required for data: 11140440
I0520 11:21:23.983918 21902 layer_factory.hpp:77] Creating layer pool2
I0520 11:21:23.983930 21902 net.cpp:106] Creating Layer pool2
I0520 11:21:23.983939 21902 net.cpp:454] pool2 <- conv2
I0520 11:21:23.983952 21902 net.cpp:411] pool2 -> pool2
I0520 11:21:23.984033 21902 net.cpp:150] Setting up pool2
I0520 11:21:23.984047 21902 net.cpp:157] Top shape: 10 20 27 46 (248400)
I0520 11:21:23.984056 21902 net.cpp:165] Memory required for data: 12134040
I0520 11:21:23.984066 21902 layer_factory.hpp:77] Creating layer conv3
I0520 11:21:23.984084 21902 net.cpp:106] Creating Layer conv3
I0520 11:21:23.984096 21902 net.cpp:454] conv3 <- pool2
I0520 11:21:23.984108 21902 net.cpp:411] conv3 -> conv3
I0520 11:21:23.986222 21902 net.cpp:150] Setting up conv3
I0520 11:21:23.986245 21902 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0520 11:21:23.986258 21902 net.cpp:165] Memory required for data: 13218200
I0520 11:21:23.986276 21902 layer_factory.hpp:77] Creating layer relu3
I0520 11:21:23.986292 21902 net.cpp:106] Creating Layer relu3
I0520 11:21:23.986302 21902 net.cpp:454] relu3 <- conv3
I0520 11:21:23.986315 21902 net.cpp:397] relu3 -> conv3 (in-place)
I0520 11:21:23.986785 21902 net.cpp:150] Setting up relu3
I0520 11:21:23.986804 21902 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0520 11:21:23.986814 21902 net.cpp:165] Memory required for data: 14302360
I0520 11:21:23.986824 21902 layer_factory.hpp:77] Creating layer pool3
I0520 11:21:23.986837 21902 net.cpp:106] Creating Layer pool3
I0520 11:21:23.986847 21902 net.cpp:454] pool3 <- conv3
I0520 11:21:23.986861 21902 net.cpp:411] pool3 -> pool3
I0520 11:21:23.986928 21902 net.cpp:150] Setting up pool3
I0520 11:21:23.986942 21902 net.cpp:157] Top shape: 10 28 11 44 (135520)
I0520 11:21:23.986950 21902 net.cpp:165] Memory required for data: 14844440
I0520 11:21:23.986958 21902 layer_factory.hpp:77] Creating layer conv4
I0520 11:21:23.986976 21902 net.cpp:106] Creating Layer conv4
I0520 11:21:23.986986 21902 net.cpp:454] conv4 <- pool3
I0520 11:21:23.986999 21902 net.cpp:411] conv4 -> conv4
I0520 11:21:23.989735 21902 net.cpp:150] Setting up conv4
I0520 11:21:23.989763 21902 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0520 11:21:23.989773 21902 net.cpp:165] Memory required for data: 15207320
I0520 11:21:23.989789 21902 layer_factory.hpp:77] Creating layer relu4
I0520 11:21:23.989804 21902 net.cpp:106] Creating Layer relu4
I0520 11:21:23.989814 21902 net.cpp:454] relu4 <- conv4
I0520 11:21:23.989826 21902 net.cpp:397] relu4 -> conv4 (in-place)
I0520 11:21:23.990293 21902 net.cpp:150] Setting up relu4
I0520 11:21:23.990309 21902 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0520 11:21:23.990319 21902 net.cpp:165] Memory required for data: 15570200
I0520 11:21:23.990329 21902 layer_factory.hpp:77] Creating layer pool4
I0520 11:21:23.990342 21902 net.cpp:106] Creating Layer pool4
I0520 11:21:23.990352 21902 net.cpp:454] pool4 <- conv4
I0520 11:21:23.990370 21902 net.cpp:411] pool4 -> pool4
I0520 11:21:23.990438 21902 net.cpp:150] Setting up pool4
I0520 11:21:23.990453 21902 net.cpp:157] Top shape: 10 36 3 42 (45360)
I0520 11:21:23.990461 21902 net.cpp:165] Memory required for data: 15751640
I0520 11:21:23.990471 21902 layer_factory.hpp:77] Creating layer ip1
I0520 11:21:23.990491 21902 net.cpp:106] Creating Layer ip1
I0520 11:21:23.990501 21902 net.cpp:454] ip1 <- pool4
I0520 11:21:23.990514 21902 net.cpp:411] ip1 -> ip1
I0520 11:21:24.005971 21902 net.cpp:150] Setting up ip1
I0520 11:21:24.006000 21902 net.cpp:157] Top shape: 10 196 (1960)
I0520 11:21:24.006011 21902 net.cpp:165] Memory required for data: 15759480
I0520 11:21:24.006033 21902 layer_factory.hpp:77] Creating layer relu5
I0520 11:21:24.006047 21902 net.cpp:106] Creating Layer relu5
I0520 11:21:24.006057 21902 net.cpp:454] relu5 <- ip1
I0520 11:21:24.006070 21902 net.cpp:397] relu5 -> ip1 (in-place)
I0520 11:21:24.006413 21902 net.cpp:150] Setting up relu5
I0520 11:21:24.006428 21902 net.cpp:157] Top shape: 10 196 (1960)
I0520 11:21:24.006438 21902 net.cpp:165] Memory required for data: 15767320
I0520 11:21:24.006448 21902 layer_factory.hpp:77] Creating layer drop1
I0520 11:21:24.006469 21902 net.cpp:106] Creating Layer drop1
I0520 11:21:24.006479 21902 net.cpp:454] drop1 <- ip1
I0520 11:21:24.006490 21902 net.cpp:397] drop1 -> ip1 (in-place)
I0520 11:21:24.006552 21902 net.cpp:150] Setting up drop1
I0520 11:21:24.006566 21902 net.cpp:157] Top shape: 10 196 (1960)
I0520 11:21:24.006575 21902 net.cpp:165] Memory required for data: 15775160
I0520 11:21:24.006585 21902 layer_factory.hpp:77] Creating layer ip2
I0520 11:21:24.006603 21902 net.cpp:106] Creating Layer ip2
I0520 11:21:24.006614 21902 net.cpp:454] ip2 <- ip1
I0520 11:21:24.006628 21902 net.cpp:411] ip2 -> ip2
I0520 11:21:24.007091 21902 net.cpp:150] Setting up ip2
I0520 11:21:24.007104 21902 net.cpp:157] Top shape: 10 98 (980)
I0520 11:21:24.007113 21902 net.cpp:165] Memory required for data: 15779080
I0520 11:21:24.007128 21902 layer_factory.hpp:77] Creating layer relu6
I0520 11:21:24.007141 21902 net.cpp:106] Creating Layer relu6
I0520 11:21:24.007150 21902 net.cpp:454] relu6 <- ip2
I0520 11:21:24.007163 21902 net.cpp:397] relu6 -> ip2 (in-place)
I0520 11:21:24.007688 21902 net.cpp:150] Setting up relu6
I0520 11:21:24.007702 21902 net.cpp:157] Top shape: 10 98 (980)
I0520 11:21:24.007712 21902 net.cpp:165] Memory required for data: 15783000
I0520 11:21:24.007722 21902 layer_factory.hpp:77] Creating layer drop2
I0520 11:21:24.007735 21902 net.cpp:106] Creating Layer drop2
I0520 11:21:24.007745 21902 net.cpp:454] drop2 <- ip2
I0520 11:21:24.007756 21902 net.cpp:397] drop2 -> ip2 (in-place)
I0520 11:21:24.007799 21902 net.cpp:150] Setting up drop2
I0520 11:21:24.007812 21902 net.cpp:157] Top shape: 10 98 (980)
I0520 11:21:24.007822 21902 net.cpp:165] Memory required for data: 15786920
I0520 11:21:24.007833 21902 layer_factory.hpp:77] Creating layer ip3
I0520 11:21:24.007845 21902 net.cpp:106] Creating Layer ip3
I0520 11:21:24.007854 21902 net.cpp:454] ip3 <- ip2
I0520 11:21:24.007868 21902 net.cpp:411] ip3 -> ip3
I0520 11:21:24.008074 21902 net.cpp:150] Setting up ip3
I0520 11:21:24.008087 21902 net.cpp:157] Top shape: 10 11 (110)
I0520 11:21:24.008097 21902 net.cpp:165] Memory required for data: 15787360
I0520 11:21:24.008111 21902 layer_factory.hpp:77] Creating layer drop3
I0520 11:21:24.008124 21902 net.cpp:106] Creating Layer drop3
I0520 11:21:24.008133 21902 net.cpp:454] drop3 <- ip3
I0520 11:21:24.008147 21902 net.cpp:397] drop3 -> ip3 (in-place)
I0520 11:21:24.008184 21902 net.cpp:150] Setting up drop3
I0520 11:21:24.008198 21902 net.cpp:157] Top shape: 10 11 (110)
I0520 11:21:24.008208 21902 net.cpp:165] Memory required for data: 15787800
I0520 11:21:24.008218 21902 layer_factory.hpp:77] Creating layer loss
I0520 11:21:24.008236 21902 net.cpp:106] Creating Layer loss
I0520 11:21:24.008246 21902 net.cpp:454] loss <- ip3
I0520 11:21:24.008257 21902 net.cpp:454] loss <- label
I0520 11:21:24.008270 21902 net.cpp:411] loss -> loss
I0520 11:21:24.008287 21902 layer_factory.hpp:77] Creating layer loss
I0520 11:21:24.008934 21902 net.cpp:150] Setting up loss
I0520 11:21:24.008955 21902 net.cpp:157] Top shape: (1)
I0520 11:21:24.008967 21902 net.cpp:160]     with loss weight 1
I0520 11:21:24.009009 21902 net.cpp:165] Memory required for data: 15787804
I0520 11:21:24.009021 21902 net.cpp:226] loss needs backward computation.
I0520 11:21:24.009032 21902 net.cpp:226] drop3 needs backward computation.
I0520 11:21:24.009040 21902 net.cpp:226] ip3 needs backward computation.
I0520 11:21:24.009052 21902 net.cpp:226] drop2 needs backward computation.
I0520 11:21:24.009059 21902 net.cpp:226] relu6 needs backward computation.
I0520 11:21:24.009069 21902 net.cpp:226] ip2 needs backward computation.
I0520 11:21:24.009079 21902 net.cpp:226] drop1 needs backward computation.
I0520 11:21:24.009088 21902 net.cpp:226] relu5 needs backward computation.
I0520 11:21:24.009097 21902 net.cpp:226] ip1 needs backward computation.
I0520 11:21:24.009107 21902 net.cpp:226] pool4 needs backward computation.
I0520 11:21:24.009117 21902 net.cpp:226] relu4 needs backward computation.
I0520 11:21:24.009127 21902 net.cpp:226] conv4 needs backward computation.
I0520 11:21:24.009137 21902 net.cpp:226] pool3 needs backward computation.
I0520 11:21:24.009148 21902 net.cpp:226] relu3 needs backward computation.
I0520 11:21:24.009166 21902 net.cpp:226] conv3 needs backward computation.
I0520 11:21:24.009177 21902 net.cpp:226] pool2 needs backward computation.
I0520 11:21:24.009188 21902 net.cpp:226] relu2 needs backward computation.
I0520 11:21:24.009198 21902 net.cpp:226] conv2 needs backward computation.
I0520 11:21:24.009209 21902 net.cpp:226] pool1 needs backward computation.
I0520 11:21:24.009220 21902 net.cpp:226] relu1 needs backward computation.
I0520 11:21:24.009228 21902 net.cpp:226] conv1 needs backward computation.
I0520 11:21:24.009240 21902 net.cpp:228] data_hdf5 does not need backward computation.
I0520 11:21:24.009250 21902 net.cpp:270] This network produces output loss
I0520 11:21:24.009274 21902 net.cpp:283] Network initialization done.
I0520 11:21:24.011064 21902 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720.prototxt
I0520 11:21:24.011135 21902 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0520 11:21:24.011487 21902 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 10
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0520 11:21:24.011677 21902 layer_factory.hpp:77] Creating layer data_hdf5
I0520 11:21:24.011693 21902 net.cpp:106] Creating Layer data_hdf5
I0520 11:21:24.011705 21902 net.cpp:411] data_hdf5 -> data
I0520 11:21:24.011723 21902 net.cpp:411] data_hdf5 -> label
I0520 11:21:24.011737 21902 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0520 11:21:24.035194 21902 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0520 11:21:45.379366 21902 net.cpp:150] Setting up data_hdf5
I0520 11:21:45.379531 21902 net.cpp:157] Top shape: 10 1 127 50 (63500)
I0520 11:21:45.379545 21902 net.cpp:157] Top shape: 10 (10)
I0520 11:21:45.379559 21902 net.cpp:165] Memory required for data: 254040
I0520 11:21:45.379572 21902 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0520 11:21:45.379601 21902 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0520 11:21:45.379611 21902 net.cpp:454] label_data_hdf5_1_split <- label
I0520 11:21:45.379626 21902 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0520 11:21:45.379647 21902 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0520 11:21:45.379720 21902 net.cpp:150] Setting up label_data_hdf5_1_split
I0520 11:21:45.379734 21902 net.cpp:157] Top shape: 10 (10)
I0520 11:21:45.379745 21902 net.cpp:157] Top shape: 10 (10)
I0520 11:21:45.379755 21902 net.cpp:165] Memory required for data: 254120
I0520 11:21:45.379766 21902 layer_factory.hpp:77] Creating layer conv1
I0520 11:21:45.379787 21902 net.cpp:106] Creating Layer conv1
I0520 11:21:45.379798 21902 net.cpp:454] conv1 <- data
I0520 11:21:45.379813 21902 net.cpp:411] conv1 -> conv1
I0520 11:21:45.381772 21902 net.cpp:150] Setting up conv1
I0520 11:21:45.381795 21902 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0520 11:21:45.381808 21902 net.cpp:165] Memory required for data: 3018920
I0520 11:21:45.381829 21902 layer_factory.hpp:77] Creating layer relu1
I0520 11:21:45.381842 21902 net.cpp:106] Creating Layer relu1
I0520 11:21:45.381852 21902 net.cpp:454] relu1 <- conv1
I0520 11:21:45.381865 21902 net.cpp:397] relu1 -> conv1 (in-place)
I0520 11:21:45.382374 21902 net.cpp:150] Setting up relu1
I0520 11:21:45.382390 21902 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0520 11:21:45.382400 21902 net.cpp:165] Memory required for data: 5783720
I0520 11:21:45.382411 21902 layer_factory.hpp:77] Creating layer pool1
I0520 11:21:45.382426 21902 net.cpp:106] Creating Layer pool1
I0520 11:21:45.382436 21902 net.cpp:454] pool1 <- conv1
I0520 11:21:45.382449 21902 net.cpp:411] pool1 -> pool1
I0520 11:21:45.382524 21902 net.cpp:150] Setting up pool1
I0520 11:21:45.382537 21902 net.cpp:157] Top shape: 10 12 60 48 (345600)
I0520 11:21:45.382547 21902 net.cpp:165] Memory required for data: 7166120
I0520 11:21:45.382560 21902 layer_factory.hpp:77] Creating layer conv2
I0520 11:21:45.382576 21902 net.cpp:106] Creating Layer conv2
I0520 11:21:45.382586 21902 net.cpp:454] conv2 <- pool1
I0520 11:21:45.382599 21902 net.cpp:411] conv2 -> conv2
I0520 11:21:45.384519 21902 net.cpp:150] Setting up conv2
I0520 11:21:45.384541 21902 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0520 11:21:45.384553 21902 net.cpp:165] Memory required for data: 9153320
I0520 11:21:45.384570 21902 layer_factory.hpp:77] Creating layer relu2
I0520 11:21:45.384584 21902 net.cpp:106] Creating Layer relu2
I0520 11:21:45.384594 21902 net.cpp:454] relu2 <- conv2
I0520 11:21:45.384606 21902 net.cpp:397] relu2 -> conv2 (in-place)
I0520 11:21:45.384950 21902 net.cpp:150] Setting up relu2
I0520 11:21:45.384964 21902 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0520 11:21:45.384974 21902 net.cpp:165] Memory required for data: 11140520
I0520 11:21:45.384985 21902 layer_factory.hpp:77] Creating layer pool2
I0520 11:21:45.384999 21902 net.cpp:106] Creating Layer pool2
I0520 11:21:45.385009 21902 net.cpp:454] pool2 <- conv2
I0520 11:21:45.385020 21902 net.cpp:411] pool2 -> pool2
I0520 11:21:45.385092 21902 net.cpp:150] Setting up pool2
I0520 11:21:45.385105 21902 net.cpp:157] Top shape: 10 20 27 46 (248400)
I0520 11:21:45.385115 21902 net.cpp:165] Memory required for data: 12134120
I0520 11:21:45.385125 21902 layer_factory.hpp:77] Creating layer conv3
I0520 11:21:45.385144 21902 net.cpp:106] Creating Layer conv3
I0520 11:21:45.385155 21902 net.cpp:454] conv3 <- pool2
I0520 11:21:45.385169 21902 net.cpp:411] conv3 -> conv3
I0520 11:21:45.387181 21902 net.cpp:150] Setting up conv3
I0520 11:21:45.387205 21902 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0520 11:21:45.387217 21902 net.cpp:165] Memory required for data: 13218280
I0520 11:21:45.387233 21902 layer_factory.hpp:77] Creating layer relu3
I0520 11:21:45.387259 21902 net.cpp:106] Creating Layer relu3
I0520 11:21:45.387270 21902 net.cpp:454] relu3 <- conv3
I0520 11:21:45.387284 21902 net.cpp:397] relu3 -> conv3 (in-place)
I0520 11:21:45.387758 21902 net.cpp:150] Setting up relu3
I0520 11:21:45.387774 21902 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0520 11:21:45.387784 21902 net.cpp:165] Memory required for data: 14302440
I0520 11:21:45.387794 21902 layer_factory.hpp:77] Creating layer pool3
I0520 11:21:45.387807 21902 net.cpp:106] Creating Layer pool3
I0520 11:21:45.387817 21902 net.cpp:454] pool3 <- conv3
I0520 11:21:45.387830 21902 net.cpp:411] pool3 -> pool3
I0520 11:21:45.387903 21902 net.cpp:150] Setting up pool3
I0520 11:21:45.387917 21902 net.cpp:157] Top shape: 10 28 11 44 (135520)
I0520 11:21:45.387926 21902 net.cpp:165] Memory required for data: 14844520
I0520 11:21:45.387934 21902 layer_factory.hpp:77] Creating layer conv4
I0520 11:21:45.387953 21902 net.cpp:106] Creating Layer conv4
I0520 11:21:45.387964 21902 net.cpp:454] conv4 <- pool3
I0520 11:21:45.387977 21902 net.cpp:411] conv4 -> conv4
I0520 11:21:45.390053 21902 net.cpp:150] Setting up conv4
I0520 11:21:45.390074 21902 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0520 11:21:45.390087 21902 net.cpp:165] Memory required for data: 15207400
I0520 11:21:45.390101 21902 layer_factory.hpp:77] Creating layer relu4
I0520 11:21:45.390115 21902 net.cpp:106] Creating Layer relu4
I0520 11:21:45.390125 21902 net.cpp:454] relu4 <- conv4
I0520 11:21:45.390137 21902 net.cpp:397] relu4 -> conv4 (in-place)
I0520 11:21:45.390604 21902 net.cpp:150] Setting up relu4
I0520 11:21:45.390620 21902 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0520 11:21:45.390631 21902 net.cpp:165] Memory required for data: 15570280
I0520 11:21:45.390641 21902 layer_factory.hpp:77] Creating layer pool4
I0520 11:21:45.390655 21902 net.cpp:106] Creating Layer pool4
I0520 11:21:45.390664 21902 net.cpp:454] pool4 <- conv4
I0520 11:21:45.390676 21902 net.cpp:411] pool4 -> pool4
I0520 11:21:45.390748 21902 net.cpp:150] Setting up pool4
I0520 11:21:45.390763 21902 net.cpp:157] Top shape: 10 36 3 42 (45360)
I0520 11:21:45.390771 21902 net.cpp:165] Memory required for data: 15751720
I0520 11:21:45.390780 21902 layer_factory.hpp:77] Creating layer ip1
I0520 11:21:45.390794 21902 net.cpp:106] Creating Layer ip1
I0520 11:21:45.390805 21902 net.cpp:454] ip1 <- pool4
I0520 11:21:45.390817 21902 net.cpp:411] ip1 -> ip1
I0520 11:21:45.406306 21902 net.cpp:150] Setting up ip1
I0520 11:21:45.406333 21902 net.cpp:157] Top shape: 10 196 (1960)
I0520 11:21:45.406350 21902 net.cpp:165] Memory required for data: 15759560
I0520 11:21:45.406373 21902 layer_factory.hpp:77] Creating layer relu5
I0520 11:21:45.406388 21902 net.cpp:106] Creating Layer relu5
I0520 11:21:45.406399 21902 net.cpp:454] relu5 <- ip1
I0520 11:21:45.406411 21902 net.cpp:397] relu5 -> ip1 (in-place)
I0520 11:21:45.406760 21902 net.cpp:150] Setting up relu5
I0520 11:21:45.406774 21902 net.cpp:157] Top shape: 10 196 (1960)
I0520 11:21:45.406785 21902 net.cpp:165] Memory required for data: 15767400
I0520 11:21:45.406795 21902 layer_factory.hpp:77] Creating layer drop1
I0520 11:21:45.406812 21902 net.cpp:106] Creating Layer drop1
I0520 11:21:45.406822 21902 net.cpp:454] drop1 <- ip1
I0520 11:21:45.406836 21902 net.cpp:397] drop1 -> ip1 (in-place)
I0520 11:21:45.406880 21902 net.cpp:150] Setting up drop1
I0520 11:21:45.406893 21902 net.cpp:157] Top shape: 10 196 (1960)
I0520 11:21:45.406901 21902 net.cpp:165] Memory required for data: 15775240
I0520 11:21:45.406911 21902 layer_factory.hpp:77] Creating layer ip2
I0520 11:21:45.406926 21902 net.cpp:106] Creating Layer ip2
I0520 11:21:45.406936 21902 net.cpp:454] ip2 <- ip1
I0520 11:21:45.406950 21902 net.cpp:411] ip2 -> ip2
I0520 11:21:45.407428 21902 net.cpp:150] Setting up ip2
I0520 11:21:45.407441 21902 net.cpp:157] Top shape: 10 98 (980)
I0520 11:21:45.407451 21902 net.cpp:165] Memory required for data: 15779160
I0520 11:21:45.407466 21902 layer_factory.hpp:77] Creating layer relu6
I0520 11:21:45.407492 21902 net.cpp:106] Creating Layer relu6
I0520 11:21:45.407502 21902 net.cpp:454] relu6 <- ip2
I0520 11:21:45.407515 21902 net.cpp:397] relu6 -> ip2 (in-place)
I0520 11:21:45.408052 21902 net.cpp:150] Setting up relu6
I0520 11:21:45.408074 21902 net.cpp:157] Top shape: 10 98 (980)
I0520 11:21:45.408083 21902 net.cpp:165] Memory required for data: 15783080
I0520 11:21:45.408093 21902 layer_factory.hpp:77] Creating layer drop2
I0520 11:21:45.408107 21902 net.cpp:106] Creating Layer drop2
I0520 11:21:45.408116 21902 net.cpp:454] drop2 <- ip2
I0520 11:21:45.408129 21902 net.cpp:397] drop2 -> ip2 (in-place)
I0520 11:21:45.408172 21902 net.cpp:150] Setting up drop2
I0520 11:21:45.408185 21902 net.cpp:157] Top shape: 10 98 (980)
I0520 11:21:45.408195 21902 net.cpp:165] Memory required for data: 15787000
I0520 11:21:45.408205 21902 layer_factory.hpp:77] Creating layer ip3
I0520 11:21:45.408220 21902 net.cpp:106] Creating Layer ip3
I0520 11:21:45.408229 21902 net.cpp:454] ip3 <- ip2
I0520 11:21:45.408242 21902 net.cpp:411] ip3 -> ip3
I0520 11:21:45.408462 21902 net.cpp:150] Setting up ip3
I0520 11:21:45.408474 21902 net.cpp:157] Top shape: 10 11 (110)
I0520 11:21:45.408484 21902 net.cpp:165] Memory required for data: 15787440
I0520 11:21:45.408499 21902 layer_factory.hpp:77] Creating layer drop3
I0520 11:21:45.408512 21902 net.cpp:106] Creating Layer drop3
I0520 11:21:45.408522 21902 net.cpp:454] drop3 <- ip3
I0520 11:21:45.408535 21902 net.cpp:397] drop3 -> ip3 (in-place)
I0520 11:21:45.408576 21902 net.cpp:150] Setting up drop3
I0520 11:21:45.408588 21902 net.cpp:157] Top shape: 10 11 (110)
I0520 11:21:45.408598 21902 net.cpp:165] Memory required for data: 15787880
I0520 11:21:45.408608 21902 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0520 11:21:45.408622 21902 net.cpp:106] Creating Layer ip3_drop3_0_split
I0520 11:21:45.408630 21902 net.cpp:454] ip3_drop3_0_split <- ip3
I0520 11:21:45.408643 21902 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0520 11:21:45.408658 21902 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0520 11:21:45.408731 21902 net.cpp:150] Setting up ip3_drop3_0_split
I0520 11:21:45.408745 21902 net.cpp:157] Top shape: 10 11 (110)
I0520 11:21:45.408756 21902 net.cpp:157] Top shape: 10 11 (110)
I0520 11:21:45.408766 21902 net.cpp:165] Memory required for data: 15788760
I0520 11:21:45.408773 21902 layer_factory.hpp:77] Creating layer accuracy
I0520 11:21:45.408795 21902 net.cpp:106] Creating Layer accuracy
I0520 11:21:45.408804 21902 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0520 11:21:45.408817 21902 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0520 11:21:45.408829 21902 net.cpp:411] accuracy -> accuracy
I0520 11:21:45.408852 21902 net.cpp:150] Setting up accuracy
I0520 11:21:45.408872 21902 net.cpp:157] Top shape: (1)
I0520 11:21:45.408882 21902 net.cpp:165] Memory required for data: 15788764
I0520 11:21:45.408892 21902 layer_factory.hpp:77] Creating layer loss
I0520 11:21:45.408906 21902 net.cpp:106] Creating Layer loss
I0520 11:21:45.408915 21902 net.cpp:454] loss <- ip3_drop3_0_split_1
I0520 11:21:45.408926 21902 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0520 11:21:45.408939 21902 net.cpp:411] loss -> loss
I0520 11:21:45.408957 21902 layer_factory.hpp:77] Creating layer loss
I0520 11:21:45.409443 21902 net.cpp:150] Setting up loss
I0520 11:21:45.409456 21902 net.cpp:157] Top shape: (1)
I0520 11:21:45.409466 21902 net.cpp:160]     with loss weight 1
I0520 11:21:45.409484 21902 net.cpp:165] Memory required for data: 15788768
I0520 11:21:45.409494 21902 net.cpp:226] loss needs backward computation.
I0520 11:21:45.409505 21902 net.cpp:228] accuracy does not need backward computation.
I0520 11:21:45.409517 21902 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0520 11:21:45.409526 21902 net.cpp:226] drop3 needs backward computation.
I0520 11:21:45.409535 21902 net.cpp:226] ip3 needs backward computation.
I0520 11:21:45.409545 21902 net.cpp:226] drop2 needs backward computation.
I0520 11:21:45.409555 21902 net.cpp:226] relu6 needs backward computation.
I0520 11:21:45.409575 21902 net.cpp:226] ip2 needs backward computation.
I0520 11:21:45.409585 21902 net.cpp:226] drop1 needs backward computation.
I0520 11:21:45.409595 21902 net.cpp:226] relu5 needs backward computation.
I0520 11:21:45.409605 21902 net.cpp:226] ip1 needs backward computation.
I0520 11:21:45.409615 21902 net.cpp:226] pool4 needs backward computation.
I0520 11:21:45.409624 21902 net.cpp:226] relu4 needs backward computation.
I0520 11:21:45.409636 21902 net.cpp:226] conv4 needs backward computation.
I0520 11:21:45.409648 21902 net.cpp:226] pool3 needs backward computation.
I0520 11:21:45.409659 21902 net.cpp:226] relu3 needs backward computation.
I0520 11:21:45.409670 21902 net.cpp:226] conv3 needs backward computation.
I0520 11:21:45.409680 21902 net.cpp:226] pool2 needs backward computation.
I0520 11:21:45.409690 21902 net.cpp:226] relu2 needs backward computation.
I0520 11:21:45.409700 21902 net.cpp:226] conv2 needs backward computation.
I0520 11:21:45.409710 21902 net.cpp:226] pool1 needs backward computation.
I0520 11:21:45.409720 21902 net.cpp:226] relu1 needs backward computation.
I0520 11:21:45.409730 21902 net.cpp:226] conv1 needs backward computation.
I0520 11:21:45.409741 21902 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0520 11:21:45.409754 21902 net.cpp:228] data_hdf5 does not need backward computation.
I0520 11:21:45.409764 21902 net.cpp:270] This network produces output accuracy
I0520 11:21:45.409775 21902 net.cpp:270] This network produces output loss
I0520 11:21:45.409803 21902 net.cpp:283] Network initialization done.
I0520 11:21:45.409936 21902 solver.cpp:60] Solver scaffolding done.
I0520 11:21:45.411075 21902 caffe.cpp:212] Starting Optimization
I0520 11:21:45.411088 21902 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0520 11:21:45.411100 21902 solver.cpp:289] Learning Rate Policy: fixed
I0520 11:21:45.412160 21902 solver.cpp:341] Iteration 0, Testing net (#0)
I0520 11:22:45.842398 21902 solver.cpp:409]     Test net output #0: accuracy = 0.0928671
I0520 11:22:45.842561 21902 solver.cpp:409]     Test net output #1: loss = 2.39809 (* 1 = 2.39809 loss)
I0520 11:22:45.860275 21902 solver.cpp:237] Iteration 0, loss = 2.39803
I0520 11:22:45.860311 21902 solver.cpp:253]     Train net output #0: loss = 2.39803 (* 1 = 2.39803 loss)
I0520 11:22:45.860329 21902 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0520 11:23:02.623579 21902 solver.cpp:237] Iteration 1500, loss = 2.26561
I0520 11:23:02.623627 21902 solver.cpp:253]     Train net output #0: loss = 2.26561 (* 1 = 2.26561 loss)
I0520 11:23:02.623642 21902 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0520 11:23:19.409898 21902 solver.cpp:237] Iteration 3000, loss = 1.83042
I0520 11:23:19.410058 21902 solver.cpp:253]     Train net output #0: loss = 1.83042 (* 1 = 1.83042 loss)
I0520 11:23:19.410073 21902 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0520 11:23:36.158483 21902 solver.cpp:237] Iteration 4500, loss = 2.0153
I0520 11:23:36.158519 21902 solver.cpp:253]     Train net output #0: loss = 2.0153 (* 1 = 2.0153 loss)
I0520 11:23:36.158535 21902 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0520 11:23:52.888967 21902 solver.cpp:237] Iteration 6000, loss = 1.51744
I0520 11:23:52.889112 21902 solver.cpp:253]     Train net output #0: loss = 1.51743 (* 1 = 1.51743 loss)
I0520 11:23:52.889127 21902 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0520 11:24:09.639076 21902 solver.cpp:237] Iteration 7500, loss = 1.45828
I0520 11:24:09.639124 21902 solver.cpp:253]     Train net output #0: loss = 1.45828 (* 1 = 1.45828 loss)
I0520 11:24:09.639142 21902 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0520 11:24:26.380903 21902 solver.cpp:237] Iteration 9000, loss = 1.65192
I0520 11:24:26.381024 21902 solver.cpp:253]     Train net output #0: loss = 1.65192 (* 1 = 1.65192 loss)
I0520 11:24:26.381038 21902 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0520 11:25:05.240357 21902 solver.cpp:237] Iteration 10500, loss = 1.60812
I0520 11:25:05.240519 21902 solver.cpp:253]     Train net output #0: loss = 1.60812 (* 1 = 1.60812 loss)
I0520 11:25:05.240533 21902 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0520 11:25:21.999833 21902 solver.cpp:237] Iteration 12000, loss = 1.34683
I0520 11:25:21.999881 21902 solver.cpp:253]     Train net output #0: loss = 1.34683 (* 1 = 1.34683 loss)
I0520 11:25:21.999897 21902 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0520 11:25:38.756055 21902 solver.cpp:237] Iteration 13500, loss = 1.11509
I0520 11:25:38.756192 21902 solver.cpp:253]     Train net output #0: loss = 1.11509 (* 1 = 1.11509 loss)
I0520 11:25:38.756206 21902 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0520 11:25:55.539757 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_15000.caffemodel
I0520 11:25:55.588538 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_15000.solverstate
I0520 11:25:55.618216 21902 solver.cpp:237] Iteration 15000, loss = 0.607928
I0520 11:25:55.618262 21902 solver.cpp:253]     Train net output #0: loss = 0.607928 (* 1 = 0.607928 loss)
I0520 11:25:55.618275 21902 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0520 11:26:12.378621 21902 solver.cpp:237] Iteration 16500, loss = 1.47361
I0520 11:26:12.378774 21902 solver.cpp:253]     Train net output #0: loss = 1.47361 (* 1 = 1.47361 loss)
I0520 11:26:12.378788 21902 sgd_solver.cpp:106] Iteration 16500, lr = 0.0025
I0520 11:26:29.133081 21902 solver.cpp:237] Iteration 18000, loss = 1.00998
I0520 11:26:29.133117 21902 solver.cpp:253]     Train net output #0: loss = 1.00998 (* 1 = 1.00998 loss)
I0520 11:26:29.133131 21902 sgd_solver.cpp:106] Iteration 18000, lr = 0.0025
I0520 11:26:45.882835 21902 solver.cpp:237] Iteration 19500, loss = 1.41152
I0520 11:26:45.882983 21902 solver.cpp:253]     Train net output #0: loss = 1.41152 (* 1 = 1.41152 loss)
I0520 11:26:45.882997 21902 sgd_solver.cpp:106] Iteration 19500, lr = 0.0025
I0520 11:27:24.735213 21902 solver.cpp:237] Iteration 21000, loss = 1.11419
I0520 11:27:24.735371 21902 solver.cpp:253]     Train net output #0: loss = 1.11419 (* 1 = 1.11419 loss)
I0520 11:27:24.735386 21902 sgd_solver.cpp:106] Iteration 21000, lr = 0.0025
I0520 11:27:41.514233 21902 solver.cpp:237] Iteration 22500, loss = 0.67127
I0520 11:27:41.514271 21902 solver.cpp:253]     Train net output #0: loss = 0.67127 (* 1 = 0.67127 loss)
I0520 11:27:41.514286 21902 sgd_solver.cpp:106] Iteration 22500, lr = 0.0025
I0520 11:27:58.314821 21902 solver.cpp:237] Iteration 24000, loss = 0.762134
I0520 11:27:58.314975 21902 solver.cpp:253]     Train net output #0: loss = 0.762133 (* 1 = 0.762133 loss)
I0520 11:27:58.314987 21902 sgd_solver.cpp:106] Iteration 24000, lr = 0.0025
I0520 11:28:15.073607 21902 solver.cpp:237] Iteration 25500, loss = 0.991749
I0520 11:28:15.073645 21902 solver.cpp:253]     Train net output #0: loss = 0.991748 (* 1 = 0.991748 loss)
I0520 11:28:15.073667 21902 sgd_solver.cpp:106] Iteration 25500, lr = 0.0025
I0520 11:28:31.823792 21902 solver.cpp:237] Iteration 27000, loss = 1.49393
I0520 11:28:31.823927 21902 solver.cpp:253]     Train net output #0: loss = 1.49393 (* 1 = 1.49393 loss)
I0520 11:28:31.823941 21902 sgd_solver.cpp:106] Iteration 27000, lr = 0.0025
I0520 11:28:48.593083 21902 solver.cpp:237] Iteration 28500, loss = 1.43161
I0520 11:28:48.593124 21902 solver.cpp:253]     Train net output #0: loss = 1.43161 (* 1 = 1.43161 loss)
I0520 11:28:48.593142 21902 sgd_solver.cpp:106] Iteration 28500, lr = 0.0025
I0520 11:29:05.358829 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_30000.caffemodel
I0520 11:29:05.406167 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_30000.solverstate
I0520 11:29:05.431429 21902 solver.cpp:341] Iteration 30000, Testing net (#0)
I0520 11:30:04.887397 21902 solver.cpp:409]     Test net output #0: accuracy = 0.857089
I0520 11:30:04.887554 21902 solver.cpp:409]     Test net output #1: loss = 0.563463 (* 1 = 0.563463 loss)
I0520 11:30:27.022217 21902 solver.cpp:237] Iteration 30000, loss = 1.19637
I0520 11:30:27.022269 21902 solver.cpp:253]     Train net output #0: loss = 1.19637 (* 1 = 1.19637 loss)
I0520 11:30:27.022286 21902 sgd_solver.cpp:106] Iteration 30000, lr = 0.0025
I0520 11:30:43.618079 21902 solver.cpp:237] Iteration 31500, loss = 1.40396
I0520 11:30:43.618227 21902 solver.cpp:253]     Train net output #0: loss = 1.40396 (* 1 = 1.40396 loss)
I0520 11:30:43.618240 21902 sgd_solver.cpp:106] Iteration 31500, lr = 0.0025
I0520 11:31:00.234545 21902 solver.cpp:237] Iteration 33000, loss = 1.75851
I0520 11:31:00.234586 21902 solver.cpp:253]     Train net output #0: loss = 1.75851 (* 1 = 1.75851 loss)
I0520 11:31:00.234602 21902 sgd_solver.cpp:106] Iteration 33000, lr = 0.0025
I0520 11:31:16.918987 21902 solver.cpp:237] Iteration 34500, loss = 1.56957
I0520 11:31:16.919134 21902 solver.cpp:253]     Train net output #0: loss = 1.56957 (* 1 = 1.56957 loss)
I0520 11:31:16.919148 21902 sgd_solver.cpp:106] Iteration 34500, lr = 0.0025
I0520 11:31:33.988951 21902 solver.cpp:237] Iteration 36000, loss = 0.60174
I0520 11:31:33.988988 21902 solver.cpp:253]     Train net output #0: loss = 0.601738 (* 1 = 0.601738 loss)
I0520 11:31:33.989003 21902 sgd_solver.cpp:106] Iteration 36000, lr = 0.0025
I0520 11:31:51.014709 21902 solver.cpp:237] Iteration 37500, loss = 1.73991
I0520 11:31:51.014854 21902 solver.cpp:253]     Train net output #0: loss = 1.7399 (* 1 = 1.7399 loss)
I0520 11:31:51.014869 21902 sgd_solver.cpp:106] Iteration 37500, lr = 0.0025
I0520 11:32:07.901536 21902 solver.cpp:237] Iteration 39000, loss = 0.905643
I0520 11:32:07.901582 21902 solver.cpp:253]     Train net output #0: loss = 0.90564 (* 1 = 0.90564 loss)
I0520 11:32:07.901598 21902 sgd_solver.cpp:106] Iteration 39000, lr = 0.0025
I0520 11:32:46.773998 21902 solver.cpp:237] Iteration 40500, loss = 1.07824
I0520 11:32:46.774158 21902 solver.cpp:253]     Train net output #0: loss = 1.07823 (* 1 = 1.07823 loss)
I0520 11:32:46.774173 21902 sgd_solver.cpp:106] Iteration 40500, lr = 0.0025
I0520 11:33:03.542217 21902 solver.cpp:237] Iteration 42000, loss = 1.45271
I0520 11:33:03.542265 21902 solver.cpp:253]     Train net output #0: loss = 1.45271 (* 1 = 1.45271 loss)
I0520 11:33:03.542282 21902 sgd_solver.cpp:106] Iteration 42000, lr = 0.0025
I0520 11:33:20.316392 21902 solver.cpp:237] Iteration 43500, loss = 0.795243
I0520 11:33:20.316565 21902 solver.cpp:253]     Train net output #0: loss = 0.795241 (* 1 = 0.795241 loss)
I0520 11:33:20.316581 21902 sgd_solver.cpp:106] Iteration 43500, lr = 0.0025
I0520 11:33:37.082579 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_45000.caffemodel
I0520 11:33:37.130246 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_45000.solverstate
I0520 11:33:37.160888 21902 solver.cpp:237] Iteration 45000, loss = 1.4635
I0520 11:33:37.160939 21902 solver.cpp:253]     Train net output #0: loss = 1.4635 (* 1 = 1.4635 loss)
I0520 11:33:37.160954 21902 sgd_solver.cpp:106] Iteration 45000, lr = 0.0025
I0520 11:33:53.965999 21902 solver.cpp:237] Iteration 46500, loss = 1.28438
I0520 11:33:53.966150 21902 solver.cpp:253]     Train net output #0: loss = 1.28437 (* 1 = 1.28437 loss)
I0520 11:33:53.966164 21902 sgd_solver.cpp:106] Iteration 46500, lr = 0.0025
I0520 11:34:10.713349 21902 solver.cpp:237] Iteration 48000, loss = 1.1074
I0520 11:34:10.713389 21902 solver.cpp:253]     Train net output #0: loss = 1.1074 (* 1 = 1.1074 loss)
I0520 11:34:10.713412 21902 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0520 11:34:27.473073 21902 solver.cpp:237] Iteration 49500, loss = 1.66429
I0520 11:34:27.473212 21902 solver.cpp:253]     Train net output #0: loss = 1.66429 (* 1 = 1.66429 loss)
I0520 11:34:27.473227 21902 sgd_solver.cpp:106] Iteration 49500, lr = 0.0025
I0520 11:35:06.370440 21902 solver.cpp:237] Iteration 51000, loss = 1.02023
I0520 11:35:06.370604 21902 solver.cpp:253]     Train net output #0: loss = 1.02023 (* 1 = 1.02023 loss)
I0520 11:35:06.370616 21902 sgd_solver.cpp:106] Iteration 51000, lr = 0.0025
I0520 11:35:23.134692 21902 solver.cpp:237] Iteration 52500, loss = 0.635653
I0520 11:35:23.134732 21902 solver.cpp:253]     Train net output #0: loss = 0.635652 (* 1 = 0.635652 loss)
I0520 11:35:23.134749 21902 sgd_solver.cpp:106] Iteration 52500, lr = 0.0025
I0520 11:35:39.909350 21902 solver.cpp:237] Iteration 54000, loss = 1.09515
I0520 11:35:39.909483 21902 solver.cpp:253]     Train net output #0: loss = 1.09515 (* 1 = 1.09515 loss)
I0520 11:35:39.909497 21902 sgd_solver.cpp:106] Iteration 54000, lr = 0.0025
I0520 11:35:56.672827 21902 solver.cpp:237] Iteration 55500, loss = 1.17506
I0520 11:35:56.672873 21902 solver.cpp:253]     Train net output #0: loss = 1.17506 (* 1 = 1.17506 loss)
I0520 11:35:56.672888 21902 sgd_solver.cpp:106] Iteration 55500, lr = 0.0025
I0520 11:36:13.452313 21902 solver.cpp:237] Iteration 57000, loss = 2.04593
I0520 11:36:13.452458 21902 solver.cpp:253]     Train net output #0: loss = 2.04593 (* 1 = 2.04593 loss)
I0520 11:36:13.452472 21902 sgd_solver.cpp:106] Iteration 57000, lr = 0.0025
I0520 11:36:30.231410 21902 solver.cpp:237] Iteration 58500, loss = 1.23376
I0520 11:36:30.231444 21902 solver.cpp:253]     Train net output #0: loss = 1.23376 (* 1 = 1.23376 loss)
I0520 11:36:30.231462 21902 sgd_solver.cpp:106] Iteration 58500, lr = 0.0025
I0520 11:36:47.001404 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_60000.caffemodel
I0520 11:36:47.049269 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_60000.solverstate
I0520 11:36:47.076458 21902 solver.cpp:341] Iteration 60000, Testing net (#0)
I0520 11:38:07.092330 21902 solver.cpp:409]     Test net output #0: accuracy = 0.860363
I0520 11:38:07.092500 21902 solver.cpp:409]     Test net output #1: loss = 0.506008 (* 1 = 0.506008 loss)
I0520 11:38:29.218907 21902 solver.cpp:237] Iteration 60000, loss = 1.31684
I0520 11:38:29.218961 21902 solver.cpp:253]     Train net output #0: loss = 1.31684 (* 1 = 1.31684 loss)
I0520 11:38:29.218976 21902 sgd_solver.cpp:106] Iteration 60000, lr = 0.0025
I0520 11:38:46.177314 21902 solver.cpp:237] Iteration 61500, loss = 1.05116
I0520 11:38:46.177474 21902 solver.cpp:253]     Train net output #0: loss = 1.05116 (* 1 = 1.05116 loss)
I0520 11:38:46.177489 21902 sgd_solver.cpp:106] Iteration 61500, lr = 0.0025
I0520 11:39:03.105957 21902 solver.cpp:237] Iteration 63000, loss = 1.64097
I0520 11:39:03.105993 21902 solver.cpp:253]     Train net output #0: loss = 1.64097 (* 1 = 1.64097 loss)
I0520 11:39:03.106008 21902 sgd_solver.cpp:106] Iteration 63000, lr = 0.0025
I0520 11:39:19.925184 21902 solver.cpp:237] Iteration 64500, loss = 1.12775
I0520 11:39:19.925334 21902 solver.cpp:253]     Train net output #0: loss = 1.12775 (* 1 = 1.12775 loss)
I0520 11:39:19.925349 21902 sgd_solver.cpp:106] Iteration 64500, lr = 0.0025
I0520 11:39:36.814996 21902 solver.cpp:237] Iteration 66000, loss = 1.03297
I0520 11:39:36.815042 21902 solver.cpp:253]     Train net output #0: loss = 1.03297 (* 1 = 1.03297 loss)
I0520 11:39:36.815057 21902 sgd_solver.cpp:106] Iteration 66000, lr = 0.0025
I0520 11:39:53.673998 21902 solver.cpp:237] Iteration 67500, loss = 1.15321
I0520 11:39:53.674147 21902 solver.cpp:253]     Train net output #0: loss = 1.15321 (* 1 = 1.15321 loss)
I0520 11:39:53.674161 21902 sgd_solver.cpp:106] Iteration 67500, lr = 0.0025
I0520 11:40:10.514436 21902 solver.cpp:237] Iteration 69000, loss = 0.856192
I0520 11:40:10.514480 21902 solver.cpp:253]     Train net output #0: loss = 0.85619 (* 1 = 0.85619 loss)
I0520 11:40:10.514495 21902 sgd_solver.cpp:106] Iteration 69000, lr = 0.0025
I0520 11:40:49.497623 21902 solver.cpp:237] Iteration 70500, loss = 1.68045
I0520 11:40:49.497786 21902 solver.cpp:253]     Train net output #0: loss = 1.68045 (* 1 = 1.68045 loss)
I0520 11:40:49.497799 21902 sgd_solver.cpp:106] Iteration 70500, lr = 0.0025
I0520 11:41:06.357866 21902 solver.cpp:237] Iteration 72000, loss = 1.77318
I0520 11:41:06.357902 21902 solver.cpp:253]     Train net output #0: loss = 1.77317 (* 1 = 1.77317 loss)
I0520 11:41:06.357918 21902 sgd_solver.cpp:106] Iteration 72000, lr = 0.0025
I0520 11:41:23.224115 21902 solver.cpp:237] Iteration 73500, loss = 0.892607
I0520 11:41:23.224266 21902 solver.cpp:253]     Train net output #0: loss = 0.892603 (* 1 = 0.892603 loss)
I0520 11:41:23.224279 21902 sgd_solver.cpp:106] Iteration 73500, lr = 0.0025
I0520 11:41:40.060150 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_75000.caffemodel
I0520 11:41:40.112020 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_75000.solverstate
I0520 11:41:40.142997 21902 solver.cpp:237] Iteration 75000, loss = 1.44841
I0520 11:41:40.143046 21902 solver.cpp:253]     Train net output #0: loss = 1.44841 (* 1 = 1.44841 loss)
I0520 11:41:40.143064 21902 sgd_solver.cpp:106] Iteration 75000, lr = 0.0025
I0520 11:41:57.029664 21902 solver.cpp:237] Iteration 76500, loss = 1.62984
I0520 11:41:57.029813 21902 solver.cpp:253]     Train net output #0: loss = 1.62984 (* 1 = 1.62984 loss)
I0520 11:41:57.029826 21902 sgd_solver.cpp:106] Iteration 76500, lr = 0.0025
I0520 11:42:13.854964 21902 solver.cpp:237] Iteration 78000, loss = 1.58689
I0520 11:42:13.855013 21902 solver.cpp:253]     Train net output #0: loss = 1.58688 (* 1 = 1.58688 loss)
I0520 11:42:13.855028 21902 sgd_solver.cpp:106] Iteration 78000, lr = 0.0025
I0520 11:42:30.651131 21902 solver.cpp:237] Iteration 79500, loss = 0.552326
I0520 11:42:30.651299 21902 solver.cpp:253]     Train net output #0: loss = 0.552322 (* 1 = 0.552322 loss)
I0520 11:42:30.651314 21902 sgd_solver.cpp:106] Iteration 79500, lr = 0.0025
I0520 11:43:09.668092 21902 solver.cpp:237] Iteration 81000, loss = 1.03666
I0520 11:43:09.668262 21902 solver.cpp:253]     Train net output #0: loss = 1.03666 (* 1 = 1.03666 loss)
I0520 11:43:09.668275 21902 sgd_solver.cpp:106] Iteration 81000, lr = 0.0025
I0520 11:43:26.579673 21902 solver.cpp:237] Iteration 82500, loss = 0.792155
I0520 11:43:26.579722 21902 solver.cpp:253]     Train net output #0: loss = 0.79215 (* 1 = 0.79215 loss)
I0520 11:43:26.579736 21902 sgd_solver.cpp:106] Iteration 82500, lr = 0.0025
I0520 11:43:43.440901 21902 solver.cpp:237] Iteration 84000, loss = 1.52213
I0520 11:43:43.441057 21902 solver.cpp:253]     Train net output #0: loss = 1.52212 (* 1 = 1.52212 loss)
I0520 11:43:43.441071 21902 sgd_solver.cpp:106] Iteration 84000, lr = 0.0025
I0520 11:44:00.357703 21902 solver.cpp:237] Iteration 85500, loss = 1.70847
I0520 11:44:00.357738 21902 solver.cpp:253]     Train net output #0: loss = 1.70847 (* 1 = 1.70847 loss)
I0520 11:44:00.357755 21902 sgd_solver.cpp:106] Iteration 85500, lr = 0.0025
I0520 11:44:17.238242 21902 solver.cpp:237] Iteration 87000, loss = 1.19768
I0520 11:44:17.238399 21902 solver.cpp:253]     Train net output #0: loss = 1.19767 (* 1 = 1.19767 loss)
I0520 11:44:17.238415 21902 sgd_solver.cpp:106] Iteration 87000, lr = 0.0025
I0520 11:44:34.129788 21902 solver.cpp:237] Iteration 88500, loss = 1.20008
I0520 11:44:34.129832 21902 solver.cpp:253]     Train net output #0: loss = 1.20007 (* 1 = 1.20007 loss)
I0520 11:44:34.129848 21902 sgd_solver.cpp:106] Iteration 88500, lr = 0.0025
I0520 11:44:50.973997 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_90000.caffemodel
I0520 11:44:51.020030 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_90000.solverstate
I0520 11:44:51.045184 21902 solver.cpp:341] Iteration 90000, Testing net (#0)
I0520 11:45:50.036314 21902 solver.cpp:409]     Test net output #0: accuracy = 0.875879
I0520 11:45:50.036473 21902 solver.cpp:409]     Test net output #1: loss = 0.448309 (* 1 = 0.448309 loss)
I0520 11:46:12.200752 21902 solver.cpp:237] Iteration 90000, loss = 1.06841
I0520 11:46:12.200805 21902 solver.cpp:253]     Train net output #0: loss = 1.06841 (* 1 = 1.06841 loss)
I0520 11:46:12.200820 21902 sgd_solver.cpp:106] Iteration 90000, lr = 0.0025
I0520 11:46:28.968529 21902 solver.cpp:237] Iteration 91500, loss = 1.52639
I0520 11:46:28.968680 21902 solver.cpp:253]     Train net output #0: loss = 1.52638 (* 1 = 1.52638 loss)
I0520 11:46:28.968694 21902 sgd_solver.cpp:106] Iteration 91500, lr = 0.0025
I0520 11:46:45.730458 21902 solver.cpp:237] Iteration 93000, loss = 1.90349
I0520 11:46:45.730504 21902 solver.cpp:253]     Train net output #0: loss = 1.90349 (* 1 = 1.90349 loss)
I0520 11:46:45.730516 21902 sgd_solver.cpp:106] Iteration 93000, lr = 0.0025
I0520 11:47:02.497102 21902 solver.cpp:237] Iteration 94500, loss = 0.871191
I0520 11:47:02.497257 21902 solver.cpp:253]     Train net output #0: loss = 0.871186 (* 1 = 0.871186 loss)
I0520 11:47:02.497272 21902 sgd_solver.cpp:106] Iteration 94500, lr = 0.0025
I0520 11:47:19.268419 21902 solver.cpp:237] Iteration 96000, loss = 1.33892
I0520 11:47:19.268455 21902 solver.cpp:253]     Train net output #0: loss = 1.33892 (* 1 = 1.33892 loss)
I0520 11:47:19.268468 21902 sgd_solver.cpp:106] Iteration 96000, lr = 0.0025
I0520 11:47:36.026391 21902 solver.cpp:237] Iteration 97500, loss = 1.19099
I0520 11:47:36.026545 21902 solver.cpp:253]     Train net output #0: loss = 1.19098 (* 1 = 1.19098 loss)
I0520 11:47:36.026561 21902 sgd_solver.cpp:106] Iteration 97500, lr = 0.0025
I0520 11:47:52.801991 21902 solver.cpp:237] Iteration 99000, loss = 1.30785
I0520 11:47:52.802039 21902 solver.cpp:253]     Train net output #0: loss = 1.30784 (* 1 = 1.30784 loss)
I0520 11:47:52.802055 21902 sgd_solver.cpp:106] Iteration 99000, lr = 0.0025
I0520 11:48:31.656390 21902 solver.cpp:237] Iteration 100500, loss = 1.32429
I0520 11:48:31.656566 21902 solver.cpp:253]     Train net output #0: loss = 1.32429 (* 1 = 1.32429 loss)
I0520 11:48:31.656580 21902 sgd_solver.cpp:106] Iteration 100500, lr = 0.0025
I0520 11:48:48.437310 21902 solver.cpp:237] Iteration 102000, loss = 1.24085
I0520 11:48:48.437356 21902 solver.cpp:253]     Train net output #0: loss = 1.24084 (* 1 = 1.24084 loss)
I0520 11:48:48.437372 21902 sgd_solver.cpp:106] Iteration 102000, lr = 0.0025
I0520 11:49:05.237172 21902 solver.cpp:237] Iteration 103500, loss = 0.648936
I0520 11:49:05.237339 21902 solver.cpp:253]     Train net output #0: loss = 0.648932 (* 1 = 0.648932 loss)
I0520 11:49:05.237352 21902 sgd_solver.cpp:106] Iteration 103500, lr = 0.0025
I0520 11:49:21.987105 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_105000.caffemodel
I0520 11:49:22.033104 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_105000.solverstate
I0520 11:49:22.062000 21902 solver.cpp:237] Iteration 105000, loss = 1.39414
I0520 11:49:22.062041 21902 solver.cpp:253]     Train net output #0: loss = 1.39414 (* 1 = 1.39414 loss)
I0520 11:49:22.062063 21902 sgd_solver.cpp:106] Iteration 105000, lr = 0.0025
I0520 11:49:38.827045 21902 solver.cpp:237] Iteration 106500, loss = 1.22834
I0520 11:49:38.827203 21902 solver.cpp:253]     Train net output #0: loss = 1.22834 (* 1 = 1.22834 loss)
I0520 11:49:38.827219 21902 sgd_solver.cpp:106] Iteration 106500, lr = 0.0025
I0520 11:49:55.593971 21902 solver.cpp:237] Iteration 108000, loss = 1.01508
I0520 11:49:55.594015 21902 solver.cpp:253]     Train net output #0: loss = 1.01508 (* 1 = 1.01508 loss)
I0520 11:49:55.594035 21902 sgd_solver.cpp:106] Iteration 108000, lr = 0.0025
I0520 11:50:12.351829 21902 solver.cpp:237] Iteration 109500, loss = 1.71736
I0520 11:50:12.351972 21902 solver.cpp:253]     Train net output #0: loss = 1.71736 (* 1 = 1.71736 loss)
I0520 11:50:12.351985 21902 sgd_solver.cpp:106] Iteration 109500, lr = 0.0025
I0520 11:50:51.454001 21902 solver.cpp:237] Iteration 111000, loss = 1.35166
I0520 11:50:51.454166 21902 solver.cpp:253]     Train net output #0: loss = 1.35166 (* 1 = 1.35166 loss)
I0520 11:50:51.454180 21902 sgd_solver.cpp:106] Iteration 111000, lr = 0.0025
I0520 11:51:08.637565 21902 solver.cpp:237] Iteration 112500, loss = 1.76584
I0520 11:51:08.637603 21902 solver.cpp:253]     Train net output #0: loss = 1.76583 (* 1 = 1.76583 loss)
I0520 11:51:08.637619 21902 sgd_solver.cpp:106] Iteration 112500, lr = 0.0025
I0520 11:51:25.787658 21902 solver.cpp:237] Iteration 114000, loss = 1.05654
I0520 11:51:25.787818 21902 solver.cpp:253]     Train net output #0: loss = 1.05654 (* 1 = 1.05654 loss)
I0520 11:51:25.787832 21902 sgd_solver.cpp:106] Iteration 114000, lr = 0.0025
I0520 11:51:42.941401 21902 solver.cpp:237] Iteration 115500, loss = 1.37406
I0520 11:51:42.941447 21902 solver.cpp:253]     Train net output #0: loss = 1.37406 (* 1 = 1.37406 loss)
I0520 11:51:42.941462 21902 sgd_solver.cpp:106] Iteration 115500, lr = 0.0025
I0520 11:52:00.099203 21902 solver.cpp:237] Iteration 117000, loss = 0.869114
I0520 11:52:00.099354 21902 solver.cpp:253]     Train net output #0: loss = 0.86911 (* 1 = 0.86911 loss)
I0520 11:52:00.099367 21902 sgd_solver.cpp:106] Iteration 117000, lr = 0.0025
I0520 11:52:17.270993 21902 solver.cpp:237] Iteration 118500, loss = 1.2662
I0520 11:52:17.271028 21902 solver.cpp:253]     Train net output #0: loss = 1.2662 (* 1 = 1.2662 loss)
I0520 11:52:17.271046 21902 sgd_solver.cpp:106] Iteration 118500, lr = 0.0025
I0520 11:52:34.450080 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_120000.caffemodel
I0520 11:52:34.495394 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_120000.solverstate
I0520 11:52:34.520370 21902 solver.cpp:341] Iteration 120000, Testing net (#0)
I0520 11:53:54.698966 21902 solver.cpp:409]     Test net output #0: accuracy = 0.880054
I0520 11:53:54.699129 21902 solver.cpp:409]     Test net output #1: loss = 0.380358 (* 1 = 0.380358 loss)
I0520 11:54:16.879441 21902 solver.cpp:237] Iteration 120000, loss = 1.26581
I0520 11:54:16.879494 21902 solver.cpp:253]     Train net output #0: loss = 1.2658 (* 1 = 1.2658 loss)
I0520 11:54:16.879508 21902 sgd_solver.cpp:106] Iteration 120000, lr = 0.0025
I0520 11:54:33.825850 21902 solver.cpp:237] Iteration 121500, loss = 0.846503
I0520 11:54:33.826019 21902 solver.cpp:253]     Train net output #0: loss = 0.846499 (* 1 = 0.846499 loss)
I0520 11:54:33.826032 21902 sgd_solver.cpp:106] Iteration 121500, lr = 0.0025
I0520 11:54:50.782629 21902 solver.cpp:237] Iteration 123000, loss = 1.20276
I0520 11:54:50.782665 21902 solver.cpp:253]     Train net output #0: loss = 1.20275 (* 1 = 1.20275 loss)
I0520 11:54:50.782678 21902 sgd_solver.cpp:106] Iteration 123000, lr = 0.0025
I0520 11:55:07.754426 21902 solver.cpp:237] Iteration 124500, loss = 0.9104
I0520 11:55:07.754586 21902 solver.cpp:253]     Train net output #0: loss = 0.910396 (* 1 = 0.910396 loss)
I0520 11:55:07.754601 21902 sgd_solver.cpp:106] Iteration 124500, lr = 0.0025
I0520 11:55:24.670649 21902 solver.cpp:237] Iteration 126000, loss = 0.954495
I0520 11:55:24.670694 21902 solver.cpp:253]     Train net output #0: loss = 0.95449 (* 1 = 0.95449 loss)
I0520 11:55:24.670714 21902 sgd_solver.cpp:106] Iteration 126000, lr = 0.0025
I0520 11:55:41.603934 21902 solver.cpp:237] Iteration 127500, loss = 1.14744
I0520 11:55:41.604080 21902 solver.cpp:253]     Train net output #0: loss = 1.14744 (* 1 = 1.14744 loss)
I0520 11:55:41.604094 21902 sgd_solver.cpp:106] Iteration 127500, lr = 0.0025
I0520 11:55:58.547222 21902 solver.cpp:237] Iteration 129000, loss = 0.82145
I0520 11:55:58.547272 21902 solver.cpp:253]     Train net output #0: loss = 0.821445 (* 1 = 0.821445 loss)
I0520 11:55:58.547286 21902 sgd_solver.cpp:106] Iteration 129000, lr = 0.0025
I0520 11:56:37.703461 21902 solver.cpp:237] Iteration 130500, loss = 0.893102
I0520 11:56:37.703631 21902 solver.cpp:253]     Train net output #0: loss = 0.893098 (* 1 = 0.893098 loss)
I0520 11:56:37.703645 21902 sgd_solver.cpp:106] Iteration 130500, lr = 0.0025
I0520 11:56:54.660009 21902 solver.cpp:237] Iteration 132000, loss = 0.609634
I0520 11:56:54.660048 21902 solver.cpp:253]     Train net output #0: loss = 0.60963 (* 1 = 0.60963 loss)
I0520 11:56:54.660063 21902 sgd_solver.cpp:106] Iteration 132000, lr = 0.0025
I0520 11:57:11.585513 21902 solver.cpp:237] Iteration 133500, loss = 1.28122
I0520 11:57:11.585671 21902 solver.cpp:253]     Train net output #0: loss = 1.28121 (* 1 = 1.28121 loss)
I0520 11:57:11.585685 21902 sgd_solver.cpp:106] Iteration 133500, lr = 0.0025
I0520 11:57:28.523491 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_135000.caffemodel
I0520 11:57:28.571254 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_135000.solverstate
I0520 11:57:28.601996 21902 solver.cpp:237] Iteration 135000, loss = 2.188
I0520 11:57:28.602046 21902 solver.cpp:253]     Train net output #0: loss = 2.18799 (* 1 = 2.18799 loss)
I0520 11:57:28.602063 21902 sgd_solver.cpp:106] Iteration 135000, lr = 0.0025
I0520 11:57:45.536085 21902 solver.cpp:237] Iteration 136500, loss = 0.894827
I0520 11:57:45.536248 21902 solver.cpp:253]     Train net output #0: loss = 0.894821 (* 1 = 0.894821 loss)
I0520 11:57:45.536263 21902 sgd_solver.cpp:106] Iteration 136500, lr = 0.0025
I0520 11:58:02.486685 21902 solver.cpp:237] Iteration 138000, loss = 1.23482
I0520 11:58:02.486734 21902 solver.cpp:253]     Train net output #0: loss = 1.23482 (* 1 = 1.23482 loss)
I0520 11:58:02.486754 21902 sgd_solver.cpp:106] Iteration 138000, lr = 0.0025
I0520 11:58:19.470896 21902 solver.cpp:237] Iteration 139500, loss = 1.4354
I0520 11:58:19.471057 21902 solver.cpp:253]     Train net output #0: loss = 1.43539 (* 1 = 1.43539 loss)
I0520 11:58:19.471071 21902 sgd_solver.cpp:106] Iteration 139500, lr = 0.0025
I0520 11:58:58.572698 21902 solver.cpp:237] Iteration 141000, loss = 1.39193
I0520 11:58:58.572873 21902 solver.cpp:253]     Train net output #0: loss = 1.39193 (* 1 = 1.39193 loss)
I0520 11:58:58.572886 21902 sgd_solver.cpp:106] Iteration 141000, lr = 0.0025
I0520 11:59:15.523974 21902 solver.cpp:237] Iteration 142500, loss = 1.13332
I0520 11:59:15.524021 21902 solver.cpp:253]     Train net output #0: loss = 1.13332 (* 1 = 1.13332 loss)
I0520 11:59:15.524035 21902 sgd_solver.cpp:106] Iteration 142500, lr = 0.0025
I0520 11:59:32.500123 21902 solver.cpp:237] Iteration 144000, loss = 1.72816
I0520 11:59:32.500283 21902 solver.cpp:253]     Train net output #0: loss = 1.72815 (* 1 = 1.72815 loss)
I0520 11:59:32.500295 21902 sgd_solver.cpp:106] Iteration 144000, lr = 0.0025
I0520 11:59:49.450305 21902 solver.cpp:237] Iteration 145500, loss = 1.72289
I0520 11:59:49.450341 21902 solver.cpp:253]     Train net output #0: loss = 1.72288 (* 1 = 1.72288 loss)
I0520 11:59:49.450357 21902 sgd_solver.cpp:106] Iteration 145500, lr = 0.0025
I0520 12:00:06.362107 21902 solver.cpp:237] Iteration 147000, loss = 1.08854
I0520 12:00:06.362265 21902 solver.cpp:253]     Train net output #0: loss = 1.08854 (* 1 = 1.08854 loss)
I0520 12:00:06.362282 21902 sgd_solver.cpp:106] Iteration 147000, lr = 0.0025
I0520 12:00:23.315807 21902 solver.cpp:237] Iteration 148500, loss = 0.947386
I0520 12:00:23.315851 21902 solver.cpp:253]     Train net output #0: loss = 0.94738 (* 1 = 0.94738 loss)
I0520 12:00:23.315870 21902 sgd_solver.cpp:106] Iteration 148500, lr = 0.0025
I0520 12:00:40.285399 21902 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_150000.caffemodel
I0520 12:00:40.333716 21902 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720_iter_150000.solverstate
I0520 12:01:01.230307 21902 solver.cpp:321] Iteration 150000, loss = 1.01239
I0520 12:01:01.230352 21902 solver.cpp:341] Iteration 150000, Testing net (#0)
I0520 12:02:00.635996 21902 solver.cpp:409]     Test net output #0: accuracy = 0.883535
I0520 12:02:00.636154 21902 solver.cpp:409]     Test net output #1: loss = 0.392147 (* 1 = 0.392147 loss)
I0520 12:02:00.636169 21902 solver.cpp:326] Optimization Done.
I0520 12:02:00.636178 21902 caffe.cpp:215] Optimization Done.
Application 11231723 resources: utime ~2135s, stime ~326s, Rss ~5333056, inblocks ~3744348, outblocks ~179817
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_10_2016-05-20T11.20.32.846720.solver"
	User time (seconds): 0.56
	System time (seconds): 0.16
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 41:09.19
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8656
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15072
	Voluntary context switches: 3865
	Involuntary context switches: 196
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
