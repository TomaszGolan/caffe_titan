2806269
I0521 04:57:16.857967 20178 caffe.cpp:184] Using GPUs 0
I0521 04:57:17.297511 20178 solver.cpp:48] Initializing solver from parameters: 
test_iter: 220
test_interval: 441
base_lr: 0.0025
display: 22
max_iter: 2205
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 220
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635.prototxt"
I0521 04:57:17.299216 20178 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635.prototxt
I0521 04:57:17.327190 20178 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0521 04:57:17.327250 20178 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0521 04:57:17.327594 20178 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 680
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:57:17.327776 20178 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:57:17.327800 20178 net.cpp:106] Creating Layer data_hdf5
I0521 04:57:17.327814 20178 net.cpp:411] data_hdf5 -> data
I0521 04:57:17.327847 20178 net.cpp:411] data_hdf5 -> label
I0521 04:57:17.327879 20178 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0521 04:57:17.329197 20178 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0521 04:57:17.331435 20178 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0521 04:57:38.877797 20178 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0521 04:57:38.882885 20178 net.cpp:150] Setting up data_hdf5
I0521 04:57:38.882925 20178 net.cpp:157] Top shape: 680 1 127 50 (4318000)
I0521 04:57:38.882939 20178 net.cpp:157] Top shape: 680 (680)
I0521 04:57:38.882953 20178 net.cpp:165] Memory required for data: 17274720
I0521 04:57:38.882966 20178 layer_factory.hpp:77] Creating layer conv1
I0521 04:57:38.883000 20178 net.cpp:106] Creating Layer conv1
I0521 04:57:38.883013 20178 net.cpp:454] conv1 <- data
I0521 04:57:38.883034 20178 net.cpp:411] conv1 -> conv1
I0521 04:57:39.251086 20178 net.cpp:150] Setting up conv1
I0521 04:57:39.251132 20178 net.cpp:157] Top shape: 680 12 120 48 (47001600)
I0521 04:57:39.251143 20178 net.cpp:165] Memory required for data: 205281120
I0521 04:57:39.251173 20178 layer_factory.hpp:77] Creating layer relu1
I0521 04:57:39.251194 20178 net.cpp:106] Creating Layer relu1
I0521 04:57:39.251204 20178 net.cpp:454] relu1 <- conv1
I0521 04:57:39.251219 20178 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:57:39.251734 20178 net.cpp:150] Setting up relu1
I0521 04:57:39.251749 20178 net.cpp:157] Top shape: 680 12 120 48 (47001600)
I0521 04:57:39.251760 20178 net.cpp:165] Memory required for data: 393287520
I0521 04:57:39.251770 20178 layer_factory.hpp:77] Creating layer pool1
I0521 04:57:39.251786 20178 net.cpp:106] Creating Layer pool1
I0521 04:57:39.251796 20178 net.cpp:454] pool1 <- conv1
I0521 04:57:39.251809 20178 net.cpp:411] pool1 -> pool1
I0521 04:57:39.251889 20178 net.cpp:150] Setting up pool1
I0521 04:57:39.251904 20178 net.cpp:157] Top shape: 680 12 60 48 (23500800)
I0521 04:57:39.251914 20178 net.cpp:165] Memory required for data: 487290720
I0521 04:57:39.251924 20178 layer_factory.hpp:77] Creating layer conv2
I0521 04:57:39.251945 20178 net.cpp:106] Creating Layer conv2
I0521 04:57:39.251956 20178 net.cpp:454] conv2 <- pool1
I0521 04:57:39.251971 20178 net.cpp:411] conv2 -> conv2
I0521 04:57:39.254660 20178 net.cpp:150] Setting up conv2
I0521 04:57:39.254688 20178 net.cpp:157] Top shape: 680 20 54 46 (33782400)
I0521 04:57:39.254698 20178 net.cpp:165] Memory required for data: 622420320
I0521 04:57:39.254717 20178 layer_factory.hpp:77] Creating layer relu2
I0521 04:57:39.254731 20178 net.cpp:106] Creating Layer relu2
I0521 04:57:39.254741 20178 net.cpp:454] relu2 <- conv2
I0521 04:57:39.254755 20178 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:57:39.255084 20178 net.cpp:150] Setting up relu2
I0521 04:57:39.255098 20178 net.cpp:157] Top shape: 680 20 54 46 (33782400)
I0521 04:57:39.255110 20178 net.cpp:165] Memory required for data: 757549920
I0521 04:57:39.255120 20178 layer_factory.hpp:77] Creating layer pool2
I0521 04:57:39.255132 20178 net.cpp:106] Creating Layer pool2
I0521 04:57:39.255142 20178 net.cpp:454] pool2 <- conv2
I0521 04:57:39.255167 20178 net.cpp:411] pool2 -> pool2
I0521 04:57:39.255235 20178 net.cpp:150] Setting up pool2
I0521 04:57:39.255249 20178 net.cpp:157] Top shape: 680 20 27 46 (16891200)
I0521 04:57:39.255259 20178 net.cpp:165] Memory required for data: 825114720
I0521 04:57:39.255270 20178 layer_factory.hpp:77] Creating layer conv3
I0521 04:57:39.255286 20178 net.cpp:106] Creating Layer conv3
I0521 04:57:39.255297 20178 net.cpp:454] conv3 <- pool2
I0521 04:57:39.255311 20178 net.cpp:411] conv3 -> conv3
I0521 04:57:39.257221 20178 net.cpp:150] Setting up conv3
I0521 04:57:39.257244 20178 net.cpp:157] Top shape: 680 28 22 44 (18430720)
I0521 04:57:39.257256 20178 net.cpp:165] Memory required for data: 898837600
I0521 04:57:39.257275 20178 layer_factory.hpp:77] Creating layer relu3
I0521 04:57:39.257292 20178 net.cpp:106] Creating Layer relu3
I0521 04:57:39.257302 20178 net.cpp:454] relu3 <- conv3
I0521 04:57:39.257314 20178 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:57:39.257793 20178 net.cpp:150] Setting up relu3
I0521 04:57:39.257812 20178 net.cpp:157] Top shape: 680 28 22 44 (18430720)
I0521 04:57:39.257822 20178 net.cpp:165] Memory required for data: 972560480
I0521 04:57:39.257832 20178 layer_factory.hpp:77] Creating layer pool3
I0521 04:57:39.257845 20178 net.cpp:106] Creating Layer pool3
I0521 04:57:39.257855 20178 net.cpp:454] pool3 <- conv3
I0521 04:57:39.257868 20178 net.cpp:411] pool3 -> pool3
I0521 04:57:39.257935 20178 net.cpp:150] Setting up pool3
I0521 04:57:39.257948 20178 net.cpp:157] Top shape: 680 28 11 44 (9215360)
I0521 04:57:39.257958 20178 net.cpp:165] Memory required for data: 1009421920
I0521 04:57:39.257968 20178 layer_factory.hpp:77] Creating layer conv4
I0521 04:57:39.257983 20178 net.cpp:106] Creating Layer conv4
I0521 04:57:39.257994 20178 net.cpp:454] conv4 <- pool3
I0521 04:57:39.258008 20178 net.cpp:411] conv4 -> conv4
I0521 04:57:39.260794 20178 net.cpp:150] Setting up conv4
I0521 04:57:39.260823 20178 net.cpp:157] Top shape: 680 36 6 42 (6168960)
I0521 04:57:39.260833 20178 net.cpp:165] Memory required for data: 1034097760
I0521 04:57:39.260849 20178 layer_factory.hpp:77] Creating layer relu4
I0521 04:57:39.260864 20178 net.cpp:106] Creating Layer relu4
I0521 04:57:39.260874 20178 net.cpp:454] relu4 <- conv4
I0521 04:57:39.260887 20178 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:57:39.261360 20178 net.cpp:150] Setting up relu4
I0521 04:57:39.261378 20178 net.cpp:157] Top shape: 680 36 6 42 (6168960)
I0521 04:57:39.261387 20178 net.cpp:165] Memory required for data: 1058773600
I0521 04:57:39.261399 20178 layer_factory.hpp:77] Creating layer pool4
I0521 04:57:39.261411 20178 net.cpp:106] Creating Layer pool4
I0521 04:57:39.261420 20178 net.cpp:454] pool4 <- conv4
I0521 04:57:39.261433 20178 net.cpp:411] pool4 -> pool4
I0521 04:57:39.261502 20178 net.cpp:150] Setting up pool4
I0521 04:57:39.261515 20178 net.cpp:157] Top shape: 680 36 3 42 (3084480)
I0521 04:57:39.261525 20178 net.cpp:165] Memory required for data: 1071111520
I0521 04:57:39.261535 20178 layer_factory.hpp:77] Creating layer ip1
I0521 04:57:39.261554 20178 net.cpp:106] Creating Layer ip1
I0521 04:57:39.261565 20178 net.cpp:454] ip1 <- pool4
I0521 04:57:39.261579 20178 net.cpp:411] ip1 -> ip1
I0521 04:57:39.277096 20178 net.cpp:150] Setting up ip1
I0521 04:57:39.277122 20178 net.cpp:157] Top shape: 680 196 (133280)
I0521 04:57:39.277139 20178 net.cpp:165] Memory required for data: 1071644640
I0521 04:57:39.277168 20178 layer_factory.hpp:77] Creating layer relu5
I0521 04:57:39.277182 20178 net.cpp:106] Creating Layer relu5
I0521 04:57:39.277192 20178 net.cpp:454] relu5 <- ip1
I0521 04:57:39.277205 20178 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:57:39.277545 20178 net.cpp:150] Setting up relu5
I0521 04:57:39.277560 20178 net.cpp:157] Top shape: 680 196 (133280)
I0521 04:57:39.277570 20178 net.cpp:165] Memory required for data: 1072177760
I0521 04:57:39.277580 20178 layer_factory.hpp:77] Creating layer drop1
I0521 04:57:39.277601 20178 net.cpp:106] Creating Layer drop1
I0521 04:57:39.277612 20178 net.cpp:454] drop1 <- ip1
I0521 04:57:39.277637 20178 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:57:39.277683 20178 net.cpp:150] Setting up drop1
I0521 04:57:39.277696 20178 net.cpp:157] Top shape: 680 196 (133280)
I0521 04:57:39.277706 20178 net.cpp:165] Memory required for data: 1072710880
I0521 04:57:39.277715 20178 layer_factory.hpp:77] Creating layer ip2
I0521 04:57:39.277739 20178 net.cpp:106] Creating Layer ip2
I0521 04:57:39.277750 20178 net.cpp:454] ip2 <- ip1
I0521 04:57:39.277762 20178 net.cpp:411] ip2 -> ip2
I0521 04:57:39.278226 20178 net.cpp:150] Setting up ip2
I0521 04:57:39.278239 20178 net.cpp:157] Top shape: 680 98 (66640)
I0521 04:57:39.278249 20178 net.cpp:165] Memory required for data: 1072977440
I0521 04:57:39.278264 20178 layer_factory.hpp:77] Creating layer relu6
I0521 04:57:39.278276 20178 net.cpp:106] Creating Layer relu6
I0521 04:57:39.278286 20178 net.cpp:454] relu6 <- ip2
I0521 04:57:39.278298 20178 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:57:39.278815 20178 net.cpp:150] Setting up relu6
I0521 04:57:39.278832 20178 net.cpp:157] Top shape: 680 98 (66640)
I0521 04:57:39.278843 20178 net.cpp:165] Memory required for data: 1073244000
I0521 04:57:39.278852 20178 layer_factory.hpp:77] Creating layer drop2
I0521 04:57:39.278866 20178 net.cpp:106] Creating Layer drop2
I0521 04:57:39.278875 20178 net.cpp:454] drop2 <- ip2
I0521 04:57:39.278887 20178 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:57:39.278929 20178 net.cpp:150] Setting up drop2
I0521 04:57:39.278942 20178 net.cpp:157] Top shape: 680 98 (66640)
I0521 04:57:39.278952 20178 net.cpp:165] Memory required for data: 1073510560
I0521 04:57:39.278962 20178 layer_factory.hpp:77] Creating layer ip3
I0521 04:57:39.278976 20178 net.cpp:106] Creating Layer ip3
I0521 04:57:39.278985 20178 net.cpp:454] ip3 <- ip2
I0521 04:57:39.278998 20178 net.cpp:411] ip3 -> ip3
I0521 04:57:39.279208 20178 net.cpp:150] Setting up ip3
I0521 04:57:39.279222 20178 net.cpp:157] Top shape: 680 11 (7480)
I0521 04:57:39.279232 20178 net.cpp:165] Memory required for data: 1073540480
I0521 04:57:39.279247 20178 layer_factory.hpp:77] Creating layer drop3
I0521 04:57:39.279260 20178 net.cpp:106] Creating Layer drop3
I0521 04:57:39.279269 20178 net.cpp:454] drop3 <- ip3
I0521 04:57:39.279281 20178 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:57:39.279320 20178 net.cpp:150] Setting up drop3
I0521 04:57:39.279333 20178 net.cpp:157] Top shape: 680 11 (7480)
I0521 04:57:39.279343 20178 net.cpp:165] Memory required for data: 1073570400
I0521 04:57:39.279353 20178 layer_factory.hpp:77] Creating layer loss
I0521 04:57:39.279372 20178 net.cpp:106] Creating Layer loss
I0521 04:57:39.279382 20178 net.cpp:454] loss <- ip3
I0521 04:57:39.279393 20178 net.cpp:454] loss <- label
I0521 04:57:39.279405 20178 net.cpp:411] loss -> loss
I0521 04:57:39.279422 20178 layer_factory.hpp:77] Creating layer loss
I0521 04:57:39.280071 20178 net.cpp:150] Setting up loss
I0521 04:57:39.280091 20178 net.cpp:157] Top shape: (1)
I0521 04:57:39.280104 20178 net.cpp:160]     with loss weight 1
I0521 04:57:39.280145 20178 net.cpp:165] Memory required for data: 1073570404
I0521 04:57:39.280155 20178 net.cpp:226] loss needs backward computation.
I0521 04:57:39.280166 20178 net.cpp:226] drop3 needs backward computation.
I0521 04:57:39.280176 20178 net.cpp:226] ip3 needs backward computation.
I0521 04:57:39.280186 20178 net.cpp:226] drop2 needs backward computation.
I0521 04:57:39.280196 20178 net.cpp:226] relu6 needs backward computation.
I0521 04:57:39.280206 20178 net.cpp:226] ip2 needs backward computation.
I0521 04:57:39.280216 20178 net.cpp:226] drop1 needs backward computation.
I0521 04:57:39.280225 20178 net.cpp:226] relu5 needs backward computation.
I0521 04:57:39.280236 20178 net.cpp:226] ip1 needs backward computation.
I0521 04:57:39.280246 20178 net.cpp:226] pool4 needs backward computation.
I0521 04:57:39.280256 20178 net.cpp:226] relu4 needs backward computation.
I0521 04:57:39.280266 20178 net.cpp:226] conv4 needs backward computation.
I0521 04:57:39.280275 20178 net.cpp:226] pool3 needs backward computation.
I0521 04:57:39.280294 20178 net.cpp:226] relu3 needs backward computation.
I0521 04:57:39.280304 20178 net.cpp:226] conv3 needs backward computation.
I0521 04:57:39.280313 20178 net.cpp:226] pool2 needs backward computation.
I0521 04:57:39.280323 20178 net.cpp:226] relu2 needs backward computation.
I0521 04:57:39.280333 20178 net.cpp:226] conv2 needs backward computation.
I0521 04:57:39.280344 20178 net.cpp:226] pool1 needs backward computation.
I0521 04:57:39.280354 20178 net.cpp:226] relu1 needs backward computation.
I0521 04:57:39.280365 20178 net.cpp:226] conv1 needs backward computation.
I0521 04:57:39.280376 20178 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:57:39.280386 20178 net.cpp:270] This network produces output loss
I0521 04:57:39.280410 20178 net.cpp:283] Network initialization done.
I0521 04:57:39.281971 20178 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635.prototxt
I0521 04:57:39.282042 20178 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0521 04:57:39.282398 20178 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 680
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0521 04:57:39.282588 20178 layer_factory.hpp:77] Creating layer data_hdf5
I0521 04:57:39.282603 20178 net.cpp:106] Creating Layer data_hdf5
I0521 04:57:39.282613 20178 net.cpp:411] data_hdf5 -> data
I0521 04:57:39.282629 20178 net.cpp:411] data_hdf5 -> label
I0521 04:57:39.282645 20178 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0521 04:57:39.283890 20178 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0521 04:58:00.580075 20178 net.cpp:150] Setting up data_hdf5
I0521 04:58:00.580240 20178 net.cpp:157] Top shape: 680 1 127 50 (4318000)
I0521 04:58:00.580255 20178 net.cpp:157] Top shape: 680 (680)
I0521 04:58:00.580268 20178 net.cpp:165] Memory required for data: 17274720
I0521 04:58:00.580282 20178 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0521 04:58:00.580310 20178 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0521 04:58:00.580322 20178 net.cpp:454] label_data_hdf5_1_split <- label
I0521 04:58:00.580337 20178 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0521 04:58:00.580358 20178 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0521 04:58:00.580431 20178 net.cpp:150] Setting up label_data_hdf5_1_split
I0521 04:58:00.580446 20178 net.cpp:157] Top shape: 680 (680)
I0521 04:58:00.580456 20178 net.cpp:157] Top shape: 680 (680)
I0521 04:58:00.580466 20178 net.cpp:165] Memory required for data: 17280160
I0521 04:58:00.580476 20178 layer_factory.hpp:77] Creating layer conv1
I0521 04:58:00.580497 20178 net.cpp:106] Creating Layer conv1
I0521 04:58:00.580508 20178 net.cpp:454] conv1 <- data
I0521 04:58:00.580523 20178 net.cpp:411] conv1 -> conv1
I0521 04:58:00.582458 20178 net.cpp:150] Setting up conv1
I0521 04:58:00.582482 20178 net.cpp:157] Top shape: 680 12 120 48 (47001600)
I0521 04:58:00.582494 20178 net.cpp:165] Memory required for data: 205286560
I0521 04:58:00.582515 20178 layer_factory.hpp:77] Creating layer relu1
I0521 04:58:00.582530 20178 net.cpp:106] Creating Layer relu1
I0521 04:58:00.582540 20178 net.cpp:454] relu1 <- conv1
I0521 04:58:00.582553 20178 net.cpp:397] relu1 -> conv1 (in-place)
I0521 04:58:00.583051 20178 net.cpp:150] Setting up relu1
I0521 04:58:00.583067 20178 net.cpp:157] Top shape: 680 12 120 48 (47001600)
I0521 04:58:00.583078 20178 net.cpp:165] Memory required for data: 393292960
I0521 04:58:00.583088 20178 layer_factory.hpp:77] Creating layer pool1
I0521 04:58:00.583106 20178 net.cpp:106] Creating Layer pool1
I0521 04:58:00.583115 20178 net.cpp:454] pool1 <- conv1
I0521 04:58:00.583128 20178 net.cpp:411] pool1 -> pool1
I0521 04:58:00.583204 20178 net.cpp:150] Setting up pool1
I0521 04:58:00.583216 20178 net.cpp:157] Top shape: 680 12 60 48 (23500800)
I0521 04:58:00.583228 20178 net.cpp:165] Memory required for data: 487296160
I0521 04:58:00.583238 20178 layer_factory.hpp:77] Creating layer conv2
I0521 04:58:00.583256 20178 net.cpp:106] Creating Layer conv2
I0521 04:58:00.583267 20178 net.cpp:454] conv2 <- pool1
I0521 04:58:00.583281 20178 net.cpp:411] conv2 -> conv2
I0521 04:58:00.585192 20178 net.cpp:150] Setting up conv2
I0521 04:58:00.585214 20178 net.cpp:157] Top shape: 680 20 54 46 (33782400)
I0521 04:58:00.585227 20178 net.cpp:165] Memory required for data: 622425760
I0521 04:58:00.585245 20178 layer_factory.hpp:77] Creating layer relu2
I0521 04:58:00.585258 20178 net.cpp:106] Creating Layer relu2
I0521 04:58:00.585268 20178 net.cpp:454] relu2 <- conv2
I0521 04:58:00.585281 20178 net.cpp:397] relu2 -> conv2 (in-place)
I0521 04:58:00.585615 20178 net.cpp:150] Setting up relu2
I0521 04:58:00.585630 20178 net.cpp:157] Top shape: 680 20 54 46 (33782400)
I0521 04:58:00.585639 20178 net.cpp:165] Memory required for data: 757555360
I0521 04:58:00.585649 20178 layer_factory.hpp:77] Creating layer pool2
I0521 04:58:00.585664 20178 net.cpp:106] Creating Layer pool2
I0521 04:58:00.585674 20178 net.cpp:454] pool2 <- conv2
I0521 04:58:00.585685 20178 net.cpp:411] pool2 -> pool2
I0521 04:58:00.585765 20178 net.cpp:150] Setting up pool2
I0521 04:58:00.585779 20178 net.cpp:157] Top shape: 680 20 27 46 (16891200)
I0521 04:58:00.585789 20178 net.cpp:165] Memory required for data: 825120160
I0521 04:58:00.585796 20178 layer_factory.hpp:77] Creating layer conv3
I0521 04:58:00.585818 20178 net.cpp:106] Creating Layer conv3
I0521 04:58:00.585827 20178 net.cpp:454] conv3 <- pool2
I0521 04:58:00.585841 20178 net.cpp:411] conv3 -> conv3
I0521 04:58:00.587817 20178 net.cpp:150] Setting up conv3
I0521 04:58:00.587836 20178 net.cpp:157] Top shape: 680 28 22 44 (18430720)
I0521 04:58:00.587846 20178 net.cpp:165] Memory required for data: 898843040
I0521 04:58:00.587878 20178 layer_factory.hpp:77] Creating layer relu3
I0521 04:58:00.587893 20178 net.cpp:106] Creating Layer relu3
I0521 04:58:00.587903 20178 net.cpp:454] relu3 <- conv3
I0521 04:58:00.587914 20178 net.cpp:397] relu3 -> conv3 (in-place)
I0521 04:58:00.588392 20178 net.cpp:150] Setting up relu3
I0521 04:58:00.588407 20178 net.cpp:157] Top shape: 680 28 22 44 (18430720)
I0521 04:58:00.588418 20178 net.cpp:165] Memory required for data: 972565920
I0521 04:58:00.588428 20178 layer_factory.hpp:77] Creating layer pool3
I0521 04:58:00.588441 20178 net.cpp:106] Creating Layer pool3
I0521 04:58:00.588451 20178 net.cpp:454] pool3 <- conv3
I0521 04:58:00.588464 20178 net.cpp:411] pool3 -> pool3
I0521 04:58:00.588536 20178 net.cpp:150] Setting up pool3
I0521 04:58:00.588548 20178 net.cpp:157] Top shape: 680 28 11 44 (9215360)
I0521 04:58:00.588558 20178 net.cpp:165] Memory required for data: 1009427360
I0521 04:58:00.588568 20178 layer_factory.hpp:77] Creating layer conv4
I0521 04:58:00.588584 20178 net.cpp:106] Creating Layer conv4
I0521 04:58:00.588595 20178 net.cpp:454] conv4 <- pool3
I0521 04:58:00.588609 20178 net.cpp:411] conv4 -> conv4
I0521 04:58:00.590667 20178 net.cpp:150] Setting up conv4
I0521 04:58:00.590687 20178 net.cpp:157] Top shape: 680 36 6 42 (6168960)
I0521 04:58:00.590701 20178 net.cpp:165] Memory required for data: 1034103200
I0521 04:58:00.590716 20178 layer_factory.hpp:77] Creating layer relu4
I0521 04:58:00.590730 20178 net.cpp:106] Creating Layer relu4
I0521 04:58:00.590740 20178 net.cpp:454] relu4 <- conv4
I0521 04:58:00.590752 20178 net.cpp:397] relu4 -> conv4 (in-place)
I0521 04:58:00.591223 20178 net.cpp:150] Setting up relu4
I0521 04:58:00.591238 20178 net.cpp:157] Top shape: 680 36 6 42 (6168960)
I0521 04:58:00.591248 20178 net.cpp:165] Memory required for data: 1058779040
I0521 04:58:00.591259 20178 layer_factory.hpp:77] Creating layer pool4
I0521 04:58:00.591271 20178 net.cpp:106] Creating Layer pool4
I0521 04:58:00.591281 20178 net.cpp:454] pool4 <- conv4
I0521 04:58:00.591295 20178 net.cpp:411] pool4 -> pool4
I0521 04:58:00.591366 20178 net.cpp:150] Setting up pool4
I0521 04:58:00.591379 20178 net.cpp:157] Top shape: 680 36 3 42 (3084480)
I0521 04:58:00.591388 20178 net.cpp:165] Memory required for data: 1071116960
I0521 04:58:00.591398 20178 layer_factory.hpp:77] Creating layer ip1
I0521 04:58:00.591416 20178 net.cpp:106] Creating Layer ip1
I0521 04:58:00.591426 20178 net.cpp:454] ip1 <- pool4
I0521 04:58:00.591440 20178 net.cpp:411] ip1 -> ip1
I0521 04:58:00.606860 20178 net.cpp:150] Setting up ip1
I0521 04:58:00.606887 20178 net.cpp:157] Top shape: 680 196 (133280)
I0521 04:58:00.606900 20178 net.cpp:165] Memory required for data: 1071650080
I0521 04:58:00.606922 20178 layer_factory.hpp:77] Creating layer relu5
I0521 04:58:00.606937 20178 net.cpp:106] Creating Layer relu5
I0521 04:58:00.606947 20178 net.cpp:454] relu5 <- ip1
I0521 04:58:00.606961 20178 net.cpp:397] relu5 -> ip1 (in-place)
I0521 04:58:00.607308 20178 net.cpp:150] Setting up relu5
I0521 04:58:00.607322 20178 net.cpp:157] Top shape: 680 196 (133280)
I0521 04:58:00.607331 20178 net.cpp:165] Memory required for data: 1072183200
I0521 04:58:00.607342 20178 layer_factory.hpp:77] Creating layer drop1
I0521 04:58:00.607360 20178 net.cpp:106] Creating Layer drop1
I0521 04:58:00.607370 20178 net.cpp:454] drop1 <- ip1
I0521 04:58:00.607383 20178 net.cpp:397] drop1 -> ip1 (in-place)
I0521 04:58:00.607429 20178 net.cpp:150] Setting up drop1
I0521 04:58:00.607441 20178 net.cpp:157] Top shape: 680 196 (133280)
I0521 04:58:00.607450 20178 net.cpp:165] Memory required for data: 1072716320
I0521 04:58:00.607460 20178 layer_factory.hpp:77] Creating layer ip2
I0521 04:58:00.607475 20178 net.cpp:106] Creating Layer ip2
I0521 04:58:00.607486 20178 net.cpp:454] ip2 <- ip1
I0521 04:58:00.607498 20178 net.cpp:411] ip2 -> ip2
I0521 04:58:00.607978 20178 net.cpp:150] Setting up ip2
I0521 04:58:00.607992 20178 net.cpp:157] Top shape: 680 98 (66640)
I0521 04:58:00.608001 20178 net.cpp:165] Memory required for data: 1072982880
I0521 04:58:00.608029 20178 layer_factory.hpp:77] Creating layer relu6
I0521 04:58:00.608043 20178 net.cpp:106] Creating Layer relu6
I0521 04:58:00.608053 20178 net.cpp:454] relu6 <- ip2
I0521 04:58:00.608065 20178 net.cpp:397] relu6 -> ip2 (in-place)
I0521 04:58:00.608603 20178 net.cpp:150] Setting up relu6
I0521 04:58:00.608625 20178 net.cpp:157] Top shape: 680 98 (66640)
I0521 04:58:00.608635 20178 net.cpp:165] Memory required for data: 1073249440
I0521 04:58:00.608645 20178 layer_factory.hpp:77] Creating layer drop2
I0521 04:58:00.608659 20178 net.cpp:106] Creating Layer drop2
I0521 04:58:00.608669 20178 net.cpp:454] drop2 <- ip2
I0521 04:58:00.608682 20178 net.cpp:397] drop2 -> ip2 (in-place)
I0521 04:58:00.608726 20178 net.cpp:150] Setting up drop2
I0521 04:58:00.608739 20178 net.cpp:157] Top shape: 680 98 (66640)
I0521 04:58:00.608749 20178 net.cpp:165] Memory required for data: 1073516000
I0521 04:58:00.608759 20178 layer_factory.hpp:77] Creating layer ip3
I0521 04:58:00.608773 20178 net.cpp:106] Creating Layer ip3
I0521 04:58:00.608783 20178 net.cpp:454] ip3 <- ip2
I0521 04:58:00.608798 20178 net.cpp:411] ip3 -> ip3
I0521 04:58:00.609020 20178 net.cpp:150] Setting up ip3
I0521 04:58:00.609033 20178 net.cpp:157] Top shape: 680 11 (7480)
I0521 04:58:00.609045 20178 net.cpp:165] Memory required for data: 1073545920
I0521 04:58:00.609060 20178 layer_factory.hpp:77] Creating layer drop3
I0521 04:58:00.609072 20178 net.cpp:106] Creating Layer drop3
I0521 04:58:00.609082 20178 net.cpp:454] drop3 <- ip3
I0521 04:58:00.609094 20178 net.cpp:397] drop3 -> ip3 (in-place)
I0521 04:58:00.609135 20178 net.cpp:150] Setting up drop3
I0521 04:58:00.609148 20178 net.cpp:157] Top shape: 680 11 (7480)
I0521 04:58:00.609158 20178 net.cpp:165] Memory required for data: 1073575840
I0521 04:58:00.609169 20178 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0521 04:58:00.609180 20178 net.cpp:106] Creating Layer ip3_drop3_0_split
I0521 04:58:00.609190 20178 net.cpp:454] ip3_drop3_0_split <- ip3
I0521 04:58:00.609203 20178 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0521 04:58:00.609218 20178 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0521 04:58:00.609292 20178 net.cpp:150] Setting up ip3_drop3_0_split
I0521 04:58:00.609305 20178 net.cpp:157] Top shape: 680 11 (7480)
I0521 04:58:00.609318 20178 net.cpp:157] Top shape: 680 11 (7480)
I0521 04:58:00.609328 20178 net.cpp:165] Memory required for data: 1073635680
I0521 04:58:00.609338 20178 layer_factory.hpp:77] Creating layer accuracy
I0521 04:58:00.609359 20178 net.cpp:106] Creating Layer accuracy
I0521 04:58:00.609370 20178 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0521 04:58:00.609381 20178 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0521 04:58:00.609395 20178 net.cpp:411] accuracy -> accuracy
I0521 04:58:00.609418 20178 net.cpp:150] Setting up accuracy
I0521 04:58:00.609431 20178 net.cpp:157] Top shape: (1)
I0521 04:58:00.609441 20178 net.cpp:165] Memory required for data: 1073635684
I0521 04:58:00.609452 20178 layer_factory.hpp:77] Creating layer loss
I0521 04:58:00.609465 20178 net.cpp:106] Creating Layer loss
I0521 04:58:00.609475 20178 net.cpp:454] loss <- ip3_drop3_0_split_1
I0521 04:58:00.609486 20178 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0521 04:58:00.609499 20178 net.cpp:411] loss -> loss
I0521 04:58:00.609518 20178 layer_factory.hpp:77] Creating layer loss
I0521 04:58:00.610018 20178 net.cpp:150] Setting up loss
I0521 04:58:00.610031 20178 net.cpp:157] Top shape: (1)
I0521 04:58:00.610041 20178 net.cpp:160]     with loss weight 1
I0521 04:58:00.610059 20178 net.cpp:165] Memory required for data: 1073635688
I0521 04:58:00.610070 20178 net.cpp:226] loss needs backward computation.
I0521 04:58:00.610081 20178 net.cpp:228] accuracy does not need backward computation.
I0521 04:58:00.610092 20178 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0521 04:58:00.610102 20178 net.cpp:226] drop3 needs backward computation.
I0521 04:58:00.610113 20178 net.cpp:226] ip3 needs backward computation.
I0521 04:58:00.610122 20178 net.cpp:226] drop2 needs backward computation.
I0521 04:58:00.610141 20178 net.cpp:226] relu6 needs backward computation.
I0521 04:58:00.610151 20178 net.cpp:226] ip2 needs backward computation.
I0521 04:58:00.610162 20178 net.cpp:226] drop1 needs backward computation.
I0521 04:58:00.610170 20178 net.cpp:226] relu5 needs backward computation.
I0521 04:58:00.610179 20178 net.cpp:226] ip1 needs backward computation.
I0521 04:58:00.610189 20178 net.cpp:226] pool4 needs backward computation.
I0521 04:58:00.610200 20178 net.cpp:226] relu4 needs backward computation.
I0521 04:58:00.610209 20178 net.cpp:226] conv4 needs backward computation.
I0521 04:58:00.610220 20178 net.cpp:226] pool3 needs backward computation.
I0521 04:58:00.610230 20178 net.cpp:226] relu3 needs backward computation.
I0521 04:58:00.610240 20178 net.cpp:226] conv3 needs backward computation.
I0521 04:58:00.610251 20178 net.cpp:226] pool2 needs backward computation.
I0521 04:58:00.610261 20178 net.cpp:226] relu2 needs backward computation.
I0521 04:58:00.610270 20178 net.cpp:226] conv2 needs backward computation.
I0521 04:58:00.610281 20178 net.cpp:226] pool1 needs backward computation.
I0521 04:58:00.610292 20178 net.cpp:226] relu1 needs backward computation.
I0521 04:58:00.610302 20178 net.cpp:226] conv1 needs backward computation.
I0521 04:58:00.610313 20178 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0521 04:58:00.610326 20178 net.cpp:228] data_hdf5 does not need backward computation.
I0521 04:58:00.610335 20178 net.cpp:270] This network produces output accuracy
I0521 04:58:00.610345 20178 net.cpp:270] This network produces output loss
I0521 04:58:00.610373 20178 net.cpp:283] Network initialization done.
I0521 04:58:00.610507 20178 solver.cpp:60] Solver scaffolding done.
I0521 04:58:00.611634 20178 caffe.cpp:212] Starting Optimization
I0521 04:58:00.611647 20178 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0521 04:58:00.611657 20178 solver.cpp:289] Learning Rate Policy: fixed
I0521 04:58:00.612874 20178 solver.cpp:341] Iteration 0, Testing net (#0)
I0521 04:58:46.514794 20178 solver.cpp:409]     Test net output #0: accuracy = 0.128603
I0521 04:58:46.514955 20178 solver.cpp:409]     Test net output #1: loss = 2.39783 (* 1 = 2.39783 loss)
I0521 04:58:46.642918 20178 solver.cpp:237] Iteration 0, loss = 2.40041
I0521 04:58:46.642954 20178 solver.cpp:253]     Train net output #0: loss = 2.40041 (* 1 = 2.40041 loss)
I0521 04:58:46.642972 20178 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0521 04:58:54.682004 20178 solver.cpp:237] Iteration 22, loss = 2.38221
I0521 04:58:54.682044 20178 solver.cpp:253]     Train net output #0: loss = 2.38221 (* 1 = 2.38221 loss)
I0521 04:58:54.682062 20178 sgd_solver.cpp:106] Iteration 22, lr = 0.0025
I0521 04:59:02.716688 20178 solver.cpp:237] Iteration 44, loss = 2.36671
I0521 04:59:02.716720 20178 solver.cpp:253]     Train net output #0: loss = 2.36671 (* 1 = 2.36671 loss)
I0521 04:59:02.716737 20178 sgd_solver.cpp:106] Iteration 44, lr = 0.0025
I0521 04:59:10.748800 20178 solver.cpp:237] Iteration 66, loss = 2.34784
I0521 04:59:10.748831 20178 solver.cpp:253]     Train net output #0: loss = 2.34784 (* 1 = 2.34784 loss)
I0521 04:59:10.748849 20178 sgd_solver.cpp:106] Iteration 66, lr = 0.0025
I0521 04:59:18.786453 20178 solver.cpp:237] Iteration 88, loss = 2.33621
I0521 04:59:18.786599 20178 solver.cpp:253]     Train net output #0: loss = 2.33621 (* 1 = 2.33621 loss)
I0521 04:59:18.786613 20178 sgd_solver.cpp:106] Iteration 88, lr = 0.0025
I0521 04:59:26.824673 20178 solver.cpp:237] Iteration 110, loss = 2.3406
I0521 04:59:26.824704 20178 solver.cpp:253]     Train net output #0: loss = 2.3406 (* 1 = 2.3406 loss)
I0521 04:59:26.824723 20178 sgd_solver.cpp:106] Iteration 110, lr = 0.0025
I0521 04:59:34.858484 20178 solver.cpp:237] Iteration 132, loss = 2.33247
I0521 04:59:34.858515 20178 solver.cpp:253]     Train net output #0: loss = 2.33247 (* 1 = 2.33247 loss)
I0521 04:59:34.858531 20178 sgd_solver.cpp:106] Iteration 132, lr = 0.0025
I0521 05:00:05.023608 20178 solver.cpp:237] Iteration 154, loss = 2.32765
I0521 05:00:05.023772 20178 solver.cpp:253]     Train net output #0: loss = 2.32765 (* 1 = 2.32765 loss)
I0521 05:00:05.023787 20178 sgd_solver.cpp:106] Iteration 154, lr = 0.0025
I0521 05:00:13.059141 20178 solver.cpp:237] Iteration 176, loss = 2.28072
I0521 05:00:13.059171 20178 solver.cpp:253]     Train net output #0: loss = 2.28072 (* 1 = 2.28072 loss)
I0521 05:00:13.059192 20178 sgd_solver.cpp:106] Iteration 176, lr = 0.0025
I0521 05:00:21.097301 20178 solver.cpp:237] Iteration 198, loss = 2.2521
I0521 05:00:21.097333 20178 solver.cpp:253]     Train net output #0: loss = 2.2521 (* 1 = 2.2521 loss)
I0521 05:00:21.097350 20178 sgd_solver.cpp:106] Iteration 198, lr = 0.0025
I0521 05:00:28.770756 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_220.caffemodel
I0521 05:00:29.074340 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_220.solverstate
I0521 05:00:29.211376 20178 solver.cpp:237] Iteration 220, loss = 2.23123
I0521 05:00:29.211421 20178 solver.cpp:253]     Train net output #0: loss = 2.23123 (* 1 = 2.23123 loss)
I0521 05:00:29.211442 20178 sgd_solver.cpp:106] Iteration 220, lr = 0.0025
I0521 05:00:37.246382 20178 solver.cpp:237] Iteration 242, loss = 2.21335
I0521 05:00:37.246541 20178 solver.cpp:253]     Train net output #0: loss = 2.21335 (* 1 = 2.21335 loss)
I0521 05:00:37.246556 20178 sgd_solver.cpp:106] Iteration 242, lr = 0.0025
I0521 05:00:45.282475 20178 solver.cpp:237] Iteration 264, loss = 2.17098
I0521 05:00:45.282508 20178 solver.cpp:253]     Train net output #0: loss = 2.17098 (* 1 = 2.17098 loss)
I0521 05:00:45.282522 20178 sgd_solver.cpp:106] Iteration 264, lr = 0.0025
I0521 05:00:53.320365 20178 solver.cpp:237] Iteration 286, loss = 2.1408
I0521 05:00:53.320399 20178 solver.cpp:253]     Train net output #0: loss = 2.1408 (* 1 = 2.1408 loss)
I0521 05:00:53.320415 20178 sgd_solver.cpp:106] Iteration 286, lr = 0.0025
I0521 05:01:23.480411 20178 solver.cpp:237] Iteration 308, loss = 2.15606
I0521 05:01:23.480564 20178 solver.cpp:253]     Train net output #0: loss = 2.15606 (* 1 = 2.15606 loss)
I0521 05:01:23.480581 20178 sgd_solver.cpp:106] Iteration 308, lr = 0.0025
I0521 05:01:31.524080 20178 solver.cpp:237] Iteration 330, loss = 2.10897
I0521 05:01:31.524113 20178 solver.cpp:253]     Train net output #0: loss = 2.10897 (* 1 = 2.10897 loss)
I0521 05:01:31.524129 20178 sgd_solver.cpp:106] Iteration 330, lr = 0.0025
I0521 05:01:39.563922 20178 solver.cpp:237] Iteration 352, loss = 2.048
I0521 05:01:39.563954 20178 solver.cpp:253]     Train net output #0: loss = 2.048 (* 1 = 2.048 loss)
I0521 05:01:39.563971 20178 sgd_solver.cpp:106] Iteration 352, lr = 0.0025
I0521 05:01:47.606155 20178 solver.cpp:237] Iteration 374, loss = 2.03673
I0521 05:01:47.606194 20178 solver.cpp:253]     Train net output #0: loss = 2.03673 (* 1 = 2.03673 loss)
I0521 05:01:47.606212 20178 sgd_solver.cpp:106] Iteration 374, lr = 0.0025
I0521 05:01:55.646334 20178 solver.cpp:237] Iteration 396, loss = 2.00438
I0521 05:01:55.646476 20178 solver.cpp:253]     Train net output #0: loss = 2.00438 (* 1 = 2.00438 loss)
I0521 05:01:55.646489 20178 sgd_solver.cpp:106] Iteration 396, lr = 0.0025
I0521 05:02:03.685600 20178 solver.cpp:237] Iteration 418, loss = 2.02498
I0521 05:02:03.685632 20178 solver.cpp:253]     Train net output #0: loss = 2.02498 (* 1 = 2.02498 loss)
I0521 05:02:03.685650 20178 sgd_solver.cpp:106] Iteration 418, lr = 0.0025
I0521 05:02:11.359977 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_440.caffemodel
I0521 05:02:11.654430 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_440.solverstate
I0521 05:02:11.789057 20178 solver.cpp:237] Iteration 440, loss = 1.99198
I0521 05:02:11.789105 20178 solver.cpp:253]     Train net output #0: loss = 1.99198 (* 1 = 1.99198 loss)
I0521 05:02:11.789121 20178 sgd_solver.cpp:106] Iteration 440, lr = 0.0025
I0521 05:02:11.789629 20178 solver.cpp:341] Iteration 441, Testing net (#0)
I0521 05:02:56.988858 20178 solver.cpp:409]     Test net output #0: accuracy = 0.528784
I0521 05:02:56.989018 20178 solver.cpp:409]     Test net output #1: loss = 1.69576 (* 1 = 1.69576 loss)
I0521 05:03:26.894551 20178 solver.cpp:237] Iteration 462, loss = 1.96873
I0521 05:03:26.894603 20178 solver.cpp:253]     Train net output #0: loss = 1.96873 (* 1 = 1.96873 loss)
I0521 05:03:26.894616 20178 sgd_solver.cpp:106] Iteration 462, lr = 0.0025
I0521 05:03:34.929404 20178 solver.cpp:237] Iteration 484, loss = 1.87339
I0521 05:03:34.929558 20178 solver.cpp:253]     Train net output #0: loss = 1.87339 (* 1 = 1.87339 loss)
I0521 05:03:34.929572 20178 sgd_solver.cpp:106] Iteration 484, lr = 0.0025
I0521 05:03:42.961756 20178 solver.cpp:237] Iteration 506, loss = 1.91799
I0521 05:03:42.961787 20178 solver.cpp:253]     Train net output #0: loss = 1.91799 (* 1 = 1.91799 loss)
I0521 05:03:42.961805 20178 sgd_solver.cpp:106] Iteration 506, lr = 0.0025
I0521 05:03:50.994287 20178 solver.cpp:237] Iteration 528, loss = 1.9416
I0521 05:03:50.994318 20178 solver.cpp:253]     Train net output #0: loss = 1.9416 (* 1 = 1.9416 loss)
I0521 05:03:50.994334 20178 sgd_solver.cpp:106] Iteration 528, lr = 0.0025
I0521 05:03:59.027078 20178 solver.cpp:237] Iteration 550, loss = 1.92587
I0521 05:03:59.027114 20178 solver.cpp:253]     Train net output #0: loss = 1.92587 (* 1 = 1.92587 loss)
I0521 05:03:59.027135 20178 sgd_solver.cpp:106] Iteration 550, lr = 0.0025
I0521 05:04:07.062243 20178 solver.cpp:237] Iteration 572, loss = 1.84694
I0521 05:04:07.062386 20178 solver.cpp:253]     Train net output #0: loss = 1.84694 (* 1 = 1.84694 loss)
I0521 05:04:07.062400 20178 sgd_solver.cpp:106] Iteration 572, lr = 0.0025
I0521 05:04:37.311724 20178 solver.cpp:237] Iteration 594, loss = 1.93142
I0521 05:04:37.311889 20178 solver.cpp:253]     Train net output #0: loss = 1.93142 (* 1 = 1.93142 loss)
I0521 05:04:37.311905 20178 sgd_solver.cpp:106] Iteration 594, lr = 0.0025
I0521 05:04:45.338882 20178 solver.cpp:237] Iteration 616, loss = 1.83147
I0521 05:04:45.338920 20178 solver.cpp:253]     Train net output #0: loss = 1.83147 (* 1 = 1.83147 loss)
I0521 05:04:45.338934 20178 sgd_solver.cpp:106] Iteration 616, lr = 0.0025
I0521 05:04:53.371588 20178 solver.cpp:237] Iteration 638, loss = 1.88389
I0521 05:04:53.371621 20178 solver.cpp:253]     Train net output #0: loss = 1.88389 (* 1 = 1.88389 loss)
I0521 05:04:53.371639 20178 sgd_solver.cpp:106] Iteration 638, lr = 0.0025
I0521 05:05:01.037145 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_660.caffemodel
I0521 05:05:01.333155 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_660.solverstate
I0521 05:05:01.468592 20178 solver.cpp:237] Iteration 660, loss = 1.89062
I0521 05:05:01.468637 20178 solver.cpp:253]     Train net output #0: loss = 1.89062 (* 1 = 1.89062 loss)
I0521 05:05:01.468658 20178 sgd_solver.cpp:106] Iteration 660, lr = 0.0025
I0521 05:05:09.499285 20178 solver.cpp:237] Iteration 682, loss = 1.87578
I0521 05:05:09.499436 20178 solver.cpp:253]     Train net output #0: loss = 1.87578 (* 1 = 1.87578 loss)
I0521 05:05:09.499450 20178 sgd_solver.cpp:106] Iteration 682, lr = 0.0025
I0521 05:05:17.531461 20178 solver.cpp:237] Iteration 704, loss = 1.79637
I0521 05:05:17.531491 20178 solver.cpp:253]     Train net output #0: loss = 1.79637 (* 1 = 1.79637 loss)
I0521 05:05:17.531510 20178 sgd_solver.cpp:106] Iteration 704, lr = 0.0025
I0521 05:05:25.564990 20178 solver.cpp:237] Iteration 726, loss = 1.84442
I0521 05:05:25.565022 20178 solver.cpp:253]     Train net output #0: loss = 1.84442 (* 1 = 1.84442 loss)
I0521 05:05:25.565039 20178 sgd_solver.cpp:106] Iteration 726, lr = 0.0025
I0521 05:05:55.762981 20178 solver.cpp:237] Iteration 748, loss = 1.89242
I0521 05:05:55.763152 20178 solver.cpp:253]     Train net output #0: loss = 1.89242 (* 1 = 1.89242 loss)
I0521 05:05:55.763167 20178 sgd_solver.cpp:106] Iteration 748, lr = 0.0025
I0521 05:06:03.791573 20178 solver.cpp:237] Iteration 770, loss = 1.81025
I0521 05:06:03.791612 20178 solver.cpp:253]     Train net output #0: loss = 1.81025 (* 1 = 1.81025 loss)
I0521 05:06:03.791627 20178 sgd_solver.cpp:106] Iteration 770, lr = 0.0025
I0521 05:06:11.826251 20178 solver.cpp:237] Iteration 792, loss = 1.81671
I0521 05:06:11.826283 20178 solver.cpp:253]     Train net output #0: loss = 1.81671 (* 1 = 1.81671 loss)
I0521 05:06:11.826302 20178 sgd_solver.cpp:106] Iteration 792, lr = 0.0025
I0521 05:06:19.854205 20178 solver.cpp:237] Iteration 814, loss = 1.83755
I0521 05:06:19.854238 20178 solver.cpp:253]     Train net output #0: loss = 1.83755 (* 1 = 1.83755 loss)
I0521 05:06:19.854255 20178 sgd_solver.cpp:106] Iteration 814, lr = 0.0025
I0521 05:06:27.883049 20178 solver.cpp:237] Iteration 836, loss = 1.83107
I0521 05:06:27.883199 20178 solver.cpp:253]     Train net output #0: loss = 1.83107 (* 1 = 1.83107 loss)
I0521 05:06:27.883213 20178 sgd_solver.cpp:106] Iteration 836, lr = 0.0025
I0521 05:06:35.915235 20178 solver.cpp:237] Iteration 858, loss = 1.76772
I0521 05:06:35.915266 20178 solver.cpp:253]     Train net output #0: loss = 1.76772 (* 1 = 1.76772 loss)
I0521 05:06:35.915284 20178 sgd_solver.cpp:106] Iteration 858, lr = 0.0025
I0521 05:06:43.579299 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_880.caffemodel
I0521 05:06:43.875778 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_880.solverstate
I0521 05:06:44.011243 20178 solver.cpp:237] Iteration 880, loss = 1.81145
I0521 05:06:44.011292 20178 solver.cpp:253]     Train net output #0: loss = 1.81145 (* 1 = 1.81145 loss)
I0521 05:06:44.011308 20178 sgd_solver.cpp:106] Iteration 880, lr = 0.0025
I0521 05:06:44.377092 20178 solver.cpp:341] Iteration 882, Testing net (#0)
I0521 05:07:50.440888 20178 solver.cpp:409]     Test net output #0: accuracy = 0.606143
I0521 05:07:50.441056 20178 solver.cpp:409]     Test net output #1: loss = 1.3555 (* 1 = 1.3555 loss)
I0521 05:08:20.031699 20178 solver.cpp:237] Iteration 902, loss = 1.76012
I0521 05:08:20.031749 20178 solver.cpp:253]     Train net output #0: loss = 1.76012 (* 1 = 1.76012 loss)
I0521 05:08:20.031764 20178 sgd_solver.cpp:106] Iteration 902, lr = 0.0025
I0521 05:08:28.056794 20178 solver.cpp:237] Iteration 924, loss = 1.81648
I0521 05:08:28.056944 20178 solver.cpp:253]     Train net output #0: loss = 1.81648 (* 1 = 1.81648 loss)
I0521 05:08:28.056958 20178 sgd_solver.cpp:106] Iteration 924, lr = 0.0025
I0521 05:08:36.082743 20178 solver.cpp:237] Iteration 946, loss = 1.74445
I0521 05:08:36.082775 20178 solver.cpp:253]     Train net output #0: loss = 1.74445 (* 1 = 1.74445 loss)
I0521 05:08:36.082793 20178 sgd_solver.cpp:106] Iteration 946, lr = 0.0025
I0521 05:08:44.112145 20178 solver.cpp:237] Iteration 968, loss = 1.84382
I0521 05:08:44.112177 20178 solver.cpp:253]     Train net output #0: loss = 1.84382 (* 1 = 1.84382 loss)
I0521 05:08:44.112193 20178 sgd_solver.cpp:106] Iteration 968, lr = 0.0025
I0521 05:08:52.137547 20178 solver.cpp:237] Iteration 990, loss = 1.80716
I0521 05:08:52.137578 20178 solver.cpp:253]     Train net output #0: loss = 1.80716 (* 1 = 1.80716 loss)
I0521 05:08:52.137595 20178 sgd_solver.cpp:106] Iteration 990, lr = 0.0025
I0521 05:09:00.166503 20178 solver.cpp:237] Iteration 1012, loss = 1.67647
I0521 05:09:00.166651 20178 solver.cpp:253]     Train net output #0: loss = 1.67647 (* 1 = 1.67647 loss)
I0521 05:09:00.166664 20178 sgd_solver.cpp:106] Iteration 1012, lr = 0.0025
I0521 05:09:30.361307 20178 solver.cpp:237] Iteration 1034, loss = 1.74896
I0521 05:09:30.361475 20178 solver.cpp:253]     Train net output #0: loss = 1.74896 (* 1 = 1.74896 loss)
I0521 05:09:30.361491 20178 sgd_solver.cpp:106] Iteration 1034, lr = 0.0025
I0521 05:09:38.387079 20178 solver.cpp:237] Iteration 1056, loss = 1.73871
I0521 05:09:38.387111 20178 solver.cpp:253]     Train net output #0: loss = 1.73871 (* 1 = 1.73871 loss)
I0521 05:09:38.387128 20178 sgd_solver.cpp:106] Iteration 1056, lr = 0.0025
I0521 05:09:46.412674 20178 solver.cpp:237] Iteration 1078, loss = 1.79679
I0521 05:09:46.412714 20178 solver.cpp:253]     Train net output #0: loss = 1.79679 (* 1 = 1.79679 loss)
I0521 05:09:46.412732 20178 sgd_solver.cpp:106] Iteration 1078, lr = 0.0025
I0521 05:09:54.071871 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1100.caffemodel
I0521 05:09:54.368998 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1100.solverstate
I0521 05:09:54.505456 20178 solver.cpp:237] Iteration 1100, loss = 1.78962
I0521 05:09:54.505501 20178 solver.cpp:253]     Train net output #0: loss = 1.78962 (* 1 = 1.78962 loss)
I0521 05:09:54.505518 20178 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0521 05:10:02.529634 20178 solver.cpp:237] Iteration 1122, loss = 1.70494
I0521 05:10:02.529784 20178 solver.cpp:253]     Train net output #0: loss = 1.70494 (* 1 = 1.70494 loss)
I0521 05:10:02.529798 20178 sgd_solver.cpp:106] Iteration 1122, lr = 0.0025
I0521 05:10:10.558414 20178 solver.cpp:237] Iteration 1144, loss = 1.78152
I0521 05:10:10.558452 20178 solver.cpp:253]     Train net output #0: loss = 1.78152 (* 1 = 1.78152 loss)
I0521 05:10:10.558475 20178 sgd_solver.cpp:106] Iteration 1144, lr = 0.0025
I0521 05:10:18.583966 20178 solver.cpp:237] Iteration 1166, loss = 1.71341
I0521 05:10:18.583998 20178 solver.cpp:253]     Train net output #0: loss = 1.71341 (* 1 = 1.71341 loss)
I0521 05:10:18.584015 20178 sgd_solver.cpp:106] Iteration 1166, lr = 0.0025
I0521 05:10:48.786695 20178 solver.cpp:237] Iteration 1188, loss = 1.74372
I0521 05:10:48.786865 20178 solver.cpp:253]     Train net output #0: loss = 1.74372 (* 1 = 1.74372 loss)
I0521 05:10:48.786881 20178 sgd_solver.cpp:106] Iteration 1188, lr = 0.0025
I0521 05:10:56.815094 20178 solver.cpp:237] Iteration 1210, loss = 1.77434
I0521 05:10:56.815127 20178 solver.cpp:253]     Train net output #0: loss = 1.77434 (* 1 = 1.77434 loss)
I0521 05:10:56.815145 20178 sgd_solver.cpp:106] Iteration 1210, lr = 0.0025
I0521 05:11:04.841548 20178 solver.cpp:237] Iteration 1232, loss = 1.67721
I0521 05:11:04.841578 20178 solver.cpp:253]     Train net output #0: loss = 1.67721 (* 1 = 1.67721 loss)
I0521 05:11:04.841595 20178 sgd_solver.cpp:106] Iteration 1232, lr = 0.0025
I0521 05:11:12.870895 20178 solver.cpp:237] Iteration 1254, loss = 1.72937
I0521 05:11:12.870929 20178 solver.cpp:253]     Train net output #0: loss = 1.72937 (* 1 = 1.72937 loss)
I0521 05:11:12.870945 20178 sgd_solver.cpp:106] Iteration 1254, lr = 0.0025
I0521 05:11:20.898406 20178 solver.cpp:237] Iteration 1276, loss = 1.67414
I0521 05:11:20.898540 20178 solver.cpp:253]     Train net output #0: loss = 1.67414 (* 1 = 1.67414 loss)
I0521 05:11:20.898553 20178 sgd_solver.cpp:106] Iteration 1276, lr = 0.0025
I0521 05:11:28.928895 20178 solver.cpp:237] Iteration 1298, loss = 1.69531
I0521 05:11:28.928927 20178 solver.cpp:253]     Train net output #0: loss = 1.69531 (* 1 = 1.69531 loss)
I0521 05:11:28.928947 20178 sgd_solver.cpp:106] Iteration 1298, lr = 0.0025
I0521 05:11:36.593076 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1320.caffemodel
I0521 05:11:36.887576 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1320.solverstate
I0521 05:11:37.022686 20178 solver.cpp:237] Iteration 1320, loss = 1.70726
I0521 05:11:37.022727 20178 solver.cpp:253]     Train net output #0: loss = 1.70726 (* 1 = 1.70726 loss)
I0521 05:11:37.022743 20178 sgd_solver.cpp:106] Iteration 1320, lr = 0.0025
I0521 05:11:37.752416 20178 solver.cpp:341] Iteration 1323, Testing net (#0)
I0521 05:12:22.670433 20178 solver.cpp:409]     Test net output #0: accuracy = 0.658697
I0521 05:12:22.670603 20178 solver.cpp:409]     Test net output #1: loss = 1.20264 (* 1 = 1.20264 loss)
I0521 05:12:51.859577 20178 solver.cpp:237] Iteration 1342, loss = 1.75134
I0521 05:12:51.859629 20178 solver.cpp:253]     Train net output #0: loss = 1.75134 (* 1 = 1.75134 loss)
I0521 05:12:51.859643 20178 sgd_solver.cpp:106] Iteration 1342, lr = 0.0025
I0521 05:12:59.892122 20178 solver.cpp:237] Iteration 1364, loss = 1.74423
I0521 05:12:59.892271 20178 solver.cpp:253]     Train net output #0: loss = 1.74423 (* 1 = 1.74423 loss)
I0521 05:12:59.892284 20178 sgd_solver.cpp:106] Iteration 1364, lr = 0.0025
I0521 05:13:07.928416 20178 solver.cpp:237] Iteration 1386, loss = 1.72647
I0521 05:13:07.928453 20178 solver.cpp:253]     Train net output #0: loss = 1.72647 (* 1 = 1.72647 loss)
I0521 05:13:07.928474 20178 sgd_solver.cpp:106] Iteration 1386, lr = 0.0025
I0521 05:13:15.963105 20178 solver.cpp:237] Iteration 1408, loss = 1.952
I0521 05:13:15.963138 20178 solver.cpp:253]     Train net output #0: loss = 1.952 (* 1 = 1.952 loss)
I0521 05:13:15.963155 20178 sgd_solver.cpp:106] Iteration 1408, lr = 0.0025
I0521 05:13:24.000776 20178 solver.cpp:237] Iteration 1430, loss = 1.73201
I0521 05:13:24.000808 20178 solver.cpp:253]     Train net output #0: loss = 1.73201 (* 1 = 1.73201 loss)
I0521 05:13:24.000825 20178 sgd_solver.cpp:106] Iteration 1430, lr = 0.0025
I0521 05:13:32.031947 20178 solver.cpp:237] Iteration 1452, loss = 1.70146
I0521 05:13:32.032106 20178 solver.cpp:253]     Train net output #0: loss = 1.70146 (* 1 = 1.70146 loss)
I0521 05:13:32.032120 20178 sgd_solver.cpp:106] Iteration 1452, lr = 0.0025
I0521 05:14:02.193994 20178 solver.cpp:237] Iteration 1474, loss = 1.66906
I0521 05:14:02.194169 20178 solver.cpp:253]     Train net output #0: loss = 1.66906 (* 1 = 1.66906 loss)
I0521 05:14:02.194183 20178 sgd_solver.cpp:106] Iteration 1474, lr = 0.0025
I0521 05:14:10.227082 20178 solver.cpp:237] Iteration 1496, loss = 1.66973
I0521 05:14:10.227114 20178 solver.cpp:253]     Train net output #0: loss = 1.66973 (* 1 = 1.66973 loss)
I0521 05:14:10.227128 20178 sgd_solver.cpp:106] Iteration 1496, lr = 0.0025
I0521 05:14:18.265835 20178 solver.cpp:237] Iteration 1518, loss = 1.71658
I0521 05:14:18.265880 20178 solver.cpp:253]     Train net output #0: loss = 1.71658 (* 1 = 1.71658 loss)
I0521 05:14:18.265897 20178 sgd_solver.cpp:106] Iteration 1518, lr = 0.0025
I0521 05:14:25.931742 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1540.caffemodel
I0521 05:14:26.226361 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1540.solverstate
I0521 05:14:26.361564 20178 solver.cpp:237] Iteration 1540, loss = 1.68996
I0521 05:14:26.361605 20178 solver.cpp:253]     Train net output #0: loss = 1.68996 (* 1 = 1.68996 loss)
I0521 05:14:26.361623 20178 sgd_solver.cpp:106] Iteration 1540, lr = 0.0025
I0521 05:14:34.392007 20178 solver.cpp:237] Iteration 1562, loss = 1.71631
I0521 05:14:34.392153 20178 solver.cpp:253]     Train net output #0: loss = 1.71631 (* 1 = 1.71631 loss)
I0521 05:14:34.392165 20178 sgd_solver.cpp:106] Iteration 1562, lr = 0.0025
I0521 05:14:42.425709 20178 solver.cpp:237] Iteration 1584, loss = 1.69241
I0521 05:14:42.425752 20178 solver.cpp:253]     Train net output #0: loss = 1.69241 (* 1 = 1.69241 loss)
I0521 05:14:42.425776 20178 sgd_solver.cpp:106] Iteration 1584, lr = 0.0025
I0521 05:14:50.452599 20178 solver.cpp:237] Iteration 1606, loss = 1.67917
I0521 05:14:50.452632 20178 solver.cpp:253]     Train net output #0: loss = 1.67917 (* 1 = 1.67917 loss)
I0521 05:14:50.452647 20178 sgd_solver.cpp:106] Iteration 1606, lr = 0.0025
I0521 05:15:20.660513 20178 solver.cpp:237] Iteration 1628, loss = 1.6417
I0521 05:15:20.660676 20178 solver.cpp:253]     Train net output #0: loss = 1.6417 (* 1 = 1.6417 loss)
I0521 05:15:20.660691 20178 sgd_solver.cpp:106] Iteration 1628, lr = 0.0025
I0521 05:15:28.693883 20178 solver.cpp:237] Iteration 1650, loss = 1.74966
I0521 05:15:28.693917 20178 solver.cpp:253]     Train net output #0: loss = 1.74966 (* 1 = 1.74966 loss)
I0521 05:15:28.693933 20178 sgd_solver.cpp:106] Iteration 1650, lr = 0.0025
I0521 05:15:36.721750 20178 solver.cpp:237] Iteration 1672, loss = 1.68077
I0521 05:15:36.721787 20178 solver.cpp:253]     Train net output #0: loss = 1.68077 (* 1 = 1.68077 loss)
I0521 05:15:36.721808 20178 sgd_solver.cpp:106] Iteration 1672, lr = 0.0025
I0521 05:15:44.754076 20178 solver.cpp:237] Iteration 1694, loss = 1.67146
I0521 05:15:44.754108 20178 solver.cpp:253]     Train net output #0: loss = 1.67146 (* 1 = 1.67146 loss)
I0521 05:15:44.754127 20178 sgd_solver.cpp:106] Iteration 1694, lr = 0.0025
I0521 05:15:52.785899 20178 solver.cpp:237] Iteration 1716, loss = 1.70495
I0521 05:15:52.786038 20178 solver.cpp:253]     Train net output #0: loss = 1.70495 (* 1 = 1.70495 loss)
I0521 05:15:52.786052 20178 sgd_solver.cpp:106] Iteration 1716, lr = 0.0025
I0521 05:16:00.819150 20178 solver.cpp:237] Iteration 1738, loss = 1.70678
I0521 05:16:00.819187 20178 solver.cpp:253]     Train net output #0: loss = 1.70678 (* 1 = 1.70678 loss)
I0521 05:16:00.819211 20178 sgd_solver.cpp:106] Iteration 1738, lr = 0.0025
I0521 05:16:08.488740 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1760.caffemodel
I0521 05:16:08.783017 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1760.solverstate
I0521 05:16:08.917349 20178 solver.cpp:237] Iteration 1760, loss = 1.5712
I0521 05:16:08.917393 20178 solver.cpp:253]     Train net output #0: loss = 1.5712 (* 1 = 1.5712 loss)
I0521 05:16:08.917410 20178 sgd_solver.cpp:106] Iteration 1760, lr = 0.0025
I0521 05:16:10.013229 20178 solver.cpp:341] Iteration 1764, Testing net (#0)
I0521 05:17:16.165704 20178 solver.cpp:409]     Test net output #0: accuracy = 0.671811
I0521 05:17:16.165875 20178 solver.cpp:409]     Test net output #1: loss = 1.12606 (* 1 = 1.12606 loss)
I0521 05:17:45.039894 20178 solver.cpp:237] Iteration 1782, loss = 1.71223
I0521 05:17:45.039940 20178 solver.cpp:253]     Train net output #0: loss = 1.71223 (* 1 = 1.71223 loss)
I0521 05:17:45.039954 20178 sgd_solver.cpp:106] Iteration 1782, lr = 0.0025
I0521 05:17:53.078631 20178 solver.cpp:237] Iteration 1804, loss = 1.64273
I0521 05:17:53.078793 20178 solver.cpp:253]     Train net output #0: loss = 1.64273 (* 1 = 1.64273 loss)
I0521 05:17:53.078807 20178 sgd_solver.cpp:106] Iteration 1804, lr = 0.0025
I0521 05:18:01.117976 20178 solver.cpp:237] Iteration 1826, loss = 1.65419
I0521 05:18:01.118008 20178 solver.cpp:253]     Train net output #0: loss = 1.65419 (* 1 = 1.65419 loss)
I0521 05:18:01.118026 20178 sgd_solver.cpp:106] Iteration 1826, lr = 0.0025
I0521 05:18:09.153807 20178 solver.cpp:237] Iteration 1848, loss = 1.70137
I0521 05:18:09.153847 20178 solver.cpp:253]     Train net output #0: loss = 1.70137 (* 1 = 1.70137 loss)
I0521 05:18:09.153864 20178 sgd_solver.cpp:106] Iteration 1848, lr = 0.0025
I0521 05:18:17.189641 20178 solver.cpp:237] Iteration 1870, loss = 1.6989
I0521 05:18:17.189673 20178 solver.cpp:253]     Train net output #0: loss = 1.6989 (* 1 = 1.6989 loss)
I0521 05:18:17.189692 20178 sgd_solver.cpp:106] Iteration 1870, lr = 0.0025
I0521 05:18:25.225353 20178 solver.cpp:237] Iteration 1892, loss = 1.64545
I0521 05:18:25.225492 20178 solver.cpp:253]     Train net output #0: loss = 1.64545 (* 1 = 1.64545 loss)
I0521 05:18:25.225504 20178 sgd_solver.cpp:106] Iteration 1892, lr = 0.0025
I0521 05:18:55.435709 20178 solver.cpp:237] Iteration 1914, loss = 1.72587
I0521 05:18:55.435878 20178 solver.cpp:253]     Train net output #0: loss = 1.72587 (* 1 = 1.72587 loss)
I0521 05:18:55.435894 20178 sgd_solver.cpp:106] Iteration 1914, lr = 0.0025
I0521 05:19:03.475283 20178 solver.cpp:237] Iteration 1936, loss = 1.66355
I0521 05:19:03.475316 20178 solver.cpp:253]     Train net output #0: loss = 1.66355 (* 1 = 1.66355 loss)
I0521 05:19:03.475333 20178 sgd_solver.cpp:106] Iteration 1936, lr = 0.0025
I0521 05:19:11.509960 20178 solver.cpp:237] Iteration 1958, loss = 1.70903
I0521 05:19:11.509994 20178 solver.cpp:253]     Train net output #0: loss = 1.70903 (* 1 = 1.70903 loss)
I0521 05:19:11.510010 20178 sgd_solver.cpp:106] Iteration 1958, lr = 0.0025
I0521 05:19:19.183228 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1980.caffemodel
I0521 05:19:19.480150 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_1980.solverstate
I0521 05:19:19.616703 20178 solver.cpp:237] Iteration 1980, loss = 1.64723
I0521 05:19:19.616749 20178 solver.cpp:253]     Train net output #0: loss = 1.64723 (* 1 = 1.64723 loss)
I0521 05:19:19.616765 20178 sgd_solver.cpp:106] Iteration 1980, lr = 0.0025
I0521 05:19:27.662439 20178 solver.cpp:237] Iteration 2002, loss = 1.63988
I0521 05:19:27.662583 20178 solver.cpp:253]     Train net output #0: loss = 1.63988 (* 1 = 1.63988 loss)
I0521 05:19:27.662596 20178 sgd_solver.cpp:106] Iteration 2002, lr = 0.0025
I0521 05:19:35.696494 20178 solver.cpp:237] Iteration 2024, loss = 1.67439
I0521 05:19:35.696527 20178 solver.cpp:253]     Train net output #0: loss = 1.67439 (* 1 = 1.67439 loss)
I0521 05:19:35.696543 20178 sgd_solver.cpp:106] Iteration 2024, lr = 0.0025
I0521 05:19:43.737216 20178 solver.cpp:237] Iteration 2046, loss = 1.61823
I0521 05:19:43.737249 20178 solver.cpp:253]     Train net output #0: loss = 1.61823 (* 1 = 1.61823 loss)
I0521 05:19:43.737272 20178 sgd_solver.cpp:106] Iteration 2046, lr = 0.0025
I0521 05:20:13.970023 20178 solver.cpp:237] Iteration 2068, loss = 1.68291
I0521 05:20:13.970202 20178 solver.cpp:253]     Train net output #0: loss = 1.68291 (* 1 = 1.68291 loss)
I0521 05:20:13.970217 20178 sgd_solver.cpp:106] Iteration 2068, lr = 0.0025
I0521 05:20:22.009172 20178 solver.cpp:237] Iteration 2090, loss = 1.5841
I0521 05:20:22.009204 20178 solver.cpp:253]     Train net output #0: loss = 1.5841 (* 1 = 1.5841 loss)
I0521 05:20:22.009222 20178 sgd_solver.cpp:106] Iteration 2090, lr = 0.0025
I0521 05:20:30.046205 20178 solver.cpp:237] Iteration 2112, loss = 1.56716
I0521 05:20:30.046237 20178 solver.cpp:253]     Train net output #0: loss = 1.56716 (* 1 = 1.56716 loss)
I0521 05:20:30.046255 20178 sgd_solver.cpp:106] Iteration 2112, lr = 0.0025
I0521 05:20:38.085253 20178 solver.cpp:237] Iteration 2134, loss = 1.67585
I0521 05:20:38.085294 20178 solver.cpp:253]     Train net output #0: loss = 1.67585 (* 1 = 1.67585 loss)
I0521 05:20:38.085314 20178 sgd_solver.cpp:106] Iteration 2134, lr = 0.0025
I0521 05:20:46.123097 20178 solver.cpp:237] Iteration 2156, loss = 1.72319
I0521 05:20:46.123242 20178 solver.cpp:253]     Train net output #0: loss = 1.72319 (* 1 = 1.72319 loss)
I0521 05:20:46.123255 20178 sgd_solver.cpp:106] Iteration 2156, lr = 0.0025
I0521 05:20:54.155910 20178 solver.cpp:237] Iteration 2178, loss = 1.63774
I0521 05:20:54.155944 20178 solver.cpp:253]     Train net output #0: loss = 1.63774 (* 1 = 1.63774 loss)
I0521 05:20:54.155961 20178 sgd_solver.cpp:106] Iteration 2178, lr = 0.0025
I0521 05:21:01.832250 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_2200.caffemodel
I0521 05:21:02.128742 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_2200.solverstate
I0521 05:21:02.265506 20178 solver.cpp:237] Iteration 2200, loss = 1.66846
I0521 05:21:02.265558 20178 solver.cpp:253]     Train net output #0: loss = 1.66846 (* 1 = 1.66846 loss)
I0521 05:21:02.265573 20178 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0521 05:21:03.726250 20178 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_2205.caffemodel
I0521 05:21:04.021915 20178 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635_iter_2205.solverstate
I0521 05:21:04.049841 20178 solver.cpp:341] Iteration 2205, Testing net (#0)
I0521 05:21:49.026386 20178 solver.cpp:409]     Test net output #0: accuracy = 0.680669
I0521 05:21:49.026561 20178 solver.cpp:409]     Test net output #1: loss = 1.11169 (* 1 = 1.11169 loss)
I0521 05:21:49.026576 20178 solver.cpp:326] Optimization Done.
I0521 05:21:49.026587 20178 caffe.cpp:215] Optimization Done.
Application 11236831 resources: utime ~1249s, stime ~226s, Rss ~5329384, inblocks ~3594475, outblocks ~194561
	Command being timed: "aprun -n 1 -N 1 ./build/tools/caffe train --solver=/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_batch_size_test/batch_size_680_2016-05-20T11.20.57.344635.solver"
	User time (seconds): 0.56
	System time (seconds): 0.14
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 24:37.88
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 8660
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 15079
	Voluntary context switches: 2726
	Involuntary context switches: 82
	Swaps: 0
	File system inputs: 0
	File system outputs: 0
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
