2808300
I0523 09:56:14.096194  3765 caffe.cpp:184] Using GPUs 0
I0523 09:56:14.526754  3765 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3000
test_interval: 6000
base_lr: 0.0045
display: 300
max_iter: 300000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 3000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792.prototxt"
I0523 09:56:14.528558  3765 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792.prototxt
I0523 09:56:14.545766  3765 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0523 09:56:14.545825  3765 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 09:56:14.546175  3765 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0523 09:56:14.546356  3765 layer_factory.hpp:77] Creating layer data_hdf5
I0523 09:56:14.546380  3765 net.cpp:106] Creating Layer data_hdf5
I0523 09:56:14.546394  3765 net.cpp:411] data_hdf5 -> data
I0523 09:56:14.546427  3765 net.cpp:411] data_hdf5 -> label
I0523 09:56:14.546459  3765 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0523 09:56:14.559372  3765 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0523 09:56:14.561740  3765 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0523 09:56:36.139578  3765 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0523 09:56:36.144732  3765 net.cpp:150] Setting up data_hdf5
I0523 09:56:36.144773  3765 net.cpp:157] Top shape: 50 1 127 50 (317500)
I0523 09:56:36.144788  3765 net.cpp:157] Top shape: 50 (50)
I0523 09:56:36.144800  3765 net.cpp:165] Memory required for data: 1270200
I0523 09:56:36.144814  3765 layer_factory.hpp:77] Creating layer conv1
I0523 09:56:36.144846  3765 net.cpp:106] Creating Layer conv1
I0523 09:56:36.144858  3765 net.cpp:454] conv1 <- data
I0523 09:56:36.144878  3765 net.cpp:411] conv1 -> conv1
I0523 09:56:37.531289  3765 net.cpp:150] Setting up conv1
I0523 09:56:37.531337  3765 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 09:56:37.531347  3765 net.cpp:165] Memory required for data: 15094200
I0523 09:56:37.531378  3765 layer_factory.hpp:77] Creating layer relu1
I0523 09:56:37.531399  3765 net.cpp:106] Creating Layer relu1
I0523 09:56:37.531411  3765 net.cpp:454] relu1 <- conv1
I0523 09:56:37.531435  3765 net.cpp:397] relu1 -> conv1 (in-place)
I0523 09:56:37.531954  3765 net.cpp:150] Setting up relu1
I0523 09:56:37.531970  3765 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 09:56:37.531980  3765 net.cpp:165] Memory required for data: 28918200
I0523 09:56:37.531991  3765 layer_factory.hpp:77] Creating layer pool1
I0523 09:56:37.532007  3765 net.cpp:106] Creating Layer pool1
I0523 09:56:37.532017  3765 net.cpp:454] pool1 <- conv1
I0523 09:56:37.532030  3765 net.cpp:411] pool1 -> pool1
I0523 09:56:37.532110  3765 net.cpp:150] Setting up pool1
I0523 09:56:37.532124  3765 net.cpp:157] Top shape: 50 12 60 48 (1728000)
I0523 09:56:37.532135  3765 net.cpp:165] Memory required for data: 35830200
I0523 09:56:37.532145  3765 layer_factory.hpp:77] Creating layer conv2
I0523 09:56:37.532167  3765 net.cpp:106] Creating Layer conv2
I0523 09:56:37.532178  3765 net.cpp:454] conv2 <- pool1
I0523 09:56:37.532191  3765 net.cpp:411] conv2 -> conv2
I0523 09:56:37.534857  3765 net.cpp:150] Setting up conv2
I0523 09:56:37.534886  3765 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 09:56:37.534896  3765 net.cpp:165] Memory required for data: 45766200
I0523 09:56:37.534915  3765 layer_factory.hpp:77] Creating layer relu2
I0523 09:56:37.534930  3765 net.cpp:106] Creating Layer relu2
I0523 09:56:37.534940  3765 net.cpp:454] relu2 <- conv2
I0523 09:56:37.534952  3765 net.cpp:397] relu2 -> conv2 (in-place)
I0523 09:56:37.535284  3765 net.cpp:150] Setting up relu2
I0523 09:56:37.535297  3765 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 09:56:37.535307  3765 net.cpp:165] Memory required for data: 55702200
I0523 09:56:37.535317  3765 layer_factory.hpp:77] Creating layer pool2
I0523 09:56:37.535331  3765 net.cpp:106] Creating Layer pool2
I0523 09:56:37.535341  3765 net.cpp:454] pool2 <- conv2
I0523 09:56:37.535352  3765 net.cpp:411] pool2 -> pool2
I0523 09:56:37.535445  3765 net.cpp:150] Setting up pool2
I0523 09:56:37.535459  3765 net.cpp:157] Top shape: 50 20 27 46 (1242000)
I0523 09:56:37.535468  3765 net.cpp:165] Memory required for data: 60670200
I0523 09:56:37.535480  3765 layer_factory.hpp:77] Creating layer conv3
I0523 09:56:37.535497  3765 net.cpp:106] Creating Layer conv3
I0523 09:56:37.535508  3765 net.cpp:454] conv3 <- pool2
I0523 09:56:37.535522  3765 net.cpp:411] conv3 -> conv3
I0523 09:56:37.537457  3765 net.cpp:150] Setting up conv3
I0523 09:56:37.537477  3765 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 09:56:37.537487  3765 net.cpp:165] Memory required for data: 66091000
I0523 09:56:37.537505  3765 layer_factory.hpp:77] Creating layer relu3
I0523 09:56:37.537523  3765 net.cpp:106] Creating Layer relu3
I0523 09:56:37.537531  3765 net.cpp:454] relu3 <- conv3
I0523 09:56:37.537544  3765 net.cpp:397] relu3 -> conv3 (in-place)
I0523 09:56:37.538012  3765 net.cpp:150] Setting up relu3
I0523 09:56:37.538028  3765 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 09:56:37.538038  3765 net.cpp:165] Memory required for data: 71511800
I0523 09:56:37.538048  3765 layer_factory.hpp:77] Creating layer pool3
I0523 09:56:37.538061  3765 net.cpp:106] Creating Layer pool3
I0523 09:56:37.538071  3765 net.cpp:454] pool3 <- conv3
I0523 09:56:37.538084  3765 net.cpp:411] pool3 -> pool3
I0523 09:56:37.538151  3765 net.cpp:150] Setting up pool3
I0523 09:56:37.538164  3765 net.cpp:157] Top shape: 50 28 11 44 (677600)
I0523 09:56:37.538174  3765 net.cpp:165] Memory required for data: 74222200
I0523 09:56:37.538182  3765 layer_factory.hpp:77] Creating layer conv4
I0523 09:56:37.538199  3765 net.cpp:106] Creating Layer conv4
I0523 09:56:37.538210  3765 net.cpp:454] conv4 <- pool3
I0523 09:56:37.538223  3765 net.cpp:411] conv4 -> conv4
I0523 09:56:37.541004  3765 net.cpp:150] Setting up conv4
I0523 09:56:37.541033  3765 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 09:56:37.541044  3765 net.cpp:165] Memory required for data: 76036600
I0523 09:56:37.541060  3765 layer_factory.hpp:77] Creating layer relu4
I0523 09:56:37.541074  3765 net.cpp:106] Creating Layer relu4
I0523 09:56:37.541085  3765 net.cpp:454] relu4 <- conv4
I0523 09:56:37.541097  3765 net.cpp:397] relu4 -> conv4 (in-place)
I0523 09:56:37.541566  3765 net.cpp:150] Setting up relu4
I0523 09:56:37.541582  3765 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 09:56:37.541592  3765 net.cpp:165] Memory required for data: 77851000
I0523 09:56:37.541604  3765 layer_factory.hpp:77] Creating layer pool4
I0523 09:56:37.541616  3765 net.cpp:106] Creating Layer pool4
I0523 09:56:37.541626  3765 net.cpp:454] pool4 <- conv4
I0523 09:56:37.541640  3765 net.cpp:411] pool4 -> pool4
I0523 09:56:37.541707  3765 net.cpp:150] Setting up pool4
I0523 09:56:37.541721  3765 net.cpp:157] Top shape: 50 36 3 42 (226800)
I0523 09:56:37.541731  3765 net.cpp:165] Memory required for data: 78758200
I0523 09:56:37.541741  3765 layer_factory.hpp:77] Creating layer ip1
I0523 09:56:37.541762  3765 net.cpp:106] Creating Layer ip1
I0523 09:56:37.541772  3765 net.cpp:454] ip1 <- pool4
I0523 09:56:37.541786  3765 net.cpp:411] ip1 -> ip1
I0523 09:56:37.557243  3765 net.cpp:150] Setting up ip1
I0523 09:56:37.557272  3765 net.cpp:157] Top shape: 50 196 (9800)
I0523 09:56:37.557286  3765 net.cpp:165] Memory required for data: 78797400
I0523 09:56:37.557312  3765 layer_factory.hpp:77] Creating layer relu5
I0523 09:56:37.557327  3765 net.cpp:106] Creating Layer relu5
I0523 09:56:37.557337  3765 net.cpp:454] relu5 <- ip1
I0523 09:56:37.557353  3765 net.cpp:397] relu5 -> ip1 (in-place)
I0523 09:56:37.557698  3765 net.cpp:150] Setting up relu5
I0523 09:56:37.557713  3765 net.cpp:157] Top shape: 50 196 (9800)
I0523 09:56:37.557723  3765 net.cpp:165] Memory required for data: 78836600
I0523 09:56:37.557734  3765 layer_factory.hpp:77] Creating layer drop1
I0523 09:56:37.557754  3765 net.cpp:106] Creating Layer drop1
I0523 09:56:37.557765  3765 net.cpp:454] drop1 <- ip1
I0523 09:56:37.557777  3765 net.cpp:397] drop1 -> ip1 (in-place)
I0523 09:56:37.557837  3765 net.cpp:150] Setting up drop1
I0523 09:56:37.557850  3765 net.cpp:157] Top shape: 50 196 (9800)
I0523 09:56:37.557860  3765 net.cpp:165] Memory required for data: 78875800
I0523 09:56:37.557870  3765 layer_factory.hpp:77] Creating layer ip2
I0523 09:56:37.557888  3765 net.cpp:106] Creating Layer ip2
I0523 09:56:37.557899  3765 net.cpp:454] ip2 <- ip1
I0523 09:56:37.557912  3765 net.cpp:411] ip2 -> ip2
I0523 09:56:37.558374  3765 net.cpp:150] Setting up ip2
I0523 09:56:37.558388  3765 net.cpp:157] Top shape: 50 98 (4900)
I0523 09:56:37.558398  3765 net.cpp:165] Memory required for data: 78895400
I0523 09:56:37.558413  3765 layer_factory.hpp:77] Creating layer relu6
I0523 09:56:37.558425  3765 net.cpp:106] Creating Layer relu6
I0523 09:56:37.558434  3765 net.cpp:454] relu6 <- ip2
I0523 09:56:37.558447  3765 net.cpp:397] relu6 -> ip2 (in-place)
I0523 09:56:37.558964  3765 net.cpp:150] Setting up relu6
I0523 09:56:37.558981  3765 net.cpp:157] Top shape: 50 98 (4900)
I0523 09:56:37.558991  3765 net.cpp:165] Memory required for data: 78915000
I0523 09:56:37.559001  3765 layer_factory.hpp:77] Creating layer drop2
I0523 09:56:37.559015  3765 net.cpp:106] Creating Layer drop2
I0523 09:56:37.559025  3765 net.cpp:454] drop2 <- ip2
I0523 09:56:37.559037  3765 net.cpp:397] drop2 -> ip2 (in-place)
I0523 09:56:37.559079  3765 net.cpp:150] Setting up drop2
I0523 09:56:37.559092  3765 net.cpp:157] Top shape: 50 98 (4900)
I0523 09:56:37.559103  3765 net.cpp:165] Memory required for data: 78934600
I0523 09:56:37.559113  3765 layer_factory.hpp:77] Creating layer ip3
I0523 09:56:37.559126  3765 net.cpp:106] Creating Layer ip3
I0523 09:56:37.559136  3765 net.cpp:454] ip3 <- ip2
I0523 09:56:37.559149  3765 net.cpp:411] ip3 -> ip3
I0523 09:56:37.559360  3765 net.cpp:150] Setting up ip3
I0523 09:56:37.559372  3765 net.cpp:157] Top shape: 50 11 (550)
I0523 09:56:37.559381  3765 net.cpp:165] Memory required for data: 78936800
I0523 09:56:37.559396  3765 layer_factory.hpp:77] Creating layer drop3
I0523 09:56:37.559409  3765 net.cpp:106] Creating Layer drop3
I0523 09:56:37.559419  3765 net.cpp:454] drop3 <- ip3
I0523 09:56:37.559439  3765 net.cpp:397] drop3 -> ip3 (in-place)
I0523 09:56:37.559478  3765 net.cpp:150] Setting up drop3
I0523 09:56:37.559491  3765 net.cpp:157] Top shape: 50 11 (550)
I0523 09:56:37.559501  3765 net.cpp:165] Memory required for data: 78939000
I0523 09:56:37.559511  3765 layer_factory.hpp:77] Creating layer loss
I0523 09:56:37.559530  3765 net.cpp:106] Creating Layer loss
I0523 09:56:37.559540  3765 net.cpp:454] loss <- ip3
I0523 09:56:37.559551  3765 net.cpp:454] loss <- label
I0523 09:56:37.559563  3765 net.cpp:411] loss -> loss
I0523 09:56:37.559581  3765 layer_factory.hpp:77] Creating layer loss
I0523 09:56:37.560219  3765 net.cpp:150] Setting up loss
I0523 09:56:37.560240  3765 net.cpp:157] Top shape: (1)
I0523 09:56:37.560252  3765 net.cpp:160]     with loss weight 1
I0523 09:56:37.560297  3765 net.cpp:165] Memory required for data: 78939004
I0523 09:56:37.560308  3765 net.cpp:226] loss needs backward computation.
I0523 09:56:37.560320  3765 net.cpp:226] drop3 needs backward computation.
I0523 09:56:37.560328  3765 net.cpp:226] ip3 needs backward computation.
I0523 09:56:37.560338  3765 net.cpp:226] drop2 needs backward computation.
I0523 09:56:37.560348  3765 net.cpp:226] relu6 needs backward computation.
I0523 09:56:37.560359  3765 net.cpp:226] ip2 needs backward computation.
I0523 09:56:37.560369  3765 net.cpp:226] drop1 needs backward computation.
I0523 09:56:37.560379  3765 net.cpp:226] relu5 needs backward computation.
I0523 09:56:37.560387  3765 net.cpp:226] ip1 needs backward computation.
I0523 09:56:37.560397  3765 net.cpp:226] pool4 needs backward computation.
I0523 09:56:37.560408  3765 net.cpp:226] relu4 needs backward computation.
I0523 09:56:37.560418  3765 net.cpp:226] conv4 needs backward computation.
I0523 09:56:37.560428  3765 net.cpp:226] pool3 needs backward computation.
I0523 09:56:37.560438  3765 net.cpp:226] relu3 needs backward computation.
I0523 09:56:37.560448  3765 net.cpp:226] conv3 needs backward computation.
I0523 09:56:37.560468  3765 net.cpp:226] pool2 needs backward computation.
I0523 09:56:37.560479  3765 net.cpp:226] relu2 needs backward computation.
I0523 09:56:37.560489  3765 net.cpp:226] conv2 needs backward computation.
I0523 09:56:37.560499  3765 net.cpp:226] pool1 needs backward computation.
I0523 09:56:37.560510  3765 net.cpp:226] relu1 needs backward computation.
I0523 09:56:37.560520  3765 net.cpp:226] conv1 needs backward computation.
I0523 09:56:37.560531  3765 net.cpp:228] data_hdf5 does not need backward computation.
I0523 09:56:37.560541  3765 net.cpp:270] This network produces output loss
I0523 09:56:37.560565  3765 net.cpp:283] Network initialization done.
I0523 09:56:37.562268  3765 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792.prototxt
I0523 09:56:37.562340  3765 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0523 09:56:37.562698  3765 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0523 09:56:37.562887  3765 layer_factory.hpp:77] Creating layer data_hdf5
I0523 09:56:37.562902  3765 net.cpp:106] Creating Layer data_hdf5
I0523 09:56:37.562914  3765 net.cpp:411] data_hdf5 -> data
I0523 09:56:37.562932  3765 net.cpp:411] data_hdf5 -> label
I0523 09:56:37.562947  3765 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0523 09:56:37.564270  3765 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0523 09:56:58.938534  3765 net.cpp:150] Setting up data_hdf5
I0523 09:56:58.938701  3765 net.cpp:157] Top shape: 50 1 127 50 (317500)
I0523 09:56:58.938715  3765 net.cpp:157] Top shape: 50 (50)
I0523 09:56:58.938727  3765 net.cpp:165] Memory required for data: 1270200
I0523 09:56:58.938741  3765 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0523 09:56:58.938771  3765 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0523 09:56:58.938781  3765 net.cpp:454] label_data_hdf5_1_split <- label
I0523 09:56:58.938796  3765 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0523 09:56:58.938817  3765 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0523 09:56:58.938889  3765 net.cpp:150] Setting up label_data_hdf5_1_split
I0523 09:56:58.938904  3765 net.cpp:157] Top shape: 50 (50)
I0523 09:56:58.938915  3765 net.cpp:157] Top shape: 50 (50)
I0523 09:56:58.938925  3765 net.cpp:165] Memory required for data: 1270600
I0523 09:56:58.938935  3765 layer_factory.hpp:77] Creating layer conv1
I0523 09:56:58.938957  3765 net.cpp:106] Creating Layer conv1
I0523 09:56:58.938967  3765 net.cpp:454] conv1 <- data
I0523 09:56:58.938982  3765 net.cpp:411] conv1 -> conv1
I0523 09:56:58.940930  3765 net.cpp:150] Setting up conv1
I0523 09:56:58.940955  3765 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 09:56:58.940965  3765 net.cpp:165] Memory required for data: 15094600
I0523 09:56:58.940987  3765 layer_factory.hpp:77] Creating layer relu1
I0523 09:56:58.941002  3765 net.cpp:106] Creating Layer relu1
I0523 09:56:58.941012  3765 net.cpp:454] relu1 <- conv1
I0523 09:56:58.941025  3765 net.cpp:397] relu1 -> conv1 (in-place)
I0523 09:56:58.941521  3765 net.cpp:150] Setting up relu1
I0523 09:56:58.941537  3765 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 09:56:58.941547  3765 net.cpp:165] Memory required for data: 28918600
I0523 09:56:58.941557  3765 layer_factory.hpp:77] Creating layer pool1
I0523 09:56:58.941575  3765 net.cpp:106] Creating Layer pool1
I0523 09:56:58.941584  3765 net.cpp:454] pool1 <- conv1
I0523 09:56:58.941597  3765 net.cpp:411] pool1 -> pool1
I0523 09:56:58.941673  3765 net.cpp:150] Setting up pool1
I0523 09:56:58.941685  3765 net.cpp:157] Top shape: 50 12 60 48 (1728000)
I0523 09:56:58.941695  3765 net.cpp:165] Memory required for data: 35830600
I0523 09:56:58.941705  3765 layer_factory.hpp:77] Creating layer conv2
I0523 09:56:58.941723  3765 net.cpp:106] Creating Layer conv2
I0523 09:56:58.941733  3765 net.cpp:454] conv2 <- pool1
I0523 09:56:58.941748  3765 net.cpp:411] conv2 -> conv2
I0523 09:56:58.943668  3765 net.cpp:150] Setting up conv2
I0523 09:56:58.943691  3765 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 09:56:58.943704  3765 net.cpp:165] Memory required for data: 45766600
I0523 09:56:58.943722  3765 layer_factory.hpp:77] Creating layer relu2
I0523 09:56:58.943734  3765 net.cpp:106] Creating Layer relu2
I0523 09:56:58.943744  3765 net.cpp:454] relu2 <- conv2
I0523 09:56:58.943758  3765 net.cpp:397] relu2 -> conv2 (in-place)
I0523 09:56:58.944087  3765 net.cpp:150] Setting up relu2
I0523 09:56:58.944100  3765 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 09:56:58.944110  3765 net.cpp:165] Memory required for data: 55702600
I0523 09:56:58.944120  3765 layer_factory.hpp:77] Creating layer pool2
I0523 09:56:58.944134  3765 net.cpp:106] Creating Layer pool2
I0523 09:56:58.944144  3765 net.cpp:454] pool2 <- conv2
I0523 09:56:58.944156  3765 net.cpp:411] pool2 -> pool2
I0523 09:56:58.944227  3765 net.cpp:150] Setting up pool2
I0523 09:56:58.944241  3765 net.cpp:157] Top shape: 50 20 27 46 (1242000)
I0523 09:56:58.944250  3765 net.cpp:165] Memory required for data: 60670600
I0523 09:56:58.944260  3765 layer_factory.hpp:77] Creating layer conv3
I0523 09:56:58.944278  3765 net.cpp:106] Creating Layer conv3
I0523 09:56:58.944288  3765 net.cpp:454] conv3 <- pool2
I0523 09:56:58.944301  3765 net.cpp:411] conv3 -> conv3
I0523 09:56:58.946269  3765 net.cpp:150] Setting up conv3
I0523 09:56:58.946292  3765 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 09:56:58.946303  3765 net.cpp:165] Memory required for data: 66091400
I0523 09:56:58.946336  3765 layer_factory.hpp:77] Creating layer relu3
I0523 09:56:58.946351  3765 net.cpp:106] Creating Layer relu3
I0523 09:56:58.946360  3765 net.cpp:454] relu3 <- conv3
I0523 09:56:58.946372  3765 net.cpp:397] relu3 -> conv3 (in-place)
I0523 09:56:58.946842  3765 net.cpp:150] Setting up relu3
I0523 09:56:58.946858  3765 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 09:56:58.946868  3765 net.cpp:165] Memory required for data: 71512200
I0523 09:56:58.946878  3765 layer_factory.hpp:77] Creating layer pool3
I0523 09:56:58.946892  3765 net.cpp:106] Creating Layer pool3
I0523 09:56:58.946902  3765 net.cpp:454] pool3 <- conv3
I0523 09:56:58.946914  3765 net.cpp:411] pool3 -> pool3
I0523 09:56:58.946985  3765 net.cpp:150] Setting up pool3
I0523 09:56:58.947000  3765 net.cpp:157] Top shape: 50 28 11 44 (677600)
I0523 09:56:58.947008  3765 net.cpp:165] Memory required for data: 74222600
I0523 09:56:58.947018  3765 layer_factory.hpp:77] Creating layer conv4
I0523 09:56:58.947034  3765 net.cpp:106] Creating Layer conv4
I0523 09:56:58.947044  3765 net.cpp:454] conv4 <- pool3
I0523 09:56:58.947058  3765 net.cpp:411] conv4 -> conv4
I0523 09:56:58.949126  3765 net.cpp:150] Setting up conv4
I0523 09:56:58.949149  3765 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 09:56:58.949159  3765 net.cpp:165] Memory required for data: 76037000
I0523 09:56:58.949175  3765 layer_factory.hpp:77] Creating layer relu4
I0523 09:56:58.949189  3765 net.cpp:106] Creating Layer relu4
I0523 09:56:58.949199  3765 net.cpp:454] relu4 <- conv4
I0523 09:56:58.949213  3765 net.cpp:397] relu4 -> conv4 (in-place)
I0523 09:56:58.949687  3765 net.cpp:150] Setting up relu4
I0523 09:56:58.949702  3765 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 09:56:58.949712  3765 net.cpp:165] Memory required for data: 77851400
I0523 09:56:58.949723  3765 layer_factory.hpp:77] Creating layer pool4
I0523 09:56:58.949736  3765 net.cpp:106] Creating Layer pool4
I0523 09:56:58.949746  3765 net.cpp:454] pool4 <- conv4
I0523 09:56:58.949759  3765 net.cpp:411] pool4 -> pool4
I0523 09:56:58.949831  3765 net.cpp:150] Setting up pool4
I0523 09:56:58.949843  3765 net.cpp:157] Top shape: 50 36 3 42 (226800)
I0523 09:56:58.949853  3765 net.cpp:165] Memory required for data: 78758600
I0523 09:56:58.949862  3765 layer_factory.hpp:77] Creating layer ip1
I0523 09:56:58.949877  3765 net.cpp:106] Creating Layer ip1
I0523 09:56:58.949887  3765 net.cpp:454] ip1 <- pool4
I0523 09:56:58.949900  3765 net.cpp:411] ip1 -> ip1
I0523 09:56:58.965440  3765 net.cpp:150] Setting up ip1
I0523 09:56:58.965468  3765 net.cpp:157] Top shape: 50 196 (9800)
I0523 09:56:58.965481  3765 net.cpp:165] Memory required for data: 78797800
I0523 09:56:58.965503  3765 layer_factory.hpp:77] Creating layer relu5
I0523 09:56:58.965518  3765 net.cpp:106] Creating Layer relu5
I0523 09:56:58.965529  3765 net.cpp:454] relu5 <- ip1
I0523 09:56:58.965543  3765 net.cpp:397] relu5 -> ip1 (in-place)
I0523 09:56:58.965890  3765 net.cpp:150] Setting up relu5
I0523 09:56:58.965904  3765 net.cpp:157] Top shape: 50 196 (9800)
I0523 09:56:58.965914  3765 net.cpp:165] Memory required for data: 78837000
I0523 09:56:58.965925  3765 layer_factory.hpp:77] Creating layer drop1
I0523 09:56:58.965944  3765 net.cpp:106] Creating Layer drop1
I0523 09:56:58.965953  3765 net.cpp:454] drop1 <- ip1
I0523 09:56:58.965966  3765 net.cpp:397] drop1 -> ip1 (in-place)
I0523 09:56:58.966012  3765 net.cpp:150] Setting up drop1
I0523 09:56:58.966024  3765 net.cpp:157] Top shape: 50 196 (9800)
I0523 09:56:58.966034  3765 net.cpp:165] Memory required for data: 78876200
I0523 09:56:58.966043  3765 layer_factory.hpp:77] Creating layer ip2
I0523 09:56:58.966058  3765 net.cpp:106] Creating Layer ip2
I0523 09:56:58.966068  3765 net.cpp:454] ip2 <- ip1
I0523 09:56:58.966081  3765 net.cpp:411] ip2 -> ip2
I0523 09:56:58.966560  3765 net.cpp:150] Setting up ip2
I0523 09:56:58.966573  3765 net.cpp:157] Top shape: 50 98 (4900)
I0523 09:56:58.966583  3765 net.cpp:165] Memory required for data: 78895800
I0523 09:56:58.966598  3765 layer_factory.hpp:77] Creating layer relu6
I0523 09:56:58.966624  3765 net.cpp:106] Creating Layer relu6
I0523 09:56:58.966634  3765 net.cpp:454] relu6 <- ip2
I0523 09:56:58.966647  3765 net.cpp:397] relu6 -> ip2 (in-place)
I0523 09:56:58.967183  3765 net.cpp:150] Setting up relu6
I0523 09:56:58.967206  3765 net.cpp:157] Top shape: 50 98 (4900)
I0523 09:56:58.967216  3765 net.cpp:165] Memory required for data: 78915400
I0523 09:56:58.967224  3765 layer_factory.hpp:77] Creating layer drop2
I0523 09:56:58.967238  3765 net.cpp:106] Creating Layer drop2
I0523 09:56:58.967248  3765 net.cpp:454] drop2 <- ip2
I0523 09:56:58.967262  3765 net.cpp:397] drop2 -> ip2 (in-place)
I0523 09:56:58.967306  3765 net.cpp:150] Setting up drop2
I0523 09:56:58.967319  3765 net.cpp:157] Top shape: 50 98 (4900)
I0523 09:56:58.967329  3765 net.cpp:165] Memory required for data: 78935000
I0523 09:56:58.967337  3765 layer_factory.hpp:77] Creating layer ip3
I0523 09:56:58.967352  3765 net.cpp:106] Creating Layer ip3
I0523 09:56:58.967362  3765 net.cpp:454] ip3 <- ip2
I0523 09:56:58.967375  3765 net.cpp:411] ip3 -> ip3
I0523 09:56:58.967607  3765 net.cpp:150] Setting up ip3
I0523 09:56:58.967620  3765 net.cpp:157] Top shape: 50 11 (550)
I0523 09:56:58.967629  3765 net.cpp:165] Memory required for data: 78937200
I0523 09:56:58.967644  3765 layer_factory.hpp:77] Creating layer drop3
I0523 09:56:58.967658  3765 net.cpp:106] Creating Layer drop3
I0523 09:56:58.967667  3765 net.cpp:454] drop3 <- ip3
I0523 09:56:58.967680  3765 net.cpp:397] drop3 -> ip3 (in-place)
I0523 09:56:58.967721  3765 net.cpp:150] Setting up drop3
I0523 09:56:58.967735  3765 net.cpp:157] Top shape: 50 11 (550)
I0523 09:56:58.967744  3765 net.cpp:165] Memory required for data: 78939400
I0523 09:56:58.967753  3765 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0523 09:56:58.967767  3765 net.cpp:106] Creating Layer ip3_drop3_0_split
I0523 09:56:58.967777  3765 net.cpp:454] ip3_drop3_0_split <- ip3
I0523 09:56:58.967789  3765 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0523 09:56:58.967804  3765 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0523 09:56:58.967878  3765 net.cpp:150] Setting up ip3_drop3_0_split
I0523 09:56:58.967891  3765 net.cpp:157] Top shape: 50 11 (550)
I0523 09:56:58.967903  3765 net.cpp:157] Top shape: 50 11 (550)
I0523 09:56:58.967913  3765 net.cpp:165] Memory required for data: 78943800
I0523 09:56:58.967922  3765 layer_factory.hpp:77] Creating layer accuracy
I0523 09:56:58.967944  3765 net.cpp:106] Creating Layer accuracy
I0523 09:56:58.967954  3765 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0523 09:56:58.967965  3765 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0523 09:56:58.967979  3765 net.cpp:411] accuracy -> accuracy
I0523 09:56:58.968003  3765 net.cpp:150] Setting up accuracy
I0523 09:56:58.968015  3765 net.cpp:157] Top shape: (1)
I0523 09:56:58.968025  3765 net.cpp:165] Memory required for data: 78943804
I0523 09:56:58.968034  3765 layer_factory.hpp:77] Creating layer loss
I0523 09:56:58.968049  3765 net.cpp:106] Creating Layer loss
I0523 09:56:58.968058  3765 net.cpp:454] loss <- ip3_drop3_0_split_1
I0523 09:56:58.968070  3765 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0523 09:56:58.968082  3765 net.cpp:411] loss -> loss
I0523 09:56:58.968101  3765 layer_factory.hpp:77] Creating layer loss
I0523 09:56:58.968587  3765 net.cpp:150] Setting up loss
I0523 09:56:58.968600  3765 net.cpp:157] Top shape: (1)
I0523 09:56:58.968611  3765 net.cpp:160]     with loss weight 1
I0523 09:56:58.968631  3765 net.cpp:165] Memory required for data: 78943808
I0523 09:56:58.968641  3765 net.cpp:226] loss needs backward computation.
I0523 09:56:58.968652  3765 net.cpp:228] accuracy does not need backward computation.
I0523 09:56:58.968664  3765 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0523 09:56:58.968674  3765 net.cpp:226] drop3 needs backward computation.
I0523 09:56:58.968684  3765 net.cpp:226] ip3 needs backward computation.
I0523 09:56:58.968695  3765 net.cpp:226] drop2 needs backward computation.
I0523 09:56:58.968704  3765 net.cpp:226] relu6 needs backward computation.
I0523 09:56:58.968722  3765 net.cpp:226] ip2 needs backward computation.
I0523 09:56:58.968734  3765 net.cpp:226] drop1 needs backward computation.
I0523 09:56:58.968742  3765 net.cpp:226] relu5 needs backward computation.
I0523 09:56:58.968752  3765 net.cpp:226] ip1 needs backward computation.
I0523 09:56:58.968762  3765 net.cpp:226] pool4 needs backward computation.
I0523 09:56:58.968772  3765 net.cpp:226] relu4 needs backward computation.
I0523 09:56:58.968782  3765 net.cpp:226] conv4 needs backward computation.
I0523 09:56:58.968791  3765 net.cpp:226] pool3 needs backward computation.
I0523 09:56:58.968801  3765 net.cpp:226] relu3 needs backward computation.
I0523 09:56:58.968811  3765 net.cpp:226] conv3 needs backward computation.
I0523 09:56:58.968822  3765 net.cpp:226] pool2 needs backward computation.
I0523 09:56:58.968832  3765 net.cpp:226] relu2 needs backward computation.
I0523 09:56:58.968842  3765 net.cpp:226] conv2 needs backward computation.
I0523 09:56:58.968853  3765 net.cpp:226] pool1 needs backward computation.
I0523 09:56:58.968863  3765 net.cpp:226] relu1 needs backward computation.
I0523 09:56:58.968873  3765 net.cpp:226] conv1 needs backward computation.
I0523 09:56:58.968884  3765 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0523 09:56:58.968896  3765 net.cpp:228] data_hdf5 does not need backward computation.
I0523 09:56:58.968905  3765 net.cpp:270] This network produces output accuracy
I0523 09:56:58.968914  3765 net.cpp:270] This network produces output loss
I0523 09:56:58.968942  3765 net.cpp:283] Network initialization done.
I0523 09:56:58.969075  3765 solver.cpp:60] Solver scaffolding done.
I0523 09:56:58.970218  3765 caffe.cpp:212] Starting Optimization
I0523 09:56:58.970237  3765 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0523 09:56:58.970250  3765 solver.cpp:289] Learning Rate Policy: fixed
I0523 09:56:58.971470  3765 solver.cpp:341] Iteration 0, Testing net (#0)
I0523 09:57:47.755125  3765 solver.cpp:409]     Test net output #0: accuracy = 0.0937999
I0523 09:57:47.755293  3765 solver.cpp:409]     Test net output #1: loss = 2.3979 (* 1 = 2.3979 loss)
I0523 09:57:47.779654  3765 solver.cpp:237] Iteration 0, loss = 2.39593
I0523 09:57:47.779690  3765 solver.cpp:253]     Train net output #0: loss = 2.39593 (* 1 = 2.39593 loss)
I0523 09:57:47.779708  3765 sgd_solver.cpp:106] Iteration 0, lr = 0.0045
I0523 09:57:57.072269  3765 solver.cpp:237] Iteration 300, loss = 2.07116
I0523 09:57:57.072314  3765 solver.cpp:253]     Train net output #0: loss = 2.07116 (* 1 = 2.07116 loss)
I0523 09:57:57.072334  3765 sgd_solver.cpp:106] Iteration 300, lr = 0.0045
I0523 09:58:06.364193  3765 solver.cpp:237] Iteration 600, loss = 2.16577
I0523 09:58:06.364228  3765 solver.cpp:253]     Train net output #0: loss = 2.16577 (* 1 = 2.16577 loss)
I0523 09:58:06.364246  3765 sgd_solver.cpp:106] Iteration 600, lr = 0.0045
I0523 09:58:15.657948  3765 solver.cpp:237] Iteration 900, loss = 1.60453
I0523 09:58:15.657982  3765 solver.cpp:253]     Train net output #0: loss = 1.60453 (* 1 = 1.60453 loss)
I0523 09:58:15.657996  3765 sgd_solver.cpp:106] Iteration 900, lr = 0.0045
I0523 09:58:24.952041  3765 solver.cpp:237] Iteration 1200, loss = 1.83293
I0523 09:58:24.952203  3765 solver.cpp:253]     Train net output #0: loss = 1.83293 (* 1 = 1.83293 loss)
I0523 09:58:24.952216  3765 sgd_solver.cpp:106] Iteration 1200, lr = 0.0045
I0523 09:58:34.244448  3765 solver.cpp:237] Iteration 1500, loss = 1.82632
I0523 09:58:34.244482  3765 solver.cpp:253]     Train net output #0: loss = 1.82632 (* 1 = 1.82632 loss)
I0523 09:58:34.244499  3765 sgd_solver.cpp:106] Iteration 1500, lr = 0.0045
I0523 09:58:43.535265  3765 solver.cpp:237] Iteration 1800, loss = 1.8184
I0523 09:58:43.535300  3765 solver.cpp:253]     Train net output #0: loss = 1.8184 (* 1 = 1.8184 loss)
I0523 09:58:43.535313  3765 sgd_solver.cpp:106] Iteration 1800, lr = 0.0045
I0523 09:59:15.035590  3765 solver.cpp:237] Iteration 2100, loss = 1.59766
I0523 09:59:15.035758  3765 solver.cpp:253]     Train net output #0: loss = 1.59766 (* 1 = 1.59766 loss)
I0523 09:59:15.035773  3765 sgd_solver.cpp:106] Iteration 2100, lr = 0.0045
I0523 09:59:24.331501  3765 solver.cpp:237] Iteration 2400, loss = 1.45355
I0523 09:59:24.331535  3765 solver.cpp:253]     Train net output #0: loss = 1.45355 (* 1 = 1.45355 loss)
I0523 09:59:24.331552  3765 sgd_solver.cpp:106] Iteration 2400, lr = 0.0045
I0523 09:59:33.630144  3765 solver.cpp:237] Iteration 2700, loss = 1.52443
I0523 09:59:33.630180  3765 solver.cpp:253]     Train net output #0: loss = 1.52443 (* 1 = 1.52443 loss)
I0523 09:59:33.630195  3765 sgd_solver.cpp:106] Iteration 2700, lr = 0.0045
I0523 09:59:42.894698  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_3000.caffemodel
I0523 09:59:42.957808  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_3000.solverstate
I0523 09:59:42.992605  3765 solver.cpp:237] Iteration 3000, loss = 1.20646
I0523 09:59:42.992653  3765 solver.cpp:253]     Train net output #0: loss = 1.20646 (* 1 = 1.20646 loss)
I0523 09:59:42.992667  3765 sgd_solver.cpp:106] Iteration 3000, lr = 0.0045
I0523 09:59:52.288782  3765 solver.cpp:237] Iteration 3300, loss = 1.59751
I0523 09:59:52.288923  3765 solver.cpp:253]     Train net output #0: loss = 1.59751 (* 1 = 1.59751 loss)
I0523 09:59:52.288935  3765 sgd_solver.cpp:106] Iteration 3300, lr = 0.0045
I0523 10:00:01.585985  3765 solver.cpp:237] Iteration 3600, loss = 1.2853
I0523 10:00:01.586027  3765 solver.cpp:253]     Train net output #0: loss = 1.2853 (* 1 = 1.2853 loss)
I0523 10:00:01.586043  3765 sgd_solver.cpp:106] Iteration 3600, lr = 0.0045
I0523 10:00:10.881814  3765 solver.cpp:237] Iteration 3900, loss = 1.33784
I0523 10:00:10.881849  3765 solver.cpp:253]     Train net output #0: loss = 1.33784 (* 1 = 1.33784 loss)
I0523 10:00:10.881866  3765 sgd_solver.cpp:106] Iteration 3900, lr = 0.0045
I0523 10:00:42.390712  3765 solver.cpp:237] Iteration 4200, loss = 1.11513
I0523 10:00:42.390872  3765 solver.cpp:253]     Train net output #0: loss = 1.11513 (* 1 = 1.11513 loss)
I0523 10:00:42.390887  3765 sgd_solver.cpp:106] Iteration 4200, lr = 0.0045
I0523 10:00:51.686381  3765 solver.cpp:237] Iteration 4500, loss = 1.46194
I0523 10:00:51.686421  3765 solver.cpp:253]     Train net output #0: loss = 1.46194 (* 1 = 1.46194 loss)
I0523 10:00:51.686444  3765 sgd_solver.cpp:106] Iteration 4500, lr = 0.0045
I0523 10:01:00.982781  3765 solver.cpp:237] Iteration 4800, loss = 1.48379
I0523 10:01:00.982817  3765 solver.cpp:253]     Train net output #0: loss = 1.48379 (* 1 = 1.48379 loss)
I0523 10:01:00.982836  3765 sgd_solver.cpp:106] Iteration 4800, lr = 0.0045
I0523 10:01:10.276223  3765 solver.cpp:237] Iteration 5100, loss = 1.32484
I0523 10:01:10.276259  3765 solver.cpp:253]     Train net output #0: loss = 1.32484 (* 1 = 1.32484 loss)
I0523 10:01:10.276274  3765 sgd_solver.cpp:106] Iteration 5100, lr = 0.0045
I0523 10:01:19.575137  3765 solver.cpp:237] Iteration 5400, loss = 1.58496
I0523 10:01:19.575296  3765 solver.cpp:253]     Train net output #0: loss = 1.58496 (* 1 = 1.58496 loss)
I0523 10:01:19.575310  3765 sgd_solver.cpp:106] Iteration 5400, lr = 0.0045
I0523 10:01:28.873945  3765 solver.cpp:237] Iteration 5700, loss = 1.11268
I0523 10:01:28.873980  3765 solver.cpp:253]     Train net output #0: loss = 1.11268 (* 1 = 1.11268 loss)
I0523 10:01:28.873999  3765 sgd_solver.cpp:106] Iteration 5700, lr = 0.0045
I0523 10:01:38.144342  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_6000.caffemodel
I0523 10:01:38.203564  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_6000.solverstate
I0523 10:01:38.228873  3765 solver.cpp:341] Iteration 6000, Testing net (#0)
I0523 10:02:25.975678  3765 solver.cpp:409]     Test net output #0: accuracy = 0.830043
I0523 10:02:25.975846  3765 solver.cpp:409]     Test net output #1: loss = 0.538811 (* 1 = 0.538811 loss)
I0523 10:02:48.172399  3765 solver.cpp:237] Iteration 6000, loss = 1.30689
I0523 10:02:48.172454  3765 solver.cpp:253]     Train net output #0: loss = 1.30689 (* 1 = 1.30689 loss)
I0523 10:02:48.172471  3765 sgd_solver.cpp:106] Iteration 6000, lr = 0.0045
I0523 10:02:57.455101  3765 solver.cpp:237] Iteration 6300, loss = 1.23014
I0523 10:02:57.455257  3765 solver.cpp:253]     Train net output #0: loss = 1.23014 (* 1 = 1.23014 loss)
I0523 10:02:57.455271  3765 sgd_solver.cpp:106] Iteration 6300, lr = 0.0045
I0523 10:03:06.744591  3765 solver.cpp:237] Iteration 6600, loss = 1.42796
I0523 10:03:06.744626  3765 solver.cpp:253]     Train net output #0: loss = 1.42796 (* 1 = 1.42796 loss)
I0523 10:03:06.744644  3765 sgd_solver.cpp:106] Iteration 6600, lr = 0.0045
I0523 10:03:16.033751  3765 solver.cpp:237] Iteration 6900, loss = 1.49224
I0523 10:03:16.033787  3765 solver.cpp:253]     Train net output #0: loss = 1.49224 (* 1 = 1.49224 loss)
I0523 10:03:16.033802  3765 sgd_solver.cpp:106] Iteration 6900, lr = 0.0045
I0523 10:03:25.325489  3765 solver.cpp:237] Iteration 7200, loss = 1.31897
I0523 10:03:25.325526  3765 solver.cpp:253]     Train net output #0: loss = 1.31897 (* 1 = 1.31897 loss)
I0523 10:03:25.325542  3765 sgd_solver.cpp:106] Iteration 7200, lr = 0.0045
I0523 10:03:34.615331  3765 solver.cpp:237] Iteration 7500, loss = 1.5827
I0523 10:03:34.615483  3765 solver.cpp:253]     Train net output #0: loss = 1.5827 (* 1 = 1.5827 loss)
I0523 10:03:34.615497  3765 sgd_solver.cpp:106] Iteration 7500, lr = 0.0045
I0523 10:03:43.902915  3765 solver.cpp:237] Iteration 7800, loss = 1.1678
I0523 10:03:43.902950  3765 solver.cpp:253]     Train net output #0: loss = 1.1678 (* 1 = 1.1678 loss)
I0523 10:03:43.902964  3765 sgd_solver.cpp:106] Iteration 7800, lr = 0.0045
I0523 10:04:15.389009  3765 solver.cpp:237] Iteration 8100, loss = 1.27894
I0523 10:04:15.389179  3765 solver.cpp:253]     Train net output #0: loss = 1.27894 (* 1 = 1.27894 loss)
I0523 10:04:15.389195  3765 sgd_solver.cpp:106] Iteration 8100, lr = 0.0045
I0523 10:04:24.675359  3765 solver.cpp:237] Iteration 8400, loss = 1.05043
I0523 10:04:24.675395  3765 solver.cpp:253]     Train net output #0: loss = 1.05043 (* 1 = 1.05043 loss)
I0523 10:04:24.675411  3765 sgd_solver.cpp:106] Iteration 8400, lr = 0.0045
I0523 10:04:33.962904  3765 solver.cpp:237] Iteration 8700, loss = 1.31657
I0523 10:04:33.962939  3765 solver.cpp:253]     Train net output #0: loss = 1.31657 (* 1 = 1.31657 loss)
I0523 10:04:33.962959  3765 sgd_solver.cpp:106] Iteration 8700, lr = 0.0045
I0523 10:04:43.218266  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_9000.caffemodel
I0523 10:04:43.279378  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_9000.solverstate
I0523 10:04:43.315963  3765 solver.cpp:237] Iteration 9000, loss = 1.44111
I0523 10:04:43.316011  3765 solver.cpp:253]     Train net output #0: loss = 1.44111 (* 1 = 1.44111 loss)
I0523 10:04:43.316031  3765 sgd_solver.cpp:106] Iteration 9000, lr = 0.0045
I0523 10:04:52.608043  3765 solver.cpp:237] Iteration 9300, loss = 0.777752
I0523 10:04:52.608198  3765 solver.cpp:253]     Train net output #0: loss = 0.777752 (* 1 = 0.777752 loss)
I0523 10:04:52.608213  3765 sgd_solver.cpp:106] Iteration 9300, lr = 0.0045
I0523 10:05:01.897871  3765 solver.cpp:237] Iteration 9600, loss = 1.5618
I0523 10:05:01.897922  3765 solver.cpp:253]     Train net output #0: loss = 1.5618 (* 1 = 1.5618 loss)
I0523 10:05:01.897939  3765 sgd_solver.cpp:106] Iteration 9600, lr = 0.0045
I0523 10:05:11.187093  3765 solver.cpp:237] Iteration 9900, loss = 1.41345
I0523 10:05:11.187129  3765 solver.cpp:253]     Train net output #0: loss = 1.41345 (* 1 = 1.41345 loss)
I0523 10:05:11.187142  3765 sgd_solver.cpp:106] Iteration 9900, lr = 0.0045
I0523 10:05:42.716912  3765 solver.cpp:237] Iteration 10200, loss = 1.08578
I0523 10:05:42.717080  3765 solver.cpp:253]     Train net output #0: loss = 1.08578 (* 1 = 1.08578 loss)
I0523 10:05:42.717097  3765 sgd_solver.cpp:106] Iteration 10200, lr = 0.0045
I0523 10:05:52.004745  3765 solver.cpp:237] Iteration 10500, loss = 1.28416
I0523 10:05:52.004784  3765 solver.cpp:253]     Train net output #0: loss = 1.28416 (* 1 = 1.28416 loss)
I0523 10:05:52.004806  3765 sgd_solver.cpp:106] Iteration 10500, lr = 0.0045
I0523 10:06:01.294559  3765 solver.cpp:237] Iteration 10800, loss = 1.2741
I0523 10:06:01.294595  3765 solver.cpp:253]     Train net output #0: loss = 1.2741 (* 1 = 1.2741 loss)
I0523 10:06:01.294607  3765 sgd_solver.cpp:106] Iteration 10800, lr = 0.0045
I0523 10:06:10.582362  3765 solver.cpp:237] Iteration 11100, loss = 1.26425
I0523 10:06:10.582397  3765 solver.cpp:253]     Train net output #0: loss = 1.26425 (* 1 = 1.26425 loss)
I0523 10:06:10.582413  3765 sgd_solver.cpp:106] Iteration 11100, lr = 0.0045
I0523 10:06:19.871697  3765 solver.cpp:237] Iteration 11400, loss = 1.32996
I0523 10:06:19.871871  3765 solver.cpp:253]     Train net output #0: loss = 1.32996 (* 1 = 1.32996 loss)
I0523 10:06:19.871886  3765 sgd_solver.cpp:106] Iteration 11400, lr = 0.0045
I0523 10:06:29.159365  3765 solver.cpp:237] Iteration 11700, loss = 1.14054
I0523 10:06:29.159399  3765 solver.cpp:253]     Train net output #0: loss = 1.14054 (* 1 = 1.14054 loss)
I0523 10:06:29.159416  3765 sgd_solver.cpp:106] Iteration 11700, lr = 0.0045
I0523 10:06:38.419304  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_12000.caffemodel
I0523 10:06:38.480967  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_12000.solverstate
I0523 10:06:38.508189  3765 solver.cpp:341] Iteration 12000, Testing net (#0)
I0523 10:07:47.173240  3765 solver.cpp:409]     Test net output #0: accuracy = 0.863326
I0523 10:07:47.173403  3765 solver.cpp:409]     Test net output #1: loss = 0.512629 (* 1 = 0.512629 loss)
I0523 10:08:09.415386  3765 solver.cpp:237] Iteration 12000, loss = 1.16383
I0523 10:08:09.415444  3765 solver.cpp:253]     Train net output #0: loss = 1.16383 (* 1 = 1.16383 loss)
I0523 10:08:09.415459  3765 sgd_solver.cpp:106] Iteration 12000, lr = 0.0045
I0523 10:08:18.727999  3765 solver.cpp:237] Iteration 12300, loss = 1.10428
I0523 10:08:18.728162  3765 solver.cpp:253]     Train net output #0: loss = 1.10428 (* 1 = 1.10428 loss)
I0523 10:08:18.728174  3765 sgd_solver.cpp:106] Iteration 12300, lr = 0.0045
I0523 10:08:28.028551  3765 solver.cpp:237] Iteration 12600, loss = 1.52333
I0523 10:08:28.028599  3765 solver.cpp:253]     Train net output #0: loss = 1.52333 (* 1 = 1.52333 loss)
I0523 10:08:28.028615  3765 sgd_solver.cpp:106] Iteration 12600, lr = 0.0045
I0523 10:08:37.335772  3765 solver.cpp:237] Iteration 12900, loss = 1.23088
I0523 10:08:37.335808  3765 solver.cpp:253]     Train net output #0: loss = 1.23088 (* 1 = 1.23088 loss)
I0523 10:08:37.335824  3765 sgd_solver.cpp:106] Iteration 12900, lr = 0.0045
I0523 10:08:46.638648  3765 solver.cpp:237] Iteration 13200, loss = 1.31052
I0523 10:08:46.638694  3765 solver.cpp:253]     Train net output #0: loss = 1.31052 (* 1 = 1.31052 loss)
I0523 10:08:46.638711  3765 sgd_solver.cpp:106] Iteration 13200, lr = 0.0045
I0523 10:08:55.942553  3765 solver.cpp:237] Iteration 13500, loss = 1.3016
I0523 10:08:55.942695  3765 solver.cpp:253]     Train net output #0: loss = 1.3016 (* 1 = 1.3016 loss)
I0523 10:08:55.942709  3765 sgd_solver.cpp:106] Iteration 13500, lr = 0.0045
I0523 10:09:05.248461  3765 solver.cpp:237] Iteration 13800, loss = 1.0959
I0523 10:09:05.248495  3765 solver.cpp:253]     Train net output #0: loss = 1.0959 (* 1 = 1.0959 loss)
I0523 10:09:05.248512  3765 sgd_solver.cpp:106] Iteration 13800, lr = 0.0045
I0523 10:09:36.703116  3765 solver.cpp:237] Iteration 14100, loss = 1.2853
I0523 10:09:36.703281  3765 solver.cpp:253]     Train net output #0: loss = 1.2853 (* 1 = 1.2853 loss)
I0523 10:09:36.703296  3765 sgd_solver.cpp:106] Iteration 14100, lr = 0.0045
I0523 10:09:46.008968  3765 solver.cpp:237] Iteration 14400, loss = 1.09531
I0523 10:09:46.009002  3765 solver.cpp:253]     Train net output #0: loss = 1.09531 (* 1 = 1.09531 loss)
I0523 10:09:46.009021  3765 sgd_solver.cpp:106] Iteration 14400, lr = 0.0045
I0523 10:09:55.318194  3765 solver.cpp:237] Iteration 14700, loss = 1.00082
I0523 10:09:55.318229  3765 solver.cpp:253]     Train net output #0: loss = 1.00082 (* 1 = 1.00082 loss)
I0523 10:09:55.318245  3765 sgd_solver.cpp:106] Iteration 14700, lr = 0.0045
I0523 10:10:04.594362  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_15000.caffemodel
I0523 10:10:04.655537  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_15000.solverstate
I0523 10:10:04.692492  3765 solver.cpp:237] Iteration 15000, loss = 1.2403
I0523 10:10:04.692545  3765 solver.cpp:253]     Train net output #0: loss = 1.2403 (* 1 = 1.2403 loss)
I0523 10:10:04.692559  3765 sgd_solver.cpp:106] Iteration 15000, lr = 0.0045
I0523 10:10:13.999594  3765 solver.cpp:237] Iteration 15300, loss = 1.39532
I0523 10:10:13.999747  3765 solver.cpp:253]     Train net output #0: loss = 1.39532 (* 1 = 1.39532 loss)
I0523 10:10:13.999759  3765 sgd_solver.cpp:106] Iteration 15300, lr = 0.0045
I0523 10:10:23.306502  3765 solver.cpp:237] Iteration 15600, loss = 1.19001
I0523 10:10:23.306536  3765 solver.cpp:253]     Train net output #0: loss = 1.19001 (* 1 = 1.19001 loss)
I0523 10:10:23.306553  3765 sgd_solver.cpp:106] Iteration 15600, lr = 0.0045
I0523 10:10:32.613705  3765 solver.cpp:237] Iteration 15900, loss = 1.45557
I0523 10:10:32.613751  3765 solver.cpp:253]     Train net output #0: loss = 1.45557 (* 1 = 1.45557 loss)
I0523 10:10:32.613770  3765 sgd_solver.cpp:106] Iteration 15900, lr = 0.0045
I0523 10:11:04.194533  3765 solver.cpp:237] Iteration 16200, loss = 1.16079
I0523 10:11:04.194716  3765 solver.cpp:253]     Train net output #0: loss = 1.16079 (* 1 = 1.16079 loss)
I0523 10:11:04.194733  3765 sgd_solver.cpp:106] Iteration 16200, lr = 0.0045
I0523 10:11:13.500680  3765 solver.cpp:237] Iteration 16500, loss = 1.02985
I0523 10:11:13.500715  3765 solver.cpp:253]     Train net output #0: loss = 1.02985 (* 1 = 1.02985 loss)
I0523 10:11:13.500733  3765 sgd_solver.cpp:106] Iteration 16500, lr = 0.0045
I0523 10:11:22.806784  3765 solver.cpp:237] Iteration 16800, loss = 1.25395
I0523 10:11:22.806833  3765 solver.cpp:253]     Train net output #0: loss = 1.25395 (* 1 = 1.25395 loss)
I0523 10:11:22.806849  3765 sgd_solver.cpp:106] Iteration 16800, lr = 0.0045
I0523 10:11:32.114616  3765 solver.cpp:237] Iteration 17100, loss = 1.27772
I0523 10:11:32.114652  3765 solver.cpp:253]     Train net output #0: loss = 1.27772 (* 1 = 1.27772 loss)
I0523 10:11:32.114668  3765 sgd_solver.cpp:106] Iteration 17100, lr = 0.0045
I0523 10:11:41.421409  3765 solver.cpp:237] Iteration 17400, loss = 1.36303
I0523 10:11:41.421573  3765 solver.cpp:253]     Train net output #0: loss = 1.36303 (* 1 = 1.36303 loss)
I0523 10:11:41.421587  3765 sgd_solver.cpp:106] Iteration 17400, lr = 0.0045
I0523 10:11:50.727674  3765 solver.cpp:237] Iteration 17700, loss = 1.0201
I0523 10:11:50.727708  3765 solver.cpp:253]     Train net output #0: loss = 1.0201 (* 1 = 1.0201 loss)
I0523 10:11:50.727726  3765 sgd_solver.cpp:106] Iteration 17700, lr = 0.0045
I0523 10:12:00.002109  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_18000.caffemodel
I0523 10:12:00.061370  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_18000.solverstate
I0523 10:12:00.087661  3765 solver.cpp:341] Iteration 18000, Testing net (#0)
I0523 10:12:47.582604  3765 solver.cpp:409]     Test net output #0: accuracy = 0.870983
I0523 10:12:47.582772  3765 solver.cpp:409]     Test net output #1: loss = 0.450623 (* 1 = 0.450623 loss)
I0523 10:13:09.796756  3765 solver.cpp:237] Iteration 18000, loss = 0.90715
I0523 10:13:09.796811  3765 solver.cpp:253]     Train net output #0: loss = 0.90715 (* 1 = 0.90715 loss)
I0523 10:13:09.796826  3765 sgd_solver.cpp:106] Iteration 18000, lr = 0.0045
I0523 10:13:19.080713  3765 solver.cpp:237] Iteration 18300, loss = 1.22185
I0523 10:13:19.080863  3765 solver.cpp:253]     Train net output #0: loss = 1.22185 (* 1 = 1.22185 loss)
I0523 10:13:19.080878  3765 sgd_solver.cpp:106] Iteration 18300, lr = 0.0045
I0523 10:13:28.369509  3765 solver.cpp:237] Iteration 18600, loss = 1.34772
I0523 10:13:28.369547  3765 solver.cpp:253]     Train net output #0: loss = 1.34772 (* 1 = 1.34772 loss)
I0523 10:13:28.369567  3765 sgd_solver.cpp:106] Iteration 18600, lr = 0.0045
I0523 10:13:37.658310  3765 solver.cpp:237] Iteration 18900, loss = 1.3582
I0523 10:13:37.658345  3765 solver.cpp:253]     Train net output #0: loss = 1.3582 (* 1 = 1.3582 loss)
I0523 10:13:37.658362  3765 sgd_solver.cpp:106] Iteration 18900, lr = 0.0045
I0523 10:13:46.942719  3765 solver.cpp:237] Iteration 19200, loss = 1.52528
I0523 10:13:46.942771  3765 solver.cpp:253]     Train net output #0: loss = 1.52528 (* 1 = 1.52528 loss)
I0523 10:13:46.942785  3765 sgd_solver.cpp:106] Iteration 19200, lr = 0.0045
I0523 10:13:56.232575  3765 solver.cpp:237] Iteration 19500, loss = 0.943607
I0523 10:13:56.232722  3765 solver.cpp:253]     Train net output #0: loss = 0.943607 (* 1 = 0.943607 loss)
I0523 10:13:56.232738  3765 sgd_solver.cpp:106] Iteration 19500, lr = 0.0045
I0523 10:14:05.519505  3765 solver.cpp:237] Iteration 19800, loss = 0.922775
I0523 10:14:05.519539  3765 solver.cpp:253]     Train net output #0: loss = 0.922775 (* 1 = 0.922775 loss)
I0523 10:14:05.519556  3765 sgd_solver.cpp:106] Iteration 19800, lr = 0.0045
I0523 10:14:37.031348  3765 solver.cpp:237] Iteration 20100, loss = 0.989179
I0523 10:14:37.031534  3765 solver.cpp:253]     Train net output #0: loss = 0.989179 (* 1 = 0.989179 loss)
I0523 10:14:37.031549  3765 sgd_solver.cpp:106] Iteration 20100, lr = 0.0045
I0523 10:14:46.315613  3765 solver.cpp:237] Iteration 20400, loss = 1.31039
I0523 10:14:46.315649  3765 solver.cpp:253]     Train net output #0: loss = 1.31039 (* 1 = 1.31039 loss)
I0523 10:14:46.315666  3765 sgd_solver.cpp:106] Iteration 20400, lr = 0.0045
I0523 10:14:55.602035  3765 solver.cpp:237] Iteration 20700, loss = 1.36199
I0523 10:14:55.602069  3765 solver.cpp:253]     Train net output #0: loss = 1.36199 (* 1 = 1.36199 loss)
I0523 10:14:55.602085  3765 sgd_solver.cpp:106] Iteration 20700, lr = 0.0045
I0523 10:15:04.853380  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_21000.caffemodel
I0523 10:15:04.912544  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_21000.solverstate
I0523 10:15:04.948838  3765 solver.cpp:237] Iteration 21000, loss = 1.0362
I0523 10:15:04.948889  3765 solver.cpp:253]     Train net output #0: loss = 1.0362 (* 1 = 1.0362 loss)
I0523 10:15:04.948904  3765 sgd_solver.cpp:106] Iteration 21000, lr = 0.0045
I0523 10:15:14.234321  3765 solver.cpp:237] Iteration 21300, loss = 1.31122
I0523 10:15:14.234468  3765 solver.cpp:253]     Train net output #0: loss = 1.31122 (* 1 = 1.31122 loss)
I0523 10:15:14.234483  3765 sgd_solver.cpp:106] Iteration 21300, lr = 0.0045
I0523 10:15:23.519951  3765 solver.cpp:237] Iteration 21600, loss = 1.33819
I0523 10:15:23.519986  3765 solver.cpp:253]     Train net output #0: loss = 1.33819 (* 1 = 1.33819 loss)
I0523 10:15:23.520000  3765 sgd_solver.cpp:106] Iteration 21600, lr = 0.0045
I0523 10:15:32.804479  3765 solver.cpp:237] Iteration 21900, loss = 1.03548
I0523 10:15:32.804525  3765 solver.cpp:253]     Train net output #0: loss = 1.03548 (* 1 = 1.03548 loss)
I0523 10:15:32.804540  3765 sgd_solver.cpp:106] Iteration 21900, lr = 0.0045
I0523 10:16:04.268525  3765 solver.cpp:237] Iteration 22200, loss = 0.934718
I0523 10:16:04.268700  3765 solver.cpp:253]     Train net output #0: loss = 0.934718 (* 1 = 0.934718 loss)
I0523 10:16:04.268715  3765 sgd_solver.cpp:106] Iteration 22200, lr = 0.0045
I0523 10:16:13.552438  3765 solver.cpp:237] Iteration 22500, loss = 1.10054
I0523 10:16:13.552472  3765 solver.cpp:253]     Train net output #0: loss = 1.10054 (* 1 = 1.10054 loss)
I0523 10:16:13.552489  3765 sgd_solver.cpp:106] Iteration 22500, lr = 0.0045
I0523 10:16:22.837918  3765 solver.cpp:237] Iteration 22800, loss = 1.37849
I0523 10:16:22.837961  3765 solver.cpp:253]     Train net output #0: loss = 1.37849 (* 1 = 1.37849 loss)
I0523 10:16:22.837981  3765 sgd_solver.cpp:106] Iteration 22800, lr = 0.0045
I0523 10:16:32.126163  3765 solver.cpp:237] Iteration 23100, loss = 1.60882
I0523 10:16:32.126197  3765 solver.cpp:253]     Train net output #0: loss = 1.60882 (* 1 = 1.60882 loss)
I0523 10:16:32.126214  3765 sgd_solver.cpp:106] Iteration 23100, lr = 0.0045
I0523 10:16:41.415022  3765 solver.cpp:237] Iteration 23400, loss = 1.20494
I0523 10:16:41.415181  3765 solver.cpp:253]     Train net output #0: loss = 1.20494 (* 1 = 1.20494 loss)
I0523 10:16:41.415195  3765 sgd_solver.cpp:106] Iteration 23400, lr = 0.0045
I0523 10:16:50.700108  3765 solver.cpp:237] Iteration 23700, loss = 0.927228
I0523 10:16:50.700142  3765 solver.cpp:253]     Train net output #0: loss = 0.927228 (* 1 = 0.927228 loss)
I0523 10:16:50.700156  3765 sgd_solver.cpp:106] Iteration 23700, lr = 0.0045
I0523 10:16:59.959985  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_24000.caffemodel
I0523 10:17:00.019968  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_24000.solverstate
I0523 10:17:00.046345  3765 solver.cpp:341] Iteration 24000, Testing net (#0)
I0523 10:18:08.725803  3765 solver.cpp:409]     Test net output #0: accuracy = 0.876669
I0523 10:18:08.725993  3765 solver.cpp:409]     Test net output #1: loss = 0.388311 (* 1 = 0.388311 loss)
I0523 10:18:30.989203  3765 solver.cpp:237] Iteration 24000, loss = 1.26011
I0523 10:18:30.989259  3765 solver.cpp:253]     Train net output #0: loss = 1.26011 (* 1 = 1.26011 loss)
I0523 10:18:30.989276  3765 sgd_solver.cpp:106] Iteration 24000, lr = 0.0045
I0523 10:18:40.284014  3765 solver.cpp:237] Iteration 24300, loss = 1.33424
I0523 10:18:40.284169  3765 solver.cpp:253]     Train net output #0: loss = 1.33424 (* 1 = 1.33424 loss)
I0523 10:18:40.284183  3765 sgd_solver.cpp:106] Iteration 24300, lr = 0.0045
I0523 10:18:49.579257  3765 solver.cpp:237] Iteration 24600, loss = 1.17204
I0523 10:18:49.579300  3765 solver.cpp:253]     Train net output #0: loss = 1.17204 (* 1 = 1.17204 loss)
I0523 10:18:49.579319  3765 sgd_solver.cpp:106] Iteration 24600, lr = 0.0045
I0523 10:18:58.874728  3765 solver.cpp:237] Iteration 24900, loss = 0.973894
I0523 10:18:58.874764  3765 solver.cpp:253]     Train net output #0: loss = 0.973894 (* 1 = 0.973894 loss)
I0523 10:18:58.874778  3765 sgd_solver.cpp:106] Iteration 24900, lr = 0.0045
I0523 10:19:08.172101  3765 solver.cpp:237] Iteration 25200, loss = 1.15201
I0523 10:19:08.172135  3765 solver.cpp:253]     Train net output #0: loss = 1.15201 (* 1 = 1.15201 loss)
I0523 10:19:08.172152  3765 sgd_solver.cpp:106] Iteration 25200, lr = 0.0045
I0523 10:19:17.467501  3765 solver.cpp:237] Iteration 25500, loss = 1.1378
I0523 10:19:17.467661  3765 solver.cpp:253]     Train net output #0: loss = 1.1378 (* 1 = 1.1378 loss)
I0523 10:19:17.467675  3765 sgd_solver.cpp:106] Iteration 25500, lr = 0.0045
I0523 10:19:26.763582  3765 solver.cpp:237] Iteration 25800, loss = 1.34363
I0523 10:19:26.763617  3765 solver.cpp:253]     Train net output #0: loss = 1.34363 (* 1 = 1.34363 loss)
I0523 10:19:26.763634  3765 sgd_solver.cpp:106] Iteration 25800, lr = 0.0045
I0523 10:19:58.239321  3765 solver.cpp:237] Iteration 26100, loss = 1.29411
I0523 10:19:58.239502  3765 solver.cpp:253]     Train net output #0: loss = 1.29411 (* 1 = 1.29411 loss)
I0523 10:19:58.239518  3765 sgd_solver.cpp:106] Iteration 26100, lr = 0.0045
I0523 10:20:07.535337  3765 solver.cpp:237] Iteration 26400, loss = 0.974641
I0523 10:20:07.535380  3765 solver.cpp:253]     Train net output #0: loss = 0.974641 (* 1 = 0.974641 loss)
I0523 10:20:07.535399  3765 sgd_solver.cpp:106] Iteration 26400, lr = 0.0045
I0523 10:20:16.830267  3765 solver.cpp:237] Iteration 26700, loss = 0.874044
I0523 10:20:16.830303  3765 solver.cpp:253]     Train net output #0: loss = 0.874044 (* 1 = 0.874044 loss)
I0523 10:20:16.830320  3765 sgd_solver.cpp:106] Iteration 26700, lr = 0.0045
I0523 10:20:26.095662  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_27000.caffemodel
I0523 10:20:26.161515  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_27000.solverstate
I0523 10:20:26.199796  3765 solver.cpp:237] Iteration 27000, loss = 1.42982
I0523 10:20:26.199851  3765 solver.cpp:253]     Train net output #0: loss = 1.42982 (* 1 = 1.42982 loss)
I0523 10:20:26.199865  3765 sgd_solver.cpp:106] Iteration 27000, lr = 0.0045
I0523 10:20:35.501324  3765 solver.cpp:237] Iteration 27300, loss = 1.05596
I0523 10:20:35.501473  3765 solver.cpp:253]     Train net output #0: loss = 1.05596 (* 1 = 1.05596 loss)
I0523 10:20:35.501487  3765 sgd_solver.cpp:106] Iteration 27300, lr = 0.0045
I0523 10:20:44.794322  3765 solver.cpp:237] Iteration 27600, loss = 1.331
I0523 10:20:44.794355  3765 solver.cpp:253]     Train net output #0: loss = 1.331 (* 1 = 1.331 loss)
I0523 10:20:44.794373  3765 sgd_solver.cpp:106] Iteration 27600, lr = 0.0045
I0523 10:20:54.092320  3765 solver.cpp:237] Iteration 27900, loss = 1.04637
I0523 10:20:54.092370  3765 solver.cpp:253]     Train net output #0: loss = 1.04637 (* 1 = 1.04637 loss)
I0523 10:20:54.092386  3765 sgd_solver.cpp:106] Iteration 27900, lr = 0.0045
I0523 10:21:25.619670  3765 solver.cpp:237] Iteration 28200, loss = 1.15309
I0523 10:21:25.619854  3765 solver.cpp:253]     Train net output #0: loss = 1.15309 (* 1 = 1.15309 loss)
I0523 10:21:25.619870  3765 sgd_solver.cpp:106] Iteration 28200, lr = 0.0045
I0523 10:21:34.917822  3765 solver.cpp:237] Iteration 28500, loss = 1.44685
I0523 10:21:34.917855  3765 solver.cpp:253]     Train net output #0: loss = 1.44685 (* 1 = 1.44685 loss)
I0523 10:21:34.917873  3765 sgd_solver.cpp:106] Iteration 28500, lr = 0.0045
I0523 10:21:44.214053  3765 solver.cpp:237] Iteration 28800, loss = 1.39377
I0523 10:21:44.214107  3765 solver.cpp:253]     Train net output #0: loss = 1.39377 (* 1 = 1.39377 loss)
I0523 10:21:44.214123  3765 sgd_solver.cpp:106] Iteration 28800, lr = 0.0045
I0523 10:21:53.512219  3765 solver.cpp:237] Iteration 29100, loss = 1.36408
I0523 10:21:53.512254  3765 solver.cpp:253]     Train net output #0: loss = 1.36408 (* 1 = 1.36408 loss)
I0523 10:21:53.512270  3765 sgd_solver.cpp:106] Iteration 29100, lr = 0.0045
I0523 10:22:02.808461  3765 solver.cpp:237] Iteration 29400, loss = 1.31201
I0523 10:22:02.808606  3765 solver.cpp:253]     Train net output #0: loss = 1.31201 (* 1 = 1.31201 loss)
I0523 10:22:02.808620  3765 sgd_solver.cpp:106] Iteration 29400, lr = 0.0045
I0523 10:22:12.107944  3765 solver.cpp:237] Iteration 29700, loss = 1.25992
I0523 10:22:12.107997  3765 solver.cpp:253]     Train net output #0: loss = 1.25992 (* 1 = 1.25992 loss)
I0523 10:22:12.108013  3765 sgd_solver.cpp:106] Iteration 29700, lr = 0.0045
I0523 10:22:21.373849  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_30000.caffemodel
I0523 10:22:21.435441  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_30000.solverstate
I0523 10:22:21.463769  3765 solver.cpp:341] Iteration 30000, Testing net (#0)
I0523 10:23:09.291301  3765 solver.cpp:409]     Test net output #0: accuracy = 0.885012
I0523 10:23:09.291486  3765 solver.cpp:409]     Test net output #1: loss = 0.376801 (* 1 = 0.376801 loss)
I0523 10:23:30.185654  3765 solver.cpp:237] Iteration 30000, loss = 1.108
I0523 10:23:30.185710  3765 solver.cpp:253]     Train net output #0: loss = 1.108 (* 1 = 1.108 loss)
I0523 10:23:30.185725  3765 sgd_solver.cpp:106] Iteration 30000, lr = 0.0045
I0523 10:23:39.463451  3765 solver.cpp:237] Iteration 30300, loss = 0.985392
I0523 10:23:39.463605  3765 solver.cpp:253]     Train net output #0: loss = 0.985392 (* 1 = 0.985392 loss)
I0523 10:23:39.463620  3765 sgd_solver.cpp:106] Iteration 30300, lr = 0.0045
I0523 10:23:48.742993  3765 solver.cpp:237] Iteration 30600, loss = 1.19652
I0523 10:23:48.743038  3765 solver.cpp:253]     Train net output #0: loss = 1.19652 (* 1 = 1.19652 loss)
I0523 10:23:48.743060  3765 sgd_solver.cpp:106] Iteration 30600, lr = 0.0045
I0523 10:23:58.022938  3765 solver.cpp:237] Iteration 30900, loss = 1.14635
I0523 10:23:58.022974  3765 solver.cpp:253]     Train net output #0: loss = 1.14635 (* 1 = 1.14635 loss)
I0523 10:23:58.022990  3765 sgd_solver.cpp:106] Iteration 30900, lr = 0.0045
I0523 10:24:07.301003  3765 solver.cpp:237] Iteration 31200, loss = 1.22978
I0523 10:24:07.301038  3765 solver.cpp:253]     Train net output #0: loss = 1.22978 (* 1 = 1.22978 loss)
I0523 10:24:07.301054  3765 sgd_solver.cpp:106] Iteration 31200, lr = 0.0045
I0523 10:24:16.579741  3765 solver.cpp:237] Iteration 31500, loss = 1.57363
I0523 10:24:16.579897  3765 solver.cpp:253]     Train net output #0: loss = 1.57363 (* 1 = 1.57363 loss)
I0523 10:24:16.579911  3765 sgd_solver.cpp:106] Iteration 31500, lr = 0.0045
I0523 10:24:25.859844  3765 solver.cpp:237] Iteration 31800, loss = 1.40491
I0523 10:24:25.859879  3765 solver.cpp:253]     Train net output #0: loss = 1.40491 (* 1 = 1.40491 loss)
I0523 10:24:25.859895  3765 sgd_solver.cpp:106] Iteration 31800, lr = 0.0045
I0523 10:24:56.021231  3765 solver.cpp:237] Iteration 32100, loss = 1.24496
I0523 10:24:56.021409  3765 solver.cpp:253]     Train net output #0: loss = 1.24496 (* 1 = 1.24496 loss)
I0523 10:24:56.021425  3765 sgd_solver.cpp:106] Iteration 32100, lr = 0.0045
I0523 10:25:05.302963  3765 solver.cpp:237] Iteration 32400, loss = 1.16699
I0523 10:25:05.303012  3765 solver.cpp:253]     Train net output #0: loss = 1.16699 (* 1 = 1.16699 loss)
I0523 10:25:05.303030  3765 sgd_solver.cpp:106] Iteration 32400, lr = 0.0045
I0523 10:25:14.582406  3765 solver.cpp:237] Iteration 32700, loss = 1.68156
I0523 10:25:14.582442  3765 solver.cpp:253]     Train net output #0: loss = 1.68156 (* 1 = 1.68156 loss)
I0523 10:25:14.582454  3765 sgd_solver.cpp:106] Iteration 32700, lr = 0.0045
I0523 10:25:23.825857  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_33000.caffemodel
I0523 10:25:23.886051  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_33000.solverstate
I0523 10:25:23.921928  3765 solver.cpp:237] Iteration 33000, loss = 1.10715
I0523 10:25:23.921973  3765 solver.cpp:253]     Train net output #0: loss = 1.10715 (* 1 = 1.10715 loss)
I0523 10:25:23.921996  3765 sgd_solver.cpp:106] Iteration 33000, lr = 0.0045
I0523 10:25:33.195919  3765 solver.cpp:237] Iteration 33300, loss = 1.35312
I0523 10:25:33.196079  3765 solver.cpp:253]     Train net output #0: loss = 1.35312 (* 1 = 1.35312 loss)
I0523 10:25:33.196094  3765 sgd_solver.cpp:106] Iteration 33300, lr = 0.0045
I0523 10:25:42.475636  3765 solver.cpp:237] Iteration 33600, loss = 1.13008
I0523 10:25:42.475671  3765 solver.cpp:253]     Train net output #0: loss = 1.13008 (* 1 = 1.13008 loss)
I0523 10:25:42.475684  3765 sgd_solver.cpp:106] Iteration 33600, lr = 0.0045
I0523 10:25:51.759279  3765 solver.cpp:237] Iteration 33900, loss = 1.10602
I0523 10:25:51.759321  3765 solver.cpp:253]     Train net output #0: loss = 1.10602 (* 1 = 1.10602 loss)
I0523 10:25:51.759342  3765 sgd_solver.cpp:106] Iteration 33900, lr = 0.0045
I0523 10:26:21.940827  3765 solver.cpp:237] Iteration 34200, loss = 1.05019
I0523 10:26:21.941000  3765 solver.cpp:253]     Train net output #0: loss = 1.05019 (* 1 = 1.05019 loss)
I0523 10:26:21.941016  3765 sgd_solver.cpp:106] Iteration 34200, lr = 0.0045
I0523 10:26:31.219023  3765 solver.cpp:237] Iteration 34500, loss = 1.18091
I0523 10:26:31.219058  3765 solver.cpp:253]     Train net output #0: loss = 1.18091 (* 1 = 1.18091 loss)
I0523 10:26:31.219074  3765 sgd_solver.cpp:106] Iteration 34500, lr = 0.0045
I0523 10:26:40.498296  3765 solver.cpp:237] Iteration 34800, loss = 1.47583
I0523 10:26:40.498332  3765 solver.cpp:253]     Train net output #0: loss = 1.47583 (* 1 = 1.47583 loss)
I0523 10:26:40.498347  3765 sgd_solver.cpp:106] Iteration 34800, lr = 0.0045
I0523 10:26:49.776974  3765 solver.cpp:237] Iteration 35100, loss = 1.29227
I0523 10:26:49.777024  3765 solver.cpp:253]     Train net output #0: loss = 1.29227 (* 1 = 1.29227 loss)
I0523 10:26:49.777037  3765 sgd_solver.cpp:106] Iteration 35100, lr = 0.0045
I0523 10:26:59.054569  3765 solver.cpp:237] Iteration 35400, loss = 1.05823
I0523 10:26:59.054728  3765 solver.cpp:253]     Train net output #0: loss = 1.05823 (* 1 = 1.05823 loss)
I0523 10:26:59.054741  3765 sgd_solver.cpp:106] Iteration 35400, lr = 0.0045
I0523 10:27:08.335283  3765 solver.cpp:237] Iteration 35700, loss = 1.32324
I0523 10:27:08.335330  3765 solver.cpp:253]     Train net output #0: loss = 1.32324 (* 1 = 1.32324 loss)
I0523 10:27:08.335350  3765 sgd_solver.cpp:106] Iteration 35700, lr = 0.0045
I0523 10:27:17.584012  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_36000.caffemodel
I0523 10:27:17.643004  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_36000.solverstate
I0523 10:27:17.669239  3765 solver.cpp:341] Iteration 36000, Testing net (#0)
I0523 10:28:26.323079  3765 solver.cpp:409]     Test net output #0: accuracy = 0.884633
I0523 10:28:26.323261  3765 solver.cpp:409]     Test net output #1: loss = 0.358808 (* 1 = 0.358808 loss)
I0523 10:28:47.202500  3765 solver.cpp:237] Iteration 36000, loss = 1.01954
I0523 10:28:47.202558  3765 solver.cpp:253]     Train net output #0: loss = 1.01954 (* 1 = 1.01954 loss)
I0523 10:28:47.202572  3765 sgd_solver.cpp:106] Iteration 36000, lr = 0.0045
I0523 10:28:56.508785  3765 solver.cpp:237] Iteration 36300, loss = 0.977952
I0523 10:28:56.508941  3765 solver.cpp:253]     Train net output #0: loss = 0.977952 (* 1 = 0.977952 loss)
I0523 10:28:56.508955  3765 sgd_solver.cpp:106] Iteration 36300, lr = 0.0045
I0523 10:29:05.813804  3765 solver.cpp:237] Iteration 36600, loss = 0.980174
I0523 10:29:05.813838  3765 solver.cpp:253]     Train net output #0: loss = 0.980174 (* 1 = 0.980174 loss)
I0523 10:29:05.813858  3765 sgd_solver.cpp:106] Iteration 36600, lr = 0.0045
I0523 10:29:15.120229  3765 solver.cpp:237] Iteration 36900, loss = 1.25703
I0523 10:29:15.120272  3765 solver.cpp:253]     Train net output #0: loss = 1.25703 (* 1 = 1.25703 loss)
I0523 10:29:15.120293  3765 sgd_solver.cpp:106] Iteration 36900, lr = 0.0045
I0523 10:29:24.426100  3765 solver.cpp:237] Iteration 37200, loss = 1.27879
I0523 10:29:24.426134  3765 solver.cpp:253]     Train net output #0: loss = 1.27879 (* 1 = 1.27879 loss)
I0523 10:29:24.426151  3765 sgd_solver.cpp:106] Iteration 37200, lr = 0.0045
I0523 10:29:33.730293  3765 solver.cpp:237] Iteration 37500, loss = 1.29013
I0523 10:29:33.730440  3765 solver.cpp:253]     Train net output #0: loss = 1.29013 (* 1 = 1.29013 loss)
I0523 10:29:33.730453  3765 sgd_solver.cpp:106] Iteration 37500, lr = 0.0045
I0523 10:29:43.033776  3765 solver.cpp:237] Iteration 37800, loss = 1.14877
I0523 10:29:43.033823  3765 solver.cpp:253]     Train net output #0: loss = 1.14877 (* 1 = 1.14877 loss)
I0523 10:29:43.033840  3765 sgd_solver.cpp:106] Iteration 37800, lr = 0.0045
I0523 10:30:13.241565  3765 solver.cpp:237] Iteration 38100, loss = 1.24863
I0523 10:30:13.241739  3765 solver.cpp:253]     Train net output #0: loss = 1.24863 (* 1 = 1.24863 loss)
I0523 10:30:13.241755  3765 sgd_solver.cpp:106] Iteration 38100, lr = 0.0045
I0523 10:30:22.548641  3765 solver.cpp:237] Iteration 38400, loss = 1.18141
I0523 10:30:22.548676  3765 solver.cpp:253]     Train net output #0: loss = 1.18141 (* 1 = 1.18141 loss)
I0523 10:30:22.548692  3765 sgd_solver.cpp:106] Iteration 38400, lr = 0.0045
I0523 10:30:31.854826  3765 solver.cpp:237] Iteration 38700, loss = 1.26405
I0523 10:30:31.854873  3765 solver.cpp:253]     Train net output #0: loss = 1.26405 (* 1 = 1.26405 loss)
I0523 10:30:31.854892  3765 sgd_solver.cpp:106] Iteration 38700, lr = 0.0045
I0523 10:30:41.128697  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_39000.caffemodel
I0523 10:30:41.187937  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_39000.solverstate
I0523 10:30:41.223860  3765 solver.cpp:237] Iteration 39000, loss = 1.25747
I0523 10:30:41.223909  3765 solver.cpp:253]     Train net output #0: loss = 1.25747 (* 1 = 1.25747 loss)
I0523 10:30:41.223927  3765 sgd_solver.cpp:106] Iteration 39000, lr = 0.0045
I0523 10:30:50.529001  3765 solver.cpp:237] Iteration 39300, loss = 0.96561
I0523 10:30:50.529162  3765 solver.cpp:253]     Train net output #0: loss = 0.96561 (* 1 = 0.96561 loss)
I0523 10:30:50.529178  3765 sgd_solver.cpp:106] Iteration 39300, lr = 0.0045
I0523 10:30:59.835969  3765 solver.cpp:237] Iteration 39600, loss = 0.986997
I0523 10:30:59.836001  3765 solver.cpp:253]     Train net output #0: loss = 0.986997 (* 1 = 0.986997 loss)
I0523 10:30:59.836022  3765 sgd_solver.cpp:106] Iteration 39600, lr = 0.0045
I0523 10:31:09.143229  3765 solver.cpp:237] Iteration 39900, loss = 1.27301
I0523 10:31:09.143265  3765 solver.cpp:253]     Train net output #0: loss = 1.27301 (* 1 = 1.27301 loss)
I0523 10:31:09.143280  3765 sgd_solver.cpp:106] Iteration 39900, lr = 0.0045
I0523 10:31:39.343199  3765 solver.cpp:237] Iteration 40200, loss = 0.913766
I0523 10:31:39.343379  3765 solver.cpp:253]     Train net output #0: loss = 0.913766 (* 1 = 0.913766 loss)
I0523 10:31:39.343394  3765 sgd_solver.cpp:106] Iteration 40200, lr = 0.0045
I0523 10:31:48.649912  3765 solver.cpp:237] Iteration 40500, loss = 1.0696
I0523 10:31:48.649963  3765 solver.cpp:253]     Train net output #0: loss = 1.0696 (* 1 = 1.0696 loss)
I0523 10:31:48.649976  3765 sgd_solver.cpp:106] Iteration 40500, lr = 0.0045
I0523 10:31:57.954185  3765 solver.cpp:237] Iteration 40800, loss = 1.09711
I0523 10:31:57.954218  3765 solver.cpp:253]     Train net output #0: loss = 1.09711 (* 1 = 1.09711 loss)
I0523 10:31:57.954232  3765 sgd_solver.cpp:106] Iteration 40800, lr = 0.0045
I0523 10:32:07.258103  3765 solver.cpp:237] Iteration 41100, loss = 1.30445
I0523 10:32:07.258155  3765 solver.cpp:253]     Train net output #0: loss = 1.30445 (* 1 = 1.30445 loss)
I0523 10:32:07.258169  3765 sgd_solver.cpp:106] Iteration 41100, lr = 0.0045
I0523 10:32:16.562578  3765 solver.cpp:237] Iteration 41400, loss = 1.21907
I0523 10:32:16.562731  3765 solver.cpp:253]     Train net output #0: loss = 1.21907 (* 1 = 1.21907 loss)
I0523 10:32:16.562743  3765 sgd_solver.cpp:106] Iteration 41400, lr = 0.0045
I0523 10:32:25.871974  3765 solver.cpp:237] Iteration 41700, loss = 1.17205
I0523 10:32:25.872009  3765 solver.cpp:253]     Train net output #0: loss = 1.17205 (* 1 = 1.17205 loss)
I0523 10:32:25.872022  3765 sgd_solver.cpp:106] Iteration 41700, lr = 0.0045
I0523 10:32:35.149512  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_42000.caffemodel
I0523 10:32:35.208899  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_42000.solverstate
I0523 10:32:35.234973  3765 solver.cpp:341] Iteration 42000, Testing net (#0)
I0523 10:33:22.720870  3765 solver.cpp:409]     Test net output #0: accuracy = 0.886279
I0523 10:33:22.721041  3765 solver.cpp:409]     Test net output #1: loss = 0.391806 (* 1 = 0.391806 loss)
I0523 10:33:43.629122  3765 solver.cpp:237] Iteration 42000, loss = 1.12416
I0523 10:33:43.629179  3765 solver.cpp:253]     Train net output #0: loss = 1.12416 (* 1 = 1.12416 loss)
I0523 10:33:43.629195  3765 sgd_solver.cpp:106] Iteration 42000, lr = 0.0045
I0523 10:33:52.922974  3765 solver.cpp:237] Iteration 42300, loss = 1.09106
I0523 10:33:52.923143  3765 solver.cpp:253]     Train net output #0: loss = 1.09106 (* 1 = 1.09106 loss)
I0523 10:33:52.923157  3765 sgd_solver.cpp:106] Iteration 42300, lr = 0.0045
I0523 10:34:02.210813  3765 solver.cpp:237] Iteration 42600, loss = 1.32835
I0523 10:34:02.210847  3765 solver.cpp:253]     Train net output #0: loss = 1.32835 (* 1 = 1.32835 loss)
I0523 10:34:02.210865  3765 sgd_solver.cpp:106] Iteration 42600, lr = 0.0045
I0523 10:34:11.499616  3765 solver.cpp:237] Iteration 42900, loss = 0.85474
I0523 10:34:11.499662  3765 solver.cpp:253]     Train net output #0: loss = 0.85474 (* 1 = 0.85474 loss)
I0523 10:34:11.499683  3765 sgd_solver.cpp:106] Iteration 42900, lr = 0.0045
I0523 10:34:20.789687  3765 solver.cpp:237] Iteration 43200, loss = 1.04404
I0523 10:34:20.789722  3765 solver.cpp:253]     Train net output #0: loss = 1.04404 (* 1 = 1.04404 loss)
I0523 10:34:20.789739  3765 sgd_solver.cpp:106] Iteration 43200, lr = 0.0045
I0523 10:34:30.079632  3765 solver.cpp:237] Iteration 43500, loss = 0.965685
I0523 10:34:30.079788  3765 solver.cpp:253]     Train net output #0: loss = 0.965685 (* 1 = 0.965685 loss)
I0523 10:34:30.079804  3765 sgd_solver.cpp:106] Iteration 43500, lr = 0.0045
I0523 10:34:39.372128  3765 solver.cpp:237] Iteration 43800, loss = 1.23114
I0523 10:34:39.372174  3765 solver.cpp:253]     Train net output #0: loss = 1.23114 (* 1 = 1.23114 loss)
I0523 10:34:39.372195  3765 sgd_solver.cpp:106] Iteration 43800, lr = 0.0045
I0523 10:35:09.569452  3765 solver.cpp:237] Iteration 44100, loss = 1.37721
I0523 10:35:09.569633  3765 solver.cpp:253]     Train net output #0: loss = 1.37721 (* 1 = 1.37721 loss)
I0523 10:35:09.569648  3765 sgd_solver.cpp:106] Iteration 44100, lr = 0.0045
I0523 10:35:18.857300  3765 solver.cpp:237] Iteration 44400, loss = 0.866991
I0523 10:35:18.857334  3765 solver.cpp:253]     Train net output #0: loss = 0.866991 (* 1 = 0.866991 loss)
I0523 10:35:18.857352  3765 sgd_solver.cpp:106] Iteration 44400, lr = 0.0045
I0523 10:35:28.144625  3765 solver.cpp:237] Iteration 44700, loss = 1.50891
I0523 10:35:28.144670  3765 solver.cpp:253]     Train net output #0: loss = 1.50891 (* 1 = 1.50891 loss)
I0523 10:35:28.144685  3765 sgd_solver.cpp:106] Iteration 44700, lr = 0.0045
I0523 10:35:37.403630  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_45000.caffemodel
I0523 10:35:37.465590  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_45000.solverstate
I0523 10:35:37.506129  3765 solver.cpp:237] Iteration 45000, loss = 1.14924
I0523 10:35:37.506182  3765 solver.cpp:253]     Train net output #0: loss = 1.14924 (* 1 = 1.14924 loss)
I0523 10:35:37.506196  3765 sgd_solver.cpp:106] Iteration 45000, lr = 0.0045
I0523 10:35:46.798434  3765 solver.cpp:237] Iteration 45300, loss = 1.23961
I0523 10:35:46.798601  3765 solver.cpp:253]     Train net output #0: loss = 1.23961 (* 1 = 1.23961 loss)
I0523 10:35:46.798615  3765 sgd_solver.cpp:106] Iteration 45300, lr = 0.0045
I0523 10:35:56.089957  3765 solver.cpp:237] Iteration 45600, loss = 1.19524
I0523 10:35:56.090008  3765 solver.cpp:253]     Train net output #0: loss = 1.19524 (* 1 = 1.19524 loss)
I0523 10:35:56.090025  3765 sgd_solver.cpp:106] Iteration 45600, lr = 0.0045
I0523 10:36:05.380806  3765 solver.cpp:237] Iteration 45900, loss = 0.960473
I0523 10:36:05.380842  3765 solver.cpp:253]     Train net output #0: loss = 0.960473 (* 1 = 0.960473 loss)
I0523 10:36:05.380858  3765 sgd_solver.cpp:106] Iteration 45900, lr = 0.0045
I0523 10:36:35.567335  3765 solver.cpp:237] Iteration 46200, loss = 1.28125
I0523 10:36:35.567522  3765 solver.cpp:253]     Train net output #0: loss = 1.28125 (* 1 = 1.28125 loss)
I0523 10:36:35.567538  3765 sgd_solver.cpp:106] Iteration 46200, lr = 0.0045
I0523 10:36:44.854221  3765 solver.cpp:237] Iteration 46500, loss = 1.42622
I0523 10:36:44.854265  3765 solver.cpp:253]     Train net output #0: loss = 1.42622 (* 1 = 1.42622 loss)
I0523 10:36:44.854286  3765 sgd_solver.cpp:106] Iteration 46500, lr = 0.0045
I0523 10:36:54.144667  3765 solver.cpp:237] Iteration 46800, loss = 1.22664
I0523 10:36:54.144703  3765 solver.cpp:253]     Train net output #0: loss = 1.22664 (* 1 = 1.22664 loss)
I0523 10:36:54.144721  3765 sgd_solver.cpp:106] Iteration 46800, lr = 0.0045
I0523 10:37:03.434319  3765 solver.cpp:237] Iteration 47100, loss = 1.29365
I0523 10:37:03.434353  3765 solver.cpp:253]     Train net output #0: loss = 1.29365 (* 1 = 1.29365 loss)
I0523 10:37:03.434366  3765 sgd_solver.cpp:106] Iteration 47100, lr = 0.0045
I0523 10:37:12.721670  3765 solver.cpp:237] Iteration 47400, loss = 1.05026
I0523 10:37:12.721845  3765 solver.cpp:253]     Train net output #0: loss = 1.05026 (* 1 = 1.05026 loss)
I0523 10:37:12.721860  3765 sgd_solver.cpp:106] Iteration 47400, lr = 0.0045
I0523 10:37:22.006609  3765 solver.cpp:237] Iteration 47700, loss = 1.14111
I0523 10:37:22.006644  3765 solver.cpp:253]     Train net output #0: loss = 1.14111 (* 1 = 1.14111 loss)
I0523 10:37:22.006661  3765 sgd_solver.cpp:106] Iteration 47700, lr = 0.0045
I0523 10:37:31.269322  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_48000.caffemodel
I0523 10:37:31.329042  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_48000.solverstate
I0523 10:37:31.355502  3765 solver.cpp:341] Iteration 48000, Testing net (#0)
I0523 10:38:40.044628  3765 solver.cpp:409]     Test net output #0: accuracy = 0.889006
I0523 10:38:40.044807  3765 solver.cpp:409]     Test net output #1: loss = 0.369648 (* 1 = 0.369648 loss)
I0523 10:39:00.927631  3765 solver.cpp:237] Iteration 48000, loss = 0.917962
I0523 10:39:00.927688  3765 solver.cpp:253]     Train net output #0: loss = 0.917962 (* 1 = 0.917962 loss)
I0523 10:39:00.927705  3765 sgd_solver.cpp:106] Iteration 48000, lr = 0.0045
I0523 10:39:10.219676  3765 solver.cpp:237] Iteration 48300, loss = 0.925771
I0523 10:39:10.219835  3765 solver.cpp:253]     Train net output #0: loss = 0.925771 (* 1 = 0.925771 loss)
I0523 10:39:10.219848  3765 sgd_solver.cpp:106] Iteration 48300, lr = 0.0045
I0523 10:39:19.515278  3765 solver.cpp:237] Iteration 48600, loss = 1.5563
I0523 10:39:19.515328  3765 solver.cpp:253]     Train net output #0: loss = 1.5563 (* 1 = 1.5563 loss)
I0523 10:39:19.515344  3765 sgd_solver.cpp:106] Iteration 48600, lr = 0.0045
I0523 10:39:28.807126  3765 solver.cpp:237] Iteration 48900, loss = 1.41129
I0523 10:39:28.807163  3765 solver.cpp:253]     Train net output #0: loss = 1.41129 (* 1 = 1.41129 loss)
I0523 10:39:28.807178  3765 sgd_solver.cpp:106] Iteration 48900, lr = 0.0045
I0523 10:39:38.103039  3765 solver.cpp:237] Iteration 49200, loss = 1.22872
I0523 10:39:38.103086  3765 solver.cpp:253]     Train net output #0: loss = 1.22872 (* 1 = 1.22872 loss)
I0523 10:39:38.103106  3765 sgd_solver.cpp:106] Iteration 49200, lr = 0.0045
I0523 10:39:47.400115  3765 solver.cpp:237] Iteration 49500, loss = 1.28574
I0523 10:39:47.400265  3765 solver.cpp:253]     Train net output #0: loss = 1.28574 (* 1 = 1.28574 loss)
I0523 10:39:47.400279  3765 sgd_solver.cpp:106] Iteration 49500, lr = 0.0045
I0523 10:39:56.691946  3765 solver.cpp:237] Iteration 49800, loss = 1.0057
I0523 10:39:56.691978  3765 solver.cpp:253]     Train net output #0: loss = 1.0057 (* 1 = 1.0057 loss)
I0523 10:39:56.691998  3765 sgd_solver.cpp:106] Iteration 49800, lr = 0.0045
I0523 10:40:26.884840  3765 solver.cpp:237] Iteration 50100, loss = 1.12863
I0523 10:40:26.885015  3765 solver.cpp:253]     Train net output #0: loss = 1.12863 (* 1 = 1.12863 loss)
I0523 10:40:26.885030  3765 sgd_solver.cpp:106] Iteration 50100, lr = 0.0045
I0523 10:40:36.182354  3765 solver.cpp:237] Iteration 50400, loss = 0.947308
I0523 10:40:36.182389  3765 solver.cpp:253]     Train net output #0: loss = 0.947308 (* 1 = 0.947308 loss)
I0523 10:40:36.182406  3765 sgd_solver.cpp:106] Iteration 50400, lr = 0.0045
I0523 10:40:45.481752  3765 solver.cpp:237] Iteration 50700, loss = 1.15799
I0523 10:40:45.481788  3765 solver.cpp:253]     Train net output #0: loss = 1.15799 (* 1 = 1.15799 loss)
I0523 10:40:45.481804  3765 sgd_solver.cpp:106] Iteration 50700, lr = 0.0045
I0523 10:40:54.746978  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_51000.caffemodel
I0523 10:40:54.806176  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_51000.solverstate
I0523 10:40:54.842345  3765 solver.cpp:237] Iteration 51000, loss = 0.955499
I0523 10:40:54.842393  3765 solver.cpp:253]     Train net output #0: loss = 0.955499 (* 1 = 0.955499 loss)
I0523 10:40:54.842411  3765 sgd_solver.cpp:106] Iteration 51000, lr = 0.0045
I0523 10:41:04.138556  3765 solver.cpp:237] Iteration 51300, loss = 1.14834
I0523 10:41:04.138718  3765 solver.cpp:253]     Train net output #0: loss = 1.14834 (* 1 = 1.14834 loss)
I0523 10:41:04.138731  3765 sgd_solver.cpp:106] Iteration 51300, lr = 0.0045
I0523 10:41:13.436522  3765 solver.cpp:237] Iteration 51600, loss = 1.09299
I0523 10:41:13.436558  3765 solver.cpp:253]     Train net output #0: loss = 1.09299 (* 1 = 1.09299 loss)
I0523 10:41:13.436575  3765 sgd_solver.cpp:106] Iteration 51600, lr = 0.0045
I0523 10:41:22.733242  3765 solver.cpp:237] Iteration 51900, loss = 1.22992
I0523 10:41:22.733289  3765 solver.cpp:253]     Train net output #0: loss = 1.22992 (* 1 = 1.22992 loss)
I0523 10:41:22.733307  3765 sgd_solver.cpp:106] Iteration 51900, lr = 0.0045
I0523 10:41:52.923909  3765 solver.cpp:237] Iteration 52200, loss = 1.01723
I0523 10:41:52.924090  3765 solver.cpp:253]     Train net output #0: loss = 1.01723 (* 1 = 1.01723 loss)
I0523 10:41:52.924106  3765 sgd_solver.cpp:106] Iteration 52200, lr = 0.0045
I0523 10:42:02.217525  3765 solver.cpp:237] Iteration 52500, loss = 1.21124
I0523 10:42:02.217560  3765 solver.cpp:253]     Train net output #0: loss = 1.21124 (* 1 = 1.21124 loss)
I0523 10:42:02.217576  3765 sgd_solver.cpp:106] Iteration 52500, lr = 0.0045
I0523 10:42:11.515318  3765 solver.cpp:237] Iteration 52800, loss = 1.36754
I0523 10:42:11.515363  3765 solver.cpp:253]     Train net output #0: loss = 1.36754 (* 1 = 1.36754 loss)
I0523 10:42:11.515383  3765 sgd_solver.cpp:106] Iteration 52800, lr = 0.0045
I0523 10:42:20.815095  3765 solver.cpp:237] Iteration 53100, loss = 1.14367
I0523 10:42:20.815131  3765 solver.cpp:253]     Train net output #0: loss = 1.14367 (* 1 = 1.14367 loss)
I0523 10:42:20.815145  3765 sgd_solver.cpp:106] Iteration 53100, lr = 0.0045
I0523 10:42:30.113641  3765 solver.cpp:237] Iteration 53400, loss = 1.10228
I0523 10:42:30.113795  3765 solver.cpp:253]     Train net output #0: loss = 1.10228 (* 1 = 1.10228 loss)
I0523 10:42:30.113807  3765 sgd_solver.cpp:106] Iteration 53400, lr = 0.0045
I0523 10:42:39.410265  3765 solver.cpp:237] Iteration 53700, loss = 1.03899
I0523 10:42:39.410316  3765 solver.cpp:253]     Train net output #0: loss = 1.03899 (* 1 = 1.03899 loss)
I0523 10:42:39.410331  3765 sgd_solver.cpp:106] Iteration 53700, lr = 0.0045
I0523 10:42:48.678217  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_54000.caffemodel
I0523 10:42:48.737601  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_54000.solverstate
I0523 10:42:48.763921  3765 solver.cpp:341] Iteration 54000, Testing net (#0)
I0523 10:43:36.572041  3765 solver.cpp:409]     Test net output #0: accuracy = 0.890867
I0523 10:43:36.572216  3765 solver.cpp:409]     Test net output #1: loss = 0.340821 (* 1 = 0.340821 loss)
I0523 10:43:57.494056  3765 solver.cpp:237] Iteration 54000, loss = 1.20283
I0523 10:43:57.494112  3765 solver.cpp:253]     Train net output #0: loss = 1.20283 (* 1 = 1.20283 loss)
I0523 10:43:57.494128  3765 sgd_solver.cpp:106] Iteration 54000, lr = 0.0045
I0523 10:44:06.772887  3765 solver.cpp:237] Iteration 54300, loss = 1.23487
I0523 10:44:06.773057  3765 solver.cpp:253]     Train net output #0: loss = 1.23487 (* 1 = 1.23487 loss)
I0523 10:44:06.773071  3765 sgd_solver.cpp:106] Iteration 54300, lr = 0.0045
I0523 10:44:16.054178  3765 solver.cpp:237] Iteration 54600, loss = 1.08002
I0523 10:44:16.054224  3765 solver.cpp:253]     Train net output #0: loss = 1.08002 (* 1 = 1.08002 loss)
I0523 10:44:16.054244  3765 sgd_solver.cpp:106] Iteration 54600, lr = 0.0045
I0523 10:44:25.334393  3765 solver.cpp:237] Iteration 54900, loss = 1.07201
I0523 10:44:25.334427  3765 solver.cpp:253]     Train net output #0: loss = 1.07201 (* 1 = 1.07201 loss)
I0523 10:44:25.334444  3765 sgd_solver.cpp:106] Iteration 54900, lr = 0.0045
I0523 10:44:34.613163  3765 solver.cpp:237] Iteration 55200, loss = 1.13009
I0523 10:44:34.613198  3765 solver.cpp:253]     Train net output #0: loss = 1.13009 (* 1 = 1.13009 loss)
I0523 10:44:34.613215  3765 sgd_solver.cpp:106] Iteration 55200, lr = 0.0045
I0523 10:44:43.893357  3765 solver.cpp:237] Iteration 55500, loss = 1.2772
I0523 10:44:43.893535  3765 solver.cpp:253]     Train net output #0: loss = 1.2772 (* 1 = 1.2772 loss)
I0523 10:44:43.893549  3765 sgd_solver.cpp:106] Iteration 55500, lr = 0.0045
I0523 10:44:53.172719  3765 solver.cpp:237] Iteration 55800, loss = 0.955167
I0523 10:44:53.172754  3765 solver.cpp:253]     Train net output #0: loss = 0.955167 (* 1 = 0.955167 loss)
I0523 10:44:53.172770  3765 sgd_solver.cpp:106] Iteration 55800, lr = 0.0045
I0523 10:45:23.353298  3765 solver.cpp:237] Iteration 56100, loss = 1.32517
I0523 10:45:23.353482  3765 solver.cpp:253]     Train net output #0: loss = 1.32517 (* 1 = 1.32517 loss)
I0523 10:45:23.353498  3765 sgd_solver.cpp:106] Iteration 56100, lr = 0.0045
I0523 10:45:32.630316  3765 solver.cpp:237] Iteration 56400, loss = 1.40878
I0523 10:45:32.630360  3765 solver.cpp:253]     Train net output #0: loss = 1.40878 (* 1 = 1.40878 loss)
I0523 10:45:32.630381  3765 sgd_solver.cpp:106] Iteration 56400, lr = 0.0045
I0523 10:45:41.906939  3765 solver.cpp:237] Iteration 56700, loss = 1.07402
I0523 10:45:41.906975  3765 solver.cpp:253]     Train net output #0: loss = 1.07402 (* 1 = 1.07402 loss)
I0523 10:45:41.906991  3765 sgd_solver.cpp:106] Iteration 56700, lr = 0.0045
I0523 10:45:51.156813  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_57000.caffemodel
I0523 10:45:51.217733  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_57000.solverstate
I0523 10:45:51.255686  3765 solver.cpp:237] Iteration 57000, loss = 1.2157
I0523 10:45:51.255734  3765 solver.cpp:253]     Train net output #0: loss = 1.2157 (* 1 = 1.2157 loss)
I0523 10:45:51.255753  3765 sgd_solver.cpp:106] Iteration 57000, lr = 0.0045
I0523 10:46:00.531527  3765 solver.cpp:237] Iteration 57300, loss = 0.967212
I0523 10:46:00.531679  3765 solver.cpp:253]     Train net output #0: loss = 0.967212 (* 1 = 0.967212 loss)
I0523 10:46:00.531693  3765 sgd_solver.cpp:106] Iteration 57300, lr = 0.0045
I0523 10:46:09.808552  3765 solver.cpp:237] Iteration 57600, loss = 1.17222
I0523 10:46:09.808584  3765 solver.cpp:253]     Train net output #0: loss = 1.17222 (* 1 = 1.17222 loss)
I0523 10:46:09.808603  3765 sgd_solver.cpp:106] Iteration 57600, lr = 0.0045
I0523 10:46:19.088623  3765 solver.cpp:237] Iteration 57900, loss = 0.951095
I0523 10:46:19.088672  3765 solver.cpp:253]     Train net output #0: loss = 0.951095 (* 1 = 0.951095 loss)
I0523 10:46:19.088690  3765 sgd_solver.cpp:106] Iteration 57900, lr = 0.0045
I0523 10:46:49.326534  3765 solver.cpp:237] Iteration 58200, loss = 1.18218
I0523 10:46:49.326717  3765 solver.cpp:253]     Train net output #0: loss = 1.18218 (* 1 = 1.18218 loss)
I0523 10:46:49.326733  3765 sgd_solver.cpp:106] Iteration 58200, lr = 0.0045
I0523 10:46:58.603082  3765 solver.cpp:237] Iteration 58500, loss = 1.12566
I0523 10:46:58.603116  3765 solver.cpp:253]     Train net output #0: loss = 1.12566 (* 1 = 1.12566 loss)
I0523 10:46:58.603134  3765 sgd_solver.cpp:106] Iteration 58500, lr = 0.0045
I0523 10:47:07.880789  3765 solver.cpp:237] Iteration 58800, loss = 1.15858
I0523 10:47:07.880837  3765 solver.cpp:253]     Train net output #0: loss = 1.15858 (* 1 = 1.15858 loss)
I0523 10:47:07.880854  3765 sgd_solver.cpp:106] Iteration 58800, lr = 0.0045
I0523 10:47:17.157958  3765 solver.cpp:237] Iteration 59100, loss = 1.37339
I0523 10:47:17.157992  3765 solver.cpp:253]     Train net output #0: loss = 1.37339 (* 1 = 1.37339 loss)
I0523 10:47:17.158010  3765 sgd_solver.cpp:106] Iteration 59100, lr = 0.0045
I0523 10:47:26.436779  3765 solver.cpp:237] Iteration 59400, loss = 1.40931
I0523 10:47:26.436944  3765 solver.cpp:253]     Train net output #0: loss = 1.40931 (* 1 = 1.40931 loss)
I0523 10:47:26.436957  3765 sgd_solver.cpp:106] Iteration 59400, lr = 0.0045
I0523 10:47:35.715243  3765 solver.cpp:237] Iteration 59700, loss = 1.29782
I0523 10:47:35.715292  3765 solver.cpp:253]     Train net output #0: loss = 1.29782 (* 1 = 1.29782 loss)
I0523 10:47:35.715307  3765 sgd_solver.cpp:106] Iteration 59700, lr = 0.0045
I0523 10:47:44.961817  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_60000.caffemodel
I0523 10:47:45.023365  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_60000.solverstate
I0523 10:47:45.052188  3765 solver.cpp:341] Iteration 60000, Testing net (#0)
I0523 10:48:53.762979  3765 solver.cpp:409]     Test net output #0: accuracy = 0.893618
I0523 10:48:53.763159  3765 solver.cpp:409]     Test net output #1: loss = 0.338067 (* 1 = 0.338067 loss)
I0523 10:49:14.670645  3765 solver.cpp:237] Iteration 60000, loss = 1.1788
I0523 10:49:14.670701  3765 solver.cpp:253]     Train net output #0: loss = 1.1788 (* 1 = 1.1788 loss)
I0523 10:49:14.670720  3765 sgd_solver.cpp:106] Iteration 60000, lr = 0.0045
I0523 10:49:23.971468  3765 solver.cpp:237] Iteration 60300, loss = 0.917537
I0523 10:49:23.971632  3765 solver.cpp:253]     Train net output #0: loss = 0.917537 (* 1 = 0.917537 loss)
I0523 10:49:23.971645  3765 sgd_solver.cpp:106] Iteration 60300, lr = 0.0045
I0523 10:49:33.273855  3765 solver.cpp:237] Iteration 60600, loss = 1.24722
I0523 10:49:33.273890  3765 solver.cpp:253]     Train net output #0: loss = 1.24722 (* 1 = 1.24722 loss)
I0523 10:49:33.273906  3765 sgd_solver.cpp:106] Iteration 60600, lr = 0.0045
I0523 10:49:42.574034  3765 solver.cpp:237] Iteration 60900, loss = 1.23398
I0523 10:49:42.574081  3765 solver.cpp:253]     Train net output #0: loss = 1.23398 (* 1 = 1.23398 loss)
I0523 10:49:42.574100  3765 sgd_solver.cpp:106] Iteration 60900, lr = 0.0045
I0523 10:49:51.872061  3765 solver.cpp:237] Iteration 61200, loss = 1.00218
I0523 10:49:51.872097  3765 solver.cpp:253]     Train net output #0: loss = 1.00218 (* 1 = 1.00218 loss)
I0523 10:49:51.872112  3765 sgd_solver.cpp:106] Iteration 61200, lr = 0.0045
I0523 10:50:01.169751  3765 solver.cpp:237] Iteration 61500, loss = 1.02181
I0523 10:50:01.169917  3765 solver.cpp:253]     Train net output #0: loss = 1.02181 (* 1 = 1.02181 loss)
I0523 10:50:01.169932  3765 sgd_solver.cpp:106] Iteration 61500, lr = 0.0045
I0523 10:50:10.472690  3765 solver.cpp:237] Iteration 61800, loss = 1.42233
I0523 10:50:10.472725  3765 solver.cpp:253]     Train net output #0: loss = 1.42233 (* 1 = 1.42233 loss)
I0523 10:50:10.472739  3765 sgd_solver.cpp:106] Iteration 61800, lr = 0.0045
I0523 10:50:40.643014  3765 solver.cpp:237] Iteration 62100, loss = 1.21268
I0523 10:50:40.643195  3765 solver.cpp:253]     Train net output #0: loss = 1.21268 (* 1 = 1.21268 loss)
I0523 10:50:40.643210  3765 sgd_solver.cpp:106] Iteration 62100, lr = 0.0045
I0523 10:50:49.950683  3765 solver.cpp:237] Iteration 62400, loss = 0.72871
I0523 10:50:49.950718  3765 solver.cpp:253]     Train net output #0: loss = 0.72871 (* 1 = 0.72871 loss)
I0523 10:50:49.950736  3765 sgd_solver.cpp:106] Iteration 62400, lr = 0.0045
I0523 10:50:59.251397  3765 solver.cpp:237] Iteration 62700, loss = 1.32779
I0523 10:50:59.251440  3765 solver.cpp:253]     Train net output #0: loss = 1.32779 (* 1 = 1.32779 loss)
I0523 10:50:59.251453  3765 sgd_solver.cpp:106] Iteration 62700, lr = 0.0045
I0523 10:51:08.528372  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_63000.caffemodel
I0523 10:51:08.587450  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_63000.solverstate
I0523 10:51:08.623255  3765 solver.cpp:237] Iteration 63000, loss = 1.01866
I0523 10:51:08.623297  3765 solver.cpp:253]     Train net output #0: loss = 1.01866 (* 1 = 1.01866 loss)
I0523 10:51:08.623318  3765 sgd_solver.cpp:106] Iteration 63000, lr = 0.0045
I0523 10:51:17.927050  3765 solver.cpp:237] Iteration 63300, loss = 1.25626
I0523 10:51:17.927237  3765 solver.cpp:253]     Train net output #0: loss = 1.25626 (* 1 = 1.25626 loss)
I0523 10:51:17.927251  3765 sgd_solver.cpp:106] Iteration 63300, lr = 0.0045
I0523 10:51:27.228720  3765 solver.cpp:237] Iteration 63600, loss = 1.0466
I0523 10:51:27.228756  3765 solver.cpp:253]     Train net output #0: loss = 1.0466 (* 1 = 1.0466 loss)
I0523 10:51:27.228772  3765 sgd_solver.cpp:106] Iteration 63600, lr = 0.0045
I0523 10:51:36.536000  3765 solver.cpp:237] Iteration 63900, loss = 1.07862
I0523 10:51:36.536036  3765 solver.cpp:253]     Train net output #0: loss = 1.07862 (* 1 = 1.07862 loss)
I0523 10:51:36.536052  3765 sgd_solver.cpp:106] Iteration 63900, lr = 0.0045
I0523 10:52:06.735198  3765 solver.cpp:237] Iteration 64200, loss = 1.18036
I0523 10:52:06.735378  3765 solver.cpp:253]     Train net output #0: loss = 1.18036 (* 1 = 1.18036 loss)
I0523 10:52:06.735394  3765 sgd_solver.cpp:106] Iteration 64200, lr = 0.0045
I0523 10:52:16.037727  3765 solver.cpp:237] Iteration 64500, loss = 1.0107
I0523 10:52:16.037761  3765 solver.cpp:253]     Train net output #0: loss = 1.0107 (* 1 = 1.0107 loss)
I0523 10:52:16.037778  3765 sgd_solver.cpp:106] Iteration 64500, lr = 0.0045
I0523 10:52:25.340327  3765 solver.cpp:237] Iteration 64800, loss = 1.15091
I0523 10:52:25.340361  3765 solver.cpp:253]     Train net output #0: loss = 1.15091 (* 1 = 1.15091 loss)
I0523 10:52:25.340378  3765 sgd_solver.cpp:106] Iteration 64800, lr = 0.0045
I0523 10:52:34.642566  3765 solver.cpp:237] Iteration 65100, loss = 1.30871
I0523 10:52:34.642603  3765 solver.cpp:253]     Train net output #0: loss = 1.3087 (* 1 = 1.3087 loss)
I0523 10:52:34.642621  3765 sgd_solver.cpp:106] Iteration 65100, lr = 0.0045
I0523 10:52:43.946516  3765 solver.cpp:237] Iteration 65400, loss = 1.05452
I0523 10:52:43.946666  3765 solver.cpp:253]     Train net output #0: loss = 1.05452 (* 1 = 1.05452 loss)
I0523 10:52:43.946681  3765 sgd_solver.cpp:106] Iteration 65400, lr = 0.0045
I0523 10:52:53.250586  3765 solver.cpp:237] Iteration 65700, loss = 1.15098
I0523 10:52:53.250620  3765 solver.cpp:253]     Train net output #0: loss = 1.15098 (* 1 = 1.15098 loss)
I0523 10:52:53.250638  3765 sgd_solver.cpp:106] Iteration 65700, lr = 0.0045
I0523 10:53:02.525988  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_66000.caffemodel
I0523 10:53:02.585070  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_66000.solverstate
I0523 10:53:02.611321  3765 solver.cpp:341] Iteration 66000, Testing net (#0)
I0523 10:53:50.037413  3765 solver.cpp:409]     Test net output #0: accuracy = 0.895603
I0523 10:53:50.037592  3765 solver.cpp:409]     Test net output #1: loss = 0.335953 (* 1 = 0.335953 loss)
I0523 10:54:10.913442  3765 solver.cpp:237] Iteration 66000, loss = 1.22944
I0523 10:54:10.913496  3765 solver.cpp:253]     Train net output #0: loss = 1.22944 (* 1 = 1.22944 loss)
I0523 10:54:10.913511  3765 sgd_solver.cpp:106] Iteration 66000, lr = 0.0045
I0523 10:54:20.203965  3765 solver.cpp:237] Iteration 66300, loss = 1.14287
I0523 10:54:20.204138  3765 solver.cpp:253]     Train net output #0: loss = 1.14287 (* 1 = 1.14287 loss)
I0523 10:54:20.204151  3765 sgd_solver.cpp:106] Iteration 66300, lr = 0.0045
I0523 10:54:29.494715  3765 solver.cpp:237] Iteration 66600, loss = 0.905202
I0523 10:54:29.494750  3765 solver.cpp:253]     Train net output #0: loss = 0.905202 (* 1 = 0.905202 loss)
I0523 10:54:29.494766  3765 sgd_solver.cpp:106] Iteration 66600, lr = 0.0045
I0523 10:54:38.784353  3765 solver.cpp:237] Iteration 66900, loss = 1.24155
I0523 10:54:38.784394  3765 solver.cpp:253]     Train net output #0: loss = 1.24155 (* 1 = 1.24155 loss)
I0523 10:54:38.784411  3765 sgd_solver.cpp:106] Iteration 66900, lr = 0.0045
I0523 10:54:48.072219  3765 solver.cpp:237] Iteration 67200, loss = 1.21918
I0523 10:54:48.072253  3765 solver.cpp:253]     Train net output #0: loss = 1.21918 (* 1 = 1.21918 loss)
I0523 10:54:48.072266  3765 sgd_solver.cpp:106] Iteration 67200, lr = 0.0045
I0523 10:54:57.364754  3765 solver.cpp:237] Iteration 67500, loss = 1.40189
I0523 10:54:57.364912  3765 solver.cpp:253]     Train net output #0: loss = 1.40189 (* 1 = 1.40189 loss)
I0523 10:54:57.364924  3765 sgd_solver.cpp:106] Iteration 67500, lr = 0.0045
I0523 10:55:06.654098  3765 solver.cpp:237] Iteration 67800, loss = 1.2554
I0523 10:55:06.654141  3765 solver.cpp:253]     Train net output #0: loss = 1.2554 (* 1 = 1.2554 loss)
I0523 10:55:06.654158  3765 sgd_solver.cpp:106] Iteration 67800, lr = 0.0045
I0523 10:55:36.825474  3765 solver.cpp:237] Iteration 68100, loss = 1.09495
I0523 10:55:36.825659  3765 solver.cpp:253]     Train net output #0: loss = 1.09495 (* 1 = 1.09495 loss)
I0523 10:55:36.825673  3765 sgd_solver.cpp:106] Iteration 68100, lr = 0.0045
I0523 10:55:46.107233  3765 solver.cpp:237] Iteration 68400, loss = 1.0715
I0523 10:55:46.107266  3765 solver.cpp:253]     Train net output #0: loss = 1.0715 (* 1 = 1.0715 loss)
I0523 10:55:46.107283  3765 sgd_solver.cpp:106] Iteration 68400, lr = 0.0045
I0523 10:55:55.398085  3765 solver.cpp:237] Iteration 68700, loss = 1.38308
I0523 10:55:55.398133  3765 solver.cpp:253]     Train net output #0: loss = 1.38308 (* 1 = 1.38308 loss)
I0523 10:55:55.398149  3765 sgd_solver.cpp:106] Iteration 68700, lr = 0.0045
I0523 10:56:04.657001  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_69000.caffemodel
I0523 10:56:04.715924  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_69000.solverstate
I0523 10:56:04.751672  3765 solver.cpp:237] Iteration 69000, loss = 1.07728
I0523 10:56:04.751720  3765 solver.cpp:253]     Train net output #0: loss = 1.07728 (* 1 = 1.07728 loss)
I0523 10:56:04.751734  3765 sgd_solver.cpp:106] Iteration 69000, lr = 0.0045
I0523 10:56:14.041124  3765 solver.cpp:237] Iteration 69300, loss = 1.11704
I0523 10:56:14.041285  3765 solver.cpp:253]     Train net output #0: loss = 1.11704 (* 1 = 1.11704 loss)
I0523 10:56:14.041299  3765 sgd_solver.cpp:106] Iteration 69300, lr = 0.0045
I0523 10:56:23.327239  3765 solver.cpp:237] Iteration 69600, loss = 1.04924
I0523 10:56:23.327280  3765 solver.cpp:253]     Train net output #0: loss = 1.04924 (* 1 = 1.04924 loss)
I0523 10:56:23.327298  3765 sgd_solver.cpp:106] Iteration 69600, lr = 0.0045
I0523 10:56:32.615459  3765 solver.cpp:237] Iteration 69900, loss = 1.31962
I0523 10:56:32.615494  3765 solver.cpp:253]     Train net output #0: loss = 1.31962 (* 1 = 1.31962 loss)
I0523 10:56:32.615510  3765 sgd_solver.cpp:106] Iteration 69900, lr = 0.0045
I0523 10:57:02.753872  3765 solver.cpp:237] Iteration 70200, loss = 1.24193
I0523 10:57:02.754051  3765 solver.cpp:253]     Train net output #0: loss = 1.24193 (* 1 = 1.24193 loss)
I0523 10:57:02.754066  3765 sgd_solver.cpp:106] Iteration 70200, lr = 0.0045
I0523 10:57:12.038506  3765 solver.cpp:237] Iteration 70500, loss = 1.16225
I0523 10:57:12.038552  3765 solver.cpp:253]     Train net output #0: loss = 1.16225 (* 1 = 1.16225 loss)
I0523 10:57:12.038569  3765 sgd_solver.cpp:106] Iteration 70500, lr = 0.0045
I0523 10:57:21.329753  3765 solver.cpp:237] Iteration 70800, loss = 1.26773
I0523 10:57:21.329788  3765 solver.cpp:253]     Train net output #0: loss = 1.26773 (* 1 = 1.26773 loss)
I0523 10:57:21.329805  3765 sgd_solver.cpp:106] Iteration 70800, lr = 0.0045
I0523 10:57:30.616392  3765 solver.cpp:237] Iteration 71100, loss = 0.876259
I0523 10:57:30.616428  3765 solver.cpp:253]     Train net output #0: loss = 0.876259 (* 1 = 0.876259 loss)
I0523 10:57:30.616444  3765 sgd_solver.cpp:106] Iteration 71100, lr = 0.0045
I0523 10:57:39.907670  3765 solver.cpp:237] Iteration 71400, loss = 1.11808
I0523 10:57:39.907850  3765 solver.cpp:253]     Train net output #0: loss = 1.11808 (* 1 = 1.11808 loss)
I0523 10:57:39.907863  3765 sgd_solver.cpp:106] Iteration 71400, lr = 0.0045
I0523 10:57:49.194557  3765 solver.cpp:237] Iteration 71700, loss = 0.987544
I0523 10:57:49.194592  3765 solver.cpp:253]     Train net output #0: loss = 0.987544 (* 1 = 0.987544 loss)
I0523 10:57:49.194609  3765 sgd_solver.cpp:106] Iteration 71700, lr = 0.0045
I0523 10:57:58.454661  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_72000.caffemodel
I0523 10:57:58.514681  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_72000.solverstate
I0523 10:57:58.540931  3765 solver.cpp:341] Iteration 72000, Testing net (#0)
I0523 10:59:07.180296  3765 solver.cpp:409]     Test net output #0: accuracy = 0.895271
I0523 10:59:07.180490  3765 solver.cpp:409]     Test net output #1: loss = 0.381691 (* 1 = 0.381691 loss)
I0523 10:59:28.061156  3765 solver.cpp:237] Iteration 72000, loss = 1.08449
I0523 10:59:28.061214  3765 solver.cpp:253]     Train net output #0: loss = 1.08449 (* 1 = 1.08449 loss)
I0523 10:59:28.061229  3765 sgd_solver.cpp:106] Iteration 72000, lr = 0.0045
I0523 10:59:37.362457  3765 solver.cpp:237] Iteration 72300, loss = 0.929314
I0523 10:59:37.362640  3765 solver.cpp:253]     Train net output #0: loss = 0.929314 (* 1 = 0.929314 loss)
I0523 10:59:37.362654  3765 sgd_solver.cpp:106] Iteration 72300, lr = 0.0045
I0523 10:59:46.658107  3765 solver.cpp:237] Iteration 72600, loss = 0.9948
I0523 10:59:46.658143  3765 solver.cpp:253]     Train net output #0: loss = 0.9948 (* 1 = 0.9948 loss)
I0523 10:59:46.658159  3765 sgd_solver.cpp:106] Iteration 72600, lr = 0.0045
I0523 10:59:55.955790  3765 solver.cpp:237] Iteration 72900, loss = 1.44049
I0523 10:59:55.955826  3765 solver.cpp:253]     Train net output #0: loss = 1.44049 (* 1 = 1.44049 loss)
I0523 10:59:55.955842  3765 sgd_solver.cpp:106] Iteration 72900, lr = 0.0045
I0523 11:00:05.253435  3765 solver.cpp:237] Iteration 73200, loss = 1.00366
I0523 11:00:05.253486  3765 solver.cpp:253]     Train net output #0: loss = 1.00366 (* 1 = 1.00366 loss)
I0523 11:00:05.253504  3765 sgd_solver.cpp:106] Iteration 73200, lr = 0.0045
I0523 11:00:14.548142  3765 solver.cpp:237] Iteration 73500, loss = 1.07108
I0523 11:00:14.548303  3765 solver.cpp:253]     Train net output #0: loss = 1.07108 (* 1 = 1.07108 loss)
I0523 11:00:14.548317  3765 sgd_solver.cpp:106] Iteration 73500, lr = 0.0045
I0523 11:00:23.844386  3765 solver.cpp:237] Iteration 73800, loss = 1.12532
I0523 11:00:23.844420  3765 solver.cpp:253]     Train net output #0: loss = 1.12532 (* 1 = 1.12532 loss)
I0523 11:00:23.844439  3765 sgd_solver.cpp:106] Iteration 73800, lr = 0.0045
I0523 11:00:53.977509  3765 solver.cpp:237] Iteration 74100, loss = 0.920159
I0523 11:00:53.977696  3765 solver.cpp:253]     Train net output #0: loss = 0.920159 (* 1 = 0.920159 loss)
I0523 11:00:53.977711  3765 sgd_solver.cpp:106] Iteration 74100, lr = 0.0045
I0523 11:01:03.275540  3765 solver.cpp:237] Iteration 74400, loss = 1.06104
I0523 11:01:03.275574  3765 solver.cpp:253]     Train net output #0: loss = 1.06104 (* 1 = 1.06104 loss)
I0523 11:01:03.275591  3765 sgd_solver.cpp:106] Iteration 74400, lr = 0.0045
I0523 11:01:12.572618  3765 solver.cpp:237] Iteration 74700, loss = 1.22987
I0523 11:01:12.572654  3765 solver.cpp:253]     Train net output #0: loss = 1.22987 (* 1 = 1.22987 loss)
I0523 11:01:12.572670  3765 sgd_solver.cpp:106] Iteration 74700, lr = 0.0045
I0523 11:01:21.842774  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_75000.caffemodel
I0523 11:01:21.904623  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_75000.solverstate
I0523 11:01:21.942392  3765 solver.cpp:237] Iteration 75000, loss = 1.15069
I0523 11:01:21.942446  3765 solver.cpp:253]     Train net output #0: loss = 1.15069 (* 1 = 1.15069 loss)
I0523 11:01:21.942461  3765 sgd_solver.cpp:106] Iteration 75000, lr = 0.0045
I0523 11:01:31.240963  3765 solver.cpp:237] Iteration 75300, loss = 1.4432
I0523 11:01:31.241135  3765 solver.cpp:253]     Train net output #0: loss = 1.4432 (* 1 = 1.4432 loss)
I0523 11:01:31.241149  3765 sgd_solver.cpp:106] Iteration 75300, lr = 0.0045
I0523 11:01:40.533972  3765 solver.cpp:237] Iteration 75600, loss = 1.11969
I0523 11:01:40.534006  3765 solver.cpp:253]     Train net output #0: loss = 1.11969 (* 1 = 1.11969 loss)
I0523 11:01:40.534024  3765 sgd_solver.cpp:106] Iteration 75600, lr = 0.0045
I0523 11:01:49.831209  3765 solver.cpp:237] Iteration 75900, loss = 1.13088
I0523 11:01:49.831259  3765 solver.cpp:253]     Train net output #0: loss = 1.13088 (* 1 = 1.13088 loss)
I0523 11:01:49.831279  3765 sgd_solver.cpp:106] Iteration 75900, lr = 0.0045
I0523 11:02:19.980257  3765 solver.cpp:237] Iteration 76200, loss = 1.11919
I0523 11:02:19.980453  3765 solver.cpp:253]     Train net output #0: loss = 1.11919 (* 1 = 1.11919 loss)
I0523 11:02:19.980469  3765 sgd_solver.cpp:106] Iteration 76200, lr = 0.0045
I0523 11:02:29.274139  3765 solver.cpp:237] Iteration 76500, loss = 1.00819
I0523 11:02:29.274173  3765 solver.cpp:253]     Train net output #0: loss = 1.00819 (* 1 = 1.00819 loss)
I0523 11:02:29.274191  3765 sgd_solver.cpp:106] Iteration 76500, lr = 0.0045
I0523 11:02:38.570075  3765 solver.cpp:237] Iteration 76800, loss = 1.12498
I0523 11:02:38.570127  3765 solver.cpp:253]     Train net output #0: loss = 1.12498 (* 1 = 1.12498 loss)
I0523 11:02:38.570142  3765 sgd_solver.cpp:106] Iteration 76800, lr = 0.0045
I0523 11:02:47.864110  3765 solver.cpp:237] Iteration 77100, loss = 1.03468
I0523 11:02:47.864145  3765 solver.cpp:253]     Train net output #0: loss = 1.03468 (* 1 = 1.03468 loss)
I0523 11:02:47.864162  3765 sgd_solver.cpp:106] Iteration 77100, lr = 0.0045
I0523 11:02:57.160923  3765 solver.cpp:237] Iteration 77400, loss = 0.980277
I0523 11:02:57.161105  3765 solver.cpp:253]     Train net output #0: loss = 0.980277 (* 1 = 0.980277 loss)
I0523 11:02:57.161119  3765 sgd_solver.cpp:106] Iteration 77400, lr = 0.0045
I0523 11:03:06.457007  3765 solver.cpp:237] Iteration 77700, loss = 1.07985
I0523 11:03:06.457041  3765 solver.cpp:253]     Train net output #0: loss = 1.07985 (* 1 = 1.07985 loss)
I0523 11:03:06.457059  3765 sgd_solver.cpp:106] Iteration 77700, lr = 0.0045
I0523 11:03:15.721009  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_78000.caffemodel
I0523 11:03:15.781684  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_78000.solverstate
I0523 11:03:15.807904  3765 solver.cpp:341] Iteration 78000, Testing net (#0)
I0523 11:04:03.594538  3765 solver.cpp:409]     Test net output #0: accuracy = 0.893738
I0523 11:04:03.594734  3765 solver.cpp:409]     Test net output #1: loss = 0.328789 (* 1 = 0.328789 loss)
I0523 11:04:24.440361  3765 solver.cpp:237] Iteration 78000, loss = 1.01003
I0523 11:04:24.440417  3765 solver.cpp:253]     Train net output #0: loss = 1.01003 (* 1 = 1.01003 loss)
I0523 11:04:24.440433  3765 sgd_solver.cpp:106] Iteration 78000, lr = 0.0045
I0523 11:04:33.725487  3765 solver.cpp:237] Iteration 78300, loss = 0.99058
I0523 11:04:33.725656  3765 solver.cpp:253]     Train net output #0: loss = 0.99058 (* 1 = 0.99058 loss)
I0523 11:04:33.725669  3765 sgd_solver.cpp:106] Iteration 78300, lr = 0.0045
I0523 11:04:43.003226  3765 solver.cpp:237] Iteration 78600, loss = 1.20735
I0523 11:04:43.003268  3765 solver.cpp:253]     Train net output #0: loss = 1.20735 (* 1 = 1.20735 loss)
I0523 11:04:43.003289  3765 sgd_solver.cpp:106] Iteration 78600, lr = 0.0045
I0523 11:04:52.277101  3765 solver.cpp:237] Iteration 78900, loss = 1.26935
I0523 11:04:52.277137  3765 solver.cpp:253]     Train net output #0: loss = 1.26935 (* 1 = 1.26935 loss)
I0523 11:04:52.277153  3765 sgd_solver.cpp:106] Iteration 78900, lr = 0.0045
I0523 11:05:01.556951  3765 solver.cpp:237] Iteration 79200, loss = 1.12969
I0523 11:05:01.556998  3765 solver.cpp:253]     Train net output #0: loss = 1.12969 (* 1 = 1.12969 loss)
I0523 11:05:01.557015  3765 sgd_solver.cpp:106] Iteration 79200, lr = 0.0045
I0523 11:05:10.835654  3765 solver.cpp:237] Iteration 79500, loss = 0.982797
I0523 11:05:10.835827  3765 solver.cpp:253]     Train net output #0: loss = 0.982797 (* 1 = 0.982797 loss)
I0523 11:05:10.835841  3765 sgd_solver.cpp:106] Iteration 79500, lr = 0.0045
I0523 11:05:20.114475  3765 solver.cpp:237] Iteration 79800, loss = 0.998643
I0523 11:05:20.114511  3765 solver.cpp:253]     Train net output #0: loss = 0.998643 (* 1 = 0.998643 loss)
I0523 11:05:20.114526  3765 sgd_solver.cpp:106] Iteration 79800, lr = 0.0045
I0523 11:05:50.216250  3765 solver.cpp:237] Iteration 80100, loss = 0.97478
I0523 11:05:50.216441  3765 solver.cpp:253]     Train net output #0: loss = 0.97478 (* 1 = 0.97478 loss)
I0523 11:05:50.216456  3765 sgd_solver.cpp:106] Iteration 80100, lr = 0.0045
I0523 11:05:59.492058  3765 solver.cpp:237] Iteration 80400, loss = 1.06422
I0523 11:05:59.492097  3765 solver.cpp:253]     Train net output #0: loss = 1.06422 (* 1 = 1.06422 loss)
I0523 11:05:59.492117  3765 sgd_solver.cpp:106] Iteration 80400, lr = 0.0045
I0523 11:06:08.769984  3765 solver.cpp:237] Iteration 80700, loss = 1.2456
I0523 11:06:08.770020  3765 solver.cpp:253]     Train net output #0: loss = 1.2456 (* 1 = 1.2456 loss)
I0523 11:06:08.770036  3765 sgd_solver.cpp:106] Iteration 80700, lr = 0.0045
I0523 11:06:18.015061  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_81000.caffemodel
I0523 11:06:18.075381  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_81000.solverstate
I0523 11:06:18.111590  3765 solver.cpp:237] Iteration 81000, loss = 1.0627
I0523 11:06:18.111634  3765 solver.cpp:253]     Train net output #0: loss = 1.0627 (* 1 = 1.0627 loss)
I0523 11:06:18.111652  3765 sgd_solver.cpp:106] Iteration 81000, lr = 0.0045
I0523 11:06:27.390084  3765 solver.cpp:237] Iteration 81300, loss = 1.10895
I0523 11:06:27.390251  3765 solver.cpp:253]     Train net output #0: loss = 1.10895 (* 1 = 1.10895 loss)
I0523 11:06:27.390264  3765 sgd_solver.cpp:106] Iteration 81300, lr = 0.0045
I0523 11:06:36.667721  3765 solver.cpp:237] Iteration 81600, loss = 1.22615
I0523 11:06:36.667755  3765 solver.cpp:253]     Train net output #0: loss = 1.22615 (* 1 = 1.22615 loss)
I0523 11:06:36.667773  3765 sgd_solver.cpp:106] Iteration 81600, lr = 0.0045
I0523 11:06:45.945170  3765 solver.cpp:237] Iteration 81900, loss = 0.988727
I0523 11:06:45.945216  3765 solver.cpp:253]     Train net output #0: loss = 0.988727 (* 1 = 0.988727 loss)
I0523 11:06:45.945236  3765 sgd_solver.cpp:106] Iteration 81900, lr = 0.0045
I0523 11:07:16.112491  3765 solver.cpp:237] Iteration 82200, loss = 1.05719
I0523 11:07:16.112692  3765 solver.cpp:253]     Train net output #0: loss = 1.05719 (* 1 = 1.05719 loss)
I0523 11:07:16.112709  3765 sgd_solver.cpp:106] Iteration 82200, lr = 0.0045
I0523 11:07:25.392117  3765 solver.cpp:237] Iteration 82500, loss = 1.33371
I0523 11:07:25.392153  3765 solver.cpp:253]     Train net output #0: loss = 1.33371 (* 1 = 1.33371 loss)
I0523 11:07:25.392166  3765 sgd_solver.cpp:106] Iteration 82500, lr = 0.0045
I0523 11:07:34.677327  3765 solver.cpp:237] Iteration 82800, loss = 1.24994
I0523 11:07:34.677377  3765 solver.cpp:253]     Train net output #0: loss = 1.24994 (* 1 = 1.24994 loss)
I0523 11:07:34.677395  3765 sgd_solver.cpp:106] Iteration 82800, lr = 0.0045
I0523 11:07:43.969600  3765 solver.cpp:237] Iteration 83100, loss = 1.40783
I0523 11:07:43.969636  3765 solver.cpp:253]     Train net output #0: loss = 1.40783 (* 1 = 1.40783 loss)
I0523 11:07:43.969652  3765 sgd_solver.cpp:106] Iteration 83100, lr = 0.0045
I0523 11:07:53.258716  3765 solver.cpp:237] Iteration 83400, loss = 1.06804
I0523 11:07:53.258882  3765 solver.cpp:253]     Train net output #0: loss = 1.06804 (* 1 = 1.06804 loss)
I0523 11:07:53.258895  3765 sgd_solver.cpp:106] Iteration 83400, lr = 0.0045
I0523 11:08:02.550498  3765 solver.cpp:237] Iteration 83700, loss = 1.11975
I0523 11:08:02.550539  3765 solver.cpp:253]     Train net output #0: loss = 1.11975 (* 1 = 1.11975 loss)
I0523 11:08:02.550556  3765 sgd_solver.cpp:106] Iteration 83700, lr = 0.0045
I0523 11:08:11.811758  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_84000.caffemodel
I0523 11:08:11.871086  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_84000.solverstate
I0523 11:08:11.897408  3765 solver.cpp:341] Iteration 84000, Testing net (#0)
I0523 11:09:20.591317  3765 solver.cpp:409]     Test net output #0: accuracy = 0.897877
I0523 11:09:20.591511  3765 solver.cpp:409]     Test net output #1: loss = 0.320502 (* 1 = 0.320502 loss)
I0523 11:09:41.443800  3765 solver.cpp:237] Iteration 84000, loss = 1.52186
I0523 11:09:41.443858  3765 solver.cpp:253]     Train net output #0: loss = 1.52186 (* 1 = 1.52186 loss)
I0523 11:09:41.443874  3765 sgd_solver.cpp:106] Iteration 84000, lr = 0.0045
I0523 11:09:50.741437  3765 solver.cpp:237] Iteration 84300, loss = 1.14353
I0523 11:09:50.741610  3765 solver.cpp:253]     Train net output #0: loss = 1.14353 (* 1 = 1.14353 loss)
I0523 11:09:50.741622  3765 sgd_solver.cpp:106] Iteration 84300, lr = 0.0045
I0523 11:10:00.040966  3765 solver.cpp:237] Iteration 84600, loss = 1.1462
I0523 11:10:00.041002  3765 solver.cpp:253]     Train net output #0: loss = 1.1462 (* 1 = 1.1462 loss)
I0523 11:10:00.041018  3765 sgd_solver.cpp:106] Iteration 84600, lr = 0.0045
I0523 11:10:09.339254  3765 solver.cpp:237] Iteration 84900, loss = 1.10705
I0523 11:10:09.339298  3765 solver.cpp:253]     Train net output #0: loss = 1.10705 (* 1 = 1.10705 loss)
I0523 11:10:09.339318  3765 sgd_solver.cpp:106] Iteration 84900, lr = 0.0045
I0523 11:10:18.633936  3765 solver.cpp:237] Iteration 85200, loss = 1.10412
I0523 11:10:18.633971  3765 solver.cpp:253]     Train net output #0: loss = 1.10412 (* 1 = 1.10412 loss)
I0523 11:10:18.633985  3765 sgd_solver.cpp:106] Iteration 85200, lr = 0.0045
I0523 11:10:27.932759  3765 solver.cpp:237] Iteration 85500, loss = 1.03592
I0523 11:10:27.932940  3765 solver.cpp:253]     Train net output #0: loss = 1.03592 (* 1 = 1.03592 loss)
I0523 11:10:27.932955  3765 sgd_solver.cpp:106] Iteration 85500, lr = 0.0045
I0523 11:10:37.229861  3765 solver.cpp:237] Iteration 85800, loss = 1.07567
I0523 11:10:37.229895  3765 solver.cpp:253]     Train net output #0: loss = 1.07567 (* 1 = 1.07567 loss)
I0523 11:10:37.229909  3765 sgd_solver.cpp:106] Iteration 85800, lr = 0.0045
I0523 11:11:07.385671  3765 solver.cpp:237] Iteration 86100, loss = 1.3368
I0523 11:11:07.385874  3765 solver.cpp:253]     Train net output #0: loss = 1.3368 (* 1 = 1.3368 loss)
I0523 11:11:07.385890  3765 sgd_solver.cpp:106] Iteration 86100, lr = 0.0045
I0523 11:11:16.682869  3765 solver.cpp:237] Iteration 86400, loss = 1.00465
I0523 11:11:16.682912  3765 solver.cpp:253]     Train net output #0: loss = 1.00465 (* 1 = 1.00465 loss)
I0523 11:11:16.682934  3765 sgd_solver.cpp:106] Iteration 86400, lr = 0.0045
I0523 11:11:25.981680  3765 solver.cpp:237] Iteration 86700, loss = 1.094
I0523 11:11:25.981715  3765 solver.cpp:253]     Train net output #0: loss = 1.094 (* 1 = 1.094 loss)
I0523 11:11:25.981731  3765 sgd_solver.cpp:106] Iteration 86700, lr = 0.0045
I0523 11:11:35.250876  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_87000.caffemodel
I0523 11:11:35.312474  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_87000.solverstate
I0523 11:11:35.350688  3765 solver.cpp:237] Iteration 87000, loss = 1.52546
I0523 11:11:35.350744  3765 solver.cpp:253]     Train net output #0: loss = 1.52546 (* 1 = 1.52546 loss)
I0523 11:11:35.350757  3765 sgd_solver.cpp:106] Iteration 87000, lr = 0.0045
I0523 11:11:44.650559  3765 solver.cpp:237] Iteration 87300, loss = 0.947801
I0523 11:11:44.650745  3765 solver.cpp:253]     Train net output #0: loss = 0.947801 (* 1 = 0.947801 loss)
I0523 11:11:44.650760  3765 sgd_solver.cpp:106] Iteration 87300, lr = 0.0045
I0523 11:11:53.942591  3765 solver.cpp:237] Iteration 87600, loss = 1.27334
I0523 11:11:53.942625  3765 solver.cpp:253]     Train net output #0: loss = 1.27334 (* 1 = 1.27334 loss)
I0523 11:11:53.942644  3765 sgd_solver.cpp:106] Iteration 87600, lr = 0.0045
I0523 11:12:03.242578  3765 solver.cpp:237] Iteration 87900, loss = 1.0209
I0523 11:12:03.242612  3765 solver.cpp:253]     Train net output #0: loss = 1.0209 (* 1 = 1.0209 loss)
I0523 11:12:03.242629  3765 sgd_solver.cpp:106] Iteration 87900, lr = 0.0045
I0523 11:12:33.394244  3765 solver.cpp:237] Iteration 88200, loss = 1.16284
I0523 11:12:33.394434  3765 solver.cpp:253]     Train net output #0: loss = 1.16284 (* 1 = 1.16284 loss)
I0523 11:12:33.394449  3765 sgd_solver.cpp:106] Iteration 88200, lr = 0.0045
I0523 11:12:42.695315  3765 solver.cpp:237] Iteration 88500, loss = 1.27282
I0523 11:12:42.695350  3765 solver.cpp:253]     Train net output #0: loss = 1.27282 (* 1 = 1.27282 loss)
I0523 11:12:42.695368  3765 sgd_solver.cpp:106] Iteration 88500, lr = 0.0045
I0523 11:12:51.995661  3765 solver.cpp:237] Iteration 88800, loss = 1.15475
I0523 11:12:51.995695  3765 solver.cpp:253]     Train net output #0: loss = 1.15475 (* 1 = 1.15475 loss)
I0523 11:12:51.995712  3765 sgd_solver.cpp:106] Iteration 88800, lr = 0.0045
I0523 11:13:01.294416  3765 solver.cpp:237] Iteration 89100, loss = 1.20102
I0523 11:13:01.294461  3765 solver.cpp:253]     Train net output #0: loss = 1.20102 (* 1 = 1.20102 loss)
I0523 11:13:01.294481  3765 sgd_solver.cpp:106] Iteration 89100, lr = 0.0045
I0523 11:13:10.590361  3765 solver.cpp:237] Iteration 89400, loss = 0.992371
I0523 11:13:10.590534  3765 solver.cpp:253]     Train net output #0: loss = 0.992371 (* 1 = 0.992371 loss)
I0523 11:13:10.590550  3765 sgd_solver.cpp:106] Iteration 89400, lr = 0.0045
I0523 11:13:19.890079  3765 solver.cpp:237] Iteration 89700, loss = 1.39509
I0523 11:13:19.890112  3765 solver.cpp:253]     Train net output #0: loss = 1.39509 (* 1 = 1.39509 loss)
I0523 11:13:19.890130  3765 sgd_solver.cpp:106] Iteration 89700, lr = 0.0045
I0523 11:13:29.157830  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_90000.caffemodel
I0523 11:13:29.219624  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_90000.solverstate
I0523 11:13:29.247889  3765 solver.cpp:341] Iteration 90000, Testing net (#0)
I0523 11:14:16.704138  3765 solver.cpp:409]     Test net output #0: accuracy = 0.899503
I0523 11:14:16.704336  3765 solver.cpp:409]     Test net output #1: loss = 0.321392 (* 1 = 0.321392 loss)
I0523 11:14:37.590975  3765 solver.cpp:237] Iteration 90000, loss = 1.12637
I0523 11:14:37.591032  3765 solver.cpp:253]     Train net output #0: loss = 1.12637 (* 1 = 1.12637 loss)
I0523 11:14:37.591047  3765 sgd_solver.cpp:106] Iteration 90000, lr = 0.0045
I0523 11:14:46.886349  3765 solver.cpp:237] Iteration 90300, loss = 1.08499
I0523 11:14:46.886533  3765 solver.cpp:253]     Train net output #0: loss = 1.08499 (* 1 = 1.08499 loss)
I0523 11:14:46.886546  3765 sgd_solver.cpp:106] Iteration 90300, lr = 0.0045
I0523 11:14:56.186084  3765 solver.cpp:237] Iteration 90600, loss = 1.06351
I0523 11:14:56.186117  3765 solver.cpp:253]     Train net output #0: loss = 1.06351 (* 1 = 1.06351 loss)
I0523 11:14:56.186131  3765 sgd_solver.cpp:106] Iteration 90600, lr = 0.0045
I0523 11:15:05.482256  3765 solver.cpp:237] Iteration 90900, loss = 1.15614
I0523 11:15:05.482309  3765 solver.cpp:253]     Train net output #0: loss = 1.15614 (* 1 = 1.15614 loss)
I0523 11:15:05.482324  3765 sgd_solver.cpp:106] Iteration 90900, lr = 0.0045
I0523 11:15:14.777684  3765 solver.cpp:237] Iteration 91200, loss = 1.14582
I0523 11:15:14.777719  3765 solver.cpp:253]     Train net output #0: loss = 1.14582 (* 1 = 1.14582 loss)
I0523 11:15:14.777736  3765 sgd_solver.cpp:106] Iteration 91200, lr = 0.0045
I0523 11:15:24.078675  3765 solver.cpp:237] Iteration 91500, loss = 1.01533
I0523 11:15:24.078842  3765 solver.cpp:253]     Train net output #0: loss = 1.01533 (* 1 = 1.01533 loss)
I0523 11:15:24.078855  3765 sgd_solver.cpp:106] Iteration 91500, lr = 0.0045
I0523 11:15:33.377856  3765 solver.cpp:237] Iteration 91800, loss = 1.38914
I0523 11:15:33.377900  3765 solver.cpp:253]     Train net output #0: loss = 1.38914 (* 1 = 1.38914 loss)
I0523 11:15:33.377918  3765 sgd_solver.cpp:106] Iteration 91800, lr = 0.0045
I0523 11:16:03.564193  3765 solver.cpp:237] Iteration 92100, loss = 1.03049
I0523 11:16:03.564383  3765 solver.cpp:253]     Train net output #0: loss = 1.03049 (* 1 = 1.03049 loss)
I0523 11:16:03.564399  3765 sgd_solver.cpp:106] Iteration 92100, lr = 0.0045
I0523 11:16:12.863281  3765 solver.cpp:237] Iteration 92400, loss = 0.983222
I0523 11:16:12.863317  3765 solver.cpp:253]     Train net output #0: loss = 0.983222 (* 1 = 0.983222 loss)
I0523 11:16:12.863334  3765 sgd_solver.cpp:106] Iteration 92400, lr = 0.0045
I0523 11:16:22.158360  3765 solver.cpp:237] Iteration 92700, loss = 1.33767
I0523 11:16:22.158402  3765 solver.cpp:253]     Train net output #0: loss = 1.33767 (* 1 = 1.33767 loss)
I0523 11:16:22.158421  3765 sgd_solver.cpp:106] Iteration 92700, lr = 0.0045
I0523 11:16:31.427215  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_93000.caffemodel
I0523 11:16:31.487020  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_93000.solverstate
I0523 11:16:31.523000  3765 solver.cpp:237] Iteration 93000, loss = 1.20645
I0523 11:16:31.523047  3765 solver.cpp:253]     Train net output #0: loss = 1.20645 (* 1 = 1.20645 loss)
I0523 11:16:31.523062  3765 sgd_solver.cpp:106] Iteration 93000, lr = 0.0045
I0523 11:16:40.821454  3765 solver.cpp:237] Iteration 93300, loss = 1.45249
I0523 11:16:40.821632  3765 solver.cpp:253]     Train net output #0: loss = 1.45249 (* 1 = 1.45249 loss)
I0523 11:16:40.821646  3765 sgd_solver.cpp:106] Iteration 93300, lr = 0.0045
I0523 11:16:50.119947  3765 solver.cpp:237] Iteration 93600, loss = 1.11813
I0523 11:16:50.119982  3765 solver.cpp:253]     Train net output #0: loss = 1.11813 (* 1 = 1.11813 loss)
I0523 11:16:50.119998  3765 sgd_solver.cpp:106] Iteration 93600, lr = 0.0045
I0523 11:16:59.417582  3765 solver.cpp:237] Iteration 93900, loss = 0.846179
I0523 11:16:59.417618  3765 solver.cpp:253]     Train net output #0: loss = 0.846179 (* 1 = 0.846179 loss)
I0523 11:16:59.417634  3765 sgd_solver.cpp:106] Iteration 93900, lr = 0.0045
I0523 11:17:29.619607  3765 solver.cpp:237] Iteration 94200, loss = 0.860313
I0523 11:17:29.619810  3765 solver.cpp:253]     Train net output #0: loss = 0.860313 (* 1 = 0.860313 loss)
I0523 11:17:29.619825  3765 sgd_solver.cpp:106] Iteration 94200, lr = 0.0045
I0523 11:17:38.917505  3765 solver.cpp:237] Iteration 94500, loss = 1.12738
I0523 11:17:38.917543  3765 solver.cpp:253]     Train net output #0: loss = 1.12738 (* 1 = 1.12738 loss)
I0523 11:17:38.917562  3765 sgd_solver.cpp:106] Iteration 94500, lr = 0.0045
I0523 11:17:48.219156  3765 solver.cpp:237] Iteration 94800, loss = 1.44841
I0523 11:17:48.219192  3765 solver.cpp:253]     Train net output #0: loss = 1.44841 (* 1 = 1.44841 loss)
I0523 11:17:48.219208  3765 sgd_solver.cpp:106] Iteration 94800, lr = 0.0045
I0523 11:17:57.515700  3765 solver.cpp:237] Iteration 95100, loss = 1.25384
I0523 11:17:57.515736  3765 solver.cpp:253]     Train net output #0: loss = 1.25384 (* 1 = 1.25384 loss)
I0523 11:17:57.515758  3765 sgd_solver.cpp:106] Iteration 95100, lr = 0.0045
I0523 11:18:06.812520  3765 solver.cpp:237] Iteration 95400, loss = 1.09053
I0523 11:18:06.812695  3765 solver.cpp:253]     Train net output #0: loss = 1.09053 (* 1 = 1.09053 loss)
I0523 11:18:06.812710  3765 sgd_solver.cpp:106] Iteration 95400, lr = 0.0045
I0523 11:18:16.109993  3765 solver.cpp:237] Iteration 95700, loss = 1.40903
I0523 11:18:16.110028  3765 solver.cpp:253]     Train net output #0: loss = 1.40903 (* 1 = 1.40903 loss)
I0523 11:18:16.110044  3765 sgd_solver.cpp:106] Iteration 95700, lr = 0.0045
I0523 11:18:25.378610  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_96000.caffemodel
I0523 11:18:25.438067  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_96000.solverstate
I0523 11:18:25.464223  3765 solver.cpp:341] Iteration 96000, Testing net (#0)
I0523 11:19:34.207502  3765 solver.cpp:409]     Test net output #0: accuracy = 0.899209
I0523 11:19:34.207695  3765 solver.cpp:409]     Test net output #1: loss = 0.321096 (* 1 = 0.321096 loss)
I0523 11:19:55.132493  3765 solver.cpp:237] Iteration 96000, loss = 1.10422
I0523 11:19:55.132547  3765 solver.cpp:253]     Train net output #0: loss = 1.10422 (* 1 = 1.10422 loss)
I0523 11:19:55.132563  3765 sgd_solver.cpp:106] Iteration 96000, lr = 0.0045
I0523 11:20:04.430959  3765 solver.cpp:237] Iteration 96300, loss = 1.01899
I0523 11:20:04.431149  3765 solver.cpp:253]     Train net output #0: loss = 1.01899 (* 1 = 1.01899 loss)
I0523 11:20:04.431164  3765 sgd_solver.cpp:106] Iteration 96300, lr = 0.0045
I0523 11:20:13.730073  3765 solver.cpp:237] Iteration 96600, loss = 0.896463
I0523 11:20:13.730108  3765 solver.cpp:253]     Train net output #0: loss = 0.896463 (* 1 = 0.896463 loss)
I0523 11:20:13.730125  3765 sgd_solver.cpp:106] Iteration 96600, lr = 0.0045
I0523 11:20:23.032995  3765 solver.cpp:237] Iteration 96900, loss = 1.21162
I0523 11:20:23.033031  3765 solver.cpp:253]     Train net output #0: loss = 1.21162 (* 1 = 1.21162 loss)
I0523 11:20:23.033044  3765 sgd_solver.cpp:106] Iteration 96900, lr = 0.0045
I0523 11:20:32.335892  3765 solver.cpp:237] Iteration 97200, loss = 1.36398
I0523 11:20:32.335940  3765 solver.cpp:253]     Train net output #0: loss = 1.36398 (* 1 = 1.36398 loss)
I0523 11:20:32.335958  3765 sgd_solver.cpp:106] Iteration 97200, lr = 0.0045
I0523 11:20:41.633090  3765 solver.cpp:237] Iteration 97500, loss = 1.2237
I0523 11:20:41.633268  3765 solver.cpp:253]     Train net output #0: loss = 1.2237 (* 1 = 1.2237 loss)
I0523 11:20:41.633282  3765 sgd_solver.cpp:106] Iteration 97500, lr = 0.0045
I0523 11:20:50.934507  3765 solver.cpp:237] Iteration 97800, loss = 1.09789
I0523 11:20:50.934554  3765 solver.cpp:253]     Train net output #0: loss = 1.09789 (* 1 = 1.09789 loss)
I0523 11:20:50.934571  3765 sgd_solver.cpp:106] Iteration 97800, lr = 0.0045
I0523 11:21:21.128432  3765 solver.cpp:237] Iteration 98100, loss = 0.983391
I0523 11:21:21.128630  3765 solver.cpp:253]     Train net output #0: loss = 0.983391 (* 1 = 0.983391 loss)
I0523 11:21:21.128646  3765 sgd_solver.cpp:106] Iteration 98100, lr = 0.0045
I0523 11:21:30.430348  3765 solver.cpp:237] Iteration 98400, loss = 0.919045
I0523 11:21:30.430383  3765 solver.cpp:253]     Train net output #0: loss = 0.919045 (* 1 = 0.919045 loss)
I0523 11:21:30.430400  3765 sgd_solver.cpp:106] Iteration 98400, lr = 0.0045
I0523 11:21:39.733436  3765 solver.cpp:237] Iteration 98700, loss = 1.33586
I0523 11:21:39.733471  3765 solver.cpp:253]     Train net output #0: loss = 1.33586 (* 1 = 1.33586 loss)
I0523 11:21:39.733489  3765 sgd_solver.cpp:106] Iteration 98700, lr = 0.0045
I0523 11:21:49.005040  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_99000.caffemodel
I0523 11:21:49.064509  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_99000.solverstate
I0523 11:21:49.100594  3765 solver.cpp:237] Iteration 99000, loss = 1.1044
I0523 11:21:49.100642  3765 solver.cpp:253]     Train net output #0: loss = 1.1044 (* 1 = 1.1044 loss)
I0523 11:21:49.100656  3765 sgd_solver.cpp:106] Iteration 99000, lr = 0.0045
I0523 11:21:58.399751  3765 solver.cpp:237] Iteration 99300, loss = 1.04968
I0523 11:21:58.399933  3765 solver.cpp:253]     Train net output #0: loss = 1.04968 (* 1 = 1.04968 loss)
I0523 11:21:58.399947  3765 sgd_solver.cpp:106] Iteration 99300, lr = 0.0045
I0523 11:22:07.703166  3765 solver.cpp:237] Iteration 99600, loss = 1.18648
I0523 11:22:07.703214  3765 solver.cpp:253]     Train net output #0: loss = 1.18648 (* 1 = 1.18648 loss)
I0523 11:22:07.703233  3765 sgd_solver.cpp:106] Iteration 99600, lr = 0.0045
I0523 11:22:17.004465  3765 solver.cpp:237] Iteration 99900, loss = 1.33261
I0523 11:22:17.004501  3765 solver.cpp:253]     Train net output #0: loss = 1.33261 (* 1 = 1.33261 loss)
I0523 11:22:17.004519  3765 sgd_solver.cpp:106] Iteration 99900, lr = 0.0045
I0523 11:22:47.199612  3765 solver.cpp:237] Iteration 100200, loss = 1.04192
I0523 11:22:47.199805  3765 solver.cpp:253]     Train net output #0: loss = 1.04192 (* 1 = 1.04192 loss)
I0523 11:22:47.199821  3765 sgd_solver.cpp:106] Iteration 100200, lr = 0.0045
I0523 11:22:56.507028  3765 solver.cpp:237] Iteration 100500, loss = 0.956154
I0523 11:22:56.507074  3765 solver.cpp:253]     Train net output #0: loss = 0.956154 (* 1 = 0.956154 loss)
I0523 11:22:56.507094  3765 sgd_solver.cpp:106] Iteration 100500, lr = 0.0045
I0523 11:23:05.805600  3765 solver.cpp:237] Iteration 100800, loss = 1.23234
I0523 11:23:05.805635  3765 solver.cpp:253]     Train net output #0: loss = 1.23234 (* 1 = 1.23234 loss)
I0523 11:23:05.805649  3765 sgd_solver.cpp:106] Iteration 100800, lr = 0.0045
I0523 11:23:15.108531  3765 solver.cpp:237] Iteration 101100, loss = 0.904764
I0523 11:23:15.108567  3765 solver.cpp:253]     Train net output #0: loss = 0.904764 (* 1 = 0.904764 loss)
I0523 11:23:15.108583  3765 sgd_solver.cpp:106] Iteration 101100, lr = 0.0045
I0523 11:23:24.414085  3765 solver.cpp:237] Iteration 101400, loss = 1.29099
I0523 11:23:24.414283  3765 solver.cpp:253]     Train net output #0: loss = 1.29099 (* 1 = 1.29099 loss)
I0523 11:23:24.414297  3765 sgd_solver.cpp:106] Iteration 101400, lr = 0.0045
I0523 11:23:33.714118  3765 solver.cpp:237] Iteration 101700, loss = 0.875494
I0523 11:23:33.714153  3765 solver.cpp:253]     Train net output #0: loss = 0.875494 (* 1 = 0.875494 loss)
I0523 11:23:33.714166  3765 sgd_solver.cpp:106] Iteration 101700, lr = 0.0045
I0523 11:23:42.982430  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_102000.caffemodel
I0523 11:23:43.041570  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_102000.solverstate
I0523 11:23:43.067028  3765 solver.cpp:341] Iteration 102000, Testing net (#0)
I0523 11:24:30.920871  3765 solver.cpp:409]     Test net output #0: accuracy = 0.898317
I0523 11:24:30.921073  3765 solver.cpp:409]     Test net output #1: loss = 0.3417 (* 1 = 0.3417 loss)
I0523 11:24:51.807003  3765 solver.cpp:237] Iteration 102000, loss = 1.02472
I0523 11:24:51.807060  3765 solver.cpp:253]     Train net output #0: loss = 1.02472 (* 1 = 1.02472 loss)
I0523 11:24:51.807076  3765 sgd_solver.cpp:106] Iteration 102000, lr = 0.0045
I0523 11:25:01.104555  3765 solver.cpp:237] Iteration 102300, loss = 1.30956
I0523 11:25:01.104744  3765 solver.cpp:253]     Train net output #0: loss = 1.30956 (* 1 = 1.30956 loss)
I0523 11:25:01.104759  3765 sgd_solver.cpp:106] Iteration 102300, lr = 0.0045
I0523 11:25:10.398799  3765 solver.cpp:237] Iteration 102600, loss = 1.07927
I0523 11:25:10.398834  3765 solver.cpp:253]     Train net output #0: loss = 1.07927 (* 1 = 1.07927 loss)
I0523 11:25:10.398849  3765 sgd_solver.cpp:106] Iteration 102600, lr = 0.0045
I0523 11:25:19.694088  3765 solver.cpp:237] Iteration 102900, loss = 1.01628
I0523 11:25:19.694123  3765 solver.cpp:253]     Train net output #0: loss = 1.01628 (* 1 = 1.01628 loss)
I0523 11:25:19.694140  3765 sgd_solver.cpp:106] Iteration 102900, lr = 0.0045
I0523 11:25:28.989948  3765 solver.cpp:237] Iteration 103200, loss = 1.2546
I0523 11:25:28.989995  3765 solver.cpp:253]     Train net output #0: loss = 1.2546 (* 1 = 1.2546 loss)
I0523 11:25:28.990011  3765 sgd_solver.cpp:106] Iteration 103200, lr = 0.0045
I0523 11:25:38.285609  3765 solver.cpp:237] Iteration 103500, loss = 1.00002
I0523 11:25:38.285778  3765 solver.cpp:253]     Train net output #0: loss = 1.00002 (* 1 = 1.00002 loss)
I0523 11:25:38.285791  3765 sgd_solver.cpp:106] Iteration 103500, lr = 0.0045
I0523 11:25:47.580514  3765 solver.cpp:237] Iteration 103800, loss = 0.930361
I0523 11:25:47.580549  3765 solver.cpp:253]     Train net output #0: loss = 0.930361 (* 1 = 0.930361 loss)
I0523 11:25:47.580564  3765 sgd_solver.cpp:106] Iteration 103800, lr = 0.0045
I0523 11:26:17.772073  3765 solver.cpp:237] Iteration 104100, loss = 1.01711
I0523 11:26:17.772267  3765 solver.cpp:253]     Train net output #0: loss = 1.01711 (* 1 = 1.01711 loss)
I0523 11:26:17.772281  3765 sgd_solver.cpp:106] Iteration 104100, lr = 0.0045
I0523 11:26:27.065871  3765 solver.cpp:237] Iteration 104400, loss = 0.985674
I0523 11:26:27.065906  3765 solver.cpp:253]     Train net output #0: loss = 0.985674 (* 1 = 0.985674 loss)
I0523 11:26:27.065922  3765 sgd_solver.cpp:106] Iteration 104400, lr = 0.0045
I0523 11:26:36.364145  3765 solver.cpp:237] Iteration 104700, loss = 1.22377
I0523 11:26:36.364181  3765 solver.cpp:253]     Train net output #0: loss = 1.22377 (* 1 = 1.22377 loss)
I0523 11:26:36.364197  3765 sgd_solver.cpp:106] Iteration 104700, lr = 0.0045
I0523 11:26:45.629365  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_105000.caffemodel
I0523 11:26:45.690716  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_105000.solverstate
I0523 11:26:45.727483  3765 solver.cpp:237] Iteration 105000, loss = 0.996544
I0523 11:26:45.727535  3765 solver.cpp:253]     Train net output #0: loss = 0.996544 (* 1 = 0.996544 loss)
I0523 11:26:45.727555  3765 sgd_solver.cpp:106] Iteration 105000, lr = 0.0045
I0523 11:26:55.024338  3765 solver.cpp:237] Iteration 105300, loss = 1.09171
I0523 11:26:55.024523  3765 solver.cpp:253]     Train net output #0: loss = 1.09171 (* 1 = 1.09171 loss)
I0523 11:26:55.024538  3765 sgd_solver.cpp:106] Iteration 105300, lr = 0.0045
I0523 11:27:04.316704  3765 solver.cpp:237] Iteration 105600, loss = 1.16361
I0523 11:27:04.316740  3765 solver.cpp:253]     Train net output #0: loss = 1.16361 (* 1 = 1.16361 loss)
I0523 11:27:04.316756  3765 sgd_solver.cpp:106] Iteration 105600, lr = 0.0045
I0523 11:27:13.614692  3765 solver.cpp:237] Iteration 105900, loss = 0.977661
I0523 11:27:13.614737  3765 solver.cpp:253]     Train net output #0: loss = 0.977661 (* 1 = 0.977661 loss)
I0523 11:27:13.614756  3765 sgd_solver.cpp:106] Iteration 105900, lr = 0.0045
I0523 11:27:43.833423  3765 solver.cpp:237] Iteration 106200, loss = 1.04506
I0523 11:27:43.833621  3765 solver.cpp:253]     Train net output #0: loss = 1.04506 (* 1 = 1.04506 loss)
I0523 11:27:43.833636  3765 sgd_solver.cpp:106] Iteration 106200, lr = 0.0045
I0523 11:27:53.128093  3765 solver.cpp:237] Iteration 106500, loss = 1.41349
I0523 11:27:53.128126  3765 solver.cpp:253]     Train net output #0: loss = 1.41349 (* 1 = 1.41349 loss)
I0523 11:27:53.128141  3765 sgd_solver.cpp:106] Iteration 106500, lr = 0.0045
I0523 11:28:02.422991  3765 solver.cpp:237] Iteration 106800, loss = 0.882714
I0523 11:28:02.423043  3765 solver.cpp:253]     Train net output #0: loss = 0.882714 (* 1 = 0.882714 loss)
I0523 11:28:02.423061  3765 sgd_solver.cpp:106] Iteration 106800, lr = 0.0045
I0523 11:28:11.717799  3765 solver.cpp:237] Iteration 107100, loss = 1.46055
I0523 11:28:11.717834  3765 solver.cpp:253]     Train net output #0: loss = 1.46055 (* 1 = 1.46055 loss)
I0523 11:28:11.717849  3765 sgd_solver.cpp:106] Iteration 107100, lr = 0.0045
I0523 11:28:21.011006  3765 solver.cpp:237] Iteration 107400, loss = 1.11607
I0523 11:28:21.011188  3765 solver.cpp:253]     Train net output #0: loss = 1.11607 (* 1 = 1.11607 loss)
I0523 11:28:21.011203  3765 sgd_solver.cpp:106] Iteration 107400, lr = 0.0045
I0523 11:28:30.305383  3765 solver.cpp:237] Iteration 107700, loss = 1.08613
I0523 11:28:30.305418  3765 solver.cpp:253]     Train net output #0: loss = 1.08613 (* 1 = 1.08613 loss)
I0523 11:28:30.305435  3765 sgd_solver.cpp:106] Iteration 107700, lr = 0.0045
I0523 11:28:39.571188  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_108000.caffemodel
I0523 11:28:39.642216  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_108000.solverstate
I0523 11:28:39.670159  3765 solver.cpp:341] Iteration 108000, Testing net (#0)
I0523 11:29:48.433917  3765 solver.cpp:409]     Test net output #0: accuracy = 0.898577
I0523 11:29:48.434114  3765 solver.cpp:409]     Test net output #1: loss = 0.317658 (* 1 = 0.317658 loss)
I0523 11:30:09.346367  3765 solver.cpp:237] Iteration 108000, loss = 0.909021
I0523 11:30:09.346424  3765 solver.cpp:253]     Train net output #0: loss = 0.909021 (* 1 = 0.909021 loss)
I0523 11:30:09.346441  3765 sgd_solver.cpp:106] Iteration 108000, lr = 0.0045
I0523 11:30:18.641381  3765 solver.cpp:237] Iteration 108300, loss = 1.15435
I0523 11:30:18.641559  3765 solver.cpp:253]     Train net output #0: loss = 1.15435 (* 1 = 1.15435 loss)
I0523 11:30:18.641573  3765 sgd_solver.cpp:106] Iteration 108300, lr = 0.0045
I0523 11:30:27.936463  3765 solver.cpp:237] Iteration 108600, loss = 1.37288
I0523 11:30:27.936512  3765 solver.cpp:253]     Train net output #0: loss = 1.37288 (* 1 = 1.37288 loss)
I0523 11:30:27.936533  3765 sgd_solver.cpp:106] Iteration 108600, lr = 0.0045
I0523 11:30:37.229302  3765 solver.cpp:237] Iteration 108900, loss = 1.13576
I0523 11:30:37.229339  3765 solver.cpp:253]     Train net output #0: loss = 1.13576 (* 1 = 1.13576 loss)
I0523 11:30:37.229356  3765 sgd_solver.cpp:106] Iteration 108900, lr = 0.0045
I0523 11:30:46.525786  3765 solver.cpp:237] Iteration 109200, loss = 1.26168
I0523 11:30:46.525821  3765 solver.cpp:253]     Train net output #0: loss = 1.26168 (* 1 = 1.26168 loss)
I0523 11:30:46.525836  3765 sgd_solver.cpp:106] Iteration 109200, lr = 0.0045
I0523 11:30:55.823830  3765 solver.cpp:237] Iteration 109500, loss = 1.12022
I0523 11:30:55.824038  3765 solver.cpp:253]     Train net output #0: loss = 1.12022 (* 1 = 1.12022 loss)
I0523 11:30:55.824053  3765 sgd_solver.cpp:106] Iteration 109500, lr = 0.0045
I0523 11:31:05.121613  3765 solver.cpp:237] Iteration 109800, loss = 0.953862
I0523 11:31:05.121647  3765 solver.cpp:253]     Train net output #0: loss = 0.953862 (* 1 = 0.953862 loss)
I0523 11:31:05.121665  3765 sgd_solver.cpp:106] Iteration 109800, lr = 0.0045
I0523 11:31:35.349503  3765 solver.cpp:237] Iteration 110100, loss = 0.964603
I0523 11:31:35.349705  3765 solver.cpp:253]     Train net output #0: loss = 0.964603 (* 1 = 0.964603 loss)
I0523 11:31:35.349722  3765 sgd_solver.cpp:106] Iteration 110100, lr = 0.0045
I0523 11:31:44.649015  3765 solver.cpp:237] Iteration 110400, loss = 1.07567
I0523 11:31:44.649063  3765 solver.cpp:253]     Train net output #0: loss = 1.07567 (* 1 = 1.07567 loss)
I0523 11:31:44.649080  3765 sgd_solver.cpp:106] Iteration 110400, lr = 0.0045
I0523 11:31:53.941468  3765 solver.cpp:237] Iteration 110700, loss = 1.1917
I0523 11:31:53.941504  3765 solver.cpp:253]     Train net output #0: loss = 1.1917 (* 1 = 1.1917 loss)
I0523 11:31:53.941521  3765 sgd_solver.cpp:106] Iteration 110700, lr = 0.0045
I0523 11:32:03.205533  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_111000.caffemodel
I0523 11:32:03.265808  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_111000.solverstate
I0523 11:32:03.301031  3765 solver.cpp:237] Iteration 111000, loss = 1.26434
I0523 11:32:03.301080  3765 solver.cpp:253]     Train net output #0: loss = 1.26434 (* 1 = 1.26434 loss)
I0523 11:32:03.301095  3765 sgd_solver.cpp:106] Iteration 111000, lr = 0.0045
I0523 11:32:12.597587  3765 solver.cpp:237] Iteration 111300, loss = 1.11306
I0523 11:32:12.597779  3765 solver.cpp:253]     Train net output #0: loss = 1.11306 (* 1 = 1.11306 loss)
I0523 11:32:12.597791  3765 sgd_solver.cpp:106] Iteration 111300, lr = 0.0045
I0523 11:32:21.894337  3765 solver.cpp:237] Iteration 111600, loss = 1.395
I0523 11:32:21.894371  3765 solver.cpp:253]     Train net output #0: loss = 1.395 (* 1 = 1.395 loss)
I0523 11:32:21.894389  3765 sgd_solver.cpp:106] Iteration 111600, lr = 0.0045
I0523 11:32:31.194435  3765 solver.cpp:237] Iteration 111900, loss = 1.0592
I0523 11:32:31.194489  3765 solver.cpp:253]     Train net output #0: loss = 1.0592 (* 1 = 1.0592 loss)
I0523 11:32:31.194505  3765 sgd_solver.cpp:106] Iteration 111900, lr = 0.0045
I0523 11:33:01.392709  3765 solver.cpp:237] Iteration 112200, loss = 1.04532
I0523 11:33:01.392905  3765 solver.cpp:253]     Train net output #0: loss = 1.04532 (* 1 = 1.04532 loss)
I0523 11:33:01.392921  3765 sgd_solver.cpp:106] Iteration 112200, lr = 0.0045
I0523 11:33:10.691997  3765 solver.cpp:237] Iteration 112500, loss = 1.12173
I0523 11:33:10.692030  3765 solver.cpp:253]     Train net output #0: loss = 1.12173 (* 1 = 1.12173 loss)
I0523 11:33:10.692049  3765 sgd_solver.cpp:106] Iteration 112500, lr = 0.0045
I0523 11:33:19.989032  3765 solver.cpp:237] Iteration 112800, loss = 1.3124
I0523 11:33:19.989066  3765 solver.cpp:253]     Train net output #0: loss = 1.3124 (* 1 = 1.3124 loss)
I0523 11:33:19.989083  3765 sgd_solver.cpp:106] Iteration 112800, lr = 0.0045
I0523 11:33:29.285979  3765 solver.cpp:237] Iteration 113100, loss = 1.33937
I0523 11:33:29.286025  3765 solver.cpp:253]     Train net output #0: loss = 1.33937 (* 1 = 1.33937 loss)
I0523 11:33:29.286041  3765 sgd_solver.cpp:106] Iteration 113100, lr = 0.0045
I0523 11:33:38.586174  3765 solver.cpp:237] Iteration 113400, loss = 1.1042
I0523 11:33:38.586355  3765 solver.cpp:253]     Train net output #0: loss = 1.1042 (* 1 = 1.1042 loss)
I0523 11:33:38.586369  3765 sgd_solver.cpp:106] Iteration 113400, lr = 0.0045
I0523 11:33:47.882699  3765 solver.cpp:237] Iteration 113700, loss = 0.937505
I0523 11:33:47.882750  3765 solver.cpp:253]     Train net output #0: loss = 0.937504 (* 1 = 0.937504 loss)
I0523 11:33:47.882769  3765 sgd_solver.cpp:106] Iteration 113700, lr = 0.0045
I0523 11:33:57.145848  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_114000.caffemodel
I0523 11:33:57.205464  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_114000.solverstate
I0523 11:33:57.230723  3765 solver.cpp:341] Iteration 114000, Testing net (#0)
I0523 11:34:44.696449  3765 solver.cpp:409]     Test net output #0: accuracy = 0.900789
I0523 11:34:44.696648  3765 solver.cpp:409]     Test net output #1: loss = 0.308474 (* 1 = 0.308474 loss)
I0523 11:35:05.600838  3765 solver.cpp:237] Iteration 114000, loss = 1.23308
I0523 11:35:05.600894  3765 solver.cpp:253]     Train net output #0: loss = 1.23308 (* 1 = 1.23308 loss)
I0523 11:35:05.600909  3765 sgd_solver.cpp:106] Iteration 114000, lr = 0.0045
I0523 11:35:14.896306  3765 solver.cpp:237] Iteration 114300, loss = 1.34326
I0523 11:35:14.896481  3765 solver.cpp:253]     Train net output #0: loss = 1.34326 (* 1 = 1.34326 loss)
I0523 11:35:14.896495  3765 sgd_solver.cpp:106] Iteration 114300, lr = 0.0045
I0523 11:35:24.193156  3765 solver.cpp:237] Iteration 114600, loss = 0.901793
I0523 11:35:24.193191  3765 solver.cpp:253]     Train net output #0: loss = 0.901793 (* 1 = 0.901793 loss)
I0523 11:35:24.193207  3765 sgd_solver.cpp:106] Iteration 114600, lr = 0.0045
I0523 11:35:33.487117  3765 solver.cpp:237] Iteration 114900, loss = 0.952574
I0523 11:35:33.487160  3765 solver.cpp:253]     Train net output #0: loss = 0.952574 (* 1 = 0.952574 loss)
I0523 11:35:33.487179  3765 sgd_solver.cpp:106] Iteration 114900, lr = 0.0045
I0523 11:35:42.783794  3765 solver.cpp:237] Iteration 115200, loss = 1.35253
I0523 11:35:42.783829  3765 solver.cpp:253]     Train net output #0: loss = 1.35253 (* 1 = 1.35253 loss)
I0523 11:35:42.783843  3765 sgd_solver.cpp:106] Iteration 115200, lr = 0.0045
I0523 11:35:52.079532  3765 solver.cpp:237] Iteration 115500, loss = 1.065
I0523 11:35:52.079718  3765 solver.cpp:253]     Train net output #0: loss = 1.065 (* 1 = 1.065 loss)
I0523 11:35:52.079732  3765 sgd_solver.cpp:106] Iteration 115500, lr = 0.0045
I0523 11:36:01.374689  3765 solver.cpp:237] Iteration 115800, loss = 1.00254
I0523 11:36:01.374723  3765 solver.cpp:253]     Train net output #0: loss = 1.00254 (* 1 = 1.00254 loss)
I0523 11:36:01.374743  3765 sgd_solver.cpp:106] Iteration 115800, lr = 0.0045
I0523 11:36:31.557117  3765 solver.cpp:237] Iteration 116100, loss = 1.17816
I0523 11:36:31.557317  3765 solver.cpp:253]     Train net output #0: loss = 1.17816 (* 1 = 1.17816 loss)
I0523 11:36:31.557332  3765 sgd_solver.cpp:106] Iteration 116100, lr = 0.0045
I0523 11:36:40.856230  3765 solver.cpp:237] Iteration 116400, loss = 1.16848
I0523 11:36:40.856281  3765 solver.cpp:253]     Train net output #0: loss = 1.16848 (* 1 = 1.16848 loss)
I0523 11:36:40.856298  3765 sgd_solver.cpp:106] Iteration 116400, lr = 0.0045
I0523 11:36:50.151116  3765 solver.cpp:237] Iteration 116700, loss = 1.15932
I0523 11:36:50.151151  3765 solver.cpp:253]     Train net output #0: loss = 1.15932 (* 1 = 1.15932 loss)
I0523 11:36:50.151165  3765 sgd_solver.cpp:106] Iteration 116700, lr = 0.0045
I0523 11:36:59.418041  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_117000.caffemodel
I0523 11:36:59.477383  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_117000.solverstate
I0523 11:36:59.512665  3765 solver.cpp:237] Iteration 117000, loss = 1.22084
I0523 11:36:59.512712  3765 solver.cpp:253]     Train net output #0: loss = 1.22084 (* 1 = 1.22084 loss)
I0523 11:36:59.512727  3765 sgd_solver.cpp:106] Iteration 117000, lr = 0.0045
I0523 11:37:08.808332  3765 solver.cpp:237] Iteration 117300, loss = 1.21171
I0523 11:37:08.808545  3765 solver.cpp:253]     Train net output #0: loss = 1.21171 (* 1 = 1.21171 loss)
I0523 11:37:08.808559  3765 sgd_solver.cpp:106] Iteration 117300, lr = 0.0045
I0523 11:37:18.107005  3765 solver.cpp:237] Iteration 117600, loss = 1.32493
I0523 11:37:18.107040  3765 solver.cpp:253]     Train net output #0: loss = 1.32493 (* 1 = 1.32493 loss)
I0523 11:37:18.107058  3765 sgd_solver.cpp:106] Iteration 117600, lr = 0.0045
I0523 11:37:27.404466  3765 solver.cpp:237] Iteration 117900, loss = 1.1433
I0523 11:37:27.404501  3765 solver.cpp:253]     Train net output #0: loss = 1.1433 (* 1 = 1.1433 loss)
I0523 11:37:27.404517  3765 sgd_solver.cpp:106] Iteration 117900, lr = 0.0045
I0523 11:37:57.596884  3765 solver.cpp:237] Iteration 118200, loss = 1.22512
I0523 11:37:57.597084  3765 solver.cpp:253]     Train net output #0: loss = 1.22512 (* 1 = 1.22512 loss)
I0523 11:37:57.597100  3765 sgd_solver.cpp:106] Iteration 118200, lr = 0.0045
I0523 11:38:06.891981  3765 solver.cpp:237] Iteration 118500, loss = 1.41609
I0523 11:38:06.892015  3765 solver.cpp:253]     Train net output #0: loss = 1.41609 (* 1 = 1.41609 loss)
I0523 11:38:06.892032  3765 sgd_solver.cpp:106] Iteration 118500, lr = 0.0045
I0523 11:38:16.184080  3765 solver.cpp:237] Iteration 118800, loss = 1.05762
I0523 11:38:16.184116  3765 solver.cpp:253]     Train net output #0: loss = 1.05762 (* 1 = 1.05762 loss)
I0523 11:38:16.184133  3765 sgd_solver.cpp:106] Iteration 118800, lr = 0.0045
I0523 11:38:25.476925  3765 solver.cpp:237] Iteration 119100, loss = 1.20374
I0523 11:38:25.476969  3765 solver.cpp:253]     Train net output #0: loss = 1.20374 (* 1 = 1.20374 loss)
I0523 11:38:25.476986  3765 sgd_solver.cpp:106] Iteration 119100, lr = 0.0045
I0523 11:38:34.774549  3765 solver.cpp:237] Iteration 119400, loss = 0.980772
I0523 11:38:34.774721  3765 solver.cpp:253]     Train net output #0: loss = 0.980772 (* 1 = 0.980772 loss)
I0523 11:38:34.774736  3765 sgd_solver.cpp:106] Iteration 119400, lr = 0.0045
I0523 11:38:44.071302  3765 solver.cpp:237] Iteration 119700, loss = 1.57116
I0523 11:38:44.071336  3765 solver.cpp:253]     Train net output #0: loss = 1.57116 (* 1 = 1.57116 loss)
I0523 11:38:44.071353  3765 sgd_solver.cpp:106] Iteration 119700, lr = 0.0045
I0523 11:38:53.337172  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_120000.caffemodel
I0523 11:38:53.398543  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_120000.solverstate
I0523 11:38:53.425892  3765 solver.cpp:341] Iteration 120000, Testing net (#0)
I0523 11:40:02.077610  3765 solver.cpp:409]     Test net output #0: accuracy = 0.900149
I0523 11:40:02.077807  3765 solver.cpp:409]     Test net output #1: loss = 0.325237 (* 1 = 0.325237 loss)
I0523 11:40:22.968462  3765 solver.cpp:237] Iteration 120000, loss = 1.06986
I0523 11:40:22.968518  3765 solver.cpp:253]     Train net output #0: loss = 1.06986 (* 1 = 1.06986 loss)
I0523 11:40:22.968533  3765 sgd_solver.cpp:106] Iteration 120000, lr = 0.0045
I0523 11:40:32.268393  3765 solver.cpp:237] Iteration 120300, loss = 1.1762
I0523 11:40:32.268605  3765 solver.cpp:253]     Train net output #0: loss = 1.1762 (* 1 = 1.1762 loss)
I0523 11:40:32.268620  3765 sgd_solver.cpp:106] Iteration 120300, lr = 0.0045
I0523 11:40:41.567975  3765 solver.cpp:237] Iteration 120600, loss = 1.09128
I0523 11:40:41.568008  3765 solver.cpp:253]     Train net output #0: loss = 1.09128 (* 1 = 1.09128 loss)
I0523 11:40:41.568025  3765 sgd_solver.cpp:106] Iteration 120600, lr = 0.0045
I0523 11:40:50.862692  3765 solver.cpp:237] Iteration 120900, loss = 1.0974
I0523 11:40:50.862732  3765 solver.cpp:253]     Train net output #0: loss = 1.0974 (* 1 = 1.0974 loss)
I0523 11:40:50.862749  3765 sgd_solver.cpp:106] Iteration 120900, lr = 0.0045
I0523 11:41:00.164755  3765 solver.cpp:237] Iteration 121200, loss = 1.14281
I0523 11:41:00.164790  3765 solver.cpp:253]     Train net output #0: loss = 1.14281 (* 1 = 1.14281 loss)
I0523 11:41:00.164808  3765 sgd_solver.cpp:106] Iteration 121200, lr = 0.0045
I0523 11:41:09.464323  3765 solver.cpp:237] Iteration 121500, loss = 1.00789
I0523 11:41:09.464496  3765 solver.cpp:253]     Train net output #0: loss = 1.00789 (* 1 = 1.00789 loss)
I0523 11:41:09.464509  3765 sgd_solver.cpp:106] Iteration 121500, lr = 0.0045
I0523 11:41:18.763947  3765 solver.cpp:237] Iteration 121800, loss = 1.31165
I0523 11:41:18.763995  3765 solver.cpp:253]     Train net output #0: loss = 1.31165 (* 1 = 1.31165 loss)
I0523 11:41:18.764013  3765 sgd_solver.cpp:106] Iteration 121800, lr = 0.0045
I0523 11:41:48.937018  3765 solver.cpp:237] Iteration 122100, loss = 1.19275
I0523 11:41:48.937218  3765 solver.cpp:253]     Train net output #0: loss = 1.19275 (* 1 = 1.19275 loss)
I0523 11:41:48.937233  3765 sgd_solver.cpp:106] Iteration 122100, lr = 0.0045
I0523 11:41:58.240526  3765 solver.cpp:237] Iteration 122400, loss = 1.24288
I0523 11:41:58.240561  3765 solver.cpp:253]     Train net output #0: loss = 1.24288 (* 1 = 1.24288 loss)
I0523 11:41:58.240578  3765 sgd_solver.cpp:106] Iteration 122400, lr = 0.0045
I0523 11:42:07.540343  3765 solver.cpp:237] Iteration 122700, loss = 1.51698
I0523 11:42:07.540393  3765 solver.cpp:253]     Train net output #0: loss = 1.51698 (* 1 = 1.51698 loss)
I0523 11:42:07.540410  3765 sgd_solver.cpp:106] Iteration 122700, lr = 0.0045
I0523 11:42:16.808640  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_123000.caffemodel
I0523 11:42:16.867599  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_123000.solverstate
I0523 11:42:16.902438  3765 solver.cpp:237] Iteration 123000, loss = 0.891161
I0523 11:42:16.902488  3765 solver.cpp:253]     Train net output #0: loss = 0.89116 (* 1 = 0.89116 loss)
I0523 11:42:16.902503  3765 sgd_solver.cpp:106] Iteration 123000, lr = 0.0045
I0523 11:42:26.204622  3765 solver.cpp:237] Iteration 123300, loss = 1.15324
I0523 11:42:26.204800  3765 solver.cpp:253]     Train net output #0: loss = 1.15324 (* 1 = 1.15324 loss)
I0523 11:42:26.204814  3765 sgd_solver.cpp:106] Iteration 123300, lr = 0.0045
I0523 11:42:35.499264  3765 solver.cpp:237] Iteration 123600, loss = 0.951905
I0523 11:42:35.499315  3765 solver.cpp:253]     Train net output #0: loss = 0.951905 (* 1 = 0.951905 loss)
I0523 11:42:35.499330  3765 sgd_solver.cpp:106] Iteration 123600, lr = 0.0045
I0523 11:42:44.800353  3765 solver.cpp:237] Iteration 123900, loss = 1.12861
I0523 11:42:44.800387  3765 solver.cpp:253]     Train net output #0: loss = 1.12861 (* 1 = 1.12861 loss)
I0523 11:42:44.800403  3765 sgd_solver.cpp:106] Iteration 123900, lr = 0.0045
I0523 11:43:14.975900  3765 solver.cpp:237] Iteration 124200, loss = 1.06787
I0523 11:43:14.976109  3765 solver.cpp:253]     Train net output #0: loss = 1.06787 (* 1 = 1.06787 loss)
I0523 11:43:14.976125  3765 sgd_solver.cpp:106] Iteration 124200, lr = 0.0045
I0523 11:43:24.275967  3765 solver.cpp:237] Iteration 124500, loss = 1.0196
I0523 11:43:24.276012  3765 solver.cpp:253]     Train net output #0: loss = 1.0196 (* 1 = 1.0196 loss)
I0523 11:43:24.276029  3765 sgd_solver.cpp:106] Iteration 124500, lr = 0.0045
I0523 11:43:33.580587  3765 solver.cpp:237] Iteration 124800, loss = 1.19931
I0523 11:43:33.580621  3765 solver.cpp:253]     Train net output #0: loss = 1.19931 (* 1 = 1.19931 loss)
I0523 11:43:33.580637  3765 sgd_solver.cpp:106] Iteration 124800, lr = 0.0045
I0523 11:43:42.884510  3765 solver.cpp:237] Iteration 125100, loss = 1.27235
I0523 11:43:42.884546  3765 solver.cpp:253]     Train net output #0: loss = 1.27235 (* 1 = 1.27235 loss)
I0523 11:43:42.884562  3765 sgd_solver.cpp:106] Iteration 125100, lr = 0.0045
I0523 11:43:52.184125  3765 solver.cpp:237] Iteration 125400, loss = 0.938343
I0523 11:43:52.184329  3765 solver.cpp:253]     Train net output #0: loss = 0.938343 (* 1 = 0.938343 loss)
I0523 11:43:52.184343  3765 sgd_solver.cpp:106] Iteration 125400, lr = 0.0045
I0523 11:44:01.485358  3765 solver.cpp:237] Iteration 125700, loss = 1.03742
I0523 11:44:01.485393  3765 solver.cpp:253]     Train net output #0: loss = 1.03742 (* 1 = 1.03742 loss)
I0523 11:44:01.485409  3765 sgd_solver.cpp:106] Iteration 125700, lr = 0.0045
I0523 11:44:10.754153  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_126000.caffemodel
I0523 11:44:10.813920  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_126000.solverstate
I0523 11:44:10.839408  3765 solver.cpp:341] Iteration 126000, Testing net (#0)
I0523 11:44:58.616272  3765 solver.cpp:409]     Test net output #0: accuracy = 0.90191
I0523 11:44:58.616474  3765 solver.cpp:409]     Test net output #1: loss = 0.310399 (* 1 = 0.310399 loss)
I0523 11:45:19.507401  3765 solver.cpp:237] Iteration 126000, loss = 0.951692
I0523 11:45:19.507463  3765 solver.cpp:253]     Train net output #0: loss = 0.951692 (* 1 = 0.951692 loss)
I0523 11:45:19.507479  3765 sgd_solver.cpp:106] Iteration 126000, lr = 0.0045
I0523 11:45:28.801106  3765 solver.cpp:237] Iteration 126300, loss = 1.36234
I0523 11:45:28.801298  3765 solver.cpp:253]     Train net output #0: loss = 1.36233 (* 1 = 1.36233 loss)
I0523 11:45:28.801312  3765 sgd_solver.cpp:106] Iteration 126300, lr = 0.0045
I0523 11:45:38.097662  3765 solver.cpp:237] Iteration 126600, loss = 1.08903
I0523 11:45:38.097693  3765 solver.cpp:253]     Train net output #0: loss = 1.08903 (* 1 = 1.08903 loss)
I0523 11:45:38.097707  3765 sgd_solver.cpp:106] Iteration 126600, lr = 0.0045
I0523 11:45:47.393196  3765 solver.cpp:237] Iteration 126900, loss = 1.3243
I0523 11:45:47.393232  3765 solver.cpp:253]     Train net output #0: loss = 1.3243 (* 1 = 1.3243 loss)
I0523 11:45:47.393249  3765 sgd_solver.cpp:106] Iteration 126900, lr = 0.0045
I0523 11:45:56.690733  3765 solver.cpp:237] Iteration 127200, loss = 1.33027
I0523 11:45:56.690778  3765 solver.cpp:253]     Train net output #0: loss = 1.33027 (* 1 = 1.33027 loss)
I0523 11:45:56.690799  3765 sgd_solver.cpp:106] Iteration 127200, lr = 0.0045
I0523 11:46:05.987371  3765 solver.cpp:237] Iteration 127500, loss = 1.09177
I0523 11:46:05.987550  3765 solver.cpp:253]     Train net output #0: loss = 1.09177 (* 1 = 1.09177 loss)
I0523 11:46:05.987565  3765 sgd_solver.cpp:106] Iteration 127500, lr = 0.0045
I0523 11:46:15.281024  3765 solver.cpp:237] Iteration 127800, loss = 1.11797
I0523 11:46:15.281059  3765 solver.cpp:253]     Train net output #0: loss = 1.11797 (* 1 = 1.11797 loss)
I0523 11:46:15.281074  3765 sgd_solver.cpp:106] Iteration 127800, lr = 0.0045
I0523 11:46:45.483417  3765 solver.cpp:237] Iteration 128100, loss = 1.20497
I0523 11:46:45.483625  3765 solver.cpp:253]     Train net output #0: loss = 1.20497 (* 1 = 1.20497 loss)
I0523 11:46:45.483639  3765 sgd_solver.cpp:106] Iteration 128100, lr = 0.0045
I0523 11:46:54.781134  3765 solver.cpp:237] Iteration 128400, loss = 1.05883
I0523 11:46:54.781168  3765 solver.cpp:253]     Train net output #0: loss = 1.05883 (* 1 = 1.05883 loss)
I0523 11:46:54.781186  3765 sgd_solver.cpp:106] Iteration 128400, lr = 0.0045
I0523 11:47:04.077286  3765 solver.cpp:237] Iteration 128700, loss = 1.1785
I0523 11:47:04.077319  3765 solver.cpp:253]     Train net output #0: loss = 1.1785 (* 1 = 1.1785 loss)
I0523 11:47:04.077333  3765 sgd_solver.cpp:106] Iteration 128700, lr = 0.0045
I0523 11:47:13.339560  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_129000.caffemodel
I0523 11:47:13.399085  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_129000.solverstate
I0523 11:47:13.434047  3765 solver.cpp:237] Iteration 129000, loss = 0.99052
I0523 11:47:13.434097  3765 solver.cpp:253]     Train net output #0: loss = 0.99052 (* 1 = 0.99052 loss)
I0523 11:47:13.434110  3765 sgd_solver.cpp:106] Iteration 129000, lr = 0.0045
I0523 11:47:22.729012  3765 solver.cpp:237] Iteration 129300, loss = 1.10218
I0523 11:47:22.729204  3765 solver.cpp:253]     Train net output #0: loss = 1.10218 (* 1 = 1.10218 loss)
I0523 11:47:22.729218  3765 sgd_solver.cpp:106] Iteration 129300, lr = 0.0045
I0523 11:47:32.021986  3765 solver.cpp:237] Iteration 129600, loss = 1.12631
I0523 11:47:32.022035  3765 solver.cpp:253]     Train net output #0: loss = 1.12631 (* 1 = 1.12631 loss)
I0523 11:47:32.022050  3765 sgd_solver.cpp:106] Iteration 129600, lr = 0.0045
I0523 11:47:41.315646  3765 solver.cpp:237] Iteration 129900, loss = 1.26644
I0523 11:47:41.315682  3765 solver.cpp:253]     Train net output #0: loss = 1.26644 (* 1 = 1.26644 loss)
I0523 11:47:41.315697  3765 sgd_solver.cpp:106] Iteration 129900, lr = 0.0045
I0523 11:48:11.482874  3765 solver.cpp:237] Iteration 130200, loss = 1.00652
I0523 11:48:11.483078  3765 solver.cpp:253]     Train net output #0: loss = 1.00652 (* 1 = 1.00652 loss)
I0523 11:48:11.483093  3765 sgd_solver.cpp:106] Iteration 130200, lr = 0.0045
I0523 11:48:20.780032  3765 solver.cpp:237] Iteration 130500, loss = 1.18095
I0523 11:48:20.780086  3765 solver.cpp:253]     Train net output #0: loss = 1.18095 (* 1 = 1.18095 loss)
I0523 11:48:20.780102  3765 sgd_solver.cpp:106] Iteration 130500, lr = 0.0045
I0523 11:48:30.074926  3765 solver.cpp:237] Iteration 130800, loss = 1.07451
I0523 11:48:30.074961  3765 solver.cpp:253]     Train net output #0: loss = 1.07451 (* 1 = 1.07451 loss)
I0523 11:48:30.074977  3765 sgd_solver.cpp:106] Iteration 130800, lr = 0.0045
I0523 11:48:39.371532  3765 solver.cpp:237] Iteration 131100, loss = 1.02681
I0523 11:48:39.371568  3765 solver.cpp:253]     Train net output #0: loss = 1.02681 (* 1 = 1.02681 loss)
I0523 11:48:39.371582  3765 sgd_solver.cpp:106] Iteration 131100, lr = 0.0045
I0523 11:48:48.670068  3765 solver.cpp:237] Iteration 131400, loss = 1.13148
I0523 11:48:48.670276  3765 solver.cpp:253]     Train net output #0: loss = 1.13148 (* 1 = 1.13148 loss)
I0523 11:48:48.670290  3765 sgd_solver.cpp:106] Iteration 131400, lr = 0.0045
I0523 11:48:57.968556  3765 solver.cpp:237] Iteration 131700, loss = 1.24769
I0523 11:48:57.968590  3765 solver.cpp:253]     Train net output #0: loss = 1.24769 (* 1 = 1.24769 loss)
I0523 11:48:57.968608  3765 sgd_solver.cpp:106] Iteration 131700, lr = 0.0045
I0523 11:49:07.233078  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_132000.caffemodel
I0523 11:49:07.292623  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_132000.solverstate
I0523 11:49:07.318665  3765 solver.cpp:341] Iteration 132000, Testing net (#0)
I0523 11:50:16.012955  3765 solver.cpp:409]     Test net output #0: accuracy = 0.901983
I0523 11:50:16.013166  3765 solver.cpp:409]     Test net output #1: loss = 0.322887 (* 1 = 0.322887 loss)
I0523 11:50:36.890362  3765 solver.cpp:237] Iteration 132000, loss = 0.795151
I0523 11:50:36.890420  3765 solver.cpp:253]     Train net output #0: loss = 0.79515 (* 1 = 0.79515 loss)
I0523 11:50:36.890435  3765 sgd_solver.cpp:106] Iteration 132000, lr = 0.0045
I0523 11:50:46.188426  3765 solver.cpp:237] Iteration 132300, loss = 1.18416
I0523 11:50:46.188611  3765 solver.cpp:253]     Train net output #0: loss = 1.18416 (* 1 = 1.18416 loss)
I0523 11:50:46.188624  3765 sgd_solver.cpp:106] Iteration 132300, lr = 0.0045
I0523 11:50:55.486759  3765 solver.cpp:237] Iteration 132600, loss = 1.32004
I0523 11:50:55.486807  3765 solver.cpp:253]     Train net output #0: loss = 1.32004 (* 1 = 1.32004 loss)
I0523 11:50:55.486825  3765 sgd_solver.cpp:106] Iteration 132600, lr = 0.0045
I0523 11:51:04.784603  3765 solver.cpp:237] Iteration 132900, loss = 1.05382
I0523 11:51:04.784639  3765 solver.cpp:253]     Train net output #0: loss = 1.05382 (* 1 = 1.05382 loss)
I0523 11:51:04.784653  3765 sgd_solver.cpp:106] Iteration 132900, lr = 0.0045
I0523 11:51:14.082340  3765 solver.cpp:237] Iteration 133200, loss = 1.17024
I0523 11:51:14.082375  3765 solver.cpp:253]     Train net output #0: loss = 1.17024 (* 1 = 1.17024 loss)
I0523 11:51:14.082391  3765 sgd_solver.cpp:106] Iteration 133200, lr = 0.0045
I0523 11:51:23.385046  3765 solver.cpp:237] Iteration 133500, loss = 1.16322
I0523 11:51:23.385253  3765 solver.cpp:253]     Train net output #0: loss = 1.16322 (* 1 = 1.16322 loss)
I0523 11:51:23.385267  3765 sgd_solver.cpp:106] Iteration 133500, lr = 0.0045
I0523 11:51:32.686679  3765 solver.cpp:237] Iteration 133800, loss = 1.08976
I0523 11:51:32.686713  3765 solver.cpp:253]     Train net output #0: loss = 1.08976 (* 1 = 1.08976 loss)
I0523 11:51:32.686730  3765 sgd_solver.cpp:106] Iteration 133800, lr = 0.0045
I0523 11:52:02.858625  3765 solver.cpp:237] Iteration 134100, loss = 1.0468
I0523 11:52:02.858832  3765 solver.cpp:253]     Train net output #0: loss = 1.0468 (* 1 = 1.0468 loss)
I0523 11:52:02.858847  3765 sgd_solver.cpp:106] Iteration 134100, lr = 0.0045
I0523 11:52:12.159477  3765 solver.cpp:237] Iteration 134400, loss = 0.990506
I0523 11:52:12.159524  3765 solver.cpp:253]     Train net output #0: loss = 0.990506 (* 1 = 0.990506 loss)
I0523 11:52:12.159540  3765 sgd_solver.cpp:106] Iteration 134400, lr = 0.0045
I0523 11:52:21.457262  3765 solver.cpp:237] Iteration 134700, loss = 1.11795
I0523 11:52:21.457298  3765 solver.cpp:253]     Train net output #0: loss = 1.11795 (* 1 = 1.11795 loss)
I0523 11:52:21.457310  3765 sgd_solver.cpp:106] Iteration 134700, lr = 0.0045
I0523 11:52:30.722473  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_135000.caffemodel
I0523 11:52:30.784272  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_135000.solverstate
I0523 11:52:30.821820  3765 solver.cpp:237] Iteration 135000, loss = 0.852828
I0523 11:52:30.821876  3765 solver.cpp:253]     Train net output #0: loss = 0.852827 (* 1 = 0.852827 loss)
I0523 11:52:30.821889  3765 sgd_solver.cpp:106] Iteration 135000, lr = 0.0045
I0523 11:52:40.124219  3765 solver.cpp:237] Iteration 135300, loss = 1.14123
I0523 11:52:40.124415  3765 solver.cpp:253]     Train net output #0: loss = 1.14123 (* 1 = 1.14123 loss)
I0523 11:52:40.124429  3765 sgd_solver.cpp:106] Iteration 135300, lr = 0.0045
I0523 11:52:49.419436  3765 solver.cpp:237] Iteration 135600, loss = 1.53203
I0523 11:52:49.419471  3765 solver.cpp:253]     Train net output #0: loss = 1.53203 (* 1 = 1.53203 loss)
I0523 11:52:49.419487  3765 sgd_solver.cpp:106] Iteration 135600, lr = 0.0045
I0523 11:52:58.717211  3765 solver.cpp:237] Iteration 135900, loss = 1.11284
I0523 11:52:58.717264  3765 solver.cpp:253]     Train net output #0: loss = 1.11284 (* 1 = 1.11284 loss)
I0523 11:52:58.717283  3765 sgd_solver.cpp:106] Iteration 135900, lr = 0.0045
I0523 11:53:28.921144  3765 solver.cpp:237] Iteration 136200, loss = 1.10692
I0523 11:53:28.921373  3765 solver.cpp:253]     Train net output #0: loss = 1.10692 (* 1 = 1.10692 loss)
I0523 11:53:28.921389  3765 sgd_solver.cpp:106] Iteration 136200, lr = 0.0045
I0523 11:53:38.218463  3765 solver.cpp:237] Iteration 136500, loss = 1.09349
I0523 11:53:38.218498  3765 solver.cpp:253]     Train net output #0: loss = 1.09349 (* 1 = 1.09349 loss)
I0523 11:53:38.218513  3765 sgd_solver.cpp:106] Iteration 136500, lr = 0.0045
I0523 11:53:47.516177  3765 solver.cpp:237] Iteration 136800, loss = 1.30317
I0523 11:53:47.516228  3765 solver.cpp:253]     Train net output #0: loss = 1.30317 (* 1 = 1.30317 loss)
I0523 11:53:47.516243  3765 sgd_solver.cpp:106] Iteration 136800, lr = 0.0045
I0523 11:53:56.813339  3765 solver.cpp:237] Iteration 137100, loss = 1.15518
I0523 11:53:56.813374  3765 solver.cpp:253]     Train net output #0: loss = 1.15518 (* 1 = 1.15518 loss)
I0523 11:53:56.813391  3765 sgd_solver.cpp:106] Iteration 137100, lr = 0.0045
I0523 11:54:06.108400  3765 solver.cpp:237] Iteration 137400, loss = 0.847967
I0523 11:54:06.108583  3765 solver.cpp:253]     Train net output #0: loss = 0.847966 (* 1 = 0.847966 loss)
I0523 11:54:06.108597  3765 sgd_solver.cpp:106] Iteration 137400, lr = 0.0045
I0523 11:54:15.405051  3765 solver.cpp:237] Iteration 137700, loss = 1.15903
I0523 11:54:15.405099  3765 solver.cpp:253]     Train net output #0: loss = 1.15903 (* 1 = 1.15903 loss)
I0523 11:54:15.405118  3765 sgd_solver.cpp:106] Iteration 137700, lr = 0.0045
I0523 11:54:24.674520  3765 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_138000.caffemodel
I0523 11:54:24.733595  3765 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0045_2016-05-20T15.49.08.074792_iter_138000.solverstate
I0523 11:54:24.758572  3765 solver.cpp:341] Iteration 138000, Testing net (#0)
I0523 11:55:12.274201  3765 solver.cpp:409]     Test net output #0: accuracy = 0.902634
I0523 11:55:12.274405  3765 solver.cpp:409]     Test net output #1: loss = 0.305847 (* 1 = 0.305847 loss)
I0523 11:55:33.179559  3765 solver.cpp:237] Iteration 138000, loss = 0.949589
I0523 11:55:33.179616  3765 solver.cpp:253]     Train net output #0: loss = 0.949589 (* 1 = 0.949589 loss)
I0523 11:55:33.179632  3765 sgd_solver.cpp:106] Iteration 138000, lr = 0.0045
I0523 11:55:42.472378  3765 solver.cpp:237] Iteration 138300, loss = 0.977688
I0523 11:55:42.472578  3765 solver.cpp:253]     Train net output #0: loss = 0.977688 (* 1 = 0.977688 loss)
I0523 11:55:42.472591  3765 sgd_solver.cpp:106] Iteration 138300, lr = 0.0045
I0523 11:55:51.766019  3765 solver.cpp:237] Iteration 138600, loss = 1.16162
I0523 11:55:51.766072  3765 solver.cpp:253]     Train net output #0: loss = 1.16162 (* 1 = 1.16162 loss)
I0523 11:55:51.766088  3765 sgd_solver.cpp:106] Iteration 138600, lr = 0.0045
I0523 11:56:01.064131  3765 solver.cpp:237] Iteration 138900, loss = 1.02949
I0523 11:56:01.064167  3765 solver.cpp:253]     Train net output #0: loss = 1.02949 (* 1 = 1.02949 loss)
I0523 11:56:01.064183  3765 sgd_solver.cpp:106] Iteration 138900, lr = 0.0045
I0523 11:56:10.361189  3765 solver.cpp:237] Iteration 139200, loss = 1.03249
I0523 11:56:10.361224  3765 solver.cpp:253]     Train net output #0: loss = 1.03249 (* 1 = 1.03249 loss)
I0523 11:56:10.361240  3765 sgd_solver.cpp:106] Iteration 139200, lr = 0.0045
I0523 11:56:19.657759  3765 solver.cpp:237] Iteration 139500, loss = 1.20191
I0523 11:56:19.657958  3765 solver.cpp:253]     Train net output #0: loss = 1.20191 (* 1 = 1.20191 loss)
I0523 11:56:19.657973  3765 sgd_solver.cpp:106] Iteration 139500, lr = 0.0045
aprun: Apid 11254163: Caught signal Terminated, sending to application
*** Aborted at 1464018980 (unix time) try "date -d @1464018980" if you are using GNU date ***
PC: @     0x2aaab930eb99 (unknown)
*** SIGTERM (@0xeb2) received by PID 3765 (TID 0x2aaac746f900) from PID 3762; stack trace: ***
    @     0x2aaab7c78850 (unknown)
    @     0x2aaab930eb99 (unknown)
    @     0x2aaab928a368 (unknown)
aprun: Apid 11254163: Caught signal Terminated, sending to application
    @     0x2aaab91e97a1 (unknown)
=>> PBS: job killed: walltime 7216 exceeded limit 7200
aprun: Apid 11254163: Caught signal Terminated, sending to application
    @     0x2aaab91e98af (unknown)
    @     0x2aaab928ea34 (unknown)
    @     0x2aaab928ec2c (unknown)
    @     0x2aaab926d723 (unknown)
    @     0x2aaab92655e1 (unknown)
    @     0x2aaab9266356 (unknown)
aprun: Apid 11254163: Caught signal Terminated, sending to application
    @     0x2aaab91d5562 (unknown)
    @     0x2aaab91d56ba (unknown)
    @     0x2aaab91b8715 cuMemcpy
    @     0x2aaaaacf9e92 (unknown)
    @     0x2aaaaacde306 (unknown)
    @     0x2aaaaad00328 cudaMemcpy
    @           0x4d6a10 caffe::caffe_copy<>()
aprun: Apid 11254163: Caught signal Terminated, sending to application
    @           0x626ea9 caffe::HDF5DataLayer<>::Forward_gpu()
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
    @           0x5ca109 caffe::Solver<>::Step()
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
aprun: Apid 11254163: Caught signal Terminated, sending to application
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
    @           0x438669 (unknown)
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
aprun: Apid 11254163: Caught signal Terminated, sending to application
_pmiu_daemon(SIGCHLD): [NID 00844] [c2-1c1s6n0] [Mon May 23 11:56:22 2016] PE RANK 0 exit signal Terminated
Application 11254163 exit codes: 143
Application 11254163 resources: utime ~6242s, stime ~964s, Rss ~5333084, inblocks ~15857513, outblocks ~710995
