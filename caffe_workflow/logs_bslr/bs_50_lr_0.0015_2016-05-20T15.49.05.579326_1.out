2808095
I0523 03:53:58.199496 30336 caffe.cpp:184] Using GPUs 0
I0523 03:53:58.627454 30336 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3000
test_interval: 6000
base_lr: 0.0015
display: 300
max_iter: 300000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 3000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326.prototxt"
I0523 03:53:58.629067 30336 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326.prototxt
I0523 03:53:58.650722 30336 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0523 03:53:58.650781 30336 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 03:53:58.651125 30336 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0523 03:53:58.651306 30336 layer_factory.hpp:77] Creating layer data_hdf5
I0523 03:53:58.651329 30336 net.cpp:106] Creating Layer data_hdf5
I0523 03:53:58.651345 30336 net.cpp:411] data_hdf5 -> data
I0523 03:53:58.651377 30336 net.cpp:411] data_hdf5 -> label
I0523 03:53:58.651410 30336 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0523 03:53:58.663077 30336 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0523 03:53:58.673249 30336 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0523 03:54:20.222254 30336 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0523 03:54:20.227458 30336 net.cpp:150] Setting up data_hdf5
I0523 03:54:20.227501 30336 net.cpp:157] Top shape: 50 1 127 50 (317500)
I0523 03:54:20.227516 30336 net.cpp:157] Top shape: 50 (50)
I0523 03:54:20.227527 30336 net.cpp:165] Memory required for data: 1270200
I0523 03:54:20.227540 30336 layer_factory.hpp:77] Creating layer conv1
I0523 03:54:20.227576 30336 net.cpp:106] Creating Layer conv1
I0523 03:54:20.227586 30336 net.cpp:454] conv1 <- data
I0523 03:54:20.227608 30336 net.cpp:411] conv1 -> conv1
I0523 03:54:23.097808 30336 net.cpp:150] Setting up conv1
I0523 03:54:23.097854 30336 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 03:54:23.097867 30336 net.cpp:165] Memory required for data: 15094200
I0523 03:54:23.097898 30336 layer_factory.hpp:77] Creating layer relu1
I0523 03:54:23.097918 30336 net.cpp:106] Creating Layer relu1
I0523 03:54:23.097929 30336 net.cpp:454] relu1 <- conv1
I0523 03:54:23.097942 30336 net.cpp:397] relu1 -> conv1 (in-place)
I0523 03:54:23.098456 30336 net.cpp:150] Setting up relu1
I0523 03:54:23.098474 30336 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 03:54:23.098484 30336 net.cpp:165] Memory required for data: 28918200
I0523 03:54:23.098494 30336 layer_factory.hpp:77] Creating layer pool1
I0523 03:54:23.098510 30336 net.cpp:106] Creating Layer pool1
I0523 03:54:23.098520 30336 net.cpp:454] pool1 <- conv1
I0523 03:54:23.098532 30336 net.cpp:411] pool1 -> pool1
I0523 03:54:23.098613 30336 net.cpp:150] Setting up pool1
I0523 03:54:23.098626 30336 net.cpp:157] Top shape: 50 12 60 48 (1728000)
I0523 03:54:23.098636 30336 net.cpp:165] Memory required for data: 35830200
I0523 03:54:23.098647 30336 layer_factory.hpp:77] Creating layer conv2
I0523 03:54:23.098670 30336 net.cpp:106] Creating Layer conv2
I0523 03:54:23.098680 30336 net.cpp:454] conv2 <- pool1
I0523 03:54:23.098693 30336 net.cpp:411] conv2 -> conv2
I0523 03:54:23.101361 30336 net.cpp:150] Setting up conv2
I0523 03:54:23.101388 30336 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 03:54:23.101399 30336 net.cpp:165] Memory required for data: 45766200
I0523 03:54:23.101418 30336 layer_factory.hpp:77] Creating layer relu2
I0523 03:54:23.101433 30336 net.cpp:106] Creating Layer relu2
I0523 03:54:23.101444 30336 net.cpp:454] relu2 <- conv2
I0523 03:54:23.101455 30336 net.cpp:397] relu2 -> conv2 (in-place)
I0523 03:54:23.101783 30336 net.cpp:150] Setting up relu2
I0523 03:54:23.101799 30336 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 03:54:23.101809 30336 net.cpp:165] Memory required for data: 55702200
I0523 03:54:23.101819 30336 layer_factory.hpp:77] Creating layer pool2
I0523 03:54:23.101831 30336 net.cpp:106] Creating Layer pool2
I0523 03:54:23.101841 30336 net.cpp:454] pool2 <- conv2
I0523 03:54:23.101853 30336 net.cpp:411] pool2 -> pool2
I0523 03:54:23.101933 30336 net.cpp:150] Setting up pool2
I0523 03:54:23.101948 30336 net.cpp:157] Top shape: 50 20 27 46 (1242000)
I0523 03:54:23.101958 30336 net.cpp:165] Memory required for data: 60670200
I0523 03:54:23.101966 30336 layer_factory.hpp:77] Creating layer conv3
I0523 03:54:23.101982 30336 net.cpp:106] Creating Layer conv3
I0523 03:54:23.101992 30336 net.cpp:454] conv3 <- pool2
I0523 03:54:23.102006 30336 net.cpp:411] conv3 -> conv3
I0523 03:54:23.103930 30336 net.cpp:150] Setting up conv3
I0523 03:54:23.103955 30336 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 03:54:23.103965 30336 net.cpp:165] Memory required for data: 66091000
I0523 03:54:23.103996 30336 layer_factory.hpp:77] Creating layer relu3
I0523 03:54:23.104012 30336 net.cpp:106] Creating Layer relu3
I0523 03:54:23.104022 30336 net.cpp:454] relu3 <- conv3
I0523 03:54:23.104033 30336 net.cpp:397] relu3 -> conv3 (in-place)
I0523 03:54:23.104501 30336 net.cpp:150] Setting up relu3
I0523 03:54:23.104518 30336 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 03:54:23.104528 30336 net.cpp:165] Memory required for data: 71511800
I0523 03:54:23.104538 30336 layer_factory.hpp:77] Creating layer pool3
I0523 03:54:23.104552 30336 net.cpp:106] Creating Layer pool3
I0523 03:54:23.104562 30336 net.cpp:454] pool3 <- conv3
I0523 03:54:23.104574 30336 net.cpp:411] pool3 -> pool3
I0523 03:54:23.104641 30336 net.cpp:150] Setting up pool3
I0523 03:54:23.104655 30336 net.cpp:157] Top shape: 50 28 11 44 (677600)
I0523 03:54:23.104665 30336 net.cpp:165] Memory required for data: 74222200
I0523 03:54:23.104674 30336 layer_factory.hpp:77] Creating layer conv4
I0523 03:54:23.104694 30336 net.cpp:106] Creating Layer conv4
I0523 03:54:23.104704 30336 net.cpp:454] conv4 <- pool3
I0523 03:54:23.104718 30336 net.cpp:411] conv4 -> conv4
I0523 03:54:23.107472 30336 net.cpp:150] Setting up conv4
I0523 03:54:23.107501 30336 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 03:54:23.107511 30336 net.cpp:165] Memory required for data: 76036600
I0523 03:54:23.107527 30336 layer_factory.hpp:77] Creating layer relu4
I0523 03:54:23.107542 30336 net.cpp:106] Creating Layer relu4
I0523 03:54:23.107552 30336 net.cpp:454] relu4 <- conv4
I0523 03:54:23.107564 30336 net.cpp:397] relu4 -> conv4 (in-place)
I0523 03:54:23.108053 30336 net.cpp:150] Setting up relu4
I0523 03:54:23.108070 30336 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 03:54:23.108081 30336 net.cpp:165] Memory required for data: 77851000
I0523 03:54:23.108093 30336 layer_factory.hpp:77] Creating layer pool4
I0523 03:54:23.108105 30336 net.cpp:106] Creating Layer pool4
I0523 03:54:23.108115 30336 net.cpp:454] pool4 <- conv4
I0523 03:54:23.108129 30336 net.cpp:411] pool4 -> pool4
I0523 03:54:23.108196 30336 net.cpp:150] Setting up pool4
I0523 03:54:23.108211 30336 net.cpp:157] Top shape: 50 36 3 42 (226800)
I0523 03:54:23.108222 30336 net.cpp:165] Memory required for data: 78758200
I0523 03:54:23.108232 30336 layer_factory.hpp:77] Creating layer ip1
I0523 03:54:23.108252 30336 net.cpp:106] Creating Layer ip1
I0523 03:54:23.108263 30336 net.cpp:454] ip1 <- pool4
I0523 03:54:23.108276 30336 net.cpp:411] ip1 -> ip1
I0523 03:54:23.123690 30336 net.cpp:150] Setting up ip1
I0523 03:54:23.123719 30336 net.cpp:157] Top shape: 50 196 (9800)
I0523 03:54:23.123731 30336 net.cpp:165] Memory required for data: 78797400
I0523 03:54:23.123754 30336 layer_factory.hpp:77] Creating layer relu5
I0523 03:54:23.123769 30336 net.cpp:106] Creating Layer relu5
I0523 03:54:23.123780 30336 net.cpp:454] relu5 <- ip1
I0523 03:54:23.123793 30336 net.cpp:397] relu5 -> ip1 (in-place)
I0523 03:54:23.124140 30336 net.cpp:150] Setting up relu5
I0523 03:54:23.124155 30336 net.cpp:157] Top shape: 50 196 (9800)
I0523 03:54:23.124166 30336 net.cpp:165] Memory required for data: 78836600
I0523 03:54:23.124176 30336 layer_factory.hpp:77] Creating layer drop1
I0523 03:54:23.124197 30336 net.cpp:106] Creating Layer drop1
I0523 03:54:23.124208 30336 net.cpp:454] drop1 <- ip1
I0523 03:54:23.124220 30336 net.cpp:397] drop1 -> ip1 (in-place)
I0523 03:54:23.124280 30336 net.cpp:150] Setting up drop1
I0523 03:54:23.124294 30336 net.cpp:157] Top shape: 50 196 (9800)
I0523 03:54:23.124305 30336 net.cpp:165] Memory required for data: 78875800
I0523 03:54:23.124315 30336 layer_factory.hpp:77] Creating layer ip2
I0523 03:54:23.124333 30336 net.cpp:106] Creating Layer ip2
I0523 03:54:23.124343 30336 net.cpp:454] ip2 <- ip1
I0523 03:54:23.124356 30336 net.cpp:411] ip2 -> ip2
I0523 03:54:23.124819 30336 net.cpp:150] Setting up ip2
I0523 03:54:23.124832 30336 net.cpp:157] Top shape: 50 98 (4900)
I0523 03:54:23.124842 30336 net.cpp:165] Memory required for data: 78895400
I0523 03:54:23.124857 30336 layer_factory.hpp:77] Creating layer relu6
I0523 03:54:23.124871 30336 net.cpp:106] Creating Layer relu6
I0523 03:54:23.124879 30336 net.cpp:454] relu6 <- ip2
I0523 03:54:23.124891 30336 net.cpp:397] relu6 -> ip2 (in-place)
I0523 03:54:23.125406 30336 net.cpp:150] Setting up relu6
I0523 03:54:23.125422 30336 net.cpp:157] Top shape: 50 98 (4900)
I0523 03:54:23.125432 30336 net.cpp:165] Memory required for data: 78915000
I0523 03:54:23.125442 30336 layer_factory.hpp:77] Creating layer drop2
I0523 03:54:23.125457 30336 net.cpp:106] Creating Layer drop2
I0523 03:54:23.125466 30336 net.cpp:454] drop2 <- ip2
I0523 03:54:23.125484 30336 net.cpp:397] drop2 -> ip2 (in-place)
I0523 03:54:23.125526 30336 net.cpp:150] Setting up drop2
I0523 03:54:23.125540 30336 net.cpp:157] Top shape: 50 98 (4900)
I0523 03:54:23.125550 30336 net.cpp:165] Memory required for data: 78934600
I0523 03:54:23.125560 30336 layer_factory.hpp:77] Creating layer ip3
I0523 03:54:23.125573 30336 net.cpp:106] Creating Layer ip3
I0523 03:54:23.125583 30336 net.cpp:454] ip3 <- ip2
I0523 03:54:23.125597 30336 net.cpp:411] ip3 -> ip3
I0523 03:54:23.125808 30336 net.cpp:150] Setting up ip3
I0523 03:54:23.125820 30336 net.cpp:157] Top shape: 50 11 (550)
I0523 03:54:23.125830 30336 net.cpp:165] Memory required for data: 78936800
I0523 03:54:23.125845 30336 layer_factory.hpp:77] Creating layer drop3
I0523 03:54:23.125859 30336 net.cpp:106] Creating Layer drop3
I0523 03:54:23.125869 30336 net.cpp:454] drop3 <- ip3
I0523 03:54:23.125881 30336 net.cpp:397] drop3 -> ip3 (in-place)
I0523 03:54:23.125921 30336 net.cpp:150] Setting up drop3
I0523 03:54:23.125934 30336 net.cpp:157] Top shape: 50 11 (550)
I0523 03:54:23.125944 30336 net.cpp:165] Memory required for data: 78939000
I0523 03:54:23.125954 30336 layer_factory.hpp:77] Creating layer loss
I0523 03:54:23.125973 30336 net.cpp:106] Creating Layer loss
I0523 03:54:23.125983 30336 net.cpp:454] loss <- ip3
I0523 03:54:23.125994 30336 net.cpp:454] loss <- label
I0523 03:54:23.126006 30336 net.cpp:411] loss -> loss
I0523 03:54:23.126024 30336 layer_factory.hpp:77] Creating layer loss
I0523 03:54:23.126667 30336 net.cpp:150] Setting up loss
I0523 03:54:23.126688 30336 net.cpp:157] Top shape: (1)
I0523 03:54:23.126700 30336 net.cpp:160]     with loss weight 1
I0523 03:54:23.126745 30336 net.cpp:165] Memory required for data: 78939004
I0523 03:54:23.126756 30336 net.cpp:226] loss needs backward computation.
I0523 03:54:23.126768 30336 net.cpp:226] drop3 needs backward computation.
I0523 03:54:23.126776 30336 net.cpp:226] ip3 needs backward computation.
I0523 03:54:23.126787 30336 net.cpp:226] drop2 needs backward computation.
I0523 03:54:23.126797 30336 net.cpp:226] relu6 needs backward computation.
I0523 03:54:23.126807 30336 net.cpp:226] ip2 needs backward computation.
I0523 03:54:23.126818 30336 net.cpp:226] drop1 needs backward computation.
I0523 03:54:23.126827 30336 net.cpp:226] relu5 needs backward computation.
I0523 03:54:23.126837 30336 net.cpp:226] ip1 needs backward computation.
I0523 03:54:23.126847 30336 net.cpp:226] pool4 needs backward computation.
I0523 03:54:23.126857 30336 net.cpp:226] relu4 needs backward computation.
I0523 03:54:23.126868 30336 net.cpp:226] conv4 needs backward computation.
I0523 03:54:23.126878 30336 net.cpp:226] pool3 needs backward computation.
I0523 03:54:23.126888 30336 net.cpp:226] relu3 needs backward computation.
I0523 03:54:23.126896 30336 net.cpp:226] conv3 needs backward computation.
I0523 03:54:23.126915 30336 net.cpp:226] pool2 needs backward computation.
I0523 03:54:23.126926 30336 net.cpp:226] relu2 needs backward computation.
I0523 03:54:23.126937 30336 net.cpp:226] conv2 needs backward computation.
I0523 03:54:23.126947 30336 net.cpp:226] pool1 needs backward computation.
I0523 03:54:23.126958 30336 net.cpp:226] relu1 needs backward computation.
I0523 03:54:23.126968 30336 net.cpp:226] conv1 needs backward computation.
I0523 03:54:23.126979 30336 net.cpp:228] data_hdf5 does not need backward computation.
I0523 03:54:23.126989 30336 net.cpp:270] This network produces output loss
I0523 03:54:23.127013 30336 net.cpp:283] Network initialization done.
I0523 03:54:23.128645 30336 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326.prototxt
I0523 03:54:23.128716 30336 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0523 03:54:23.129073 30336 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0523 03:54:23.129266 30336 layer_factory.hpp:77] Creating layer data_hdf5
I0523 03:54:23.129281 30336 net.cpp:106] Creating Layer data_hdf5
I0523 03:54:23.129293 30336 net.cpp:411] data_hdf5 -> data
I0523 03:54:23.129312 30336 net.cpp:411] data_hdf5 -> label
I0523 03:54:23.129326 30336 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0523 03:54:23.142722 30336 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0523 03:54:44.436244 30336 net.cpp:150] Setting up data_hdf5
I0523 03:54:44.436413 30336 net.cpp:157] Top shape: 50 1 127 50 (317500)
I0523 03:54:44.436427 30336 net.cpp:157] Top shape: 50 (50)
I0523 03:54:44.436439 30336 net.cpp:165] Memory required for data: 1270200
I0523 03:54:44.436453 30336 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0523 03:54:44.436481 30336 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0523 03:54:44.436491 30336 net.cpp:454] label_data_hdf5_1_split <- label
I0523 03:54:44.436506 30336 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0523 03:54:44.436527 30336 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0523 03:54:44.436599 30336 net.cpp:150] Setting up label_data_hdf5_1_split
I0523 03:54:44.436614 30336 net.cpp:157] Top shape: 50 (50)
I0523 03:54:44.436625 30336 net.cpp:157] Top shape: 50 (50)
I0523 03:54:44.436635 30336 net.cpp:165] Memory required for data: 1270600
I0523 03:54:44.436645 30336 layer_factory.hpp:77] Creating layer conv1
I0523 03:54:44.436666 30336 net.cpp:106] Creating Layer conv1
I0523 03:54:44.436677 30336 net.cpp:454] conv1 <- data
I0523 03:54:44.436691 30336 net.cpp:411] conv1 -> conv1
I0523 03:54:44.438607 30336 net.cpp:150] Setting up conv1
I0523 03:54:44.438632 30336 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 03:54:44.438643 30336 net.cpp:165] Memory required for data: 15094600
I0523 03:54:44.438664 30336 layer_factory.hpp:77] Creating layer relu1
I0523 03:54:44.438679 30336 net.cpp:106] Creating Layer relu1
I0523 03:54:44.438689 30336 net.cpp:454] relu1 <- conv1
I0523 03:54:44.438701 30336 net.cpp:397] relu1 -> conv1 (in-place)
I0523 03:54:44.439196 30336 net.cpp:150] Setting up relu1
I0523 03:54:44.439213 30336 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 03:54:44.439224 30336 net.cpp:165] Memory required for data: 28918600
I0523 03:54:44.439234 30336 layer_factory.hpp:77] Creating layer pool1
I0523 03:54:44.439250 30336 net.cpp:106] Creating Layer pool1
I0523 03:54:44.439260 30336 net.cpp:454] pool1 <- conv1
I0523 03:54:44.439272 30336 net.cpp:411] pool1 -> pool1
I0523 03:54:44.439347 30336 net.cpp:150] Setting up pool1
I0523 03:54:44.439363 30336 net.cpp:157] Top shape: 50 12 60 48 (1728000)
I0523 03:54:44.439371 30336 net.cpp:165] Memory required for data: 35830600
I0523 03:54:44.439379 30336 layer_factory.hpp:77] Creating layer conv2
I0523 03:54:44.439399 30336 net.cpp:106] Creating Layer conv2
I0523 03:54:44.439409 30336 net.cpp:454] conv2 <- pool1
I0523 03:54:44.439424 30336 net.cpp:411] conv2 -> conv2
I0523 03:54:44.441340 30336 net.cpp:150] Setting up conv2
I0523 03:54:44.441364 30336 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 03:54:44.441376 30336 net.cpp:165] Memory required for data: 45766600
I0523 03:54:44.441393 30336 layer_factory.hpp:77] Creating layer relu2
I0523 03:54:44.441407 30336 net.cpp:106] Creating Layer relu2
I0523 03:54:44.441418 30336 net.cpp:454] relu2 <- conv2
I0523 03:54:44.441431 30336 net.cpp:397] relu2 -> conv2 (in-place)
I0523 03:54:44.441762 30336 net.cpp:150] Setting up relu2
I0523 03:54:44.441776 30336 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 03:54:44.441787 30336 net.cpp:165] Memory required for data: 55702600
I0523 03:54:44.441798 30336 layer_factory.hpp:77] Creating layer pool2
I0523 03:54:44.441810 30336 net.cpp:106] Creating Layer pool2
I0523 03:54:44.441820 30336 net.cpp:454] pool2 <- conv2
I0523 03:54:44.441833 30336 net.cpp:411] pool2 -> pool2
I0523 03:54:44.441905 30336 net.cpp:150] Setting up pool2
I0523 03:54:44.441918 30336 net.cpp:157] Top shape: 50 20 27 46 (1242000)
I0523 03:54:44.441929 30336 net.cpp:165] Memory required for data: 60670600
I0523 03:54:44.441939 30336 layer_factory.hpp:77] Creating layer conv3
I0523 03:54:44.441958 30336 net.cpp:106] Creating Layer conv3
I0523 03:54:44.441969 30336 net.cpp:454] conv3 <- pool2
I0523 03:54:44.441983 30336 net.cpp:411] conv3 -> conv3
I0523 03:54:44.443948 30336 net.cpp:150] Setting up conv3
I0523 03:54:44.443984 30336 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 03:54:44.443995 30336 net.cpp:165] Memory required for data: 66091400
I0523 03:54:44.444026 30336 layer_factory.hpp:77] Creating layer relu3
I0523 03:54:44.444039 30336 net.cpp:106] Creating Layer relu3
I0523 03:54:44.444052 30336 net.cpp:454] relu3 <- conv3
I0523 03:54:44.444066 30336 net.cpp:397] relu3 -> conv3 (in-place)
I0523 03:54:44.444537 30336 net.cpp:150] Setting up relu3
I0523 03:54:44.444555 30336 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 03:54:44.444566 30336 net.cpp:165] Memory required for data: 71512200
I0523 03:54:44.444576 30336 layer_factory.hpp:77] Creating layer pool3
I0523 03:54:44.444588 30336 net.cpp:106] Creating Layer pool3
I0523 03:54:44.444598 30336 net.cpp:454] pool3 <- conv3
I0523 03:54:44.444612 30336 net.cpp:411] pool3 -> pool3
I0523 03:54:44.444682 30336 net.cpp:150] Setting up pool3
I0523 03:54:44.444696 30336 net.cpp:157] Top shape: 50 28 11 44 (677600)
I0523 03:54:44.444706 30336 net.cpp:165] Memory required for data: 74222600
I0523 03:54:44.444715 30336 layer_factory.hpp:77] Creating layer conv4
I0523 03:54:44.444731 30336 net.cpp:106] Creating Layer conv4
I0523 03:54:44.444741 30336 net.cpp:454] conv4 <- pool3
I0523 03:54:44.444756 30336 net.cpp:411] conv4 -> conv4
I0523 03:54:44.446810 30336 net.cpp:150] Setting up conv4
I0523 03:54:44.446832 30336 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 03:54:44.446846 30336 net.cpp:165] Memory required for data: 76037000
I0523 03:54:44.446861 30336 layer_factory.hpp:77] Creating layer relu4
I0523 03:54:44.446873 30336 net.cpp:106] Creating Layer relu4
I0523 03:54:44.446883 30336 net.cpp:454] relu4 <- conv4
I0523 03:54:44.446897 30336 net.cpp:397] relu4 -> conv4 (in-place)
I0523 03:54:44.447367 30336 net.cpp:150] Setting up relu4
I0523 03:54:44.447383 30336 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 03:54:44.447394 30336 net.cpp:165] Memory required for data: 77851400
I0523 03:54:44.447404 30336 layer_factory.hpp:77] Creating layer pool4
I0523 03:54:44.447417 30336 net.cpp:106] Creating Layer pool4
I0523 03:54:44.447427 30336 net.cpp:454] pool4 <- conv4
I0523 03:54:44.447440 30336 net.cpp:411] pool4 -> pool4
I0523 03:54:44.447510 30336 net.cpp:150] Setting up pool4
I0523 03:54:44.447523 30336 net.cpp:157] Top shape: 50 36 3 42 (226800)
I0523 03:54:44.447533 30336 net.cpp:165] Memory required for data: 78758600
I0523 03:54:44.447543 30336 layer_factory.hpp:77] Creating layer ip1
I0523 03:54:44.447556 30336 net.cpp:106] Creating Layer ip1
I0523 03:54:44.447567 30336 net.cpp:454] ip1 <- pool4
I0523 03:54:44.447581 30336 net.cpp:411] ip1 -> ip1
I0523 03:54:44.463101 30336 net.cpp:150] Setting up ip1
I0523 03:54:44.463130 30336 net.cpp:157] Top shape: 50 196 (9800)
I0523 03:54:44.463146 30336 net.cpp:165] Memory required for data: 78797800
I0523 03:54:44.463170 30336 layer_factory.hpp:77] Creating layer relu5
I0523 03:54:44.463184 30336 net.cpp:106] Creating Layer relu5
I0523 03:54:44.463196 30336 net.cpp:454] relu5 <- ip1
I0523 03:54:44.463208 30336 net.cpp:397] relu5 -> ip1 (in-place)
I0523 03:54:44.463556 30336 net.cpp:150] Setting up relu5
I0523 03:54:44.463569 30336 net.cpp:157] Top shape: 50 196 (9800)
I0523 03:54:44.463580 30336 net.cpp:165] Memory required for data: 78837000
I0523 03:54:44.463590 30336 layer_factory.hpp:77] Creating layer drop1
I0523 03:54:44.463609 30336 net.cpp:106] Creating Layer drop1
I0523 03:54:44.463619 30336 net.cpp:454] drop1 <- ip1
I0523 03:54:44.463632 30336 net.cpp:397] drop1 -> ip1 (in-place)
I0523 03:54:44.463678 30336 net.cpp:150] Setting up drop1
I0523 03:54:44.463691 30336 net.cpp:157] Top shape: 50 196 (9800)
I0523 03:54:44.463701 30336 net.cpp:165] Memory required for data: 78876200
I0523 03:54:44.463711 30336 layer_factory.hpp:77] Creating layer ip2
I0523 03:54:44.463726 30336 net.cpp:106] Creating Layer ip2
I0523 03:54:44.463735 30336 net.cpp:454] ip2 <- ip1
I0523 03:54:44.463749 30336 net.cpp:411] ip2 -> ip2
I0523 03:54:44.464233 30336 net.cpp:150] Setting up ip2
I0523 03:54:44.464246 30336 net.cpp:157] Top shape: 50 98 (4900)
I0523 03:54:44.464257 30336 net.cpp:165] Memory required for data: 78895800
I0523 03:54:44.464272 30336 layer_factory.hpp:77] Creating layer relu6
I0523 03:54:44.464298 30336 net.cpp:106] Creating Layer relu6
I0523 03:54:44.464308 30336 net.cpp:454] relu6 <- ip2
I0523 03:54:44.464320 30336 net.cpp:397] relu6 -> ip2 (in-place)
I0523 03:54:44.464854 30336 net.cpp:150] Setting up relu6
I0523 03:54:44.464870 30336 net.cpp:157] Top shape: 50 98 (4900)
I0523 03:54:44.464879 30336 net.cpp:165] Memory required for data: 78915400
I0523 03:54:44.464890 30336 layer_factory.hpp:77] Creating layer drop2
I0523 03:54:44.464905 30336 net.cpp:106] Creating Layer drop2
I0523 03:54:44.464915 30336 net.cpp:454] drop2 <- ip2
I0523 03:54:44.464927 30336 net.cpp:397] drop2 -> ip2 (in-place)
I0523 03:54:44.464972 30336 net.cpp:150] Setting up drop2
I0523 03:54:44.464984 30336 net.cpp:157] Top shape: 50 98 (4900)
I0523 03:54:44.464994 30336 net.cpp:165] Memory required for data: 78935000
I0523 03:54:44.465004 30336 layer_factory.hpp:77] Creating layer ip3
I0523 03:54:44.465018 30336 net.cpp:106] Creating Layer ip3
I0523 03:54:44.465029 30336 net.cpp:454] ip3 <- ip2
I0523 03:54:44.465042 30336 net.cpp:411] ip3 -> ip3
I0523 03:54:44.465263 30336 net.cpp:150] Setting up ip3
I0523 03:54:44.465276 30336 net.cpp:157] Top shape: 50 11 (550)
I0523 03:54:44.465287 30336 net.cpp:165] Memory required for data: 78937200
I0523 03:54:44.465302 30336 layer_factory.hpp:77] Creating layer drop3
I0523 03:54:44.465315 30336 net.cpp:106] Creating Layer drop3
I0523 03:54:44.465325 30336 net.cpp:454] drop3 <- ip3
I0523 03:54:44.465338 30336 net.cpp:397] drop3 -> ip3 (in-place)
I0523 03:54:44.465379 30336 net.cpp:150] Setting up drop3
I0523 03:54:44.465392 30336 net.cpp:157] Top shape: 50 11 (550)
I0523 03:54:44.465402 30336 net.cpp:165] Memory required for data: 78939400
I0523 03:54:44.465412 30336 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0523 03:54:44.465425 30336 net.cpp:106] Creating Layer ip3_drop3_0_split
I0523 03:54:44.465435 30336 net.cpp:454] ip3_drop3_0_split <- ip3
I0523 03:54:44.465447 30336 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0523 03:54:44.465462 30336 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0523 03:54:44.465535 30336 net.cpp:150] Setting up ip3_drop3_0_split
I0523 03:54:44.465548 30336 net.cpp:157] Top shape: 50 11 (550)
I0523 03:54:44.465560 30336 net.cpp:157] Top shape: 50 11 (550)
I0523 03:54:44.465570 30336 net.cpp:165] Memory required for data: 78943800
I0523 03:54:44.465579 30336 layer_factory.hpp:77] Creating layer accuracy
I0523 03:54:44.465600 30336 net.cpp:106] Creating Layer accuracy
I0523 03:54:44.465611 30336 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0523 03:54:44.465622 30336 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0523 03:54:44.465636 30336 net.cpp:411] accuracy -> accuracy
I0523 03:54:44.465659 30336 net.cpp:150] Setting up accuracy
I0523 03:54:44.465672 30336 net.cpp:157] Top shape: (1)
I0523 03:54:44.465682 30336 net.cpp:165] Memory required for data: 78943804
I0523 03:54:44.465689 30336 layer_factory.hpp:77] Creating layer loss
I0523 03:54:44.465703 30336 net.cpp:106] Creating Layer loss
I0523 03:54:44.465713 30336 net.cpp:454] loss <- ip3_drop3_0_split_1
I0523 03:54:44.465724 30336 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0523 03:54:44.465737 30336 net.cpp:411] loss -> loss
I0523 03:54:44.465755 30336 layer_factory.hpp:77] Creating layer loss
I0523 03:54:44.466238 30336 net.cpp:150] Setting up loss
I0523 03:54:44.466253 30336 net.cpp:157] Top shape: (1)
I0523 03:54:44.466264 30336 net.cpp:160]     with loss weight 1
I0523 03:54:44.466284 30336 net.cpp:165] Memory required for data: 78943808
I0523 03:54:44.466295 30336 net.cpp:226] loss needs backward computation.
I0523 03:54:44.466306 30336 net.cpp:228] accuracy does not need backward computation.
I0523 03:54:44.466317 30336 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0523 03:54:44.466327 30336 net.cpp:226] drop3 needs backward computation.
I0523 03:54:44.466338 30336 net.cpp:226] ip3 needs backward computation.
I0523 03:54:44.466349 30336 net.cpp:226] drop2 needs backward computation.
I0523 03:54:44.466359 30336 net.cpp:226] relu6 needs backward computation.
I0523 03:54:44.466377 30336 net.cpp:226] ip2 needs backward computation.
I0523 03:54:44.466387 30336 net.cpp:226] drop1 needs backward computation.
I0523 03:54:44.466397 30336 net.cpp:226] relu5 needs backward computation.
I0523 03:54:44.466406 30336 net.cpp:226] ip1 needs backward computation.
I0523 03:54:44.466416 30336 net.cpp:226] pool4 needs backward computation.
I0523 03:54:44.466426 30336 net.cpp:226] relu4 needs backward computation.
I0523 03:54:44.466436 30336 net.cpp:226] conv4 needs backward computation.
I0523 03:54:44.466446 30336 net.cpp:226] pool3 needs backward computation.
I0523 03:54:44.466456 30336 net.cpp:226] relu3 needs backward computation.
I0523 03:54:44.466466 30336 net.cpp:226] conv3 needs backward computation.
I0523 03:54:44.466477 30336 net.cpp:226] pool2 needs backward computation.
I0523 03:54:44.466487 30336 net.cpp:226] relu2 needs backward computation.
I0523 03:54:44.466498 30336 net.cpp:226] conv2 needs backward computation.
I0523 03:54:44.466508 30336 net.cpp:226] pool1 needs backward computation.
I0523 03:54:44.466518 30336 net.cpp:226] relu1 needs backward computation.
I0523 03:54:44.466528 30336 net.cpp:226] conv1 needs backward computation.
I0523 03:54:44.466539 30336 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0523 03:54:44.466552 30336 net.cpp:228] data_hdf5 does not need backward computation.
I0523 03:54:44.466562 30336 net.cpp:270] This network produces output accuracy
I0523 03:54:44.466572 30336 net.cpp:270] This network produces output loss
I0523 03:54:44.466601 30336 net.cpp:283] Network initialization done.
I0523 03:54:44.466735 30336 solver.cpp:60] Solver scaffolding done.
I0523 03:54:44.467864 30336 caffe.cpp:212] Starting Optimization
I0523 03:54:44.467882 30336 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0523 03:54:44.467895 30336 solver.cpp:289] Learning Rate Policy: fixed
I0523 03:54:44.469117 30336 solver.cpp:341] Iteration 0, Testing net (#0)
I0523 03:55:33.212625 30336 solver.cpp:409]     Test net output #0: accuracy = 0.059253
I0523 03:55:33.212790 30336 solver.cpp:409]     Test net output #1: loss = 2.3988 (* 1 = 2.3988 loss)
I0523 03:55:33.237053 30336 solver.cpp:237] Iteration 0, loss = 2.39948
I0523 03:55:33.237090 30336 solver.cpp:253]     Train net output #0: loss = 2.39948 (* 1 = 2.39948 loss)
I0523 03:55:33.237108 30336 sgd_solver.cpp:106] Iteration 0, lr = 0.0015
I0523 03:55:42.516206 30336 solver.cpp:237] Iteration 300, loss = 2.32112
I0523 03:55:42.516243 30336 solver.cpp:253]     Train net output #0: loss = 2.32112 (* 1 = 2.32112 loss)
I0523 03:55:42.516260 30336 sgd_solver.cpp:106] Iteration 300, lr = 0.0015
I0523 03:55:51.804505 30336 solver.cpp:237] Iteration 600, loss = 2.08216
I0523 03:55:51.804543 30336 solver.cpp:253]     Train net output #0: loss = 2.08216 (* 1 = 2.08216 loss)
I0523 03:55:51.804558 30336 sgd_solver.cpp:106] Iteration 600, lr = 0.0015
I0523 03:56:01.089993 30336 solver.cpp:237] Iteration 900, loss = 2.0407
I0523 03:56:01.090039 30336 solver.cpp:253]     Train net output #0: loss = 2.0407 (* 1 = 2.0407 loss)
I0523 03:56:01.090059 30336 sgd_solver.cpp:106] Iteration 900, lr = 0.0015
I0523 03:56:10.378610 30336 solver.cpp:237] Iteration 1200, loss = 1.90256
I0523 03:56:10.378773 30336 solver.cpp:253]     Train net output #0: loss = 1.90256 (* 1 = 1.90256 loss)
I0523 03:56:10.378788 30336 sgd_solver.cpp:106] Iteration 1200, lr = 0.0015
I0523 03:56:19.666034 30336 solver.cpp:237] Iteration 1500, loss = 2.051
I0523 03:56:19.666070 30336 solver.cpp:253]     Train net output #0: loss = 2.051 (* 1 = 2.051 loss)
I0523 03:56:19.666087 30336 sgd_solver.cpp:106] Iteration 1500, lr = 0.0015
I0523 03:56:28.952472 30336 solver.cpp:237] Iteration 1800, loss = 1.9956
I0523 03:56:28.952524 30336 solver.cpp:253]     Train net output #0: loss = 1.9956 (* 1 = 1.9956 loss)
I0523 03:56:28.952540 30336 sgd_solver.cpp:106] Iteration 1800, lr = 0.0015
I0523 03:57:00.413573 30336 solver.cpp:237] Iteration 2100, loss = 1.83216
I0523 03:57:00.413741 30336 solver.cpp:253]     Train net output #0: loss = 1.83216 (* 1 = 1.83216 loss)
I0523 03:57:00.413755 30336 sgd_solver.cpp:106] Iteration 2100, lr = 0.0015
I0523 03:57:09.705518 30336 solver.cpp:237] Iteration 2400, loss = 1.66525
I0523 03:57:09.705552 30336 solver.cpp:253]     Train net output #0: loss = 1.66525 (* 1 = 1.66525 loss)
I0523 03:57:09.705570 30336 sgd_solver.cpp:106] Iteration 2400, lr = 0.0015
I0523 03:57:18.995578 30336 solver.cpp:237] Iteration 2700, loss = 1.97479
I0523 03:57:18.995625 30336 solver.cpp:253]     Train net output #0: loss = 1.97479 (* 1 = 1.97479 loss)
I0523 03:57:18.995642 30336 sgd_solver.cpp:106] Iteration 2700, lr = 0.0015
I0523 03:57:28.254987 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_3000.caffemodel
I0523 03:57:28.317982 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_3000.solverstate
I0523 03:57:28.352489 30336 solver.cpp:237] Iteration 3000, loss = 1.93399
I0523 03:57:28.352535 30336 solver.cpp:253]     Train net output #0: loss = 1.93399 (* 1 = 1.93399 loss)
I0523 03:57:28.352556 30336 sgd_solver.cpp:106] Iteration 3000, lr = 0.0015
I0523 03:57:37.642830 30336 solver.cpp:237] Iteration 3300, loss = 1.36692
I0523 03:57:37.642983 30336 solver.cpp:253]     Train net output #0: loss = 1.36692 (* 1 = 1.36692 loss)
I0523 03:57:37.642997 30336 sgd_solver.cpp:106] Iteration 3300, lr = 0.0015
I0523 03:57:46.931386 30336 solver.cpp:237] Iteration 3600, loss = 1.64011
I0523 03:57:46.931421 30336 solver.cpp:253]     Train net output #0: loss = 1.64011 (* 1 = 1.64011 loss)
I0523 03:57:46.931440 30336 sgd_solver.cpp:106] Iteration 3600, lr = 0.0015
I0523 03:57:56.221633 30336 solver.cpp:237] Iteration 3900, loss = 1.85587
I0523 03:57:56.221669 30336 solver.cpp:253]     Train net output #0: loss = 1.85587 (* 1 = 1.85587 loss)
I0523 03:57:56.221685 30336 sgd_solver.cpp:106] Iteration 3900, lr = 0.0015
I0523 03:58:27.665340 30336 solver.cpp:237] Iteration 4200, loss = 1.57527
I0523 03:58:27.665516 30336 solver.cpp:253]     Train net output #0: loss = 1.57527 (* 1 = 1.57527 loss)
I0523 03:58:27.665531 30336 sgd_solver.cpp:106] Iteration 4200, lr = 0.0015
I0523 03:58:36.953874 30336 solver.cpp:237] Iteration 4500, loss = 1.88022
I0523 03:58:36.953910 30336 solver.cpp:253]     Train net output #0: loss = 1.88022 (* 1 = 1.88022 loss)
I0523 03:58:36.953928 30336 sgd_solver.cpp:106] Iteration 4500, lr = 0.0015
I0523 03:58:46.244287 30336 solver.cpp:237] Iteration 4800, loss = 1.77422
I0523 03:58:46.244321 30336 solver.cpp:253]     Train net output #0: loss = 1.77422 (* 1 = 1.77422 loss)
I0523 03:58:46.244338 30336 sgd_solver.cpp:106] Iteration 4800, lr = 0.0015
I0523 03:58:55.537827 30336 solver.cpp:237] Iteration 5100, loss = 1.56432
I0523 03:58:55.537873 30336 solver.cpp:253]     Train net output #0: loss = 1.56432 (* 1 = 1.56432 loss)
I0523 03:58:55.537892 30336 sgd_solver.cpp:106] Iteration 5100, lr = 0.0015
I0523 03:59:04.832409 30336 solver.cpp:237] Iteration 5400, loss = 1.83217
I0523 03:59:04.832556 30336 solver.cpp:253]     Train net output #0: loss = 1.83217 (* 1 = 1.83217 loss)
I0523 03:59:04.832569 30336 sgd_solver.cpp:106] Iteration 5400, lr = 0.0015
I0523 03:59:14.121126 30336 solver.cpp:237] Iteration 5700, loss = 1.45842
I0523 03:59:14.121162 30336 solver.cpp:253]     Train net output #0: loss = 1.45842 (* 1 = 1.45842 loss)
I0523 03:59:14.121179 30336 sgd_solver.cpp:106] Iteration 5700, lr = 0.0015
I0523 03:59:23.381083 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_6000.caffemodel
I0523 03:59:23.439671 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_6000.solverstate
I0523 03:59:23.464689 30336 solver.cpp:341] Iteration 6000, Testing net (#0)
I0523 04:00:11.233599 30336 solver.cpp:409]     Test net output #0: accuracy = 0.729173
I0523 04:00:11.233763 30336 solver.cpp:409]     Test net output #1: loss = 0.976893 (* 1 = 0.976893 loss)
I0523 04:00:33.403281 30336 solver.cpp:237] Iteration 6000, loss = 1.59172
I0523 04:00:33.403337 30336 solver.cpp:253]     Train net output #0: loss = 1.59172 (* 1 = 1.59172 loss)
I0523 04:00:33.403354 30336 sgd_solver.cpp:106] Iteration 6000, lr = 0.0015
I0523 04:00:42.688925 30336 solver.cpp:237] Iteration 6300, loss = 1.68338
I0523 04:00:42.689066 30336 solver.cpp:253]     Train net output #0: loss = 1.68338 (* 1 = 1.68338 loss)
I0523 04:00:42.689080 30336 sgd_solver.cpp:106] Iteration 6300, lr = 0.0015
I0523 04:00:51.972151 30336 solver.cpp:237] Iteration 6600, loss = 1.61045
I0523 04:00:51.972184 30336 solver.cpp:253]     Train net output #0: loss = 1.61045 (* 1 = 1.61045 loss)
I0523 04:00:51.972203 30336 sgd_solver.cpp:106] Iteration 6600, lr = 0.0015
I0523 04:01:01.256618 30336 solver.cpp:237] Iteration 6900, loss = 1.71079
I0523 04:01:01.256664 30336 solver.cpp:253]     Train net output #0: loss = 1.71079 (* 1 = 1.71079 loss)
I0523 04:01:01.256682 30336 sgd_solver.cpp:106] Iteration 6900, lr = 0.0015
I0523 04:01:10.541684 30336 solver.cpp:237] Iteration 7200, loss = 1.33305
I0523 04:01:10.541719 30336 solver.cpp:253]     Train net output #0: loss = 1.33305 (* 1 = 1.33305 loss)
I0523 04:01:10.541736 30336 sgd_solver.cpp:106] Iteration 7200, lr = 0.0015
I0523 04:01:19.828331 30336 solver.cpp:237] Iteration 7500, loss = 1.74847
I0523 04:01:19.828469 30336 solver.cpp:253]     Train net output #0: loss = 1.74847 (* 1 = 1.74847 loss)
I0523 04:01:19.828483 30336 sgd_solver.cpp:106] Iteration 7500, lr = 0.0015
I0523 04:01:29.114121 30336 solver.cpp:237] Iteration 7800, loss = 1.25038
I0523 04:01:29.114167 30336 solver.cpp:253]     Train net output #0: loss = 1.25038 (* 1 = 1.25038 loss)
I0523 04:01:29.114184 30336 sgd_solver.cpp:106] Iteration 7800, lr = 0.0015
I0523 04:02:00.583531 30336 solver.cpp:237] Iteration 8100, loss = 1.51521
I0523 04:02:00.583703 30336 solver.cpp:253]     Train net output #0: loss = 1.51521 (* 1 = 1.51521 loss)
I0523 04:02:00.583719 30336 sgd_solver.cpp:106] Iteration 8100, lr = 0.0015
I0523 04:02:09.867095 30336 solver.cpp:237] Iteration 8400, loss = 1.42839
I0523 04:02:09.867130 30336 solver.cpp:253]     Train net output #0: loss = 1.42839 (* 1 = 1.42839 loss)
I0523 04:02:09.867148 30336 sgd_solver.cpp:106] Iteration 8400, lr = 0.0015
I0523 04:02:19.152143 30336 solver.cpp:237] Iteration 8700, loss = 1.42016
I0523 04:02:19.152187 30336 solver.cpp:253]     Train net output #0: loss = 1.42016 (* 1 = 1.42016 loss)
I0523 04:02:19.152205 30336 sgd_solver.cpp:106] Iteration 8700, lr = 0.0015
I0523 04:02:28.405503 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_9000.caffemodel
I0523 04:02:28.466529 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_9000.solverstate
I0523 04:02:28.503257 30336 solver.cpp:237] Iteration 9000, loss = 1.59307
I0523 04:02:28.503309 30336 solver.cpp:253]     Train net output #0: loss = 1.59307 (* 1 = 1.59307 loss)
I0523 04:02:28.503324 30336 sgd_solver.cpp:106] Iteration 9000, lr = 0.0015
I0523 04:02:37.789741 30336 solver.cpp:237] Iteration 9300, loss = 1.47901
I0523 04:02:37.789911 30336 solver.cpp:253]     Train net output #0: loss = 1.47901 (* 1 = 1.47901 loss)
I0523 04:02:37.789927 30336 sgd_solver.cpp:106] Iteration 9300, lr = 0.0015
I0523 04:02:47.078171 30336 solver.cpp:237] Iteration 9600, loss = 1.64047
I0523 04:02:47.078207 30336 solver.cpp:253]     Train net output #0: loss = 1.64047 (* 1 = 1.64047 loss)
I0523 04:02:47.078224 30336 sgd_solver.cpp:106] Iteration 9600, lr = 0.0015
I0523 04:02:56.362208 30336 solver.cpp:237] Iteration 9900, loss = 1.41395
I0523 04:02:56.362243 30336 solver.cpp:253]     Train net output #0: loss = 1.41395 (* 1 = 1.41395 loss)
I0523 04:02:56.362262 30336 sgd_solver.cpp:106] Iteration 9900, lr = 0.0015
I0523 04:03:27.830066 30336 solver.cpp:237] Iteration 10200, loss = 1.27324
I0523 04:03:27.830219 30336 solver.cpp:253]     Train net output #0: loss = 1.27324 (* 1 = 1.27324 loss)
I0523 04:03:27.830234 30336 sgd_solver.cpp:106] Iteration 10200, lr = 0.0015
I0523 04:03:37.119230 30336 solver.cpp:237] Iteration 10500, loss = 1.26358
I0523 04:03:37.119263 30336 solver.cpp:253]     Train net output #0: loss = 1.26358 (* 1 = 1.26358 loss)
I0523 04:03:37.119282 30336 sgd_solver.cpp:106] Iteration 10500, lr = 0.0015
I0523 04:03:46.404793 30336 solver.cpp:237] Iteration 10800, loss = 1.50082
I0523 04:03:46.404829 30336 solver.cpp:253]     Train net output #0: loss = 1.50082 (* 1 = 1.50082 loss)
I0523 04:03:46.404847 30336 sgd_solver.cpp:106] Iteration 10800, lr = 0.0015
I0523 04:03:55.690721 30336 solver.cpp:237] Iteration 11100, loss = 1.48492
I0523 04:03:55.690770 30336 solver.cpp:253]     Train net output #0: loss = 1.48492 (* 1 = 1.48492 loss)
I0523 04:03:55.690786 30336 sgd_solver.cpp:106] Iteration 11100, lr = 0.0015
I0523 04:04:04.978262 30336 solver.cpp:237] Iteration 11400, loss = 1.34212
I0523 04:04:04.978402 30336 solver.cpp:253]     Train net output #0: loss = 1.34212 (* 1 = 1.34212 loss)
I0523 04:04:04.978416 30336 sgd_solver.cpp:106] Iteration 11400, lr = 0.0015
I0523 04:04:14.266082 30336 solver.cpp:237] Iteration 11700, loss = 1.00954
I0523 04:04:14.266116 30336 solver.cpp:253]     Train net output #0: loss = 1.00954 (* 1 = 1.00954 loss)
I0523 04:04:14.266131 30336 sgd_solver.cpp:106] Iteration 11700, lr = 0.0015
I0523 04:04:23.521122 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_12000.caffemodel
I0523 04:04:23.591056 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_12000.solverstate
I0523 04:04:23.618621 30336 solver.cpp:341] Iteration 12000, Testing net (#0)
I0523 04:05:32.141438 30336 solver.cpp:409]     Test net output #0: accuracy = 0.813471
I0523 04:05:32.141602 30336 solver.cpp:409]     Test net output #1: loss = 0.699781 (* 1 = 0.699781 loss)
I0523 04:05:54.318573 30336 solver.cpp:237] Iteration 12000, loss = 1.3129
I0523 04:05:54.318629 30336 solver.cpp:253]     Train net output #0: loss = 1.3129 (* 1 = 1.3129 loss)
I0523 04:05:54.318645 30336 sgd_solver.cpp:106] Iteration 12000, lr = 0.0015
I0523 04:06:03.624658 30336 solver.cpp:237] Iteration 12300, loss = 1.41297
I0523 04:06:03.624850 30336 solver.cpp:253]     Train net output #0: loss = 1.41297 (* 1 = 1.41297 loss)
I0523 04:06:03.624864 30336 sgd_solver.cpp:106] Iteration 12300, lr = 0.0015
I0523 04:06:12.930951 30336 solver.cpp:237] Iteration 12600, loss = 1.73342
I0523 04:06:12.930986 30336 solver.cpp:253]     Train net output #0: loss = 1.73342 (* 1 = 1.73342 loss)
I0523 04:06:12.931000 30336 sgd_solver.cpp:106] Iteration 12600, lr = 0.0015
I0523 04:06:22.231813 30336 solver.cpp:237] Iteration 12900, loss = 1.40663
I0523 04:06:22.231849 30336 solver.cpp:253]     Train net output #0: loss = 1.40663 (* 1 = 1.40663 loss)
I0523 04:06:22.231866 30336 sgd_solver.cpp:106] Iteration 12900, lr = 0.0015
I0523 04:06:31.537561 30336 solver.cpp:237] Iteration 13200, loss = 1.33789
I0523 04:06:31.537608 30336 solver.cpp:253]     Train net output #0: loss = 1.33789 (* 1 = 1.33789 loss)
I0523 04:06:31.537626 30336 sgd_solver.cpp:106] Iteration 13200, lr = 0.0015
I0523 04:06:40.846402 30336 solver.cpp:237] Iteration 13500, loss = 1.3128
I0523 04:06:40.846560 30336 solver.cpp:253]     Train net output #0: loss = 1.3128 (* 1 = 1.3128 loss)
I0523 04:06:40.846572 30336 sgd_solver.cpp:106] Iteration 13500, lr = 0.0015
I0523 04:06:50.148391 30336 solver.cpp:237] Iteration 13800, loss = 1.34682
I0523 04:06:50.148434 30336 solver.cpp:253]     Train net output #0: loss = 1.34682 (* 1 = 1.34682 loss)
I0523 04:06:50.148450 30336 sgd_solver.cpp:106] Iteration 13800, lr = 0.0015
I0523 04:07:21.628500 30336 solver.cpp:237] Iteration 14100, loss = 1.34181
I0523 04:07:21.628666 30336 solver.cpp:253]     Train net output #0: loss = 1.34181 (* 1 = 1.34181 loss)
I0523 04:07:21.628681 30336 sgd_solver.cpp:106] Iteration 14100, lr = 0.0015
I0523 04:07:30.931273 30336 solver.cpp:237] Iteration 14400, loss = 1.47051
I0523 04:07:30.931308 30336 solver.cpp:253]     Train net output #0: loss = 1.47051 (* 1 = 1.47051 loss)
I0523 04:07:30.931325 30336 sgd_solver.cpp:106] Iteration 14400, lr = 0.0015
I0523 04:07:40.231763 30336 solver.cpp:237] Iteration 14700, loss = 1.86716
I0523 04:07:40.231806 30336 solver.cpp:253]     Train net output #0: loss = 1.86716 (* 1 = 1.86716 loss)
I0523 04:07:40.231825 30336 sgd_solver.cpp:106] Iteration 14700, lr = 0.0015
I0523 04:07:49.505143 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_15000.caffemodel
I0523 04:07:49.566320 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_15000.solverstate
I0523 04:07:49.603406 30336 solver.cpp:237] Iteration 15000, loss = 1.31922
I0523 04:07:49.603459 30336 solver.cpp:253]     Train net output #0: loss = 1.31922 (* 1 = 1.31922 loss)
I0523 04:07:49.603473 30336 sgd_solver.cpp:106] Iteration 15000, lr = 0.0015
I0523 04:07:58.905210 30336 solver.cpp:237] Iteration 15300, loss = 1.27289
I0523 04:07:58.905354 30336 solver.cpp:253]     Train net output #0: loss = 1.27289 (* 1 = 1.27289 loss)
I0523 04:07:58.905369 30336 sgd_solver.cpp:106] Iteration 15300, lr = 0.0015
I0523 04:08:08.209900 30336 solver.cpp:237] Iteration 15600, loss = 1.15804
I0523 04:08:08.209954 30336 solver.cpp:253]     Train net output #0: loss = 1.15804 (* 1 = 1.15804 loss)
I0523 04:08:08.209969 30336 sgd_solver.cpp:106] Iteration 15600, lr = 0.0015
I0523 04:08:17.515514 30336 solver.cpp:237] Iteration 15900, loss = 1.79932
I0523 04:08:17.515548 30336 solver.cpp:253]     Train net output #0: loss = 1.79932 (* 1 = 1.79932 loss)
I0523 04:08:17.515566 30336 sgd_solver.cpp:106] Iteration 15900, lr = 0.0015
I0523 04:08:48.985357 30336 solver.cpp:237] Iteration 16200, loss = 1.25636
I0523 04:08:48.985543 30336 solver.cpp:253]     Train net output #0: loss = 1.25636 (* 1 = 1.25636 loss)
I0523 04:08:48.985558 30336 sgd_solver.cpp:106] Iteration 16200, lr = 0.0015
I0523 04:08:58.287717 30336 solver.cpp:237] Iteration 16500, loss = 1.24659
I0523 04:08:58.287775 30336 solver.cpp:253]     Train net output #0: loss = 1.24659 (* 1 = 1.24659 loss)
I0523 04:08:58.287789 30336 sgd_solver.cpp:106] Iteration 16500, lr = 0.0015
I0523 04:09:07.591090 30336 solver.cpp:237] Iteration 16800, loss = 1.32723
I0523 04:09:07.591126 30336 solver.cpp:253]     Train net output #0: loss = 1.32723 (* 1 = 1.32723 loss)
I0523 04:09:07.591142 30336 sgd_solver.cpp:106] Iteration 16800, lr = 0.0015
I0523 04:09:16.897698 30336 solver.cpp:237] Iteration 17100, loss = 1.25117
I0523 04:09:16.897733 30336 solver.cpp:253]     Train net output #0: loss = 1.25117 (* 1 = 1.25117 loss)
I0523 04:09:16.897750 30336 sgd_solver.cpp:106] Iteration 17100, lr = 0.0015
I0523 04:09:26.200017 30336 solver.cpp:237] Iteration 17400, loss = 1.27646
I0523 04:09:26.200168 30336 solver.cpp:253]     Train net output #0: loss = 1.27646 (* 1 = 1.27646 loss)
I0523 04:09:26.200183 30336 sgd_solver.cpp:106] Iteration 17400, lr = 0.0015
I0523 04:09:35.502552 30336 solver.cpp:237] Iteration 17700, loss = 1.2194
I0523 04:09:35.502588 30336 solver.cpp:253]     Train net output #0: loss = 1.2194 (* 1 = 1.2194 loss)
I0523 04:09:35.502605 30336 sgd_solver.cpp:106] Iteration 17700, lr = 0.0015
I0523 04:09:44.778400 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_18000.caffemodel
I0523 04:09:44.837187 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_18000.solverstate
I0523 04:09:44.863011 30336 solver.cpp:341] Iteration 18000, Testing net (#0)
I0523 04:10:32.263840 30336 solver.cpp:409]     Test net output #0: accuracy = 0.833857
I0523 04:10:32.264024 30336 solver.cpp:409]     Test net output #1: loss = 0.568199 (* 1 = 0.568199 loss)
I0523 04:10:54.437691 30336 solver.cpp:237] Iteration 18000, loss = 1.1115
I0523 04:10:54.437742 30336 solver.cpp:253]     Train net output #0: loss = 1.1115 (* 1 = 1.1115 loss)
I0523 04:10:54.437763 30336 sgd_solver.cpp:106] Iteration 18000, lr = 0.0015
I0523 04:11:03.727766 30336 solver.cpp:237] Iteration 18300, loss = 1.0737
I0523 04:11:03.727936 30336 solver.cpp:253]     Train net output #0: loss = 1.0737 (* 1 = 1.0737 loss)
I0523 04:11:03.727949 30336 sgd_solver.cpp:106] Iteration 18300, lr = 0.0015
I0523 04:11:13.020918 30336 solver.cpp:237] Iteration 18600, loss = 1.08172
I0523 04:11:13.020952 30336 solver.cpp:253]     Train net output #0: loss = 1.08172 (* 1 = 1.08172 loss)
I0523 04:11:13.020972 30336 sgd_solver.cpp:106] Iteration 18600, lr = 0.0015
I0523 04:11:22.307044 30336 solver.cpp:237] Iteration 18900, loss = 1.52759
I0523 04:11:22.307091 30336 solver.cpp:253]     Train net output #0: loss = 1.52759 (* 1 = 1.52759 loss)
I0523 04:11:22.307107 30336 sgd_solver.cpp:106] Iteration 18900, lr = 0.0015
I0523 04:11:31.596989 30336 solver.cpp:237] Iteration 19200, loss = 1.4483
I0523 04:11:31.597025 30336 solver.cpp:253]     Train net output #0: loss = 1.4483 (* 1 = 1.4483 loss)
I0523 04:11:31.597041 30336 sgd_solver.cpp:106] Iteration 19200, lr = 0.0015
I0523 04:11:40.883955 30336 solver.cpp:237] Iteration 19500, loss = 1.2076
I0523 04:11:40.884100 30336 solver.cpp:253]     Train net output #0: loss = 1.2076 (* 1 = 1.2076 loss)
I0523 04:11:40.884114 30336 sgd_solver.cpp:106] Iteration 19500, lr = 0.0015
I0523 04:11:50.174614 30336 solver.cpp:237] Iteration 19800, loss = 1.19666
I0523 04:11:50.174666 30336 solver.cpp:253]     Train net output #0: loss = 1.19666 (* 1 = 1.19666 loss)
I0523 04:11:50.174680 30336 sgd_solver.cpp:106] Iteration 19800, lr = 0.0015
I0523 04:12:21.687820 30336 solver.cpp:237] Iteration 20100, loss = 1.2522
I0523 04:12:21.688004 30336 solver.cpp:253]     Train net output #0: loss = 1.2522 (* 1 = 1.2522 loss)
I0523 04:12:21.688020 30336 sgd_solver.cpp:106] Iteration 20100, lr = 0.0015
I0523 04:12:30.978873 30336 solver.cpp:237] Iteration 20400, loss = 1.55249
I0523 04:12:30.978909 30336 solver.cpp:253]     Train net output #0: loss = 1.55249 (* 1 = 1.55249 loss)
I0523 04:12:30.978926 30336 sgd_solver.cpp:106] Iteration 20400, lr = 0.0015
I0523 04:12:40.268733 30336 solver.cpp:237] Iteration 20700, loss = 1.19139
I0523 04:12:40.268784 30336 solver.cpp:253]     Train net output #0: loss = 1.19139 (* 1 = 1.19139 loss)
I0523 04:12:40.268800 30336 sgd_solver.cpp:106] Iteration 20700, lr = 0.0015
I0523 04:12:49.527519 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_21000.caffemodel
I0523 04:12:49.586371 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_21000.solverstate
I0523 04:12:49.622133 30336 solver.cpp:237] Iteration 21000, loss = 1.14955
I0523 04:12:49.622181 30336 solver.cpp:253]     Train net output #0: loss = 1.14955 (* 1 = 1.14955 loss)
I0523 04:12:49.622200 30336 sgd_solver.cpp:106] Iteration 21000, lr = 0.0015
I0523 04:12:58.913785 30336 solver.cpp:237] Iteration 21300, loss = 1.54123
I0523 04:12:58.913943 30336 solver.cpp:253]     Train net output #0: loss = 1.54123 (* 1 = 1.54123 loss)
I0523 04:12:58.913956 30336 sgd_solver.cpp:106] Iteration 21300, lr = 0.0015
I0523 04:13:08.206632 30336 solver.cpp:237] Iteration 21600, loss = 1.38901
I0523 04:13:08.206686 30336 solver.cpp:253]     Train net output #0: loss = 1.38901 (* 1 = 1.38901 loss)
I0523 04:13:08.206701 30336 sgd_solver.cpp:106] Iteration 21600, lr = 0.0015
I0523 04:13:17.496418 30336 solver.cpp:237] Iteration 21900, loss = 1.35873
I0523 04:13:17.496454 30336 solver.cpp:253]     Train net output #0: loss = 1.35873 (* 1 = 1.35873 loss)
I0523 04:13:17.496470 30336 sgd_solver.cpp:106] Iteration 21900, lr = 0.0015
I0523 04:13:48.972575 30336 solver.cpp:237] Iteration 22200, loss = 1.40522
I0523 04:13:48.972748 30336 solver.cpp:253]     Train net output #0: loss = 1.40522 (* 1 = 1.40522 loss)
I0523 04:13:48.972764 30336 sgd_solver.cpp:106] Iteration 22200, lr = 0.0015
I0523 04:13:58.265295 30336 solver.cpp:237] Iteration 22500, loss = 1.20775
I0523 04:13:58.265347 30336 solver.cpp:253]     Train net output #0: loss = 1.20775 (* 1 = 1.20775 loss)
I0523 04:13:58.265362 30336 sgd_solver.cpp:106] Iteration 22500, lr = 0.0015
I0523 04:14:07.552359 30336 solver.cpp:237] Iteration 22800, loss = 1.51206
I0523 04:14:07.552395 30336 solver.cpp:253]     Train net output #0: loss = 1.51206 (* 1 = 1.51206 loss)
I0523 04:14:07.552412 30336 sgd_solver.cpp:106] Iteration 22800, lr = 0.0015
I0523 04:14:16.846177 30336 solver.cpp:237] Iteration 23100, loss = 1.17384
I0523 04:14:16.846211 30336 solver.cpp:253]     Train net output #0: loss = 1.17384 (* 1 = 1.17384 loss)
I0523 04:14:16.846230 30336 sgd_solver.cpp:106] Iteration 23100, lr = 0.0015
I0523 04:14:26.143426 30336 solver.cpp:237] Iteration 23400, loss = 1.59711
I0523 04:14:26.143586 30336 solver.cpp:253]     Train net output #0: loss = 1.59711 (* 1 = 1.59711 loss)
I0523 04:14:26.143600 30336 sgd_solver.cpp:106] Iteration 23400, lr = 0.0015
I0523 04:14:35.435098 30336 solver.cpp:237] Iteration 23700, loss = 1.36267
I0523 04:14:35.435133 30336 solver.cpp:253]     Train net output #0: loss = 1.36267 (* 1 = 1.36267 loss)
I0523 04:14:35.435151 30336 sgd_solver.cpp:106] Iteration 23700, lr = 0.0015
I0523 04:14:44.694696 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_24000.caffemodel
I0523 04:14:44.753686 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_24000.solverstate
I0523 04:14:44.779786 30336 solver.cpp:341] Iteration 24000, Testing net (#0)
I0523 04:15:53.325165 30336 solver.cpp:409]     Test net output #0: accuracy = 0.849255
I0523 04:15:53.325336 30336 solver.cpp:409]     Test net output #1: loss = 0.489429 (* 1 = 0.489429 loss)
I0523 04:16:15.534051 30336 solver.cpp:237] Iteration 24000, loss = 1.32696
I0523 04:16:15.534107 30336 solver.cpp:253]     Train net output #0: loss = 1.32696 (* 1 = 1.32696 loss)
I0523 04:16:15.534124 30336 sgd_solver.cpp:106] Iteration 24000, lr = 0.0015
I0523 04:16:24.831475 30336 solver.cpp:237] Iteration 24300, loss = 1.39606
I0523 04:16:24.831643 30336 solver.cpp:253]     Train net output #0: loss = 1.39606 (* 1 = 1.39606 loss)
I0523 04:16:24.831658 30336 sgd_solver.cpp:106] Iteration 24300, lr = 0.0015
I0523 04:16:34.130931 30336 solver.cpp:237] Iteration 24600, loss = 1.36051
I0523 04:16:34.130967 30336 solver.cpp:253]     Train net output #0: loss = 1.36051 (* 1 = 1.36051 loss)
I0523 04:16:34.130983 30336 sgd_solver.cpp:106] Iteration 24600, lr = 0.0015
I0523 04:16:43.425580 30336 solver.cpp:237] Iteration 24900, loss = 1.015
I0523 04:16:43.425616 30336 solver.cpp:253]     Train net output #0: loss = 1.015 (* 1 = 1.015 loss)
I0523 04:16:43.425631 30336 sgd_solver.cpp:106] Iteration 24900, lr = 0.0015
I0523 04:16:52.723335 30336 solver.cpp:237] Iteration 25200, loss = 1.42216
I0523 04:16:52.723387 30336 solver.cpp:253]     Train net output #0: loss = 1.42216 (* 1 = 1.42216 loss)
I0523 04:16:52.723402 30336 sgd_solver.cpp:106] Iteration 25200, lr = 0.0015
I0523 04:17:02.019019 30336 solver.cpp:237] Iteration 25500, loss = 1.3388
I0523 04:17:02.019165 30336 solver.cpp:253]     Train net output #0: loss = 1.3388 (* 1 = 1.3388 loss)
I0523 04:17:02.019178 30336 sgd_solver.cpp:106] Iteration 25500, lr = 0.0015
I0523 04:17:11.320443 30336 solver.cpp:237] Iteration 25800, loss = 1.2232
I0523 04:17:11.320477 30336 solver.cpp:253]     Train net output #0: loss = 1.2232 (* 1 = 1.2232 loss)
I0523 04:17:11.320497 30336 sgd_solver.cpp:106] Iteration 25800, lr = 0.0015
I0523 04:17:42.795632 30336 solver.cpp:237] Iteration 26100, loss = 1.34921
I0523 04:17:42.795806 30336 solver.cpp:253]     Train net output #0: loss = 1.34921 (* 1 = 1.34921 loss)
I0523 04:17:42.795822 30336 sgd_solver.cpp:106] Iteration 26100, lr = 0.0015
I0523 04:17:52.094451 30336 solver.cpp:237] Iteration 26400, loss = 1.51352
I0523 04:17:52.094486 30336 solver.cpp:253]     Train net output #0: loss = 1.51352 (* 1 = 1.51352 loss)
I0523 04:17:52.094503 30336 sgd_solver.cpp:106] Iteration 26400, lr = 0.0015
I0523 04:18:01.393447 30336 solver.cpp:237] Iteration 26700, loss = 1.21649
I0523 04:18:01.393482 30336 solver.cpp:253]     Train net output #0: loss = 1.21649 (* 1 = 1.21649 loss)
I0523 04:18:01.393499 30336 sgd_solver.cpp:106] Iteration 26700, lr = 0.0015
I0523 04:18:10.658325 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_27000.caffemodel
I0523 04:18:10.724356 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_27000.solverstate
I0523 04:18:10.777855 30336 solver.cpp:237] Iteration 27000, loss = 1.36413
I0523 04:18:10.777910 30336 solver.cpp:253]     Train net output #0: loss = 1.36413 (* 1 = 1.36413 loss)
I0523 04:18:10.777925 30336 sgd_solver.cpp:106] Iteration 27000, lr = 0.0015
I0523 04:18:20.071349 30336 solver.cpp:237] Iteration 27300, loss = 1.22701
I0523 04:18:20.071501 30336 solver.cpp:253]     Train net output #0: loss = 1.22701 (* 1 = 1.22701 loss)
I0523 04:18:20.071516 30336 sgd_solver.cpp:106] Iteration 27300, lr = 0.0015
I0523 04:18:29.368660 30336 solver.cpp:237] Iteration 27600, loss = 1.27211
I0523 04:18:29.368710 30336 solver.cpp:253]     Train net output #0: loss = 1.27211 (* 1 = 1.27211 loss)
I0523 04:18:29.368726 30336 sgd_solver.cpp:106] Iteration 27600, lr = 0.0015
I0523 04:18:38.668670 30336 solver.cpp:237] Iteration 27900, loss = 1.35407
I0523 04:18:38.668706 30336 solver.cpp:253]     Train net output #0: loss = 1.35407 (* 1 = 1.35407 loss)
I0523 04:18:38.668722 30336 sgd_solver.cpp:106] Iteration 27900, lr = 0.0015
I0523 04:19:10.122393 30336 solver.cpp:237] Iteration 28200, loss = 0.849036
I0523 04:19:10.122583 30336 solver.cpp:253]     Train net output #0: loss = 0.849036 (* 1 = 0.849036 loss)
I0523 04:19:10.122598 30336 sgd_solver.cpp:106] Iteration 28200, lr = 0.0015
I0523 04:19:19.419863 30336 solver.cpp:237] Iteration 28500, loss = 1.24154
I0523 04:19:19.419909 30336 solver.cpp:253]     Train net output #0: loss = 1.24154 (* 1 = 1.24154 loss)
I0523 04:19:19.419929 30336 sgd_solver.cpp:106] Iteration 28500, lr = 0.0015
I0523 04:19:28.717787 30336 solver.cpp:237] Iteration 28800, loss = 0.956225
I0523 04:19:28.717821 30336 solver.cpp:253]     Train net output #0: loss = 0.956225 (* 1 = 0.956225 loss)
I0523 04:19:28.717840 30336 sgd_solver.cpp:106] Iteration 28800, lr = 0.0015
I0523 04:19:38.012724 30336 solver.cpp:237] Iteration 29100, loss = 1.54599
I0523 04:19:38.012760 30336 solver.cpp:253]     Train net output #0: loss = 1.54599 (* 1 = 1.54599 loss)
I0523 04:19:38.012776 30336 sgd_solver.cpp:106] Iteration 29100, lr = 0.0015
I0523 04:19:47.306787 30336 solver.cpp:237] Iteration 29400, loss = 1.11104
I0523 04:19:47.306946 30336 solver.cpp:253]     Train net output #0: loss = 1.11104 (* 1 = 1.11104 loss)
I0523 04:19:47.306962 30336 sgd_solver.cpp:106] Iteration 29400, lr = 0.0015
I0523 04:19:56.602378 30336 solver.cpp:237] Iteration 29700, loss = 1.31805
I0523 04:19:56.602412 30336 solver.cpp:253]     Train net output #0: loss = 1.31805 (* 1 = 1.31805 loss)
I0523 04:19:56.602430 30336 sgd_solver.cpp:106] Iteration 29700, lr = 0.0015
I0523 04:20:05.865489 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_30000.caffemodel
I0523 04:20:05.927160 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_30000.solverstate
I0523 04:20:05.955481 30336 solver.cpp:341] Iteration 30000, Testing net (#0)
I0523 04:20:53.730897 30336 solver.cpp:409]     Test net output #0: accuracy = 0.856541
I0523 04:20:53.731072 30336 solver.cpp:409]     Test net output #1: loss = 0.478706 (* 1 = 0.478706 loss)
I0523 04:21:14.653579 30336 solver.cpp:237] Iteration 30000, loss = 0.989995
I0523 04:21:14.653637 30336 solver.cpp:253]     Train net output #0: loss = 0.989995 (* 1 = 0.989995 loss)
I0523 04:21:14.653652 30336 sgd_solver.cpp:106] Iteration 30000, lr = 0.0015
I0523 04:21:23.941254 30336 solver.cpp:237] Iteration 30300, loss = 1.25909
I0523 04:21:23.941416 30336 solver.cpp:253]     Train net output #0: loss = 1.25909 (* 1 = 1.25909 loss)
I0523 04:21:23.941429 30336 sgd_solver.cpp:106] Iteration 30300, lr = 0.0015
I0523 04:21:33.228870 30336 solver.cpp:237] Iteration 30600, loss = 1.50545
I0523 04:21:33.228904 30336 solver.cpp:253]     Train net output #0: loss = 1.50545 (* 1 = 1.50545 loss)
I0523 04:21:33.228921 30336 sgd_solver.cpp:106] Iteration 30600, lr = 0.0015
I0523 04:21:42.517997 30336 solver.cpp:237] Iteration 30900, loss = 1.31446
I0523 04:21:42.518033 30336 solver.cpp:253]     Train net output #0: loss = 1.31446 (* 1 = 1.31446 loss)
I0523 04:21:42.518049 30336 sgd_solver.cpp:106] Iteration 30900, lr = 0.0015
I0523 04:21:51.807131 30336 solver.cpp:237] Iteration 31200, loss = 1.25364
I0523 04:21:51.807173 30336 solver.cpp:253]     Train net output #0: loss = 1.25364 (* 1 = 1.25364 loss)
I0523 04:21:51.807198 30336 sgd_solver.cpp:106] Iteration 31200, lr = 0.0015
I0523 04:22:01.092466 30336 solver.cpp:237] Iteration 31500, loss = 1.42807
I0523 04:22:01.092612 30336 solver.cpp:253]     Train net output #0: loss = 1.42807 (* 1 = 1.42807 loss)
I0523 04:22:01.092624 30336 sgd_solver.cpp:106] Iteration 31500, lr = 0.0015
I0523 04:22:10.382532 30336 solver.cpp:237] Iteration 31800, loss = 1.41083
I0523 04:22:10.382567 30336 solver.cpp:253]     Train net output #0: loss = 1.41083 (* 1 = 1.41083 loss)
I0523 04:22:10.382586 30336 sgd_solver.cpp:106] Iteration 31800, lr = 0.0015
I0523 04:22:40.524416 30336 solver.cpp:237] Iteration 32100, loss = 1.23488
I0523 04:22:40.524600 30336 solver.cpp:253]     Train net output #0: loss = 1.23488 (* 1 = 1.23488 loss)
I0523 04:22:40.524616 30336 sgd_solver.cpp:106] Iteration 32100, lr = 0.0015
I0523 04:22:49.810528 30336 solver.cpp:237] Iteration 32400, loss = 1.30946
I0523 04:22:49.810562 30336 solver.cpp:253]     Train net output #0: loss = 1.30946 (* 1 = 1.30946 loss)
I0523 04:22:49.810580 30336 sgd_solver.cpp:106] Iteration 32400, lr = 0.0015
I0523 04:22:59.098204 30336 solver.cpp:237] Iteration 32700, loss = 1.78699
I0523 04:22:59.098240 30336 solver.cpp:253]     Train net output #0: loss = 1.78699 (* 1 = 1.78699 loss)
I0523 04:22:59.098256 30336 sgd_solver.cpp:106] Iteration 32700, lr = 0.0015
I0523 04:23:08.353073 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_33000.caffemodel
I0523 04:23:08.412068 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_33000.solverstate
I0523 04:23:08.447952 30336 solver.cpp:237] Iteration 33000, loss = 1.30264
I0523 04:23:08.448001 30336 solver.cpp:253]     Train net output #0: loss = 1.30264 (* 1 = 1.30264 loss)
I0523 04:23:08.448024 30336 sgd_solver.cpp:106] Iteration 33000, lr = 0.0015
I0523 04:23:17.740947 30336 solver.cpp:237] Iteration 33300, loss = 1.17534
I0523 04:23:17.741111 30336 solver.cpp:253]     Train net output #0: loss = 1.17534 (* 1 = 1.17534 loss)
I0523 04:23:17.741125 30336 sgd_solver.cpp:106] Iteration 33300, lr = 0.0015
I0523 04:23:27.027726 30336 solver.cpp:237] Iteration 33600, loss = 1.28395
I0523 04:23:27.027762 30336 solver.cpp:253]     Train net output #0: loss = 1.28395 (* 1 = 1.28395 loss)
I0523 04:23:27.027779 30336 sgd_solver.cpp:106] Iteration 33600, lr = 0.0015
I0523 04:23:36.315780 30336 solver.cpp:237] Iteration 33900, loss = 1.0085
I0523 04:23:36.315819 30336 solver.cpp:253]     Train net output #0: loss = 1.0085 (* 1 = 1.0085 loss)
I0523 04:23:36.315841 30336 sgd_solver.cpp:106] Iteration 33900, lr = 0.0015
I0523 04:24:06.453121 30336 solver.cpp:237] Iteration 34200, loss = 1.19753
I0523 04:24:06.453299 30336 solver.cpp:253]     Train net output #0: loss = 1.19753 (* 1 = 1.19753 loss)
I0523 04:24:06.453315 30336 sgd_solver.cpp:106] Iteration 34200, lr = 0.0015
I0523 04:24:15.741631 30336 solver.cpp:237] Iteration 34500, loss = 0.872441
I0523 04:24:15.741665 30336 solver.cpp:253]     Train net output #0: loss = 0.872441 (* 1 = 0.872441 loss)
I0523 04:24:15.741682 30336 sgd_solver.cpp:106] Iteration 34500, lr = 0.0015
I0523 04:24:25.026157 30336 solver.cpp:237] Iteration 34800, loss = 1.32777
I0523 04:24:25.026195 30336 solver.cpp:253]     Train net output #0: loss = 1.32777 (* 1 = 1.32777 loss)
I0523 04:24:25.026216 30336 sgd_solver.cpp:106] Iteration 34800, lr = 0.0015
I0523 04:24:34.314605 30336 solver.cpp:237] Iteration 35100, loss = 1.3261
I0523 04:24:34.314641 30336 solver.cpp:253]     Train net output #0: loss = 1.3261 (* 1 = 1.3261 loss)
I0523 04:24:34.314658 30336 sgd_solver.cpp:106] Iteration 35100, lr = 0.0015
I0523 04:24:43.601205 30336 solver.cpp:237] Iteration 35400, loss = 1.2844
I0523 04:24:43.601361 30336 solver.cpp:253]     Train net output #0: loss = 1.2844 (* 1 = 1.2844 loss)
I0523 04:24:43.601375 30336 sgd_solver.cpp:106] Iteration 35400, lr = 0.0015
I0523 04:24:52.885489 30336 solver.cpp:237] Iteration 35700, loss = 1.14767
I0523 04:24:52.885524 30336 solver.cpp:253]     Train net output #0: loss = 1.14767 (* 1 = 1.14767 loss)
I0523 04:24:52.885542 30336 sgd_solver.cpp:106] Iteration 35700, lr = 0.0015
I0523 04:25:02.143976 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_36000.caffemodel
I0523 04:25:02.203024 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_36000.solverstate
I0523 04:25:02.229089 30336 solver.cpp:341] Iteration 36000, Testing net (#0)
I0523 04:26:10.774684 30336 solver.cpp:409]     Test net output #0: accuracy = 0.861113
I0523 04:26:10.774866 30336 solver.cpp:409]     Test net output #1: loss = 0.446734 (* 1 = 0.446734 loss)
I0523 04:26:31.617014 30336 solver.cpp:237] Iteration 36000, loss = 0.987993
I0523 04:26:31.617072 30336 solver.cpp:253]     Train net output #0: loss = 0.987993 (* 1 = 0.987993 loss)
I0523 04:26:31.617087 30336 sgd_solver.cpp:106] Iteration 36000, lr = 0.0015
I0523 04:26:40.912770 30336 solver.cpp:237] Iteration 36300, loss = 1.25507
I0523 04:26:40.912928 30336 solver.cpp:253]     Train net output #0: loss = 1.25507 (* 1 = 1.25507 loss)
I0523 04:26:40.912943 30336 sgd_solver.cpp:106] Iteration 36300, lr = 0.0015
I0523 04:26:50.211166 30336 solver.cpp:237] Iteration 36600, loss = 1.10143
I0523 04:26:50.211213 30336 solver.cpp:253]     Train net output #0: loss = 1.10143 (* 1 = 1.10143 loss)
I0523 04:26:50.211230 30336 sgd_solver.cpp:106] Iteration 36600, lr = 0.0015
I0523 04:26:59.508970 30336 solver.cpp:237] Iteration 36900, loss = 1.22711
I0523 04:26:59.509006 30336 solver.cpp:253]     Train net output #0: loss = 1.22711 (* 1 = 1.22711 loss)
I0523 04:26:59.509021 30336 sgd_solver.cpp:106] Iteration 36900, lr = 0.0015
I0523 04:27:08.805598 30336 solver.cpp:237] Iteration 37200, loss = 1.17811
I0523 04:27:08.805632 30336 solver.cpp:253]     Train net output #0: loss = 1.17811 (* 1 = 1.17811 loss)
I0523 04:27:08.805649 30336 sgd_solver.cpp:106] Iteration 37200, lr = 0.0015
I0523 04:27:18.099632 30336 solver.cpp:237] Iteration 37500, loss = 1.18922
I0523 04:27:18.099800 30336 solver.cpp:253]     Train net output #0: loss = 1.18922 (* 1 = 1.18922 loss)
I0523 04:27:18.099814 30336 sgd_solver.cpp:106] Iteration 37500, lr = 0.0015
I0523 04:27:27.397369 30336 solver.cpp:237] Iteration 37800, loss = 1.06005
I0523 04:27:27.397405 30336 solver.cpp:253]     Train net output #0: loss = 1.06005 (* 1 = 1.06005 loss)
I0523 04:27:27.397423 30336 sgd_solver.cpp:106] Iteration 37800, lr = 0.0015
I0523 04:27:57.541751 30336 solver.cpp:237] Iteration 38100, loss = 0.995076
I0523 04:27:57.541926 30336 solver.cpp:253]     Train net output #0: loss = 0.995076 (* 1 = 0.995076 loss)
I0523 04:27:57.541941 30336 sgd_solver.cpp:106] Iteration 38100, lr = 0.0015
I0523 04:28:06.841433 30336 solver.cpp:237] Iteration 38400, loss = 1.03536
I0523 04:28:06.841478 30336 solver.cpp:253]     Train net output #0: loss = 1.03536 (* 1 = 1.03536 loss)
I0523 04:28:06.841496 30336 sgd_solver.cpp:106] Iteration 38400, lr = 0.0015
I0523 04:28:16.136947 30336 solver.cpp:237] Iteration 38700, loss = 1.4676
I0523 04:28:16.136983 30336 solver.cpp:253]     Train net output #0: loss = 1.4676 (* 1 = 1.4676 loss)
I0523 04:28:16.136999 30336 sgd_solver.cpp:106] Iteration 38700, lr = 0.0015
I0523 04:28:25.402827 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_39000.caffemodel
I0523 04:28:25.461876 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_39000.solverstate
I0523 04:28:25.497596 30336 solver.cpp:237] Iteration 39000, loss = 1.38151
I0523 04:28:25.497644 30336 solver.cpp:253]     Train net output #0: loss = 1.38151 (* 1 = 1.38151 loss)
I0523 04:28:25.497659 30336 sgd_solver.cpp:106] Iteration 39000, lr = 0.0015
I0523 04:28:34.795310 30336 solver.cpp:237] Iteration 39300, loss = 1.01012
I0523 04:28:34.795490 30336 solver.cpp:253]     Train net output #0: loss = 1.01012 (* 1 = 1.01012 loss)
I0523 04:28:34.795505 30336 sgd_solver.cpp:106] Iteration 39300, lr = 0.0015
I0523 04:28:44.092061 30336 solver.cpp:237] Iteration 39600, loss = 1.33575
I0523 04:28:44.092095 30336 solver.cpp:253]     Train net output #0: loss = 1.33575 (* 1 = 1.33575 loss)
I0523 04:28:44.092113 30336 sgd_solver.cpp:106] Iteration 39600, lr = 0.0015
I0523 04:28:53.388756 30336 solver.cpp:237] Iteration 39900, loss = 0.873085
I0523 04:28:53.388803 30336 solver.cpp:253]     Train net output #0: loss = 0.873085 (* 1 = 0.873085 loss)
I0523 04:28:53.388823 30336 sgd_solver.cpp:106] Iteration 39900, lr = 0.0015
I0523 04:29:23.540607 30336 solver.cpp:237] Iteration 40200, loss = 1.27638
I0523 04:29:23.540781 30336 solver.cpp:253]     Train net output #0: loss = 1.27638 (* 1 = 1.27638 loss)
I0523 04:29:23.540797 30336 sgd_solver.cpp:106] Iteration 40200, lr = 0.0015
I0523 04:29:32.833323 30336 solver.cpp:237] Iteration 40500, loss = 1.32154
I0523 04:29:32.833358 30336 solver.cpp:253]     Train net output #0: loss = 1.32154 (* 1 = 1.32154 loss)
I0523 04:29:32.833375 30336 sgd_solver.cpp:106] Iteration 40500, lr = 0.0015
I0523 04:29:42.130187 30336 solver.cpp:237] Iteration 40800, loss = 1.43054
I0523 04:29:42.130223 30336 solver.cpp:253]     Train net output #0: loss = 1.43054 (* 1 = 1.43054 loss)
I0523 04:29:42.130239 30336 sgd_solver.cpp:106] Iteration 40800, lr = 0.0015
I0523 04:29:51.426159 30336 solver.cpp:237] Iteration 41100, loss = 1.11914
I0523 04:29:51.426208 30336 solver.cpp:253]     Train net output #0: loss = 1.11914 (* 1 = 1.11914 loss)
I0523 04:29:51.426223 30336 sgd_solver.cpp:106] Iteration 41100, lr = 0.0015
I0523 04:30:00.723317 30336 solver.cpp:237] Iteration 41400, loss = 1.32362
I0523 04:30:00.723467 30336 solver.cpp:253]     Train net output #0: loss = 1.32362 (* 1 = 1.32362 loss)
I0523 04:30:00.723481 30336 sgd_solver.cpp:106] Iteration 41400, lr = 0.0015
I0523 04:30:10.023833 30336 solver.cpp:237] Iteration 41700, loss = 0.892425
I0523 04:30:10.023877 30336 solver.cpp:253]     Train net output #0: loss = 0.892425 (* 1 = 0.892425 loss)
I0523 04:30:10.023896 30336 sgd_solver.cpp:106] Iteration 41700, lr = 0.0015
I0523 04:30:19.287096 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_42000.caffemodel
I0523 04:30:19.345976 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_42000.solverstate
I0523 04:30:19.372119 30336 solver.cpp:341] Iteration 42000, Testing net (#0)
I0523 04:31:06.798838 30336 solver.cpp:409]     Test net output #0: accuracy = 0.863252
I0523 04:31:06.799011 30336 solver.cpp:409]     Test net output #1: loss = 0.498034 (* 1 = 0.498034 loss)
I0523 04:31:27.650487 30336 solver.cpp:237] Iteration 42000, loss = 1.38535
I0523 04:31:27.650545 30336 solver.cpp:253]     Train net output #0: loss = 1.38535 (* 1 = 1.38535 loss)
I0523 04:31:27.650562 30336 sgd_solver.cpp:106] Iteration 42000, lr = 0.0015
I0523 04:31:36.940968 30336 solver.cpp:237] Iteration 42300, loss = 1.02786
I0523 04:31:36.941123 30336 solver.cpp:253]     Train net output #0: loss = 1.02786 (* 1 = 1.02786 loss)
I0523 04:31:36.941138 30336 sgd_solver.cpp:106] Iteration 42300, lr = 0.0015
I0523 04:31:46.229951 30336 solver.cpp:237] Iteration 42600, loss = 1.63811
I0523 04:31:46.229987 30336 solver.cpp:253]     Train net output #0: loss = 1.63811 (* 1 = 1.63811 loss)
I0523 04:31:46.230000 30336 sgd_solver.cpp:106] Iteration 42600, lr = 0.0015
I0523 04:31:55.523363 30336 solver.cpp:237] Iteration 42900, loss = 1.23849
I0523 04:31:55.523408 30336 solver.cpp:253]     Train net output #0: loss = 1.23849 (* 1 = 1.23849 loss)
I0523 04:31:55.523424 30336 sgd_solver.cpp:106] Iteration 42900, lr = 0.0015
I0523 04:32:04.813982 30336 solver.cpp:237] Iteration 43200, loss = 1.52772
I0523 04:32:04.814016 30336 solver.cpp:253]     Train net output #0: loss = 1.52772 (* 1 = 1.52772 loss)
I0523 04:32:04.814034 30336 sgd_solver.cpp:106] Iteration 43200, lr = 0.0015
I0523 04:32:14.101872 30336 solver.cpp:237] Iteration 43500, loss = 1.21851
I0523 04:32:14.102054 30336 solver.cpp:253]     Train net output #0: loss = 1.21851 (* 1 = 1.21851 loss)
I0523 04:32:14.102068 30336 sgd_solver.cpp:106] Iteration 43500, lr = 0.0015
I0523 04:32:23.382063 30336 solver.cpp:237] Iteration 43800, loss = 1.15214
I0523 04:32:23.382097 30336 solver.cpp:253]     Train net output #0: loss = 1.15214 (* 1 = 1.15214 loss)
I0523 04:32:23.382115 30336 sgd_solver.cpp:106] Iteration 43800, lr = 0.0015
I0523 04:32:53.544904 30336 solver.cpp:237] Iteration 44100, loss = 1.23548
I0523 04:32:53.545090 30336 solver.cpp:253]     Train net output #0: loss = 1.23548 (* 1 = 1.23548 loss)
I0523 04:32:53.545106 30336 sgd_solver.cpp:106] Iteration 44100, lr = 0.0015
I0523 04:33:02.829504 30336 solver.cpp:237] Iteration 44400, loss = 1.06059
I0523 04:33:02.829547 30336 solver.cpp:253]     Train net output #0: loss = 1.06059 (* 1 = 1.06059 loss)
I0523 04:33:02.829569 30336 sgd_solver.cpp:106] Iteration 44400, lr = 0.0015
I0523 04:33:12.119163 30336 solver.cpp:237] Iteration 44700, loss = 1.11659
I0523 04:33:12.119197 30336 solver.cpp:253]     Train net output #0: loss = 1.11659 (* 1 = 1.11659 loss)
I0523 04:33:12.119213 30336 sgd_solver.cpp:106] Iteration 44700, lr = 0.0015
I0523 04:33:21.376932 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_45000.caffemodel
I0523 04:33:21.438535 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_45000.solverstate
I0523 04:33:21.476435 30336 solver.cpp:237] Iteration 45000, loss = 1.55395
I0523 04:33:21.476487 30336 solver.cpp:253]     Train net output #0: loss = 1.55395 (* 1 = 1.55395 loss)
I0523 04:33:21.476506 30336 sgd_solver.cpp:106] Iteration 45000, lr = 0.0015
I0523 04:33:30.767156 30336 solver.cpp:237] Iteration 45300, loss = 1.49189
I0523 04:33:30.767333 30336 solver.cpp:253]     Train net output #0: loss = 1.49189 (* 1 = 1.49189 loss)
I0523 04:33:30.767348 30336 sgd_solver.cpp:106] Iteration 45300, lr = 0.0015
I0523 04:33:40.055045 30336 solver.cpp:237] Iteration 45600, loss = 1.43221
I0523 04:33:40.055080 30336 solver.cpp:253]     Train net output #0: loss = 1.43221 (* 1 = 1.43221 loss)
I0523 04:33:40.055099 30336 sgd_solver.cpp:106] Iteration 45600, lr = 0.0015
I0523 04:33:49.345674 30336 solver.cpp:237] Iteration 45900, loss = 1.26221
I0523 04:33:49.345707 30336 solver.cpp:253]     Train net output #0: loss = 1.26221 (* 1 = 1.26221 loss)
I0523 04:33:49.345724 30336 sgd_solver.cpp:106] Iteration 45900, lr = 0.0015
I0523 04:34:19.481947 30336 solver.cpp:237] Iteration 46200, loss = 1.18139
I0523 04:34:19.482126 30336 solver.cpp:253]     Train net output #0: loss = 1.18139 (* 1 = 1.18139 loss)
I0523 04:34:19.482143 30336 sgd_solver.cpp:106] Iteration 46200, lr = 0.0015
I0523 04:34:28.769536 30336 solver.cpp:237] Iteration 46500, loss = 1.06859
I0523 04:34:28.769572 30336 solver.cpp:253]     Train net output #0: loss = 1.06859 (* 1 = 1.06859 loss)
I0523 04:34:28.769589 30336 sgd_solver.cpp:106] Iteration 46500, lr = 0.0015
I0523 04:34:38.057968 30336 solver.cpp:237] Iteration 46800, loss = 0.807714
I0523 04:34:38.058004 30336 solver.cpp:253]     Train net output #0: loss = 0.807714 (* 1 = 0.807714 loss)
I0523 04:34:38.058020 30336 sgd_solver.cpp:106] Iteration 46800, lr = 0.0015
I0523 04:34:47.345335 30336 solver.cpp:237] Iteration 47100, loss = 1.10566
I0523 04:34:47.345383 30336 solver.cpp:253]     Train net output #0: loss = 1.10566 (* 1 = 1.10566 loss)
I0523 04:34:47.345399 30336 sgd_solver.cpp:106] Iteration 47100, lr = 0.0015
I0523 04:34:56.632637 30336 solver.cpp:237] Iteration 47400, loss = 1.40591
I0523 04:34:56.632802 30336 solver.cpp:253]     Train net output #0: loss = 1.40591 (* 1 = 1.40591 loss)
I0523 04:34:56.632817 30336 sgd_solver.cpp:106] Iteration 47400, lr = 0.0015
I0523 04:35:05.920583 30336 solver.cpp:237] Iteration 47700, loss = 0.855651
I0523 04:35:05.920619 30336 solver.cpp:253]     Train net output #0: loss = 0.855651 (* 1 = 0.855651 loss)
I0523 04:35:05.920636 30336 sgd_solver.cpp:106] Iteration 47700, lr = 0.0015
I0523 04:35:15.182678 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_48000.caffemodel
I0523 04:35:15.241906 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_48000.solverstate
I0523 04:35:15.268450 30336 solver.cpp:341] Iteration 48000, Testing net (#0)
I0523 04:36:23.848995 30336 solver.cpp:409]     Test net output #0: accuracy = 0.87229
I0523 04:36:23.849174 30336 solver.cpp:409]     Test net output #1: loss = 0.430311 (* 1 = 0.430311 loss)
I0523 04:36:44.696116 30336 solver.cpp:237] Iteration 48000, loss = 0.865002
I0523 04:36:44.696174 30336 solver.cpp:253]     Train net output #0: loss = 0.865002 (* 1 = 0.865002 loss)
I0523 04:36:44.696189 30336 sgd_solver.cpp:106] Iteration 48000, lr = 0.0015
I0523 04:36:53.983268 30336 solver.cpp:237] Iteration 48300, loss = 1.00814
I0523 04:36:53.983433 30336 solver.cpp:253]     Train net output #0: loss = 1.00814 (* 1 = 1.00814 loss)
I0523 04:36:53.983448 30336 sgd_solver.cpp:106] Iteration 48300, lr = 0.0015
I0523 04:37:03.270429 30336 solver.cpp:237] Iteration 48600, loss = 1.23031
I0523 04:37:03.270465 30336 solver.cpp:253]     Train net output #0: loss = 1.23031 (* 1 = 1.23031 loss)
I0523 04:37:03.270481 30336 sgd_solver.cpp:106] Iteration 48600, lr = 0.0015
I0523 04:37:12.554420 30336 solver.cpp:237] Iteration 48900, loss = 1.25205
I0523 04:37:12.554468 30336 solver.cpp:253]     Train net output #0: loss = 1.25205 (* 1 = 1.25205 loss)
I0523 04:37:12.554486 30336 sgd_solver.cpp:106] Iteration 48900, lr = 0.0015
I0523 04:37:21.838356 30336 solver.cpp:237] Iteration 49200, loss = 1.4916
I0523 04:37:21.838392 30336 solver.cpp:253]     Train net output #0: loss = 1.4916 (* 1 = 1.4916 loss)
I0523 04:37:21.838409 30336 sgd_solver.cpp:106] Iteration 49200, lr = 0.0015
I0523 04:37:31.127132 30336 solver.cpp:237] Iteration 49500, loss = 1.42655
I0523 04:37:31.127282 30336 solver.cpp:253]     Train net output #0: loss = 1.42655 (* 1 = 1.42655 loss)
I0523 04:37:31.127297 30336 sgd_solver.cpp:106] Iteration 49500, lr = 0.0015
I0523 04:37:40.411993 30336 solver.cpp:237] Iteration 49800, loss = 0.981408
I0523 04:37:40.412037 30336 solver.cpp:253]     Train net output #0: loss = 0.981409 (* 1 = 0.981409 loss)
I0523 04:37:40.412057 30336 sgd_solver.cpp:106] Iteration 49800, lr = 0.0015
I0523 04:38:10.549466 30336 solver.cpp:237] Iteration 50100, loss = 1.33008
I0523 04:38:10.549643 30336 solver.cpp:253]     Train net output #0: loss = 1.33008 (* 1 = 1.33008 loss)
I0523 04:38:10.549659 30336 sgd_solver.cpp:106] Iteration 50100, lr = 0.0015
I0523 04:38:19.834231 30336 solver.cpp:237] Iteration 50400, loss = 1.21147
I0523 04:38:19.834266 30336 solver.cpp:253]     Train net output #0: loss = 1.21147 (* 1 = 1.21147 loss)
I0523 04:38:19.834285 30336 sgd_solver.cpp:106] Iteration 50400, lr = 0.0015
I0523 04:38:29.123881 30336 solver.cpp:237] Iteration 50700, loss = 1.10318
I0523 04:38:29.123926 30336 solver.cpp:253]     Train net output #0: loss = 1.10318 (* 1 = 1.10318 loss)
I0523 04:38:29.123944 30336 sgd_solver.cpp:106] Iteration 50700, lr = 0.0015
I0523 04:38:38.381427 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_51000.caffemodel
I0523 04:38:38.446136 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_51000.solverstate
I0523 04:38:38.482050 30336 solver.cpp:237] Iteration 51000, loss = 0.914966
I0523 04:38:38.482101 30336 solver.cpp:253]     Train net output #0: loss = 0.914966 (* 1 = 0.914966 loss)
I0523 04:38:38.482115 30336 sgd_solver.cpp:106] Iteration 51000, lr = 0.0015
I0523 04:38:47.770248 30336 solver.cpp:237] Iteration 51300, loss = 1.23434
I0523 04:38:47.770417 30336 solver.cpp:253]     Train net output #0: loss = 1.23434 (* 1 = 1.23434 loss)
I0523 04:38:47.770431 30336 sgd_solver.cpp:106] Iteration 51300, lr = 0.0015
I0523 04:38:57.056963 30336 solver.cpp:237] Iteration 51600, loss = 1.05893
I0523 04:38:57.057009 30336 solver.cpp:253]     Train net output #0: loss = 1.05893 (* 1 = 1.05893 loss)
I0523 04:38:57.057027 30336 sgd_solver.cpp:106] Iteration 51600, lr = 0.0015
I0523 04:39:06.342239 30336 solver.cpp:237] Iteration 51900, loss = 1.21372
I0523 04:39:06.342274 30336 solver.cpp:253]     Train net output #0: loss = 1.21372 (* 1 = 1.21372 loss)
I0523 04:39:06.342291 30336 sgd_solver.cpp:106] Iteration 51900, lr = 0.0015
I0523 04:39:36.443773 30336 solver.cpp:237] Iteration 52200, loss = 1.06136
I0523 04:39:36.443959 30336 solver.cpp:253]     Train net output #0: loss = 1.06136 (* 1 = 1.06136 loss)
I0523 04:39:36.443981 30336 sgd_solver.cpp:106] Iteration 52200, lr = 0.0015
I0523 04:39:45.727341 30336 solver.cpp:237] Iteration 52500, loss = 1.24286
I0523 04:39:45.727390 30336 solver.cpp:253]     Train net output #0: loss = 1.24286 (* 1 = 1.24286 loss)
I0523 04:39:45.727407 30336 sgd_solver.cpp:106] Iteration 52500, lr = 0.0015
I0523 04:39:55.010848 30336 solver.cpp:237] Iteration 52800, loss = 1.33561
I0523 04:39:55.010885 30336 solver.cpp:253]     Train net output #0: loss = 1.33561 (* 1 = 1.33561 loss)
I0523 04:39:55.010900 30336 sgd_solver.cpp:106] Iteration 52800, lr = 0.0015
I0523 04:40:04.298935 30336 solver.cpp:237] Iteration 53100, loss = 1.18716
I0523 04:40:04.298971 30336 solver.cpp:253]     Train net output #0: loss = 1.18716 (* 1 = 1.18716 loss)
I0523 04:40:04.298988 30336 sgd_solver.cpp:106] Iteration 53100, lr = 0.0015
I0523 04:40:13.587905 30336 solver.cpp:237] Iteration 53400, loss = 1.13498
I0523 04:40:13.588078 30336 solver.cpp:253]     Train net output #0: loss = 1.13498 (* 1 = 1.13498 loss)
I0523 04:40:13.588093 30336 sgd_solver.cpp:106] Iteration 53400, lr = 0.0015
I0523 04:40:22.873188 30336 solver.cpp:237] Iteration 53700, loss = 1.07024
I0523 04:40:22.873224 30336 solver.cpp:253]     Train net output #0: loss = 1.07024 (* 1 = 1.07024 loss)
I0523 04:40:22.873240 30336 sgd_solver.cpp:106] Iteration 53700, lr = 0.0015
I0523 04:40:32.129721 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_54000.caffemodel
I0523 04:40:32.189039 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_54000.solverstate
I0523 04:40:32.214985 30336 solver.cpp:341] Iteration 54000, Testing net (#0)
I0523 04:41:19.986079 30336 solver.cpp:409]     Test net output #0: accuracy = 0.873631
I0523 04:41:19.986258 30336 solver.cpp:409]     Test net output #1: loss = 0.420595 (* 1 = 0.420595 loss)
I0523 04:41:40.866453 30336 solver.cpp:237] Iteration 54000, loss = 1.32503
I0523 04:41:40.866509 30336 solver.cpp:253]     Train net output #0: loss = 1.32503 (* 1 = 1.32503 loss)
I0523 04:41:40.866524 30336 sgd_solver.cpp:106] Iteration 54000, lr = 0.0015
I0523 04:41:50.149024 30336 solver.cpp:237] Iteration 54300, loss = 1.28838
I0523 04:41:50.149195 30336 solver.cpp:253]     Train net output #0: loss = 1.28838 (* 1 = 1.28838 loss)
I0523 04:41:50.149209 30336 sgd_solver.cpp:106] Iteration 54300, lr = 0.0015
I0523 04:41:59.436501 30336 solver.cpp:237] Iteration 54600, loss = 1.01718
I0523 04:41:59.436535 30336 solver.cpp:253]     Train net output #0: loss = 1.01718 (* 1 = 1.01718 loss)
I0523 04:41:59.436553 30336 sgd_solver.cpp:106] Iteration 54600, lr = 0.0015
I0523 04:42:08.723917 30336 solver.cpp:237] Iteration 54900, loss = 1.09352
I0523 04:42:08.723953 30336 solver.cpp:253]     Train net output #0: loss = 1.09352 (* 1 = 1.09352 loss)
I0523 04:42:08.723976 30336 sgd_solver.cpp:106] Iteration 54900, lr = 0.0015
I0523 04:42:18.013120 30336 solver.cpp:237] Iteration 55200, loss = 1.28352
I0523 04:42:18.013159 30336 solver.cpp:253]     Train net output #0: loss = 1.28352 (* 1 = 1.28352 loss)
I0523 04:42:18.013178 30336 sgd_solver.cpp:106] Iteration 55200, lr = 0.0015
I0523 04:42:27.299054 30336 solver.cpp:237] Iteration 55500, loss = 1.2831
I0523 04:42:27.299216 30336 solver.cpp:253]     Train net output #0: loss = 1.2831 (* 1 = 1.2831 loss)
I0523 04:42:27.299232 30336 sgd_solver.cpp:106] Iteration 55500, lr = 0.0015
I0523 04:42:36.587316 30336 solver.cpp:237] Iteration 55800, loss = 0.899853
I0523 04:42:36.587350 30336 solver.cpp:253]     Train net output #0: loss = 0.899853 (* 1 = 0.899853 loss)
I0523 04:42:36.587368 30336 sgd_solver.cpp:106] Iteration 55800, lr = 0.0015
I0523 04:43:06.739856 30336 solver.cpp:237] Iteration 56100, loss = 1.35039
I0523 04:43:06.740051 30336 solver.cpp:253]     Train net output #0: loss = 1.3504 (* 1 = 1.3504 loss)
I0523 04:43:06.740067 30336 sgd_solver.cpp:106] Iteration 56100, lr = 0.0015
I0523 04:43:16.021900 30336 solver.cpp:237] Iteration 56400, loss = 1.42641
I0523 04:43:16.021935 30336 solver.cpp:253]     Train net output #0: loss = 1.42641 (* 1 = 1.42641 loss)
I0523 04:43:16.021952 30336 sgd_solver.cpp:106] Iteration 56400, lr = 0.0015
I0523 04:43:25.304977 30336 solver.cpp:237] Iteration 56700, loss = 1.02217
I0523 04:43:25.305013 30336 solver.cpp:253]     Train net output #0: loss = 1.02217 (* 1 = 1.02217 loss)
I0523 04:43:25.305030 30336 sgd_solver.cpp:106] Iteration 56700, lr = 0.0015
I0523 04:43:34.557304 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_57000.caffemodel
I0523 04:43:34.618571 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_57000.solverstate
I0523 04:43:34.656556 30336 solver.cpp:237] Iteration 57000, loss = 0.996225
I0523 04:43:34.656610 30336 solver.cpp:253]     Train net output #0: loss = 0.996225 (* 1 = 0.996225 loss)
I0523 04:43:34.656627 30336 sgd_solver.cpp:106] Iteration 57000, lr = 0.0015
I0523 04:43:43.942873 30336 solver.cpp:237] Iteration 57300, loss = 1.18234
I0523 04:43:43.943034 30336 solver.cpp:253]     Train net output #0: loss = 1.18234 (* 1 = 1.18234 loss)
I0523 04:43:43.943048 30336 sgd_solver.cpp:106] Iteration 57300, lr = 0.0015
I0523 04:43:53.225356 30336 solver.cpp:237] Iteration 57600, loss = 1.49593
I0523 04:43:53.225405 30336 solver.cpp:253]     Train net output #0: loss = 1.49593 (* 1 = 1.49593 loss)
I0523 04:43:53.225424 30336 sgd_solver.cpp:106] Iteration 57600, lr = 0.0015
I0523 04:44:02.510917 30336 solver.cpp:237] Iteration 57900, loss = 1.12247
I0523 04:44:02.510952 30336 solver.cpp:253]     Train net output #0: loss = 1.12247 (* 1 = 1.12247 loss)
I0523 04:44:02.510968 30336 sgd_solver.cpp:106] Iteration 57900, lr = 0.0015
I0523 04:44:32.613767 30336 solver.cpp:237] Iteration 58200, loss = 1.27019
I0523 04:44:32.613950 30336 solver.cpp:253]     Train net output #0: loss = 1.27019 (* 1 = 1.27019 loss)
I0523 04:44:32.613965 30336 sgd_solver.cpp:106] Iteration 58200, lr = 0.0015
I0523 04:44:41.894390 30336 solver.cpp:237] Iteration 58500, loss = 1.27931
I0523 04:44:41.894425 30336 solver.cpp:253]     Train net output #0: loss = 1.27931 (* 1 = 1.27931 loss)
I0523 04:44:41.894443 30336 sgd_solver.cpp:106] Iteration 58500, lr = 0.0015
I0523 04:44:51.177659 30336 solver.cpp:237] Iteration 58800, loss = 0.936062
I0523 04:44:51.177706 30336 solver.cpp:253]     Train net output #0: loss = 0.936062 (* 1 = 0.936062 loss)
I0523 04:44:51.177726 30336 sgd_solver.cpp:106] Iteration 58800, lr = 0.0015
I0523 04:45:00.461611 30336 solver.cpp:237] Iteration 59100, loss = 1.23473
I0523 04:45:00.461647 30336 solver.cpp:253]     Train net output #0: loss = 1.23473 (* 1 = 1.23473 loss)
I0523 04:45:00.461664 30336 sgd_solver.cpp:106] Iteration 59100, lr = 0.0015
I0523 04:45:09.745265 30336 solver.cpp:237] Iteration 59400, loss = 1.20484
I0523 04:45:09.745447 30336 solver.cpp:253]     Train net output #0: loss = 1.20484 (* 1 = 1.20484 loss)
I0523 04:45:09.745461 30336 sgd_solver.cpp:106] Iteration 59400, lr = 0.0015
I0523 04:45:19.028939 30336 solver.cpp:237] Iteration 59700, loss = 1.06946
I0523 04:45:19.028973 30336 solver.cpp:253]     Train net output #0: loss = 1.06946 (* 1 = 1.06946 loss)
I0523 04:45:19.028990 30336 sgd_solver.cpp:106] Iteration 59700, lr = 0.0015
I0523 04:45:28.283850 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_60000.caffemodel
I0523 04:45:28.345051 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_60000.solverstate
I0523 04:45:28.373263 30336 solver.cpp:341] Iteration 60000, Testing net (#0)
I0523 04:46:36.953163 30336 solver.cpp:409]     Test net output #0: accuracy = 0.874749
I0523 04:46:36.953346 30336 solver.cpp:409]     Test net output #1: loss = 0.410538 (* 1 = 0.410538 loss)
I0523 04:46:57.738853 30336 solver.cpp:237] Iteration 60000, loss = 1.24797
I0523 04:46:57.738911 30336 solver.cpp:253]     Train net output #0: loss = 1.24797 (* 1 = 1.24797 loss)
I0523 04:46:57.738926 30336 sgd_solver.cpp:106] Iteration 60000, lr = 0.0015
I0523 04:47:07.048521 30336 solver.cpp:237] Iteration 60300, loss = 1.08104
I0523 04:47:07.048701 30336 solver.cpp:253]     Train net output #0: loss = 1.08104 (* 1 = 1.08104 loss)
I0523 04:47:07.048714 30336 sgd_solver.cpp:106] Iteration 60300, lr = 0.0015
I0523 04:47:16.353435 30336 solver.cpp:237] Iteration 60600, loss = 1.20273
I0523 04:47:16.353487 30336 solver.cpp:253]     Train net output #0: loss = 1.20273 (* 1 = 1.20273 loss)
I0523 04:47:16.353504 30336 sgd_solver.cpp:106] Iteration 60600, lr = 0.0015
I0523 04:47:25.659750 30336 solver.cpp:237] Iteration 60900, loss = 1.20711
I0523 04:47:25.659786 30336 solver.cpp:253]     Train net output #0: loss = 1.20711 (* 1 = 1.20711 loss)
I0523 04:47:25.659803 30336 sgd_solver.cpp:106] Iteration 60900, lr = 0.0015
I0523 04:47:34.962724 30336 solver.cpp:237] Iteration 61200, loss = 1.08147
I0523 04:47:34.962759 30336 solver.cpp:253]     Train net output #0: loss = 1.08147 (* 1 = 1.08147 loss)
I0523 04:47:34.962775 30336 sgd_solver.cpp:106] Iteration 61200, lr = 0.0015
I0523 04:47:44.267591 30336 solver.cpp:237] Iteration 61500, loss = 1.04935
I0523 04:47:44.267765 30336 solver.cpp:253]     Train net output #0: loss = 1.04935 (* 1 = 1.04935 loss)
I0523 04:47:44.267779 30336 sgd_solver.cpp:106] Iteration 61500, lr = 0.0015
I0523 04:47:53.572852 30336 solver.cpp:237] Iteration 61800, loss = 1.32866
I0523 04:47:53.572887 30336 solver.cpp:253]     Train net output #0: loss = 1.32866 (* 1 = 1.32866 loss)
I0523 04:47:53.572904 30336 sgd_solver.cpp:106] Iteration 61800, lr = 0.0015
I0523 04:48:23.663743 30336 solver.cpp:237] Iteration 62100, loss = 1.23178
I0523 04:48:23.663925 30336 solver.cpp:253]     Train net output #0: loss = 1.23178 (* 1 = 1.23178 loss)
I0523 04:48:23.663941 30336 sgd_solver.cpp:106] Iteration 62100, lr = 0.0015
I0523 04:48:32.971071 30336 solver.cpp:237] Iteration 62400, loss = 1.32835
I0523 04:48:32.971115 30336 solver.cpp:253]     Train net output #0: loss = 1.32835 (* 1 = 1.32835 loss)
I0523 04:48:32.971132 30336 sgd_solver.cpp:106] Iteration 62400, lr = 0.0015
I0523 04:48:42.278031 30336 solver.cpp:237] Iteration 62700, loss = 1.44892
I0523 04:48:42.278066 30336 solver.cpp:253]     Train net output #0: loss = 1.44892 (* 1 = 1.44892 loss)
I0523 04:48:42.278082 30336 sgd_solver.cpp:106] Iteration 62700, lr = 0.0015
I0523 04:48:51.552709 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_63000.caffemodel
I0523 04:48:51.611374 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_63000.solverstate
I0523 04:48:51.647517 30336 solver.cpp:237] Iteration 63000, loss = 0.952602
I0523 04:48:51.647567 30336 solver.cpp:253]     Train net output #0: loss = 0.952602 (* 1 = 0.952602 loss)
I0523 04:48:51.647580 30336 sgd_solver.cpp:106] Iteration 63000, lr = 0.0015
I0523 04:49:00.958479 30336 solver.cpp:237] Iteration 63300, loss = 1.41461
I0523 04:49:00.958667 30336 solver.cpp:253]     Train net output #0: loss = 1.41461 (* 1 = 1.41461 loss)
I0523 04:49:00.958681 30336 sgd_solver.cpp:106] Iteration 63300, lr = 0.0015
I0523 04:49:10.263403 30336 solver.cpp:237] Iteration 63600, loss = 1.13018
I0523 04:49:10.263437 30336 solver.cpp:253]     Train net output #0: loss = 1.13018 (* 1 = 1.13018 loss)
I0523 04:49:10.263456 30336 sgd_solver.cpp:106] Iteration 63600, lr = 0.0015
I0523 04:49:19.566519 30336 solver.cpp:237] Iteration 63900, loss = 0.990852
I0523 04:49:19.566570 30336 solver.cpp:253]     Train net output #0: loss = 0.990852 (* 1 = 0.990852 loss)
I0523 04:49:19.566586 30336 sgd_solver.cpp:106] Iteration 63900, lr = 0.0015
I0523 04:49:49.665488 30336 solver.cpp:237] Iteration 64200, loss = 1.19371
I0523 04:49:49.665673 30336 solver.cpp:253]     Train net output #0: loss = 1.19371 (* 1 = 1.19371 loss)
I0523 04:49:49.665688 30336 sgd_solver.cpp:106] Iteration 64200, lr = 0.0015
I0523 04:49:58.973603 30336 solver.cpp:237] Iteration 64500, loss = 1.27576
I0523 04:49:58.973636 30336 solver.cpp:253]     Train net output #0: loss = 1.27576 (* 1 = 1.27576 loss)
I0523 04:49:58.973654 30336 sgd_solver.cpp:106] Iteration 64500, lr = 0.0015
I0523 04:50:08.281047 30336 solver.cpp:237] Iteration 64800, loss = 1.26198
I0523 04:50:08.281096 30336 solver.cpp:253]     Train net output #0: loss = 1.26198 (* 1 = 1.26198 loss)
I0523 04:50:08.281116 30336 sgd_solver.cpp:106] Iteration 64800, lr = 0.0015
I0523 04:50:17.585639 30336 solver.cpp:237] Iteration 65100, loss = 1.18331
I0523 04:50:17.585675 30336 solver.cpp:253]     Train net output #0: loss = 1.18331 (* 1 = 1.18331 loss)
I0523 04:50:17.585691 30336 sgd_solver.cpp:106] Iteration 65100, lr = 0.0015
I0523 04:50:26.891315 30336 solver.cpp:237] Iteration 65400, loss = 1.11031
I0523 04:50:26.891474 30336 solver.cpp:253]     Train net output #0: loss = 1.11031 (* 1 = 1.11031 loss)
I0523 04:50:26.891489 30336 sgd_solver.cpp:106] Iteration 65400, lr = 0.0015
I0523 04:50:36.196341 30336 solver.cpp:237] Iteration 65700, loss = 1.48071
I0523 04:50:36.196394 30336 solver.cpp:253]     Train net output #0: loss = 1.48071 (* 1 = 1.48071 loss)
I0523 04:50:36.196410 30336 sgd_solver.cpp:106] Iteration 65700, lr = 0.0015
I0523 04:50:45.471824 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_66000.caffemodel
I0523 04:50:45.531208 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_66000.solverstate
I0523 04:50:45.557404 30336 solver.cpp:341] Iteration 66000, Testing net (#0)
I0523 04:51:32.962741 30336 solver.cpp:409]     Test net output #0: accuracy = 0.878468
I0523 04:51:32.962924 30336 solver.cpp:409]     Test net output #1: loss = 0.377214 (* 1 = 0.377214 loss)
I0523 04:51:53.751338 30336 solver.cpp:237] Iteration 66000, loss = 1.1693
I0523 04:51:53.751394 30336 solver.cpp:253]     Train net output #0: loss = 1.1693 (* 1 = 1.1693 loss)
I0523 04:51:53.751410 30336 sgd_solver.cpp:106] Iteration 66000, lr = 0.0015
I0523 04:52:03.043170 30336 solver.cpp:237] Iteration 66300, loss = 1.22182
I0523 04:52:03.043344 30336 solver.cpp:253]     Train net output #0: loss = 1.22182 (* 1 = 1.22182 loss)
I0523 04:52:03.043360 30336 sgd_solver.cpp:106] Iteration 66300, lr = 0.0015
I0523 04:52:12.331862 30336 solver.cpp:237] Iteration 66600, loss = 1.21267
I0523 04:52:12.331907 30336 solver.cpp:253]     Train net output #0: loss = 1.21267 (* 1 = 1.21267 loss)
I0523 04:52:12.331925 30336 sgd_solver.cpp:106] Iteration 66600, lr = 0.0015
I0523 04:52:21.620810 30336 solver.cpp:237] Iteration 66900, loss = 1.21497
I0523 04:52:21.620846 30336 solver.cpp:253]     Train net output #0: loss = 1.21497 (* 1 = 1.21497 loss)
I0523 04:52:21.620862 30336 sgd_solver.cpp:106] Iteration 66900, lr = 0.0015
I0523 04:52:30.909915 30336 solver.cpp:237] Iteration 67200, loss = 1.53295
I0523 04:52:30.909951 30336 solver.cpp:253]     Train net output #0: loss = 1.53295 (* 1 = 1.53295 loss)
I0523 04:52:30.909967 30336 sgd_solver.cpp:106] Iteration 67200, lr = 0.0015
I0523 04:52:40.198606 30336 solver.cpp:237] Iteration 67500, loss = 1.32936
I0523 04:52:40.198781 30336 solver.cpp:253]     Train net output #0: loss = 1.32936 (* 1 = 1.32936 loss)
I0523 04:52:40.198794 30336 sgd_solver.cpp:106] Iteration 67500, lr = 0.0015
I0523 04:52:49.483098 30336 solver.cpp:237] Iteration 67800, loss = 1.02746
I0523 04:52:49.483132 30336 solver.cpp:253]     Train net output #0: loss = 1.02746 (* 1 = 1.02746 loss)
I0523 04:52:49.483150 30336 sgd_solver.cpp:106] Iteration 67800, lr = 0.0015
I0523 04:53:19.600481 30336 solver.cpp:237] Iteration 68100, loss = 0.972707
I0523 04:53:19.600672 30336 solver.cpp:253]     Train net output #0: loss = 0.972707 (* 1 = 0.972707 loss)
I0523 04:53:19.600687 30336 sgd_solver.cpp:106] Iteration 68100, lr = 0.0015
I0523 04:53:28.890023 30336 solver.cpp:237] Iteration 68400, loss = 1.10896
I0523 04:53:28.890063 30336 solver.cpp:253]     Train net output #0: loss = 1.10897 (* 1 = 1.10897 loss)
I0523 04:53:28.890084 30336 sgd_solver.cpp:106] Iteration 68400, lr = 0.0015
I0523 04:53:38.177830 30336 solver.cpp:237] Iteration 68700, loss = 1.0956
I0523 04:53:38.177865 30336 solver.cpp:253]     Train net output #0: loss = 1.0956 (* 1 = 1.0956 loss)
I0523 04:53:38.177881 30336 sgd_solver.cpp:106] Iteration 68700, lr = 0.0015
I0523 04:53:47.434005 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_69000.caffemodel
I0523 04:53:47.494472 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_69000.solverstate
I0523 04:53:47.530228 30336 solver.cpp:237] Iteration 69000, loss = 1.10631
I0523 04:53:47.530275 30336 solver.cpp:253]     Train net output #0: loss = 1.10631 (* 1 = 1.10631 loss)
I0523 04:53:47.530293 30336 sgd_solver.cpp:106] Iteration 69000, lr = 0.0015
I0523 04:53:56.817283 30336 solver.cpp:237] Iteration 69300, loss = 1.18902
I0523 04:53:56.817456 30336 solver.cpp:253]     Train net output #0: loss = 1.18902 (* 1 = 1.18902 loss)
I0523 04:53:56.817471 30336 sgd_solver.cpp:106] Iteration 69300, lr = 0.0015
I0523 04:54:06.104960 30336 solver.cpp:237] Iteration 69600, loss = 1.13829
I0523 04:54:06.104995 30336 solver.cpp:253]     Train net output #0: loss = 1.13829 (* 1 = 1.13829 loss)
I0523 04:54:06.105013 30336 sgd_solver.cpp:106] Iteration 69600, lr = 0.0015
I0523 04:54:15.389034 30336 solver.cpp:237] Iteration 69900, loss = 1.18063
I0523 04:54:15.389070 30336 solver.cpp:253]     Train net output #0: loss = 1.18063 (* 1 = 1.18063 loss)
I0523 04:54:15.389086 30336 sgd_solver.cpp:106] Iteration 69900, lr = 0.0015
I0523 04:54:45.491181 30336 solver.cpp:237] Iteration 70200, loss = 1.23205
I0523 04:54:45.491366 30336 solver.cpp:253]     Train net output #0: loss = 1.23205 (* 1 = 1.23205 loss)
I0523 04:54:45.491381 30336 sgd_solver.cpp:106] Iteration 70200, lr = 0.0015
I0523 04:54:54.778750 30336 solver.cpp:237] Iteration 70500, loss = 1.28484
I0523 04:54:54.778785 30336 solver.cpp:253]     Train net output #0: loss = 1.28484 (* 1 = 1.28484 loss)
I0523 04:54:54.778803 30336 sgd_solver.cpp:106] Iteration 70500, lr = 0.0015
I0523 04:55:04.067028 30336 solver.cpp:237] Iteration 70800, loss = 1.34052
I0523 04:55:04.067065 30336 solver.cpp:253]     Train net output #0: loss = 1.34052 (* 1 = 1.34052 loss)
I0523 04:55:04.067080 30336 sgd_solver.cpp:106] Iteration 70800, lr = 0.0015
I0523 04:55:13.357221 30336 solver.cpp:237] Iteration 71100, loss = 1.30106
I0523 04:55:13.357273 30336 solver.cpp:253]     Train net output #0: loss = 1.30106 (* 1 = 1.30106 loss)
I0523 04:55:13.357287 30336 sgd_solver.cpp:106] Iteration 71100, lr = 0.0015
I0523 04:55:22.644871 30336 solver.cpp:237] Iteration 71400, loss = 1.34261
I0523 04:55:22.645043 30336 solver.cpp:253]     Train net output #0: loss = 1.34261 (* 1 = 1.34261 loss)
I0523 04:55:22.645058 30336 sgd_solver.cpp:106] Iteration 71400, lr = 0.0015
I0523 04:55:31.936463 30336 solver.cpp:237] Iteration 71700, loss = 0.978393
I0523 04:55:31.936499 30336 solver.cpp:253]     Train net output #0: loss = 0.978393 (* 1 = 0.978393 loss)
I0523 04:55:31.936516 30336 sgd_solver.cpp:106] Iteration 71700, lr = 0.0015
I0523 04:55:41.192721 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_72000.caffemodel
I0523 04:55:41.252125 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_72000.solverstate
I0523 04:55:41.278259 30336 solver.cpp:341] Iteration 72000, Testing net (#0)
I0523 04:56:49.854665 30336 solver.cpp:409]     Test net output #0: accuracy = 0.87901
I0523 04:56:49.854848 30336 solver.cpp:409]     Test net output #1: loss = 0.394724 (* 1 = 0.394724 loss)
I0523 04:57:10.681922 30336 solver.cpp:237] Iteration 72000, loss = 1.19065
I0523 04:57:10.681979 30336 solver.cpp:253]     Train net output #0: loss = 1.19065 (* 1 = 1.19065 loss)
I0523 04:57:10.681994 30336 sgd_solver.cpp:106] Iteration 72000, lr = 0.0015
I0523 04:57:19.968222 30336 solver.cpp:237] Iteration 72300, loss = 1.07748
I0523 04:57:19.968403 30336 solver.cpp:253]     Train net output #0: loss = 1.07748 (* 1 = 1.07748 loss)
I0523 04:57:19.968417 30336 sgd_solver.cpp:106] Iteration 72300, lr = 0.0015
I0523 04:57:29.252851 30336 solver.cpp:237] Iteration 72600, loss = 1.39325
I0523 04:57:29.252885 30336 solver.cpp:253]     Train net output #0: loss = 1.39325 (* 1 = 1.39325 loss)
I0523 04:57:29.252902 30336 sgd_solver.cpp:106] Iteration 72600, lr = 0.0015
I0523 04:57:38.541031 30336 solver.cpp:237] Iteration 72900, loss = 1.33896
I0523 04:57:38.541072 30336 solver.cpp:253]     Train net output #0: loss = 1.33896 (* 1 = 1.33896 loss)
I0523 04:57:38.541093 30336 sgd_solver.cpp:106] Iteration 72900, lr = 0.0015
I0523 04:57:47.825053 30336 solver.cpp:237] Iteration 73200, loss = 0.958911
I0523 04:57:47.825089 30336 solver.cpp:253]     Train net output #0: loss = 0.958912 (* 1 = 0.958912 loss)
I0523 04:57:47.825106 30336 sgd_solver.cpp:106] Iteration 73200, lr = 0.0015
I0523 04:57:57.111954 30336 solver.cpp:237] Iteration 73500, loss = 1.17447
I0523 04:57:57.112115 30336 solver.cpp:253]     Train net output #0: loss = 1.17447 (* 1 = 1.17447 loss)
I0523 04:57:57.112129 30336 sgd_solver.cpp:106] Iteration 73500, lr = 0.0015
I0523 04:58:06.401172 30336 solver.cpp:237] Iteration 73800, loss = 1.10183
I0523 04:58:06.401218 30336 solver.cpp:253]     Train net output #0: loss = 1.10183 (* 1 = 1.10183 loss)
I0523 04:58:06.401239 30336 sgd_solver.cpp:106] Iteration 73800, lr = 0.0015
I0523 04:58:36.544363 30336 solver.cpp:237] Iteration 74100, loss = 1.12684
I0523 04:58:36.544548 30336 solver.cpp:253]     Train net output #0: loss = 1.12684 (* 1 = 1.12684 loss)
I0523 04:58:36.544564 30336 sgd_solver.cpp:106] Iteration 74100, lr = 0.0015
I0523 04:58:45.829805 30336 solver.cpp:237] Iteration 74400, loss = 1.04046
I0523 04:58:45.829840 30336 solver.cpp:253]     Train net output #0: loss = 1.04047 (* 1 = 1.04047 loss)
I0523 04:58:45.829857 30336 sgd_solver.cpp:106] Iteration 74400, lr = 0.0015
I0523 04:58:55.119035 30336 solver.cpp:237] Iteration 74700, loss = 1.22379
I0523 04:58:55.119081 30336 solver.cpp:253]     Train net output #0: loss = 1.22379 (* 1 = 1.22379 loss)
I0523 04:58:55.119099 30336 sgd_solver.cpp:106] Iteration 74700, lr = 0.0015
I0523 04:59:04.373714 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_75000.caffemodel
I0523 04:59:04.434840 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_75000.solverstate
I0523 04:59:04.472684 30336 solver.cpp:237] Iteration 75000, loss = 1.33463
I0523 04:59:04.472736 30336 solver.cpp:253]     Train net output #0: loss = 1.33463 (* 1 = 1.33463 loss)
I0523 04:59:04.472754 30336 sgd_solver.cpp:106] Iteration 75000, lr = 0.0015
I0523 04:59:13.761739 30336 solver.cpp:237] Iteration 75300, loss = 1.41216
I0523 04:59:13.761914 30336 solver.cpp:253]     Train net output #0: loss = 1.41216 (* 1 = 1.41216 loss)
I0523 04:59:13.761929 30336 sgd_solver.cpp:106] Iteration 75300, lr = 0.0015
I0523 04:59:23.052117 30336 solver.cpp:237] Iteration 75600, loss = 1.32263
I0523 04:59:23.052166 30336 solver.cpp:253]     Train net output #0: loss = 1.32263 (* 1 = 1.32263 loss)
I0523 04:59:23.052183 30336 sgd_solver.cpp:106] Iteration 75600, lr = 0.0015
I0523 04:59:32.340716 30336 solver.cpp:237] Iteration 75900, loss = 1.29443
I0523 04:59:32.340751 30336 solver.cpp:253]     Train net output #0: loss = 1.29443 (* 1 = 1.29443 loss)
I0523 04:59:32.340769 30336 sgd_solver.cpp:106] Iteration 75900, lr = 0.0015
I0523 05:00:02.427304 30336 solver.cpp:237] Iteration 76200, loss = 1.21304
I0523 05:00:02.427495 30336 solver.cpp:253]     Train net output #0: loss = 1.21304 (* 1 = 1.21304 loss)
I0523 05:00:02.427511 30336 sgd_solver.cpp:106] Iteration 76200, lr = 0.0015
I0523 05:00:11.719202 30336 solver.cpp:237] Iteration 76500, loss = 1.12046
I0523 05:00:11.719251 30336 solver.cpp:253]     Train net output #0: loss = 1.12046 (* 1 = 1.12046 loss)
I0523 05:00:11.719269 30336 sgd_solver.cpp:106] Iteration 76500, lr = 0.0015
I0523 05:00:21.010252 30336 solver.cpp:237] Iteration 76800, loss = 1.16019
I0523 05:00:21.010288 30336 solver.cpp:253]     Train net output #0: loss = 1.16019 (* 1 = 1.16019 loss)
I0523 05:00:21.010301 30336 sgd_solver.cpp:106] Iteration 76800, lr = 0.0015
I0523 05:00:30.302268 30336 solver.cpp:237] Iteration 77100, loss = 1.27919
I0523 05:00:30.302304 30336 solver.cpp:253]     Train net output #0: loss = 1.27919 (* 1 = 1.27919 loss)
I0523 05:00:30.302319 30336 sgd_solver.cpp:106] Iteration 77100, lr = 0.0015
I0523 05:00:39.589618 30336 solver.cpp:237] Iteration 77400, loss = 1.3572
I0523 05:00:39.589793 30336 solver.cpp:253]     Train net output #0: loss = 1.3572 (* 1 = 1.3572 loss)
I0523 05:00:39.589807 30336 sgd_solver.cpp:106] Iteration 77400, lr = 0.0015
I0523 05:00:48.882406 30336 solver.cpp:237] Iteration 77700, loss = 1.02134
I0523 05:00:48.882441 30336 solver.cpp:253]     Train net output #0: loss = 1.02134 (* 1 = 1.02134 loss)
I0523 05:00:48.882458 30336 sgd_solver.cpp:106] Iteration 77700, lr = 0.0015
I0523 05:00:58.138870 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_78000.caffemodel
I0523 05:00:58.197603 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_78000.solverstate
I0523 05:00:58.223503 30336 solver.cpp:341] Iteration 78000, Testing net (#0)
I0523 05:01:45.989706 30336 solver.cpp:409]     Test net output #0: accuracy = 0.884061
I0523 05:01:45.989902 30336 solver.cpp:409]     Test net output #1: loss = 0.379202 (* 1 = 0.379202 loss)
I0523 05:02:06.784529 30336 solver.cpp:237] Iteration 78000, loss = 1.23484
I0523 05:02:06.784585 30336 solver.cpp:253]     Train net output #0: loss = 1.23484 (* 1 = 1.23484 loss)
I0523 05:02:06.784600 30336 sgd_solver.cpp:106] Iteration 78000, lr = 0.0015
I0523 05:02:16.066762 30336 solver.cpp:237] Iteration 78300, loss = 0.981272
I0523 05:02:16.066949 30336 solver.cpp:253]     Train net output #0: loss = 0.981272 (* 1 = 0.981272 loss)
I0523 05:02:16.066964 30336 sgd_solver.cpp:106] Iteration 78300, lr = 0.0015
I0523 05:02:25.347702 30336 solver.cpp:237] Iteration 78600, loss = 1.28725
I0523 05:02:25.347738 30336 solver.cpp:253]     Train net output #0: loss = 1.28725 (* 1 = 1.28725 loss)
I0523 05:02:25.347754 30336 sgd_solver.cpp:106] Iteration 78600, lr = 0.0015
I0523 05:02:34.631388 30336 solver.cpp:237] Iteration 78900, loss = 1.2067
I0523 05:02:34.631423 30336 solver.cpp:253]     Train net output #0: loss = 1.2067 (* 1 = 1.2067 loss)
I0523 05:02:34.631440 30336 sgd_solver.cpp:106] Iteration 78900, lr = 0.0015
I0523 05:02:43.909901 30336 solver.cpp:237] Iteration 79200, loss = 1.17937
I0523 05:02:43.909945 30336 solver.cpp:253]     Train net output #0: loss = 1.17937 (* 1 = 1.17937 loss)
I0523 05:02:43.909965 30336 sgd_solver.cpp:106] Iteration 79200, lr = 0.0015
I0523 05:02:53.192582 30336 solver.cpp:237] Iteration 79500, loss = 1.03295
I0523 05:02:53.192742 30336 solver.cpp:253]     Train net output #0: loss = 1.03295 (* 1 = 1.03295 loss)
I0523 05:02:53.192756 30336 sgd_solver.cpp:106] Iteration 79500, lr = 0.0015
I0523 05:03:02.475821 30336 solver.cpp:237] Iteration 79800, loss = 0.82555
I0523 05:03:02.475872 30336 solver.cpp:253]     Train net output #0: loss = 0.825551 (* 1 = 0.825551 loss)
I0523 05:03:02.475888 30336 sgd_solver.cpp:106] Iteration 79800, lr = 0.0015
I0523 05:03:32.572732 30336 solver.cpp:237] Iteration 80100, loss = 1.13332
I0523 05:03:32.572923 30336 solver.cpp:253]     Train net output #0: loss = 1.13332 (* 1 = 1.13332 loss)
I0523 05:03:32.572939 30336 sgd_solver.cpp:106] Iteration 80100, lr = 0.0015
I0523 05:03:41.855885 30336 solver.cpp:237] Iteration 80400, loss = 1.24287
I0523 05:03:41.855919 30336 solver.cpp:253]     Train net output #0: loss = 1.24287 (* 1 = 1.24287 loss)
I0523 05:03:41.855937 30336 sgd_solver.cpp:106] Iteration 80400, lr = 0.0015
I0523 05:03:51.142014 30336 solver.cpp:237] Iteration 80700, loss = 1.06355
I0523 05:03:51.142048 30336 solver.cpp:253]     Train net output #0: loss = 1.06355 (* 1 = 1.06355 loss)
I0523 05:03:51.142065 30336 sgd_solver.cpp:106] Iteration 80700, lr = 0.0015
I0523 05:04:00.395933 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_81000.caffemodel
I0523 05:04:00.455081 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_81000.solverstate
I0523 05:04:00.490665 30336 solver.cpp:237] Iteration 81000, loss = 0.946122
I0523 05:04:00.490710 30336 solver.cpp:253]     Train net output #0: loss = 0.946122 (* 1 = 0.946122 loss)
I0523 05:04:00.490726 30336 sgd_solver.cpp:106] Iteration 81000, lr = 0.0015
I0523 05:04:09.777254 30336 solver.cpp:237] Iteration 81300, loss = 1.37463
I0523 05:04:09.777421 30336 solver.cpp:253]     Train net output #0: loss = 1.37463 (* 1 = 1.37463 loss)
I0523 05:04:09.777437 30336 sgd_solver.cpp:106] Iteration 81300, lr = 0.0015
I0523 05:04:19.062685 30336 solver.cpp:237] Iteration 81600, loss = 1.21826
I0523 05:04:19.062732 30336 solver.cpp:253]     Train net output #0: loss = 1.21826 (* 1 = 1.21826 loss)
I0523 05:04:19.062750 30336 sgd_solver.cpp:106] Iteration 81600, lr = 0.0015
I0523 05:04:28.345348 30336 solver.cpp:237] Iteration 81900, loss = 1.12934
I0523 05:04:28.345384 30336 solver.cpp:253]     Train net output #0: loss = 1.12934 (* 1 = 1.12934 loss)
I0523 05:04:28.345401 30336 sgd_solver.cpp:106] Iteration 81900, lr = 0.0015
I0523 05:04:58.460240 30336 solver.cpp:237] Iteration 82200, loss = 1.12917
I0523 05:04:58.460440 30336 solver.cpp:253]     Train net output #0: loss = 1.12917 (* 1 = 1.12917 loss)
I0523 05:04:58.460456 30336 sgd_solver.cpp:106] Iteration 82200, lr = 0.0015
I0523 05:05:07.747381 30336 solver.cpp:237] Iteration 82500, loss = 1.21902
I0523 05:05:07.747431 30336 solver.cpp:253]     Train net output #0: loss = 1.21902 (* 1 = 1.21902 loss)
I0523 05:05:07.747448 30336 sgd_solver.cpp:106] Iteration 82500, lr = 0.0015
I0523 05:05:17.037281 30336 solver.cpp:237] Iteration 82800, loss = 1.02567
I0523 05:05:17.037315 30336 solver.cpp:253]     Train net output #0: loss = 1.02567 (* 1 = 1.02567 loss)
I0523 05:05:17.037333 30336 sgd_solver.cpp:106] Iteration 82800, lr = 0.0015
I0523 05:05:26.321740 30336 solver.cpp:237] Iteration 83100, loss = 1.4281
I0523 05:05:26.321776 30336 solver.cpp:253]     Train net output #0: loss = 1.4281 (* 1 = 1.4281 loss)
I0523 05:05:26.321794 30336 sgd_solver.cpp:106] Iteration 83100, lr = 0.0015
I0523 05:05:35.602704 30336 solver.cpp:237] Iteration 83400, loss = 1.19506
I0523 05:05:35.602875 30336 solver.cpp:253]     Train net output #0: loss = 1.19506 (* 1 = 1.19506 loss)
I0523 05:05:35.602890 30336 sgd_solver.cpp:106] Iteration 83400, lr = 0.0015
I0523 05:05:44.888736 30336 solver.cpp:237] Iteration 83700, loss = 1.1365
I0523 05:05:44.888770 30336 solver.cpp:253]     Train net output #0: loss = 1.1365 (* 1 = 1.1365 loss)
I0523 05:05:44.888788 30336 sgd_solver.cpp:106] Iteration 83700, lr = 0.0015
I0523 05:05:54.142491 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_84000.caffemodel
I0523 05:05:54.201659 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_84000.solverstate
I0523 05:05:54.227746 30336 solver.cpp:341] Iteration 84000, Testing net (#0)
I0523 05:07:02.815227 30336 solver.cpp:409]     Test net output #0: accuracy = 0.885187
I0523 05:07:02.815417 30336 solver.cpp:409]     Test net output #1: loss = 0.369735 (* 1 = 0.369735 loss)
I0523 05:07:23.677552 30336 solver.cpp:237] Iteration 84000, loss = 1.29548
I0523 05:07:23.677608 30336 solver.cpp:253]     Train net output #0: loss = 1.29548 (* 1 = 1.29548 loss)
I0523 05:07:23.677623 30336 sgd_solver.cpp:106] Iteration 84000, lr = 0.0015
I0523 05:07:32.982926 30336 solver.cpp:237] Iteration 84300, loss = 0.996841
I0523 05:07:32.983098 30336 solver.cpp:253]     Train net output #0: loss = 0.996841 (* 1 = 0.996841 loss)
I0523 05:07:32.983114 30336 sgd_solver.cpp:106] Iteration 84300, lr = 0.0015
I0523 05:07:42.285822 30336 solver.cpp:237] Iteration 84600, loss = 1.16561
I0523 05:07:42.285876 30336 solver.cpp:253]     Train net output #0: loss = 1.16561 (* 1 = 1.16561 loss)
I0523 05:07:42.285890 30336 sgd_solver.cpp:106] Iteration 84600, lr = 0.0015
I0523 05:07:51.596204 30336 solver.cpp:237] Iteration 84900, loss = 0.887632
I0523 05:07:51.596242 30336 solver.cpp:253]     Train net output #0: loss = 0.887633 (* 1 = 0.887633 loss)
I0523 05:07:51.596259 30336 sgd_solver.cpp:106] Iteration 84900, lr = 0.0015
I0523 05:08:00.900070 30336 solver.cpp:237] Iteration 85200, loss = 1.51881
I0523 05:08:00.900106 30336 solver.cpp:253]     Train net output #0: loss = 1.51881 (* 1 = 1.51881 loss)
I0523 05:08:00.900122 30336 sgd_solver.cpp:106] Iteration 85200, lr = 0.0015
I0523 05:08:10.204777 30336 solver.cpp:237] Iteration 85500, loss = 0.964024
I0523 05:08:10.204962 30336 solver.cpp:253]     Train net output #0: loss = 0.964024 (* 1 = 0.964024 loss)
I0523 05:08:10.204977 30336 sgd_solver.cpp:106] Iteration 85500, lr = 0.0015
I0523 05:08:19.501734 30336 solver.cpp:237] Iteration 85800, loss = 1.1616
I0523 05:08:19.501768 30336 solver.cpp:253]     Train net output #0: loss = 1.1616 (* 1 = 1.1616 loss)
I0523 05:08:19.501785 30336 sgd_solver.cpp:106] Iteration 85800, lr = 0.0015
I0523 05:08:49.700141 30336 solver.cpp:237] Iteration 86100, loss = 1.45898
I0523 05:08:49.700342 30336 solver.cpp:253]     Train net output #0: loss = 1.45898 (* 1 = 1.45898 loss)
I0523 05:08:49.700359 30336 sgd_solver.cpp:106] Iteration 86100, lr = 0.0015
I0523 05:08:58.999538 30336 solver.cpp:237] Iteration 86400, loss = 1.1922
I0523 05:08:58.999591 30336 solver.cpp:253]     Train net output #0: loss = 1.1922 (* 1 = 1.1922 loss)
I0523 05:08:58.999605 30336 sgd_solver.cpp:106] Iteration 86400, lr = 0.0015
I0523 05:09:08.297564 30336 solver.cpp:237] Iteration 86700, loss = 1.06127
I0523 05:09:08.297600 30336 solver.cpp:253]     Train net output #0: loss = 1.06127 (* 1 = 1.06127 loss)
I0523 05:09:08.297616 30336 sgd_solver.cpp:106] Iteration 86700, lr = 0.0015
I0523 05:09:17.567570 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_87000.caffemodel
I0523 05:09:17.628995 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_87000.solverstate
I0523 05:09:17.667150 30336 solver.cpp:237] Iteration 87000, loss = 1.28456
I0523 05:09:17.667201 30336 solver.cpp:253]     Train net output #0: loss = 1.28456 (* 1 = 1.28456 loss)
I0523 05:09:17.667219 30336 sgd_solver.cpp:106] Iteration 87000, lr = 0.0015
I0523 05:09:26.964263 30336 solver.cpp:237] Iteration 87300, loss = 1.00613
I0523 05:09:26.964437 30336 solver.cpp:253]     Train net output #0: loss = 1.00613 (* 1 = 1.00613 loss)
I0523 05:09:26.964450 30336 sgd_solver.cpp:106] Iteration 87300, lr = 0.0015
I0523 05:09:36.263612 30336 solver.cpp:237] Iteration 87600, loss = 1.18867
I0523 05:09:36.263648 30336 solver.cpp:253]     Train net output #0: loss = 1.18867 (* 1 = 1.18867 loss)
I0523 05:09:36.263664 30336 sgd_solver.cpp:106] Iteration 87600, lr = 0.0015
I0523 05:09:45.564810 30336 solver.cpp:237] Iteration 87900, loss = 1.24362
I0523 05:09:45.564859 30336 solver.cpp:253]     Train net output #0: loss = 1.24362 (* 1 = 1.24362 loss)
I0523 05:09:45.564877 30336 sgd_solver.cpp:106] Iteration 87900, lr = 0.0015
I0523 05:10:15.696830 30336 solver.cpp:237] Iteration 88200, loss = 1.36856
I0523 05:10:15.697021 30336 solver.cpp:253]     Train net output #0: loss = 1.36856 (* 1 = 1.36856 loss)
I0523 05:10:15.697036 30336 sgd_solver.cpp:106] Iteration 88200, lr = 0.0015
I0523 05:10:24.993564 30336 solver.cpp:237] Iteration 88500, loss = 1.61444
I0523 05:10:24.993599 30336 solver.cpp:253]     Train net output #0: loss = 1.61444 (* 1 = 1.61444 loss)
I0523 05:10:24.993616 30336 sgd_solver.cpp:106] Iteration 88500, lr = 0.0015
I0523 05:10:34.287869 30336 solver.cpp:237] Iteration 88800, loss = 1.21182
I0523 05:10:34.287921 30336 solver.cpp:253]     Train net output #0: loss = 1.21182 (* 1 = 1.21182 loss)
I0523 05:10:34.287935 30336 sgd_solver.cpp:106] Iteration 88800, lr = 0.0015
I0523 05:10:43.584506 30336 solver.cpp:237] Iteration 89100, loss = 1.28705
I0523 05:10:43.584542 30336 solver.cpp:253]     Train net output #0: loss = 1.28705 (* 1 = 1.28705 loss)
I0523 05:10:43.584558 30336 sgd_solver.cpp:106] Iteration 89100, lr = 0.0015
I0523 05:10:52.879603 30336 solver.cpp:237] Iteration 89400, loss = 1.30014
I0523 05:10:52.879770 30336 solver.cpp:253]     Train net output #0: loss = 1.30014 (* 1 = 1.30014 loss)
I0523 05:10:52.879786 30336 sgd_solver.cpp:106] Iteration 89400, lr = 0.0015
I0523 05:11:02.174985 30336 solver.cpp:237] Iteration 89700, loss = 1.02293
I0523 05:11:02.175036 30336 solver.cpp:253]     Train net output #0: loss = 1.02293 (* 1 = 1.02293 loss)
I0523 05:11:02.175050 30336 sgd_solver.cpp:106] Iteration 89700, lr = 0.0015
I0523 05:11:11.442773 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_90000.caffemodel
I0523 05:11:11.504344 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_90000.solverstate
I0523 05:11:11.532635 30336 solver.cpp:341] Iteration 90000, Testing net (#0)
I0523 05:11:58.939734 30336 solver.cpp:409]     Test net output #0: accuracy = 0.881081
I0523 05:11:58.939934 30336 solver.cpp:409]     Test net output #1: loss = 0.384228 (* 1 = 0.384228 loss)
I0523 05:12:19.803038 30336 solver.cpp:237] Iteration 90000, loss = 1.50713
I0523 05:12:19.803095 30336 solver.cpp:253]     Train net output #0: loss = 1.50713 (* 1 = 1.50713 loss)
I0523 05:12:19.803110 30336 sgd_solver.cpp:106] Iteration 90000, lr = 0.0015
I0523 05:12:29.094174 30336 solver.cpp:237] Iteration 90300, loss = 1.04585
I0523 05:12:29.094348 30336 solver.cpp:253]     Train net output #0: loss = 1.04585 (* 1 = 1.04585 loss)
I0523 05:12:29.094362 30336 sgd_solver.cpp:106] Iteration 90300, lr = 0.0015
I0523 05:12:38.382776 30336 solver.cpp:237] Iteration 90600, loss = 1.38672
I0523 05:12:38.382824 30336 solver.cpp:253]     Train net output #0: loss = 1.38672 (* 1 = 1.38672 loss)
I0523 05:12:38.382843 30336 sgd_solver.cpp:106] Iteration 90600, lr = 0.0015
I0523 05:12:47.674051 30336 solver.cpp:237] Iteration 90900, loss = 1.28182
I0523 05:12:47.674087 30336 solver.cpp:253]     Train net output #0: loss = 1.28182 (* 1 = 1.28182 loss)
I0523 05:12:47.674103 30336 sgd_solver.cpp:106] Iteration 90900, lr = 0.0015
I0523 05:12:56.963589 30336 solver.cpp:237] Iteration 91200, loss = 0.851184
I0523 05:12:56.963626 30336 solver.cpp:253]     Train net output #0: loss = 0.851184 (* 1 = 0.851184 loss)
I0523 05:12:56.963642 30336 sgd_solver.cpp:106] Iteration 91200, lr = 0.0015
I0523 05:13:06.255622 30336 solver.cpp:237] Iteration 91500, loss = 1.25348
I0523 05:13:06.255806 30336 solver.cpp:253]     Train net output #0: loss = 1.25348 (* 1 = 1.25348 loss)
I0523 05:13:06.255820 30336 sgd_solver.cpp:106] Iteration 91500, lr = 0.0015
I0523 05:13:15.547366 30336 solver.cpp:237] Iteration 91800, loss = 1.31994
I0523 05:13:15.547402 30336 solver.cpp:253]     Train net output #0: loss = 1.31994 (* 1 = 1.31994 loss)
I0523 05:13:15.547418 30336 sgd_solver.cpp:106] Iteration 91800, lr = 0.0015
I0523 05:13:45.672965 30336 solver.cpp:237] Iteration 92100, loss = 1.23891
I0523 05:13:45.673156 30336 solver.cpp:253]     Train net output #0: loss = 1.23891 (* 1 = 1.23891 loss)
I0523 05:13:45.673171 30336 sgd_solver.cpp:106] Iteration 92100, lr = 0.0015
I0523 05:13:54.965939 30336 solver.cpp:237] Iteration 92400, loss = 1.12443
I0523 05:13:54.965984 30336 solver.cpp:253]     Train net output #0: loss = 1.12443 (* 1 = 1.12443 loss)
I0523 05:13:54.966006 30336 sgd_solver.cpp:106] Iteration 92400, lr = 0.0015
I0523 05:14:04.254817 30336 solver.cpp:237] Iteration 92700, loss = 1.50152
I0523 05:14:04.254853 30336 solver.cpp:253]     Train net output #0: loss = 1.50152 (* 1 = 1.50152 loss)
I0523 05:14:04.254868 30336 sgd_solver.cpp:106] Iteration 92700, lr = 0.0015
I0523 05:14:13.515815 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_93000.caffemodel
I0523 05:14:13.574702 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_93000.solverstate
I0523 05:14:13.610407 30336 solver.cpp:237] Iteration 93000, loss = 0.919219
I0523 05:14:13.610452 30336 solver.cpp:253]     Train net output #0: loss = 0.919219 (* 1 = 0.919219 loss)
I0523 05:14:13.610471 30336 sgd_solver.cpp:106] Iteration 93000, lr = 0.0015
I0523 05:14:22.902662 30336 solver.cpp:237] Iteration 93300, loss = 1.23929
I0523 05:14:22.902845 30336 solver.cpp:253]     Train net output #0: loss = 1.23929 (* 1 = 1.23929 loss)
I0523 05:14:22.902860 30336 sgd_solver.cpp:106] Iteration 93300, lr = 0.0015
I0523 05:14:32.196487 30336 solver.cpp:237] Iteration 93600, loss = 1.21402
I0523 05:14:32.196521 30336 solver.cpp:253]     Train net output #0: loss = 1.21402 (* 1 = 1.21402 loss)
I0523 05:14:32.196538 30336 sgd_solver.cpp:106] Iteration 93600, lr = 0.0015
I0523 05:14:41.488608 30336 solver.cpp:237] Iteration 93900, loss = 0.965077
I0523 05:14:41.488646 30336 solver.cpp:253]     Train net output #0: loss = 0.965077 (* 1 = 0.965077 loss)
I0523 05:14:41.488662 30336 sgd_solver.cpp:106] Iteration 93900, lr = 0.0015
I0523 05:15:11.641901 30336 solver.cpp:237] Iteration 94200, loss = 1.4365
I0523 05:15:11.642104 30336 solver.cpp:253]     Train net output #0: loss = 1.4365 (* 1 = 1.4365 loss)
I0523 05:15:11.642122 30336 sgd_solver.cpp:106] Iteration 94200, lr = 0.0015
I0523 05:15:20.934727 30336 solver.cpp:237] Iteration 94500, loss = 1.24429
I0523 05:15:20.934762 30336 solver.cpp:253]     Train net output #0: loss = 1.24429 (* 1 = 1.24429 loss)
I0523 05:15:20.934779 30336 sgd_solver.cpp:106] Iteration 94500, lr = 0.0015
I0523 05:15:30.223584 30336 solver.cpp:237] Iteration 94800, loss = 1.49331
I0523 05:15:30.223620 30336 solver.cpp:253]     Train net output #0: loss = 1.49331 (* 1 = 1.49331 loss)
I0523 05:15:30.223636 30336 sgd_solver.cpp:106] Iteration 94800, lr = 0.0015
I0523 05:15:39.515230 30336 solver.cpp:237] Iteration 95100, loss = 1.11466
I0523 05:15:39.515280 30336 solver.cpp:253]     Train net output #0: loss = 1.11466 (* 1 = 1.11466 loss)
I0523 05:15:39.515298 30336 sgd_solver.cpp:106] Iteration 95100, lr = 0.0015
I0523 05:15:48.805658 30336 solver.cpp:237] Iteration 95400, loss = 1.38589
I0523 05:15:48.805829 30336 solver.cpp:253]     Train net output #0: loss = 1.38589 (* 1 = 1.38589 loss)
I0523 05:15:48.805843 30336 sgd_solver.cpp:106] Iteration 95400, lr = 0.0015
I0523 05:15:58.097391 30336 solver.cpp:237] Iteration 95700, loss = 1.11966
I0523 05:15:58.097445 30336 solver.cpp:253]     Train net output #0: loss = 1.11966 (* 1 = 1.11966 loss)
I0523 05:15:58.097460 30336 sgd_solver.cpp:106] Iteration 95700, lr = 0.0015
I0523 05:16:07.353947 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_96000.caffemodel
I0523 05:16:07.413136 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_96000.solverstate
I0523 05:16:07.439301 30336 solver.cpp:341] Iteration 96000, Testing net (#0)
I0523 05:17:16.062474 30336 solver.cpp:409]     Test net output #0: accuracy = 0.885981
I0523 05:17:16.062664 30336 solver.cpp:409]     Test net output #1: loss = 0.350638 (* 1 = 0.350638 loss)
I0523 05:17:36.918179 30336 solver.cpp:237] Iteration 96000, loss = 0.986345
I0523 05:17:36.918236 30336 solver.cpp:253]     Train net output #0: loss = 0.986345 (* 1 = 0.986345 loss)
I0523 05:17:36.918251 30336 sgd_solver.cpp:106] Iteration 96000, lr = 0.0015
I0523 05:17:46.207958 30336 solver.cpp:237] Iteration 96300, loss = 1.45294
I0523 05:17:46.208148 30336 solver.cpp:253]     Train net output #0: loss = 1.45294 (* 1 = 1.45294 loss)
I0523 05:17:46.208161 30336 sgd_solver.cpp:106] Iteration 96300, lr = 0.0015
I0523 05:17:55.499270 30336 solver.cpp:237] Iteration 96600, loss = 1.09894
I0523 05:17:55.499305 30336 solver.cpp:253]     Train net output #0: loss = 1.09894 (* 1 = 1.09894 loss)
I0523 05:17:55.499322 30336 sgd_solver.cpp:106] Iteration 96600, lr = 0.0015
I0523 05:18:04.789753 30336 solver.cpp:237] Iteration 96900, loss = 1.17911
I0523 05:18:04.789804 30336 solver.cpp:253]     Train net output #0: loss = 1.17911 (* 1 = 1.17911 loss)
I0523 05:18:04.789820 30336 sgd_solver.cpp:106] Iteration 96900, lr = 0.0015
I0523 05:18:14.078018 30336 solver.cpp:237] Iteration 97200, loss = 0.871455
I0523 05:18:14.078054 30336 solver.cpp:253]     Train net output #0: loss = 0.871455 (* 1 = 0.871455 loss)
I0523 05:18:14.078070 30336 sgd_solver.cpp:106] Iteration 97200, lr = 0.0015
I0523 05:18:23.369809 30336 solver.cpp:237] Iteration 97500, loss = 1.09105
I0523 05:18:23.369989 30336 solver.cpp:253]     Train net output #0: loss = 1.09105 (* 1 = 1.09105 loss)
I0523 05:18:23.370004 30336 sgd_solver.cpp:106] Iteration 97500, lr = 0.0015
I0523 05:18:32.658251 30336 solver.cpp:237] Iteration 97800, loss = 1.14279
I0523 05:18:32.658298 30336 solver.cpp:253]     Train net output #0: loss = 1.14279 (* 1 = 1.14279 loss)
I0523 05:18:32.658313 30336 sgd_solver.cpp:106] Iteration 97800, lr = 0.0015
I0523 05:19:02.790680 30336 solver.cpp:237] Iteration 98100, loss = 0.91139
I0523 05:19:02.790879 30336 solver.cpp:253]     Train net output #0: loss = 0.911391 (* 1 = 0.911391 loss)
I0523 05:19:02.790895 30336 sgd_solver.cpp:106] Iteration 98100, lr = 0.0015
I0523 05:19:12.081022 30336 solver.cpp:237] Iteration 98400, loss = 0.930931
I0523 05:19:12.081058 30336 solver.cpp:253]     Train net output #0: loss = 0.930931 (* 1 = 0.930931 loss)
I0523 05:19:12.081073 30336 sgd_solver.cpp:106] Iteration 98400, lr = 0.0015
I0523 05:19:21.369834 30336 solver.cpp:237] Iteration 98700, loss = 1.1411
I0523 05:19:21.369886 30336 solver.cpp:253]     Train net output #0: loss = 1.1411 (* 1 = 1.1411 loss)
I0523 05:19:21.369901 30336 sgd_solver.cpp:106] Iteration 98700, lr = 0.0015
I0523 05:19:30.631773 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_99000.caffemodel
I0523 05:19:30.691143 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_99000.solverstate
I0523 05:19:30.726847 30336 solver.cpp:237] Iteration 99000, loss = 1.2256
I0523 05:19:30.726894 30336 solver.cpp:253]     Train net output #0: loss = 1.2256 (* 1 = 1.2256 loss)
I0523 05:19:30.726912 30336 sgd_solver.cpp:106] Iteration 99000, lr = 0.0015
I0523 05:19:40.017940 30336 solver.cpp:237] Iteration 99300, loss = 1.07652
I0523 05:19:40.018115 30336 solver.cpp:253]     Train net output #0: loss = 1.07652 (* 1 = 1.07652 loss)
I0523 05:19:40.018128 30336 sgd_solver.cpp:106] Iteration 99300, lr = 0.0015
I0523 05:19:49.310503 30336 solver.cpp:237] Iteration 99600, loss = 1.17483
I0523 05:19:49.310554 30336 solver.cpp:253]     Train net output #0: loss = 1.17483 (* 1 = 1.17483 loss)
I0523 05:19:49.310570 30336 sgd_solver.cpp:106] Iteration 99600, lr = 0.0015
I0523 05:19:58.598634 30336 solver.cpp:237] Iteration 99900, loss = 1.14034
I0523 05:19:58.598667 30336 solver.cpp:253]     Train net output #0: loss = 1.14034 (* 1 = 1.14034 loss)
I0523 05:19:58.598685 30336 sgd_solver.cpp:106] Iteration 99900, lr = 0.0015
I0523 05:20:28.733614 30336 solver.cpp:237] Iteration 100200, loss = 1.06818
I0523 05:20:28.733808 30336 solver.cpp:253]     Train net output #0: loss = 1.06818 (* 1 = 1.06818 loss)
I0523 05:20:28.733822 30336 sgd_solver.cpp:106] Iteration 100200, lr = 0.0015
I0523 05:20:38.023001 30336 solver.cpp:237] Iteration 100500, loss = 1.27349
I0523 05:20:38.023053 30336 solver.cpp:253]     Train net output #0: loss = 1.27349 (* 1 = 1.27349 loss)
I0523 05:20:38.023069 30336 sgd_solver.cpp:106] Iteration 100500, lr = 0.0015
I0523 05:20:47.315212 30336 solver.cpp:237] Iteration 100800, loss = 1.18632
I0523 05:20:47.315246 30336 solver.cpp:253]     Train net output #0: loss = 1.18632 (* 1 = 1.18632 loss)
I0523 05:20:47.315263 30336 sgd_solver.cpp:106] Iteration 100800, lr = 0.0015
I0523 05:20:56.610469 30336 solver.cpp:237] Iteration 101100, loss = 1.45211
I0523 05:20:56.610505 30336 solver.cpp:253]     Train net output #0: loss = 1.45211 (* 1 = 1.45211 loss)
I0523 05:20:56.610522 30336 sgd_solver.cpp:106] Iteration 101100, lr = 0.0015
I0523 05:21:05.901621 30336 solver.cpp:237] Iteration 101400, loss = 1.24835
I0523 05:21:05.901808 30336 solver.cpp:253]     Train net output #0: loss = 1.24835 (* 1 = 1.24835 loss)
I0523 05:21:05.901823 30336 sgd_solver.cpp:106] Iteration 101400, lr = 0.0015
I0523 05:21:15.194382 30336 solver.cpp:237] Iteration 101700, loss = 0.971007
I0523 05:21:15.194418 30336 solver.cpp:253]     Train net output #0: loss = 0.971007 (* 1 = 0.971007 loss)
I0523 05:21:15.194433 30336 sgd_solver.cpp:106] Iteration 101700, lr = 0.0015
I0523 05:21:24.456787 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_102000.caffemodel
I0523 05:21:24.515679 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_102000.solverstate
I0523 05:21:24.540735 30336 solver.cpp:341] Iteration 102000, Testing net (#0)
I0523 05:22:12.292568 30336 solver.cpp:409]     Test net output #0: accuracy = 0.888426
I0523 05:22:12.299598 30336 solver.cpp:409]     Test net output #1: loss = 0.371965 (* 1 = 0.371965 loss)
I0523 05:22:33.145949 30336 solver.cpp:237] Iteration 102000, loss = 1.10811
I0523 05:22:33.146008 30336 solver.cpp:253]     Train net output #0: loss = 1.10811 (* 1 = 1.10811 loss)
I0523 05:22:33.146023 30336 sgd_solver.cpp:106] Iteration 102000, lr = 0.0015
I0523 05:22:42.436347 30336 solver.cpp:237] Iteration 102300, loss = 1.0392
I0523 05:22:42.436538 30336 solver.cpp:253]     Train net output #0: loss = 1.0392 (* 1 = 1.0392 loss)
I0523 05:22:42.436553 30336 sgd_solver.cpp:106] Iteration 102300, lr = 0.0015
I0523 05:22:51.726094 30336 solver.cpp:237] Iteration 102600, loss = 1.0498
I0523 05:22:51.726128 30336 solver.cpp:253]     Train net output #0: loss = 1.0498 (* 1 = 1.0498 loss)
I0523 05:22:51.726145 30336 sgd_solver.cpp:106] Iteration 102600, lr = 0.0015
I0523 05:23:01.016337 30336 solver.cpp:237] Iteration 102900, loss = 1.25589
I0523 05:23:01.016373 30336 solver.cpp:253]     Train net output #0: loss = 1.25589 (* 1 = 1.25589 loss)
I0523 05:23:01.016391 30336 sgd_solver.cpp:106] Iteration 102900, lr = 0.0015
I0523 05:23:10.307296 30336 solver.cpp:237] Iteration 103200, loss = 1.33666
I0523 05:23:10.307343 30336 solver.cpp:253]     Train net output #0: loss = 1.33666 (* 1 = 1.33666 loss)
I0523 05:23:10.307360 30336 sgd_solver.cpp:106] Iteration 103200, lr = 0.0015
I0523 05:23:19.596626 30336 solver.cpp:237] Iteration 103500, loss = 0.992944
I0523 05:23:19.596797 30336 solver.cpp:253]     Train net output #0: loss = 0.992944 (* 1 = 0.992944 loss)
I0523 05:23:19.596812 30336 sgd_solver.cpp:106] Iteration 103500, lr = 0.0015
I0523 05:23:28.886023 30336 solver.cpp:237] Iteration 103800, loss = 1.01432
I0523 05:23:28.886075 30336 solver.cpp:253]     Train net output #0: loss = 1.01432 (* 1 = 1.01432 loss)
I0523 05:23:28.886091 30336 sgd_solver.cpp:106] Iteration 103800, lr = 0.0015
I0523 05:23:59.003151 30336 solver.cpp:237] Iteration 104100, loss = 1.22956
I0523 05:23:59.003347 30336 solver.cpp:253]     Train net output #0: loss = 1.22956 (* 1 = 1.22956 loss)
I0523 05:23:59.003363 30336 sgd_solver.cpp:106] Iteration 104100, lr = 0.0015
I0523 05:24:08.293913 30336 solver.cpp:237] Iteration 104400, loss = 1.28196
I0523 05:24:08.293943 30336 solver.cpp:253]     Train net output #0: loss = 1.28196 (* 1 = 1.28196 loss)
I0523 05:24:08.293957 30336 sgd_solver.cpp:106] Iteration 104400, lr = 0.0015
I0523 05:24:17.588659 30336 solver.cpp:237] Iteration 104700, loss = 1.13607
I0523 05:24:17.588706 30336 solver.cpp:253]     Train net output #0: loss = 1.13607 (* 1 = 1.13607 loss)
I0523 05:24:17.588723 30336 sgd_solver.cpp:106] Iteration 104700, lr = 0.0015
I0523 05:24:26.850239 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_105000.caffemodel
I0523 05:24:26.911072 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_105000.solverstate
I0523 05:24:26.948180 30336 solver.cpp:237] Iteration 105000, loss = 1.26986
I0523 05:24:26.948233 30336 solver.cpp:253]     Train net output #0: loss = 1.26986 (* 1 = 1.26986 loss)
I0523 05:24:26.948251 30336 sgd_solver.cpp:106] Iteration 105000, lr = 0.0015
I0523 05:24:36.240599 30336 solver.cpp:237] Iteration 105300, loss = 1.29941
I0523 05:24:36.240787 30336 solver.cpp:253]     Train net output #0: loss = 1.29941 (* 1 = 1.29941 loss)
I0523 05:24:36.240802 30336 sgd_solver.cpp:106] Iteration 105300, lr = 0.0015
I0523 05:24:45.529095 30336 solver.cpp:237] Iteration 105600, loss = 1.25606
I0523 05:24:45.529139 30336 solver.cpp:253]     Train net output #0: loss = 1.25606 (* 1 = 1.25606 loss)
I0523 05:24:45.529161 30336 sgd_solver.cpp:106] Iteration 105600, lr = 0.0015
I0523 05:24:54.818892 30336 solver.cpp:237] Iteration 105900, loss = 1.2831
I0523 05:24:54.818927 30336 solver.cpp:253]     Train net output #0: loss = 1.2831 (* 1 = 1.2831 loss)
I0523 05:24:54.818943 30336 sgd_solver.cpp:106] Iteration 105900, lr = 0.0015
I0523 05:25:24.967344 30336 solver.cpp:237] Iteration 106200, loss = 1.24078
I0523 05:25:24.967541 30336 solver.cpp:253]     Train net output #0: loss = 1.24078 (* 1 = 1.24078 loss)
I0523 05:25:24.967557 30336 sgd_solver.cpp:106] Iteration 106200, lr = 0.0015
I0523 05:25:34.258050 30336 solver.cpp:237] Iteration 106500, loss = 1.26692
I0523 05:25:34.258100 30336 solver.cpp:253]     Train net output #0: loss = 1.26692 (* 1 = 1.26692 loss)
I0523 05:25:34.258116 30336 sgd_solver.cpp:106] Iteration 106500, lr = 0.0015
I0523 05:25:43.548210 30336 solver.cpp:237] Iteration 106800, loss = 1.32902
I0523 05:25:43.548246 30336 solver.cpp:253]     Train net output #0: loss = 1.32902 (* 1 = 1.32902 loss)
I0523 05:25:43.548264 30336 sgd_solver.cpp:106] Iteration 106800, lr = 0.0015
I0523 05:25:52.840494 30336 solver.cpp:237] Iteration 107100, loss = 1.25622
I0523 05:25:52.840529 30336 solver.cpp:253]     Train net output #0: loss = 1.25622 (* 1 = 1.25622 loss)
I0523 05:25:52.840543 30336 sgd_solver.cpp:106] Iteration 107100, lr = 0.0015
I0523 05:26:02.134690 30336 solver.cpp:237] Iteration 107400, loss = 1.50922
I0523 05:26:02.134873 30336 solver.cpp:253]     Train net output #0: loss = 1.50922 (* 1 = 1.50922 loss)
I0523 05:26:02.134887 30336 sgd_solver.cpp:106] Iteration 107400, lr = 0.0015
I0523 05:26:11.426731 30336 solver.cpp:237] Iteration 107700, loss = 0.79092
I0523 05:26:11.426765 30336 solver.cpp:253]     Train net output #0: loss = 0.79092 (* 1 = 0.79092 loss)
I0523 05:26:11.426784 30336 sgd_solver.cpp:106] Iteration 107700, lr = 0.0015
I0523 05:26:20.687309 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_108000.caffemodel
I0523 05:26:20.747987 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_108000.solverstate
I0523 05:26:20.774786 30336 solver.cpp:341] Iteration 108000, Testing net (#0)
I0523 05:27:29.370311 30336 solver.cpp:409]     Test net output #0: accuracy = 0.890173
I0523 05:27:29.370507 30336 solver.cpp:409]     Test net output #1: loss = 0.348528 (* 1 = 0.348528 loss)
I0523 05:27:50.211184 30336 solver.cpp:237] Iteration 108000, loss = 1.03308
I0523 05:27:50.211239 30336 solver.cpp:253]     Train net output #0: loss = 1.03308 (* 1 = 1.03308 loss)
I0523 05:27:50.211257 30336 sgd_solver.cpp:106] Iteration 108000, lr = 0.0015
I0523 05:27:59.505187 30336 solver.cpp:237] Iteration 108300, loss = 1.16976
I0523 05:27:59.505362 30336 solver.cpp:253]     Train net output #0: loss = 1.16976 (* 1 = 1.16976 loss)
I0523 05:27:59.505375 30336 sgd_solver.cpp:106] Iteration 108300, lr = 0.0015
I0523 05:28:08.806371 30336 solver.cpp:237] Iteration 108600, loss = 1.33189
I0523 05:28:08.806412 30336 solver.cpp:253]     Train net output #0: loss = 1.33189 (* 1 = 1.33189 loss)
I0523 05:28:08.806432 30336 sgd_solver.cpp:106] Iteration 108600, lr = 0.0015
I0523 05:28:18.098773 30336 solver.cpp:237] Iteration 108900, loss = 1.18384
I0523 05:28:18.098809 30336 solver.cpp:253]     Train net output #0: loss = 1.18384 (* 1 = 1.18384 loss)
I0523 05:28:18.098824 30336 sgd_solver.cpp:106] Iteration 108900, lr = 0.0015
I0523 05:28:27.400769 30336 solver.cpp:237] Iteration 109200, loss = 1.42436
I0523 05:28:27.400813 30336 solver.cpp:253]     Train net output #0: loss = 1.42436 (* 1 = 1.42436 loss)
I0523 05:28:27.400830 30336 sgd_solver.cpp:106] Iteration 109200, lr = 0.0015
I0523 05:28:36.699182 30336 solver.cpp:237] Iteration 109500, loss = 1.06416
I0523 05:28:36.699363 30336 solver.cpp:253]     Train net output #0: loss = 1.06416 (* 1 = 1.06416 loss)
I0523 05:28:36.699378 30336 sgd_solver.cpp:106] Iteration 109500, lr = 0.0015
I0523 05:28:45.997581 30336 solver.cpp:237] Iteration 109800, loss = 1.08776
I0523 05:28:45.997617 30336 solver.cpp:253]     Train net output #0: loss = 1.08776 (* 1 = 1.08776 loss)
I0523 05:28:45.997632 30336 sgd_solver.cpp:106] Iteration 109800, lr = 0.0015
I0523 05:29:16.139642 30336 solver.cpp:237] Iteration 110100, loss = 1.1601
I0523 05:29:16.139844 30336 solver.cpp:253]     Train net output #0: loss = 1.1601 (* 1 = 1.1601 loss)
I0523 05:29:16.139860 30336 sgd_solver.cpp:106] Iteration 110100, lr = 0.0015
I0523 05:29:25.433526 30336 solver.cpp:237] Iteration 110400, loss = 1.10829
I0523 05:29:25.433562 30336 solver.cpp:253]     Train net output #0: loss = 1.10829 (* 1 = 1.10829 loss)
I0523 05:29:25.433583 30336 sgd_solver.cpp:106] Iteration 110400, lr = 0.0015
I0523 05:29:34.731035 30336 solver.cpp:237] Iteration 110700, loss = 1.14038
I0523 05:29:34.731072 30336 solver.cpp:253]     Train net output #0: loss = 1.14038 (* 1 = 1.14038 loss)
I0523 05:29:34.731088 30336 sgd_solver.cpp:106] Iteration 110700, lr = 0.0015
I0523 05:29:43.999362 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_111000.caffemodel
I0523 05:29:44.058269 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_111000.solverstate
I0523 05:29:44.092978 30336 solver.cpp:237] Iteration 111000, loss = 0.848493
I0523 05:29:44.093024 30336 solver.cpp:253]     Train net output #0: loss = 0.848493 (* 1 = 0.848493 loss)
I0523 05:29:44.093045 30336 sgd_solver.cpp:106] Iteration 111000, lr = 0.0015
I0523 05:29:53.393534 30336 solver.cpp:237] Iteration 111300, loss = 1.14437
I0523 05:29:53.393712 30336 solver.cpp:253]     Train net output #0: loss = 1.14437 (* 1 = 1.14437 loss)
I0523 05:29:53.393728 30336 sgd_solver.cpp:106] Iteration 111300, lr = 0.0015
I0523 05:30:02.687067 30336 solver.cpp:237] Iteration 111600, loss = 1.08992
I0523 05:30:02.687100 30336 solver.cpp:253]     Train net output #0: loss = 1.08992 (* 1 = 1.08992 loss)
I0523 05:30:02.687119 30336 sgd_solver.cpp:106] Iteration 111600, lr = 0.0015
I0523 05:30:11.982457 30336 solver.cpp:237] Iteration 111900, loss = 0.863254
I0523 05:30:11.982506 30336 solver.cpp:253]     Train net output #0: loss = 0.863254 (* 1 = 0.863254 loss)
I0523 05:30:11.982522 30336 sgd_solver.cpp:106] Iteration 111900, lr = 0.0015
I0523 05:30:42.114092 30336 solver.cpp:237] Iteration 112200, loss = 1.04206
I0523 05:30:42.114290 30336 solver.cpp:253]     Train net output #0: loss = 1.04206 (* 1 = 1.04206 loss)
I0523 05:30:42.114306 30336 sgd_solver.cpp:106] Iteration 112200, lr = 0.0015
I0523 05:30:51.410886 30336 solver.cpp:237] Iteration 112500, loss = 1.28958
I0523 05:30:51.410920 30336 solver.cpp:253]     Train net output #0: loss = 1.28958 (* 1 = 1.28958 loss)
I0523 05:30:51.410938 30336 sgd_solver.cpp:106] Iteration 112500, lr = 0.0015
I0523 05:31:00.711382 30336 solver.cpp:237] Iteration 112800, loss = 1.37125
I0523 05:31:00.711432 30336 solver.cpp:253]     Train net output #0: loss = 1.37125 (* 1 = 1.37125 loss)
I0523 05:31:00.711447 30336 sgd_solver.cpp:106] Iteration 112800, lr = 0.0015
I0523 05:31:10.011847 30336 solver.cpp:237] Iteration 113100, loss = 1.32767
I0523 05:31:10.011883 30336 solver.cpp:253]     Train net output #0: loss = 1.32767 (* 1 = 1.32767 loss)
I0523 05:31:10.011898 30336 sgd_solver.cpp:106] Iteration 113100, lr = 0.0015
I0523 05:31:19.305443 30336 solver.cpp:237] Iteration 113400, loss = 1.17222
I0523 05:31:19.305626 30336 solver.cpp:253]     Train net output #0: loss = 1.17222 (* 1 = 1.17222 loss)
I0523 05:31:19.305640 30336 sgd_solver.cpp:106] Iteration 113400, lr = 0.0015
I0523 05:31:28.607580 30336 solver.cpp:237] Iteration 113700, loss = 0.967411
I0523 05:31:28.607628 30336 solver.cpp:253]     Train net output #0: loss = 0.967411 (* 1 = 0.967411 loss)
I0523 05:31:28.607646 30336 sgd_solver.cpp:106] Iteration 113700, lr = 0.0015
I0523 05:31:37.874150 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_114000.caffemodel
I0523 05:31:37.933751 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_114000.solverstate
I0523 05:31:37.958446 30336 solver.cpp:341] Iteration 114000, Testing net (#0)
I0523 05:32:25.427805 30336 solver.cpp:409]     Test net output #0: accuracy = 0.886711
I0523 05:32:25.428007 30336 solver.cpp:409]     Test net output #1: loss = 0.371339 (* 1 = 0.371339 loss)
I0523 05:32:46.329061 30336 solver.cpp:237] Iteration 114000, loss = 1.25321
I0523 05:32:46.329118 30336 solver.cpp:253]     Train net output #0: loss = 1.25321 (* 1 = 1.25321 loss)
I0523 05:32:46.329135 30336 sgd_solver.cpp:106] Iteration 114000, lr = 0.0015
I0523 05:32:55.620986 30336 solver.cpp:237] Iteration 114300, loss = 1.17836
I0523 05:32:55.621168 30336 solver.cpp:253]     Train net output #0: loss = 1.17836 (* 1 = 1.17836 loss)
I0523 05:32:55.621182 30336 sgd_solver.cpp:106] Iteration 114300, lr = 0.0015
I0523 05:33:04.907368 30336 solver.cpp:237] Iteration 114600, loss = 1.02244
I0523 05:33:04.907413 30336 solver.cpp:253]     Train net output #0: loss = 1.02244 (* 1 = 1.02244 loss)
I0523 05:33:04.907433 30336 sgd_solver.cpp:106] Iteration 114600, lr = 0.0015
I0523 05:33:14.194947 30336 solver.cpp:237] Iteration 114900, loss = 1.01331
I0523 05:33:14.194983 30336 solver.cpp:253]     Train net output #0: loss = 1.01331 (* 1 = 1.01331 loss)
I0523 05:33:14.194998 30336 sgd_solver.cpp:106] Iteration 114900, lr = 0.0015
I0523 05:33:23.481838 30336 solver.cpp:237] Iteration 115200, loss = 1.22506
I0523 05:33:23.481873 30336 solver.cpp:253]     Train net output #0: loss = 1.22506 (* 1 = 1.22506 loss)
I0523 05:33:23.481890 30336 sgd_solver.cpp:106] Iteration 115200, lr = 0.0015
I0523 05:33:32.774039 30336 solver.cpp:237] Iteration 115500, loss = 0.923668
I0523 05:33:32.774233 30336 solver.cpp:253]     Train net output #0: loss = 0.923668 (* 1 = 0.923668 loss)
I0523 05:33:32.774248 30336 sgd_solver.cpp:106] Iteration 115500, lr = 0.0015
I0523 05:33:42.062198 30336 solver.cpp:237] Iteration 115800, loss = 0.936771
I0523 05:33:42.062234 30336 solver.cpp:253]     Train net output #0: loss = 0.936771 (* 1 = 0.936771 loss)
I0523 05:33:42.062250 30336 sgd_solver.cpp:106] Iteration 115800, lr = 0.0015
I0523 05:34:12.204294 30336 solver.cpp:237] Iteration 116100, loss = 1.56336
I0523 05:34:12.204495 30336 solver.cpp:253]     Train net output #0: loss = 1.56336 (* 1 = 1.56336 loss)
I0523 05:34:12.204510 30336 sgd_solver.cpp:106] Iteration 116100, lr = 0.0015
I0523 05:34:21.498278 30336 solver.cpp:237] Iteration 116400, loss = 1.37042
I0523 05:34:21.498327 30336 solver.cpp:253]     Train net output #0: loss = 1.37042 (* 1 = 1.37042 loss)
I0523 05:34:21.498342 30336 sgd_solver.cpp:106] Iteration 116400, lr = 0.0015
I0523 05:34:30.781685 30336 solver.cpp:237] Iteration 116700, loss = 1.05277
I0523 05:34:30.781720 30336 solver.cpp:253]     Train net output #0: loss = 1.05277 (* 1 = 1.05277 loss)
I0523 05:34:30.781739 30336 sgd_solver.cpp:106] Iteration 116700, lr = 0.0015
I0523 05:34:40.034694 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_117000.caffemodel
I0523 05:34:40.093499 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_117000.solverstate
I0523 05:34:40.128113 30336 solver.cpp:237] Iteration 117000, loss = 1.25329
I0523 05:34:40.128159 30336 solver.cpp:253]     Train net output #0: loss = 1.25329 (* 1 = 1.25329 loss)
I0523 05:34:40.128177 30336 sgd_solver.cpp:106] Iteration 117000, lr = 0.0015
I0523 05:34:49.413586 30336 solver.cpp:237] Iteration 117300, loss = 1.29679
I0523 05:34:49.413795 30336 solver.cpp:253]     Train net output #0: loss = 1.29679 (* 1 = 1.29679 loss)
I0523 05:34:49.413810 30336 sgd_solver.cpp:106] Iteration 117300, lr = 0.0015
I0523 05:34:58.696534 30336 solver.cpp:237] Iteration 117600, loss = 1.54718
I0523 05:34:58.696569 30336 solver.cpp:253]     Train net output #0: loss = 1.54718 (* 1 = 1.54718 loss)
I0523 05:34:58.696588 30336 sgd_solver.cpp:106] Iteration 117600, lr = 0.0015
I0523 05:35:07.987977 30336 solver.cpp:237] Iteration 117900, loss = 1.15627
I0523 05:35:07.988029 30336 solver.cpp:253]     Train net output #0: loss = 1.15627 (* 1 = 1.15627 loss)
I0523 05:35:07.988044 30336 sgd_solver.cpp:106] Iteration 117900, lr = 0.0015
I0523 05:35:38.147423 30336 solver.cpp:237] Iteration 118200, loss = 1.4814
I0523 05:35:38.147621 30336 solver.cpp:253]     Train net output #0: loss = 1.4814 (* 1 = 1.4814 loss)
I0523 05:35:38.147637 30336 sgd_solver.cpp:106] Iteration 118200, lr = 0.0015
I0523 05:35:47.434386 30336 solver.cpp:237] Iteration 118500, loss = 1.32532
I0523 05:35:47.434420 30336 solver.cpp:253]     Train net output #0: loss = 1.32532 (* 1 = 1.32532 loss)
I0523 05:35:47.434437 30336 sgd_solver.cpp:106] Iteration 118500, lr = 0.0015
I0523 05:35:56.721496 30336 solver.cpp:237] Iteration 118800, loss = 1.12179
I0523 05:35:56.721532 30336 solver.cpp:253]     Train net output #0: loss = 1.12179 (* 1 = 1.12179 loss)
I0523 05:35:56.721549 30336 sgd_solver.cpp:106] Iteration 118800, lr = 0.0015
I0523 05:36:06.010376 30336 solver.cpp:237] Iteration 119100, loss = 1.29334
I0523 05:36:06.010418 30336 solver.cpp:253]     Train net output #0: loss = 1.29334 (* 1 = 1.29334 loss)
I0523 05:36:06.010439 30336 sgd_solver.cpp:106] Iteration 119100, lr = 0.0015
I0523 05:36:15.299919 30336 solver.cpp:237] Iteration 119400, loss = 0.828513
I0523 05:36:15.300092 30336 solver.cpp:253]     Train net output #0: loss = 0.828513 (* 1 = 0.828513 loss)
I0523 05:36:15.300107 30336 sgd_solver.cpp:106] Iteration 119400, lr = 0.0015
I0523 05:36:24.586619 30336 solver.cpp:237] Iteration 119700, loss = 1.29356
I0523 05:36:24.586668 30336 solver.cpp:253]     Train net output #0: loss = 1.29356 (* 1 = 1.29356 loss)
I0523 05:36:24.586681 30336 sgd_solver.cpp:106] Iteration 119700, lr = 0.0015
I0523 05:36:33.841074 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_120000.caffemodel
I0523 05:36:33.901826 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_120000.solverstate
I0523 05:36:33.929249 30336 solver.cpp:341] Iteration 120000, Testing net (#0)
I0523 05:37:42.626003 30336 solver.cpp:409]     Test net output #0: accuracy = 0.889373
I0523 05:37:42.626204 30336 solver.cpp:409]     Test net output #1: loss = 0.353601 (* 1 = 0.353601 loss)
I0523 05:38:03.464655 30336 solver.cpp:237] Iteration 120000, loss = 1.18023
I0523 05:38:03.464710 30336 solver.cpp:253]     Train net output #0: loss = 1.18023 (* 1 = 1.18023 loss)
I0523 05:38:03.464727 30336 sgd_solver.cpp:106] Iteration 120000, lr = 0.0015
I0523 05:38:12.750636 30336 solver.cpp:237] Iteration 120300, loss = 1.03212
I0523 05:38:12.750828 30336 solver.cpp:253]     Train net output #0: loss = 1.03212 (* 1 = 1.03212 loss)
I0523 05:38:12.750841 30336 sgd_solver.cpp:106] Iteration 120300, lr = 0.0015
I0523 05:38:22.034132 30336 solver.cpp:237] Iteration 120600, loss = 1.33603
I0523 05:38:22.034167 30336 solver.cpp:253]     Train net output #0: loss = 1.33603 (* 1 = 1.33603 loss)
I0523 05:38:22.034183 30336 sgd_solver.cpp:106] Iteration 120600, lr = 0.0015
I0523 05:38:31.321274 30336 solver.cpp:237] Iteration 120900, loss = 1.14608
I0523 05:38:31.321319 30336 solver.cpp:253]     Train net output #0: loss = 1.14608 (* 1 = 1.14608 loss)
I0523 05:38:31.321337 30336 sgd_solver.cpp:106] Iteration 120900, lr = 0.0015
I0523 05:38:40.602064 30336 solver.cpp:237] Iteration 121200, loss = 1.03074
I0523 05:38:40.602100 30336 solver.cpp:253]     Train net output #0: loss = 1.03074 (* 1 = 1.03074 loss)
I0523 05:38:40.602118 30336 sgd_solver.cpp:106] Iteration 121200, lr = 0.0015
I0523 05:38:49.887127 30336 solver.cpp:237] Iteration 121500, loss = 1.2037
I0523 05:38:49.887305 30336 solver.cpp:253]     Train net output #0: loss = 1.2037 (* 1 = 1.2037 loss)
I0523 05:38:49.887318 30336 sgd_solver.cpp:106] Iteration 121500, lr = 0.0015
I0523 05:38:59.172952 30336 solver.cpp:237] Iteration 121800, loss = 1.58705
I0523 05:38:59.172991 30336 solver.cpp:253]     Train net output #0: loss = 1.58705 (* 1 = 1.58705 loss)
I0523 05:38:59.173009 30336 sgd_solver.cpp:106] Iteration 121800, lr = 0.0015
I0523 05:39:29.327350 30336 solver.cpp:237] Iteration 122100, loss = 1.22104
I0523 05:39:29.327551 30336 solver.cpp:253]     Train net output #0: loss = 1.22104 (* 1 = 1.22104 loss)
I0523 05:39:29.327569 30336 sgd_solver.cpp:106] Iteration 122100, lr = 0.0015
I0523 05:39:38.613116 30336 solver.cpp:237] Iteration 122400, loss = 1.04137
I0523 05:39:38.613149 30336 solver.cpp:253]     Train net output #0: loss = 1.04137 (* 1 = 1.04137 loss)
I0523 05:39:38.613168 30336 sgd_solver.cpp:106] Iteration 122400, lr = 0.0015
I0523 05:39:47.900209 30336 solver.cpp:237] Iteration 122700, loss = 1.64773
I0523 05:39:47.900252 30336 solver.cpp:253]     Train net output #0: loss = 1.64773 (* 1 = 1.64773 loss)
I0523 05:39:47.900270 30336 sgd_solver.cpp:106] Iteration 122700, lr = 0.0015
I0523 05:39:57.154585 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_123000.caffemodel
I0523 05:39:57.213707 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_123000.solverstate
I0523 05:39:57.248930 30336 solver.cpp:237] Iteration 123000, loss = 1.07191
I0523 05:39:57.248980 30336 solver.cpp:253]     Train net output #0: loss = 1.07191 (* 1 = 1.07191 loss)
I0523 05:39:57.248996 30336 sgd_solver.cpp:106] Iteration 123000, lr = 0.0015
I0523 05:40:06.532502 30336 solver.cpp:237] Iteration 123300, loss = 1.1959
I0523 05:40:06.532681 30336 solver.cpp:253]     Train net output #0: loss = 1.1959 (* 1 = 1.1959 loss)
I0523 05:40:06.532696 30336 sgd_solver.cpp:106] Iteration 123300, lr = 0.0015
I0523 05:40:15.816035 30336 solver.cpp:237] Iteration 123600, loss = 1.0896
I0523 05:40:15.816082 30336 solver.cpp:253]     Train net output #0: loss = 1.0896 (* 1 = 1.0896 loss)
I0523 05:40:15.816100 30336 sgd_solver.cpp:106] Iteration 123600, lr = 0.0015
I0523 05:40:25.101830 30336 solver.cpp:237] Iteration 123900, loss = 1.08973
I0523 05:40:25.101864 30336 solver.cpp:253]     Train net output #0: loss = 1.08973 (* 1 = 1.08973 loss)
I0523 05:40:25.101881 30336 sgd_solver.cpp:106] Iteration 123900, lr = 0.0015
I0523 05:40:55.245029 30336 solver.cpp:237] Iteration 124200, loss = 1.1589
I0523 05:40:55.245231 30336 solver.cpp:253]     Train net output #0: loss = 1.1589 (* 1 = 1.1589 loss)
I0523 05:40:55.245247 30336 sgd_solver.cpp:106] Iteration 124200, lr = 0.0015
I0523 05:41:04.531682 30336 solver.cpp:237] Iteration 124500, loss = 1.35064
I0523 05:41:04.531721 30336 solver.cpp:253]     Train net output #0: loss = 1.35064 (* 1 = 1.35064 loss)
I0523 05:41:04.531740 30336 sgd_solver.cpp:106] Iteration 124500, lr = 0.0015
I0523 05:41:13.816294 30336 solver.cpp:237] Iteration 124800, loss = 1.23003
I0523 05:41:13.816330 30336 solver.cpp:253]     Train net output #0: loss = 1.23003 (* 1 = 1.23003 loss)
I0523 05:41:13.816347 30336 sgd_solver.cpp:106] Iteration 124800, lr = 0.0015
I0523 05:41:23.104662 30336 solver.cpp:237] Iteration 125100, loss = 1.09892
I0523 05:41:23.104713 30336 solver.cpp:253]     Train net output #0: loss = 1.09892 (* 1 = 1.09892 loss)
I0523 05:41:23.104732 30336 sgd_solver.cpp:106] Iteration 125100, lr = 0.0015
I0523 05:41:32.388598 30336 solver.cpp:237] Iteration 125400, loss = 1.0601
I0523 05:41:32.388787 30336 solver.cpp:253]     Train net output #0: loss = 1.0601 (* 1 = 1.0601 loss)
I0523 05:41:32.388800 30336 sgd_solver.cpp:106] Iteration 125400, lr = 0.0015
I0523 05:41:41.676087 30336 solver.cpp:237] Iteration 125700, loss = 0.990861
I0523 05:41:41.676122 30336 solver.cpp:253]     Train net output #0: loss = 0.990861 (* 1 = 0.990861 loss)
I0523 05:41:41.676139 30336 sgd_solver.cpp:106] Iteration 125700, lr = 0.0015
I0523 05:41:50.932790 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_126000.caffemodel
I0523 05:41:50.991910 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_126000.solverstate
I0523 05:41:51.017143 30336 solver.cpp:341] Iteration 126000, Testing net (#0)
I0523 05:42:38.763263 30336 solver.cpp:409]     Test net output #0: accuracy = 0.892252
I0523 05:42:38.763463 30336 solver.cpp:409]     Test net output #1: loss = 0.34264 (* 1 = 0.34264 loss)
I0523 05:42:59.601274 30336 solver.cpp:237] Iteration 126000, loss = 0.978395
I0523 05:42:59.601330 30336 solver.cpp:253]     Train net output #0: loss = 0.978395 (* 1 = 0.978395 loss)
I0523 05:42:59.601346 30336 sgd_solver.cpp:106] Iteration 126000, lr = 0.0015
I0523 05:43:08.894719 30336 solver.cpp:237] Iteration 126300, loss = 1.30085
I0523 05:43:08.894920 30336 solver.cpp:253]     Train net output #0: loss = 1.30085 (* 1 = 1.30085 loss)
I0523 05:43:08.894934 30336 sgd_solver.cpp:106] Iteration 126300, lr = 0.0015
I0523 05:43:18.185073 30336 solver.cpp:237] Iteration 126600, loss = 1.08904
I0523 05:43:18.185109 30336 solver.cpp:253]     Train net output #0: loss = 1.08904 (* 1 = 1.08904 loss)
I0523 05:43:18.185125 30336 sgd_solver.cpp:106] Iteration 126600, lr = 0.0015
I0523 05:43:27.478338 30336 solver.cpp:237] Iteration 126900, loss = 0.990724
I0523 05:43:27.478394 30336 solver.cpp:253]     Train net output #0: loss = 0.990724 (* 1 = 0.990724 loss)
I0523 05:43:27.478407 30336 sgd_solver.cpp:106] Iteration 126900, lr = 0.0015
I0523 05:43:36.768901 30336 solver.cpp:237] Iteration 127200, loss = 1.25124
I0523 05:43:36.768937 30336 solver.cpp:253]     Train net output #0: loss = 1.25124 (* 1 = 1.25124 loss)
I0523 05:43:36.768954 30336 sgd_solver.cpp:106] Iteration 127200, lr = 0.0015
I0523 05:43:46.062060 30336 solver.cpp:237] Iteration 127500, loss = 1.40221
I0523 05:43:46.062238 30336 solver.cpp:253]     Train net output #0: loss = 1.40221 (* 1 = 1.40221 loss)
I0523 05:43:46.062252 30336 sgd_solver.cpp:106] Iteration 127500, lr = 0.0015
I0523 05:43:55.357673 30336 solver.cpp:237] Iteration 127800, loss = 0.849704
I0523 05:43:55.357725 30336 solver.cpp:253]     Train net output #0: loss = 0.849704 (* 1 = 0.849704 loss)
I0523 05:43:55.357740 30336 sgd_solver.cpp:106] Iteration 127800, lr = 0.0015
I0523 05:44:25.510761 30336 solver.cpp:237] Iteration 128100, loss = 1.07156
I0523 05:44:25.510967 30336 solver.cpp:253]     Train net output #0: loss = 1.07156 (* 1 = 1.07156 loss)
I0523 05:44:25.510983 30336 sgd_solver.cpp:106] Iteration 128100, lr = 0.0015
I0523 05:44:34.802808 30336 solver.cpp:237] Iteration 128400, loss = 1.123
I0523 05:44:34.802842 30336 solver.cpp:253]     Train net output #0: loss = 1.123 (* 1 = 1.123 loss)
I0523 05:44:34.802856 30336 sgd_solver.cpp:106] Iteration 128400, lr = 0.0015
I0523 05:44:44.093765 30336 solver.cpp:237] Iteration 128700, loss = 1.09013
I0523 05:44:44.093817 30336 solver.cpp:253]     Train net output #0: loss = 1.09013 (* 1 = 1.09013 loss)
I0523 05:44:44.093834 30336 sgd_solver.cpp:106] Iteration 128700, lr = 0.0015
I0523 05:44:53.354419 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_129000.caffemodel
I0523 05:44:53.413316 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_129000.solverstate
I0523 05:44:53.448261 30336 solver.cpp:237] Iteration 129000, loss = 1.1562
I0523 05:44:53.448308 30336 solver.cpp:253]     Train net output #0: loss = 1.1562 (* 1 = 1.1562 loss)
I0523 05:44:53.448323 30336 sgd_solver.cpp:106] Iteration 129000, lr = 0.0015
I0523 05:45:02.735960 30336 solver.cpp:237] Iteration 129300, loss = 1.08212
I0523 05:45:02.736156 30336 solver.cpp:253]     Train net output #0: loss = 1.08212 (* 1 = 1.08212 loss)
I0523 05:45:02.736171 30336 sgd_solver.cpp:106] Iteration 129300, lr = 0.0015
I0523 05:45:12.030673 30336 solver.cpp:237] Iteration 129600, loss = 1.17647
I0523 05:45:12.030722 30336 solver.cpp:253]     Train net output #0: loss = 1.17647 (* 1 = 1.17647 loss)
I0523 05:45:12.030740 30336 sgd_solver.cpp:106] Iteration 129600, lr = 0.0015
I0523 05:45:21.324113 30336 solver.cpp:237] Iteration 129900, loss = 1.42726
I0523 05:45:21.324149 30336 solver.cpp:253]     Train net output #0: loss = 1.42726 (* 1 = 1.42726 loss)
I0523 05:45:21.324165 30336 sgd_solver.cpp:106] Iteration 129900, lr = 0.0015
I0523 05:45:51.462695 30336 solver.cpp:237] Iteration 130200, loss = 0.987488
I0523 05:45:51.462900 30336 solver.cpp:253]     Train net output #0: loss = 0.987488 (* 1 = 0.987488 loss)
I0523 05:45:51.462915 30336 sgd_solver.cpp:106] Iteration 130200, lr = 0.0015
I0523 05:46:00.752463 30336 solver.cpp:237] Iteration 130500, loss = 1.35132
I0523 05:46:00.752513 30336 solver.cpp:253]     Train net output #0: loss = 1.35132 (* 1 = 1.35132 loss)
I0523 05:46:00.752532 30336 sgd_solver.cpp:106] Iteration 130500, lr = 0.0015
I0523 05:46:10.047114 30336 solver.cpp:237] Iteration 130800, loss = 1.28886
I0523 05:46:10.047150 30336 solver.cpp:253]     Train net output #0: loss = 1.28886 (* 1 = 1.28886 loss)
I0523 05:46:10.047168 30336 sgd_solver.cpp:106] Iteration 130800, lr = 0.0015
I0523 05:46:19.339259 30336 solver.cpp:237] Iteration 131100, loss = 1.15856
I0523 05:46:19.339295 30336 solver.cpp:253]     Train net output #0: loss = 1.15856 (* 1 = 1.15856 loss)
I0523 05:46:19.339311 30336 sgd_solver.cpp:106] Iteration 131100, lr = 0.0015
I0523 05:46:28.629878 30336 solver.cpp:237] Iteration 131400, loss = 1.2289
I0523 05:46:28.630077 30336 solver.cpp:253]     Train net output #0: loss = 1.2289 (* 1 = 1.2289 loss)
I0523 05:46:28.630092 30336 sgd_solver.cpp:106] Iteration 131400, lr = 0.0015
I0523 05:46:37.917976 30336 solver.cpp:237] Iteration 131700, loss = 0.885057
I0523 05:46:37.918011 30336 solver.cpp:253]     Train net output #0: loss = 0.885057 (* 1 = 0.885057 loss)
I0523 05:46:37.918030 30336 sgd_solver.cpp:106] Iteration 131700, lr = 0.0015
I0523 05:46:47.175895 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_132000.caffemodel
I0523 05:46:47.234860 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_132000.solverstate
I0523 05:46:47.260021 30336 solver.cpp:341] Iteration 132000, Testing net (#0)
I0523 05:47:55.853785 30336 solver.cpp:409]     Test net output #0: accuracy = 0.890112
I0523 05:47:55.854001 30336 solver.cpp:409]     Test net output #1: loss = 0.376757 (* 1 = 0.376757 loss)
I0523 05:48:16.699868 30336 solver.cpp:237] Iteration 132000, loss = 1.12599
I0523 05:48:16.699925 30336 solver.cpp:253]     Train net output #0: loss = 1.12599 (* 1 = 1.12599 loss)
I0523 05:48:16.699941 30336 sgd_solver.cpp:106] Iteration 132000, lr = 0.0015
I0523 05:48:25.989848 30336 solver.cpp:237] Iteration 132300, loss = 0.905276
I0523 05:48:25.990036 30336 solver.cpp:253]     Train net output #0: loss = 0.905276 (* 1 = 0.905276 loss)
I0523 05:48:25.990052 30336 sgd_solver.cpp:106] Iteration 132300, lr = 0.0015
I0523 05:48:35.280643 30336 solver.cpp:237] Iteration 132600, loss = 1.19668
I0523 05:48:35.280688 30336 solver.cpp:253]     Train net output #0: loss = 1.19668 (* 1 = 1.19668 loss)
I0523 05:48:35.280704 30336 sgd_solver.cpp:106] Iteration 132600, lr = 0.0015
I0523 05:48:44.572573 30336 solver.cpp:237] Iteration 132900, loss = 1.10081
I0523 05:48:44.572610 30336 solver.cpp:253]     Train net output #0: loss = 1.10081 (* 1 = 1.10081 loss)
I0523 05:48:44.572628 30336 sgd_solver.cpp:106] Iteration 132900, lr = 0.0015
I0523 05:48:53.866560 30336 solver.cpp:237] Iteration 133200, loss = 1.00492
I0523 05:48:53.866603 30336 solver.cpp:253]     Train net output #0: loss = 1.00492 (* 1 = 1.00492 loss)
I0523 05:48:53.866624 30336 sgd_solver.cpp:106] Iteration 133200, lr = 0.0015
I0523 05:49:03.156247 30336 solver.cpp:237] Iteration 133500, loss = 1.32305
I0523 05:49:03.156430 30336 solver.cpp:253]     Train net output #0: loss = 1.32305 (* 1 = 1.32305 loss)
I0523 05:49:03.156443 30336 sgd_solver.cpp:106] Iteration 133500, lr = 0.0015
I0523 05:49:12.447976 30336 solver.cpp:237] Iteration 133800, loss = 1.32179
I0523 05:49:12.448010 30336 solver.cpp:253]     Train net output #0: loss = 1.32179 (* 1 = 1.32179 loss)
I0523 05:49:12.448029 30336 sgd_solver.cpp:106] Iteration 133800, lr = 0.0015
I0523 05:49:42.556835 30336 solver.cpp:237] Iteration 134100, loss = 1.17098
I0523 05:49:42.557032 30336 solver.cpp:253]     Train net output #0: loss = 1.17098 (* 1 = 1.17098 loss)
I0523 05:49:42.557047 30336 sgd_solver.cpp:106] Iteration 134100, lr = 0.0015
I0523 05:49:51.848417 30336 solver.cpp:237] Iteration 134400, loss = 0.944896
I0523 05:49:51.848451 30336 solver.cpp:253]     Train net output #0: loss = 0.944896 (* 1 = 0.944896 loss)
I0523 05:49:51.848470 30336 sgd_solver.cpp:106] Iteration 134400, lr = 0.0015
I0523 05:50:01.136481 30336 solver.cpp:237] Iteration 134700, loss = 1.11319
I0523 05:50:01.136515 30336 solver.cpp:253]     Train net output #0: loss = 1.11319 (* 1 = 1.11319 loss)
I0523 05:50:01.136533 30336 sgd_solver.cpp:106] Iteration 134700, lr = 0.0015
I0523 05:50:10.398358 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_135000.caffemodel
I0523 05:50:10.459288 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_135000.solverstate
I0523 05:50:10.496184 30336 solver.cpp:237] Iteration 135000, loss = 1.28918
I0523 05:50:10.496237 30336 solver.cpp:253]     Train net output #0: loss = 1.28918 (* 1 = 1.28918 loss)
I0523 05:50:10.496253 30336 sgd_solver.cpp:106] Iteration 135000, lr = 0.0015
I0523 05:50:19.789283 30336 solver.cpp:237] Iteration 135300, loss = 1.03237
I0523 05:50:19.789469 30336 solver.cpp:253]     Train net output #0: loss = 1.03237 (* 1 = 1.03237 loss)
I0523 05:50:19.789484 30336 sgd_solver.cpp:106] Iteration 135300, lr = 0.0015
I0523 05:50:29.080801 30336 solver.cpp:237] Iteration 135600, loss = 1.36956
I0523 05:50:29.080837 30336 solver.cpp:253]     Train net output #0: loss = 1.36956 (* 1 = 1.36956 loss)
I0523 05:50:29.080852 30336 sgd_solver.cpp:106] Iteration 135600, lr = 0.0015
I0523 05:50:38.372735 30336 solver.cpp:237] Iteration 135900, loss = 1.14363
I0523 05:50:38.372786 30336 solver.cpp:253]     Train net output #0: loss = 1.14363 (* 1 = 1.14363 loss)
I0523 05:50:38.372804 30336 sgd_solver.cpp:106] Iteration 135900, lr = 0.0015
I0523 05:51:08.504706 30336 solver.cpp:237] Iteration 136200, loss = 1.26939
I0523 05:51:08.504925 30336 solver.cpp:253]     Train net output #0: loss = 1.26939 (* 1 = 1.26939 loss)
I0523 05:51:08.504940 30336 sgd_solver.cpp:106] Iteration 136200, lr = 0.0015
I0523 05:51:17.799937 30336 solver.cpp:237] Iteration 136500, loss = 1.27935
I0523 05:51:17.799979 30336 solver.cpp:253]     Train net output #0: loss = 1.27935 (* 1 = 1.27935 loss)
I0523 05:51:17.799993 30336 sgd_solver.cpp:106] Iteration 136500, lr = 0.0015
I0523 05:51:27.089205 30336 solver.cpp:237] Iteration 136800, loss = 1.06065
I0523 05:51:27.089256 30336 solver.cpp:253]     Train net output #0: loss = 1.06065 (* 1 = 1.06065 loss)
I0523 05:51:27.089274 30336 sgd_solver.cpp:106] Iteration 136800, lr = 0.0015
I0523 05:51:36.382114 30336 solver.cpp:237] Iteration 137100, loss = 1.16849
I0523 05:51:36.382150 30336 solver.cpp:253]     Train net output #0: loss = 1.16849 (* 1 = 1.16849 loss)
I0523 05:51:36.382166 30336 sgd_solver.cpp:106] Iteration 137100, lr = 0.0015
I0523 05:51:45.675690 30336 solver.cpp:237] Iteration 137400, loss = 1.14029
I0523 05:51:45.675873 30336 solver.cpp:253]     Train net output #0: loss = 1.14029 (* 1 = 1.14029 loss)
I0523 05:51:45.675886 30336 sgd_solver.cpp:106] Iteration 137400, lr = 0.0015
I0523 05:51:54.966213 30336 solver.cpp:237] Iteration 137700, loss = 0.984681
I0523 05:51:54.966264 30336 solver.cpp:253]     Train net output #0: loss = 0.984681 (* 1 = 0.984681 loss)
I0523 05:51:54.966279 30336 sgd_solver.cpp:106] Iteration 137700, lr = 0.0015
I0523 05:52:04.227926 30336 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_138000.caffemodel
I0523 05:52:04.287160 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0015_2016-05-20T15.49.05.579326_iter_138000.solverstate
I0523 05:52:04.312350 30336 solver.cpp:341] Iteration 138000, Testing net (#0)
I0523 05:52:51.770808 30336 solver.cpp:409]     Test net output #0: accuracy = 0.889785
I0523 05:52:51.771013 30336 solver.cpp:409]     Test net output #1: loss = 0.348536 (* 1 = 0.348536 loss)
I0523 05:53:12.589556 30336 solver.cpp:237] Iteration 138000, loss = 0.977511
I0523 05:53:12.589614 30336 solver.cpp:253]     Train net output #0: loss = 0.977511 (* 1 = 0.977511 loss)
I0523 05:53:12.589629 30336 sgd_solver.cpp:106] Iteration 138000, lr = 0.0015
I0523 05:53:21.877491 30336 solver.cpp:237] Iteration 138300, loss = 1.1901
I0523 05:53:21.877679 30336 solver.cpp:253]     Train net output #0: loss = 1.1901 (* 1 = 1.1901 loss)
I0523 05:53:21.877692 30336 sgd_solver.cpp:106] Iteration 138300, lr = 0.0015
I0523 05:53:31.165654 30336 solver.cpp:237] Iteration 138600, loss = 1.36652
I0523 05:53:31.165701 30336 solver.cpp:253]     Train net output #0: loss = 1.36652 (* 1 = 1.36652 loss)
I0523 05:53:31.165717 30336 sgd_solver.cpp:106] Iteration 138600, lr = 0.0015
I0523 05:53:40.450392 30336 solver.cpp:237] Iteration 138900, loss = 1.37282
I0523 05:53:40.450428 30336 solver.cpp:253]     Train net output #0: loss = 1.37282 (* 1 = 1.37282 loss)
I0523 05:53:40.450445 30336 sgd_solver.cpp:106] Iteration 138900, lr = 0.0015
I0523 05:53:49.738879 30336 solver.cpp:237] Iteration 139200, loss = 1.22791
I0523 05:53:49.738914 30336 solver.cpp:253]     Train net output #0: loss = 1.22791 (* 1 = 1.22791 loss)
I0523 05:53:49.738931 30336 sgd_solver.cpp:106] Iteration 139200, lr = 0.0015
I0523 05:53:59.029597 30336 solver.cpp:237] Iteration 139500, loss = 0.946544
I0523 05:53:59.029786 30336 solver.cpp:253]     Train net output #0: loss = 0.946544 (* 1 = 0.946544 loss)
I0523 05:53:59.029801 30336 sgd_solver.cpp:106] Iteration 139500, lr = 0.0015
aprun: Apid 11252887: Caught signal Terminated, sending to application
*** Aborted at 1463997244 (unix time) try "date -d @1463997244" if you are using GNU date ***
PC: @     0x2aaaaaaca834 ([vdso]+0x833)
aprun: Apid 11252887: Caught signal Terminated, sending to application
*** SIGTERM (@0x767d) received by PID 30336 (TID 0x2aaac746f900) from PID 30333; stack trace: ***
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab7c78850 (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaaaaaca834 ([vdso]+0x833)
    @     0x2aaab82072d0 maybe_syscall_gettime_cpu
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab82074b0 __GI_clock_gettime
=>> PBS: job killed: walltime 7218 exceeded limit 7200
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab9898f3e (unknown)
    @     0x2aaab928ec5b (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab926d723 (unknown)
    @     0x2aaab92655e1 (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab9266356 (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab91d5562 (unknown)
    @     0x2aaab91d56ba (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab91b8715 cuMemcpy
    @     0x2aaaaacf9e92 (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaaaacde306 (unknown)
    @     0x2aaaaad00328 cudaMemcpy
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @           0x4d6a10 caffe::caffe_copy<>()
    @           0x626ea9 caffe::HDF5DataLayer<>::Forward_gpu()
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @           0x5ca109 caffe::Solver<>::Step()
    @           0x5caba5 caffe::Solver<>::Solve()
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @           0x43b3b8 train()
    @           0x43020c main
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @     0x2aaab7ea4c36 __libc_start_main
aprun: Apid 11252887: Caught signal Terminated, sending to application
    @           0x438669 (unknown)
aprun: Apid 11252887: Caught signal Terminated, sending to application
aprun: Apid 11252887: Caught signal Terminated, sending to application
aprun: Apid 11252887: Caught signal Terminated, sending to application
aprun: Apid 11252887: Caught signal Terminated, sending to application
