2811311
I0526 05:00:58.027341 27645 caffe.cpp:184] Using GPUs 0
I0526 05:00:58.452946 27645 solver.cpp:48] Initializing solver from parameters: 
test_iter: 15000
test_interval: 30000
base_lr: 0.0015
display: 1500
max_iter: 1500000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 15000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036.prototxt"
I0526 05:00:58.454972 27645 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036.prototxt
I0526 05:00:58.473477 27645 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0526 05:00:58.473539 27645 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0526 05:00:58.473918 27645 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 10
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0526 05:00:58.474123 27645 layer_factory.hpp:77] Creating layer data_hdf5
I0526 05:00:58.474153 27645 net.cpp:106] Creating Layer data_hdf5
I0526 05:00:58.474177 27645 net.cpp:411] data_hdf5 -> data
I0526 05:00:58.474211 27645 net.cpp:411] data_hdf5 -> label
I0526 05:00:58.474248 27645 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0526 05:00:58.488092 27645 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0526 05:00:58.497967 27645 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0526 05:01:20.151940 27645 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0526 05:01:20.157198 27645 net.cpp:150] Setting up data_hdf5
I0526 05:01:20.157245 27645 net.cpp:157] Top shape: 10 1 127 50 (63500)
I0526 05:01:20.157263 27645 net.cpp:157] Top shape: 10 (10)
I0526 05:01:20.157274 27645 net.cpp:165] Memory required for data: 254040
I0526 05:01:20.157294 27645 layer_factory.hpp:77] Creating layer conv1
I0526 05:01:20.157340 27645 net.cpp:106] Creating Layer conv1
I0526 05:01:20.157363 27645 net.cpp:454] conv1 <- data
I0526 05:01:20.157388 27645 net.cpp:411] conv1 -> conv1
I0526 05:01:21.085604 27645 net.cpp:150] Setting up conv1
I0526 05:01:21.085662 27645 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 05:01:21.085685 27645 net.cpp:165] Memory required for data: 3018840
I0526 05:01:21.085716 27645 layer_factory.hpp:77] Creating layer relu1
I0526 05:01:21.085737 27645 net.cpp:106] Creating Layer relu1
I0526 05:01:21.085757 27645 net.cpp:454] relu1 <- conv1
I0526 05:01:21.085793 27645 net.cpp:397] relu1 -> conv1 (in-place)
I0526 05:01:21.086328 27645 net.cpp:150] Setting up relu1
I0526 05:01:21.086350 27645 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 05:01:21.086364 27645 net.cpp:165] Memory required for data: 5783640
I0526 05:01:21.086380 27645 layer_factory.hpp:77] Creating layer pool1
I0526 05:01:21.086408 27645 net.cpp:106] Creating Layer pool1
I0526 05:01:21.086422 27645 net.cpp:454] pool1 <- conv1
I0526 05:01:21.086439 27645 net.cpp:411] pool1 -> pool1
I0526 05:01:21.086532 27645 net.cpp:150] Setting up pool1
I0526 05:01:21.086552 27645 net.cpp:157] Top shape: 10 12 60 48 (345600)
I0526 05:01:21.086565 27645 net.cpp:165] Memory required for data: 7166040
I0526 05:01:21.086585 27645 layer_factory.hpp:77] Creating layer conv2
I0526 05:01:21.086608 27645 net.cpp:106] Creating Layer conv2
I0526 05:01:21.086622 27645 net.cpp:454] conv2 <- pool1
I0526 05:01:21.086642 27645 net.cpp:411] conv2 -> conv2
I0526 05:01:21.089359 27645 net.cpp:150] Setting up conv2
I0526 05:01:21.089390 27645 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 05:01:21.089406 27645 net.cpp:165] Memory required for data: 9153240
I0526 05:01:21.089429 27645 layer_factory.hpp:77] Creating layer relu2
I0526 05:01:21.089462 27645 net.cpp:106] Creating Layer relu2
I0526 05:01:21.089475 27645 net.cpp:454] relu2 <- conv2
I0526 05:01:21.089493 27645 net.cpp:397] relu2 -> conv2 (in-place)
I0526 05:01:21.089849 27645 net.cpp:150] Setting up relu2
I0526 05:01:21.089869 27645 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 05:01:21.089882 27645 net.cpp:165] Memory required for data: 11140440
I0526 05:01:21.089895 27645 layer_factory.hpp:77] Creating layer pool2
I0526 05:01:21.089915 27645 net.cpp:106] Creating Layer pool2
I0526 05:01:21.089927 27645 net.cpp:454] pool2 <- conv2
I0526 05:01:21.089942 27645 net.cpp:411] pool2 -> pool2
I0526 05:01:21.090046 27645 net.cpp:150] Setting up pool2
I0526 05:01:21.090064 27645 net.cpp:157] Top shape: 10 20 27 46 (248400)
I0526 05:01:21.090078 27645 net.cpp:165] Memory required for data: 12134040
I0526 05:01:21.090097 27645 layer_factory.hpp:77] Creating layer conv3
I0526 05:01:21.090118 27645 net.cpp:106] Creating Layer conv3
I0526 05:01:21.090138 27645 net.cpp:454] conv3 <- pool2
I0526 05:01:21.090154 27645 net.cpp:411] conv3 -> conv3
I0526 05:01:21.092286 27645 net.cpp:150] Setting up conv3
I0526 05:01:21.092313 27645 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 05:01:21.092334 27645 net.cpp:165] Memory required for data: 13218200
I0526 05:01:21.092355 27645 layer_factory.hpp:77] Creating layer relu3
I0526 05:01:21.092378 27645 net.cpp:106] Creating Layer relu3
I0526 05:01:21.092401 27645 net.cpp:454] relu3 <- conv3
I0526 05:01:21.092417 27645 net.cpp:397] relu3 -> conv3 (in-place)
I0526 05:01:21.092898 27645 net.cpp:150] Setting up relu3
I0526 05:01:21.092923 27645 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 05:01:21.092936 27645 net.cpp:165] Memory required for data: 14302360
I0526 05:01:21.092952 27645 layer_factory.hpp:77] Creating layer pool3
I0526 05:01:21.092967 27645 net.cpp:106] Creating Layer pool3
I0526 05:01:21.092990 27645 net.cpp:454] pool3 <- conv3
I0526 05:01:21.093006 27645 net.cpp:411] pool3 -> pool3
I0526 05:01:21.093091 27645 net.cpp:150] Setting up pool3
I0526 05:01:21.093112 27645 net.cpp:157] Top shape: 10 28 11 44 (135520)
I0526 05:01:21.093125 27645 net.cpp:165] Memory required for data: 14844440
I0526 05:01:21.093139 27645 layer_factory.hpp:77] Creating layer conv4
I0526 05:01:21.093165 27645 net.cpp:106] Creating Layer conv4
I0526 05:01:21.093179 27645 net.cpp:454] conv4 <- pool3
I0526 05:01:21.093195 27645 net.cpp:411] conv4 -> conv4
I0526 05:01:21.095934 27645 net.cpp:150] Setting up conv4
I0526 05:01:21.095965 27645 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 05:01:21.095985 27645 net.cpp:165] Memory required for data: 15207320
I0526 05:01:21.096005 27645 layer_factory.hpp:77] Creating layer relu4
I0526 05:01:21.096026 27645 net.cpp:106] Creating Layer relu4
I0526 05:01:21.096050 27645 net.cpp:454] relu4 <- conv4
I0526 05:01:21.096067 27645 net.cpp:397] relu4 -> conv4 (in-place)
I0526 05:01:21.096561 27645 net.cpp:150] Setting up relu4
I0526 05:01:21.096585 27645 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 05:01:21.096598 27645 net.cpp:165] Memory required for data: 15570200
I0526 05:01:21.096614 27645 layer_factory.hpp:77] Creating layer pool4
I0526 05:01:21.096631 27645 net.cpp:106] Creating Layer pool4
I0526 05:01:21.096652 27645 net.cpp:454] pool4 <- conv4
I0526 05:01:21.096669 27645 net.cpp:411] pool4 -> pool4
I0526 05:01:21.096752 27645 net.cpp:150] Setting up pool4
I0526 05:01:21.096774 27645 net.cpp:157] Top shape: 10 36 3 42 (45360)
I0526 05:01:21.096787 27645 net.cpp:165] Memory required for data: 15751640
I0526 05:01:21.096801 27645 layer_factory.hpp:77] Creating layer ip1
I0526 05:01:21.096828 27645 net.cpp:106] Creating Layer ip1
I0526 05:01:21.096843 27645 net.cpp:454] ip1 <- pool4
I0526 05:01:21.096858 27645 net.cpp:411] ip1 -> ip1
I0526 05:01:21.112293 27645 net.cpp:150] Setting up ip1
I0526 05:01:21.112324 27645 net.cpp:157] Top shape: 10 196 (1960)
I0526 05:01:21.112345 27645 net.cpp:165] Memory required for data: 15759480
I0526 05:01:21.112371 27645 layer_factory.hpp:77] Creating layer relu5
I0526 05:01:21.112393 27645 net.cpp:106] Creating Layer relu5
I0526 05:01:21.112417 27645 net.cpp:454] relu5 <- ip1
I0526 05:01:21.112437 27645 net.cpp:397] relu5 -> ip1 (in-place)
I0526 05:01:21.112797 27645 net.cpp:150] Setting up relu5
I0526 05:01:21.112818 27645 net.cpp:157] Top shape: 10 196 (1960)
I0526 05:01:21.112830 27645 net.cpp:165] Memory required for data: 15767320
I0526 05:01:21.112846 27645 layer_factory.hpp:77] Creating layer drop1
I0526 05:01:21.112877 27645 net.cpp:106] Creating Layer drop1
I0526 05:01:21.112891 27645 net.cpp:454] drop1 <- ip1
I0526 05:01:21.112913 27645 net.cpp:397] drop1 -> ip1 (in-place)
I0526 05:01:21.112987 27645 net.cpp:150] Setting up drop1
I0526 05:01:21.113003 27645 net.cpp:157] Top shape: 10 196 (1960)
I0526 05:01:21.113016 27645 net.cpp:165] Memory required for data: 15775160
I0526 05:01:21.113030 27645 layer_factory.hpp:77] Creating layer ip2
I0526 05:01:21.113051 27645 net.cpp:106] Creating Layer ip2
I0526 05:01:21.113065 27645 net.cpp:454] ip2 <- ip1
I0526 05:01:21.113081 27645 net.cpp:411] ip2 -> ip2
I0526 05:01:21.113571 27645 net.cpp:150] Setting up ip2
I0526 05:01:21.113590 27645 net.cpp:157] Top shape: 10 98 (980)
I0526 05:01:21.113602 27645 net.cpp:165] Memory required for data: 15779080
I0526 05:01:21.113623 27645 layer_factory.hpp:77] Creating layer relu6
I0526 05:01:21.113646 27645 net.cpp:106] Creating Layer relu6
I0526 05:01:21.113658 27645 net.cpp:454] relu6 <- ip2
I0526 05:01:21.113673 27645 net.cpp:397] relu6 -> ip2 (in-place)
I0526 05:01:21.114223 27645 net.cpp:150] Setting up relu6
I0526 05:01:21.114245 27645 net.cpp:157] Top shape: 10 98 (980)
I0526 05:01:21.114259 27645 net.cpp:165] Memory required for data: 15783000
I0526 05:01:21.114274 27645 layer_factory.hpp:77] Creating layer drop2
I0526 05:01:21.114298 27645 net.cpp:106] Creating Layer drop2
I0526 05:01:21.114311 27645 net.cpp:454] drop2 <- ip2
I0526 05:01:21.114327 27645 net.cpp:397] drop2 -> ip2 (in-place)
I0526 05:01:21.114384 27645 net.cpp:150] Setting up drop2
I0526 05:01:21.114400 27645 net.cpp:157] Top shape: 10 98 (980)
I0526 05:01:21.114413 27645 net.cpp:165] Memory required for data: 15786920
I0526 05:01:21.114426 27645 layer_factory.hpp:77] Creating layer ip3
I0526 05:01:21.114444 27645 net.cpp:106] Creating Layer ip3
I0526 05:01:21.114456 27645 net.cpp:454] ip3 <- ip2
I0526 05:01:21.114480 27645 net.cpp:411] ip3 -> ip3
I0526 05:01:21.114702 27645 net.cpp:150] Setting up ip3
I0526 05:01:21.114720 27645 net.cpp:157] Top shape: 10 11 (110)
I0526 05:01:21.114733 27645 net.cpp:165] Memory required for data: 15787360
I0526 05:01:21.114753 27645 layer_factory.hpp:77] Creating layer drop3
I0526 05:01:21.114775 27645 net.cpp:106] Creating Layer drop3
I0526 05:01:21.114789 27645 net.cpp:454] drop3 <- ip3
I0526 05:01:21.114804 27645 net.cpp:397] drop3 -> ip3 (in-place)
I0526 05:01:21.114850 27645 net.cpp:150] Setting up drop3
I0526 05:01:21.114872 27645 net.cpp:157] Top shape: 10 11 (110)
I0526 05:01:21.114886 27645 net.cpp:165] Memory required for data: 15787800
I0526 05:01:21.114905 27645 layer_factory.hpp:77] Creating layer loss
I0526 05:01:21.114928 27645 net.cpp:106] Creating Layer loss
I0526 05:01:21.114943 27645 net.cpp:454] loss <- ip3
I0526 05:01:21.114956 27645 net.cpp:454] loss <- label
I0526 05:01:21.114979 27645 net.cpp:411] loss -> loss
I0526 05:01:21.114998 27645 layer_factory.hpp:77] Creating layer loss
I0526 05:01:21.115664 27645 net.cpp:150] Setting up loss
I0526 05:01:21.115686 27645 net.cpp:157] Top shape: (1)
I0526 05:01:21.115702 27645 net.cpp:160]     with loss weight 1
I0526 05:01:21.115762 27645 net.cpp:165] Memory required for data: 15787804
I0526 05:01:21.115777 27645 net.cpp:226] loss needs backward computation.
I0526 05:01:21.115790 27645 net.cpp:226] drop3 needs backward computation.
I0526 05:01:21.115808 27645 net.cpp:226] ip3 needs backward computation.
I0526 05:01:21.115820 27645 net.cpp:226] drop2 needs backward computation.
I0526 05:01:21.115833 27645 net.cpp:226] relu6 needs backward computation.
I0526 05:01:21.115847 27645 net.cpp:226] ip2 needs backward computation.
I0526 05:01:21.115860 27645 net.cpp:226] drop1 needs backward computation.
I0526 05:01:21.115880 27645 net.cpp:226] relu5 needs backward computation.
I0526 05:01:21.115892 27645 net.cpp:226] ip1 needs backward computation.
I0526 05:01:21.115909 27645 net.cpp:226] pool4 needs backward computation.
I0526 05:01:21.115922 27645 net.cpp:226] relu4 needs backward computation.
I0526 05:01:21.115934 27645 net.cpp:226] conv4 needs backward computation.
I0526 05:01:21.115947 27645 net.cpp:226] pool3 needs backward computation.
I0526 05:01:21.115962 27645 net.cpp:226] relu3 needs backward computation.
I0526 05:01:21.115983 27645 net.cpp:226] conv3 needs backward computation.
I0526 05:01:21.116005 27645 net.cpp:226] pool2 needs backward computation.
I0526 05:01:21.116021 27645 net.cpp:226] relu2 needs backward computation.
I0526 05:01:21.116034 27645 net.cpp:226] conv2 needs backward computation.
I0526 05:01:21.116046 27645 net.cpp:226] pool1 needs backward computation.
I0526 05:01:21.116065 27645 net.cpp:226] relu1 needs backward computation.
I0526 05:01:21.116085 27645 net.cpp:226] conv1 needs backward computation.
I0526 05:01:21.116098 27645 net.cpp:228] data_hdf5 does not need backward computation.
I0526 05:01:21.116116 27645 net.cpp:270] This network produces output loss
I0526 05:01:21.116142 27645 net.cpp:283] Network initialization done.
I0526 05:01:21.117835 27645 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036.prototxt
I0526 05:01:21.117914 27645 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0526 05:01:21.118290 27645 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 10
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0526 05:01:21.118515 27645 layer_factory.hpp:77] Creating layer data_hdf5
I0526 05:01:21.118535 27645 net.cpp:106] Creating Layer data_hdf5
I0526 05:01:21.118553 27645 net.cpp:411] data_hdf5 -> data
I0526 05:01:21.118576 27645 net.cpp:411] data_hdf5 -> label
I0526 05:01:21.118593 27645 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0526 05:01:21.119894 27645 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0526 05:01:42.472558 27645 net.cpp:150] Setting up data_hdf5
I0526 05:01:42.472730 27645 net.cpp:157] Top shape: 10 1 127 50 (63500)
I0526 05:01:42.472748 27645 net.cpp:157] Top shape: 10 (10)
I0526 05:01:42.472761 27645 net.cpp:165] Memory required for data: 254040
I0526 05:01:42.472776 27645 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0526 05:01:42.472810 27645 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0526 05:01:42.472823 27645 net.cpp:454] label_data_hdf5_1_split <- label
I0526 05:01:42.472859 27645 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0526 05:01:42.472882 27645 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0526 05:01:42.472970 27645 net.cpp:150] Setting up label_data_hdf5_1_split
I0526 05:01:42.472987 27645 net.cpp:157] Top shape: 10 (10)
I0526 05:01:42.473001 27645 net.cpp:157] Top shape: 10 (10)
I0526 05:01:42.473014 27645 net.cpp:165] Memory required for data: 254120
I0526 05:01:42.473026 27645 layer_factory.hpp:77] Creating layer conv1
I0526 05:01:42.473058 27645 net.cpp:106] Creating Layer conv1
I0526 05:01:42.473073 27645 net.cpp:454] conv1 <- data
I0526 05:01:42.473091 27645 net.cpp:411] conv1 -> conv1
I0526 05:01:42.475054 27645 net.cpp:150] Setting up conv1
I0526 05:01:42.475085 27645 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 05:01:42.475097 27645 net.cpp:165] Memory required for data: 3018920
I0526 05:01:42.475121 27645 layer_factory.hpp:77] Creating layer relu1
I0526 05:01:42.475152 27645 net.cpp:106] Creating Layer relu1
I0526 05:01:42.475165 27645 net.cpp:454] relu1 <- conv1
I0526 05:01:42.475181 27645 net.cpp:397] relu1 -> conv1 (in-place)
I0526 05:01:42.475718 27645 net.cpp:150] Setting up relu1
I0526 05:01:42.475741 27645 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 05:01:42.475755 27645 net.cpp:165] Memory required for data: 5783720
I0526 05:01:42.475767 27645 layer_factory.hpp:77] Creating layer pool1
I0526 05:01:42.475790 27645 net.cpp:106] Creating Layer pool1
I0526 05:01:42.475811 27645 net.cpp:454] pool1 <- conv1
I0526 05:01:42.475828 27645 net.cpp:411] pool1 -> pool1
I0526 05:01:42.475919 27645 net.cpp:150] Setting up pool1
I0526 05:01:42.475935 27645 net.cpp:157] Top shape: 10 12 60 48 (345600)
I0526 05:01:42.475950 27645 net.cpp:165] Memory required for data: 7166120
I0526 05:01:42.475970 27645 layer_factory.hpp:77] Creating layer conv2
I0526 05:01:42.475991 27645 net.cpp:106] Creating Layer conv2
I0526 05:01:42.476011 27645 net.cpp:454] conv2 <- pool1
I0526 05:01:42.476029 27645 net.cpp:411] conv2 -> conv2
I0526 05:01:42.477978 27645 net.cpp:150] Setting up conv2
I0526 05:01:42.478003 27645 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 05:01:42.478024 27645 net.cpp:165] Memory required for data: 9153320
I0526 05:01:42.478046 27645 layer_factory.hpp:77] Creating layer relu2
I0526 05:01:42.478067 27645 net.cpp:106] Creating Layer relu2
I0526 05:01:42.478088 27645 net.cpp:454] relu2 <- conv2
I0526 05:01:42.478106 27645 net.cpp:397] relu2 -> conv2 (in-place)
I0526 05:01:42.478456 27645 net.cpp:150] Setting up relu2
I0526 05:01:42.478476 27645 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 05:01:42.478488 27645 net.cpp:165] Memory required for data: 11140520
I0526 05:01:42.478503 27645 layer_factory.hpp:77] Creating layer pool2
I0526 05:01:42.478525 27645 net.cpp:106] Creating Layer pool2
I0526 05:01:42.478538 27645 net.cpp:454] pool2 <- conv2
I0526 05:01:42.478554 27645 net.cpp:411] pool2 -> pool2
I0526 05:01:42.478644 27645 net.cpp:150] Setting up pool2
I0526 05:01:42.478662 27645 net.cpp:157] Top shape: 10 20 27 46 (248400)
I0526 05:01:42.478677 27645 net.cpp:165] Memory required for data: 12134120
I0526 05:01:42.478689 27645 layer_factory.hpp:77] Creating layer conv3
I0526 05:01:42.478718 27645 net.cpp:106] Creating Layer conv3
I0526 05:01:42.478732 27645 net.cpp:454] conv3 <- pool2
I0526 05:01:42.478749 27645 net.cpp:411] conv3 -> conv3
I0526 05:01:42.480785 27645 net.cpp:150] Setting up conv3
I0526 05:01:42.480811 27645 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 05:01:42.480825 27645 net.cpp:165] Memory required for data: 13218280
I0526 05:01:42.480851 27645 layer_factory.hpp:77] Creating layer relu3
I0526 05:01:42.480890 27645 net.cpp:106] Creating Layer relu3
I0526 05:01:42.480903 27645 net.cpp:454] relu3 <- conv3
I0526 05:01:42.480926 27645 net.cpp:397] relu3 -> conv3 (in-place)
I0526 05:01:42.481422 27645 net.cpp:150] Setting up relu3
I0526 05:01:42.481446 27645 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 05:01:42.481459 27645 net.cpp:165] Memory required for data: 14302440
I0526 05:01:42.481475 27645 layer_factory.hpp:77] Creating layer pool3
I0526 05:01:42.481499 27645 net.cpp:106] Creating Layer pool3
I0526 05:01:42.481513 27645 net.cpp:454] pool3 <- conv3
I0526 05:01:42.481528 27645 net.cpp:411] pool3 -> pool3
I0526 05:01:42.481614 27645 net.cpp:150] Setting up pool3
I0526 05:01:42.481631 27645 net.cpp:157] Top shape: 10 28 11 44 (135520)
I0526 05:01:42.481647 27645 net.cpp:165] Memory required for data: 14844520
I0526 05:01:42.481658 27645 layer_factory.hpp:77] Creating layer conv4
I0526 05:01:42.481685 27645 net.cpp:106] Creating Layer conv4
I0526 05:01:42.481698 27645 net.cpp:454] conv4 <- pool3
I0526 05:01:42.481715 27645 net.cpp:411] conv4 -> conv4
I0526 05:01:42.483798 27645 net.cpp:150] Setting up conv4
I0526 05:01:42.483826 27645 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 05:01:42.483840 27645 net.cpp:165] Memory required for data: 15207400
I0526 05:01:42.483858 27645 layer_factory.hpp:77] Creating layer relu4
I0526 05:01:42.483878 27645 net.cpp:106] Creating Layer relu4
I0526 05:01:42.483901 27645 net.cpp:454] relu4 <- conv4
I0526 05:01:42.483917 27645 net.cpp:397] relu4 -> conv4 (in-place)
I0526 05:01:42.484412 27645 net.cpp:150] Setting up relu4
I0526 05:01:42.484436 27645 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 05:01:42.484448 27645 net.cpp:165] Memory required for data: 15570280
I0526 05:01:42.484464 27645 layer_factory.hpp:77] Creating layer pool4
I0526 05:01:42.484489 27645 net.cpp:106] Creating Layer pool4
I0526 05:01:42.484503 27645 net.cpp:454] pool4 <- conv4
I0526 05:01:42.484519 27645 net.cpp:411] pool4 -> pool4
I0526 05:01:42.484606 27645 net.cpp:150] Setting up pool4
I0526 05:01:42.484622 27645 net.cpp:157] Top shape: 10 36 3 42 (45360)
I0526 05:01:42.484637 27645 net.cpp:165] Memory required for data: 15751720
I0526 05:01:42.484649 27645 layer_factory.hpp:77] Creating layer ip1
I0526 05:01:42.484674 27645 net.cpp:106] Creating Layer ip1
I0526 05:01:42.484688 27645 net.cpp:454] ip1 <- pool4
I0526 05:01:42.484705 27645 net.cpp:411] ip1 -> ip1
I0526 05:01:42.499980 27645 net.cpp:150] Setting up ip1
I0526 05:01:42.500018 27645 net.cpp:157] Top shape: 10 196 (1960)
I0526 05:01:42.500032 27645 net.cpp:165] Memory required for data: 15759560
I0526 05:01:42.500062 27645 layer_factory.hpp:77] Creating layer relu5
I0526 05:01:42.500092 27645 net.cpp:106] Creating Layer relu5
I0526 05:01:42.500105 27645 net.cpp:454] relu5 <- ip1
I0526 05:01:42.500123 27645 net.cpp:397] relu5 -> ip1 (in-place)
I0526 05:01:42.500499 27645 net.cpp:150] Setting up relu5
I0526 05:01:42.500519 27645 net.cpp:157] Top shape: 10 196 (1960)
I0526 05:01:42.500532 27645 net.cpp:165] Memory required for data: 15767400
I0526 05:01:42.500548 27645 layer_factory.hpp:77] Creating layer drop1
I0526 05:01:42.500577 27645 net.cpp:106] Creating Layer drop1
I0526 05:01:42.500591 27645 net.cpp:454] drop1 <- ip1
I0526 05:01:42.500607 27645 net.cpp:397] drop1 -> ip1 (in-place)
I0526 05:01:42.500666 27645 net.cpp:150] Setting up drop1
I0526 05:01:42.500684 27645 net.cpp:157] Top shape: 10 196 (1960)
I0526 05:01:42.500704 27645 net.cpp:165] Memory required for data: 15775240
I0526 05:01:42.500716 27645 layer_factory.hpp:77] Creating layer ip2
I0526 05:01:42.500737 27645 net.cpp:106] Creating Layer ip2
I0526 05:01:42.500749 27645 net.cpp:454] ip2 <- ip1
I0526 05:01:42.500773 27645 net.cpp:411] ip2 -> ip2
I0526 05:01:42.501263 27645 net.cpp:150] Setting up ip2
I0526 05:01:42.501282 27645 net.cpp:157] Top shape: 10 98 (980)
I0526 05:01:42.501296 27645 net.cpp:165] Memory required for data: 15779160
I0526 05:01:42.501317 27645 layer_factory.hpp:77] Creating layer relu6
I0526 05:01:42.501350 27645 net.cpp:106] Creating Layer relu6
I0526 05:01:42.501364 27645 net.cpp:454] relu6 <- ip2
I0526 05:01:42.501379 27645 net.cpp:397] relu6 -> ip2 (in-place)
I0526 05:01:42.501940 27645 net.cpp:150] Setting up relu6
I0526 05:01:42.501963 27645 net.cpp:157] Top shape: 10 98 (980)
I0526 05:01:42.501976 27645 net.cpp:165] Memory required for data: 15783080
I0526 05:01:42.501988 27645 layer_factory.hpp:77] Creating layer drop2
I0526 05:01:42.502008 27645 net.cpp:106] Creating Layer drop2
I0526 05:01:42.502029 27645 net.cpp:454] drop2 <- ip2
I0526 05:01:42.502046 27645 net.cpp:397] drop2 -> ip2 (in-place)
I0526 05:01:42.502097 27645 net.cpp:150] Setting up drop2
I0526 05:01:42.502120 27645 net.cpp:157] Top shape: 10 98 (980)
I0526 05:01:42.502133 27645 net.cpp:165] Memory required for data: 15787000
I0526 05:01:42.502152 27645 layer_factory.hpp:77] Creating layer ip3
I0526 05:01:42.502169 27645 net.cpp:106] Creating Layer ip3
I0526 05:01:42.502182 27645 net.cpp:454] ip3 <- ip2
I0526 05:01:42.502200 27645 net.cpp:411] ip3 -> ip3
I0526 05:01:42.502441 27645 net.cpp:150] Setting up ip3
I0526 05:01:42.502460 27645 net.cpp:157] Top shape: 10 11 (110)
I0526 05:01:42.502473 27645 net.cpp:165] Memory required for data: 15787440
I0526 05:01:42.502495 27645 layer_factory.hpp:77] Creating layer drop3
I0526 05:01:42.502517 27645 net.cpp:106] Creating Layer drop3
I0526 05:01:42.502532 27645 net.cpp:454] drop3 <- ip3
I0526 05:01:42.502547 27645 net.cpp:397] drop3 -> ip3 (in-place)
I0526 05:01:42.502594 27645 net.cpp:150] Setting up drop3
I0526 05:01:42.502610 27645 net.cpp:157] Top shape: 10 11 (110)
I0526 05:01:42.502629 27645 net.cpp:165] Memory required for data: 15787880
I0526 05:01:42.502641 27645 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0526 05:01:42.502660 27645 net.cpp:106] Creating Layer ip3_drop3_0_split
I0526 05:01:42.502672 27645 net.cpp:454] ip3_drop3_0_split <- ip3
I0526 05:01:42.502688 27645 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0526 05:01:42.502713 27645 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0526 05:01:42.502804 27645 net.cpp:150] Setting up ip3_drop3_0_split
I0526 05:01:42.502820 27645 net.cpp:157] Top shape: 10 11 (110)
I0526 05:01:42.502837 27645 net.cpp:157] Top shape: 10 11 (110)
I0526 05:01:42.502849 27645 net.cpp:165] Memory required for data: 15788760
I0526 05:01:42.502862 27645 layer_factory.hpp:77] Creating layer accuracy
I0526 05:01:42.502890 27645 net.cpp:106] Creating Layer accuracy
I0526 05:01:42.502904 27645 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0526 05:01:42.502918 27645 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0526 05:01:42.502938 27645 net.cpp:411] accuracy -> accuracy
I0526 05:01:42.502970 27645 net.cpp:150] Setting up accuracy
I0526 05:01:42.502992 27645 net.cpp:157] Top shape: (1)
I0526 05:01:42.503005 27645 net.cpp:165] Memory required for data: 15788764
I0526 05:01:42.503016 27645 layer_factory.hpp:77] Creating layer loss
I0526 05:01:42.503036 27645 net.cpp:106] Creating Layer loss
I0526 05:01:42.503048 27645 net.cpp:454] loss <- ip3_drop3_0_split_1
I0526 05:01:42.503070 27645 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0526 05:01:42.503089 27645 net.cpp:411] loss -> loss
I0526 05:01:42.503110 27645 layer_factory.hpp:77] Creating layer loss
I0526 05:01:42.503620 27645 net.cpp:150] Setting up loss
I0526 05:01:42.503640 27645 net.cpp:157] Top shape: (1)
I0526 05:01:42.503654 27645 net.cpp:160]     with loss weight 1
I0526 05:01:42.503680 27645 net.cpp:165] Memory required for data: 15788768
I0526 05:01:42.503700 27645 net.cpp:226] loss needs backward computation.
I0526 05:01:42.503716 27645 net.cpp:228] accuracy does not need backward computation.
I0526 05:01:42.503732 27645 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0526 05:01:42.503746 27645 net.cpp:226] drop3 needs backward computation.
I0526 05:01:42.503758 27645 net.cpp:226] ip3 needs backward computation.
I0526 05:01:42.503774 27645 net.cpp:226] drop2 needs backward computation.
I0526 05:01:42.503793 27645 net.cpp:226] relu6 needs backward computation.
I0526 05:01:42.503813 27645 net.cpp:226] ip2 needs backward computation.
I0526 05:01:42.503829 27645 net.cpp:226] drop1 needs backward computation.
I0526 05:01:42.503842 27645 net.cpp:226] relu5 needs backward computation.
I0526 05:01:42.503854 27645 net.cpp:226] ip1 needs backward computation.
I0526 05:01:42.503868 27645 net.cpp:226] pool4 needs backward computation.
I0526 05:01:42.503881 27645 net.cpp:226] relu4 needs backward computation.
I0526 05:01:42.503901 27645 net.cpp:226] conv4 needs backward computation.
I0526 05:01:42.503914 27645 net.cpp:226] pool3 needs backward computation.
I0526 05:01:42.503931 27645 net.cpp:226] relu3 needs backward computation.
I0526 05:01:42.503943 27645 net.cpp:226] conv3 needs backward computation.
I0526 05:01:42.503957 27645 net.cpp:226] pool2 needs backward computation.
I0526 05:01:42.503971 27645 net.cpp:226] relu2 needs backward computation.
I0526 05:01:42.503983 27645 net.cpp:226] conv2 needs backward computation.
I0526 05:01:42.504004 27645 net.cpp:226] pool1 needs backward computation.
I0526 05:01:42.504017 27645 net.cpp:226] relu1 needs backward computation.
I0526 05:01:42.504034 27645 net.cpp:226] conv1 needs backward computation.
I0526 05:01:42.504047 27645 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0526 05:01:42.504060 27645 net.cpp:228] data_hdf5 does not need backward computation.
I0526 05:01:42.504076 27645 net.cpp:270] This network produces output accuracy
I0526 05:01:42.504088 27645 net.cpp:270] This network produces output loss
I0526 05:01:42.504119 27645 net.cpp:283] Network initialization done.
I0526 05:01:42.504271 27645 solver.cpp:60] Solver scaffolding done.
I0526 05:01:42.505409 27645 caffe.cpp:202] Resuming from /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_450000.solverstate
I0526 05:01:42.730927 27645 sgd_solver.cpp:318] SGDSolver: restoring history
I0526 05:01:42.736351 27645 caffe.cpp:212] Starting Optimization
I0526 05:01:42.736394 27645 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0526 05:01:42.736416 27645 solver.cpp:289] Learning Rate Policy: fixed
I0526 05:01:42.737651 27645 solver.cpp:341] Iteration 450000, Testing net (#0)
I0526 05:02:43.360026 27645 solver.cpp:409]     Test net output #0: accuracy = 0.897884
I0526 05:02:43.360203 27645 solver.cpp:409]     Test net output #1: loss = 0.318096 (* 1 = 0.318096 loss)
I0526 05:02:43.377938 27645 solver.cpp:237] Iteration 450000, loss = 1.11962
I0526 05:02:43.377977 27645 solver.cpp:253]     Train net output #0: loss = 1.11962 (* 1 = 1.11962 loss)
I0526 05:02:43.377998 27645 sgd_solver.cpp:106] Iteration 450000, lr = 0.0015
I0526 05:03:00.496476 27645 solver.cpp:237] Iteration 451500, loss = 0.912496
I0526 05:03:00.496536 27645 solver.cpp:253]     Train net output #0: loss = 0.912495 (* 1 = 0.912495 loss)
I0526 05:03:00.496553 27645 sgd_solver.cpp:106] Iteration 451500, lr = 0.0015
I0526 05:03:17.502007 27645 solver.cpp:237] Iteration 453000, loss = 1.58037
I0526 05:03:17.502177 27645 solver.cpp:253]     Train net output #0: loss = 1.58037 (* 1 = 1.58037 loss)
I0526 05:03:17.502195 27645 sgd_solver.cpp:106] Iteration 453000, lr = 0.0015
I0526 05:03:34.282095 27645 solver.cpp:237] Iteration 454500, loss = 0.991783
I0526 05:03:34.282135 27645 solver.cpp:253]     Train net output #0: loss = 0.991782 (* 1 = 0.991782 loss)
I0526 05:03:34.282152 27645 sgd_solver.cpp:106] Iteration 454500, lr = 0.0015
I0526 05:03:51.127784 27645 solver.cpp:237] Iteration 456000, loss = 1.3081
I0526 05:03:51.127957 27645 solver.cpp:253]     Train net output #0: loss = 1.3081 (* 1 = 1.3081 loss)
I0526 05:03:51.127974 27645 sgd_solver.cpp:106] Iteration 456000, lr = 0.0015
I0526 05:04:07.932653 27645 solver.cpp:237] Iteration 457500, loss = 1.11244
I0526 05:04:07.932710 27645 solver.cpp:253]     Train net output #0: loss = 1.11244 (* 1 = 1.11244 loss)
I0526 05:04:07.932736 27645 sgd_solver.cpp:106] Iteration 457500, lr = 0.0015
I0526 05:04:24.545270 27645 solver.cpp:237] Iteration 459000, loss = 1.65423
I0526 05:04:24.545413 27645 solver.cpp:253]     Train net output #0: loss = 1.65423 (* 1 = 1.65423 loss)
I0526 05:04:24.545428 27645 sgd_solver.cpp:106] Iteration 459000, lr = 0.0015
I0526 05:05:03.744969 27645 solver.cpp:237] Iteration 460500, loss = 1.24888
I0526 05:05:03.745141 27645 solver.cpp:253]     Train net output #0: loss = 1.24888 (* 1 = 1.24888 loss)
I0526 05:05:03.745164 27645 sgd_solver.cpp:106] Iteration 460500, lr = 0.0015
I0526 05:05:20.684711 27645 solver.cpp:237] Iteration 462000, loss = 1.33405
I0526 05:05:20.684767 27645 solver.cpp:253]     Train net output #0: loss = 1.33405 (* 1 = 1.33405 loss)
I0526 05:05:20.684784 27645 sgd_solver.cpp:106] Iteration 462000, lr = 0.0015
I0526 05:05:37.334983 27645 solver.cpp:237] Iteration 463500, loss = 1.0177
I0526 05:05:37.335129 27645 solver.cpp:253]     Train net output #0: loss = 1.0177 (* 1 = 1.0177 loss)
I0526 05:05:37.335145 27645 sgd_solver.cpp:106] Iteration 463500, lr = 0.0015
I0526 05:05:54.414433 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_465000.caffemodel
I0526 05:05:54.463435 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_465000.solverstate
I0526 05:05:54.494344 27645 solver.cpp:237] Iteration 465000, loss = 1.377
I0526 05:05:54.494410 27645 solver.cpp:253]     Train net output #0: loss = 1.37699 (* 1 = 1.37699 loss)
I0526 05:05:54.494426 27645 sgd_solver.cpp:106] Iteration 465000, lr = 0.0015
I0526 05:06:11.572752 27645 solver.cpp:237] Iteration 466500, loss = 0.792391
I0526 05:06:11.572921 27645 solver.cpp:253]     Train net output #0: loss = 0.792389 (* 1 = 0.792389 loss)
I0526 05:06:11.572942 27645 sgd_solver.cpp:106] Iteration 466500, lr = 0.0015
I0526 05:06:28.232067 27645 solver.cpp:237] Iteration 468000, loss = 1.09143
I0526 05:06:28.232110 27645 solver.cpp:253]     Train net output #0: loss = 1.09143 (* 1 = 1.09143 loss)
I0526 05:06:28.232133 27645 sgd_solver.cpp:106] Iteration 468000, lr = 0.0015
I0526 05:06:45.141016 27645 solver.cpp:237] Iteration 469500, loss = 0.780144
I0526 05:06:45.141177 27645 solver.cpp:253]     Train net output #0: loss = 0.780143 (* 1 = 0.780143 loss)
I0526 05:06:45.141194 27645 sgd_solver.cpp:106] Iteration 469500, lr = 0.0015
I0526 05:07:24.258507 27645 solver.cpp:237] Iteration 471000, loss = 0.548005
I0526 05:07:24.258692 27645 solver.cpp:253]     Train net output #0: loss = 0.548004 (* 1 = 0.548004 loss)
I0526 05:07:24.258713 27645 sgd_solver.cpp:106] Iteration 471000, lr = 0.0015
I0526 05:07:41.318367 27645 solver.cpp:237] Iteration 472500, loss = 0.958468
I0526 05:07:41.318406 27645 solver.cpp:253]     Train net output #0: loss = 0.958468 (* 1 = 0.958468 loss)
I0526 05:07:41.318429 27645 sgd_solver.cpp:106] Iteration 472500, lr = 0.0015
I0526 05:07:58.097683 27645 solver.cpp:237] Iteration 474000, loss = 1.00039
I0526 05:07:58.097847 27645 solver.cpp:253]     Train net output #0: loss = 1.00039 (* 1 = 1.00039 loss)
I0526 05:07:58.097864 27645 sgd_solver.cpp:106] Iteration 474000, lr = 0.0015
I0526 05:08:14.727386 27645 solver.cpp:237] Iteration 475500, loss = 0.842764
I0526 05:08:14.727447 27645 solver.cpp:253]     Train net output #0: loss = 0.842764 (* 1 = 0.842764 loss)
I0526 05:08:14.727471 27645 sgd_solver.cpp:106] Iteration 475500, lr = 0.0015
I0526 05:08:31.364245 27645 solver.cpp:237] Iteration 477000, loss = 1.48235
I0526 05:08:31.364392 27645 solver.cpp:253]     Train net output #0: loss = 1.48235 (* 1 = 1.48235 loss)
I0526 05:08:31.364408 27645 sgd_solver.cpp:106] Iteration 477000, lr = 0.0015
I0526 05:08:48.130794 27645 solver.cpp:237] Iteration 478500, loss = 1.34287
I0526 05:08:48.130851 27645 solver.cpp:253]     Train net output #0: loss = 1.34287 (* 1 = 1.34287 loss)
I0526 05:08:48.130869 27645 sgd_solver.cpp:106] Iteration 478500, lr = 0.0015
I0526 05:09:04.966423 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_480000.caffemodel
I0526 05:09:05.015086 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_480000.solverstate
I0526 05:09:05.043874 27645 solver.cpp:341] Iteration 480000, Testing net (#0)
I0526 05:10:04.682952 27645 solver.cpp:409]     Test net output #0: accuracy = 0.89795
I0526 05:10:04.683116 27645 solver.cpp:409]     Test net output #1: loss = 0.330897 (* 1 = 0.330897 loss)
I0526 05:10:26.838066 27645 solver.cpp:237] Iteration 480000, loss = 1.31466
I0526 05:10:26.838130 27645 solver.cpp:253]     Train net output #0: loss = 1.31466 (* 1 = 1.31466 loss)
I0526 05:10:26.838161 27645 sgd_solver.cpp:106] Iteration 480000, lr = 0.0015
I0526 05:10:43.703460 27645 solver.cpp:237] Iteration 481500, loss = 1.14636
I0526 05:10:43.703627 27645 solver.cpp:253]     Train net output #0: loss = 1.14636 (* 1 = 1.14636 loss)
I0526 05:10:43.703647 27645 sgd_solver.cpp:106] Iteration 481500, lr = 0.0015
I0526 05:11:00.503219 27645 solver.cpp:237] Iteration 483000, loss = 0.693573
I0526 05:11:00.503259 27645 solver.cpp:253]     Train net output #0: loss = 0.693572 (* 1 = 0.693572 loss)
I0526 05:11:00.503278 27645 sgd_solver.cpp:106] Iteration 483000, lr = 0.0015
I0526 05:11:17.563293 27645 solver.cpp:237] Iteration 484500, loss = 1.42024
I0526 05:11:17.563453 27645 solver.cpp:253]     Train net output #0: loss = 1.42024 (* 1 = 1.42024 loss)
I0526 05:11:17.563472 27645 sgd_solver.cpp:106] Iteration 484500, lr = 0.0015
I0526 05:11:34.712215 27645 solver.cpp:237] Iteration 486000, loss = 1.36424
I0526 05:11:34.712272 27645 solver.cpp:253]     Train net output #0: loss = 1.36424 (* 1 = 1.36424 loss)
I0526 05:11:34.712298 27645 sgd_solver.cpp:106] Iteration 486000, lr = 0.0015
I0526 05:11:51.495893 27645 solver.cpp:237] Iteration 487500, loss = 1.29838
I0526 05:11:51.496054 27645 solver.cpp:253]     Train net output #0: loss = 1.29838 (* 1 = 1.29838 loss)
I0526 05:11:51.496070 27645 sgd_solver.cpp:106] Iteration 487500, lr = 0.0015
I0526 05:12:08.568500 27645 solver.cpp:237] Iteration 489000, loss = 1.52942
I0526 05:12:08.568557 27645 solver.cpp:253]     Train net output #0: loss = 1.52941 (* 1 = 1.52941 loss)
I0526 05:12:08.568573 27645 sgd_solver.cpp:106] Iteration 489000, lr = 0.0015
I0526 05:12:47.980170 27645 solver.cpp:237] Iteration 490500, loss = 1.19924
I0526 05:12:47.980353 27645 solver.cpp:253]     Train net output #0: loss = 1.19924 (* 1 = 1.19924 loss)
I0526 05:12:47.980372 27645 sgd_solver.cpp:106] Iteration 490500, lr = 0.0015
I0526 05:13:04.897248 27645 solver.cpp:237] Iteration 492000, loss = 1.06579
I0526 05:13:04.897286 27645 solver.cpp:253]     Train net output #0: loss = 1.06579 (* 1 = 1.06579 loss)
I0526 05:13:04.897303 27645 sgd_solver.cpp:106] Iteration 492000, lr = 0.0015
I0526 05:13:21.783164 27645 solver.cpp:237] Iteration 493500, loss = 1.81008
I0526 05:13:21.783318 27645 solver.cpp:253]     Train net output #0: loss = 1.81007 (* 1 = 1.81007 loss)
I0526 05:13:21.783336 27645 sgd_solver.cpp:106] Iteration 493500, lr = 0.0015
I0526 05:13:38.586482 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_495000.caffemodel
I0526 05:13:38.634753 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_495000.solverstate
I0526 05:13:38.666280 27645 solver.cpp:237] Iteration 495000, loss = 1.02538
I0526 05:13:38.666338 27645 solver.cpp:253]     Train net output #0: loss = 1.02538 (* 1 = 1.02538 loss)
I0526 05:13:38.666366 27645 sgd_solver.cpp:106] Iteration 495000, lr = 0.0015
I0526 05:13:55.861014 27645 solver.cpp:237] Iteration 496500, loss = 1.32256
I0526 05:13:55.861165 27645 solver.cpp:253]     Train net output #0: loss = 1.32255 (* 1 = 1.32255 loss)
I0526 05:13:55.861181 27645 sgd_solver.cpp:106] Iteration 496500, lr = 0.0015
I0526 05:14:12.974963 27645 solver.cpp:237] Iteration 498000, loss = 1.2285
I0526 05:14:12.975020 27645 solver.cpp:253]     Train net output #0: loss = 1.2285 (* 1 = 1.2285 loss)
I0526 05:14:12.975045 27645 sgd_solver.cpp:106] Iteration 498000, lr = 0.0015
I0526 05:14:29.999663 27645 solver.cpp:237] Iteration 499500, loss = 1.09606
I0526 05:14:29.999828 27645 solver.cpp:253]     Train net output #0: loss = 1.09606 (* 1 = 1.09606 loss)
I0526 05:14:29.999847 27645 sgd_solver.cpp:106] Iteration 499500, lr = 0.0015
I0526 05:15:08.945371 27645 solver.cpp:237] Iteration 501000, loss = 1.3584
I0526 05:15:08.945543 27645 solver.cpp:253]     Train net output #0: loss = 1.3584 (* 1 = 1.3584 loss)
I0526 05:15:08.945559 27645 sgd_solver.cpp:106] Iteration 501000, lr = 0.0015
I0526 05:15:25.673151 27645 solver.cpp:237] Iteration 502500, loss = 1.01487
I0526 05:15:25.673204 27645 solver.cpp:253]     Train net output #0: loss = 1.01487 (* 1 = 1.01487 loss)
I0526 05:15:25.673223 27645 sgd_solver.cpp:106] Iteration 502500, lr = 0.0015
I0526 05:15:42.319329 27645 solver.cpp:237] Iteration 504000, loss = 1.42921
I0526 05:15:42.319494 27645 solver.cpp:253]     Train net output #0: loss = 1.42921 (* 1 = 1.42921 loss)
I0526 05:15:42.319511 27645 sgd_solver.cpp:106] Iteration 504000, lr = 0.0015
I0526 05:15:59.393375 27645 solver.cpp:237] Iteration 505500, loss = 1.15807
I0526 05:15:59.393414 27645 solver.cpp:253]     Train net output #0: loss = 1.15807 (* 1 = 1.15807 loss)
I0526 05:15:59.393437 27645 sgd_solver.cpp:106] Iteration 505500, lr = 0.0015
I0526 05:16:16.424299 27645 solver.cpp:237] Iteration 507000, loss = 1.09382
I0526 05:16:16.424471 27645 solver.cpp:253]     Train net output #0: loss = 1.09382 (* 1 = 1.09382 loss)
I0526 05:16:16.424489 27645 sgd_solver.cpp:106] Iteration 507000, lr = 0.0015
I0526 05:16:33.374168 27645 solver.cpp:237] Iteration 508500, loss = 0.62922
I0526 05:16:33.374219 27645 solver.cpp:253]     Train net output #0: loss = 0.629218 (* 1 = 0.629218 loss)
I0526 05:16:33.374236 27645 sgd_solver.cpp:106] Iteration 508500, lr = 0.0015
I0526 05:16:50.306470 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_510000.caffemodel
I0526 05:16:50.355588 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_510000.solverstate
I0526 05:16:50.383234 27645 solver.cpp:341] Iteration 510000, Testing net (#0)
I0526 05:18:10.637984 27645 solver.cpp:409]     Test net output #0: accuracy = 0.897665
I0526 05:18:10.638161 27645 solver.cpp:409]     Test net output #1: loss = 0.339099 (* 1 = 0.339099 loss)
I0526 05:18:32.807693 27645 solver.cpp:237] Iteration 510000, loss = 0.881584
I0526 05:18:32.807760 27645 solver.cpp:253]     Train net output #0: loss = 0.881582 (* 1 = 0.881582 loss)
I0526 05:18:32.807788 27645 sgd_solver.cpp:106] Iteration 510000, lr = 0.0015
I0526 05:18:49.479780 27645 solver.cpp:237] Iteration 511500, loss = 0.761021
I0526 05:18:49.479944 27645 solver.cpp:253]     Train net output #0: loss = 0.76102 (* 1 = 0.76102 loss)
I0526 05:18:49.479962 27645 sgd_solver.cpp:106] Iteration 511500, lr = 0.0015
I0526 05:19:06.135984 27645 solver.cpp:237] Iteration 513000, loss = 0.741027
I0526 05:19:06.136039 27645 solver.cpp:253]     Train net output #0: loss = 0.741025 (* 1 = 0.741025 loss)
I0526 05:19:06.136065 27645 sgd_solver.cpp:106] Iteration 513000, lr = 0.0015
I0526 05:19:22.768350 27645 solver.cpp:237] Iteration 514500, loss = 0.734083
I0526 05:19:22.768471 27645 solver.cpp:253]     Train net output #0: loss = 0.734081 (* 1 = 0.734081 loss)
I0526 05:19:22.768489 27645 sgd_solver.cpp:106] Iteration 514500, lr = 0.0015
I0526 05:19:39.745945 27645 solver.cpp:237] Iteration 516000, loss = 0.82482
I0526 05:19:39.745996 27645 solver.cpp:253]     Train net output #0: loss = 0.824818 (* 1 = 0.824818 loss)
I0526 05:19:39.746014 27645 sgd_solver.cpp:106] Iteration 516000, lr = 0.0015
I0526 05:19:56.841820 27645 solver.cpp:237] Iteration 517500, loss = 1.12369
I0526 05:19:56.841980 27645 solver.cpp:253]     Train net output #0: loss = 1.12369 (* 1 = 1.12369 loss)
I0526 05:19:56.841998 27645 sgd_solver.cpp:106] Iteration 517500, lr = 0.0015
I0526 05:20:14.040338 27645 solver.cpp:237] Iteration 519000, loss = 0.737659
I0526 05:20:14.040379 27645 solver.cpp:253]     Train net output #0: loss = 0.737656 (* 1 = 0.737656 loss)
I0526 05:20:14.040395 27645 sgd_solver.cpp:106] Iteration 519000, lr = 0.0015
I0526 05:20:53.303937 27645 solver.cpp:237] Iteration 520500, loss = 1.10284
I0526 05:20:53.304112 27645 solver.cpp:253]     Train net output #0: loss = 1.10283 (* 1 = 1.10283 loss)
I0526 05:20:53.304131 27645 sgd_solver.cpp:106] Iteration 520500, lr = 0.0015
I0526 05:21:10.378653 27645 solver.cpp:237] Iteration 522000, loss = 0.739401
I0526 05:21:10.378711 27645 solver.cpp:253]     Train net output #0: loss = 0.739398 (* 1 = 0.739398 loss)
I0526 05:21:10.378737 27645 sgd_solver.cpp:106] Iteration 522000, lr = 0.0015
I0526 05:21:27.436735 27645 solver.cpp:237] Iteration 523500, loss = 1.00596
I0526 05:21:27.436882 27645 solver.cpp:253]     Train net output #0: loss = 1.00596 (* 1 = 1.00596 loss)
I0526 05:21:27.436898 27645 sgd_solver.cpp:106] Iteration 523500, lr = 0.0015
I0526 05:21:44.170943 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_525000.caffemodel
I0526 05:21:44.219516 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_525000.solverstate
I0526 05:21:44.250995 27645 solver.cpp:237] Iteration 525000, loss = 1.43649
I0526 05:21:44.251055 27645 solver.cpp:253]     Train net output #0: loss = 1.43649 (* 1 = 1.43649 loss)
I0526 05:21:44.251076 27645 sgd_solver.cpp:106] Iteration 525000, lr = 0.0015
I0526 05:22:01.008860 27645 solver.cpp:237] Iteration 526500, loss = 1.91988
I0526 05:22:01.009029 27645 solver.cpp:253]     Train net output #0: loss = 1.91987 (* 1 = 1.91987 loss)
I0526 05:22:01.009049 27645 sgd_solver.cpp:106] Iteration 526500, lr = 0.0015
I0526 05:22:18.085608 27645 solver.cpp:237] Iteration 528000, loss = 1.39163
I0526 05:22:18.085647 27645 solver.cpp:253]     Train net output #0: loss = 1.39162 (* 1 = 1.39162 loss)
I0526 05:22:18.085672 27645 sgd_solver.cpp:106] Iteration 528000, lr = 0.0015
I0526 05:22:35.083833 27645 solver.cpp:237] Iteration 529500, loss = 0.444052
I0526 05:22:35.084012 27645 solver.cpp:253]     Train net output #0: loss = 0.444049 (* 1 = 0.444049 loss)
I0526 05:22:35.084030 27645 sgd_solver.cpp:106] Iteration 529500, lr = 0.0015
I0526 05:23:14.164384 27645 solver.cpp:237] Iteration 531000, loss = 1.21324
I0526 05:23:14.164564 27645 solver.cpp:253]     Train net output #0: loss = 1.21324 (* 1 = 1.21324 loss)
I0526 05:23:14.164583 27645 sgd_solver.cpp:106] Iteration 531000, lr = 0.0015
I0526 05:23:30.788853 27645 solver.cpp:237] Iteration 532500, loss = 1.23674
I0526 05:23:30.788892 27645 solver.cpp:253]     Train net output #0: loss = 1.23674 (* 1 = 1.23674 loss)
I0526 05:23:30.788910 27645 sgd_solver.cpp:106] Iteration 532500, lr = 0.0015
I0526 05:23:47.426625 27645 solver.cpp:237] Iteration 534000, loss = 1.62747
I0526 05:23:47.426785 27645 solver.cpp:253]     Train net output #0: loss = 1.62746 (* 1 = 1.62746 loss)
I0526 05:23:47.426802 27645 sgd_solver.cpp:106] Iteration 534000, lr = 0.0015
I0526 05:24:04.091295 27645 solver.cpp:237] Iteration 535500, loss = 0.473709
I0526 05:24:04.091351 27645 solver.cpp:253]     Train net output #0: loss = 0.473705 (* 1 = 0.473705 loss)
I0526 05:24:04.091368 27645 sgd_solver.cpp:106] Iteration 535500, lr = 0.0015
I0526 05:24:20.956241 27645 solver.cpp:237] Iteration 537000, loss = 0.825032
I0526 05:24:20.956389 27645 solver.cpp:253]     Train net output #0: loss = 0.825028 (* 1 = 0.825028 loss)
I0526 05:24:20.956406 27645 sgd_solver.cpp:106] Iteration 537000, lr = 0.0015
I0526 05:24:38.068414 27645 solver.cpp:237] Iteration 538500, loss = 1.61132
I0526 05:24:38.068469 27645 solver.cpp:253]     Train net output #0: loss = 1.61131 (* 1 = 1.61131 loss)
I0526 05:24:38.068488 27645 sgd_solver.cpp:106] Iteration 538500, lr = 0.0015
I0526 05:24:55.238428 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_540000.caffemodel
I0526 05:24:55.284108 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_540000.solverstate
I0526 05:24:55.309725 27645 solver.cpp:341] Iteration 540000, Testing net (#0)
I0526 05:25:54.515509 27645 solver.cpp:409]     Test net output #0: accuracy = 0.899952
I0526 05:25:54.515676 27645 solver.cpp:409]     Test net output #1: loss = 0.312041 (* 1 = 0.312041 loss)
I0526 05:26:16.655958 27645 solver.cpp:237] Iteration 540000, loss = 0.9177
I0526 05:26:16.656019 27645 solver.cpp:253]     Train net output #0: loss = 0.917696 (* 1 = 0.917696 loss)
I0526 05:26:16.656045 27645 sgd_solver.cpp:106] Iteration 540000, lr = 0.0015
I0526 05:26:33.805680 27645 solver.cpp:237] Iteration 541500, loss = 0.770139
I0526 05:26:33.805855 27645 solver.cpp:253]     Train net output #0: loss = 0.770134 (* 1 = 0.770134 loss)
I0526 05:26:33.805874 27645 sgd_solver.cpp:106] Iteration 541500, lr = 0.0015
I0526 05:26:50.597913 27645 solver.cpp:237] Iteration 543000, loss = 1.36428
I0526 05:26:50.597949 27645 solver.cpp:253]     Train net output #0: loss = 1.36427 (* 1 = 1.36427 loss)
I0526 05:26:50.597972 27645 sgd_solver.cpp:106] Iteration 543000, lr = 0.0015
I0526 05:27:07.645582 27645 solver.cpp:237] Iteration 544500, loss = 1.15599
I0526 05:27:07.645746 27645 solver.cpp:253]     Train net output #0: loss = 1.15598 (* 1 = 1.15598 loss)
I0526 05:27:07.645763 27645 sgd_solver.cpp:106] Iteration 544500, lr = 0.0015
I0526 05:27:24.738590 27645 solver.cpp:237] Iteration 546000, loss = 0.732609
I0526 05:27:24.738646 27645 solver.cpp:253]     Train net output #0: loss = 0.732604 (* 1 = 0.732604 loss)
I0526 05:27:24.738663 27645 sgd_solver.cpp:106] Iteration 546000, lr = 0.0015
I0526 05:27:41.374882 27645 solver.cpp:237] Iteration 547500, loss = 1.18663
I0526 05:27:41.375046 27645 solver.cpp:253]     Train net output #0: loss = 1.18663 (* 1 = 1.18663 loss)
I0526 05:27:41.375061 27645 sgd_solver.cpp:106] Iteration 547500, lr = 0.0015
I0526 05:27:58.088598 27645 solver.cpp:237] Iteration 549000, loss = 1.12693
I0526 05:27:58.088654 27645 solver.cpp:253]     Train net output #0: loss = 1.12693 (* 1 = 1.12693 loss)
I0526 05:27:58.088671 27645 sgd_solver.cpp:106] Iteration 549000, lr = 0.0015
I0526 05:28:37.080272 27645 solver.cpp:237] Iteration 550500, loss = 1.00939
I0526 05:28:37.080451 27645 solver.cpp:253]     Train net output #0: loss = 1.00938 (* 1 = 1.00938 loss)
I0526 05:28:37.080471 27645 sgd_solver.cpp:106] Iteration 550500, lr = 0.0015
I0526 05:28:54.167682 27645 solver.cpp:237] Iteration 552000, loss = 0.878103
I0526 05:28:54.167721 27645 solver.cpp:253]     Train net output #0: loss = 0.878098 (* 1 = 0.878098 loss)
I0526 05:28:54.167740 27645 sgd_solver.cpp:106] Iteration 552000, lr = 0.0015
I0526 05:29:11.097182 27645 solver.cpp:237] Iteration 553500, loss = 1.22199
I0526 05:29:11.097347 27645 solver.cpp:253]     Train net output #0: loss = 1.22199 (* 1 = 1.22199 loss)
I0526 05:29:11.097364 27645 sgd_solver.cpp:106] Iteration 553500, lr = 0.0015
I0526 05:29:27.893970 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_555000.caffemodel
I0526 05:29:27.940615 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_555000.solverstate
I0526 05:29:27.970082 27645 solver.cpp:237] Iteration 555000, loss = 1.66283
I0526 05:29:27.970137 27645 solver.cpp:253]     Train net output #0: loss = 1.66283 (* 1 = 1.66283 loss)
I0526 05:29:27.970163 27645 sgd_solver.cpp:106] Iteration 555000, lr = 0.0015
I0526 05:29:45.018839 27645 solver.cpp:237] Iteration 556500, loss = 0.673653
I0526 05:29:45.018990 27645 solver.cpp:253]     Train net output #0: loss = 0.673648 (* 1 = 0.673648 loss)
I0526 05:29:45.019006 27645 sgd_solver.cpp:106] Iteration 556500, lr = 0.0015
I0526 05:30:01.974205 27645 solver.cpp:237] Iteration 558000, loss = 1.39473
I0526 05:30:01.974259 27645 solver.cpp:253]     Train net output #0: loss = 1.39472 (* 1 = 1.39472 loss)
I0526 05:30:01.974277 27645 sgd_solver.cpp:106] Iteration 558000, lr = 0.0015
I0526 05:30:18.884774 27645 solver.cpp:237] Iteration 559500, loss = 1.45527
I0526 05:30:18.884954 27645 solver.cpp:253]     Train net output #0: loss = 1.45527 (* 1 = 1.45527 loss)
I0526 05:30:18.884973 27645 sgd_solver.cpp:106] Iteration 559500, lr = 0.0015
I0526 05:30:58.227252 27645 solver.cpp:237] Iteration 561000, loss = 1.57271
I0526 05:30:58.227427 27645 solver.cpp:253]     Train net output #0: loss = 1.57271 (* 1 = 1.57271 loss)
I0526 05:30:58.227443 27645 sgd_solver.cpp:106] Iteration 561000, lr = 0.0015
I0526 05:31:15.172662 27645 solver.cpp:237] Iteration 562500, loss = 1.94554
I0526 05:31:15.172719 27645 solver.cpp:253]     Train net output #0: loss = 1.94554 (* 1 = 1.94554 loss)
I0526 05:31:15.172736 27645 sgd_solver.cpp:106] Iteration 562500, lr = 0.0015
I0526 05:31:31.818464 27645 solver.cpp:237] Iteration 564000, loss = 1.25149
I0526 05:31:31.818632 27645 solver.cpp:253]     Train net output #0: loss = 1.25148 (* 1 = 1.25148 loss)
I0526 05:31:31.818650 27645 sgd_solver.cpp:106] Iteration 564000, lr = 0.0015
I0526 05:31:49.062772 27645 solver.cpp:237] Iteration 565500, loss = 1.16371
I0526 05:31:49.062810 27645 solver.cpp:253]     Train net output #0: loss = 1.16371 (* 1 = 1.16371 loss)
I0526 05:31:49.062829 27645 sgd_solver.cpp:106] Iteration 565500, lr = 0.0015
I0526 05:32:05.950084 27645 solver.cpp:237] Iteration 567000, loss = 0.71208
I0526 05:32:05.950261 27645 solver.cpp:253]     Train net output #0: loss = 0.712075 (* 1 = 0.712075 loss)
I0526 05:32:05.950279 27645 sgd_solver.cpp:106] Iteration 567000, lr = 0.0015
I0526 05:32:22.601716 27645 solver.cpp:237] Iteration 568500, loss = 1.41563
I0526 05:32:22.601776 27645 solver.cpp:253]     Train net output #0: loss = 1.41562 (* 1 = 1.41562 loss)
I0526 05:32:22.601802 27645 sgd_solver.cpp:106] Iteration 568500, lr = 0.0015
I0526 05:32:39.527276 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_570000.caffemodel
I0526 05:32:39.574265 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_570000.solverstate
I0526 05:32:39.599901 27645 solver.cpp:341] Iteration 570000, Testing net (#0)
I0526 05:34:00.000654 27645 solver.cpp:409]     Test net output #0: accuracy = 0.899277
I0526 05:34:00.000830 27645 solver.cpp:409]     Test net output #1: loss = 0.322344 (* 1 = 0.322344 loss)
I0526 05:34:22.188722 27645 solver.cpp:237] Iteration 570000, loss = 1.50987
I0526 05:34:22.188789 27645 solver.cpp:253]     Train net output #0: loss = 1.50986 (* 1 = 1.50986 loss)
I0526 05:34:22.188817 27645 sgd_solver.cpp:106] Iteration 570000, lr = 0.0015
I0526 05:34:38.954776 27645 solver.cpp:237] Iteration 571500, loss = 1.31456
I0526 05:34:38.954944 27645 solver.cpp:253]     Train net output #0: loss = 1.31455 (* 1 = 1.31455 loss)
I0526 05:34:38.954962 27645 sgd_solver.cpp:106] Iteration 571500, lr = 0.0015
I0526 05:34:55.805253 27645 solver.cpp:237] Iteration 573000, loss = 0.685697
I0526 05:34:55.805311 27645 solver.cpp:253]     Train net output #0: loss = 0.685691 (* 1 = 0.685691 loss)
I0526 05:34:55.805328 27645 sgd_solver.cpp:106] Iteration 573000, lr = 0.0015
I0526 05:35:12.875062 27645 solver.cpp:237] Iteration 574500, loss = 1.21141
I0526 05:35:12.875216 27645 solver.cpp:253]     Train net output #0: loss = 1.2114 (* 1 = 1.2114 loss)
I0526 05:35:12.875233 27645 sgd_solver.cpp:106] Iteration 574500, lr = 0.0015
I0526 05:35:29.840742 27645 solver.cpp:237] Iteration 576000, loss = 1.14273
I0526 05:35:29.840796 27645 solver.cpp:253]     Train net output #0: loss = 1.14272 (* 1 = 1.14272 loss)
I0526 05:35:29.840816 27645 sgd_solver.cpp:106] Iteration 576000, lr = 0.0015
I0526 05:35:46.883925 27645 solver.cpp:237] Iteration 577500, loss = 1.00432
I0526 05:35:46.884096 27645 solver.cpp:253]     Train net output #0: loss = 1.00431 (* 1 = 1.00431 loss)
I0526 05:35:46.884115 27645 sgd_solver.cpp:106] Iteration 577500, lr = 0.0015
I0526 05:36:04.086987 27645 solver.cpp:237] Iteration 579000, loss = 1.3305
I0526 05:36:04.087024 27645 solver.cpp:253]     Train net output #0: loss = 1.33049 (* 1 = 1.33049 loss)
I0526 05:36:04.087047 27645 sgd_solver.cpp:106] Iteration 579000, lr = 0.0015
I0526 05:36:43.355505 27645 solver.cpp:237] Iteration 580500, loss = 1.47933
I0526 05:36:43.355685 27645 solver.cpp:253]     Train net output #0: loss = 1.47932 (* 1 = 1.47932 loss)
I0526 05:36:43.355702 27645 sgd_solver.cpp:106] Iteration 580500, lr = 0.0015
I0526 05:37:00.351722 27645 solver.cpp:237] Iteration 582000, loss = 0.638966
I0526 05:37:00.351783 27645 solver.cpp:253]     Train net output #0: loss = 0.638959 (* 1 = 0.638959 loss)
I0526 05:37:00.351807 27645 sgd_solver.cpp:106] Iteration 582000, lr = 0.0015
I0526 05:37:17.402839 27645 solver.cpp:237] Iteration 583500, loss = 0.592147
I0526 05:37:17.402992 27645 solver.cpp:253]     Train net output #0: loss = 0.59214 (* 1 = 0.59214 loss)
I0526 05:37:17.403009 27645 sgd_solver.cpp:106] Iteration 583500, lr = 0.0015
I0526 05:37:34.111119 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_585000.caffemodel
I0526 05:37:34.160677 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_585000.solverstate
I0526 05:37:34.192132 27645 solver.cpp:237] Iteration 585000, loss = 2.05553
I0526 05:37:34.192198 27645 solver.cpp:253]     Train net output #0: loss = 2.05553 (* 1 = 2.05553 loss)
I0526 05:37:34.192225 27645 sgd_solver.cpp:106] Iteration 585000, lr = 0.0015
I0526 05:37:50.987011 27645 solver.cpp:237] Iteration 586500, loss = 1.06449
I0526 05:37:50.987197 27645 solver.cpp:253]     Train net output #0: loss = 1.06449 (* 1 = 1.06449 loss)
I0526 05:37:50.987217 27645 sgd_solver.cpp:106] Iteration 586500, lr = 0.0015
I0526 05:38:08.192587 27645 solver.cpp:237] Iteration 588000, loss = 1.79576
I0526 05:38:08.192627 27645 solver.cpp:253]     Train net output #0: loss = 1.79575 (* 1 = 1.79575 loss)
I0526 05:38:08.192646 27645 sgd_solver.cpp:106] Iteration 588000, lr = 0.0015
I0526 05:38:24.947695 27645 solver.cpp:237] Iteration 589500, loss = 1.24837
I0526 05:38:24.947865 27645 solver.cpp:253]     Train net output #0: loss = 1.24836 (* 1 = 1.24836 loss)
I0526 05:38:24.947882 27645 sgd_solver.cpp:106] Iteration 589500, lr = 0.0015
I0526 05:39:03.887943 27645 solver.cpp:237] Iteration 591000, loss = 0.929543
I0526 05:39:03.888123 27645 solver.cpp:253]     Train net output #0: loss = 0.929537 (* 1 = 0.929537 loss)
I0526 05:39:03.888144 27645 sgd_solver.cpp:106] Iteration 591000, lr = 0.0015
I0526 05:39:21.068277 27645 solver.cpp:237] Iteration 592500, loss = 1.29937
I0526 05:39:21.068317 27645 solver.cpp:253]     Train net output #0: loss = 1.29937 (* 1 = 1.29937 loss)
I0526 05:39:21.068336 27645 sgd_solver.cpp:106] Iteration 592500, lr = 0.0015
I0526 05:39:38.012447 27645 solver.cpp:237] Iteration 594000, loss = 1.27764
I0526 05:39:38.012614 27645 solver.cpp:253]     Train net output #0: loss = 1.27763 (* 1 = 1.27763 loss)
I0526 05:39:38.012631 27645 sgd_solver.cpp:106] Iteration 594000, lr = 0.0015
I0526 05:39:54.882542 27645 solver.cpp:237] Iteration 595500, loss = 0.886083
I0526 05:39:54.882598 27645 solver.cpp:253]     Train net output #0: loss = 0.886078 (* 1 = 0.886078 loss)
I0526 05:39:54.882616 27645 sgd_solver.cpp:106] Iteration 595500, lr = 0.0015
I0526 05:40:11.678735 27645 solver.cpp:237] Iteration 597000, loss = 1.48211
I0526 05:40:11.678889 27645 solver.cpp:253]     Train net output #0: loss = 1.48211 (* 1 = 1.48211 loss)
I0526 05:40:11.678906 27645 sgd_solver.cpp:106] Iteration 597000, lr = 0.0015
I0526 05:40:28.771365 27645 solver.cpp:237] Iteration 598500, loss = 1.20792
I0526 05:40:28.771420 27645 solver.cpp:253]     Train net output #0: loss = 1.20791 (* 1 = 1.20791 loss)
I0526 05:40:28.771438 27645 sgd_solver.cpp:106] Iteration 598500, lr = 0.0015
I0526 05:40:45.843929 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_600000.caffemodel
I0526 05:40:45.893139 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_600000.solverstate
I0526 05:40:45.921385 27645 solver.cpp:341] Iteration 600000, Testing net (#0)
I0526 05:41:45.255638 27645 solver.cpp:409]     Test net output #0: accuracy = 0.897858
I0526 05:41:45.255813 27645 solver.cpp:409]     Test net output #1: loss = 0.329246 (* 1 = 0.329246 loss)
I0526 05:42:06.197366 27645 solver.cpp:237] Iteration 600000, loss = 1.4553
I0526 05:42:06.197428 27645 solver.cpp:253]     Train net output #0: loss = 1.45529 (* 1 = 1.45529 loss)
I0526 05:42:06.197453 27645 sgd_solver.cpp:106] Iteration 600000, lr = 0.0015
I0526 05:42:23.140728 27645 solver.cpp:237] Iteration 601500, loss = 1.24503
I0526 05:42:23.140902 27645 solver.cpp:253]     Train net output #0: loss = 1.24502 (* 1 = 1.24502 loss)
I0526 05:42:23.140928 27645 sgd_solver.cpp:106] Iteration 601500, lr = 0.0015
I0526 05:42:39.787719 27645 solver.cpp:237] Iteration 603000, loss = 1.28787
I0526 05:42:39.787758 27645 solver.cpp:253]     Train net output #0: loss = 1.28787 (* 1 = 1.28787 loss)
I0526 05:42:39.787777 27645 sgd_solver.cpp:106] Iteration 603000, lr = 0.0015
I0526 05:42:56.771500 27645 solver.cpp:237] Iteration 604500, loss = 1.5155
I0526 05:42:56.771693 27645 solver.cpp:253]     Train net output #0: loss = 1.5155 (* 1 = 1.5155 loss)
I0526 05:42:56.771710 27645 sgd_solver.cpp:106] Iteration 604500, lr = 0.0015
I0526 05:43:13.954478 27645 solver.cpp:237] Iteration 606000, loss = 0.834628
I0526 05:43:13.954533 27645 solver.cpp:253]     Train net output #0: loss = 0.834621 (* 1 = 0.834621 loss)
I0526 05:43:13.954551 27645 sgd_solver.cpp:106] Iteration 606000, lr = 0.0015
I0526 05:43:31.009511 27645 solver.cpp:237] Iteration 607500, loss = 1.29784
I0526 05:43:31.009666 27645 solver.cpp:253]     Train net output #0: loss = 1.29783 (* 1 = 1.29783 loss)
I0526 05:43:31.009683 27645 sgd_solver.cpp:106] Iteration 607500, lr = 0.0015
I0526 05:43:47.802919 27645 solver.cpp:237] Iteration 609000, loss = 1.63006
I0526 05:43:47.802975 27645 solver.cpp:253]     Train net output #0: loss = 1.63005 (* 1 = 1.63005 loss)
I0526 05:43:47.802994 27645 sgd_solver.cpp:106] Iteration 609000, lr = 0.0015
I0526 05:44:25.318353 27645 solver.cpp:237] Iteration 610500, loss = 0.586493
I0526 05:44:25.318537 27645 solver.cpp:253]     Train net output #0: loss = 0.586485 (* 1 = 0.586485 loss)
I0526 05:44:25.318557 27645 sgd_solver.cpp:106] Iteration 610500, lr = 0.0015
I0526 05:44:42.247221 27645 solver.cpp:237] Iteration 612000, loss = 1.11908
I0526 05:44:42.247278 27645 solver.cpp:253]     Train net output #0: loss = 1.11908 (* 1 = 1.11908 loss)
I0526 05:44:42.247295 27645 sgd_solver.cpp:106] Iteration 612000, lr = 0.0015
I0526 05:44:59.105556 27645 solver.cpp:237] Iteration 613500, loss = 1.4507
I0526 05:44:59.105731 27645 solver.cpp:253]     Train net output #0: loss = 1.45069 (* 1 = 1.45069 loss)
I0526 05:44:59.105747 27645 sgd_solver.cpp:106] Iteration 613500, lr = 0.0015
I0526 05:45:15.885238 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_615000.caffemodel
I0526 05:45:15.931316 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_615000.solverstate
I0526 05:45:15.960757 27645 solver.cpp:237] Iteration 615000, loss = 1.52637
I0526 05:45:15.960814 27645 solver.cpp:253]     Train net output #0: loss = 1.52636 (* 1 = 1.52636 loss)
I0526 05:45:15.960831 27645 sgd_solver.cpp:106] Iteration 615000, lr = 0.0015
I0526 05:45:33.127524 27645 solver.cpp:237] Iteration 616500, loss = 0.831549
I0526 05:45:33.127701 27645 solver.cpp:253]     Train net output #0: loss = 0.831541 (* 1 = 0.831541 loss)
I0526 05:45:33.127719 27645 sgd_solver.cpp:106] Iteration 616500, lr = 0.0015
I0526 05:45:50.055750 27645 solver.cpp:237] Iteration 618000, loss = 1.07782
I0526 05:45:50.055809 27645 solver.cpp:253]     Train net output #0: loss = 1.07781 (* 1 = 1.07781 loss)
I0526 05:45:50.055827 27645 sgd_solver.cpp:106] Iteration 618000, lr = 0.0015
I0526 05:46:06.704059 27645 solver.cpp:237] Iteration 619500, loss = 1.45528
I0526 05:46:06.704239 27645 solver.cpp:253]     Train net output #0: loss = 1.45527 (* 1 = 1.45527 loss)
I0526 05:46:06.704257 27645 sgd_solver.cpp:106] Iteration 619500, lr = 0.0015
I0526 05:46:44.363224 27645 solver.cpp:237] Iteration 621000, loss = 0.729802
I0526 05:46:44.363401 27645 solver.cpp:253]     Train net output #0: loss = 0.729794 (* 1 = 0.729794 loss)
I0526 05:46:44.363418 27645 sgd_solver.cpp:106] Iteration 621000, lr = 0.0015
I0526 05:47:01.249161 27645 solver.cpp:237] Iteration 622500, loss = 1.02155
I0526 05:47:01.249215 27645 solver.cpp:253]     Train net output #0: loss = 1.02155 (* 1 = 1.02155 loss)
I0526 05:47:01.249243 27645 sgd_solver.cpp:106] Iteration 622500, lr = 0.0015
I0526 05:47:18.450327 27645 solver.cpp:237] Iteration 624000, loss = 1.0877
I0526 05:47:18.450481 27645 solver.cpp:253]     Train net output #0: loss = 1.08769 (* 1 = 1.08769 loss)
I0526 05:47:18.450498 27645 sgd_solver.cpp:106] Iteration 624000, lr = 0.0015
I0526 05:47:35.198966 27645 solver.cpp:237] Iteration 625500, loss = 1.06698
I0526 05:47:35.199021 27645 solver.cpp:253]     Train net output #0: loss = 1.06697 (* 1 = 1.06697 loss)
I0526 05:47:35.199039 27645 sgd_solver.cpp:106] Iteration 625500, lr = 0.0015
I0526 05:47:51.934865 27645 solver.cpp:237] Iteration 627000, loss = 1.5338
I0526 05:47:51.935050 27645 solver.cpp:253]     Train net output #0: loss = 1.53379 (* 1 = 1.53379 loss)
I0526 05:47:51.935068 27645 sgd_solver.cpp:106] Iteration 627000, lr = 0.0015
I0526 05:48:08.827566 27645 solver.cpp:237] Iteration 628500, loss = 1.63941
I0526 05:48:08.827605 27645 solver.cpp:253]     Train net output #0: loss = 1.6394 (* 1 = 1.6394 loss)
I0526 05:48:08.827623 27645 sgd_solver.cpp:106] Iteration 628500, lr = 0.0015
I0526 05:48:25.484474 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_630000.caffemodel
I0526 05:48:25.594182 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_630000.solverstate
I0526 05:48:25.708588 27645 solver.cpp:341] Iteration 630000, Testing net (#0)
I0526 05:49:45.885424 27645 solver.cpp:409]     Test net output #0: accuracy = 0.901111
I0526 05:49:45.885603 27645 solver.cpp:409]     Test net output #1: loss = 0.319086 (* 1 = 0.319086 loss)
I0526 05:50:06.749719 27645 solver.cpp:237] Iteration 630000, loss = 0.960649
I0526 05:50:06.749781 27645 solver.cpp:253]     Train net output #0: loss = 0.960641 (* 1 = 0.960641 loss)
I0526 05:50:06.749806 27645 sgd_solver.cpp:106] Iteration 630000, lr = 0.0015
I0526 05:50:23.798521 27645 solver.cpp:237] Iteration 631500, loss = 1.43986
I0526 05:50:23.798705 27645 solver.cpp:253]     Train net output #0: loss = 1.43985 (* 1 = 1.43985 loss)
I0526 05:50:23.798723 27645 sgd_solver.cpp:106] Iteration 631500, lr = 0.0015
I0526 05:50:40.994613 27645 solver.cpp:237] Iteration 633000, loss = 0.958018
I0526 05:50:40.994654 27645 solver.cpp:253]     Train net output #0: loss = 0.958009 (* 1 = 0.958009 loss)
I0526 05:50:40.994671 27645 sgd_solver.cpp:106] Iteration 633000, lr = 0.0015
I0526 05:50:57.639287 27645 solver.cpp:237] Iteration 634500, loss = 1.08315
I0526 05:50:57.639514 27645 solver.cpp:253]     Train net output #0: loss = 1.08314 (* 1 = 1.08314 loss)
I0526 05:50:57.639533 27645 sgd_solver.cpp:106] Iteration 634500, lr = 0.0015
I0526 05:51:14.368612 27645 solver.cpp:237] Iteration 636000, loss = 1.50038
I0526 05:51:14.368669 27645 solver.cpp:253]     Train net output #0: loss = 1.50037 (* 1 = 1.50037 loss)
I0526 05:51:14.368685 27645 sgd_solver.cpp:106] Iteration 636000, lr = 0.0015
I0526 05:51:31.177599 27645 solver.cpp:237] Iteration 637500, loss = 1.48403
I0526 05:51:31.177755 27645 solver.cpp:253]     Train net output #0: loss = 1.48402 (* 1 = 1.48402 loss)
I0526 05:51:31.177772 27645 sgd_solver.cpp:106] Iteration 637500, lr = 0.0015
I0526 05:51:47.826120 27645 solver.cpp:237] Iteration 639000, loss = 0.927306
I0526 05:51:47.826177 27645 solver.cpp:253]     Train net output #0: loss = 0.927297 (* 1 = 0.927297 loss)
I0526 05:51:47.826195 27645 sgd_solver.cpp:106] Iteration 639000, lr = 0.0015
I0526 05:52:25.481886 27645 solver.cpp:237] Iteration 640500, loss = 0.937194
I0526 05:52:25.482069 27645 solver.cpp:253]     Train net output #0: loss = 0.937185 (* 1 = 0.937185 loss)
I0526 05:52:25.482087 27645 sgd_solver.cpp:106] Iteration 640500, lr = 0.0015
I0526 05:52:42.542163 27645 solver.cpp:237] Iteration 642000, loss = 0.796875
I0526 05:52:42.542202 27645 solver.cpp:253]     Train net output #0: loss = 0.796865 (* 1 = 0.796865 loss)
I0526 05:52:42.542220 27645 sgd_solver.cpp:106] Iteration 642000, lr = 0.0015
I0526 05:52:59.520627 27645 solver.cpp:237] Iteration 643500, loss = 1.42189
I0526 05:52:59.520800 27645 solver.cpp:253]     Train net output #0: loss = 1.42188 (* 1 = 1.42188 loss)
I0526 05:52:59.520818 27645 sgd_solver.cpp:106] Iteration 643500, lr = 0.0015
I0526 05:53:16.360188 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_645000.caffemodel
I0526 05:53:16.406029 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_645000.solverstate
I0526 05:53:16.438215 27645 solver.cpp:237] Iteration 645000, loss = 1.24115
I0526 05:53:16.438278 27645 solver.cpp:253]     Train net output #0: loss = 1.24114 (* 1 = 1.24114 loss)
I0526 05:53:16.438297 27645 sgd_solver.cpp:106] Iteration 645000, lr = 0.0015
I0526 05:53:33.074769 27645 solver.cpp:237] Iteration 646500, loss = 0.87032
I0526 05:53:33.074941 27645 solver.cpp:253]     Train net output #0: loss = 0.87031 (* 1 = 0.87031 loss)
I0526 05:53:33.074959 27645 sgd_solver.cpp:106] Iteration 646500, lr = 0.0015
I0526 05:53:50.144250 27645 solver.cpp:237] Iteration 648000, loss = 1.15055
I0526 05:53:50.144309 27645 solver.cpp:253]     Train net output #0: loss = 1.15054 (* 1 = 1.15054 loss)
I0526 05:53:50.144326 27645 sgd_solver.cpp:106] Iteration 648000, lr = 0.0015
I0526 05:54:07.253052 27645 solver.cpp:237] Iteration 649500, loss = 1.88253
I0526 05:54:07.253228 27645 solver.cpp:253]     Train net output #0: loss = 1.88252 (* 1 = 1.88252 loss)
I0526 05:54:07.253247 27645 sgd_solver.cpp:106] Iteration 649500, lr = 0.0015
I0526 05:54:45.018025 27645 solver.cpp:237] Iteration 651000, loss = 1.73817
I0526 05:54:45.018203 27645 solver.cpp:253]     Train net output #0: loss = 1.73815 (* 1 = 1.73815 loss)
I0526 05:54:45.018221 27645 sgd_solver.cpp:106] Iteration 651000, lr = 0.0015
I0526 05:55:01.710604 27645 solver.cpp:237] Iteration 652500, loss = 1.24425
I0526 05:55:01.710657 27645 solver.cpp:253]     Train net output #0: loss = 1.24424 (* 1 = 1.24424 loss)
I0526 05:55:01.710677 27645 sgd_solver.cpp:106] Iteration 652500, lr = 0.0015
I0526 05:55:18.314924 27645 solver.cpp:237] Iteration 654000, loss = 1.63462
I0526 05:55:18.315100 27645 solver.cpp:253]     Train net output #0: loss = 1.63461 (* 1 = 1.63461 loss)
I0526 05:55:18.315119 27645 sgd_solver.cpp:106] Iteration 654000, lr = 0.0015
I0526 05:55:34.931903 27645 solver.cpp:237] Iteration 655500, loss = 0.594406
I0526 05:55:34.931941 27645 solver.cpp:253]     Train net output #0: loss = 0.594393 (* 1 = 0.594393 loss)
I0526 05:55:34.931959 27645 sgd_solver.cpp:106] Iteration 655500, lr = 0.0015
I0526 05:55:51.576493 27645 solver.cpp:237] Iteration 657000, loss = 1.25524
I0526 05:55:51.576668 27645 solver.cpp:253]     Train net output #0: loss = 1.25522 (* 1 = 1.25522 loss)
I0526 05:55:51.576685 27645 sgd_solver.cpp:106] Iteration 657000, lr = 0.0015
I0526 05:56:08.261946 27645 solver.cpp:237] Iteration 658500, loss = 0.964652
I0526 05:56:08.262002 27645 solver.cpp:253]     Train net output #0: loss = 0.964638 (* 1 = 0.964638 loss)
I0526 05:56:08.262032 27645 sgd_solver.cpp:106] Iteration 658500, lr = 0.0015
I0526 05:56:25.330281 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_660000.caffemodel
I0526 05:56:25.378496 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_660000.solverstate
I0526 05:56:25.406069 27645 solver.cpp:341] Iteration 660000, Testing net (#0)
I0526 05:57:24.834875 27645 solver.cpp:409]     Test net output #0: accuracy = 0.900227
I0526 05:57:24.835052 27645 solver.cpp:409]     Test net output #1: loss = 0.346067 (* 1 = 0.346067 loss)
I0526 05:57:45.698169 27645 solver.cpp:237] Iteration 660000, loss = 1.34207
I0526 05:57:45.698237 27645 solver.cpp:253]     Train net output #0: loss = 1.34205 (* 1 = 1.34205 loss)
I0526 05:57:45.698258 27645 sgd_solver.cpp:106] Iteration 660000, lr = 0.0015
I0526 05:58:02.485321 27645 solver.cpp:237] Iteration 661500, loss = 1.0256
I0526 05:58:02.485514 27645 solver.cpp:253]     Train net output #0: loss = 1.02559 (* 1 = 1.02559 loss)
I0526 05:58:02.485532 27645 sgd_solver.cpp:106] Iteration 661500, lr = 0.0015
I0526 05:58:19.472954 27645 solver.cpp:237] Iteration 663000, loss = 1.07126
I0526 05:58:19.473009 27645 solver.cpp:253]     Train net output #0: loss = 1.07125 (* 1 = 1.07125 loss)
I0526 05:58:19.473028 27645 sgd_solver.cpp:106] Iteration 663000, lr = 0.0015
I0526 05:58:36.689880 27645 solver.cpp:237] Iteration 664500, loss = 1.1962
I0526 05:58:36.690052 27645 solver.cpp:253]     Train net output #0: loss = 1.19619 (* 1 = 1.19619 loss)
I0526 05:58:36.690070 27645 sgd_solver.cpp:106] Iteration 664500, lr = 0.0015
I0526 05:58:53.661306 27645 solver.cpp:237] Iteration 666000, loss = 1.5542
I0526 05:58:53.661344 27645 solver.cpp:253]     Train net output #0: loss = 1.55419 (* 1 = 1.55419 loss)
I0526 05:58:53.661362 27645 sgd_solver.cpp:106] Iteration 666000, lr = 0.0015
I0526 05:59:10.707262 27645 solver.cpp:237] Iteration 667500, loss = 0.953445
I0526 05:59:10.707437 27645 solver.cpp:253]     Train net output #0: loss = 0.953432 (* 1 = 0.953432 loss)
I0526 05:59:10.707454 27645 sgd_solver.cpp:106] Iteration 667500, lr = 0.0015
I0526 05:59:27.738941 27645 solver.cpp:237] Iteration 669000, loss = 1.58232
I0526 05:59:27.738996 27645 solver.cpp:253]     Train net output #0: loss = 1.5823 (* 1 = 1.5823 loss)
I0526 05:59:27.739014 27645 sgd_solver.cpp:106] Iteration 669000, lr = 0.0015
I0526 06:00:05.269063 27645 solver.cpp:237] Iteration 670500, loss = 0.920822
I0526 06:00:05.269246 27645 solver.cpp:253]     Train net output #0: loss = 0.920808 (* 1 = 0.920808 loss)
I0526 06:00:05.269265 27645 sgd_solver.cpp:106] Iteration 670500, lr = 0.0015
I0526 06:00:21.943533 27645 solver.cpp:237] Iteration 672000, loss = 0.875498
I0526 06:00:21.943593 27645 solver.cpp:253]     Train net output #0: loss = 0.875484 (* 1 = 0.875484 loss)
I0526 06:00:21.943621 27645 sgd_solver.cpp:106] Iteration 672000, lr = 0.0015
I0526 06:00:38.709827 27645 solver.cpp:237] Iteration 673500, loss = 0.797235
I0526 06:00:38.709986 27645 solver.cpp:253]     Train net output #0: loss = 0.79722 (* 1 = 0.79722 loss)
I0526 06:00:38.710003 27645 sgd_solver.cpp:106] Iteration 673500, lr = 0.0015
I0526 06:00:55.375887 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_675000.caffemodel
I0526 06:00:55.424659 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_675000.solverstate
I0526 06:00:55.456454 27645 solver.cpp:237] Iteration 675000, loss = 1.13264
I0526 06:00:55.456514 27645 solver.cpp:253]     Train net output #0: loss = 1.13262 (* 1 = 1.13262 loss)
I0526 06:00:55.456532 27645 sgd_solver.cpp:106] Iteration 675000, lr = 0.0015
I0526 06:01:12.210417 27645 solver.cpp:237] Iteration 676500, loss = 1.27036
I0526 06:01:12.210597 27645 solver.cpp:253]     Train net output #0: loss = 1.27034 (* 1 = 1.27034 loss)
I0526 06:01:12.210614 27645 sgd_solver.cpp:106] Iteration 676500, lr = 0.0015
I0526 06:01:29.170030 27645 solver.cpp:237] Iteration 678000, loss = 0.844067
I0526 06:01:29.170073 27645 solver.cpp:253]     Train net output #0: loss = 0.844053 (* 1 = 0.844053 loss)
I0526 06:01:29.170089 27645 sgd_solver.cpp:106] Iteration 678000, lr = 0.0015
I0526 06:01:45.852625 27645 solver.cpp:237] Iteration 679500, loss = 1.16656
I0526 06:01:45.852798 27645 solver.cpp:253]     Train net output #0: loss = 1.16655 (* 1 = 1.16655 loss)
I0526 06:01:45.852816 27645 sgd_solver.cpp:106] Iteration 679500, lr = 0.0015
I0526 06:02:23.402472 27645 solver.cpp:237] Iteration 681000, loss = 1.07954
I0526 06:02:23.402659 27645 solver.cpp:253]     Train net output #0: loss = 1.07953 (* 1 = 1.07953 loss)
I0526 06:02:23.402678 27645 sgd_solver.cpp:106] Iteration 681000, lr = 0.0015
I0526 06:02:40.354989 27645 solver.cpp:237] Iteration 682500, loss = 1.20837
I0526 06:02:40.355026 27645 solver.cpp:253]     Train net output #0: loss = 1.20835 (* 1 = 1.20835 loss)
I0526 06:02:40.355049 27645 sgd_solver.cpp:106] Iteration 682500, lr = 0.0015
I0526 06:02:57.290895 27645 solver.cpp:237] Iteration 684000, loss = 0.80235
I0526 06:02:57.291082 27645 solver.cpp:253]     Train net output #0: loss = 0.802338 (* 1 = 0.802338 loss)
I0526 06:02:57.291100 27645 sgd_solver.cpp:106] Iteration 684000, lr = 0.0015
I0526 06:03:14.190408 27645 solver.cpp:237] Iteration 685500, loss = 0.877329
I0526 06:03:14.190467 27645 solver.cpp:253]     Train net output #0: loss = 0.877316 (* 1 = 0.877316 loss)
I0526 06:03:14.190488 27645 sgd_solver.cpp:106] Iteration 685500, lr = 0.0015
I0526 06:03:31.133396 27645 solver.cpp:237] Iteration 687000, loss = 1.31173
I0526 06:03:31.133558 27645 solver.cpp:253]     Train net output #0: loss = 1.31171 (* 1 = 1.31171 loss)
I0526 06:03:31.133574 27645 sgd_solver.cpp:106] Iteration 687000, lr = 0.0015
I0526 06:03:47.850108 27645 solver.cpp:237] Iteration 688500, loss = 1.10177
I0526 06:03:47.850163 27645 solver.cpp:253]     Train net output #0: loss = 1.10176 (* 1 = 1.10176 loss)
I0526 06:03:47.850180 27645 sgd_solver.cpp:106] Iteration 688500, lr = 0.0015
I0526 06:04:04.575253 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_690000.caffemodel
I0526 06:04:04.622699 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_690000.solverstate
I0526 06:04:04.648874 27645 solver.cpp:341] Iteration 690000, Testing net (#0)
I0526 06:05:25.061403 27645 solver.cpp:409]     Test net output #0: accuracy = 0.903498
I0526 06:05:25.061581 27645 solver.cpp:409]     Test net output #1: loss = 0.31 (* 1 = 0.31 loss)
I0526 06:05:45.939849 27645 solver.cpp:237] Iteration 690000, loss = 1.23946
I0526 06:05:45.939911 27645 solver.cpp:253]     Train net output #0: loss = 1.23945 (* 1 = 1.23945 loss)
I0526 06:05:45.939939 27645 sgd_solver.cpp:106] Iteration 690000, lr = 0.0015
I0526 06:06:03.017004 27645 solver.cpp:237] Iteration 691500, loss = 1.33532
I0526 06:06:03.017168 27645 solver.cpp:253]     Train net output #0: loss = 1.33531 (* 1 = 1.33531 loss)
I0526 06:06:03.017185 27645 sgd_solver.cpp:106] Iteration 691500, lr = 0.0015
I0526 06:06:19.747741 27645 solver.cpp:237] Iteration 693000, loss = 0.929717
I0526 06:06:19.747797 27645 solver.cpp:253]     Train net output #0: loss = 0.929706 (* 1 = 0.929706 loss)
I0526 06:06:19.747814 27645 sgd_solver.cpp:106] Iteration 693000, lr = 0.0015
I0526 06:06:36.421138 27645 solver.cpp:237] Iteration 694500, loss = 0.794216
I0526 06:06:36.421315 27645 solver.cpp:253]     Train net output #0: loss = 0.794205 (* 1 = 0.794205 loss)
I0526 06:06:36.421334 27645 sgd_solver.cpp:106] Iteration 694500, lr = 0.0015
I0526 06:06:53.061307 27645 solver.cpp:237] Iteration 696000, loss = 0.6105
I0526 06:06:53.061347 27645 solver.cpp:253]     Train net output #0: loss = 0.610489 (* 1 = 0.610489 loss)
I0526 06:06:53.061364 27645 sgd_solver.cpp:106] Iteration 696000, lr = 0.0015
I0526 06:07:09.815863 27645 solver.cpp:237] Iteration 697500, loss = 1.23487
I0526 06:07:09.816040 27645 solver.cpp:253]     Train net output #0: loss = 1.23486 (* 1 = 1.23486 loss)
I0526 06:07:09.816058 27645 sgd_solver.cpp:106] Iteration 697500, lr = 0.0015
I0526 06:07:26.632591 27645 solver.cpp:237] Iteration 699000, loss = 0.754451
I0526 06:07:26.632647 27645 solver.cpp:253]     Train net output #0: loss = 0.754441 (* 1 = 0.754441 loss)
I0526 06:07:26.632663 27645 sgd_solver.cpp:106] Iteration 699000, lr = 0.0015
I0526 06:08:04.119868 27645 solver.cpp:237] Iteration 700500, loss = 1.28241
I0526 06:08:04.120048 27645 solver.cpp:253]     Train net output #0: loss = 1.2824 (* 1 = 1.2824 loss)
I0526 06:08:04.120066 27645 sgd_solver.cpp:106] Iteration 700500, lr = 0.0015
I0526 06:08:21.071668 27645 solver.cpp:237] Iteration 702000, loss = 1.04912
I0526 06:08:21.071727 27645 solver.cpp:253]     Train net output #0: loss = 1.04911 (* 1 = 1.04911 loss)
I0526 06:08:21.071744 27645 sgd_solver.cpp:106] Iteration 702000, lr = 0.0015
I0526 06:08:38.267159 27645 solver.cpp:237] Iteration 703500, loss = 0.958508
I0526 06:08:38.267348 27645 solver.cpp:253]     Train net output #0: loss = 0.958496 (* 1 = 0.958496 loss)
I0526 06:08:38.267365 27645 sgd_solver.cpp:106] Iteration 703500, lr = 0.0015
I0526 06:08:55.329351 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_705000.caffemodel
I0526 06:08:55.375588 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_705000.solverstate
I0526 06:08:55.404913 27645 solver.cpp:237] Iteration 705000, loss = 1.43991
I0526 06:08:55.404971 27645 solver.cpp:253]     Train net output #0: loss = 1.4399 (* 1 = 1.4399 loss)
I0526 06:08:55.404996 27645 sgd_solver.cpp:106] Iteration 705000, lr = 0.0015
I0526 06:09:12.211019 27645 solver.cpp:237] Iteration 706500, loss = 0.61212
I0526 06:09:12.211205 27645 solver.cpp:253]     Train net output #0: loss = 0.612109 (* 1 = 0.612109 loss)
I0526 06:09:12.211222 27645 sgd_solver.cpp:106] Iteration 706500, lr = 0.0015
I0526 06:09:28.886351 27645 solver.cpp:237] Iteration 708000, loss = 1.36163
I0526 06:09:28.886411 27645 solver.cpp:253]     Train net output #0: loss = 1.36162 (* 1 = 1.36162 loss)
I0526 06:09:28.886438 27645 sgd_solver.cpp:106] Iteration 708000, lr = 0.0015
I0526 06:09:45.653653 27645 solver.cpp:237] Iteration 709500, loss = 0.423705
I0526 06:09:45.653823 27645 solver.cpp:253]     Train net output #0: loss = 0.423695 (* 1 = 0.423695 loss)
I0526 06:09:45.653841 27645 sgd_solver.cpp:106] Iteration 709500, lr = 0.0015
I0526 06:10:23.446861 27645 solver.cpp:237] Iteration 711000, loss = 0.99711
I0526 06:10:23.447046 27645 solver.cpp:253]     Train net output #0: loss = 0.9971 (* 1 = 0.9971 loss)
I0526 06:10:23.447065 27645 sgd_solver.cpp:106] Iteration 711000, lr = 0.0015
I0526 06:10:40.393685 27645 solver.cpp:237] Iteration 712500, loss = 1.8739
I0526 06:10:40.393723 27645 solver.cpp:253]     Train net output #0: loss = 1.87389 (* 1 = 1.87389 loss)
I0526 06:10:40.393746 27645 sgd_solver.cpp:106] Iteration 712500, lr = 0.0015
I0526 06:10:57.475255 27645 solver.cpp:237] Iteration 714000, loss = 1.53862
I0526 06:10:57.475422 27645 solver.cpp:253]     Train net output #0: loss = 1.53861 (* 1 = 1.53861 loss)
I0526 06:10:57.475440 27645 sgd_solver.cpp:106] Iteration 714000, lr = 0.0015
I0526 06:11:14.619225 27645 solver.cpp:237] Iteration 715500, loss = 1.25861
I0526 06:11:14.619277 27645 solver.cpp:253]     Train net output #0: loss = 1.2586 (* 1 = 1.2586 loss)
I0526 06:11:14.619295 27645 sgd_solver.cpp:106] Iteration 715500, lr = 0.0015
I0526 06:11:31.859146 27645 solver.cpp:237] Iteration 717000, loss = 0.894454
I0526 06:11:31.859326 27645 solver.cpp:253]     Train net output #0: loss = 0.894443 (* 1 = 0.894443 loss)
I0526 06:11:31.859352 27645 sgd_solver.cpp:106] Iteration 717000, lr = 0.0015
I0526 06:11:48.782143 27645 solver.cpp:237] Iteration 718500, loss = 1.3019
I0526 06:11:48.782187 27645 solver.cpp:253]     Train net output #0: loss = 1.30189 (* 1 = 1.30189 loss)
I0526 06:11:48.782204 27645 sgd_solver.cpp:106] Iteration 718500, lr = 0.0015
I0526 06:12:05.758795 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_720000.caffemodel
I0526 06:12:05.805632 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_720000.solverstate
I0526 06:12:05.832052 27645 solver.cpp:341] Iteration 720000, Testing net (#0)
I0526 06:13:05.252527 27645 solver.cpp:409]     Test net output #0: accuracy = 0.899077
I0526 06:13:05.252719 27645 solver.cpp:409]     Test net output #1: loss = 0.312728 (* 1 = 0.312728 loss)
I0526 06:13:26.131527 27645 solver.cpp:237] Iteration 720000, loss = 1.46683
I0526 06:13:26.131592 27645 solver.cpp:253]     Train net output #0: loss = 1.46682 (* 1 = 1.46682 loss)
I0526 06:13:26.131619 27645 sgd_solver.cpp:106] Iteration 720000, lr = 0.0015
I0526 06:13:43.178793 27645 solver.cpp:237] Iteration 721500, loss = 1.83751
I0526 06:13:43.178977 27645 solver.cpp:253]     Train net output #0: loss = 1.8375 (* 1 = 1.8375 loss)
I0526 06:13:43.178997 27645 sgd_solver.cpp:106] Iteration 721500, lr = 0.0015
I0526 06:14:00.394384 27645 solver.cpp:237] Iteration 723000, loss = 1.66805
I0526 06:14:00.394423 27645 solver.cpp:253]     Train net output #0: loss = 1.66804 (* 1 = 1.66804 loss)
I0526 06:14:00.394441 27645 sgd_solver.cpp:106] Iteration 723000, lr = 0.0015
I0526 06:14:17.264999 27645 solver.cpp:237] Iteration 724500, loss = 0.690452
I0526 06:14:17.265182 27645 solver.cpp:253]     Train net output #0: loss = 0.690442 (* 1 = 0.690442 loss)
I0526 06:14:17.265198 27645 sgd_solver.cpp:106] Iteration 724500, lr = 0.0015
I0526 06:14:34.055614 27645 solver.cpp:237] Iteration 726000, loss = 1.32135
I0526 06:14:34.055668 27645 solver.cpp:253]     Train net output #0: loss = 1.32134 (* 1 = 1.32134 loss)
I0526 06:14:34.055685 27645 sgd_solver.cpp:106] Iteration 726000, lr = 0.0015
I0526 06:14:50.710055 27645 solver.cpp:237] Iteration 727500, loss = 0.945128
I0526 06:14:50.710219 27645 solver.cpp:253]     Train net output #0: loss = 0.945118 (* 1 = 0.945118 loss)
I0526 06:14:50.710237 27645 sgd_solver.cpp:106] Iteration 727500, lr = 0.0015
I0526 06:15:07.613328 27645 solver.cpp:237] Iteration 729000, loss = 1.18771
I0526 06:15:07.613384 27645 solver.cpp:253]     Train net output #0: loss = 1.1877 (* 1 = 1.1877 loss)
I0526 06:15:07.613401 27645 sgd_solver.cpp:106] Iteration 729000, lr = 0.0015
I0526 06:15:45.476294 27645 solver.cpp:237] Iteration 730500, loss = 1.04059
I0526 06:15:45.476480 27645 solver.cpp:253]     Train net output #0: loss = 1.04058 (* 1 = 1.04058 loss)
I0526 06:15:45.476503 27645 sgd_solver.cpp:106] Iteration 730500, lr = 0.0015
I0526 06:16:02.252162 27645 solver.cpp:237] Iteration 732000, loss = 0.983213
I0526 06:16:02.252200 27645 solver.cpp:253]     Train net output #0: loss = 0.983203 (* 1 = 0.983203 loss)
I0526 06:16:02.252219 27645 sgd_solver.cpp:106] Iteration 732000, lr = 0.0015
I0526 06:16:19.350055 27645 solver.cpp:237] Iteration 733500, loss = 0.906214
I0526 06:16:19.350236 27645 solver.cpp:253]     Train net output #0: loss = 0.906204 (* 1 = 0.906204 loss)
I0526 06:16:19.350253 27645 sgd_solver.cpp:106] Iteration 733500, lr = 0.0015
I0526 06:16:36.470087 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_735000.caffemodel
I0526 06:16:36.519561 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_735000.solverstate
I0526 06:16:36.550508 27645 solver.cpp:237] Iteration 735000, loss = 2.10392
I0526 06:16:36.550572 27645 solver.cpp:253]     Train net output #0: loss = 2.10391 (* 1 = 2.10391 loss)
I0526 06:16:36.550590 27645 sgd_solver.cpp:106] Iteration 735000, lr = 0.0015
I0526 06:16:53.501165 27645 solver.cpp:237] Iteration 736500, loss = 1.11419
I0526 06:16:53.501329 27645 solver.cpp:253]     Train net output #0: loss = 1.11418 (* 1 = 1.11418 loss)
I0526 06:16:53.501346 27645 sgd_solver.cpp:106] Iteration 736500, lr = 0.0015
I0526 06:17:10.265555 27645 solver.cpp:237] Iteration 738000, loss = 3.04444
I0526 06:17:10.265610 27645 solver.cpp:253]     Train net output #0: loss = 3.04444 (* 1 = 3.04444 loss)
I0526 06:17:10.265628 27645 sgd_solver.cpp:106] Iteration 738000, lr = 0.0015
I0526 06:17:27.010723 27645 solver.cpp:237] Iteration 739500, loss = 1.18448
I0526 06:17:27.010916 27645 solver.cpp:253]     Train net output #0: loss = 1.18447 (* 1 = 1.18447 loss)
I0526 06:17:27.010936 27645 sgd_solver.cpp:106] Iteration 739500, lr = 0.0015
I0526 06:18:04.493240 27645 solver.cpp:237] Iteration 741000, loss = 1.07036
I0526 06:18:04.493432 27645 solver.cpp:253]     Train net output #0: loss = 1.07035 (* 1 = 1.07035 loss)
I0526 06:18:04.493450 27645 sgd_solver.cpp:106] Iteration 741000, lr = 0.0015
I0526 06:18:21.227598 27645 solver.cpp:237] Iteration 742500, loss = 1.17224
I0526 06:18:21.227653 27645 solver.cpp:253]     Train net output #0: loss = 1.17223 (* 1 = 1.17223 loss)
I0526 06:18:21.227671 27645 sgd_solver.cpp:106] Iteration 742500, lr = 0.0015
I0526 06:18:37.996069 27645 solver.cpp:237] Iteration 744000, loss = 1.05683
I0526 06:18:37.996255 27645 solver.cpp:253]     Train net output #0: loss = 1.05682 (* 1 = 1.05682 loss)
I0526 06:18:37.996274 27645 sgd_solver.cpp:106] Iteration 744000, lr = 0.0015
I0526 06:18:54.633050 27645 solver.cpp:237] Iteration 745500, loss = 2.67922
I0526 06:18:54.633088 27645 solver.cpp:253]     Train net output #0: loss = 2.67921 (* 1 = 2.67921 loss)
I0526 06:18:54.633112 27645 sgd_solver.cpp:106] Iteration 745500, lr = 0.0015
I0526 06:19:11.509590 27645 solver.cpp:237] Iteration 747000, loss = 1.46249
I0526 06:19:11.509770 27645 solver.cpp:253]     Train net output #0: loss = 1.46248 (* 1 = 1.46248 loss)
I0526 06:19:11.509788 27645 sgd_solver.cpp:106] Iteration 747000, lr = 0.0015
I0526 06:19:28.541489 27645 solver.cpp:237] Iteration 748500, loss = 0.961808
I0526 06:19:28.541549 27645 solver.cpp:253]     Train net output #0: loss = 0.961798 (* 1 = 0.961798 loss)
I0526 06:19:28.541575 27645 sgd_solver.cpp:106] Iteration 748500, lr = 0.0015
I0526 06:19:45.426935 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_750000.caffemodel
I0526 06:19:45.475221 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_750000.solverstate
I0526 06:19:45.503228 27645 solver.cpp:341] Iteration 750000, Testing net (#0)
I0526 06:21:05.840628 27645 solver.cpp:409]     Test net output #0: accuracy = 0.902205
I0526 06:21:05.840813 27645 solver.cpp:409]     Test net output #1: loss = 0.314239 (* 1 = 0.314239 loss)
I0526 06:21:26.691643 27645 solver.cpp:237] Iteration 750000, loss = 0.993358
I0526 06:21:26.691709 27645 solver.cpp:253]     Train net output #0: loss = 0.993349 (* 1 = 0.993349 loss)
I0526 06:21:26.691730 27645 sgd_solver.cpp:106] Iteration 750000, lr = 0.0015
I0526 06:21:43.445384 27645 solver.cpp:237] Iteration 751500, loss = 1.51048
I0526 06:21:43.445566 27645 solver.cpp:253]     Train net output #0: loss = 1.51048 (* 1 = 1.51048 loss)
I0526 06:21:43.445585 27645 sgd_solver.cpp:106] Iteration 751500, lr = 0.0015
I0526 06:22:00.122098 27645 solver.cpp:237] Iteration 753000, loss = 1.55247
I0526 06:22:00.122156 27645 solver.cpp:253]     Train net output #0: loss = 1.55246 (* 1 = 1.55246 loss)
I0526 06:22:00.122182 27645 sgd_solver.cpp:106] Iteration 753000, lr = 0.0015
I0526 06:22:16.994561 27645 solver.cpp:237] Iteration 754500, loss = 1.47468
I0526 06:22:16.994725 27645 solver.cpp:253]     Train net output #0: loss = 1.47467 (* 1 = 1.47467 loss)
I0526 06:22:16.994742 27645 sgd_solver.cpp:106] Iteration 754500, lr = 0.0015
I0526 06:22:33.698546 27645 solver.cpp:237] Iteration 756000, loss = 1.32962
I0526 06:22:33.698601 27645 solver.cpp:253]     Train net output #0: loss = 1.32961 (* 1 = 1.32961 loss)
I0526 06:22:33.698619 27645 sgd_solver.cpp:106] Iteration 756000, lr = 0.0015
I0526 06:22:50.450722 27645 solver.cpp:237] Iteration 757500, loss = 0.772606
I0526 06:22:50.450906 27645 solver.cpp:253]     Train net output #0: loss = 0.772598 (* 1 = 0.772598 loss)
I0526 06:22:50.450923 27645 sgd_solver.cpp:106] Iteration 757500, lr = 0.0015
I0526 06:23:07.683424 27645 solver.cpp:237] Iteration 759000, loss = 1.01872
I0526 06:23:07.683461 27645 solver.cpp:253]     Train net output #0: loss = 1.01871 (* 1 = 1.01871 loss)
I0526 06:23:07.683480 27645 sgd_solver.cpp:106] Iteration 759000, lr = 0.0015
I0526 06:23:45.497942 27645 solver.cpp:237] Iteration 760500, loss = 1.57922
I0526 06:23:45.498136 27645 solver.cpp:253]     Train net output #0: loss = 1.57921 (* 1 = 1.57921 loss)
I0526 06:23:45.498153 27645 sgd_solver.cpp:106] Iteration 760500, lr = 0.0015
I0526 06:24:02.376724 27645 solver.cpp:237] Iteration 762000, loss = 0.877129
I0526 06:24:02.376783 27645 solver.cpp:253]     Train net output #0: loss = 0.87712 (* 1 = 0.87712 loss)
I0526 06:24:02.376811 27645 sgd_solver.cpp:106] Iteration 762000, lr = 0.0015
I0526 06:24:19.298686 27645 solver.cpp:237] Iteration 763500, loss = 0.902463
I0526 06:24:19.298852 27645 solver.cpp:253]     Train net output #0: loss = 0.902454 (* 1 = 0.902454 loss)
I0526 06:24:19.298869 27645 sgd_solver.cpp:106] Iteration 763500, lr = 0.0015
I0526 06:24:36.254815 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_765000.caffemodel
I0526 06:24:36.301232 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_765000.solverstate
I0526 06:24:36.330039 27645 solver.cpp:237] Iteration 765000, loss = 0.359934
I0526 06:24:36.330098 27645 solver.cpp:253]     Train net output #0: loss = 0.359925 (* 1 = 0.359925 loss)
I0526 06:24:36.330117 27645 sgd_solver.cpp:106] Iteration 765000, lr = 0.0015
I0526 06:24:53.172087 27645 solver.cpp:237] Iteration 766500, loss = 0.829317
I0526 06:24:53.172279 27645 solver.cpp:253]     Train net output #0: loss = 0.829308 (* 1 = 0.829308 loss)
I0526 06:24:53.172300 27645 sgd_solver.cpp:106] Iteration 766500, lr = 0.0015
I0526 06:25:09.784394 27645 solver.cpp:237] Iteration 768000, loss = 0.280704
I0526 06:25:09.784433 27645 solver.cpp:253]     Train net output #0: loss = 0.280695 (* 1 = 0.280695 loss)
I0526 06:25:09.784452 27645 sgd_solver.cpp:106] Iteration 768000, lr = 0.0015
I0526 06:25:26.693693 27645 solver.cpp:237] Iteration 769500, loss = 1.38317
I0526 06:25:26.693876 27645 solver.cpp:253]     Train net output #0: loss = 1.38316 (* 1 = 1.38316 loss)
I0526 06:25:26.693893 27645 sgd_solver.cpp:106] Iteration 769500, lr = 0.0015
I0526 06:26:04.630708 27645 solver.cpp:237] Iteration 771000, loss = 0.940974
I0526 06:26:04.630894 27645 solver.cpp:253]     Train net output #0: loss = 0.940967 (* 1 = 0.940967 loss)
I0526 06:26:04.630913 27645 sgd_solver.cpp:106] Iteration 771000, lr = 0.0015
I0526 06:26:21.571557 27645 solver.cpp:237] Iteration 772500, loss = 1.63244
I0526 06:26:21.571612 27645 solver.cpp:253]     Train net output #0: loss = 1.63244 (* 1 = 1.63244 loss)
I0526 06:26:21.571631 27645 sgd_solver.cpp:106] Iteration 772500, lr = 0.0015
I0526 06:26:38.341316 27645 solver.cpp:237] Iteration 774000, loss = 0.344531
I0526 06:26:38.341500 27645 solver.cpp:253]     Train net output #0: loss = 0.344524 (* 1 = 0.344524 loss)
I0526 06:26:38.341517 27645 sgd_solver.cpp:106] Iteration 774000, lr = 0.0015
I0526 06:26:55.073345 27645 solver.cpp:237] Iteration 775500, loss = 1.01431
I0526 06:26:55.073385 27645 solver.cpp:253]     Train net output #0: loss = 1.0143 (* 1 = 1.0143 loss)
I0526 06:26:55.073401 27645 sgd_solver.cpp:106] Iteration 775500, lr = 0.0015
I0526 06:27:12.130061 27645 solver.cpp:237] Iteration 777000, loss = 0.610487
I0526 06:27:12.130244 27645 solver.cpp:253]     Train net output #0: loss = 0.61048 (* 1 = 0.61048 loss)
I0526 06:27:12.130262 27645 sgd_solver.cpp:106] Iteration 777000, lr = 0.0015
I0526 06:27:29.268409 27645 solver.cpp:237] Iteration 778500, loss = 0.72336
I0526 06:27:29.268465 27645 solver.cpp:253]     Train net output #0: loss = 0.723353 (* 1 = 0.723353 loss)
I0526 06:27:29.268483 27645 sgd_solver.cpp:106] Iteration 778500, lr = 0.0015
I0526 06:27:46.472381 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_780000.caffemodel
I0526 06:27:46.519501 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_780000.solverstate
I0526 06:27:46.545264 27645 solver.cpp:341] Iteration 780000, Testing net (#0)
I0526 06:28:45.570462 27645 solver.cpp:409]     Test net output #0: accuracy = 0.899158
I0526 06:28:45.570650 27645 solver.cpp:409]     Test net output #1: loss = 0.322938 (* 1 = 0.322938 loss)
I0526 06:29:06.450258 27645 solver.cpp:237] Iteration 780000, loss = 1.36845
I0526 06:29:06.450320 27645 solver.cpp:253]     Train net output #0: loss = 1.36844 (* 1 = 1.36844 loss)
I0526 06:29:06.450345 27645 sgd_solver.cpp:106] Iteration 780000, lr = 0.0015
I0526 06:29:23.042392 27645 solver.cpp:237] Iteration 781500, loss = 0.886175
I0526 06:29:23.042562 27645 solver.cpp:253]     Train net output #0: loss = 0.886168 (* 1 = 0.886168 loss)
I0526 06:29:23.042578 27645 sgd_solver.cpp:106] Iteration 781500, lr = 0.0015
I0526 06:29:39.941391 27645 solver.cpp:237] Iteration 783000, loss = 1.0094
I0526 06:29:39.941449 27645 solver.cpp:253]     Train net output #0: loss = 1.00939 (* 1 = 1.00939 loss)
I0526 06:29:39.941467 27645 sgd_solver.cpp:106] Iteration 783000, lr = 0.0015
I0526 06:29:56.854274 27645 solver.cpp:237] Iteration 784500, loss = 1.35644
I0526 06:29:56.854459 27645 solver.cpp:253]     Train net output #0: loss = 1.35644 (* 1 = 1.35644 loss)
I0526 06:29:56.854476 27645 sgd_solver.cpp:106] Iteration 784500, lr = 0.0015
I0526 06:30:13.715688 27645 solver.cpp:237] Iteration 786000, loss = 0.82913
I0526 06:30:13.715728 27645 solver.cpp:253]     Train net output #0: loss = 0.829123 (* 1 = 0.829123 loss)
I0526 06:30:13.715746 27645 sgd_solver.cpp:106] Iteration 786000, lr = 0.0015
I0526 06:30:30.432165 27645 solver.cpp:237] Iteration 787500, loss = 1.11237
I0526 06:30:30.432348 27645 solver.cpp:253]     Train net output #0: loss = 1.11236 (* 1 = 1.11236 loss)
I0526 06:30:30.432364 27645 sgd_solver.cpp:106] Iteration 787500, lr = 0.0015
I0526 06:30:47.222709 27645 solver.cpp:237] Iteration 789000, loss = 0.743564
I0526 06:30:47.222771 27645 solver.cpp:253]     Train net output #0: loss = 0.743557 (* 1 = 0.743557 loss)
I0526 06:30:47.222790 27645 sgd_solver.cpp:106] Iteration 789000, lr = 0.0015
I0526 06:31:25.098011 27645 solver.cpp:237] Iteration 790500, loss = 1.42783
I0526 06:31:25.098199 27645 solver.cpp:253]     Train net output #0: loss = 1.42782 (* 1 = 1.42782 loss)
I0526 06:31:25.098217 27645 sgd_solver.cpp:106] Iteration 790500, lr = 0.0015
I0526 06:31:41.987275 27645 solver.cpp:237] Iteration 792000, loss = 0.704687
I0526 06:31:41.987332 27645 solver.cpp:253]     Train net output #0: loss = 0.704681 (* 1 = 0.704681 loss)
I0526 06:31:41.987349 27645 sgd_solver.cpp:106] Iteration 792000, lr = 0.0015
I0526 06:31:58.754813 27645 solver.cpp:237] Iteration 793500, loss = 1.15074
I0526 06:31:58.754999 27645 solver.cpp:253]     Train net output #0: loss = 1.15073 (* 1 = 1.15073 loss)
I0526 06:31:58.755017 27645 sgd_solver.cpp:106] Iteration 793500, lr = 0.0015
I0526 06:32:15.382122 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_795000.caffemodel
I0526 06:32:15.431840 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_795000.solverstate
I0526 06:32:15.460811 27645 solver.cpp:237] Iteration 795000, loss = 0.248528
I0526 06:32:15.460870 27645 solver.cpp:253]     Train net output #0: loss = 0.248522 (* 1 = 0.248522 loss)
I0526 06:32:15.460887 27645 sgd_solver.cpp:106] Iteration 795000, lr = 0.0015
I0526 06:32:32.215055 27645 solver.cpp:237] Iteration 796500, loss = 1.6325
I0526 06:32:32.215251 27645 solver.cpp:253]     Train net output #0: loss = 1.6325 (* 1 = 1.6325 loss)
I0526 06:32:32.215270 27645 sgd_solver.cpp:106] Iteration 796500, lr = 0.0015
I0526 06:32:48.998368 27645 solver.cpp:237] Iteration 798000, loss = 1.35415
I0526 06:32:48.998426 27645 solver.cpp:253]     Train net output #0: loss = 1.35414 (* 1 = 1.35414 loss)
I0526 06:32:48.998453 27645 sgd_solver.cpp:106] Iteration 798000, lr = 0.0015
I0526 06:33:05.922709 27645 solver.cpp:237] Iteration 799500, loss = 0.814521
I0526 06:33:05.922880 27645 solver.cpp:253]     Train net output #0: loss = 0.814515 (* 1 = 0.814515 loss)
I0526 06:33:05.922897 27645 sgd_solver.cpp:106] Iteration 799500, lr = 0.0015
I0526 06:33:43.747822 27645 solver.cpp:237] Iteration 801000, loss = 1.07815
I0526 06:33:43.748011 27645 solver.cpp:253]     Train net output #0: loss = 1.07814 (* 1 = 1.07814 loss)
I0526 06:33:43.748030 27645 sgd_solver.cpp:106] Iteration 801000, lr = 0.0015
I0526 06:34:00.723017 27645 solver.cpp:237] Iteration 802500, loss = 1.26889
I0526 06:34:00.723057 27645 solver.cpp:253]     Train net output #0: loss = 1.26888 (* 1 = 1.26888 loss)
I0526 06:34:00.723073 27645 sgd_solver.cpp:106] Iteration 802500, lr = 0.0015
I0526 06:34:17.921159 27645 solver.cpp:237] Iteration 804000, loss = 1.36493
I0526 06:34:17.921340 27645 solver.cpp:253]     Train net output #0: loss = 1.36492 (* 1 = 1.36492 loss)
I0526 06:34:17.921357 27645 sgd_solver.cpp:106] Iteration 804000, lr = 0.0015
I0526 06:34:35.007642 27645 solver.cpp:237] Iteration 805500, loss = 1.157
I0526 06:34:35.007699 27645 solver.cpp:253]     Train net output #0: loss = 1.15699 (* 1 = 1.15699 loss)
I0526 06:34:35.007717 27645 sgd_solver.cpp:106] Iteration 805500, lr = 0.0015
I0526 06:34:51.965940 27645 solver.cpp:237] Iteration 807000, loss = 1.48698
I0526 06:34:51.966125 27645 solver.cpp:253]     Train net output #0: loss = 1.48697 (* 1 = 1.48697 loss)
I0526 06:34:51.966141 27645 sgd_solver.cpp:106] Iteration 807000, lr = 0.0015
I0526 06:35:08.585815 27645 solver.cpp:237] Iteration 808500, loss = 1.04346
I0526 06:35:08.585855 27645 solver.cpp:253]     Train net output #0: loss = 1.04346 (* 1 = 1.04346 loss)
I0526 06:35:08.585871 27645 sgd_solver.cpp:106] Iteration 808500, lr = 0.0015
I0526 06:35:25.268991 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_810000.caffemodel
I0526 06:35:25.318028 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_810000.solverstate
I0526 06:35:25.345019 27645 solver.cpp:341] Iteration 810000, Testing net (#0)
I0526 06:36:45.775405 27645 solver.cpp:409]     Test net output #0: accuracy = 0.902859
I0526 06:36:45.775605 27645 solver.cpp:409]     Test net output #1: loss = 0.33218 (* 1 = 0.33218 loss)
I0526 06:37:06.649039 27645 solver.cpp:237] Iteration 810000, loss = 1.17055
I0526 06:37:06.649103 27645 solver.cpp:253]     Train net output #0: loss = 1.17054 (* 1 = 1.17054 loss)
I0526 06:37:06.649132 27645 sgd_solver.cpp:106] Iteration 810000, lr = 0.0015
I0526 06:37:23.273516 27645 solver.cpp:237] Iteration 811500, loss = 1.14863
I0526 06:37:23.273705 27645 solver.cpp:253]     Train net output #0: loss = 1.14862 (* 1 = 1.14862 loss)
I0526 06:37:23.273722 27645 sgd_solver.cpp:106] Iteration 811500, lr = 0.0015
I0526 06:37:39.912930 27645 solver.cpp:237] Iteration 813000, loss = 0.916034
I0526 06:37:39.912969 27645 solver.cpp:253]     Train net output #0: loss = 0.91603 (* 1 = 0.91603 loss)
I0526 06:37:39.912986 27645 sgd_solver.cpp:106] Iteration 813000, lr = 0.0015
I0526 06:37:56.693712 27645 solver.cpp:237] Iteration 814500, loss = 0.879715
I0526 06:37:56.693898 27645 solver.cpp:253]     Train net output #0: loss = 0.879711 (* 1 = 0.879711 loss)
I0526 06:37:56.693915 27645 sgd_solver.cpp:106] Iteration 814500, lr = 0.0015
I0526 06:38:13.566889 27645 solver.cpp:237] Iteration 816000, loss = 1.43662
I0526 06:38:13.566946 27645 solver.cpp:253]     Train net output #0: loss = 1.43662 (* 1 = 1.43662 loss)
I0526 06:38:13.566972 27645 sgd_solver.cpp:106] Iteration 816000, lr = 0.0015
I0526 06:38:30.523747 27645 solver.cpp:237] Iteration 817500, loss = 0.870463
I0526 06:38:30.523926 27645 solver.cpp:253]     Train net output #0: loss = 0.870459 (* 1 = 0.870459 loss)
I0526 06:38:30.523943 27645 sgd_solver.cpp:106] Iteration 817500, lr = 0.0015
I0526 06:38:47.265287 27645 solver.cpp:237] Iteration 819000, loss = 0.943701
I0526 06:38:47.265346 27645 solver.cpp:253]     Train net output #0: loss = 0.943698 (* 1 = 0.943698 loss)
I0526 06:38:47.265363 27645 sgd_solver.cpp:106] Iteration 819000, lr = 0.0015
I0526 06:39:24.781205 27645 solver.cpp:237] Iteration 820500, loss = 1.33033
I0526 06:39:24.781412 27645 solver.cpp:253]     Train net output #0: loss = 1.33033 (* 1 = 1.33033 loss)
I0526 06:39:24.781430 27645 sgd_solver.cpp:106] Iteration 820500, lr = 0.0015
I0526 06:39:41.658484 27645 solver.cpp:237] Iteration 822000, loss = 0.61453
I0526 06:39:41.658543 27645 solver.cpp:253]     Train net output #0: loss = 0.614527 (* 1 = 0.614527 loss)
I0526 06:39:41.658561 27645 sgd_solver.cpp:106] Iteration 822000, lr = 0.0015
I0526 06:39:58.409384 27645 solver.cpp:237] Iteration 823500, loss = 1.4629
I0526 06:39:58.409569 27645 solver.cpp:253]     Train net output #0: loss = 1.4629 (* 1 = 1.4629 loss)
I0526 06:39:58.409586 27645 sgd_solver.cpp:106] Iteration 823500, lr = 0.0015
I0526 06:40:15.055049 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_825000.caffemodel
I0526 06:40:15.110836 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_825000.solverstate
I0526 06:40:15.151990 27645 solver.cpp:237] Iteration 825000, loss = 1.03788
I0526 06:40:15.152055 27645 solver.cpp:253]     Train net output #0: loss = 1.03788 (* 1 = 1.03788 loss)
I0526 06:40:15.152071 27645 sgd_solver.cpp:106] Iteration 825000, lr = 0.0015
I0526 06:40:32.018704 27645 solver.cpp:237] Iteration 826500, loss = 1.21397
I0526 06:40:32.018892 27645 solver.cpp:253]     Train net output #0: loss = 1.21397 (* 1 = 1.21397 loss)
I0526 06:40:32.018908 27645 sgd_solver.cpp:106] Iteration 826500, lr = 0.0015
I0526 06:40:49.061512 27645 solver.cpp:237] Iteration 828000, loss = 0.894987
I0526 06:40:49.061573 27645 solver.cpp:253]     Train net output #0: loss = 0.894985 (* 1 = 0.894985 loss)
I0526 06:40:49.061591 27645 sgd_solver.cpp:106] Iteration 828000, lr = 0.0015
I0526 06:41:06.265008 27645 solver.cpp:237] Iteration 829500, loss = 1.06956
I0526 06:41:06.265179 27645 solver.cpp:253]     Train net output #0: loss = 1.06956 (* 1 = 1.06956 loss)
I0526 06:41:06.265197 27645 sgd_solver.cpp:106] Iteration 829500, lr = 0.0015
I0526 06:41:44.260200 27645 solver.cpp:237] Iteration 831000, loss = 0.825228
I0526 06:41:44.260392 27645 solver.cpp:253]     Train net output #0: loss = 0.825225 (* 1 = 0.825225 loss)
I0526 06:41:44.260409 27645 sgd_solver.cpp:106] Iteration 831000, lr = 0.0015
I0526 06:42:01.231092 27645 solver.cpp:237] Iteration 832500, loss = 0.855427
I0526 06:42:01.231153 27645 solver.cpp:253]     Train net output #0: loss = 0.855424 (* 1 = 0.855424 loss)
I0526 06:42:01.231178 27645 sgd_solver.cpp:106] Iteration 832500, lr = 0.0015
I0526 06:42:17.868751 27645 solver.cpp:237] Iteration 834000, loss = 1.13259
I0526 06:42:17.868916 27645 solver.cpp:253]     Train net output #0: loss = 1.13259 (* 1 = 1.13259 loss)
I0526 06:42:17.868933 27645 sgd_solver.cpp:106] Iteration 834000, lr = 0.0015
I0526 06:42:34.738171 27645 solver.cpp:237] Iteration 835500, loss = 1.14775
I0526 06:42:34.738225 27645 solver.cpp:253]     Train net output #0: loss = 1.14775 (* 1 = 1.14775 loss)
I0526 06:42:34.738243 27645 sgd_solver.cpp:106] Iteration 835500, lr = 0.0015
I0526 06:42:51.573971 27645 solver.cpp:237] Iteration 837000, loss = 1.2918
I0526 06:42:51.574168 27645 solver.cpp:253]     Train net output #0: loss = 1.29179 (* 1 = 1.29179 loss)
I0526 06:42:51.574187 27645 sgd_solver.cpp:106] Iteration 837000, lr = 0.0015
I0526 06:43:08.205535 27645 solver.cpp:237] Iteration 838500, loss = 0.975259
I0526 06:43:08.205574 27645 solver.cpp:253]     Train net output #0: loss = 0.975257 (* 1 = 0.975257 loss)
I0526 06:43:08.205592 27645 sgd_solver.cpp:106] Iteration 838500, lr = 0.0015
I0526 06:43:25.017402 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_840000.caffemodel
I0526 06:43:25.069480 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_840000.solverstate
I0526 06:43:25.103196 27645 solver.cpp:341] Iteration 840000, Testing net (#0)
I0526 06:44:24.713089 27645 solver.cpp:409]     Test net output #0: accuracy = 0.902628
I0526 06:44:24.713291 27645 solver.cpp:409]     Test net output #1: loss = 0.311327 (* 1 = 0.311327 loss)
I0526 06:44:45.572654 27645 solver.cpp:237] Iteration 840000, loss = 1.30216
I0526 06:44:45.572715 27645 solver.cpp:253]     Train net output #0: loss = 1.30216 (* 1 = 1.30216 loss)
I0526 06:44:45.572739 27645 sgd_solver.cpp:106] Iteration 840000, lr = 0.0015
I0526 06:45:02.546912 27645 solver.cpp:237] Iteration 841500, loss = 0.898139
I0526 06:45:02.547101 27645 solver.cpp:253]     Train net output #0: loss = 0.898136 (* 1 = 0.898136 loss)
I0526 06:45:02.547117 27645 sgd_solver.cpp:106] Iteration 841500, lr = 0.0015
I0526 06:45:19.471063 27645 solver.cpp:237] Iteration 843000, loss = 1.3342
I0526 06:45:19.471119 27645 solver.cpp:253]     Train net output #0: loss = 1.3342 (* 1 = 1.3342 loss)
I0526 06:45:19.471148 27645 sgd_solver.cpp:106] Iteration 843000, lr = 0.0015
I0526 06:45:36.357779 27645 solver.cpp:237] Iteration 844500, loss = 1.40039
I0526 06:45:36.357949 27645 solver.cpp:253]     Train net output #0: loss = 1.40039 (* 1 = 1.40039 loss)
I0526 06:45:36.357964 27645 sgd_solver.cpp:106] Iteration 844500, lr = 0.0015
I0526 06:45:53.176553 27645 solver.cpp:237] Iteration 846000, loss = 0.931643
I0526 06:45:53.176609 27645 solver.cpp:253]     Train net output #0: loss = 0.931641 (* 1 = 0.931641 loss)
I0526 06:45:53.176627 27645 sgd_solver.cpp:106] Iteration 846000, lr = 0.0015
I0526 06:46:10.015449 27645 solver.cpp:237] Iteration 847500, loss = 1.49259
I0526 06:46:10.015636 27645 solver.cpp:253]     Train net output #0: loss = 1.49258 (* 1 = 1.49258 loss)
I0526 06:46:10.015655 27645 sgd_solver.cpp:106] Iteration 847500, lr = 0.0015
I0526 06:46:26.972360 27645 solver.cpp:237] Iteration 849000, loss = 1.2555
I0526 06:46:26.972399 27645 solver.cpp:253]     Train net output #0: loss = 1.25549 (* 1 = 1.25549 loss)
I0526 06:46:26.972417 27645 sgd_solver.cpp:106] Iteration 849000, lr = 0.0015
I0526 06:47:04.901396 27645 solver.cpp:237] Iteration 850500, loss = 1.23148
I0526 06:47:04.901589 27645 solver.cpp:253]     Train net output #0: loss = 1.23148 (* 1 = 1.23148 loss)
I0526 06:47:04.901607 27645 sgd_solver.cpp:106] Iteration 850500, lr = 0.0015
I0526 06:47:21.980515 27645 solver.cpp:237] Iteration 852000, loss = 0.582158
I0526 06:47:21.980574 27645 solver.cpp:253]     Train net output #0: loss = 0.582155 (* 1 = 0.582155 loss)
I0526 06:47:21.980600 27645 sgd_solver.cpp:106] Iteration 852000, lr = 0.0015
I0526 06:47:38.619655 27645 solver.cpp:237] Iteration 853500, loss = 0.693725
I0526 06:47:38.619827 27645 solver.cpp:253]     Train net output #0: loss = 0.693722 (* 1 = 0.693722 loss)
I0526 06:47:38.619843 27645 sgd_solver.cpp:106] Iteration 853500, lr = 0.0015
I0526 06:47:55.399629 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_855000.caffemodel
I0526 06:47:55.455358 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_855000.solverstate
I0526 06:47:55.496580 27645 solver.cpp:237] Iteration 855000, loss = 1.58561
I0526 06:47:55.496639 27645 solver.cpp:253]     Train net output #0: loss = 1.58561 (* 1 = 1.58561 loss)
I0526 06:47:55.496655 27645 sgd_solver.cpp:106] Iteration 855000, lr = 0.0015
I0526 06:48:12.483958 27645 solver.cpp:237] Iteration 856500, loss = 0.527105
I0526 06:48:12.484163 27645 solver.cpp:253]     Train net output #0: loss = 0.527101 (* 1 = 0.527101 loss)
I0526 06:48:12.484180 27645 sgd_solver.cpp:106] Iteration 856500, lr = 0.0015
I0526 06:48:29.355126 27645 solver.cpp:237] Iteration 858000, loss = 1.42792
I0526 06:48:29.355166 27645 solver.cpp:253]     Train net output #0: loss = 1.42791 (* 1 = 1.42791 loss)
I0526 06:48:29.355185 27645 sgd_solver.cpp:106] Iteration 858000, lr = 0.0015
I0526 06:48:46.312518 27645 solver.cpp:237] Iteration 859500, loss = 1.04033
I0526 06:48:46.312703 27645 solver.cpp:253]     Train net output #0: loss = 1.04033 (* 1 = 1.04033 loss)
I0526 06:48:46.312721 27645 sgd_solver.cpp:106] Iteration 859500, lr = 0.0015
I0526 06:49:24.176517 27645 solver.cpp:237] Iteration 861000, loss = 0.902072
I0526 06:49:24.176712 27645 solver.cpp:253]     Train net output #0: loss = 0.902069 (* 1 = 0.902069 loss)
I0526 06:49:24.176730 27645 sgd_solver.cpp:106] Iteration 861000, lr = 0.0015
I0526 06:49:40.833602 27645 solver.cpp:237] Iteration 862500, loss = 1.88432
I0526 06:49:40.833657 27645 solver.cpp:253]     Train net output #0: loss = 1.88431 (* 1 = 1.88431 loss)
I0526 06:49:40.833674 27645 sgd_solver.cpp:106] Iteration 862500, lr = 0.0015
I0526 06:49:57.589543 27645 solver.cpp:237] Iteration 864000, loss = 2.31954
I0526 06:49:57.589730 27645 solver.cpp:253]     Train net output #0: loss = 2.31954 (* 1 = 2.31954 loss)
I0526 06:49:57.589747 27645 sgd_solver.cpp:106] Iteration 864000, lr = 0.0015
I0526 06:50:14.459403 27645 solver.cpp:237] Iteration 865500, loss = 1.09252
I0526 06:50:14.459441 27645 solver.cpp:253]     Train net output #0: loss = 1.09252 (* 1 = 1.09252 loss)
I0526 06:50:14.459460 27645 sgd_solver.cpp:106] Iteration 865500, lr = 0.0015
I0526 06:50:31.111186 27645 solver.cpp:237] Iteration 867000, loss = 1.5108
I0526 06:50:31.111377 27645 solver.cpp:253]     Train net output #0: loss = 1.5108 (* 1 = 1.5108 loss)
I0526 06:50:31.111393 27645 sgd_solver.cpp:106] Iteration 867000, lr = 0.0015
I0526 06:50:47.731446 27645 solver.cpp:237] Iteration 868500, loss = 0.791571
I0526 06:50:47.731503 27645 solver.cpp:253]     Train net output #0: loss = 0.791568 (* 1 = 0.791568 loss)
I0526 06:50:47.731529 27645 sgd_solver.cpp:106] Iteration 868500, lr = 0.0015
I0526 06:51:04.381537 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_870000.caffemodel
I0526 06:51:04.435542 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_870000.solverstate
I0526 06:51:04.471880 27645 solver.cpp:341] Iteration 870000, Testing net (#0)
I0526 06:52:24.695643 27645 solver.cpp:409]     Test net output #0: accuracy = 0.904087
I0526 06:52:24.695850 27645 solver.cpp:409]     Test net output #1: loss = 0.302282 (* 1 = 0.302282 loss)
I0526 06:52:45.564870 27645 solver.cpp:237] Iteration 870000, loss = 2.39621
I0526 06:52:45.564934 27645 solver.cpp:253]     Train net output #0: loss = 2.3962 (* 1 = 2.3962 loss)
I0526 06:52:45.564960 27645 sgd_solver.cpp:106] Iteration 870000, lr = 0.0015
I0526 06:53:02.402789 27645 solver.cpp:237] Iteration 871500, loss = 0.669839
I0526 06:53:02.402979 27645 solver.cpp:253]     Train net output #0: loss = 0.669836 (* 1 = 0.669836 loss)
I0526 06:53:02.402997 27645 sgd_solver.cpp:106] Iteration 871500, lr = 0.0015
I0526 06:53:19.403241 27645 solver.cpp:237] Iteration 873000, loss = 0.639529
I0526 06:53:19.403297 27645 solver.cpp:253]     Train net output #0: loss = 0.639526 (* 1 = 0.639526 loss)
I0526 06:53:19.403316 27645 sgd_solver.cpp:106] Iteration 873000, lr = 0.0015
I0526 06:53:36.466963 27645 solver.cpp:237] Iteration 874500, loss = 1.08821
I0526 06:53:36.467142 27645 solver.cpp:253]     Train net output #0: loss = 1.0882 (* 1 = 1.0882 loss)
I0526 06:53:36.467159 27645 sgd_solver.cpp:106] Iteration 874500, lr = 0.0015
I0526 06:53:53.097964 27645 solver.cpp:237] Iteration 876000, loss = 0.603435
I0526 06:53:53.098018 27645 solver.cpp:253]     Train net output #0: loss = 0.603433 (* 1 = 0.603433 loss)
I0526 06:53:53.098036 27645 sgd_solver.cpp:106] Iteration 876000, lr = 0.0015
I0526 06:54:09.708937 27645 solver.cpp:237] Iteration 877500, loss = 1.1187
I0526 06:54:09.709128 27645 solver.cpp:253]     Train net output #0: loss = 1.11869 (* 1 = 1.11869 loss)
I0526 06:54:09.709146 27645 sgd_solver.cpp:106] Iteration 877500, lr = 0.0015
I0526 06:54:26.367597 27645 solver.cpp:237] Iteration 879000, loss = 0.279076
I0526 06:54:26.367635 27645 solver.cpp:253]     Train net output #0: loss = 0.279074 (* 1 = 0.279074 loss)
I0526 06:54:26.367652 27645 sgd_solver.cpp:106] Iteration 879000, lr = 0.0015
I0526 06:55:04.329131 27645 solver.cpp:237] Iteration 880500, loss = 0.941503
I0526 06:55:04.329329 27645 solver.cpp:253]     Train net output #0: loss = 0.9415 (* 1 = 0.9415 loss)
I0526 06:55:04.329349 27645 sgd_solver.cpp:106] Iteration 880500, lr = 0.0015
I0526 06:55:21.258954 27645 solver.cpp:237] Iteration 882000, loss = 0.89415
I0526 06:55:21.259009 27645 solver.cpp:253]     Train net output #0: loss = 0.894148 (* 1 = 0.894148 loss)
I0526 06:55:21.259033 27645 sgd_solver.cpp:106] Iteration 882000, lr = 0.0015
I0526 06:55:37.890240 27645 solver.cpp:237] Iteration 883500, loss = 1.0641
I0526 06:55:37.890408 27645 solver.cpp:253]     Train net output #0: loss = 1.0641 (* 1 = 1.0641 loss)
I0526 06:55:37.890424 27645 sgd_solver.cpp:106] Iteration 883500, lr = 0.0015
I0526 06:55:54.693593 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_885000.caffemodel
I0526 06:55:54.750023 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_885000.solverstate
I0526 06:55:54.791720 27645 solver.cpp:237] Iteration 885000, loss = 1.80771
I0526 06:55:54.791784 27645 solver.cpp:253]     Train net output #0: loss = 1.80771 (* 1 = 1.80771 loss)
I0526 06:55:54.791801 27645 sgd_solver.cpp:106] Iteration 885000, lr = 0.0015
I0526 06:56:11.595041 27645 solver.cpp:237] Iteration 886500, loss = 1.11012
I0526 06:56:11.595235 27645 solver.cpp:253]     Train net output #0: loss = 1.11012 (* 1 = 1.11012 loss)
I0526 06:56:11.595254 27645 sgd_solver.cpp:106] Iteration 886500, lr = 0.0015
I0526 06:56:28.232264 27645 solver.cpp:237] Iteration 888000, loss = 2.45429
I0526 06:56:28.232303 27645 solver.cpp:253]     Train net output #0: loss = 2.45429 (* 1 = 2.45429 loss)
I0526 06:56:28.232327 27645 sgd_solver.cpp:106] Iteration 888000, lr = 0.0015
I0526 06:56:44.874974 27645 solver.cpp:237] Iteration 889500, loss = 0.949839
I0526 06:56:44.875165 27645 solver.cpp:253]     Train net output #0: loss = 0.949838 (* 1 = 0.949838 loss)
I0526 06:56:44.875182 27645 sgd_solver.cpp:106] Iteration 889500, lr = 0.0015
I0526 06:57:22.411275 27645 solver.cpp:237] Iteration 891000, loss = 1.11907
I0526 06:57:22.411468 27645 solver.cpp:253]     Train net output #0: loss = 1.11907 (* 1 = 1.11907 loss)
I0526 06:57:22.411485 27645 sgd_solver.cpp:106] Iteration 891000, lr = 0.0015
I0526 06:57:39.471932 27645 solver.cpp:237] Iteration 892500, loss = 1.22488
I0526 06:57:39.471972 27645 solver.cpp:253]     Train net output #0: loss = 1.22487 (* 1 = 1.22487 loss)
I0526 06:57:39.471989 27645 sgd_solver.cpp:106] Iteration 892500, lr = 0.0015
I0526 06:57:56.441067 27645 solver.cpp:237] Iteration 894000, loss = 1.40874
I0526 06:57:56.441268 27645 solver.cpp:253]     Train net output #0: loss = 1.40874 (* 1 = 1.40874 loss)
I0526 06:57:56.441285 27645 sgd_solver.cpp:106] Iteration 894000, lr = 0.0015
I0526 06:58:13.314275 27645 solver.cpp:237] Iteration 895500, loss = 1.29754
I0526 06:58:13.314332 27645 solver.cpp:253]     Train net output #0: loss = 1.29754 (* 1 = 1.29754 loss)
I0526 06:58:13.314357 27645 sgd_solver.cpp:106] Iteration 895500, lr = 0.0015
I0526 06:58:29.945713 27645 solver.cpp:237] Iteration 897000, loss = 1.51338
I0526 06:58:29.945904 27645 solver.cpp:253]     Train net output #0: loss = 1.51338 (* 1 = 1.51338 loss)
I0526 06:58:29.945921 27645 sgd_solver.cpp:106] Iteration 897000, lr = 0.0015
I0526 06:58:46.952842 27645 solver.cpp:237] Iteration 898500, loss = 0.866705
I0526 06:58:46.952895 27645 solver.cpp:253]     Train net output #0: loss = 0.866704 (* 1 = 0.866704 loss)
I0526 06:58:46.952913 27645 sgd_solver.cpp:106] Iteration 898500, lr = 0.0015
I0526 06:59:04.159289 27645 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_900000.caffemodel
I0526 06:59:04.217838 27645 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0015_2016-05-20T15.48.50.039036_iter_900000.solverstate
I0526 06:59:04.254093 27645 solver.cpp:341] Iteration 900000, Testing net (#0)
I0526 07:00:03.773197 27645 solver.cpp:409]     Test net output #0: accuracy = 0.903706
I0526 07:00:03.773387 27645 solver.cpp:409]     Test net output #1: loss = 0.307143 (* 1 = 0.307143 loss)
I0526 07:00:24.658725 27645 solver.cpp:237] Iteration 900000, loss = 0.909
I0526 07:00:24.658785 27645 solver.cpp:253]     Train net output #0: loss = 0.908999 (* 1 = 0.908999 loss)
I0526 07:00:24.658805 27645 sgd_solver.cpp:106] Iteration 900000, lr = 0.0015
I0526 07:00:41.311615 27645 solver.cpp:237] Iteration 901500, loss = 1.01632
I0526 07:00:41.311790 27645 solver.cpp:253]     Train net output #0: loss = 1.01632 (* 1 = 1.01632 loss)
I0526 07:00:41.311807 27645 sgd_solver.cpp:106] Iteration 901500, lr = 0.0015
I0526 07:00:58.490031 27645 solver.cpp:237] Iteration 903000, loss = 1.50243
I0526 07:00:58.490084 27645 solver.cpp:253]     Train net output #0: loss = 1.50243 (* 1 = 1.50243 loss)
I0526 07:00:58.490103 27645 sgd_solver.cpp:106] Iteration 903000, lr = 0.0015
I0526 07:01:15.428313 27645 solver.cpp:237] Iteration 904500, loss = 1.18614
I0526 07:01:15.428504 27645 solver.cpp:253]     Train net output #0: loss = 1.18614 (* 1 = 1.18614 loss)
I0526 07:01:15.428521 27645 sgd_solver.cpp:106] Iteration 904500, lr = 0.0015
aprun: Apid 11266887: Caught signal Terminated, sending to application
*** Aborted at 1464260485 (unix time) try "date -d @1464260485" if you are using GNU date ***
aprun: Apid 11266887: Caught signal Terminated, sending to application
aprun: Apid 11266887: Caught signal Terminated, sending to application
PC: @     0x2aaab9899078 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
*** SIGTERM (@0x6bfa) received by PID 27645 (TID 0x2aaac746f900) from PID 27642; stack trace: ***
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab7c78850 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab9899078 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab929fa43 (unknown)
=>> PBS: job killed: walltime 7237 exceeded limit 7200
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab929fbd2 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab929fd28 (unknown)
    @     0x2aaab92c4e7c (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab92607ce (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab937f75d (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab92c66e9 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab9266e14 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab91e154c (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab91e15fe (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaab91b7bea cuMemsetD8_v2
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaaaacf7de2 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaaaacde692 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @     0x2aaaaacfe8e5 cudaMemset
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @           0x610a69 caffe::caffe_gpu_set<>()
aprun: Apid 11266887: Caught signal Terminated, sending to application
    @           0x5ec0d4 caffe::Net<>::ClearParamDiffs()
    @           0x5c9fbb caffe::Solver<>::Step()
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
    @           0x438669 (unknown)
aprun: Apid 11266887: Caught signal Terminated, sending to application
