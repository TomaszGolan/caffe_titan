2807571
I0522 14:09:20.424381 27620 caffe.cpp:184] Using GPUs 0
I0522 14:09:20.847383 27620 solver.cpp:48] Initializing solver from parameters: 
test_iter: 5000
test_interval: 10000
base_lr: 0.0045
display: 500
max_iter: 500000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 5000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182.prototxt"
I0522 14:09:20.849441 27620 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182.prototxt
I0522 14:09:20.862865 27620 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0522 14:09:20.862925 27620 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0522 14:09:20.863271 27620 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 30
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0522 14:09:20.863451 27620 layer_factory.hpp:77] Creating layer data_hdf5
I0522 14:09:20.863474 27620 net.cpp:106] Creating Layer data_hdf5
I0522 14:09:20.863489 27620 net.cpp:411] data_hdf5 -> data
I0522 14:09:20.863523 27620 net.cpp:411] data_hdf5 -> label
I0522 14:09:20.863556 27620 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0522 14:09:20.865494 27620 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0522 14:09:20.867776 27620 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0522 14:09:42.382673 27620 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0522 14:09:42.387785 27620 net.cpp:150] Setting up data_hdf5
I0522 14:09:42.387825 27620 net.cpp:157] Top shape: 30 1 127 50 (190500)
I0522 14:09:42.387840 27620 net.cpp:157] Top shape: 30 (30)
I0522 14:09:42.387850 27620 net.cpp:165] Memory required for data: 762120
I0522 14:09:42.387863 27620 layer_factory.hpp:77] Creating layer conv1
I0522 14:09:42.387897 27620 net.cpp:106] Creating Layer conv1
I0522 14:09:42.387909 27620 net.cpp:454] conv1 <- data
I0522 14:09:42.387933 27620 net.cpp:411] conv1 -> conv1
I0522 14:09:42.751528 27620 net.cpp:150] Setting up conv1
I0522 14:09:42.751575 27620 net.cpp:157] Top shape: 30 12 120 48 (2073600)
I0522 14:09:42.751586 27620 net.cpp:165] Memory required for data: 9056520
I0522 14:09:42.751616 27620 layer_factory.hpp:77] Creating layer relu1
I0522 14:09:42.751637 27620 net.cpp:106] Creating Layer relu1
I0522 14:09:42.751648 27620 net.cpp:454] relu1 <- conv1
I0522 14:09:42.751662 27620 net.cpp:397] relu1 -> conv1 (in-place)
I0522 14:09:42.752177 27620 net.cpp:150] Setting up relu1
I0522 14:09:42.752193 27620 net.cpp:157] Top shape: 30 12 120 48 (2073600)
I0522 14:09:42.752203 27620 net.cpp:165] Memory required for data: 17350920
I0522 14:09:42.752215 27620 layer_factory.hpp:77] Creating layer pool1
I0522 14:09:42.752231 27620 net.cpp:106] Creating Layer pool1
I0522 14:09:42.752241 27620 net.cpp:454] pool1 <- conv1
I0522 14:09:42.752256 27620 net.cpp:411] pool1 -> pool1
I0522 14:09:42.752336 27620 net.cpp:150] Setting up pool1
I0522 14:09:42.752349 27620 net.cpp:157] Top shape: 30 12 60 48 (1036800)
I0522 14:09:42.752359 27620 net.cpp:165] Memory required for data: 21498120
I0522 14:09:42.752370 27620 layer_factory.hpp:77] Creating layer conv2
I0522 14:09:42.752393 27620 net.cpp:106] Creating Layer conv2
I0522 14:09:42.752403 27620 net.cpp:454] conv2 <- pool1
I0522 14:09:42.752414 27620 net.cpp:411] conv2 -> conv2
I0522 14:09:42.755096 27620 net.cpp:150] Setting up conv2
I0522 14:09:42.755125 27620 net.cpp:157] Top shape: 30 20 54 46 (1490400)
I0522 14:09:42.755136 27620 net.cpp:165] Memory required for data: 27459720
I0522 14:09:42.755154 27620 layer_factory.hpp:77] Creating layer relu2
I0522 14:09:42.755169 27620 net.cpp:106] Creating Layer relu2
I0522 14:09:42.755179 27620 net.cpp:454] relu2 <- conv2
I0522 14:09:42.755192 27620 net.cpp:397] relu2 -> conv2 (in-place)
I0522 14:09:42.755520 27620 net.cpp:150] Setting up relu2
I0522 14:09:42.755535 27620 net.cpp:157] Top shape: 30 20 54 46 (1490400)
I0522 14:09:42.755545 27620 net.cpp:165] Memory required for data: 33421320
I0522 14:09:42.755556 27620 layer_factory.hpp:77] Creating layer pool2
I0522 14:09:42.755568 27620 net.cpp:106] Creating Layer pool2
I0522 14:09:42.755579 27620 net.cpp:454] pool2 <- conv2
I0522 14:09:42.755592 27620 net.cpp:411] pool2 -> pool2
I0522 14:09:42.755672 27620 net.cpp:150] Setting up pool2
I0522 14:09:42.755686 27620 net.cpp:157] Top shape: 30 20 27 46 (745200)
I0522 14:09:42.755697 27620 net.cpp:165] Memory required for data: 36402120
I0522 14:09:42.755707 27620 layer_factory.hpp:77] Creating layer conv3
I0522 14:09:42.755724 27620 net.cpp:106] Creating Layer conv3
I0522 14:09:42.755734 27620 net.cpp:454] conv3 <- pool2
I0522 14:09:42.755748 27620 net.cpp:411] conv3 -> conv3
I0522 14:09:42.757690 27620 net.cpp:150] Setting up conv3
I0522 14:09:42.757714 27620 net.cpp:157] Top shape: 30 28 22 44 (813120)
I0522 14:09:42.757725 27620 net.cpp:165] Memory required for data: 39654600
I0522 14:09:42.757743 27620 layer_factory.hpp:77] Creating layer relu3
I0522 14:09:42.757761 27620 net.cpp:106] Creating Layer relu3
I0522 14:09:42.757771 27620 net.cpp:454] relu3 <- conv3
I0522 14:09:42.757782 27620 net.cpp:397] relu3 -> conv3 (in-place)
I0522 14:09:42.758257 27620 net.cpp:150] Setting up relu3
I0522 14:09:42.758275 27620 net.cpp:157] Top shape: 30 28 22 44 (813120)
I0522 14:09:42.758285 27620 net.cpp:165] Memory required for data: 42907080
I0522 14:09:42.758296 27620 layer_factory.hpp:77] Creating layer pool3
I0522 14:09:42.758308 27620 net.cpp:106] Creating Layer pool3
I0522 14:09:42.758318 27620 net.cpp:454] pool3 <- conv3
I0522 14:09:42.758332 27620 net.cpp:411] pool3 -> pool3
I0522 14:09:42.758399 27620 net.cpp:150] Setting up pool3
I0522 14:09:42.758412 27620 net.cpp:157] Top shape: 30 28 11 44 (406560)
I0522 14:09:42.758422 27620 net.cpp:165] Memory required for data: 44533320
I0522 14:09:42.758431 27620 layer_factory.hpp:77] Creating layer conv4
I0522 14:09:42.758450 27620 net.cpp:106] Creating Layer conv4
I0522 14:09:42.758460 27620 net.cpp:454] conv4 <- pool3
I0522 14:09:42.758473 27620 net.cpp:411] conv4 -> conv4
I0522 14:09:42.761185 27620 net.cpp:150] Setting up conv4
I0522 14:09:42.761214 27620 net.cpp:157] Top shape: 30 36 6 42 (272160)
I0522 14:09:42.761224 27620 net.cpp:165] Memory required for data: 45621960
I0522 14:09:42.761240 27620 layer_factory.hpp:77] Creating layer relu4
I0522 14:09:42.761253 27620 net.cpp:106] Creating Layer relu4
I0522 14:09:42.761263 27620 net.cpp:454] relu4 <- conv4
I0522 14:09:42.761276 27620 net.cpp:397] relu4 -> conv4 (in-place)
I0522 14:09:42.761739 27620 net.cpp:150] Setting up relu4
I0522 14:09:42.761755 27620 net.cpp:157] Top shape: 30 36 6 42 (272160)
I0522 14:09:42.761766 27620 net.cpp:165] Memory required for data: 46710600
I0522 14:09:42.761777 27620 layer_factory.hpp:77] Creating layer pool4
I0522 14:09:42.761790 27620 net.cpp:106] Creating Layer pool4
I0522 14:09:42.761800 27620 net.cpp:454] pool4 <- conv4
I0522 14:09:42.761812 27620 net.cpp:411] pool4 -> pool4
I0522 14:09:42.761880 27620 net.cpp:150] Setting up pool4
I0522 14:09:42.761893 27620 net.cpp:157] Top shape: 30 36 3 42 (136080)
I0522 14:09:42.761904 27620 net.cpp:165] Memory required for data: 47254920
I0522 14:09:42.761914 27620 layer_factory.hpp:77] Creating layer ip1
I0522 14:09:42.761932 27620 net.cpp:106] Creating Layer ip1
I0522 14:09:42.761942 27620 net.cpp:454] ip1 <- pool4
I0522 14:09:42.761955 27620 net.cpp:411] ip1 -> ip1
I0522 14:09:42.777371 27620 net.cpp:150] Setting up ip1
I0522 14:09:42.777400 27620 net.cpp:157] Top shape: 30 196 (5880)
I0522 14:09:42.777411 27620 net.cpp:165] Memory required for data: 47278440
I0522 14:09:42.777434 27620 layer_factory.hpp:77] Creating layer relu5
I0522 14:09:42.777449 27620 net.cpp:106] Creating Layer relu5
I0522 14:09:42.777459 27620 net.cpp:454] relu5 <- ip1
I0522 14:09:42.777472 27620 net.cpp:397] relu5 -> ip1 (in-place)
I0522 14:09:42.777812 27620 net.cpp:150] Setting up relu5
I0522 14:09:42.777827 27620 net.cpp:157] Top shape: 30 196 (5880)
I0522 14:09:42.777837 27620 net.cpp:165] Memory required for data: 47301960
I0522 14:09:42.777848 27620 layer_factory.hpp:77] Creating layer drop1
I0522 14:09:42.777868 27620 net.cpp:106] Creating Layer drop1
I0522 14:09:42.777878 27620 net.cpp:454] drop1 <- ip1
I0522 14:09:42.777890 27620 net.cpp:397] drop1 -> ip1 (in-place)
I0522 14:09:42.777951 27620 net.cpp:150] Setting up drop1
I0522 14:09:42.777964 27620 net.cpp:157] Top shape: 30 196 (5880)
I0522 14:09:42.777974 27620 net.cpp:165] Memory required for data: 47325480
I0522 14:09:42.777984 27620 layer_factory.hpp:77] Creating layer ip2
I0522 14:09:42.778002 27620 net.cpp:106] Creating Layer ip2
I0522 14:09:42.778013 27620 net.cpp:454] ip2 <- ip1
I0522 14:09:42.778026 27620 net.cpp:411] ip2 -> ip2
I0522 14:09:42.778493 27620 net.cpp:150] Setting up ip2
I0522 14:09:42.778506 27620 net.cpp:157] Top shape: 30 98 (2940)
I0522 14:09:42.778517 27620 net.cpp:165] Memory required for data: 47337240
I0522 14:09:42.778532 27620 layer_factory.hpp:77] Creating layer relu6
I0522 14:09:42.778544 27620 net.cpp:106] Creating Layer relu6
I0522 14:09:42.778554 27620 net.cpp:454] relu6 <- ip2
I0522 14:09:42.778566 27620 net.cpp:397] relu6 -> ip2 (in-place)
I0522 14:09:42.779079 27620 net.cpp:150] Setting up relu6
I0522 14:09:42.779095 27620 net.cpp:157] Top shape: 30 98 (2940)
I0522 14:09:42.779108 27620 net.cpp:165] Memory required for data: 47349000
I0522 14:09:42.779119 27620 layer_factory.hpp:77] Creating layer drop2
I0522 14:09:42.779131 27620 net.cpp:106] Creating Layer drop2
I0522 14:09:42.779141 27620 net.cpp:454] drop2 <- ip2
I0522 14:09:42.779153 27620 net.cpp:397] drop2 -> ip2 (in-place)
I0522 14:09:42.779196 27620 net.cpp:150] Setting up drop2
I0522 14:09:42.779209 27620 net.cpp:157] Top shape: 30 98 (2940)
I0522 14:09:42.779219 27620 net.cpp:165] Memory required for data: 47360760
I0522 14:09:42.779229 27620 layer_factory.hpp:77] Creating layer ip3
I0522 14:09:42.779242 27620 net.cpp:106] Creating Layer ip3
I0522 14:09:42.779253 27620 net.cpp:454] ip3 <- ip2
I0522 14:09:42.779265 27620 net.cpp:411] ip3 -> ip3
I0522 14:09:42.779475 27620 net.cpp:150] Setting up ip3
I0522 14:09:42.779489 27620 net.cpp:157] Top shape: 30 11 (330)
I0522 14:09:42.779498 27620 net.cpp:165] Memory required for data: 47362080
I0522 14:09:42.779513 27620 layer_factory.hpp:77] Creating layer drop3
I0522 14:09:42.779526 27620 net.cpp:106] Creating Layer drop3
I0522 14:09:42.779536 27620 net.cpp:454] drop3 <- ip3
I0522 14:09:42.779548 27620 net.cpp:397] drop3 -> ip3 (in-place)
I0522 14:09:42.779587 27620 net.cpp:150] Setting up drop3
I0522 14:09:42.779600 27620 net.cpp:157] Top shape: 30 11 (330)
I0522 14:09:42.779610 27620 net.cpp:165] Memory required for data: 47363400
I0522 14:09:42.779619 27620 layer_factory.hpp:77] Creating layer loss
I0522 14:09:42.779639 27620 net.cpp:106] Creating Layer loss
I0522 14:09:42.779649 27620 net.cpp:454] loss <- ip3
I0522 14:09:42.779662 27620 net.cpp:454] loss <- label
I0522 14:09:42.779675 27620 net.cpp:411] loss -> loss
I0522 14:09:42.779691 27620 layer_factory.hpp:77] Creating layer loss
I0522 14:09:42.780333 27620 net.cpp:150] Setting up loss
I0522 14:09:42.780349 27620 net.cpp:157] Top shape: (1)
I0522 14:09:42.780360 27620 net.cpp:160]     with loss weight 1
I0522 14:09:42.780403 27620 net.cpp:165] Memory required for data: 47363404
I0522 14:09:42.780413 27620 net.cpp:226] loss needs backward computation.
I0522 14:09:42.780424 27620 net.cpp:226] drop3 needs backward computation.
I0522 14:09:42.780434 27620 net.cpp:226] ip3 needs backward computation.
I0522 14:09:42.780446 27620 net.cpp:226] drop2 needs backward computation.
I0522 14:09:42.780455 27620 net.cpp:226] relu6 needs backward computation.
I0522 14:09:42.780465 27620 net.cpp:226] ip2 needs backward computation.
I0522 14:09:42.780475 27620 net.cpp:226] drop1 needs backward computation.
I0522 14:09:42.780485 27620 net.cpp:226] relu5 needs backward computation.
I0522 14:09:42.780494 27620 net.cpp:226] ip1 needs backward computation.
I0522 14:09:42.780505 27620 net.cpp:226] pool4 needs backward computation.
I0522 14:09:42.780515 27620 net.cpp:226] relu4 needs backward computation.
I0522 14:09:42.780525 27620 net.cpp:226] conv4 needs backward computation.
I0522 14:09:42.780535 27620 net.cpp:226] pool3 needs backward computation.
I0522 14:09:42.780545 27620 net.cpp:226] relu3 needs backward computation.
I0522 14:09:42.780555 27620 net.cpp:226] conv3 needs backward computation.
I0522 14:09:42.780575 27620 net.cpp:226] pool2 needs backward computation.
I0522 14:09:42.780586 27620 net.cpp:226] relu2 needs backward computation.
I0522 14:09:42.780596 27620 net.cpp:226] conv2 needs backward computation.
I0522 14:09:42.780606 27620 net.cpp:226] pool1 needs backward computation.
I0522 14:09:42.780617 27620 net.cpp:226] relu1 needs backward computation.
I0522 14:09:42.780627 27620 net.cpp:226] conv1 needs backward computation.
I0522 14:09:42.780638 27620 net.cpp:228] data_hdf5 does not need backward computation.
I0522 14:09:42.780648 27620 net.cpp:270] This network produces output loss
I0522 14:09:42.780673 27620 net.cpp:283] Network initialization done.
I0522 14:09:42.782372 27620 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182.prototxt
I0522 14:09:42.782443 27620 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0522 14:09:42.782799 27620 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 30
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0522 14:09:42.782989 27620 layer_factory.hpp:77] Creating layer data_hdf5
I0522 14:09:42.783004 27620 net.cpp:106] Creating Layer data_hdf5
I0522 14:09:42.783017 27620 net.cpp:411] data_hdf5 -> data
I0522 14:09:42.783033 27620 net.cpp:411] data_hdf5 -> label
I0522 14:09:42.783049 27620 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0522 14:09:42.784312 27620 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0522 14:10:04.134793 27620 net.cpp:150] Setting up data_hdf5
I0522 14:10:04.134961 27620 net.cpp:157] Top shape: 30 1 127 50 (190500)
I0522 14:10:04.134976 27620 net.cpp:157] Top shape: 30 (30)
I0522 14:10:04.134987 27620 net.cpp:165] Memory required for data: 762120
I0522 14:10:04.135001 27620 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0522 14:10:04.135030 27620 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0522 14:10:04.135040 27620 net.cpp:454] label_data_hdf5_1_split <- label
I0522 14:10:04.135056 27620 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0522 14:10:04.135077 27620 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0522 14:10:04.135150 27620 net.cpp:150] Setting up label_data_hdf5_1_split
I0522 14:10:04.135164 27620 net.cpp:157] Top shape: 30 (30)
I0522 14:10:04.135175 27620 net.cpp:157] Top shape: 30 (30)
I0522 14:10:04.135185 27620 net.cpp:165] Memory required for data: 762360
I0522 14:10:04.135195 27620 layer_factory.hpp:77] Creating layer conv1
I0522 14:10:04.135217 27620 net.cpp:106] Creating Layer conv1
I0522 14:10:04.135227 27620 net.cpp:454] conv1 <- data
I0522 14:10:04.135242 27620 net.cpp:411] conv1 -> conv1
I0522 14:10:04.137173 27620 net.cpp:150] Setting up conv1
I0522 14:10:04.137197 27620 net.cpp:157] Top shape: 30 12 120 48 (2073600)
I0522 14:10:04.137209 27620 net.cpp:165] Memory required for data: 9056760
I0522 14:10:04.137229 27620 layer_factory.hpp:77] Creating layer relu1
I0522 14:10:04.137244 27620 net.cpp:106] Creating Layer relu1
I0522 14:10:04.137254 27620 net.cpp:454] relu1 <- conv1
I0522 14:10:04.137267 27620 net.cpp:397] relu1 -> conv1 (in-place)
I0522 14:10:04.137764 27620 net.cpp:150] Setting up relu1
I0522 14:10:04.137781 27620 net.cpp:157] Top shape: 30 12 120 48 (2073600)
I0522 14:10:04.137791 27620 net.cpp:165] Memory required for data: 17351160
I0522 14:10:04.137801 27620 layer_factory.hpp:77] Creating layer pool1
I0522 14:10:04.137819 27620 net.cpp:106] Creating Layer pool1
I0522 14:10:04.137827 27620 net.cpp:454] pool1 <- conv1
I0522 14:10:04.137842 27620 net.cpp:411] pool1 -> pool1
I0522 14:10:04.137917 27620 net.cpp:150] Setting up pool1
I0522 14:10:04.137929 27620 net.cpp:157] Top shape: 30 12 60 48 (1036800)
I0522 14:10:04.137939 27620 net.cpp:165] Memory required for data: 21498360
I0522 14:10:04.137951 27620 layer_factory.hpp:77] Creating layer conv2
I0522 14:10:04.137969 27620 net.cpp:106] Creating Layer conv2
I0522 14:10:04.137979 27620 net.cpp:454] conv2 <- pool1
I0522 14:10:04.137994 27620 net.cpp:411] conv2 -> conv2
I0522 14:10:04.139916 27620 net.cpp:150] Setting up conv2
I0522 14:10:04.139940 27620 net.cpp:157] Top shape: 30 20 54 46 (1490400)
I0522 14:10:04.139950 27620 net.cpp:165] Memory required for data: 27459960
I0522 14:10:04.139967 27620 layer_factory.hpp:77] Creating layer relu2
I0522 14:10:04.139981 27620 net.cpp:106] Creating Layer relu2
I0522 14:10:04.139991 27620 net.cpp:454] relu2 <- conv2
I0522 14:10:04.140003 27620 net.cpp:397] relu2 -> conv2 (in-place)
I0522 14:10:04.140333 27620 net.cpp:150] Setting up relu2
I0522 14:10:04.140347 27620 net.cpp:157] Top shape: 30 20 54 46 (1490400)
I0522 14:10:04.140357 27620 net.cpp:165] Memory required for data: 33421560
I0522 14:10:04.140367 27620 layer_factory.hpp:77] Creating layer pool2
I0522 14:10:04.140379 27620 net.cpp:106] Creating Layer pool2
I0522 14:10:04.140390 27620 net.cpp:454] pool2 <- conv2
I0522 14:10:04.140403 27620 net.cpp:411] pool2 -> pool2
I0522 14:10:04.140473 27620 net.cpp:150] Setting up pool2
I0522 14:10:04.140486 27620 net.cpp:157] Top shape: 30 20 27 46 (745200)
I0522 14:10:04.140496 27620 net.cpp:165] Memory required for data: 36402360
I0522 14:10:04.140507 27620 layer_factory.hpp:77] Creating layer conv3
I0522 14:10:04.140525 27620 net.cpp:106] Creating Layer conv3
I0522 14:10:04.140537 27620 net.cpp:454] conv3 <- pool2
I0522 14:10:04.140552 27620 net.cpp:411] conv3 -> conv3
I0522 14:10:04.142549 27620 net.cpp:150] Setting up conv3
I0522 14:10:04.142570 27620 net.cpp:157] Top shape: 30 28 22 44 (813120)
I0522 14:10:04.142583 27620 net.cpp:165] Memory required for data: 39654840
I0522 14:10:04.142617 27620 layer_factory.hpp:77] Creating layer relu3
I0522 14:10:04.142632 27620 net.cpp:106] Creating Layer relu3
I0522 14:10:04.142642 27620 net.cpp:454] relu3 <- conv3
I0522 14:10:04.142654 27620 net.cpp:397] relu3 -> conv3 (in-place)
I0522 14:10:04.143129 27620 net.cpp:150] Setting up relu3
I0522 14:10:04.143146 27620 net.cpp:157] Top shape: 30 28 22 44 (813120)
I0522 14:10:04.143156 27620 net.cpp:165] Memory required for data: 42907320
I0522 14:10:04.143167 27620 layer_factory.hpp:77] Creating layer pool3
I0522 14:10:04.143178 27620 net.cpp:106] Creating Layer pool3
I0522 14:10:04.143188 27620 net.cpp:454] pool3 <- conv3
I0522 14:10:04.143201 27620 net.cpp:411] pool3 -> pool3
I0522 14:10:04.143273 27620 net.cpp:150] Setting up pool3
I0522 14:10:04.143286 27620 net.cpp:157] Top shape: 30 28 11 44 (406560)
I0522 14:10:04.143296 27620 net.cpp:165] Memory required for data: 44533560
I0522 14:10:04.143306 27620 layer_factory.hpp:77] Creating layer conv4
I0522 14:10:04.143326 27620 net.cpp:106] Creating Layer conv4
I0522 14:10:04.143335 27620 net.cpp:454] conv4 <- pool3
I0522 14:10:04.143350 27620 net.cpp:411] conv4 -> conv4
I0522 14:10:04.145406 27620 net.cpp:150] Setting up conv4
I0522 14:10:04.145427 27620 net.cpp:157] Top shape: 30 36 6 42 (272160)
I0522 14:10:04.145439 27620 net.cpp:165] Memory required for data: 45622200
I0522 14:10:04.145455 27620 layer_factory.hpp:77] Creating layer relu4
I0522 14:10:04.145469 27620 net.cpp:106] Creating Layer relu4
I0522 14:10:04.145479 27620 net.cpp:454] relu4 <- conv4
I0522 14:10:04.145493 27620 net.cpp:397] relu4 -> conv4 (in-place)
I0522 14:10:04.145961 27620 net.cpp:150] Setting up relu4
I0522 14:10:04.145977 27620 net.cpp:157] Top shape: 30 36 6 42 (272160)
I0522 14:10:04.145987 27620 net.cpp:165] Memory required for data: 46710840
I0522 14:10:04.145998 27620 layer_factory.hpp:77] Creating layer pool4
I0522 14:10:04.146010 27620 net.cpp:106] Creating Layer pool4
I0522 14:10:04.146020 27620 net.cpp:454] pool4 <- conv4
I0522 14:10:04.146034 27620 net.cpp:411] pool4 -> pool4
I0522 14:10:04.146105 27620 net.cpp:150] Setting up pool4
I0522 14:10:04.146118 27620 net.cpp:157] Top shape: 30 36 3 42 (136080)
I0522 14:10:04.146127 27620 net.cpp:165] Memory required for data: 47255160
I0522 14:10:04.146137 27620 layer_factory.hpp:77] Creating layer ip1
I0522 14:10:04.146158 27620 net.cpp:106] Creating Layer ip1
I0522 14:10:04.146169 27620 net.cpp:454] ip1 <- pool4
I0522 14:10:04.146183 27620 net.cpp:411] ip1 -> ip1
I0522 14:10:04.161625 27620 net.cpp:150] Setting up ip1
I0522 14:10:04.161653 27620 net.cpp:157] Top shape: 30 196 (5880)
I0522 14:10:04.161664 27620 net.cpp:165] Memory required for data: 47278680
I0522 14:10:04.161686 27620 layer_factory.hpp:77] Creating layer relu5
I0522 14:10:04.161701 27620 net.cpp:106] Creating Layer relu5
I0522 14:10:04.161712 27620 net.cpp:454] relu5 <- ip1
I0522 14:10:04.161725 27620 net.cpp:397] relu5 -> ip1 (in-place)
I0522 14:10:04.162072 27620 net.cpp:150] Setting up relu5
I0522 14:10:04.162086 27620 net.cpp:157] Top shape: 30 196 (5880)
I0522 14:10:04.162096 27620 net.cpp:165] Memory required for data: 47302200
I0522 14:10:04.162106 27620 layer_factory.hpp:77] Creating layer drop1
I0522 14:10:04.162124 27620 net.cpp:106] Creating Layer drop1
I0522 14:10:04.162133 27620 net.cpp:454] drop1 <- ip1
I0522 14:10:04.162153 27620 net.cpp:397] drop1 -> ip1 (in-place)
I0522 14:10:04.162199 27620 net.cpp:150] Setting up drop1
I0522 14:10:04.162212 27620 net.cpp:157] Top shape: 30 196 (5880)
I0522 14:10:04.162222 27620 net.cpp:165] Memory required for data: 47325720
I0522 14:10:04.162232 27620 layer_factory.hpp:77] Creating layer ip2
I0522 14:10:04.162246 27620 net.cpp:106] Creating Layer ip2
I0522 14:10:04.162256 27620 net.cpp:454] ip2 <- ip1
I0522 14:10:04.162271 27620 net.cpp:411] ip2 -> ip2
I0522 14:10:04.162749 27620 net.cpp:150] Setting up ip2
I0522 14:10:04.162761 27620 net.cpp:157] Top shape: 30 98 (2940)
I0522 14:10:04.162771 27620 net.cpp:165] Memory required for data: 47337480
I0522 14:10:04.162786 27620 layer_factory.hpp:77] Creating layer relu6
I0522 14:10:04.162812 27620 net.cpp:106] Creating Layer relu6
I0522 14:10:04.162822 27620 net.cpp:454] relu6 <- ip2
I0522 14:10:04.162835 27620 net.cpp:397] relu6 -> ip2 (in-place)
I0522 14:10:04.163365 27620 net.cpp:150] Setting up relu6
I0522 14:10:04.163381 27620 net.cpp:157] Top shape: 30 98 (2940)
I0522 14:10:04.163390 27620 net.cpp:165] Memory required for data: 47349240
I0522 14:10:04.163401 27620 layer_factory.hpp:77] Creating layer drop2
I0522 14:10:04.163415 27620 net.cpp:106] Creating Layer drop2
I0522 14:10:04.163425 27620 net.cpp:454] drop2 <- ip2
I0522 14:10:04.163439 27620 net.cpp:397] drop2 -> ip2 (in-place)
I0522 14:10:04.163482 27620 net.cpp:150] Setting up drop2
I0522 14:10:04.163496 27620 net.cpp:157] Top shape: 30 98 (2940)
I0522 14:10:04.163504 27620 net.cpp:165] Memory required for data: 47361000
I0522 14:10:04.163514 27620 layer_factory.hpp:77] Creating layer ip3
I0522 14:10:04.163528 27620 net.cpp:106] Creating Layer ip3
I0522 14:10:04.163538 27620 net.cpp:454] ip3 <- ip2
I0522 14:10:04.163552 27620 net.cpp:411] ip3 -> ip3
I0522 14:10:04.163775 27620 net.cpp:150] Setting up ip3
I0522 14:10:04.163789 27620 net.cpp:157] Top shape: 30 11 (330)
I0522 14:10:04.163799 27620 net.cpp:165] Memory required for data: 47362320
I0522 14:10:04.163815 27620 layer_factory.hpp:77] Creating layer drop3
I0522 14:10:04.163827 27620 net.cpp:106] Creating Layer drop3
I0522 14:10:04.163837 27620 net.cpp:454] drop3 <- ip3
I0522 14:10:04.163849 27620 net.cpp:397] drop3 -> ip3 (in-place)
I0522 14:10:04.163892 27620 net.cpp:150] Setting up drop3
I0522 14:10:04.163904 27620 net.cpp:157] Top shape: 30 11 (330)
I0522 14:10:04.163913 27620 net.cpp:165] Memory required for data: 47363640
I0522 14:10:04.163923 27620 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0522 14:10:04.163938 27620 net.cpp:106] Creating Layer ip3_drop3_0_split
I0522 14:10:04.163947 27620 net.cpp:454] ip3_drop3_0_split <- ip3
I0522 14:10:04.163959 27620 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0522 14:10:04.163975 27620 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0522 14:10:04.164049 27620 net.cpp:150] Setting up ip3_drop3_0_split
I0522 14:10:04.164062 27620 net.cpp:157] Top shape: 30 11 (330)
I0522 14:10:04.164074 27620 net.cpp:157] Top shape: 30 11 (330)
I0522 14:10:04.164084 27620 net.cpp:165] Memory required for data: 47366280
I0522 14:10:04.164094 27620 layer_factory.hpp:77] Creating layer accuracy
I0522 14:10:04.164116 27620 net.cpp:106] Creating Layer accuracy
I0522 14:10:04.164126 27620 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0522 14:10:04.164137 27620 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0522 14:10:04.164151 27620 net.cpp:411] accuracy -> accuracy
I0522 14:10:04.164175 27620 net.cpp:150] Setting up accuracy
I0522 14:10:04.164187 27620 net.cpp:157] Top shape: (1)
I0522 14:10:04.164197 27620 net.cpp:165] Memory required for data: 47366284
I0522 14:10:04.164207 27620 layer_factory.hpp:77] Creating layer loss
I0522 14:10:04.164222 27620 net.cpp:106] Creating Layer loss
I0522 14:10:04.164232 27620 net.cpp:454] loss <- ip3_drop3_0_split_1
I0522 14:10:04.164243 27620 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0522 14:10:04.164257 27620 net.cpp:411] loss -> loss
I0522 14:10:04.164274 27620 layer_factory.hpp:77] Creating layer loss
I0522 14:10:04.164759 27620 net.cpp:150] Setting up loss
I0522 14:10:04.164773 27620 net.cpp:157] Top shape: (1)
I0522 14:10:04.164783 27620 net.cpp:160]     with loss weight 1
I0522 14:10:04.164801 27620 net.cpp:165] Memory required for data: 47366288
I0522 14:10:04.164811 27620 net.cpp:226] loss needs backward computation.
I0522 14:10:04.164822 27620 net.cpp:228] accuracy does not need backward computation.
I0522 14:10:04.164834 27620 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0522 14:10:04.164844 27620 net.cpp:226] drop3 needs backward computation.
I0522 14:10:04.164855 27620 net.cpp:226] ip3 needs backward computation.
I0522 14:10:04.164865 27620 net.cpp:226] drop2 needs backward computation.
I0522 14:10:04.164875 27620 net.cpp:226] relu6 needs backward computation.
I0522 14:10:04.164893 27620 net.cpp:226] ip2 needs backward computation.
I0522 14:10:04.164904 27620 net.cpp:226] drop1 needs backward computation.
I0522 14:10:04.164913 27620 net.cpp:226] relu5 needs backward computation.
I0522 14:10:04.164923 27620 net.cpp:226] ip1 needs backward computation.
I0522 14:10:04.164933 27620 net.cpp:226] pool4 needs backward computation.
I0522 14:10:04.164943 27620 net.cpp:226] relu4 needs backward computation.
I0522 14:10:04.164953 27620 net.cpp:226] conv4 needs backward computation.
I0522 14:10:04.164964 27620 net.cpp:226] pool3 needs backward computation.
I0522 14:10:04.164975 27620 net.cpp:226] relu3 needs backward computation.
I0522 14:10:04.164986 27620 net.cpp:226] conv3 needs backward computation.
I0522 14:10:04.164996 27620 net.cpp:226] pool2 needs backward computation.
I0522 14:10:04.165006 27620 net.cpp:226] relu2 needs backward computation.
I0522 14:10:04.165016 27620 net.cpp:226] conv2 needs backward computation.
I0522 14:10:04.165026 27620 net.cpp:226] pool1 needs backward computation.
I0522 14:10:04.165037 27620 net.cpp:226] relu1 needs backward computation.
I0522 14:10:04.165046 27620 net.cpp:226] conv1 needs backward computation.
I0522 14:10:04.165058 27620 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0522 14:10:04.165069 27620 net.cpp:228] data_hdf5 does not need backward computation.
I0522 14:10:04.165079 27620 net.cpp:270] This network produces output accuracy
I0522 14:10:04.165089 27620 net.cpp:270] This network produces output loss
I0522 14:10:04.165120 27620 net.cpp:283] Network initialization done.
I0522 14:10:04.165256 27620 solver.cpp:60] Solver scaffolding done.
I0522 14:10:04.166401 27620 caffe.cpp:212] Starting Optimization
I0522 14:10:04.166419 27620 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0522 14:10:04.166434 27620 solver.cpp:289] Learning Rate Policy: fixed
I0522 14:10:04.167654 27620 solver.cpp:341] Iteration 0, Testing net (#0)
I0522 14:10:54.732594 27620 solver.cpp:409]     Test net output #0: accuracy = 0.0687349
I0522 14:10:54.732764 27620 solver.cpp:409]     Test net output #1: loss = 2.39771 (* 1 = 2.39771 loss)
I0522 14:10:54.753756 27620 solver.cpp:237] Iteration 0, loss = 2.40074
I0522 14:10:54.753793 27620 solver.cpp:253]     Train net output #0: loss = 2.40074 (* 1 = 2.40074 loss)
I0522 14:10:54.753811 27620 sgd_solver.cpp:106] Iteration 0, lr = 0.0045
I0522 14:11:05.297660 27620 solver.cpp:237] Iteration 500, loss = 1.94445
I0522 14:11:05.297698 27620 solver.cpp:253]     Train net output #0: loss = 1.94445 (* 1 = 1.94445 loss)
I0522 14:11:05.297714 27620 sgd_solver.cpp:106] Iteration 500, lr = 0.0045
I0522 14:11:15.843765 27620 solver.cpp:237] Iteration 1000, loss = 1.84716
I0522 14:11:15.843809 27620 solver.cpp:253]     Train net output #0: loss = 1.84716 (* 1 = 1.84716 loss)
I0522 14:11:15.843824 27620 sgd_solver.cpp:106] Iteration 1000, lr = 0.0045
I0522 14:11:26.391805 27620 solver.cpp:237] Iteration 1500, loss = 1.61276
I0522 14:11:26.391953 27620 solver.cpp:253]     Train net output #0: loss = 1.61276 (* 1 = 1.61276 loss)
I0522 14:11:26.391968 27620 sgd_solver.cpp:106] Iteration 1500, lr = 0.0045
I0522 14:11:36.951663 27620 solver.cpp:237] Iteration 2000, loss = 1.77596
I0522 14:11:36.951699 27620 solver.cpp:253]     Train net output #0: loss = 1.77596 (* 1 = 1.77596 loss)
I0522 14:11:36.951717 27620 sgd_solver.cpp:106] Iteration 2000, lr = 0.0045
I0522 14:11:47.511826 27620 solver.cpp:237] Iteration 2500, loss = 1.44653
I0522 14:11:47.511867 27620 solver.cpp:253]     Train net output #0: loss = 1.44653 (* 1 = 1.44653 loss)
I0522 14:11:47.511880 27620 sgd_solver.cpp:106] Iteration 2500, lr = 0.0045
I0522 14:11:58.049002 27620 solver.cpp:237] Iteration 3000, loss = 1.71271
I0522 14:11:58.049147 27620 solver.cpp:253]     Train net output #0: loss = 1.71271 (* 1 = 1.71271 loss)
I0522 14:11:58.049162 27620 sgd_solver.cpp:106] Iteration 3000, lr = 0.0045
I0522 14:12:30.705982 27620 solver.cpp:237] Iteration 3500, loss = 1.49667
I0522 14:12:30.706143 27620 solver.cpp:253]     Train net output #0: loss = 1.49667 (* 1 = 1.49667 loss)
I0522 14:12:30.706166 27620 sgd_solver.cpp:106] Iteration 3500, lr = 0.0045
I0522 14:12:41.246114 27620 solver.cpp:237] Iteration 4000, loss = 1.43975
I0522 14:12:41.246156 27620 solver.cpp:253]     Train net output #0: loss = 1.43975 (* 1 = 1.43975 loss)
I0522 14:12:41.246170 27620 sgd_solver.cpp:106] Iteration 4000, lr = 0.0045
I0522 14:12:51.790874 27620 solver.cpp:237] Iteration 4500, loss = 1.82653
I0522 14:12:51.790910 27620 solver.cpp:253]     Train net output #0: loss = 1.82653 (* 1 = 1.82653 loss)
I0522 14:12:51.790926 27620 sgd_solver.cpp:106] Iteration 4500, lr = 0.0045
I0522 14:13:02.323375 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_5000.caffemodel
I0522 14:13:02.379015 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_5000.solverstate
I0522 14:13:02.410683 27620 solver.cpp:237] Iteration 5000, loss = 1.2679
I0522 14:13:02.410728 27620 solver.cpp:253]     Train net output #0: loss = 1.2679 (* 1 = 1.2679 loss)
I0522 14:13:02.410742 27620 sgd_solver.cpp:106] Iteration 5000, lr = 0.0045
I0522 14:13:12.975960 27620 solver.cpp:237] Iteration 5500, loss = 1.38833
I0522 14:13:12.975996 27620 solver.cpp:253]     Train net output #0: loss = 1.38833 (* 1 = 1.38833 loss)
I0522 14:13:12.976012 27620 sgd_solver.cpp:106] Iteration 5500, lr = 0.0045
I0522 14:13:23.529173 27620 solver.cpp:237] Iteration 6000, loss = 1.33272
I0522 14:13:23.529222 27620 solver.cpp:253]     Train net output #0: loss = 1.33272 (* 1 = 1.33272 loss)
I0522 14:13:23.529235 27620 sgd_solver.cpp:106] Iteration 6000, lr = 0.0045
I0522 14:13:34.076804 27620 solver.cpp:237] Iteration 6500, loss = 1.31134
I0522 14:13:34.076944 27620 solver.cpp:253]     Train net output #0: loss = 1.31134 (* 1 = 1.31134 loss)
I0522 14:13:34.076958 27620 sgd_solver.cpp:106] Iteration 6500, lr = 0.0045
I0522 14:14:06.756465 27620 solver.cpp:237] Iteration 7000, loss = 1.30594
I0522 14:14:06.756623 27620 solver.cpp:253]     Train net output #0: loss = 1.30594 (* 1 = 1.30594 loss)
I0522 14:14:06.756636 27620 sgd_solver.cpp:106] Iteration 7000, lr = 0.0045
I0522 14:14:17.313904 27620 solver.cpp:237] Iteration 7500, loss = 0.942229
I0522 14:14:17.313951 27620 solver.cpp:253]     Train net output #0: loss = 0.942229 (* 1 = 0.942229 loss)
I0522 14:14:17.313967 27620 sgd_solver.cpp:106] Iteration 7500, lr = 0.0045
I0522 14:14:27.857645 27620 solver.cpp:237] Iteration 8000, loss = 1.24453
I0522 14:14:27.857679 27620 solver.cpp:253]     Train net output #0: loss = 1.24453 (* 1 = 1.24453 loss)
I0522 14:14:27.857697 27620 sgd_solver.cpp:106] Iteration 8000, lr = 0.0045
I0522 14:14:38.420917 27620 solver.cpp:237] Iteration 8500, loss = 1.29883
I0522 14:14:38.421082 27620 solver.cpp:253]     Train net output #0: loss = 1.29883 (* 1 = 1.29883 loss)
I0522 14:14:38.421095 27620 sgd_solver.cpp:106] Iteration 8500, lr = 0.0045
I0522 14:14:48.982571 27620 solver.cpp:237] Iteration 9000, loss = 1.22102
I0522 14:14:48.982606 27620 solver.cpp:253]     Train net output #0: loss = 1.22102 (* 1 = 1.22102 loss)
I0522 14:14:48.982623 27620 sgd_solver.cpp:106] Iteration 9000, lr = 0.0045
I0522 14:14:59.531978 27620 solver.cpp:237] Iteration 9500, loss = 1.84705
I0522 14:14:59.532014 27620 solver.cpp:253]     Train net output #0: loss = 1.84705 (* 1 = 1.84705 loss)
I0522 14:14:59.532029 27620 sgd_solver.cpp:106] Iteration 9500, lr = 0.0045
I0522 14:15:10.067288 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_10000.caffemodel
I0522 14:15:10.119917 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_10000.solverstate
I0522 14:15:10.145478 27620 solver.cpp:341] Iteration 10000, Testing net (#0)
I0522 14:15:59.752893 27620 solver.cpp:409]     Test net output #0: accuracy = 0.838742
I0522 14:15:59.753156 27620 solver.cpp:409]     Test net output #1: loss = 0.569489 (* 1 = 0.569489 loss)
I0522 14:16:21.854925 27620 solver.cpp:237] Iteration 10000, loss = 1.0332
I0522 14:16:21.854979 27620 solver.cpp:253]     Train net output #0: loss = 1.0332 (* 1 = 1.0332 loss)
I0522 14:16:21.854995 27620 sgd_solver.cpp:106] Iteration 10000, lr = 0.0045
I0522 14:16:32.378046 27620 solver.cpp:237] Iteration 10500, loss = 1.11338
I0522 14:16:32.378198 27620 solver.cpp:253]     Train net output #0: loss = 1.11338 (* 1 = 1.11338 loss)
I0522 14:16:32.378213 27620 sgd_solver.cpp:106] Iteration 10500, lr = 0.0045
I0522 14:16:42.883445 27620 solver.cpp:237] Iteration 11000, loss = 1.34775
I0522 14:16:42.883494 27620 solver.cpp:253]     Train net output #0: loss = 1.34775 (* 1 = 1.34775 loss)
I0522 14:16:42.883509 27620 sgd_solver.cpp:106] Iteration 11000, lr = 0.0045
I0522 14:16:53.389672 27620 solver.cpp:237] Iteration 11500, loss = 1.05887
I0522 14:16:53.389708 27620 solver.cpp:253]     Train net output #0: loss = 1.05887 (* 1 = 1.05887 loss)
I0522 14:16:53.389724 27620 sgd_solver.cpp:106] Iteration 11500, lr = 0.0045
I0522 14:17:03.902974 27620 solver.cpp:237] Iteration 12000, loss = 1.39141
I0522 14:17:03.903111 27620 solver.cpp:253]     Train net output #0: loss = 1.39141 (* 1 = 1.39141 loss)
I0522 14:17:03.903126 27620 sgd_solver.cpp:106] Iteration 12000, lr = 0.0045
I0522 14:17:14.421341 27620 solver.cpp:237] Iteration 12500, loss = 1.2168
I0522 14:17:14.421386 27620 solver.cpp:253]     Train net output #0: loss = 1.2168 (* 1 = 1.2168 loss)
I0522 14:17:14.421401 27620 sgd_solver.cpp:106] Iteration 12500, lr = 0.0045
I0522 14:17:24.935063 27620 solver.cpp:237] Iteration 13000, loss = 1.10624
I0522 14:17:24.935099 27620 solver.cpp:253]     Train net output #0: loss = 1.10624 (* 1 = 1.10624 loss)
I0522 14:17:24.935116 27620 sgd_solver.cpp:106] Iteration 13000, lr = 0.0045
I0522 14:17:57.587710 27620 solver.cpp:237] Iteration 13500, loss = 1.28828
I0522 14:17:57.587873 27620 solver.cpp:253]     Train net output #0: loss = 1.28828 (* 1 = 1.28828 loss)
I0522 14:17:57.587888 27620 sgd_solver.cpp:106] Iteration 13500, lr = 0.0045
I0522 14:18:08.100760 27620 solver.cpp:237] Iteration 14000, loss = 0.932767
I0522 14:18:08.100796 27620 solver.cpp:253]     Train net output #0: loss = 0.932767 (* 1 = 0.932767 loss)
I0522 14:18:08.100812 27620 sgd_solver.cpp:106] Iteration 14000, lr = 0.0045
I0522 14:18:18.614841 27620 solver.cpp:237] Iteration 14500, loss = 1.1631
I0522 14:18:18.614877 27620 solver.cpp:253]     Train net output #0: loss = 1.1631 (* 1 = 1.1631 loss)
I0522 14:18:18.614891 27620 sgd_solver.cpp:106] Iteration 14500, lr = 0.0045
I0522 14:18:29.103646 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_15000.caffemodel
I0522 14:18:29.158375 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_15000.solverstate
I0522 14:18:29.192513 27620 solver.cpp:237] Iteration 15000, loss = 1.15065
I0522 14:18:29.192564 27620 solver.cpp:253]     Train net output #0: loss = 1.15065 (* 1 = 1.15065 loss)
I0522 14:18:29.192582 27620 sgd_solver.cpp:106] Iteration 15000, lr = 0.0045
I0522 14:18:39.705447 27620 solver.cpp:237] Iteration 15500, loss = 1.00369
I0522 14:18:39.705483 27620 solver.cpp:253]     Train net output #0: loss = 1.00369 (* 1 = 1.00369 loss)
I0522 14:18:39.705499 27620 sgd_solver.cpp:106] Iteration 15500, lr = 0.0045
I0522 14:18:50.217867 27620 solver.cpp:237] Iteration 16000, loss = 1.18214
I0522 14:18:50.217917 27620 solver.cpp:253]     Train net output #0: loss = 1.18214 (* 1 = 1.18214 loss)
I0522 14:18:50.217931 27620 sgd_solver.cpp:106] Iteration 16000, lr = 0.0045
I0522 14:19:00.732524 27620 solver.cpp:237] Iteration 16500, loss = 1.81041
I0522 14:19:00.732671 27620 solver.cpp:253]     Train net output #0: loss = 1.81041 (* 1 = 1.81041 loss)
I0522 14:19:00.732686 27620 sgd_solver.cpp:106] Iteration 16500, lr = 0.0045
I0522 14:19:33.403033 27620 solver.cpp:237] Iteration 17000, loss = 1.33695
I0522 14:19:33.403208 27620 solver.cpp:253]     Train net output #0: loss = 1.33695 (* 1 = 1.33695 loss)
I0522 14:19:33.403223 27620 sgd_solver.cpp:106] Iteration 17000, lr = 0.0045
I0522 14:19:43.922966 27620 solver.cpp:237] Iteration 17500, loss = 1.28555
I0522 14:19:43.923012 27620 solver.cpp:253]     Train net output #0: loss = 1.28555 (* 1 = 1.28555 loss)
I0522 14:19:43.923028 27620 sgd_solver.cpp:106] Iteration 17500, lr = 0.0045
I0522 14:19:54.437548 27620 solver.cpp:237] Iteration 18000, loss = 1.22269
I0522 14:19:54.437582 27620 solver.cpp:253]     Train net output #0: loss = 1.22269 (* 1 = 1.22269 loss)
I0522 14:19:54.437600 27620 sgd_solver.cpp:106] Iteration 18000, lr = 0.0045
I0522 14:20:04.956089 27620 solver.cpp:237] Iteration 18500, loss = 1.04058
I0522 14:20:04.956245 27620 solver.cpp:253]     Train net output #0: loss = 1.04058 (* 1 = 1.04058 loss)
I0522 14:20:04.956261 27620 sgd_solver.cpp:106] Iteration 18500, lr = 0.0045
I0522 14:20:15.469240 27620 solver.cpp:237] Iteration 19000, loss = 1.25272
I0522 14:20:15.469276 27620 solver.cpp:253]     Train net output #0: loss = 1.25272 (* 1 = 1.25272 loss)
I0522 14:20:15.469292 27620 sgd_solver.cpp:106] Iteration 19000, lr = 0.0045
I0522 14:20:25.989302 27620 solver.cpp:237] Iteration 19500, loss = 0.681969
I0522 14:20:25.989346 27620 solver.cpp:253]     Train net output #0: loss = 0.681969 (* 1 = 0.681969 loss)
I0522 14:20:25.989362 27620 sgd_solver.cpp:106] Iteration 19500, lr = 0.0045
I0522 14:20:36.477540 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_20000.caffemodel
I0522 14:20:36.532562 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_20000.solverstate
I0522 14:20:36.561236 27620 solver.cpp:341] Iteration 20000, Testing net (#0)
I0522 14:21:47.010171 27620 solver.cpp:409]     Test net output #0: accuracy = 0.859786
I0522 14:21:47.010339 27620 solver.cpp:409]     Test net output #1: loss = 0.457933 (* 1 = 0.457933 loss)
I0522 14:22:09.188993 27620 solver.cpp:237] Iteration 20000, loss = 1.03645
I0522 14:22:09.189046 27620 solver.cpp:253]     Train net output #0: loss = 1.03645 (* 1 = 1.03645 loss)
I0522 14:22:09.189064 27620 sgd_solver.cpp:106] Iteration 20000, lr = 0.0045
I0522 14:22:19.760900 27620 solver.cpp:237] Iteration 20500, loss = 1.02284
I0522 14:22:19.761050 27620 solver.cpp:253]     Train net output #0: loss = 1.02284 (* 1 = 1.02284 loss)
I0522 14:22:19.761065 27620 sgd_solver.cpp:106] Iteration 20500, lr = 0.0045
I0522 14:22:30.333698 27620 solver.cpp:237] Iteration 21000, loss = 1.56067
I0522 14:22:30.333745 27620 solver.cpp:253]     Train net output #0: loss = 1.56067 (* 1 = 1.56067 loss)
I0522 14:22:30.333762 27620 sgd_solver.cpp:106] Iteration 21000, lr = 0.0045
I0522 14:22:40.890573 27620 solver.cpp:237] Iteration 21500, loss = 1.23261
I0522 14:22:40.890609 27620 solver.cpp:253]     Train net output #0: loss = 1.23261 (* 1 = 1.23261 loss)
I0522 14:22:40.890626 27620 sgd_solver.cpp:106] Iteration 21500, lr = 0.0045
I0522 14:22:51.443970 27620 solver.cpp:237] Iteration 22000, loss = 1.46956
I0522 14:22:51.444110 27620 solver.cpp:253]     Train net output #0: loss = 1.46956 (* 1 = 1.46956 loss)
I0522 14:22:51.444125 27620 sgd_solver.cpp:106] Iteration 22000, lr = 0.0045
I0522 14:23:02.007397 27620 solver.cpp:237] Iteration 22500, loss = 1.09219
I0522 14:23:02.007442 27620 solver.cpp:253]     Train net output #0: loss = 1.09219 (* 1 = 1.09219 loss)
I0522 14:23:02.007457 27620 sgd_solver.cpp:106] Iteration 22500, lr = 0.0045
I0522 14:23:12.576308 27620 solver.cpp:237] Iteration 23000, loss = 1.33899
I0522 14:23:12.576344 27620 solver.cpp:253]     Train net output #0: loss = 1.33899 (* 1 = 1.33899 loss)
I0522 14:23:12.576359 27620 sgd_solver.cpp:106] Iteration 23000, lr = 0.0045
I0522 14:23:45.287240 27620 solver.cpp:237] Iteration 23500, loss = 1.07314
I0522 14:23:45.287405 27620 solver.cpp:253]     Train net output #0: loss = 1.07314 (* 1 = 1.07314 loss)
I0522 14:23:45.287418 27620 sgd_solver.cpp:106] Iteration 23500, lr = 0.0045
I0522 14:23:55.861426 27620 solver.cpp:237] Iteration 24000, loss = 1.04566
I0522 14:23:55.861461 27620 solver.cpp:253]     Train net output #0: loss = 1.04566 (* 1 = 1.04566 loss)
I0522 14:23:55.861477 27620 sgd_solver.cpp:106] Iteration 24000, lr = 0.0045
I0522 14:24:06.415163 27620 solver.cpp:237] Iteration 24500, loss = 1.21694
I0522 14:24:06.415199 27620 solver.cpp:253]     Train net output #0: loss = 1.21694 (* 1 = 1.21694 loss)
I0522 14:24:06.415212 27620 sgd_solver.cpp:106] Iteration 24500, lr = 0.0045
I0522 14:24:17.005650 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_25000.caffemodel
I0522 14:24:17.061455 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_25000.solverstate
I0522 14:24:17.096447 27620 solver.cpp:237] Iteration 25000, loss = 1.37065
I0522 14:24:17.096493 27620 solver.cpp:253]     Train net output #0: loss = 1.37065 (* 1 = 1.37065 loss)
I0522 14:24:17.096513 27620 sgd_solver.cpp:106] Iteration 25000, lr = 0.0045
I0522 14:24:27.722304 27620 solver.cpp:237] Iteration 25500, loss = 1.19915
I0522 14:24:27.722340 27620 solver.cpp:253]     Train net output #0: loss = 1.19915 (* 1 = 1.19915 loss)
I0522 14:24:27.722357 27620 sgd_solver.cpp:106] Iteration 25500, lr = 0.0045
I0522 14:24:38.344535 27620 solver.cpp:237] Iteration 26000, loss = 1.33123
I0522 14:24:38.344586 27620 solver.cpp:253]     Train net output #0: loss = 1.33123 (* 1 = 1.33123 loss)
I0522 14:24:38.344600 27620 sgd_solver.cpp:106] Iteration 26000, lr = 0.0045
I0522 14:24:48.984621 27620 solver.cpp:237] Iteration 26500, loss = 1.6188
I0522 14:24:48.984766 27620 solver.cpp:253]     Train net output #0: loss = 1.6188 (* 1 = 1.6188 loss)
I0522 14:24:48.984779 27620 sgd_solver.cpp:106] Iteration 26500, lr = 0.0045
I0522 14:25:21.729828 27620 solver.cpp:237] Iteration 27000, loss = 1.18364
I0522 14:25:21.730006 27620 solver.cpp:253]     Train net output #0: loss = 1.18364 (* 1 = 1.18364 loss)
I0522 14:25:21.730021 27620 sgd_solver.cpp:106] Iteration 27000, lr = 0.0045
I0522 14:25:32.300384 27620 solver.cpp:237] Iteration 27500, loss = 1.18214
I0522 14:25:32.300432 27620 solver.cpp:253]     Train net output #0: loss = 1.18214 (* 1 = 1.18214 loss)
I0522 14:25:32.300446 27620 sgd_solver.cpp:106] Iteration 27500, lr = 0.0045
I0522 14:25:42.822705 27620 solver.cpp:237] Iteration 28000, loss = 1.09244
I0522 14:25:42.822741 27620 solver.cpp:253]     Train net output #0: loss = 1.09244 (* 1 = 1.09244 loss)
I0522 14:25:42.822757 27620 sgd_solver.cpp:106] Iteration 28000, lr = 0.0045
I0522 14:25:53.370877 27620 solver.cpp:237] Iteration 28500, loss = 1.4227
I0522 14:25:53.371036 27620 solver.cpp:253]     Train net output #0: loss = 1.4227 (* 1 = 1.4227 loss)
I0522 14:25:53.371049 27620 sgd_solver.cpp:106] Iteration 28500, lr = 0.0045
I0522 14:26:03.885632 27620 solver.cpp:237] Iteration 29000, loss = 0.768087
I0522 14:26:03.885668 27620 solver.cpp:253]     Train net output #0: loss = 0.768088 (* 1 = 0.768088 loss)
I0522 14:26:03.885684 27620 sgd_solver.cpp:106] Iteration 29000, lr = 0.0045
I0522 14:26:14.413460 27620 solver.cpp:237] Iteration 29500, loss = 0.907477
I0522 14:26:14.413497 27620 solver.cpp:253]     Train net output #0: loss = 0.907477 (* 1 = 0.907477 loss)
I0522 14:26:14.413513 27620 sgd_solver.cpp:106] Iteration 29500, lr = 0.0045
I0522 14:26:24.930949 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_30000.caffemodel
I0522 14:26:24.983687 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_30000.solverstate
I0522 14:26:25.011148 27620 solver.cpp:341] Iteration 30000, Testing net (#0)
I0522 14:27:14.316908 27620 solver.cpp:409]     Test net output #0: accuracy = 0.869798
I0522 14:27:14.317066 27620 solver.cpp:409]     Test net output #1: loss = 0.404633 (* 1 = 0.404633 loss)
I0522 14:27:36.455528 27620 solver.cpp:237] Iteration 30000, loss = 1.13328
I0522 14:27:36.455581 27620 solver.cpp:253]     Train net output #0: loss = 1.13328 (* 1 = 1.13328 loss)
I0522 14:27:36.455598 27620 sgd_solver.cpp:106] Iteration 30000, lr = 0.0045
I0522 14:27:47.030055 27620 solver.cpp:237] Iteration 30500, loss = 1.23216
I0522 14:27:47.030217 27620 solver.cpp:253]     Train net output #0: loss = 1.23216 (* 1 = 1.23216 loss)
I0522 14:27:47.030231 27620 sgd_solver.cpp:106] Iteration 30500, lr = 0.0045
I0522 14:27:57.596498 27620 solver.cpp:237] Iteration 31000, loss = 1.15984
I0522 14:27:57.596544 27620 solver.cpp:253]     Train net output #0: loss = 1.15984 (* 1 = 1.15984 loss)
I0522 14:27:57.596559 27620 sgd_solver.cpp:106] Iteration 31000, lr = 0.0045
I0522 14:28:08.161082 27620 solver.cpp:237] Iteration 31500, loss = 1.39512
I0522 14:28:08.161118 27620 solver.cpp:253]     Train net output #0: loss = 1.39512 (* 1 = 1.39512 loss)
I0522 14:28:08.161130 27620 sgd_solver.cpp:106] Iteration 31500, lr = 0.0045
I0522 14:28:18.727118 27620 solver.cpp:237] Iteration 32000, loss = 0.931851
I0522 14:28:18.727259 27620 solver.cpp:253]     Train net output #0: loss = 0.931852 (* 1 = 0.931852 loss)
I0522 14:28:18.727273 27620 sgd_solver.cpp:106] Iteration 32000, lr = 0.0045
I0522 14:28:29.311669 27620 solver.cpp:237] Iteration 32500, loss = 1.01029
I0522 14:28:29.311717 27620 solver.cpp:253]     Train net output #0: loss = 1.01029 (* 1 = 1.01029 loss)
I0522 14:28:29.311729 27620 sgd_solver.cpp:106] Iteration 32500, lr = 0.0045
I0522 14:28:39.911870 27620 solver.cpp:237] Iteration 33000, loss = 0.98177
I0522 14:28:39.911906 27620 solver.cpp:253]     Train net output #0: loss = 0.981771 (* 1 = 0.981771 loss)
I0522 14:28:39.911919 27620 sgd_solver.cpp:106] Iteration 33000, lr = 0.0045
I0522 14:29:12.666250 27620 solver.cpp:237] Iteration 33500, loss = 1.2854
I0522 14:29:12.666425 27620 solver.cpp:253]     Train net output #0: loss = 1.2854 (* 1 = 1.2854 loss)
I0522 14:29:12.666441 27620 sgd_solver.cpp:106] Iteration 33500, lr = 0.0045
I0522 14:29:23.278326 27620 solver.cpp:237] Iteration 34000, loss = 1.36316
I0522 14:29:23.278360 27620 solver.cpp:253]     Train net output #0: loss = 1.36316 (* 1 = 1.36316 loss)
I0522 14:29:23.278374 27620 sgd_solver.cpp:106] Iteration 34000, lr = 0.0045
I0522 14:29:33.880383 27620 solver.cpp:237] Iteration 34500, loss = 1.28121
I0522 14:29:33.880419 27620 solver.cpp:253]     Train net output #0: loss = 1.28121 (* 1 = 1.28121 loss)
I0522 14:29:33.880436 27620 sgd_solver.cpp:106] Iteration 34500, lr = 0.0045
I0522 14:29:44.450186 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_35000.caffemodel
I0522 14:29:44.502789 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_35000.solverstate
I0522 14:29:44.536272 27620 solver.cpp:237] Iteration 35000, loss = 1.59141
I0522 14:29:44.536319 27620 solver.cpp:253]     Train net output #0: loss = 1.59141 (* 1 = 1.59141 loss)
I0522 14:29:44.536332 27620 sgd_solver.cpp:106] Iteration 35000, lr = 0.0045
I0522 14:29:55.129081 27620 solver.cpp:237] Iteration 35500, loss = 1.36965
I0522 14:29:55.129117 27620 solver.cpp:253]     Train net output #0: loss = 1.36965 (* 1 = 1.36965 loss)
I0522 14:29:55.129133 27620 sgd_solver.cpp:106] Iteration 35500, lr = 0.0045
I0522 14:30:05.727643 27620 solver.cpp:237] Iteration 36000, loss = 1.25265
I0522 14:30:05.727692 27620 solver.cpp:253]     Train net output #0: loss = 1.25265 (* 1 = 1.25265 loss)
I0522 14:30:05.727707 27620 sgd_solver.cpp:106] Iteration 36000, lr = 0.0045
I0522 14:30:16.328212 27620 solver.cpp:237] Iteration 36500, loss = 1.05545
I0522 14:30:16.328361 27620 solver.cpp:253]     Train net output #0: loss = 1.05545 (* 1 = 1.05545 loss)
I0522 14:30:16.328374 27620 sgd_solver.cpp:106] Iteration 36500, lr = 0.0045
I0522 14:30:49.063514 27620 solver.cpp:237] Iteration 37000, loss = 1.43556
I0522 14:30:49.063699 27620 solver.cpp:253]     Train net output #0: loss = 1.43556 (* 1 = 1.43556 loss)
I0522 14:30:49.063712 27620 sgd_solver.cpp:106] Iteration 37000, lr = 0.0045
I0522 14:30:59.658980 27620 solver.cpp:237] Iteration 37500, loss = 1.45334
I0522 14:30:59.659018 27620 solver.cpp:253]     Train net output #0: loss = 1.45334 (* 1 = 1.45334 loss)
I0522 14:30:59.659032 27620 sgd_solver.cpp:106] Iteration 37500, lr = 0.0045
I0522 14:31:10.263094 27620 solver.cpp:237] Iteration 38000, loss = 1.08864
I0522 14:31:10.263130 27620 solver.cpp:253]     Train net output #0: loss = 1.08864 (* 1 = 1.08864 loss)
I0522 14:31:10.263149 27620 sgd_solver.cpp:106] Iteration 38000, lr = 0.0045
I0522 14:31:20.872925 27620 solver.cpp:237] Iteration 38500, loss = 1.42332
I0522 14:31:20.873076 27620 solver.cpp:253]     Train net output #0: loss = 1.42332 (* 1 = 1.42332 loss)
I0522 14:31:20.873090 27620 sgd_solver.cpp:106] Iteration 38500, lr = 0.0045
I0522 14:31:31.468037 27620 solver.cpp:237] Iteration 39000, loss = 0.924742
I0522 14:31:31.468073 27620 solver.cpp:253]     Train net output #0: loss = 0.924743 (* 1 = 0.924743 loss)
I0522 14:31:31.468091 27620 sgd_solver.cpp:106] Iteration 39000, lr = 0.0045
I0522 14:31:42.062695 27620 solver.cpp:237] Iteration 39500, loss = 1.09551
I0522 14:31:42.062736 27620 solver.cpp:253]     Train net output #0: loss = 1.09551 (* 1 = 1.09551 loss)
I0522 14:31:42.062752 27620 sgd_solver.cpp:106] Iteration 39500, lr = 0.0045
I0522 14:31:52.641955 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_40000.caffemodel
I0522 14:31:52.695027 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_40000.solverstate
I0522 14:31:52.721465 27620 solver.cpp:341] Iteration 40000, Testing net (#0)
I0522 14:33:03.185899 27620 solver.cpp:409]     Test net output #0: accuracy = 0.876589
I0522 14:33:03.186064 27620 solver.cpp:409]     Test net output #1: loss = 0.384687 (* 1 = 0.384687 loss)
I0522 14:33:25.366260 27620 solver.cpp:237] Iteration 40000, loss = 1.25194
I0522 14:33:25.366313 27620 solver.cpp:253]     Train net output #0: loss = 1.25194 (* 1 = 1.25194 loss)
I0522 14:33:25.366328 27620 sgd_solver.cpp:106] Iteration 40000, lr = 0.0045
I0522 14:33:35.895891 27620 solver.cpp:237] Iteration 40500, loss = 1.27221
I0522 14:33:35.896081 27620 solver.cpp:253]     Train net output #0: loss = 1.27221 (* 1 = 1.27221 loss)
I0522 14:33:35.896095 27620 sgd_solver.cpp:106] Iteration 40500, lr = 0.0045
I0522 14:33:46.448072 27620 solver.cpp:237] Iteration 41000, loss = 1.02462
I0522 14:33:46.448120 27620 solver.cpp:253]     Train net output #0: loss = 1.02462 (* 1 = 1.02462 loss)
I0522 14:33:46.448135 27620 sgd_solver.cpp:106] Iteration 41000, lr = 0.0045
I0522 14:33:56.987941 27620 solver.cpp:237] Iteration 41500, loss = 1.17772
I0522 14:33:56.987977 27620 solver.cpp:253]     Train net output #0: loss = 1.17773 (* 1 = 1.17773 loss)
I0522 14:33:56.987993 27620 sgd_solver.cpp:106] Iteration 41500, lr = 0.0045
I0522 14:34:07.532757 27620 solver.cpp:237] Iteration 42000, loss = 1.06546
I0522 14:34:07.532907 27620 solver.cpp:253]     Train net output #0: loss = 1.06547 (* 1 = 1.06547 loss)
I0522 14:34:07.532922 27620 sgd_solver.cpp:106] Iteration 42000, lr = 0.0045
I0522 14:34:18.080287 27620 solver.cpp:237] Iteration 42500, loss = 1.17994
I0522 14:34:18.080327 27620 solver.cpp:253]     Train net output #0: loss = 1.17994 (* 1 = 1.17994 loss)
I0522 14:34:18.080343 27620 sgd_solver.cpp:106] Iteration 42500, lr = 0.0045
I0522 14:34:28.614114 27620 solver.cpp:237] Iteration 43000, loss = 1.19252
I0522 14:34:28.614159 27620 solver.cpp:253]     Train net output #0: loss = 1.19252 (* 1 = 1.19252 loss)
I0522 14:34:28.614174 27620 sgd_solver.cpp:106] Iteration 43000, lr = 0.0045
I0522 14:35:01.360352 27620 solver.cpp:237] Iteration 43500, loss = 1.23417
I0522 14:35:01.360522 27620 solver.cpp:253]     Train net output #0: loss = 1.23418 (* 1 = 1.23418 loss)
I0522 14:35:01.360537 27620 sgd_solver.cpp:106] Iteration 43500, lr = 0.0045
I0522 14:35:11.893846 27620 solver.cpp:237] Iteration 44000, loss = 1.29183
I0522 14:35:11.893882 27620 solver.cpp:253]     Train net output #0: loss = 1.29183 (* 1 = 1.29183 loss)
I0522 14:35:11.893898 27620 sgd_solver.cpp:106] Iteration 44000, lr = 0.0045
I0522 14:35:22.413971 27620 solver.cpp:237] Iteration 44500, loss = 1.07529
I0522 14:35:22.414007 27620 solver.cpp:253]     Train net output #0: loss = 1.07529 (* 1 = 1.07529 loss)
I0522 14:35:22.414023 27620 sgd_solver.cpp:106] Iteration 44500, lr = 0.0045
I0522 14:35:32.931888 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_45000.caffemodel
I0522 14:35:32.986270 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_45000.solverstate
I0522 14:35:33.020920 27620 solver.cpp:237] Iteration 45000, loss = 1.66176
I0522 14:35:33.020965 27620 solver.cpp:253]     Train net output #0: loss = 1.66176 (* 1 = 1.66176 loss)
I0522 14:35:33.020982 27620 sgd_solver.cpp:106] Iteration 45000, lr = 0.0045
I0522 14:35:43.550282 27620 solver.cpp:237] Iteration 45500, loss = 1.26404
I0522 14:35:43.550313 27620 solver.cpp:253]     Train net output #0: loss = 1.26404 (* 1 = 1.26404 loss)
I0522 14:35:43.550326 27620 sgd_solver.cpp:106] Iteration 45500, lr = 0.0045
I0522 14:35:54.090050 27620 solver.cpp:237] Iteration 46000, loss = 1.86896
I0522 14:35:54.090096 27620 solver.cpp:253]     Train net output #0: loss = 1.86896 (* 1 = 1.86896 loss)
I0522 14:35:54.090111 27620 sgd_solver.cpp:106] Iteration 46000, lr = 0.0045
I0522 14:36:04.630453 27620 solver.cpp:237] Iteration 46500, loss = 1.10867
I0522 14:36:04.630625 27620 solver.cpp:253]     Train net output #0: loss = 1.10867 (* 1 = 1.10867 loss)
I0522 14:36:04.630640 27620 sgd_solver.cpp:106] Iteration 46500, lr = 0.0045
I0522 14:36:37.295258 27620 solver.cpp:237] Iteration 47000, loss = 1.25994
I0522 14:36:37.295433 27620 solver.cpp:253]     Train net output #0: loss = 1.25994 (* 1 = 1.25994 loss)
I0522 14:36:37.295449 27620 sgd_solver.cpp:106] Iteration 47000, lr = 0.0045
I0522 14:36:47.821825 27620 solver.cpp:237] Iteration 47500, loss = 1.0492
I0522 14:36:47.821868 27620 solver.cpp:253]     Train net output #0: loss = 1.0492 (* 1 = 1.0492 loss)
I0522 14:36:47.821882 27620 sgd_solver.cpp:106] Iteration 47500, lr = 0.0045
I0522 14:36:58.347792 27620 solver.cpp:237] Iteration 48000, loss = 1.55122
I0522 14:36:58.347828 27620 solver.cpp:253]     Train net output #0: loss = 1.55122 (* 1 = 1.55122 loss)
I0522 14:36:58.347844 27620 sgd_solver.cpp:106] Iteration 48000, lr = 0.0045
I0522 14:37:08.885777 27620 solver.cpp:237] Iteration 48500, loss = 1.36833
I0522 14:37:08.885933 27620 solver.cpp:253]     Train net output #0: loss = 1.36833 (* 1 = 1.36833 loss)
I0522 14:37:08.885948 27620 sgd_solver.cpp:106] Iteration 48500, lr = 0.0045
I0522 14:37:19.411679 27620 solver.cpp:237] Iteration 49000, loss = 1.33303
I0522 14:37:19.411715 27620 solver.cpp:253]     Train net output #0: loss = 1.33303 (* 1 = 1.33303 loss)
I0522 14:37:19.411731 27620 sgd_solver.cpp:106] Iteration 49000, lr = 0.0045
I0522 14:37:29.951167 27620 solver.cpp:237] Iteration 49500, loss = 1.60735
I0522 14:37:29.951220 27620 solver.cpp:253]     Train net output #0: loss = 1.60735 (* 1 = 1.60735 loss)
I0522 14:37:29.951233 27620 sgd_solver.cpp:106] Iteration 49500, lr = 0.0045
I0522 14:37:40.456377 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_50000.caffemodel
I0522 14:37:40.510931 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_50000.solverstate
I0522 14:37:40.539445 27620 solver.cpp:341] Iteration 50000, Testing net (#0)
I0522 14:38:30.198117 27620 solver.cpp:409]     Test net output #0: accuracy = 0.883343
I0522 14:38:30.198303 27620 solver.cpp:409]     Test net output #1: loss = 0.398244 (* 1 = 0.398244 loss)
I0522 14:38:51.092489 27620 solver.cpp:237] Iteration 50000, loss = 1.14579
I0522 14:38:51.092541 27620 solver.cpp:253]     Train net output #0: loss = 1.14579 (* 1 = 1.14579 loss)
I0522 14:38:51.092556 27620 sgd_solver.cpp:106] Iteration 50000, lr = 0.0045
I0522 14:39:01.587134 27620 solver.cpp:237] Iteration 50500, loss = 1.16783
I0522 14:39:01.587291 27620 solver.cpp:253]     Train net output #0: loss = 1.16783 (* 1 = 1.16783 loss)
I0522 14:39:01.587306 27620 sgd_solver.cpp:106] Iteration 50500, lr = 0.0045
I0522 14:39:12.077411 27620 solver.cpp:237] Iteration 51000, loss = 1.50163
I0522 14:39:12.077456 27620 solver.cpp:253]     Train net output #0: loss = 1.50163 (* 1 = 1.50163 loss)
I0522 14:39:12.077471 27620 sgd_solver.cpp:106] Iteration 51000, lr = 0.0045
I0522 14:39:22.571318 27620 solver.cpp:237] Iteration 51500, loss = 1.16318
I0522 14:39:22.571355 27620 solver.cpp:253]     Train net output #0: loss = 1.16318 (* 1 = 1.16318 loss)
I0522 14:39:22.571368 27620 sgd_solver.cpp:106] Iteration 51500, lr = 0.0045
I0522 14:39:33.076458 27620 solver.cpp:237] Iteration 52000, loss = 1.10789
I0522 14:39:33.076603 27620 solver.cpp:253]     Train net output #0: loss = 1.10789 (* 1 = 1.10789 loss)
I0522 14:39:33.076617 27620 sgd_solver.cpp:106] Iteration 52000, lr = 0.0045
I0522 14:39:43.573339 27620 solver.cpp:237] Iteration 52500, loss = 1.05388
I0522 14:39:43.573385 27620 solver.cpp:253]     Train net output #0: loss = 1.05388 (* 1 = 1.05388 loss)
I0522 14:39:43.573400 27620 sgd_solver.cpp:106] Iteration 52500, lr = 0.0045
I0522 14:39:54.084331 27620 solver.cpp:237] Iteration 53000, loss = 1.69645
I0522 14:39:54.084367 27620 solver.cpp:253]     Train net output #0: loss = 1.69645 (* 1 = 1.69645 loss)
I0522 14:39:54.084384 27620 sgd_solver.cpp:106] Iteration 53000, lr = 0.0045
I0522 14:40:25.507210 27620 solver.cpp:237] Iteration 53500, loss = 1.0576
I0522 14:40:25.507390 27620 solver.cpp:253]     Train net output #0: loss = 1.0576 (* 1 = 1.0576 loss)
I0522 14:40:25.507405 27620 sgd_solver.cpp:106] Iteration 53500, lr = 0.0045
I0522 14:40:36.057646 27620 solver.cpp:237] Iteration 54000, loss = 1.12462
I0522 14:40:36.057682 27620 solver.cpp:253]     Train net output #0: loss = 1.12462 (* 1 = 1.12462 loss)
I0522 14:40:36.057699 27620 sgd_solver.cpp:106] Iteration 54000, lr = 0.0045
I0522 14:40:46.599479 27620 solver.cpp:237] Iteration 54500, loss = 2.06607
I0522 14:40:46.599515 27620 solver.cpp:253]     Train net output #0: loss = 2.06607 (* 1 = 2.06607 loss)
I0522 14:40:46.599530 27620 sgd_solver.cpp:106] Iteration 54500, lr = 0.0045
I0522 14:40:57.116302 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_55000.caffemodel
I0522 14:40:57.168540 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_55000.solverstate
I0522 14:40:57.201503 27620 solver.cpp:237] Iteration 55000, loss = 1.46656
I0522 14:40:57.201544 27620 solver.cpp:253]     Train net output #0: loss = 1.46656 (* 1 = 1.46656 loss)
I0522 14:40:57.201560 27620 sgd_solver.cpp:106] Iteration 55000, lr = 0.0045
I0522 14:41:07.757560 27620 solver.cpp:237] Iteration 55500, loss = 1.61399
I0522 14:41:07.757596 27620 solver.cpp:253]     Train net output #0: loss = 1.61399 (* 1 = 1.61399 loss)
I0522 14:41:07.757611 27620 sgd_solver.cpp:106] Iteration 55500, lr = 0.0045
I0522 14:41:18.308660 27620 solver.cpp:237] Iteration 56000, loss = 0.964437
I0522 14:41:18.308706 27620 solver.cpp:253]     Train net output #0: loss = 0.964437 (* 1 = 0.964437 loss)
I0522 14:41:18.308722 27620 sgd_solver.cpp:106] Iteration 56000, lr = 0.0045
I0522 14:41:28.851094 27620 solver.cpp:237] Iteration 56500, loss = 1.20188
I0522 14:41:28.851246 27620 solver.cpp:253]     Train net output #0: loss = 1.20188 (* 1 = 1.20188 loss)
I0522 14:41:28.851261 27620 sgd_solver.cpp:106] Iteration 56500, lr = 0.0045
I0522 14:42:00.261248 27620 solver.cpp:237] Iteration 57000, loss = 0.7998
I0522 14:42:00.261421 27620 solver.cpp:253]     Train net output #0: loss = 0.7998 (* 1 = 0.7998 loss)
I0522 14:42:00.261438 27620 sgd_solver.cpp:106] Iteration 57000, lr = 0.0045
I0522 14:42:10.809032 27620 solver.cpp:237] Iteration 57500, loss = 1.03739
I0522 14:42:10.809077 27620 solver.cpp:253]     Train net output #0: loss = 1.03739 (* 1 = 1.03739 loss)
I0522 14:42:10.809093 27620 sgd_solver.cpp:106] Iteration 57500, lr = 0.0045
I0522 14:42:21.351120 27620 solver.cpp:237] Iteration 58000, loss = 0.993974
I0522 14:42:21.351156 27620 solver.cpp:253]     Train net output #0: loss = 0.993974 (* 1 = 0.993974 loss)
I0522 14:42:21.351172 27620 sgd_solver.cpp:106] Iteration 58000, lr = 0.0045
I0522 14:42:31.904173 27620 solver.cpp:237] Iteration 58500, loss = 1.08734
I0522 14:42:31.904330 27620 solver.cpp:253]     Train net output #0: loss = 1.08734 (* 1 = 1.08734 loss)
I0522 14:42:31.904343 27620 sgd_solver.cpp:106] Iteration 58500, lr = 0.0045
I0522 14:42:42.449756 27620 solver.cpp:237] Iteration 59000, loss = 1.28532
I0522 14:42:42.449791 27620 solver.cpp:253]     Train net output #0: loss = 1.28532 (* 1 = 1.28532 loss)
I0522 14:42:42.449807 27620 sgd_solver.cpp:106] Iteration 59000, lr = 0.0045
I0522 14:42:53.003603 27620 solver.cpp:237] Iteration 59500, loss = 1.4023
I0522 14:42:53.003639 27620 solver.cpp:253]     Train net output #0: loss = 1.4023 (* 1 = 1.4023 loss)
I0522 14:42:53.003655 27620 sgd_solver.cpp:106] Iteration 59500, lr = 0.0045
I0522 14:43:03.540287 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_60000.caffemodel
I0522 14:43:03.592685 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_60000.solverstate
I0522 14:43:03.618917 27620 solver.cpp:341] Iteration 60000, Testing net (#0)
I0522 14:44:14.143949 27620 solver.cpp:409]     Test net output #0: accuracy = 0.882821
I0522 14:44:14.144119 27620 solver.cpp:409]     Test net output #1: loss = 0.361894 (* 1 = 0.361894 loss)
I0522 14:44:35.004870 27620 solver.cpp:237] Iteration 60000, loss = 1.12955
I0522 14:44:35.004919 27620 solver.cpp:253]     Train net output #0: loss = 1.12955 (* 1 = 1.12955 loss)
I0522 14:44:35.004936 27620 sgd_solver.cpp:106] Iteration 60000, lr = 0.0045
I0522 14:44:45.501567 27620 solver.cpp:237] Iteration 60500, loss = 1.2895
I0522 14:44:45.501721 27620 solver.cpp:253]     Train net output #0: loss = 1.2895 (* 1 = 1.2895 loss)
I0522 14:44:45.501735 27620 sgd_solver.cpp:106] Iteration 60500, lr = 0.0045
I0522 14:44:56.009925 27620 solver.cpp:237] Iteration 61000, loss = 1.49329
I0522 14:44:56.009961 27620 solver.cpp:253]     Train net output #0: loss = 1.49329 (* 1 = 1.49329 loss)
I0522 14:44:56.009979 27620 sgd_solver.cpp:106] Iteration 61000, lr = 0.0045
I0522 14:45:06.498570 27620 solver.cpp:237] Iteration 61500, loss = 1.38274
I0522 14:45:06.498617 27620 solver.cpp:253]     Train net output #0: loss = 1.38274 (* 1 = 1.38274 loss)
I0522 14:45:06.498631 27620 sgd_solver.cpp:106] Iteration 61500, lr = 0.0045
I0522 14:45:16.984920 27620 solver.cpp:237] Iteration 62000, loss = 1.22845
I0522 14:45:16.985061 27620 solver.cpp:253]     Train net output #0: loss = 1.22845 (* 1 = 1.22845 loss)
I0522 14:45:16.985076 27620 sgd_solver.cpp:106] Iteration 62000, lr = 0.0045
I0522 14:45:27.482417 27620 solver.cpp:237] Iteration 62500, loss = 1.12254
I0522 14:45:27.482460 27620 solver.cpp:253]     Train net output #0: loss = 1.12254 (* 1 = 1.12254 loss)
I0522 14:45:27.482475 27620 sgd_solver.cpp:106] Iteration 62500, lr = 0.0045
I0522 14:45:38.006357 27620 solver.cpp:237] Iteration 63000, loss = 1.00365
I0522 14:45:38.006393 27620 solver.cpp:253]     Train net output #0: loss = 1.00366 (* 1 = 1.00366 loss)
I0522 14:45:38.006410 27620 sgd_solver.cpp:106] Iteration 63000, lr = 0.0045
I0522 14:46:09.440450 27620 solver.cpp:237] Iteration 63500, loss = 0.94126
I0522 14:46:09.440626 27620 solver.cpp:253]     Train net output #0: loss = 0.94126 (* 1 = 0.94126 loss)
I0522 14:46:09.440641 27620 sgd_solver.cpp:106] Iteration 63500, lr = 0.0045
I0522 14:46:19.971287 27620 solver.cpp:237] Iteration 64000, loss = 0.777106
I0522 14:46:19.971334 27620 solver.cpp:253]     Train net output #0: loss = 0.777106 (* 1 = 0.777106 loss)
I0522 14:46:19.971348 27620 sgd_solver.cpp:106] Iteration 64000, lr = 0.0045
I0522 14:46:30.499222 27620 solver.cpp:237] Iteration 64500, loss = 1.39682
I0522 14:46:30.499258 27620 solver.cpp:253]     Train net output #0: loss = 1.39682 (* 1 = 1.39682 loss)
I0522 14:46:30.499272 27620 sgd_solver.cpp:106] Iteration 64500, lr = 0.0045
I0522 14:46:41.015281 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_65000.caffemodel
I0522 14:46:41.067739 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_65000.solverstate
I0522 14:46:41.100656 27620 solver.cpp:237] Iteration 65000, loss = 1.35788
I0522 14:46:41.100703 27620 solver.cpp:253]     Train net output #0: loss = 1.35788 (* 1 = 1.35788 loss)
I0522 14:46:41.100716 27620 sgd_solver.cpp:106] Iteration 65000, lr = 0.0045
I0522 14:46:51.621501 27620 solver.cpp:237] Iteration 65500, loss = 1.32316
I0522 14:46:51.621537 27620 solver.cpp:253]     Train net output #0: loss = 1.32316 (* 1 = 1.32316 loss)
I0522 14:46:51.621553 27620 sgd_solver.cpp:106] Iteration 65500, lr = 0.0045
I0522 14:47:02.154937 27620 solver.cpp:237] Iteration 66000, loss = 1.27986
I0522 14:47:02.154973 27620 solver.cpp:253]     Train net output #0: loss = 1.27986 (* 1 = 1.27986 loss)
I0522 14:47:02.154989 27620 sgd_solver.cpp:106] Iteration 66000, lr = 0.0045
I0522 14:47:12.674775 27620 solver.cpp:237] Iteration 66500, loss = 1.34687
I0522 14:47:12.674957 27620 solver.cpp:253]     Train net output #0: loss = 1.34687 (* 1 = 1.34687 loss)
I0522 14:47:12.674973 27620 sgd_solver.cpp:106] Iteration 66500, lr = 0.0045
I0522 14:47:44.079273 27620 solver.cpp:237] Iteration 67000, loss = 1.48248
I0522 14:47:44.079450 27620 solver.cpp:253]     Train net output #0: loss = 1.48248 (* 1 = 1.48248 loss)
I0522 14:47:44.079465 27620 sgd_solver.cpp:106] Iteration 67000, lr = 0.0045
I0522 14:47:54.608073 27620 solver.cpp:237] Iteration 67500, loss = 1.27391
I0522 14:47:54.608108 27620 solver.cpp:253]     Train net output #0: loss = 1.27391 (* 1 = 1.27391 loss)
I0522 14:47:54.608124 27620 sgd_solver.cpp:106] Iteration 67500, lr = 0.0045
I0522 14:48:05.137933 27620 solver.cpp:237] Iteration 68000, loss = 0.786735
I0522 14:48:05.137979 27620 solver.cpp:253]     Train net output #0: loss = 0.786735 (* 1 = 0.786735 loss)
I0522 14:48:05.137995 27620 sgd_solver.cpp:106] Iteration 68000, lr = 0.0045
I0522 14:48:15.670649 27620 solver.cpp:237] Iteration 68500, loss = 1.16791
I0522 14:48:15.670799 27620 solver.cpp:253]     Train net output #0: loss = 1.16791 (* 1 = 1.16791 loss)
I0522 14:48:15.670814 27620 sgd_solver.cpp:106] Iteration 68500, lr = 0.0045
I0522 14:48:26.205621 27620 solver.cpp:237] Iteration 69000, loss = 1.02996
I0522 14:48:26.205670 27620 solver.cpp:253]     Train net output #0: loss = 1.02996 (* 1 = 1.02996 loss)
I0522 14:48:26.205684 27620 sgd_solver.cpp:106] Iteration 69000, lr = 0.0045
I0522 14:48:36.730515 27620 solver.cpp:237] Iteration 69500, loss = 0.821482
I0522 14:48:36.730551 27620 solver.cpp:253]     Train net output #0: loss = 0.821483 (* 1 = 0.821483 loss)
I0522 14:48:36.730566 27620 sgd_solver.cpp:106] Iteration 69500, lr = 0.0045
I0522 14:48:47.238549 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_70000.caffemodel
I0522 14:48:47.290652 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_70000.solverstate
I0522 14:48:47.316984 27620 solver.cpp:341] Iteration 70000, Testing net (#0)
I0522 14:49:36.587884 27620 solver.cpp:409]     Test net output #0: accuracy = 0.885914
I0522 14:49:36.588053 27620 solver.cpp:409]     Test net output #1: loss = 0.368168 (* 1 = 0.368168 loss)
I0522 14:49:57.466949 27620 solver.cpp:237] Iteration 70000, loss = 0.795354
I0522 14:49:57.467003 27620 solver.cpp:253]     Train net output #0: loss = 0.795355 (* 1 = 0.795355 loss)
I0522 14:49:57.467018 27620 sgd_solver.cpp:106] Iteration 70000, lr = 0.0045
I0522 14:50:08.066826 27620 solver.cpp:237] Iteration 70500, loss = 0.778701
I0522 14:50:08.066990 27620 solver.cpp:253]     Train net output #0: loss = 0.778701 (* 1 = 0.778701 loss)
I0522 14:50:08.067005 27620 sgd_solver.cpp:106] Iteration 70500, lr = 0.0045
I0522 14:50:18.679733 27620 solver.cpp:237] Iteration 71000, loss = 1.61291
I0522 14:50:18.679769 27620 solver.cpp:253]     Train net output #0: loss = 1.61291 (* 1 = 1.61291 loss)
I0522 14:50:18.679785 27620 sgd_solver.cpp:106] Iteration 71000, lr = 0.0045
I0522 14:50:29.289582 27620 solver.cpp:237] Iteration 71500, loss = 1.16866
I0522 14:50:29.289628 27620 solver.cpp:253]     Train net output #0: loss = 1.16866 (* 1 = 1.16866 loss)
I0522 14:50:29.289644 27620 sgd_solver.cpp:106] Iteration 71500, lr = 0.0045
I0522 14:50:39.899919 27620 solver.cpp:237] Iteration 72000, loss = 1.60524
I0522 14:50:39.900080 27620 solver.cpp:253]     Train net output #0: loss = 1.60524 (* 1 = 1.60524 loss)
I0522 14:50:39.900094 27620 sgd_solver.cpp:106] Iteration 72000, lr = 0.0045
I0522 14:50:50.503859 27620 solver.cpp:237] Iteration 72500, loss = 1.05631
I0522 14:50:50.503906 27620 solver.cpp:253]     Train net output #0: loss = 1.05632 (* 1 = 1.05632 loss)
I0522 14:50:50.503921 27620 sgd_solver.cpp:106] Iteration 72500, lr = 0.0045
I0522 14:51:01.051287 27620 solver.cpp:237] Iteration 73000, loss = 1.10636
I0522 14:51:01.051323 27620 solver.cpp:253]     Train net output #0: loss = 1.10636 (* 1 = 1.10636 loss)
I0522 14:51:01.051339 27620 sgd_solver.cpp:106] Iteration 73000, lr = 0.0045
I0522 14:51:32.454212 27620 solver.cpp:237] Iteration 73500, loss = 1.21163
I0522 14:51:32.454396 27620 solver.cpp:253]     Train net output #0: loss = 1.21163 (* 1 = 1.21163 loss)
I0522 14:51:32.454411 27620 sgd_solver.cpp:106] Iteration 73500, lr = 0.0045
I0522 14:51:43.003124 27620 solver.cpp:237] Iteration 74000, loss = 1.33614
I0522 14:51:43.003173 27620 solver.cpp:253]     Train net output #0: loss = 1.33614 (* 1 = 1.33614 loss)
I0522 14:51:43.003188 27620 sgd_solver.cpp:106] Iteration 74000, lr = 0.0045
I0522 14:51:53.559345 27620 solver.cpp:237] Iteration 74500, loss = 1.20741
I0522 14:51:53.559381 27620 solver.cpp:253]     Train net output #0: loss = 1.20741 (* 1 = 1.20741 loss)
I0522 14:51:53.559396 27620 sgd_solver.cpp:106] Iteration 74500, lr = 0.0045
I0522 14:52:04.085966 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_75000.caffemodel
I0522 14:52:04.140105 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_75000.solverstate
I0522 14:52:04.174929 27620 solver.cpp:237] Iteration 75000, loss = 1.24734
I0522 14:52:04.174976 27620 solver.cpp:253]     Train net output #0: loss = 1.24734 (* 1 = 1.24734 loss)
I0522 14:52:04.174993 27620 sgd_solver.cpp:106] Iteration 75000, lr = 0.0045
I0522 14:52:14.709167 27620 solver.cpp:237] Iteration 75500, loss = 0.910267
I0522 14:52:14.709214 27620 solver.cpp:253]     Train net output #0: loss = 0.910268 (* 1 = 0.910268 loss)
I0522 14:52:14.709229 27620 sgd_solver.cpp:106] Iteration 75500, lr = 0.0045
I0522 14:52:25.234261 27620 solver.cpp:237] Iteration 76000, loss = 1.05023
I0522 14:52:25.234297 27620 solver.cpp:253]     Train net output #0: loss = 1.05023 (* 1 = 1.05023 loss)
I0522 14:52:25.234313 27620 sgd_solver.cpp:106] Iteration 76000, lr = 0.0045
I0522 14:52:35.770974 27620 solver.cpp:237] Iteration 76500, loss = 0.982463
I0522 14:52:35.771158 27620 solver.cpp:253]     Train net output #0: loss = 0.982464 (* 1 = 0.982464 loss)
I0522 14:52:35.771174 27620 sgd_solver.cpp:106] Iteration 76500, lr = 0.0045
I0522 14:53:07.162277 27620 solver.cpp:237] Iteration 77000, loss = 1.32379
I0522 14:53:07.162446 27620 solver.cpp:253]     Train net output #0: loss = 1.32379 (* 1 = 1.32379 loss)
I0522 14:53:07.162462 27620 sgd_solver.cpp:106] Iteration 77000, lr = 0.0045
I0522 14:53:17.712170 27620 solver.cpp:237] Iteration 77500, loss = 1.46813
I0522 14:53:17.712206 27620 solver.cpp:253]     Train net output #0: loss = 1.46814 (* 1 = 1.46814 loss)
I0522 14:53:17.712222 27620 sgd_solver.cpp:106] Iteration 77500, lr = 0.0045
I0522 14:53:28.277864 27620 solver.cpp:237] Iteration 78000, loss = 1.15051
I0522 14:53:28.277915 27620 solver.cpp:253]     Train net output #0: loss = 1.15051 (* 1 = 1.15051 loss)
I0522 14:53:28.277930 27620 sgd_solver.cpp:106] Iteration 78000, lr = 0.0045
I0522 14:53:38.856384 27620 solver.cpp:237] Iteration 78500, loss = 1.12335
I0522 14:53:38.856534 27620 solver.cpp:253]     Train net output #0: loss = 1.12335 (* 1 = 1.12335 loss)
I0522 14:53:38.856549 27620 sgd_solver.cpp:106] Iteration 78500, lr = 0.0045
I0522 14:53:49.425837 27620 solver.cpp:237] Iteration 79000, loss = 0.908177
I0522 14:53:49.425889 27620 solver.cpp:253]     Train net output #0: loss = 0.908178 (* 1 = 0.908178 loss)
I0522 14:53:49.425902 27620 sgd_solver.cpp:106] Iteration 79000, lr = 0.0045
I0522 14:53:59.992787 27620 solver.cpp:237] Iteration 79500, loss = 1.0097
I0522 14:53:59.992823 27620 solver.cpp:253]     Train net output #0: loss = 1.0097 (* 1 = 1.0097 loss)
I0522 14:53:59.992840 27620 sgd_solver.cpp:106] Iteration 79500, lr = 0.0045
I0522 14:54:10.559896 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_80000.caffemodel
I0522 14:54:10.611940 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_80000.solverstate
I0522 14:54:10.638396 27620 solver.cpp:341] Iteration 80000, Testing net (#0)
I0522 14:55:21.086768 27620 solver.cpp:409]     Test net output #0: accuracy = 0.889254
I0522 14:55:21.086940 27620 solver.cpp:409]     Test net output #1: loss = 0.352611 (* 1 = 0.352611 loss)
I0522 14:55:41.941769 27620 solver.cpp:237] Iteration 80000, loss = 0.953004
I0522 14:55:41.941823 27620 solver.cpp:253]     Train net output #0: loss = 0.953005 (* 1 = 0.953005 loss)
I0522 14:55:41.941838 27620 sgd_solver.cpp:106] Iteration 80000, lr = 0.0045
I0522 14:55:52.492596 27620 solver.cpp:237] Iteration 80500, loss = 1.23915
I0522 14:55:52.492761 27620 solver.cpp:253]     Train net output #0: loss = 1.23915 (* 1 = 1.23915 loss)
I0522 14:55:52.492774 27620 sgd_solver.cpp:106] Iteration 80500, lr = 0.0045
I0522 14:56:03.045801 27620 solver.cpp:237] Iteration 81000, loss = 1.33828
I0522 14:56:03.045836 27620 solver.cpp:253]     Train net output #0: loss = 1.33828 (* 1 = 1.33828 loss)
I0522 14:56:03.045853 27620 sgd_solver.cpp:106] Iteration 81000, lr = 0.0045
I0522 14:56:13.591313 27620 solver.cpp:237] Iteration 81500, loss = 0.876017
I0522 14:56:13.591349 27620 solver.cpp:253]     Train net output #0: loss = 0.876017 (* 1 = 0.876017 loss)
I0522 14:56:13.591366 27620 sgd_solver.cpp:106] Iteration 81500, lr = 0.0045
I0522 14:56:24.134160 27620 solver.cpp:237] Iteration 82000, loss = 0.962979
I0522 14:56:24.134313 27620 solver.cpp:253]     Train net output #0: loss = 0.962979 (* 1 = 0.962979 loss)
I0522 14:56:24.134327 27620 sgd_solver.cpp:106] Iteration 82000, lr = 0.0045
I0522 14:56:34.686830 27620 solver.cpp:237] Iteration 82500, loss = 1.18216
I0522 14:56:34.686866 27620 solver.cpp:253]     Train net output #0: loss = 1.18216 (* 1 = 1.18216 loss)
I0522 14:56:34.686883 27620 sgd_solver.cpp:106] Iteration 82500, lr = 0.0045
I0522 14:56:45.236444 27620 solver.cpp:237] Iteration 83000, loss = 0.518919
I0522 14:56:45.236484 27620 solver.cpp:253]     Train net output #0: loss = 0.51892 (* 1 = 0.51892 loss)
I0522 14:56:45.236500 27620 sgd_solver.cpp:106] Iteration 83000, lr = 0.0045
I0522 14:57:16.660961 27620 solver.cpp:237] Iteration 83500, loss = 1.04063
I0522 14:57:16.661137 27620 solver.cpp:253]     Train net output #0: loss = 1.04063 (* 1 = 1.04063 loss)
I0522 14:57:16.661152 27620 sgd_solver.cpp:106] Iteration 83500, lr = 0.0045
I0522 14:57:27.218771 27620 solver.cpp:237] Iteration 84000, loss = 0.885691
I0522 14:57:27.218807 27620 solver.cpp:253]     Train net output #0: loss = 0.885691 (* 1 = 0.885691 loss)
I0522 14:57:27.218824 27620 sgd_solver.cpp:106] Iteration 84000, lr = 0.0045
I0522 14:57:37.750092 27620 solver.cpp:237] Iteration 84500, loss = 1.17489
I0522 14:57:37.750138 27620 solver.cpp:253]     Train net output #0: loss = 1.17489 (* 1 = 1.17489 loss)
I0522 14:57:37.750159 27620 sgd_solver.cpp:106] Iteration 84500, lr = 0.0045
I0522 14:57:48.286309 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_85000.caffemodel
I0522 14:57:48.338413 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_85000.solverstate
I0522 14:57:48.371083 27620 solver.cpp:237] Iteration 85000, loss = 0.999362
I0522 14:57:48.371125 27620 solver.cpp:253]     Train net output #0: loss = 0.999362 (* 1 = 0.999362 loss)
I0522 14:57:48.371143 27620 sgd_solver.cpp:106] Iteration 85000, lr = 0.0045
I0522 14:57:58.917008 27620 solver.cpp:237] Iteration 85500, loss = 1.11166
I0522 14:57:58.917055 27620 solver.cpp:253]     Train net output #0: loss = 1.11166 (* 1 = 1.11166 loss)
I0522 14:57:58.917069 27620 sgd_solver.cpp:106] Iteration 85500, lr = 0.0045
I0522 14:58:09.469571 27620 solver.cpp:237] Iteration 86000, loss = 1.03175
I0522 14:58:09.469607 27620 solver.cpp:253]     Train net output #0: loss = 1.03175 (* 1 = 1.03175 loss)
I0522 14:58:09.469624 27620 sgd_solver.cpp:106] Iteration 86000, lr = 0.0045
I0522 14:58:20.024114 27620 solver.cpp:237] Iteration 86500, loss = 1.04154
I0522 14:58:20.024272 27620 solver.cpp:253]     Train net output #0: loss = 1.04154 (* 1 = 1.04154 loss)
I0522 14:58:20.024286 27620 sgd_solver.cpp:106] Iteration 86500, lr = 0.0045
I0522 14:58:51.490025 27620 solver.cpp:237] Iteration 87000, loss = 1.43617
I0522 14:58:51.490211 27620 solver.cpp:253]     Train net output #0: loss = 1.43617 (* 1 = 1.43617 loss)
I0522 14:58:51.490226 27620 sgd_solver.cpp:106] Iteration 87000, lr = 0.0045
I0522 14:59:02.032623 27620 solver.cpp:237] Iteration 87500, loss = 1.62563
I0522 14:59:02.032658 27620 solver.cpp:253]     Train net output #0: loss = 1.62563 (* 1 = 1.62563 loss)
I0522 14:59:02.032675 27620 sgd_solver.cpp:106] Iteration 87500, lr = 0.0045
I0522 14:59:12.577927 27620 solver.cpp:237] Iteration 88000, loss = 1.59991
I0522 14:59:12.577971 27620 solver.cpp:253]     Train net output #0: loss = 1.59991 (* 1 = 1.59991 loss)
I0522 14:59:12.577987 27620 sgd_solver.cpp:106] Iteration 88000, lr = 0.0045
I0522 14:59:23.127982 27620 solver.cpp:237] Iteration 88500, loss = 1.03519
I0522 14:59:23.128134 27620 solver.cpp:253]     Train net output #0: loss = 1.03519 (* 1 = 1.03519 loss)
I0522 14:59:23.128149 27620 sgd_solver.cpp:106] Iteration 88500, lr = 0.0045
I0522 14:59:33.694780 27620 solver.cpp:237] Iteration 89000, loss = 1.30895
I0522 14:59:33.694818 27620 solver.cpp:253]     Train net output #0: loss = 1.30895 (* 1 = 1.30895 loss)
I0522 14:59:33.694833 27620 sgd_solver.cpp:106] Iteration 89000, lr = 0.0045
I0522 14:59:44.245537 27620 solver.cpp:237] Iteration 89500, loss = 1.0443
I0522 14:59:44.245584 27620 solver.cpp:253]     Train net output #0: loss = 1.0443 (* 1 = 1.0443 loss)
I0522 14:59:44.245599 27620 sgd_solver.cpp:106] Iteration 89500, lr = 0.0045
I0522 14:59:54.778607 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_90000.caffemodel
I0522 14:59:54.832331 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_90000.solverstate
I0522 14:59:54.858777 27620 solver.cpp:341] Iteration 90000, Testing net (#0)
I0522 15:00:44.466150 27620 solver.cpp:409]     Test net output #0: accuracy = 0.8904
I0522 15:00:44.466326 27620 solver.cpp:409]     Test net output #1: loss = 0.354127 (* 1 = 0.354127 loss)
I0522 15:01:05.353261 27620 solver.cpp:237] Iteration 90000, loss = 1.25067
I0522 15:01:05.353314 27620 solver.cpp:253]     Train net output #0: loss = 1.25067 (* 1 = 1.25067 loss)
I0522 15:01:05.353330 27620 sgd_solver.cpp:106] Iteration 90000, lr = 0.0045
I0522 15:01:15.885154 27620 solver.cpp:237] Iteration 90500, loss = 0.916357
I0522 15:01:15.885326 27620 solver.cpp:253]     Train net output #0: loss = 0.916358 (* 1 = 0.916358 loss)
I0522 15:01:15.885341 27620 sgd_solver.cpp:106] Iteration 90500, lr = 0.0045
I0522 15:01:26.429014 27620 solver.cpp:237] Iteration 91000, loss = 1.11385
I0522 15:01:26.429050 27620 solver.cpp:253]     Train net output #0: loss = 1.11385 (* 1 = 1.11385 loss)
I0522 15:01:26.429065 27620 sgd_solver.cpp:106] Iteration 91000, lr = 0.0045
I0522 15:01:36.961917 27620 solver.cpp:237] Iteration 91500, loss = 0.986368
I0522 15:01:36.961954 27620 solver.cpp:253]     Train net output #0: loss = 0.986368 (* 1 = 0.986368 loss)
I0522 15:01:36.961967 27620 sgd_solver.cpp:106] Iteration 91500, lr = 0.0045
I0522 15:01:47.488567 27620 solver.cpp:237] Iteration 92000, loss = 1.37277
I0522 15:01:47.488756 27620 solver.cpp:253]     Train net output #0: loss = 1.37277 (* 1 = 1.37277 loss)
I0522 15:01:47.488771 27620 sgd_solver.cpp:106] Iteration 92000, lr = 0.0045
I0522 15:01:58.023080 27620 solver.cpp:237] Iteration 92500, loss = 1.24731
I0522 15:01:58.023118 27620 solver.cpp:253]     Train net output #0: loss = 1.24731 (* 1 = 1.24731 loss)
I0522 15:01:58.023134 27620 sgd_solver.cpp:106] Iteration 92500, lr = 0.0045
I0522 15:02:08.552608 27620 solver.cpp:237] Iteration 93000, loss = 0.945648
I0522 15:02:08.552651 27620 solver.cpp:253]     Train net output #0: loss = 0.945648 (* 1 = 0.945648 loss)
I0522 15:02:08.552665 27620 sgd_solver.cpp:106] Iteration 93000, lr = 0.0045
I0522 15:02:40.014391 27620 solver.cpp:237] Iteration 93500, loss = 1.07131
I0522 15:02:40.014574 27620 solver.cpp:253]     Train net output #0: loss = 1.07131 (* 1 = 1.07131 loss)
I0522 15:02:40.014588 27620 sgd_solver.cpp:106] Iteration 93500, lr = 0.0045
I0522 15:02:50.559725 27620 solver.cpp:237] Iteration 94000, loss = 1.13465
I0522 15:02:50.559762 27620 solver.cpp:253]     Train net output #0: loss = 1.13465 (* 1 = 1.13465 loss)
I0522 15:02:50.559777 27620 sgd_solver.cpp:106] Iteration 94000, lr = 0.0045
I0522 15:03:01.075264 27620 solver.cpp:237] Iteration 94500, loss = 1.22528
I0522 15:03:01.075312 27620 solver.cpp:253]     Train net output #0: loss = 1.22528 (* 1 = 1.22528 loss)
I0522 15:03:01.075325 27620 sgd_solver.cpp:106] Iteration 94500, lr = 0.0045
I0522 15:03:11.595155 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_95000.caffemodel
I0522 15:03:11.649566 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_95000.solverstate
I0522 15:03:11.684494 27620 solver.cpp:237] Iteration 95000, loss = 1.36464
I0522 15:03:11.684540 27620 solver.cpp:253]     Train net output #0: loss = 1.36464 (* 1 = 1.36464 loss)
I0522 15:03:11.684561 27620 sgd_solver.cpp:106] Iteration 95000, lr = 0.0045
I0522 15:03:22.213646 27620 solver.cpp:237] Iteration 95500, loss = 1.17366
I0522 15:03:22.213697 27620 solver.cpp:253]     Train net output #0: loss = 1.17366 (* 1 = 1.17366 loss)
I0522 15:03:22.213711 27620 sgd_solver.cpp:106] Iteration 95500, lr = 0.0045
I0522 15:03:32.723494 27620 solver.cpp:237] Iteration 96000, loss = 1.50255
I0522 15:03:32.723531 27620 solver.cpp:253]     Train net output #0: loss = 1.50255 (* 1 = 1.50255 loss)
I0522 15:03:32.723546 27620 sgd_solver.cpp:106] Iteration 96000, lr = 0.0045
I0522 15:03:43.260879 27620 solver.cpp:237] Iteration 96500, loss = 1.1416
I0522 15:03:43.261051 27620 solver.cpp:253]     Train net output #0: loss = 1.1416 (* 1 = 1.1416 loss)
I0522 15:03:43.261066 27620 sgd_solver.cpp:106] Iteration 96500, lr = 0.0045
I0522 15:04:14.647326 27620 solver.cpp:237] Iteration 97000, loss = 1.20362
I0522 15:04:14.647501 27620 solver.cpp:253]     Train net output #0: loss = 1.20362 (* 1 = 1.20362 loss)
I0522 15:04:14.647516 27620 sgd_solver.cpp:106] Iteration 97000, lr = 0.0045
I0522 15:04:25.175989 27620 solver.cpp:237] Iteration 97500, loss = 1.147
I0522 15:04:25.176025 27620 solver.cpp:253]     Train net output #0: loss = 1.147 (* 1 = 1.147 loss)
I0522 15:04:25.176043 27620 sgd_solver.cpp:106] Iteration 97500, lr = 0.0045
I0522 15:04:35.727272 27620 solver.cpp:237] Iteration 98000, loss = 1.37314
I0522 15:04:35.727318 27620 solver.cpp:253]     Train net output #0: loss = 1.37314 (* 1 = 1.37314 loss)
I0522 15:04:35.727332 27620 sgd_solver.cpp:106] Iteration 98000, lr = 0.0045
I0522 15:04:46.269845 27620 solver.cpp:237] Iteration 98500, loss = 0.980528
I0522 15:04:46.270011 27620 solver.cpp:253]     Train net output #0: loss = 0.980529 (* 1 = 0.980529 loss)
I0522 15:04:46.270025 27620 sgd_solver.cpp:106] Iteration 98500, lr = 0.0045
I0522 15:04:56.821035 27620 solver.cpp:237] Iteration 99000, loss = 1.63519
I0522 15:04:56.821071 27620 solver.cpp:253]     Train net output #0: loss = 1.63519 (* 1 = 1.63519 loss)
I0522 15:04:56.821089 27620 sgd_solver.cpp:106] Iteration 99000, lr = 0.0045
I0522 15:05:07.353415 27620 solver.cpp:237] Iteration 99500, loss = 1.83342
I0522 15:05:07.353459 27620 solver.cpp:253]     Train net output #0: loss = 1.83342 (* 1 = 1.83342 loss)
I0522 15:05:07.353474 27620 sgd_solver.cpp:106] Iteration 99500, lr = 0.0045
I0522 15:05:17.871755 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_100000.caffemodel
I0522 15:05:17.928526 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_100000.solverstate
I0522 15:05:17.956073 27620 solver.cpp:341] Iteration 100000, Testing net (#0)
I0522 15:06:28.484150 27620 solver.cpp:409]     Test net output #0: accuracy = 0.892919
I0522 15:06:28.484329 27620 solver.cpp:409]     Test net output #1: loss = 0.353332 (* 1 = 0.353332 loss)
I0522 15:06:49.371640 27620 solver.cpp:237] Iteration 100000, loss = 1.26829
I0522 15:06:49.371693 27620 solver.cpp:253]     Train net output #0: loss = 1.26829 (* 1 = 1.26829 loss)
I0522 15:06:49.371711 27620 sgd_solver.cpp:106] Iteration 100000, lr = 0.0045
I0522 15:06:59.887522 27620 solver.cpp:237] Iteration 100500, loss = 1.13548
I0522 15:06:59.887696 27620 solver.cpp:253]     Train net output #0: loss = 1.13548 (* 1 = 1.13548 loss)
I0522 15:06:59.887709 27620 sgd_solver.cpp:106] Iteration 100500, lr = 0.0045
I0522 15:07:10.390971 27620 solver.cpp:237] Iteration 101000, loss = 1.45025
I0522 15:07:10.391021 27620 solver.cpp:253]     Train net output #0: loss = 1.45025 (* 1 = 1.45025 loss)
I0522 15:07:10.391033 27620 sgd_solver.cpp:106] Iteration 101000, lr = 0.0045
I0522 15:07:20.901038 27620 solver.cpp:237] Iteration 101500, loss = 1.36707
I0522 15:07:20.901074 27620 solver.cpp:253]     Train net output #0: loss = 1.36707 (* 1 = 1.36707 loss)
I0522 15:07:20.901089 27620 sgd_solver.cpp:106] Iteration 101500, lr = 0.0045
I0522 15:07:31.410085 27620 solver.cpp:237] Iteration 102000, loss = 0.808161
I0522 15:07:31.410254 27620 solver.cpp:253]     Train net output #0: loss = 0.808161 (* 1 = 0.808161 loss)
I0522 15:07:31.410269 27620 sgd_solver.cpp:106] Iteration 102000, lr = 0.0045
I0522 15:07:41.920168 27620 solver.cpp:237] Iteration 102500, loss = 1.28858
I0522 15:07:41.920204 27620 solver.cpp:253]     Train net output #0: loss = 1.28858 (* 1 = 1.28858 loss)
I0522 15:07:41.920217 27620 sgd_solver.cpp:106] Iteration 102500, lr = 0.0045
I0522 15:07:52.422318 27620 solver.cpp:237] Iteration 103000, loss = 1.28245
I0522 15:07:52.422354 27620 solver.cpp:253]     Train net output #0: loss = 1.28245 (* 1 = 1.28245 loss)
I0522 15:07:52.422371 27620 sgd_solver.cpp:106] Iteration 103000, lr = 0.0045
I0522 15:08:23.814162 27620 solver.cpp:237] Iteration 103500, loss = 1.38211
I0522 15:08:23.814343 27620 solver.cpp:253]     Train net output #0: loss = 1.38211 (* 1 = 1.38211 loss)
I0522 15:08:23.814359 27620 sgd_solver.cpp:106] Iteration 103500, lr = 0.0045
I0522 15:08:34.328656 27620 solver.cpp:237] Iteration 104000, loss = 1.21184
I0522 15:08:34.328692 27620 solver.cpp:253]     Train net output #0: loss = 1.21184 (* 1 = 1.21184 loss)
I0522 15:08:34.328708 27620 sgd_solver.cpp:106] Iteration 104000, lr = 0.0045
I0522 15:08:44.832993 27620 solver.cpp:237] Iteration 104500, loss = 2.17827
I0522 15:08:44.833034 27620 solver.cpp:253]     Train net output #0: loss = 2.17827 (* 1 = 2.17827 loss)
I0522 15:08:44.833055 27620 sgd_solver.cpp:106] Iteration 104500, lr = 0.0045
I0522 15:08:55.321393 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_105000.caffemodel
I0522 15:08:55.374135 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_105000.solverstate
I0522 15:08:55.406018 27620 solver.cpp:237] Iteration 105000, loss = 1.18232
I0522 15:08:55.406059 27620 solver.cpp:253]     Train net output #0: loss = 1.18232 (* 1 = 1.18232 loss)
I0522 15:08:55.406077 27620 sgd_solver.cpp:106] Iteration 105000, lr = 0.0045
I0522 15:09:05.925058 27620 solver.cpp:237] Iteration 105500, loss = 1.09811
I0522 15:09:05.925094 27620 solver.cpp:253]     Train net output #0: loss = 1.09812 (* 1 = 1.09812 loss)
I0522 15:09:05.925109 27620 sgd_solver.cpp:106] Iteration 105500, lr = 0.0045
I0522 15:09:16.423246 27620 solver.cpp:237] Iteration 106000, loss = 0.873964
I0522 15:09:16.423297 27620 solver.cpp:253]     Train net output #0: loss = 0.873965 (* 1 = 0.873965 loss)
I0522 15:09:16.423312 27620 sgd_solver.cpp:106] Iteration 106000, lr = 0.0045
I0522 15:09:26.933107 27620 solver.cpp:237] Iteration 106500, loss = 1.14564
I0522 15:09:26.933266 27620 solver.cpp:253]     Train net output #0: loss = 1.14565 (* 1 = 1.14565 loss)
I0522 15:09:26.933281 27620 sgd_solver.cpp:106] Iteration 106500, lr = 0.0045
I0522 15:09:58.342608 27620 solver.cpp:237] Iteration 107000, loss = 1.16174
I0522 15:09:58.342792 27620 solver.cpp:253]     Train net output #0: loss = 1.16174 (* 1 = 1.16174 loss)
I0522 15:09:58.342806 27620 sgd_solver.cpp:106] Iteration 107000, lr = 0.0045
I0522 15:10:08.848428 27620 solver.cpp:237] Iteration 107500, loss = 1.40893
I0522 15:10:08.848471 27620 solver.cpp:253]     Train net output #0: loss = 1.40893 (* 1 = 1.40893 loss)
I0522 15:10:08.848487 27620 sgd_solver.cpp:106] Iteration 107500, lr = 0.0045
I0522 15:10:19.360707 27620 solver.cpp:237] Iteration 108000, loss = 1.18671
I0522 15:10:19.360743 27620 solver.cpp:253]     Train net output #0: loss = 1.18671 (* 1 = 1.18671 loss)
I0522 15:10:19.360759 27620 sgd_solver.cpp:106] Iteration 108000, lr = 0.0045
I0522 15:10:29.861410 27620 solver.cpp:237] Iteration 108500, loss = 1.46943
I0522 15:10:29.861584 27620 solver.cpp:253]     Train net output #0: loss = 1.46943 (* 1 = 1.46943 loss)
I0522 15:10:29.861599 27620 sgd_solver.cpp:106] Iteration 108500, lr = 0.0045
I0522 15:10:40.364691 27620 solver.cpp:237] Iteration 109000, loss = 1.16492
I0522 15:10:40.364727 27620 solver.cpp:253]     Train net output #0: loss = 1.16492 (* 1 = 1.16492 loss)
I0522 15:10:40.364742 27620 sgd_solver.cpp:106] Iteration 109000, lr = 0.0045
I0522 15:10:50.876930 27620 solver.cpp:237] Iteration 109500, loss = 1.066
I0522 15:10:50.876976 27620 solver.cpp:253]     Train net output #0: loss = 1.06601 (* 1 = 1.06601 loss)
I0522 15:10:50.876991 27620 sgd_solver.cpp:106] Iteration 109500, lr = 0.0045
I0522 15:11:01.359648 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_110000.caffemodel
I0522 15:11:01.413321 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_110000.solverstate
I0522 15:11:01.439244 27620 solver.cpp:341] Iteration 110000, Testing net (#0)
I0522 15:11:50.691524 27620 solver.cpp:409]     Test net output #0: accuracy = 0.893507
I0522 15:11:50.691706 27620 solver.cpp:409]     Test net output #1: loss = 0.331819 (* 1 = 0.331819 loss)
I0522 15:12:11.491821 27620 solver.cpp:237] Iteration 110000, loss = 1.14659
I0522 15:12:11.491873 27620 solver.cpp:253]     Train net output #0: loss = 1.14659 (* 1 = 1.14659 loss)
I0522 15:12:11.491888 27620 sgd_solver.cpp:106] Iteration 110000, lr = 0.0045
I0522 15:12:22.137810 27620 solver.cpp:237] Iteration 110500, loss = 1.37428
I0522 15:12:22.137976 27620 solver.cpp:253]     Train net output #0: loss = 1.37428 (* 1 = 1.37428 loss)
I0522 15:12:22.137990 27620 sgd_solver.cpp:106] Iteration 110500, lr = 0.0045
I0522 15:12:32.789942 27620 solver.cpp:237] Iteration 111000, loss = 1.1155
I0522 15:12:32.789985 27620 solver.cpp:253]     Train net output #0: loss = 1.1155 (* 1 = 1.1155 loss)
I0522 15:12:32.790000 27620 sgd_solver.cpp:106] Iteration 111000, lr = 0.0045
I0522 15:12:43.461699 27620 solver.cpp:237] Iteration 111500, loss = 1.23947
I0522 15:12:43.461735 27620 solver.cpp:253]     Train net output #0: loss = 1.23947 (* 1 = 1.23947 loss)
I0522 15:12:43.461750 27620 sgd_solver.cpp:106] Iteration 111500, lr = 0.0045
I0522 15:12:54.137603 27620 solver.cpp:237] Iteration 112000, loss = 1.20452
I0522 15:12:54.137759 27620 solver.cpp:253]     Train net output #0: loss = 1.20452 (* 1 = 1.20452 loss)
I0522 15:12:54.137774 27620 sgd_solver.cpp:106] Iteration 112000, lr = 0.0045
I0522 15:13:04.796277 27620 solver.cpp:237] Iteration 112500, loss = 1.4286
I0522 15:13:04.796321 27620 solver.cpp:253]     Train net output #0: loss = 1.4286 (* 1 = 1.4286 loss)
I0522 15:13:04.796336 27620 sgd_solver.cpp:106] Iteration 112500, lr = 0.0045
I0522 15:13:15.439957 27620 solver.cpp:237] Iteration 113000, loss = 1.07756
I0522 15:13:15.439992 27620 solver.cpp:253]     Train net output #0: loss = 1.07756 (* 1 = 1.07756 loss)
I0522 15:13:15.440006 27620 sgd_solver.cpp:106] Iteration 113000, lr = 0.0045
I0522 15:13:46.945981 27620 solver.cpp:237] Iteration 113500, loss = 0.90961
I0522 15:13:46.946166 27620 solver.cpp:253]     Train net output #0: loss = 0.909611 (* 1 = 0.909611 loss)
I0522 15:13:46.946180 27620 sgd_solver.cpp:106] Iteration 113500, lr = 0.0045
I0522 15:13:57.548794 27620 solver.cpp:237] Iteration 114000, loss = 0.833462
I0522 15:13:57.548832 27620 solver.cpp:253]     Train net output #0: loss = 0.833462 (* 1 = 0.833462 loss)
I0522 15:13:57.548847 27620 sgd_solver.cpp:106] Iteration 114000, lr = 0.0045
I0522 15:14:08.157199 27620 solver.cpp:237] Iteration 114500, loss = 1.32712
I0522 15:14:08.157235 27620 solver.cpp:253]     Train net output #0: loss = 1.32712 (* 1 = 1.32712 loss)
I0522 15:14:08.157251 27620 sgd_solver.cpp:106] Iteration 114500, lr = 0.0045
I0522 15:14:18.749972 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_115000.caffemodel
I0522 15:14:18.803390 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_115000.solverstate
I0522 15:14:18.835841 27620 solver.cpp:237] Iteration 115000, loss = 0.910991
I0522 15:14:18.835887 27620 solver.cpp:253]     Train net output #0: loss = 0.910992 (* 1 = 0.910992 loss)
I0522 15:14:18.835901 27620 sgd_solver.cpp:106] Iteration 115000, lr = 0.0045
I0522 15:14:29.436897 27620 solver.cpp:237] Iteration 115500, loss = 1.36413
I0522 15:14:29.436933 27620 solver.cpp:253]     Train net output #0: loss = 1.36413 (* 1 = 1.36413 loss)
I0522 15:14:29.436946 27620 sgd_solver.cpp:106] Iteration 115500, lr = 0.0045
I0522 15:14:40.044811 27620 solver.cpp:237] Iteration 116000, loss = 0.891365
I0522 15:14:40.044858 27620 solver.cpp:253]     Train net output #0: loss = 0.891366 (* 1 = 0.891366 loss)
I0522 15:14:40.044873 27620 sgd_solver.cpp:106] Iteration 116000, lr = 0.0045
I0522 15:14:50.640650 27620 solver.cpp:237] Iteration 116500, loss = 0.83926
I0522 15:14:50.640815 27620 solver.cpp:253]     Train net output #0: loss = 0.839261 (* 1 = 0.839261 loss)
I0522 15:14:50.640828 27620 sgd_solver.cpp:106] Iteration 116500, lr = 0.0045
I0522 15:15:22.095224 27620 solver.cpp:237] Iteration 117000, loss = 1.2692
I0522 15:15:22.095424 27620 solver.cpp:253]     Train net output #0: loss = 1.2692 (* 1 = 1.2692 loss)
I0522 15:15:22.095440 27620 sgd_solver.cpp:106] Iteration 117000, lr = 0.0045
I0522 15:15:32.712978 27620 solver.cpp:237] Iteration 117500, loss = 1.29332
I0522 15:15:32.713024 27620 solver.cpp:253]     Train net output #0: loss = 1.29332 (* 1 = 1.29332 loss)
I0522 15:15:32.713040 27620 sgd_solver.cpp:106] Iteration 117500, lr = 0.0045
I0522 15:15:43.315659 27620 solver.cpp:237] Iteration 118000, loss = 0.74526
I0522 15:15:43.315696 27620 solver.cpp:253]     Train net output #0: loss = 0.745261 (* 1 = 0.745261 loss)
I0522 15:15:43.315712 27620 sgd_solver.cpp:106] Iteration 118000, lr = 0.0045
I0522 15:15:53.925458 27620 solver.cpp:237] Iteration 118500, loss = 1.2527
I0522 15:15:53.925631 27620 solver.cpp:253]     Train net output #0: loss = 1.2527 (* 1 = 1.2527 loss)
I0522 15:15:53.925647 27620 sgd_solver.cpp:106] Iteration 118500, lr = 0.0045
I0522 15:16:04.521340 27620 solver.cpp:237] Iteration 119000, loss = 1.13041
I0522 15:16:04.521378 27620 solver.cpp:253]     Train net output #0: loss = 1.13041 (* 1 = 1.13041 loss)
I0522 15:16:04.521394 27620 sgd_solver.cpp:106] Iteration 119000, lr = 0.0045
I0522 15:16:15.127378 27620 solver.cpp:237] Iteration 119500, loss = 1.19284
I0522 15:16:15.127424 27620 solver.cpp:253]     Train net output #0: loss = 1.19284 (* 1 = 1.19284 loss)
I0522 15:16:15.127442 27620 sgd_solver.cpp:106] Iteration 119500, lr = 0.0045
I0522 15:16:25.709239 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_120000.caffemodel
I0522 15:16:25.761586 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_120000.solverstate
I0522 15:16:25.787016 27620 solver.cpp:341] Iteration 120000, Testing net (#0)
I0522 15:17:36.215756 27620 solver.cpp:409]     Test net output #0: accuracy = 0.893292
I0522 15:17:36.215932 27620 solver.cpp:409]     Test net output #1: loss = 0.349607 (* 1 = 0.349607 loss)
I0522 15:17:57.071300 27620 solver.cpp:237] Iteration 120000, loss = 0.91429
I0522 15:17:57.071354 27620 solver.cpp:253]     Train net output #0: loss = 0.91429 (* 1 = 0.91429 loss)
I0522 15:17:57.071372 27620 sgd_solver.cpp:106] Iteration 120000, lr = 0.0045
I0522 15:18:07.704016 27620 solver.cpp:237] Iteration 120500, loss = 0.871881
I0522 15:18:07.704182 27620 solver.cpp:253]     Train net output #0: loss = 0.871882 (* 1 = 0.871882 loss)
I0522 15:18:07.704196 27620 sgd_solver.cpp:106] Iteration 120500, lr = 0.0045
I0522 15:18:18.324604 27620 solver.cpp:237] Iteration 121000, loss = 1.49202
I0522 15:18:18.324640 27620 solver.cpp:253]     Train net output #0: loss = 1.49202 (* 1 = 1.49202 loss)
I0522 15:18:18.324656 27620 sgd_solver.cpp:106] Iteration 121000, lr = 0.0045
I0522 15:18:28.963627 27620 solver.cpp:237] Iteration 121500, loss = 0.87804
I0522 15:18:28.963675 27620 solver.cpp:253]     Train net output #0: loss = 0.878041 (* 1 = 0.878041 loss)
I0522 15:18:28.963690 27620 sgd_solver.cpp:106] Iteration 121500, lr = 0.0045
I0522 15:18:39.573575 27620 solver.cpp:237] Iteration 122000, loss = 1.37758
I0522 15:18:39.573735 27620 solver.cpp:253]     Train net output #0: loss = 1.37759 (* 1 = 1.37759 loss)
I0522 15:18:39.573750 27620 sgd_solver.cpp:106] Iteration 122000, lr = 0.0045
I0522 15:18:50.188243 27620 solver.cpp:237] Iteration 122500, loss = 1.07245
I0522 15:18:50.188284 27620 solver.cpp:253]     Train net output #0: loss = 1.07245 (* 1 = 1.07245 loss)
I0522 15:18:50.188302 27620 sgd_solver.cpp:106] Iteration 122500, lr = 0.0045
I0522 15:19:00.813454 27620 solver.cpp:237] Iteration 123000, loss = 1.19781
I0522 15:19:00.813490 27620 solver.cpp:253]     Train net output #0: loss = 1.19781 (* 1 = 1.19781 loss)
I0522 15:19:00.813506 27620 sgd_solver.cpp:106] Iteration 123000, lr = 0.0045
I0522 15:19:32.299121 27620 solver.cpp:237] Iteration 123500, loss = 1.17866
I0522 15:19:32.299311 27620 solver.cpp:253]     Train net output #0: loss = 1.17866 (* 1 = 1.17866 loss)
I0522 15:19:32.299326 27620 sgd_solver.cpp:106] Iteration 123500, lr = 0.0045
I0522 15:19:42.914513 27620 solver.cpp:237] Iteration 124000, loss = 1.07569
I0522 15:19:42.914558 27620 solver.cpp:253]     Train net output #0: loss = 1.07569 (* 1 = 1.07569 loss)
I0522 15:19:42.914573 27620 sgd_solver.cpp:106] Iteration 124000, lr = 0.0045
I0522 15:19:53.529312 27620 solver.cpp:237] Iteration 124500, loss = 1.05408
I0522 15:19:53.529348 27620 solver.cpp:253]     Train net output #0: loss = 1.05408 (* 1 = 1.05408 loss)
I0522 15:19:53.529361 27620 sgd_solver.cpp:106] Iteration 124500, lr = 0.0045
I0522 15:20:04.122241 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_125000.caffemodel
I0522 15:20:04.177448 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_125000.solverstate
I0522 15:20:04.211294 27620 solver.cpp:237] Iteration 125000, loss = 1.30259
I0522 15:20:04.211345 27620 solver.cpp:253]     Train net output #0: loss = 1.3026 (* 1 = 1.3026 loss)
I0522 15:20:04.211359 27620 sgd_solver.cpp:106] Iteration 125000, lr = 0.0045
I0522 15:20:14.830164 27620 solver.cpp:237] Iteration 125500, loss = 0.801467
I0522 15:20:14.830202 27620 solver.cpp:253]     Train net output #0: loss = 0.801468 (* 1 = 0.801468 loss)
I0522 15:20:14.830219 27620 sgd_solver.cpp:106] Iteration 125500, lr = 0.0045
I0522 15:20:25.428958 27620 solver.cpp:237] Iteration 126000, loss = 0.894256
I0522 15:20:25.429013 27620 solver.cpp:253]     Train net output #0: loss = 0.894257 (* 1 = 0.894257 loss)
I0522 15:20:25.429026 27620 sgd_solver.cpp:106] Iteration 126000, lr = 0.0045
I0522 15:20:36.036800 27620 solver.cpp:237] Iteration 126500, loss = 1.15936
I0522 15:20:36.036965 27620 solver.cpp:253]     Train net output #0: loss = 1.15937 (* 1 = 1.15937 loss)
I0522 15:20:36.036978 27620 sgd_solver.cpp:106] Iteration 126500, lr = 0.0045
I0522 15:21:07.518805 27620 solver.cpp:237] Iteration 127000, loss = 1.08455
I0522 15:21:07.519001 27620 solver.cpp:253]     Train net output #0: loss = 1.08455 (* 1 = 1.08455 loss)
I0522 15:21:07.519016 27620 sgd_solver.cpp:106] Iteration 127000, lr = 0.0045
I0522 15:21:18.120833 27620 solver.cpp:237] Iteration 127500, loss = 0.786438
I0522 15:21:18.120882 27620 solver.cpp:253]     Train net output #0: loss = 0.786439 (* 1 = 0.786439 loss)
I0522 15:21:18.120894 27620 sgd_solver.cpp:106] Iteration 127500, lr = 0.0045
I0522 15:21:28.715253 27620 solver.cpp:237] Iteration 128000, loss = 1.13626
I0522 15:21:28.715289 27620 solver.cpp:253]     Train net output #0: loss = 1.13626 (* 1 = 1.13626 loss)
I0522 15:21:28.715306 27620 sgd_solver.cpp:106] Iteration 128000, lr = 0.0045
I0522 15:21:39.313334 27620 solver.cpp:237] Iteration 128500, loss = 1.10216
I0522 15:21:39.313505 27620 solver.cpp:253]     Train net output #0: loss = 1.10217 (* 1 = 1.10217 loss)
I0522 15:21:39.313519 27620 sgd_solver.cpp:106] Iteration 128500, lr = 0.0045
I0522 15:21:49.937041 27620 solver.cpp:237] Iteration 129000, loss = 1.01091
I0522 15:21:49.937088 27620 solver.cpp:253]     Train net output #0: loss = 1.01091 (* 1 = 1.01091 loss)
I0522 15:21:49.937103 27620 sgd_solver.cpp:106] Iteration 129000, lr = 0.0045
I0522 15:22:00.557426 27620 solver.cpp:237] Iteration 129500, loss = 1.04695
I0522 15:22:00.557463 27620 solver.cpp:253]     Train net output #0: loss = 1.04695 (* 1 = 1.04695 loss)
I0522 15:22:00.557478 27620 sgd_solver.cpp:106] Iteration 129500, lr = 0.0045
I0522 15:22:11.142122 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_130000.caffemodel
I0522 15:22:11.195641 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_130000.solverstate
I0522 15:22:11.221148 27620 solver.cpp:341] Iteration 130000, Testing net (#0)
I0522 15:23:00.848275 27620 solver.cpp:409]     Test net output #0: accuracy = 0.897531
I0522 15:23:00.848464 27620 solver.cpp:409]     Test net output #1: loss = 0.330042 (* 1 = 0.330042 loss)
I0522 15:23:21.686535 27620 solver.cpp:237] Iteration 130000, loss = 1.15202
I0522 15:23:21.686589 27620 solver.cpp:253]     Train net output #0: loss = 1.15202 (* 1 = 1.15202 loss)
I0522 15:23:21.686604 27620 sgd_solver.cpp:106] Iteration 130000, lr = 0.0045
I0522 15:23:32.187367 27620 solver.cpp:237] Iteration 130500, loss = 0.967928
I0522 15:23:32.187537 27620 solver.cpp:253]     Train net output #0: loss = 0.967929 (* 1 = 0.967929 loss)
I0522 15:23:32.187552 27620 sgd_solver.cpp:106] Iteration 130500, lr = 0.0045
I0522 15:23:42.704166 27620 solver.cpp:237] Iteration 131000, loss = 1.26611
I0522 15:23:42.704202 27620 solver.cpp:253]     Train net output #0: loss = 1.26611 (* 1 = 1.26611 loss)
I0522 15:23:42.704218 27620 sgd_solver.cpp:106] Iteration 131000, lr = 0.0045
I0522 15:23:53.202180 27620 solver.cpp:237] Iteration 131500, loss = 1.12464
I0522 15:23:53.202232 27620 solver.cpp:253]     Train net output #0: loss = 1.12464 (* 1 = 1.12464 loss)
I0522 15:23:53.202245 27620 sgd_solver.cpp:106] Iteration 131500, lr = 0.0045
I0522 15:24:03.701186 27620 solver.cpp:237] Iteration 132000, loss = 1.06279
I0522 15:24:03.701356 27620 solver.cpp:253]     Train net output #0: loss = 1.06279 (* 1 = 1.06279 loss)
I0522 15:24:03.701371 27620 sgd_solver.cpp:106] Iteration 132000, lr = 0.0045
I0522 15:24:14.211633 27620 solver.cpp:237] Iteration 132500, loss = 1.28972
I0522 15:24:14.211681 27620 solver.cpp:253]     Train net output #0: loss = 1.28972 (* 1 = 1.28972 loss)
I0522 15:24:14.211694 27620 sgd_solver.cpp:106] Iteration 132500, lr = 0.0045
I0522 15:24:24.721479 27620 solver.cpp:237] Iteration 133000, loss = 0.826463
I0522 15:24:24.721523 27620 solver.cpp:253]     Train net output #0: loss = 0.826463 (* 1 = 0.826463 loss)
I0522 15:24:24.721536 27620 sgd_solver.cpp:106] Iteration 133000, lr = 0.0045
I0522 15:24:56.134395 27620 solver.cpp:237] Iteration 133500, loss = 1.26002
I0522 15:24:56.134579 27620 solver.cpp:253]     Train net output #0: loss = 1.26002 (* 1 = 1.26002 loss)
I0522 15:24:56.134595 27620 sgd_solver.cpp:106] Iteration 133500, lr = 0.0045
I0522 15:25:06.636731 27620 solver.cpp:237] Iteration 134000, loss = 0.874358
I0522 15:25:06.636777 27620 solver.cpp:253]     Train net output #0: loss = 0.874358 (* 1 = 0.874358 loss)
I0522 15:25:06.636792 27620 sgd_solver.cpp:106] Iteration 134000, lr = 0.0045
I0522 15:25:17.148499 27620 solver.cpp:237] Iteration 134500, loss = 1.42971
I0522 15:25:17.148535 27620 solver.cpp:253]     Train net output #0: loss = 1.42971 (* 1 = 1.42971 loss)
I0522 15:25:17.148548 27620 sgd_solver.cpp:106] Iteration 134500, lr = 0.0045
I0522 15:25:27.637979 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_135000.caffemodel
I0522 15:25:27.690961 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_135000.solverstate
I0522 15:25:27.723407 27620 solver.cpp:237] Iteration 135000, loss = 1.11571
I0522 15:25:27.723449 27620 solver.cpp:253]     Train net output #0: loss = 1.11571 (* 1 = 1.11571 loss)
I0522 15:25:27.723467 27620 sgd_solver.cpp:106] Iteration 135000, lr = 0.0045
I0522 15:25:38.220505 27620 solver.cpp:237] Iteration 135500, loss = 1.25078
I0522 15:25:38.220542 27620 solver.cpp:253]     Train net output #0: loss = 1.25078 (* 1 = 1.25078 loss)
I0522 15:25:38.220558 27620 sgd_solver.cpp:106] Iteration 135500, lr = 0.0045
I0522 15:25:48.728631 27620 solver.cpp:237] Iteration 136000, loss = 1.06456
I0522 15:25:48.728667 27620 solver.cpp:253]     Train net output #0: loss = 1.06456 (* 1 = 1.06456 loss)
I0522 15:25:48.728682 27620 sgd_solver.cpp:106] Iteration 136000, lr = 0.0045
I0522 15:25:59.236374 27620 solver.cpp:237] Iteration 136500, loss = 1.02074
I0522 15:25:59.236564 27620 solver.cpp:253]     Train net output #0: loss = 1.02074 (* 1 = 1.02074 loss)
I0522 15:25:59.236579 27620 sgd_solver.cpp:106] Iteration 136500, lr = 0.0045
I0522 15:26:30.618237 27620 solver.cpp:237] Iteration 137000, loss = 1.31243
I0522 15:26:30.618413 27620 solver.cpp:253]     Train net output #0: loss = 1.31243 (* 1 = 1.31243 loss)
I0522 15:26:30.618428 27620 sgd_solver.cpp:106] Iteration 137000, lr = 0.0045
I0522 15:26:41.125129 27620 solver.cpp:237] Iteration 137500, loss = 1.66158
I0522 15:26:41.125175 27620 solver.cpp:253]     Train net output #0: loss = 1.66158 (* 1 = 1.66158 loss)
I0522 15:26:41.125191 27620 sgd_solver.cpp:106] Iteration 137500, lr = 0.0045
I0522 15:26:51.634649 27620 solver.cpp:237] Iteration 138000, loss = 1.19599
I0522 15:26:51.634686 27620 solver.cpp:253]     Train net output #0: loss = 1.19599 (* 1 = 1.19599 loss)
I0522 15:26:51.634701 27620 sgd_solver.cpp:106] Iteration 138000, lr = 0.0045
I0522 15:27:02.138154 27620 solver.cpp:237] Iteration 138500, loss = 1.09088
I0522 15:27:02.138317 27620 solver.cpp:253]     Train net output #0: loss = 1.09088 (* 1 = 1.09088 loss)
I0522 15:27:02.138331 27620 sgd_solver.cpp:106] Iteration 138500, lr = 0.0045
I0522 15:27:12.647047 27620 solver.cpp:237] Iteration 139000, loss = 1.27511
I0522 15:27:12.647092 27620 solver.cpp:253]     Train net output #0: loss = 1.27511 (* 1 = 1.27511 loss)
I0522 15:27:12.647107 27620 sgd_solver.cpp:106] Iteration 139000, lr = 0.0045
I0522 15:27:23.167829 27620 solver.cpp:237] Iteration 139500, loss = 1.27192
I0522 15:27:23.167865 27620 solver.cpp:253]     Train net output #0: loss = 1.27192 (* 1 = 1.27192 loss)
I0522 15:27:23.167879 27620 sgd_solver.cpp:106] Iteration 139500, lr = 0.0045
I0522 15:27:33.657131 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_140000.caffemodel
I0522 15:27:33.710100 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_140000.solverstate
I0522 15:27:33.735769 27620 solver.cpp:341] Iteration 140000, Testing net (#0)
I0522 15:28:44.197335 27620 solver.cpp:409]     Test net output #0: accuracy = 0.897153
I0522 15:28:44.197516 27620 solver.cpp:409]     Test net output #1: loss = 0.32738 (* 1 = 0.32738 loss)
I0522 15:29:05.053364 27620 solver.cpp:237] Iteration 140000, loss = 1.24951
I0522 15:29:05.053417 27620 solver.cpp:253]     Train net output #0: loss = 1.24951 (* 1 = 1.24951 loss)
I0522 15:29:05.053432 27620 sgd_solver.cpp:106] Iteration 140000, lr = 0.0045
I0522 15:29:15.555660 27620 solver.cpp:237] Iteration 140500, loss = 1.05267
I0522 15:29:15.555843 27620 solver.cpp:253]     Train net output #0: loss = 1.05267 (* 1 = 1.05267 loss)
I0522 15:29:15.555857 27620 sgd_solver.cpp:106] Iteration 140500, lr = 0.0045
I0522 15:29:26.065985 27620 solver.cpp:237] Iteration 141000, loss = 1.10984
I0522 15:29:26.066022 27620 solver.cpp:253]     Train net output #0: loss = 1.10984 (* 1 = 1.10984 loss)
I0522 15:29:26.066038 27620 sgd_solver.cpp:106] Iteration 141000, lr = 0.0045
I0522 15:29:36.565901 27620 solver.cpp:237] Iteration 141500, loss = 1.22928
I0522 15:29:36.565949 27620 solver.cpp:253]     Train net output #0: loss = 1.22928 (* 1 = 1.22928 loss)
I0522 15:29:36.565963 27620 sgd_solver.cpp:106] Iteration 141500, lr = 0.0045
I0522 15:29:47.066634 27620 solver.cpp:237] Iteration 142000, loss = 1.38101
I0522 15:29:47.066795 27620 solver.cpp:253]     Train net output #0: loss = 1.38101 (* 1 = 1.38101 loss)
I0522 15:29:47.066812 27620 sgd_solver.cpp:106] Iteration 142000, lr = 0.0045
I0522 15:29:57.561573 27620 solver.cpp:237] Iteration 142500, loss = 1.13566
I0522 15:29:57.561609 27620 solver.cpp:253]     Train net output #0: loss = 1.13566 (* 1 = 1.13566 loss)
I0522 15:29:57.561625 27620 sgd_solver.cpp:106] Iteration 142500, lr = 0.0045
I0522 15:30:08.055650 27620 solver.cpp:237] Iteration 143000, loss = 1.16604
I0522 15:30:08.055696 27620 solver.cpp:253]     Train net output #0: loss = 1.16604 (* 1 = 1.16604 loss)
I0522 15:30:08.055712 27620 sgd_solver.cpp:106] Iteration 143000, lr = 0.0045
I0522 15:30:39.428679 27620 solver.cpp:237] Iteration 143500, loss = 1.05658
I0522 15:30:39.428874 27620 solver.cpp:253]     Train net output #0: loss = 1.05659 (* 1 = 1.05659 loss)
I0522 15:30:39.428889 27620 sgd_solver.cpp:106] Iteration 143500, lr = 0.0045
I0522 15:30:49.918751 27620 solver.cpp:237] Iteration 144000, loss = 0.971318
I0522 15:30:49.918799 27620 solver.cpp:253]     Train net output #0: loss = 0.971319 (* 1 = 0.971319 loss)
I0522 15:30:49.918817 27620 sgd_solver.cpp:106] Iteration 144000, lr = 0.0045
I0522 15:31:00.410956 27620 solver.cpp:237] Iteration 144500, loss = 1.41657
I0522 15:31:00.410992 27620 solver.cpp:253]     Train net output #0: loss = 1.41657 (* 1 = 1.41657 loss)
I0522 15:31:00.411008 27620 sgd_solver.cpp:106] Iteration 144500, lr = 0.0045
I0522 15:31:10.872817 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_145000.caffemodel
I0522 15:31:10.932090 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_145000.solverstate
I0522 15:31:10.965587 27620 solver.cpp:237] Iteration 145000, loss = 1.49144
I0522 15:31:10.965637 27620 solver.cpp:253]     Train net output #0: loss = 1.49144 (* 1 = 1.49144 loss)
I0522 15:31:10.965651 27620 sgd_solver.cpp:106] Iteration 145000, lr = 0.0045
I0522 15:31:21.471204 27620 solver.cpp:237] Iteration 145500, loss = 1.40974
I0522 15:31:21.471248 27620 solver.cpp:253]     Train net output #0: loss = 1.40974 (* 1 = 1.40974 loss)
I0522 15:31:21.471263 27620 sgd_solver.cpp:106] Iteration 145500, lr = 0.0045
I0522 15:31:31.967594 27620 solver.cpp:237] Iteration 146000, loss = 1.05276
I0522 15:31:31.967630 27620 solver.cpp:253]     Train net output #0: loss = 1.05276 (* 1 = 1.05276 loss)
I0522 15:31:31.967648 27620 sgd_solver.cpp:106] Iteration 146000, lr = 0.0045
I0522 15:31:42.460276 27620 solver.cpp:237] Iteration 146500, loss = 0.84273
I0522 15:31:42.460463 27620 solver.cpp:253]     Train net output #0: loss = 0.84273 (* 1 = 0.84273 loss)
I0522 15:31:42.460479 27620 sgd_solver.cpp:106] Iteration 146500, lr = 0.0045
I0522 15:32:13.802676 27620 solver.cpp:237] Iteration 147000, loss = 1.25077
I0522 15:32:13.802863 27620 solver.cpp:253]     Train net output #0: loss = 1.25077 (* 1 = 1.25077 loss)
I0522 15:32:13.802880 27620 sgd_solver.cpp:106] Iteration 147000, lr = 0.0045
I0522 15:32:24.299747 27620 solver.cpp:237] Iteration 147500, loss = 1.14824
I0522 15:32:24.299784 27620 solver.cpp:253]     Train net output #0: loss = 1.14824 (* 1 = 1.14824 loss)
I0522 15:32:24.299800 27620 sgd_solver.cpp:106] Iteration 147500, lr = 0.0045
I0522 15:32:34.809927 27620 solver.cpp:237] Iteration 148000, loss = 1.271
I0522 15:32:34.809970 27620 solver.cpp:253]     Train net output #0: loss = 1.271 (* 1 = 1.271 loss)
I0522 15:32:34.809986 27620 sgd_solver.cpp:106] Iteration 148000, lr = 0.0045
I0522 15:32:45.314654 27620 solver.cpp:237] Iteration 148500, loss = 1.36644
I0522 15:32:45.314816 27620 solver.cpp:253]     Train net output #0: loss = 1.36644 (* 1 = 1.36644 loss)
I0522 15:32:45.314831 27620 sgd_solver.cpp:106] Iteration 148500, lr = 0.0045
I0522 15:32:55.807332 27620 solver.cpp:237] Iteration 149000, loss = 1.11923
I0522 15:32:55.807380 27620 solver.cpp:253]     Train net output #0: loss = 1.11923 (* 1 = 1.11923 loss)
I0522 15:32:55.807395 27620 sgd_solver.cpp:106] Iteration 149000, lr = 0.0045
I0522 15:33:06.310045 27620 solver.cpp:237] Iteration 149500, loss = 1.1594
I0522 15:33:06.310081 27620 solver.cpp:253]     Train net output #0: loss = 1.1594 (* 1 = 1.1594 loss)
I0522 15:33:06.310097 27620 sgd_solver.cpp:106] Iteration 149500, lr = 0.0045
I0522 15:33:16.803844 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_150000.caffemodel
I0522 15:33:16.858103 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_150000.solverstate
I0522 15:33:16.885259 27620 solver.cpp:341] Iteration 150000, Testing net (#0)
I0522 15:34:06.168475 27620 solver.cpp:409]     Test net output #0: accuracy = 0.897292
I0522 15:34:06.168663 27620 solver.cpp:409]     Test net output #1: loss = 0.328254 (* 1 = 0.328254 loss)
I0522 15:34:27.055340 27620 solver.cpp:237] Iteration 150000, loss = 1.22113
I0522 15:34:27.055392 27620 solver.cpp:253]     Train net output #0: loss = 1.22113 (* 1 = 1.22113 loss)
I0522 15:34:27.055409 27620 sgd_solver.cpp:106] Iteration 150000, lr = 0.0045
I0522 15:34:37.587910 27620 solver.cpp:237] Iteration 150500, loss = 1.23274
I0522 15:34:37.588090 27620 solver.cpp:253]     Train net output #0: loss = 1.23274 (* 1 = 1.23274 loss)
I0522 15:34:37.588106 27620 sgd_solver.cpp:106] Iteration 150500, lr = 0.0045
I0522 15:34:48.117643 27620 solver.cpp:237] Iteration 151000, loss = 1.49844
I0522 15:34:48.117679 27620 solver.cpp:253]     Train net output #0: loss = 1.49844 (* 1 = 1.49844 loss)
I0522 15:34:48.117692 27620 sgd_solver.cpp:106] Iteration 151000, lr = 0.0045
I0522 15:34:58.633203 27620 solver.cpp:237] Iteration 151500, loss = 1.52661
I0522 15:34:58.633239 27620 solver.cpp:253]     Train net output #0: loss = 1.52661 (* 1 = 1.52661 loss)
I0522 15:34:58.633255 27620 sgd_solver.cpp:106] Iteration 151500, lr = 0.0045
I0522 15:35:09.152245 27620 solver.cpp:237] Iteration 152000, loss = 1.22021
I0522 15:35:09.152425 27620 solver.cpp:253]     Train net output #0: loss = 1.22021 (* 1 = 1.22021 loss)
I0522 15:35:09.152441 27620 sgd_solver.cpp:106] Iteration 152000, lr = 0.0045
I0522 15:35:19.671190 27620 solver.cpp:237] Iteration 152500, loss = 1.18767
I0522 15:35:19.671226 27620 solver.cpp:253]     Train net output #0: loss = 1.18767 (* 1 = 1.18767 loss)
I0522 15:35:19.671243 27620 sgd_solver.cpp:106] Iteration 152500, lr = 0.0045
I0522 15:35:30.195055 27620 solver.cpp:237] Iteration 153000, loss = 1.60897
I0522 15:35:30.195098 27620 solver.cpp:253]     Train net output #0: loss = 1.60897 (* 1 = 1.60897 loss)
I0522 15:35:30.195114 27620 sgd_solver.cpp:106] Iteration 153000, lr = 0.0045
I0522 15:36:01.578508 27620 solver.cpp:237] Iteration 153500, loss = 1.09151
I0522 15:36:01.578686 27620 solver.cpp:253]     Train net output #0: loss = 1.09151 (* 1 = 1.09151 loss)
I0522 15:36:01.578702 27620 sgd_solver.cpp:106] Iteration 153500, lr = 0.0045
I0522 15:36:12.103838 27620 solver.cpp:237] Iteration 154000, loss = 0.920199
I0522 15:36:12.103874 27620 solver.cpp:253]     Train net output #0: loss = 0.920199 (* 1 = 0.920199 loss)
I0522 15:36:12.103890 27620 sgd_solver.cpp:106] Iteration 154000, lr = 0.0045
I0522 15:36:22.644726 27620 solver.cpp:237] Iteration 154500, loss = 1.41743
I0522 15:36:22.644768 27620 solver.cpp:253]     Train net output #0: loss = 1.41743 (* 1 = 1.41743 loss)
I0522 15:36:22.644784 27620 sgd_solver.cpp:106] Iteration 154500, lr = 0.0045
I0522 15:36:33.152578 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_155000.caffemodel
I0522 15:36:33.204704 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_155000.solverstate
I0522 15:36:33.236660 27620 solver.cpp:237] Iteration 155000, loss = 1.07012
I0522 15:36:33.236704 27620 solver.cpp:253]     Train net output #0: loss = 1.07012 (* 1 = 1.07012 loss)
I0522 15:36:33.236721 27620 sgd_solver.cpp:106] Iteration 155000, lr = 0.0045
I0522 15:36:43.757841 27620 solver.cpp:237] Iteration 155500, loss = 1.53258
I0522 15:36:43.757884 27620 solver.cpp:253]     Train net output #0: loss = 1.53258 (* 1 = 1.53258 loss)
I0522 15:36:43.757900 27620 sgd_solver.cpp:106] Iteration 155500, lr = 0.0045
I0522 15:36:54.291049 27620 solver.cpp:237] Iteration 156000, loss = 1.09108
I0522 15:36:54.291086 27620 solver.cpp:253]     Train net output #0: loss = 1.09108 (* 1 = 1.09108 loss)
I0522 15:36:54.291102 27620 sgd_solver.cpp:106] Iteration 156000, lr = 0.0045
I0522 15:37:04.811040 27620 solver.cpp:237] Iteration 156500, loss = 0.881372
I0522 15:37:04.811221 27620 solver.cpp:253]     Train net output #0: loss = 0.881373 (* 1 = 0.881373 loss)
I0522 15:37:04.811235 27620 sgd_solver.cpp:106] Iteration 156500, lr = 0.0045
I0522 15:37:36.171357 27620 solver.cpp:237] Iteration 157000, loss = 0.894344
I0522 15:37:36.171546 27620 solver.cpp:253]     Train net output #0: loss = 0.894345 (* 1 = 0.894345 loss)
I0522 15:37:36.171561 27620 sgd_solver.cpp:106] Iteration 157000, lr = 0.0045
I0522 15:37:46.698201 27620 solver.cpp:237] Iteration 157500, loss = 1.42026
I0522 15:37:46.698236 27620 solver.cpp:253]     Train net output #0: loss = 1.42026 (* 1 = 1.42026 loss)
I0522 15:37:46.698253 27620 sgd_solver.cpp:106] Iteration 157500, lr = 0.0045
I0522 15:37:57.246011 27620 solver.cpp:237] Iteration 158000, loss = 0.789075
I0522 15:37:57.246060 27620 solver.cpp:253]     Train net output #0: loss = 0.789076 (* 1 = 0.789076 loss)
I0522 15:37:57.246075 27620 sgd_solver.cpp:106] Iteration 158000, lr = 0.0045
I0522 15:38:07.846626 27620 solver.cpp:237] Iteration 158500, loss = 1.52022
I0522 15:38:07.846794 27620 solver.cpp:253]     Train net output #0: loss = 1.52022 (* 1 = 1.52022 loss)
I0522 15:38:07.846809 27620 sgd_solver.cpp:106] Iteration 158500, lr = 0.0045
I0522 15:38:18.454951 27620 solver.cpp:237] Iteration 159000, loss = 1.20981
I0522 15:38:18.454987 27620 solver.cpp:253]     Train net output #0: loss = 1.20981 (* 1 = 1.20981 loss)
I0522 15:38:18.455003 27620 sgd_solver.cpp:106] Iteration 159000, lr = 0.0045
I0522 15:38:29.043319 27620 solver.cpp:237] Iteration 159500, loss = 0.95811
I0522 15:38:29.043365 27620 solver.cpp:253]     Train net output #0: loss = 0.958111 (* 1 = 0.958111 loss)
I0522 15:38:29.043380 27620 sgd_solver.cpp:106] Iteration 159500, lr = 0.0045
I0522 15:38:39.627315 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_160000.caffemodel
I0522 15:38:39.689143 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_160000.solverstate
I0522 15:38:39.716084 27620 solver.cpp:341] Iteration 160000, Testing net (#0)
I0522 15:39:50.245884 27620 solver.cpp:409]     Test net output #0: accuracy = 0.89708
I0522 15:39:50.246075 27620 solver.cpp:409]     Test net output #1: loss = 0.332604 (* 1 = 0.332604 loss)
I0522 15:40:11.139832 27620 solver.cpp:237] Iteration 160000, loss = 1.01796
I0522 15:40:11.139884 27620 solver.cpp:253]     Train net output #0: loss = 1.01796 (* 1 = 1.01796 loss)
I0522 15:40:11.139901 27620 sgd_solver.cpp:106] Iteration 160000, lr = 0.0045
I0522 15:40:21.765460 27620 solver.cpp:237] Iteration 160500, loss = 0.931063
I0522 15:40:21.765635 27620 solver.cpp:253]     Train net output #0: loss = 0.931064 (* 1 = 0.931064 loss)
I0522 15:40:21.765650 27620 sgd_solver.cpp:106] Iteration 160500, lr = 0.0045
I0522 15:40:32.393658 27620 solver.cpp:237] Iteration 161000, loss = 0.887132
I0522 15:40:32.393702 27620 solver.cpp:253]     Train net output #0: loss = 0.887133 (* 1 = 0.887133 loss)
I0522 15:40:32.393717 27620 sgd_solver.cpp:106] Iteration 161000, lr = 0.0045
I0522 15:40:43.015436 27620 solver.cpp:237] Iteration 161500, loss = 1.18317
I0522 15:40:43.015472 27620 solver.cpp:253]     Train net output #0: loss = 1.18317 (* 1 = 1.18317 loss)
I0522 15:40:43.015489 27620 sgd_solver.cpp:106] Iteration 161500, lr = 0.0045
I0522 15:40:53.637208 27620 solver.cpp:237] Iteration 162000, loss = 1.09151
I0522 15:40:53.637399 27620 solver.cpp:253]     Train net output #0: loss = 1.09151 (* 1 = 1.09151 loss)
I0522 15:40:53.637413 27620 sgd_solver.cpp:106] Iteration 162000, lr = 0.0045
I0522 15:41:04.268492 27620 solver.cpp:237] Iteration 162500, loss = 1.4704
I0522 15:41:04.268528 27620 solver.cpp:253]     Train net output #0: loss = 1.4704 (* 1 = 1.4704 loss)
I0522 15:41:04.268544 27620 sgd_solver.cpp:106] Iteration 162500, lr = 0.0045
I0522 15:41:14.893663 27620 solver.cpp:237] Iteration 163000, loss = 0.888219
I0522 15:41:14.893713 27620 solver.cpp:253]     Train net output #0: loss = 0.88822 (* 1 = 0.88822 loss)
I0522 15:41:14.893728 27620 sgd_solver.cpp:106] Iteration 163000, lr = 0.0045
I0522 15:41:46.447511 27620 solver.cpp:237] Iteration 163500, loss = 1.0372
I0522 15:41:46.447705 27620 solver.cpp:253]     Train net output #0: loss = 1.0372 (* 1 = 1.0372 loss)
I0522 15:41:46.447721 27620 sgd_solver.cpp:106] Iteration 163500, lr = 0.0045
I0522 15:41:57.064157 27620 solver.cpp:237] Iteration 164000, loss = 0.900092
I0522 15:41:57.064194 27620 solver.cpp:253]     Train net output #0: loss = 0.900092 (* 1 = 0.900092 loss)
I0522 15:41:57.064210 27620 sgd_solver.cpp:106] Iteration 164000, lr = 0.0045
I0522 15:42:07.687863 27620 solver.cpp:237] Iteration 164500, loss = 1.5788
I0522 15:42:07.687913 27620 solver.cpp:253]     Train net output #0: loss = 1.5788 (* 1 = 1.5788 loss)
I0522 15:42:07.687930 27620 sgd_solver.cpp:106] Iteration 164500, lr = 0.0045
I0522 15:42:18.291918 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_165000.caffemodel
I0522 15:42:18.345245 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_165000.solverstate
I0522 15:42:18.377601 27620 solver.cpp:237] Iteration 165000, loss = 1.18155
I0522 15:42:18.377648 27620 solver.cpp:253]     Train net output #0: loss = 1.18155 (* 1 = 1.18155 loss)
I0522 15:42:18.377662 27620 sgd_solver.cpp:106] Iteration 165000, lr = 0.0045
I0522 15:42:29.001075 27620 solver.cpp:237] Iteration 165500, loss = 1.20852
I0522 15:42:29.001112 27620 solver.cpp:253]     Train net output #0: loss = 1.20852 (* 1 = 1.20852 loss)
I0522 15:42:29.001126 27620 sgd_solver.cpp:106] Iteration 165500, lr = 0.0045
I0522 15:42:39.623841 27620 solver.cpp:237] Iteration 166000, loss = 0.787433
I0522 15:42:39.623888 27620 solver.cpp:253]     Train net output #0: loss = 0.787434 (* 1 = 0.787434 loss)
I0522 15:42:39.623901 27620 sgd_solver.cpp:106] Iteration 166000, lr = 0.0045
I0522 15:42:50.256103 27620 solver.cpp:237] Iteration 166500, loss = 0.836005
I0522 15:42:50.256278 27620 solver.cpp:253]     Train net output #0: loss = 0.836006 (* 1 = 0.836006 loss)
I0522 15:42:50.256292 27620 sgd_solver.cpp:106] Iteration 166500, lr = 0.0045
I0522 15:43:21.755218 27620 solver.cpp:237] Iteration 167000, loss = 1.08564
I0522 15:43:21.755406 27620 solver.cpp:253]     Train net output #0: loss = 1.08564 (* 1 = 1.08564 loss)
I0522 15:43:21.755422 27620 sgd_solver.cpp:106] Iteration 167000, lr = 0.0045
I0522 15:43:32.381518 27620 solver.cpp:237] Iteration 167500, loss = 0.993872
I0522 15:43:32.381556 27620 solver.cpp:253]     Train net output #0: loss = 0.993873 (* 1 = 0.993873 loss)
I0522 15:43:32.381568 27620 sgd_solver.cpp:106] Iteration 167500, lr = 0.0045
I0522 15:43:42.995678 27620 solver.cpp:237] Iteration 168000, loss = 1.16086
I0522 15:43:42.995714 27620 solver.cpp:253]     Train net output #0: loss = 1.16086 (* 1 = 1.16086 loss)
I0522 15:43:42.995730 27620 sgd_solver.cpp:106] Iteration 168000, lr = 0.0045
I0522 15:43:53.638336 27620 solver.cpp:237] Iteration 168500, loss = 0.909489
I0522 15:43:53.638496 27620 solver.cpp:253]     Train net output #0: loss = 0.90949 (* 1 = 0.90949 loss)
I0522 15:43:53.638511 27620 sgd_solver.cpp:106] Iteration 168500, lr = 0.0045
I0522 15:44:04.252491 27620 solver.cpp:237] Iteration 169000, loss = 1.27685
I0522 15:44:04.252526 27620 solver.cpp:253]     Train net output #0: loss = 1.27685 (* 1 = 1.27685 loss)
I0522 15:44:04.252543 27620 sgd_solver.cpp:106] Iteration 169000, lr = 0.0045
I0522 15:44:14.878324 27620 solver.cpp:237] Iteration 169500, loss = 1.03337
I0522 15:44:14.878365 27620 solver.cpp:253]     Train net output #0: loss = 1.03337 (* 1 = 1.03337 loss)
I0522 15:44:14.878381 27620 sgd_solver.cpp:106] Iteration 169500, lr = 0.0045
I0522 15:44:25.479967 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_170000.caffemodel
I0522 15:44:25.533032 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_170000.solverstate
I0522 15:44:25.558733 27620 solver.cpp:341] Iteration 170000, Testing net (#0)
I0522 15:45:15.134893 27620 solver.cpp:409]     Test net output #0: accuracy = 0.896131
I0522 15:45:15.135081 27620 solver.cpp:409]     Test net output #1: loss = 0.335728 (* 1 = 0.335728 loss)
I0522 15:45:36.034735 27620 solver.cpp:237] Iteration 170000, loss = 0.781986
I0522 15:45:36.034790 27620 solver.cpp:253]     Train net output #0: loss = 0.781987 (* 1 = 0.781987 loss)
I0522 15:45:36.034804 27620 sgd_solver.cpp:106] Iteration 170000, lr = 0.0045
I0522 15:45:46.591462 27620 solver.cpp:237] Iteration 170500, loss = 0.692536
I0522 15:45:46.591640 27620 solver.cpp:253]     Train net output #0: loss = 0.692537 (* 1 = 0.692537 loss)
I0522 15:45:46.591653 27620 sgd_solver.cpp:106] Iteration 170500, lr = 0.0045
I0522 15:45:57.149674 27620 solver.cpp:237] Iteration 171000, loss = 1.57412
I0522 15:45:57.149724 27620 solver.cpp:253]     Train net output #0: loss = 1.57412 (* 1 = 1.57412 loss)
I0522 15:45:57.149739 27620 sgd_solver.cpp:106] Iteration 171000, lr = 0.0045
I0522 15:46:07.702481 27620 solver.cpp:237] Iteration 171500, loss = 1.1156
I0522 15:46:07.702517 27620 solver.cpp:253]     Train net output #0: loss = 1.1156 (* 1 = 1.1156 loss)
I0522 15:46:07.702533 27620 sgd_solver.cpp:106] Iteration 171500, lr = 0.0045
I0522 15:46:18.267514 27620 solver.cpp:237] Iteration 172000, loss = 1.31934
I0522 15:46:18.267700 27620 solver.cpp:253]     Train net output #0: loss = 1.31934 (* 1 = 1.31934 loss)
I0522 15:46:18.267714 27620 sgd_solver.cpp:106] Iteration 172000, lr = 0.0045
I0522 15:46:28.827591 27620 solver.cpp:237] Iteration 172500, loss = 1.13471
I0522 15:46:28.827626 27620 solver.cpp:253]     Train net output #0: loss = 1.13471 (* 1 = 1.13471 loss)
I0522 15:46:28.827643 27620 sgd_solver.cpp:106] Iteration 172500, lr = 0.0045
I0522 15:46:39.401828 27620 solver.cpp:237] Iteration 173000, loss = 1.2982
I0522 15:46:39.401862 27620 solver.cpp:253]     Train net output #0: loss = 1.2982 (* 1 = 1.2982 loss)
I0522 15:46:39.401878 27620 sgd_solver.cpp:106] Iteration 173000, lr = 0.0045
I0522 15:47:10.766803 27620 solver.cpp:237] Iteration 173500, loss = 1.13573
I0522 15:47:10.766996 27620 solver.cpp:253]     Train net output #0: loss = 1.13573 (* 1 = 1.13573 loss)
I0522 15:47:10.767011 27620 sgd_solver.cpp:106] Iteration 173500, lr = 0.0045
I0522 15:47:21.296032 27620 solver.cpp:237] Iteration 174000, loss = 1.38202
I0522 15:47:21.296068 27620 solver.cpp:253]     Train net output #0: loss = 1.38203 (* 1 = 1.38203 loss)
I0522 15:47:21.296084 27620 sgd_solver.cpp:106] Iteration 174000, lr = 0.0045
I0522 15:47:31.830951 27620 solver.cpp:237] Iteration 174500, loss = 0.943156
I0522 15:47:31.830997 27620 solver.cpp:253]     Train net output #0: loss = 0.943157 (* 1 = 0.943157 loss)
I0522 15:47:31.831013 27620 sgd_solver.cpp:106] Iteration 174500, lr = 0.0045
I0522 15:47:42.342907 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_175000.caffemodel
I0522 15:47:42.397161 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_175000.solverstate
I0522 15:47:42.431303 27620 solver.cpp:237] Iteration 175000, loss = 0.903405
I0522 15:47:42.431354 27620 solver.cpp:253]     Train net output #0: loss = 0.903405 (* 1 = 0.903405 loss)
I0522 15:47:42.431368 27620 sgd_solver.cpp:106] Iteration 175000, lr = 0.0045
I0522 15:47:52.975097 27620 solver.cpp:237] Iteration 175500, loss = 1.55648
I0522 15:47:52.975133 27620 solver.cpp:253]     Train net output #0: loss = 1.55648 (* 1 = 1.55648 loss)
I0522 15:47:52.975147 27620 sgd_solver.cpp:106] Iteration 175500, lr = 0.0045
I0522 15:48:03.517398 27620 solver.cpp:237] Iteration 176000, loss = 1.12329
I0522 15:48:03.517444 27620 solver.cpp:253]     Train net output #0: loss = 1.12329 (* 1 = 1.12329 loss)
I0522 15:48:03.517457 27620 sgd_solver.cpp:106] Iteration 176000, lr = 0.0045
I0522 15:48:14.063114 27620 solver.cpp:237] Iteration 176500, loss = 1.08947
I0522 15:48:14.063288 27620 solver.cpp:253]     Train net output #0: loss = 1.08947 (* 1 = 1.08947 loss)
I0522 15:48:14.063302 27620 sgd_solver.cpp:106] Iteration 176500, lr = 0.0045
I0522 15:48:45.465018 27620 solver.cpp:237] Iteration 177000, loss = 1.14365
I0522 15:48:45.465210 27620 solver.cpp:253]     Train net output #0: loss = 1.14365 (* 1 = 1.14365 loss)
I0522 15:48:45.465225 27620 sgd_solver.cpp:106] Iteration 177000, lr = 0.0045
I0522 15:48:56.005764 27620 solver.cpp:237] Iteration 177500, loss = 1.10003
I0522 15:48:56.005800 27620 solver.cpp:253]     Train net output #0: loss = 1.10003 (* 1 = 1.10003 loss)
I0522 15:48:56.005815 27620 sgd_solver.cpp:106] Iteration 177500, lr = 0.0045
I0522 15:49:06.548027 27620 solver.cpp:237] Iteration 178000, loss = 1.27457
I0522 15:49:06.548063 27620 solver.cpp:253]     Train net output #0: loss = 1.27457 (* 1 = 1.27457 loss)
I0522 15:49:06.548079 27620 sgd_solver.cpp:106] Iteration 178000, lr = 0.0045
I0522 15:49:17.077546 27620 solver.cpp:237] Iteration 178500, loss = 1.03221
I0522 15:49:17.077720 27620 solver.cpp:253]     Train net output #0: loss = 1.03221 (* 1 = 1.03221 loss)
I0522 15:49:17.077736 27620 sgd_solver.cpp:106] Iteration 178500, lr = 0.0045
I0522 15:49:27.604208 27620 solver.cpp:237] Iteration 179000, loss = 1.17667
I0522 15:49:27.604244 27620 solver.cpp:253]     Train net output #0: loss = 1.17667 (* 1 = 1.17667 loss)
I0522 15:49:27.604260 27620 sgd_solver.cpp:106] Iteration 179000, lr = 0.0045
I0522 15:49:38.135304 27620 solver.cpp:237] Iteration 179500, loss = 1.00764
I0522 15:49:38.135350 27620 solver.cpp:253]     Train net output #0: loss = 1.00764 (* 1 = 1.00764 loss)
I0522 15:49:38.135365 27620 sgd_solver.cpp:106] Iteration 179500, lr = 0.0045
I0522 15:49:48.646540 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_180000.caffemodel
I0522 15:49:48.700698 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_180000.solverstate
I0522 15:49:48.727959 27620 solver.cpp:341] Iteration 180000, Testing net (#0)
I0522 15:50:59.226306 27620 solver.cpp:409]     Test net output #0: accuracy = 0.899945
I0522 15:50:59.226497 27620 solver.cpp:409]     Test net output #1: loss = 0.322752 (* 1 = 0.322752 loss)
I0522 15:51:20.080211 27620 solver.cpp:237] Iteration 180000, loss = 0.673354
I0522 15:51:20.080265 27620 solver.cpp:253]     Train net output #0: loss = 0.673355 (* 1 = 0.673355 loss)
I0522 15:51:20.080281 27620 sgd_solver.cpp:106] Iteration 180000, lr = 0.0045
I0522 15:51:30.623396 27620 solver.cpp:237] Iteration 180500, loss = 0.90094
I0522 15:51:30.623574 27620 solver.cpp:253]     Train net output #0: loss = 0.900941 (* 1 = 0.900941 loss)
I0522 15:51:30.623589 27620 sgd_solver.cpp:106] Iteration 180500, lr = 0.0045
I0522 15:51:41.194963 27620 solver.cpp:237] Iteration 181000, loss = 1.29641
I0522 15:51:41.195009 27620 solver.cpp:253]     Train net output #0: loss = 1.29641 (* 1 = 1.29641 loss)
I0522 15:51:41.195024 27620 sgd_solver.cpp:106] Iteration 181000, lr = 0.0045
I0522 15:51:51.738658 27620 solver.cpp:237] Iteration 181500, loss = 1.28348
I0522 15:51:51.738693 27620 solver.cpp:253]     Train net output #0: loss = 1.28348 (* 1 = 1.28348 loss)
I0522 15:51:51.738708 27620 sgd_solver.cpp:106] Iteration 181500, lr = 0.0045
I0522 15:52:02.315032 27620 solver.cpp:237] Iteration 182000, loss = 0.750656
I0522 15:52:02.315214 27620 solver.cpp:253]     Train net output #0: loss = 0.750656 (* 1 = 0.750656 loss)
I0522 15:52:02.315229 27620 sgd_solver.cpp:106] Iteration 182000, lr = 0.0045
I0522 15:52:12.885673 27620 solver.cpp:237] Iteration 182500, loss = 1.4672
I0522 15:52:12.885720 27620 solver.cpp:253]     Train net output #0: loss = 1.4672 (* 1 = 1.4672 loss)
I0522 15:52:12.885735 27620 sgd_solver.cpp:106] Iteration 182500, lr = 0.0045
I0522 15:52:23.439364 27620 solver.cpp:237] Iteration 183000, loss = 0.957337
I0522 15:52:23.439401 27620 solver.cpp:253]     Train net output #0: loss = 0.957338 (* 1 = 0.957338 loss)
I0522 15:52:23.439416 27620 sgd_solver.cpp:106] Iteration 183000, lr = 0.0045
I0522 15:52:54.826133 27620 solver.cpp:237] Iteration 183500, loss = 1.23776
I0522 15:52:54.826331 27620 solver.cpp:253]     Train net output #0: loss = 1.23776 (* 1 = 1.23776 loss)
I0522 15:52:54.826346 27620 sgd_solver.cpp:106] Iteration 183500, lr = 0.0045
I0522 15:53:05.398483 27620 solver.cpp:237] Iteration 184000, loss = 0.974093
I0522 15:53:05.398519 27620 solver.cpp:253]     Train net output #0: loss = 0.974093 (* 1 = 0.974093 loss)
I0522 15:53:05.398533 27620 sgd_solver.cpp:106] Iteration 184000, lr = 0.0045
I0522 15:53:15.985617 27620 solver.cpp:237] Iteration 184500, loss = 0.950067
I0522 15:53:15.985654 27620 solver.cpp:253]     Train net output #0: loss = 0.950068 (* 1 = 0.950068 loss)
I0522 15:53:15.985667 27620 sgd_solver.cpp:106] Iteration 184500, lr = 0.0045
I0522 15:53:26.535449 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_185000.caffemodel
I0522 15:53:26.588188 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_185000.solverstate
I0522 15:53:26.620239 27620 solver.cpp:237] Iteration 185000, loss = 1.42561
I0522 15:53:26.620286 27620 solver.cpp:253]     Train net output #0: loss = 1.42561 (* 1 = 1.42561 loss)
I0522 15:53:26.620302 27620 sgd_solver.cpp:106] Iteration 185000, lr = 0.0045
I0522 15:53:37.200531 27620 solver.cpp:237] Iteration 185500, loss = 1.28372
I0522 15:53:37.200567 27620 solver.cpp:253]     Train net output #0: loss = 1.28372 (* 1 = 1.28372 loss)
I0522 15:53:37.200582 27620 sgd_solver.cpp:106] Iteration 185500, lr = 0.0045
I0522 15:53:47.777552 27620 solver.cpp:237] Iteration 186000, loss = 0.830904
I0522 15:53:47.777601 27620 solver.cpp:253]     Train net output #0: loss = 0.830905 (* 1 = 0.830905 loss)
I0522 15:53:47.777616 27620 sgd_solver.cpp:106] Iteration 186000, lr = 0.0045
I0522 15:53:58.353979 27620 solver.cpp:237] Iteration 186500, loss = 0.852221
I0522 15:53:58.354172 27620 solver.cpp:253]     Train net output #0: loss = 0.852222 (* 1 = 0.852222 loss)
I0522 15:53:58.354188 27620 sgd_solver.cpp:106] Iteration 186500, lr = 0.0045
I0522 15:54:29.822055 27620 solver.cpp:237] Iteration 187000, loss = 1.1542
I0522 15:54:29.822258 27620 solver.cpp:253]     Train net output #0: loss = 1.1542 (* 1 = 1.1542 loss)
I0522 15:54:29.822273 27620 sgd_solver.cpp:106] Iteration 187000, lr = 0.0045
I0522 15:54:40.399855 27620 solver.cpp:237] Iteration 187500, loss = 1.29884
I0522 15:54:40.399901 27620 solver.cpp:253]     Train net output #0: loss = 1.29884 (* 1 = 1.29884 loss)
I0522 15:54:40.399914 27620 sgd_solver.cpp:106] Iteration 187500, lr = 0.0045
I0522 15:54:50.975795 27620 solver.cpp:237] Iteration 188000, loss = 1.47848
I0522 15:54:50.975831 27620 solver.cpp:253]     Train net output #0: loss = 1.47848 (* 1 = 1.47848 loss)
I0522 15:54:50.975844 27620 sgd_solver.cpp:106] Iteration 188000, lr = 0.0045
I0522 15:55:01.556228 27620 solver.cpp:237] Iteration 188500, loss = 1.25743
I0522 15:55:01.556416 27620 solver.cpp:253]     Train net output #0: loss = 1.25743 (* 1 = 1.25743 loss)
I0522 15:55:01.556430 27620 sgd_solver.cpp:106] Iteration 188500, lr = 0.0045
I0522 15:55:12.133987 27620 solver.cpp:237] Iteration 189000, loss = 1.17347
I0522 15:55:12.134023 27620 solver.cpp:253]     Train net output #0: loss = 1.17347 (* 1 = 1.17347 loss)
I0522 15:55:12.134040 27620 sgd_solver.cpp:106] Iteration 189000, lr = 0.0045
I0522 15:55:22.712995 27620 solver.cpp:237] Iteration 189500, loss = 1.13412
I0522 15:55:22.713030 27620 solver.cpp:253]     Train net output #0: loss = 1.13412 (* 1 = 1.13412 loss)
I0522 15:55:22.713047 27620 sgd_solver.cpp:106] Iteration 189500, lr = 0.0045
I0522 15:55:33.268667 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_190000.caffemodel
I0522 15:55:33.321202 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_190000.solverstate
I0522 15:55:33.346611 27620 solver.cpp:341] Iteration 190000, Testing net (#0)
I0522 15:56:22.627266 27620 solver.cpp:409]     Test net output #0: accuracy = 0.898498
I0522 15:56:22.627457 27620 solver.cpp:409]     Test net output #1: loss = 0.324997 (* 1 = 0.324997 loss)
I0522 15:56:43.442160 27620 solver.cpp:237] Iteration 190000, loss = 1.15149
I0522 15:56:43.442214 27620 solver.cpp:253]     Train net output #0: loss = 1.15149 (* 1 = 1.15149 loss)
I0522 15:56:43.442229 27620 sgd_solver.cpp:106] Iteration 190000, lr = 0.0045
I0522 15:56:53.940903 27620 solver.cpp:237] Iteration 190500, loss = 1.38853
I0522 15:56:53.941085 27620 solver.cpp:253]     Train net output #0: loss = 1.38853 (* 1 = 1.38853 loss)
I0522 15:56:53.941100 27620 sgd_solver.cpp:106] Iteration 190500, lr = 0.0045
I0522 15:57:04.448824 27620 solver.cpp:237] Iteration 191000, loss = 0.926768
I0522 15:57:04.448860 27620 solver.cpp:253]     Train net output #0: loss = 0.926769 (* 1 = 0.926769 loss)
I0522 15:57:04.448874 27620 sgd_solver.cpp:106] Iteration 191000, lr = 0.0045
I0522 15:57:14.993671 27620 solver.cpp:237] Iteration 191500, loss = 1.39709
I0522 15:57:14.993716 27620 solver.cpp:253]     Train net output #0: loss = 1.39709 (* 1 = 1.39709 loss)
I0522 15:57:14.993731 27620 sgd_solver.cpp:106] Iteration 191500, lr = 0.0045
I0522 15:57:25.517572 27620 solver.cpp:237] Iteration 192000, loss = 1.39075
I0522 15:57:25.517742 27620 solver.cpp:253]     Train net output #0: loss = 1.39075 (* 1 = 1.39075 loss)
I0522 15:57:25.517756 27620 sgd_solver.cpp:106] Iteration 192000, lr = 0.0045
I0522 15:57:36.040290 27620 solver.cpp:237] Iteration 192500, loss = 1.2708
I0522 15:57:36.040340 27620 solver.cpp:253]     Train net output #0: loss = 1.2708 (* 1 = 1.2708 loss)
I0522 15:57:36.040354 27620 sgd_solver.cpp:106] Iteration 192500, lr = 0.0045
I0522 15:57:46.579287 27620 solver.cpp:237] Iteration 193000, loss = 0.703156
I0522 15:57:46.579324 27620 solver.cpp:253]     Train net output #0: loss = 0.703157 (* 1 = 0.703157 loss)
I0522 15:57:46.579339 27620 sgd_solver.cpp:106] Iteration 193000, lr = 0.0045
I0522 15:58:17.967245 27620 solver.cpp:237] Iteration 193500, loss = 0.956412
I0522 15:58:17.967443 27620 solver.cpp:253]     Train net output #0: loss = 0.956413 (* 1 = 0.956413 loss)
I0522 15:58:17.967459 27620 sgd_solver.cpp:106] Iteration 193500, lr = 0.0045
I0522 15:58:28.513671 27620 solver.cpp:237] Iteration 194000, loss = 1.06259
I0522 15:58:28.513718 27620 solver.cpp:253]     Train net output #0: loss = 1.0626 (* 1 = 1.0626 loss)
I0522 15:58:28.513732 27620 sgd_solver.cpp:106] Iteration 194000, lr = 0.0045
I0522 15:58:39.052495 27620 solver.cpp:237] Iteration 194500, loss = 1.26939
I0522 15:58:39.052531 27620 solver.cpp:253]     Train net output #0: loss = 1.26939 (* 1 = 1.26939 loss)
I0522 15:58:39.052547 27620 sgd_solver.cpp:106] Iteration 194500, lr = 0.0045
I0522 15:58:49.573518 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_195000.caffemodel
I0522 15:58:49.636970 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_195000.solverstate
I0522 15:58:49.669095 27620 solver.cpp:237] Iteration 195000, loss = 1.30419
I0522 15:58:49.669140 27620 solver.cpp:253]     Train net output #0: loss = 1.30419 (* 1 = 1.30419 loss)
I0522 15:58:49.669155 27620 sgd_solver.cpp:106] Iteration 195000, lr = 0.0045
I0522 15:59:00.197540 27620 solver.cpp:237] Iteration 195500, loss = 1.21485
I0522 15:59:00.197577 27620 solver.cpp:253]     Train net output #0: loss = 1.21485 (* 1 = 1.21485 loss)
I0522 15:59:00.197593 27620 sgd_solver.cpp:106] Iteration 195500, lr = 0.0045
I0522 15:59:10.716742 27620 solver.cpp:237] Iteration 196000, loss = 1.55941
I0522 15:59:10.716787 27620 solver.cpp:253]     Train net output #0: loss = 1.55941 (* 1 = 1.55941 loss)
I0522 15:59:10.716802 27620 sgd_solver.cpp:106] Iteration 196000, lr = 0.0045
I0522 15:59:21.235307 27620 solver.cpp:237] Iteration 196500, loss = 0.906773
I0522 15:59:21.235487 27620 solver.cpp:253]     Train net output #0: loss = 0.906774 (* 1 = 0.906774 loss)
I0522 15:59:21.235503 27620 sgd_solver.cpp:106] Iteration 196500, lr = 0.0045
I0522 15:59:52.661741 27620 solver.cpp:237] Iteration 197000, loss = 1.23019
I0522 15:59:52.661936 27620 solver.cpp:253]     Train net output #0: loss = 1.23019 (* 1 = 1.23019 loss)
I0522 15:59:52.661952 27620 sgd_solver.cpp:106] Iteration 197000, lr = 0.0045
I0522 16:00:03.200280 27620 solver.cpp:237] Iteration 197500, loss = 1.3755
I0522 16:00:03.200325 27620 solver.cpp:253]     Train net output #0: loss = 1.3755 (* 1 = 1.3755 loss)
I0522 16:00:03.200340 27620 sgd_solver.cpp:106] Iteration 197500, lr = 0.0045
I0522 16:00:13.720806 27620 solver.cpp:237] Iteration 198000, loss = 1.1277
I0522 16:00:13.720844 27620 solver.cpp:253]     Train net output #0: loss = 1.1277 (* 1 = 1.1277 loss)
I0522 16:00:13.720860 27620 sgd_solver.cpp:106] Iteration 198000, lr = 0.0045
I0522 16:00:24.240398 27620 solver.cpp:237] Iteration 198500, loss = 1.24806
I0522 16:00:24.240581 27620 solver.cpp:253]     Train net output #0: loss = 1.24806 (* 1 = 1.24806 loss)
I0522 16:00:24.240594 27620 sgd_solver.cpp:106] Iteration 198500, lr = 0.0045
I0522 16:00:34.770412 27620 solver.cpp:237] Iteration 199000, loss = 1.22833
I0522 16:00:34.770458 27620 solver.cpp:253]     Train net output #0: loss = 1.22833 (* 1 = 1.22833 loss)
I0522 16:00:34.770474 27620 sgd_solver.cpp:106] Iteration 199000, lr = 0.0045
I0522 16:00:45.313863 27620 solver.cpp:237] Iteration 199500, loss = 1.21663
I0522 16:00:45.313899 27620 solver.cpp:253]     Train net output #0: loss = 1.21663 (* 1 = 1.21663 loss)
I0522 16:00:45.313915 27620 sgd_solver.cpp:106] Iteration 199500, lr = 0.0045
I0522 16:00:55.829470 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_200000.caffemodel
I0522 16:00:55.884410 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_200000.solverstate
I0522 16:00:55.912158 27620 solver.cpp:341] Iteration 200000, Testing net (#0)
I0522 16:02:06.439519 27620 solver.cpp:409]     Test net output #0: accuracy = 0.899699
I0522 16:02:06.439713 27620 solver.cpp:409]     Test net output #1: loss = 0.308328 (* 1 = 0.308328 loss)
I0522 16:02:27.359760 27620 solver.cpp:237] Iteration 200000, loss = 0.964757
I0522 16:02:27.359814 27620 solver.cpp:253]     Train net output #0: loss = 0.964758 (* 1 = 0.964758 loss)
I0522 16:02:27.359829 27620 sgd_solver.cpp:106] Iteration 200000, lr = 0.0045
I0522 16:02:37.983319 27620 solver.cpp:237] Iteration 200500, loss = 1.36288
I0522 16:02:37.983525 27620 solver.cpp:253]     Train net output #0: loss = 1.36288 (* 1 = 1.36288 loss)
I0522 16:02:37.983538 27620 sgd_solver.cpp:106] Iteration 200500, lr = 0.0045
I0522 16:02:48.601389 27620 solver.cpp:237] Iteration 201000, loss = 1.15873
I0522 16:02:48.601426 27620 solver.cpp:253]     Train net output #0: loss = 1.15873 (* 1 = 1.15873 loss)
I0522 16:02:48.601441 27620 sgd_solver.cpp:106] Iteration 201000, lr = 0.0045
I0522 16:02:59.234174 27620 solver.cpp:237] Iteration 201500, loss = 1.2508
I0522 16:02:59.234212 27620 solver.cpp:253]     Train net output #0: loss = 1.2508 (* 1 = 1.2508 loss)
I0522 16:02:59.234230 27620 sgd_solver.cpp:106] Iteration 201500, lr = 0.0045
I0522 16:03:09.864079 27620 solver.cpp:237] Iteration 202000, loss = 1.01383
I0522 16:03:09.864260 27620 solver.cpp:253]     Train net output #0: loss = 1.01383 (* 1 = 1.01383 loss)
I0522 16:03:09.864275 27620 sgd_solver.cpp:106] Iteration 202000, lr = 0.0045
I0522 16:03:20.477737 27620 solver.cpp:237] Iteration 202500, loss = 1.096
I0522 16:03:20.477787 27620 solver.cpp:253]     Train net output #0: loss = 1.096 (* 1 = 1.096 loss)
I0522 16:03:20.477802 27620 sgd_solver.cpp:106] Iteration 202500, lr = 0.0045
I0522 16:03:31.081588 27620 solver.cpp:237] Iteration 203000, loss = 1.36098
I0522 16:03:31.081624 27620 solver.cpp:253]     Train net output #0: loss = 1.36098 (* 1 = 1.36098 loss)
I0522 16:03:31.081640 27620 sgd_solver.cpp:106] Iteration 203000, lr = 0.0045
I0522 16:04:02.597851 27620 solver.cpp:237] Iteration 203500, loss = 1.40769
I0522 16:04:02.598050 27620 solver.cpp:253]     Train net output #0: loss = 1.40769 (* 1 = 1.40769 loss)
I0522 16:04:02.598065 27620 sgd_solver.cpp:106] Iteration 203500, lr = 0.0045
I0522 16:04:13.227321 27620 solver.cpp:237] Iteration 204000, loss = 1.18336
I0522 16:04:13.227366 27620 solver.cpp:253]     Train net output #0: loss = 1.18336 (* 1 = 1.18336 loss)
I0522 16:04:13.227380 27620 sgd_solver.cpp:106] Iteration 204000, lr = 0.0045
I0522 16:04:23.855602 27620 solver.cpp:237] Iteration 204500, loss = 1.47555
I0522 16:04:23.855638 27620 solver.cpp:253]     Train net output #0: loss = 1.47555 (* 1 = 1.47555 loss)
I0522 16:04:23.855654 27620 sgd_solver.cpp:106] Iteration 204500, lr = 0.0045
I0522 16:04:34.470022 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_205000.caffemodel
I0522 16:04:34.522913 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_205000.solverstate
I0522 16:04:34.554757 27620 solver.cpp:237] Iteration 205000, loss = 1.10409
I0522 16:04:34.554800 27620 solver.cpp:253]     Train net output #0: loss = 1.10409 (* 1 = 1.10409 loss)
I0522 16:04:34.554822 27620 sgd_solver.cpp:106] Iteration 205000, lr = 0.0045
I0522 16:04:45.180851 27620 solver.cpp:237] Iteration 205500, loss = 1.44188
I0522 16:04:45.180889 27620 solver.cpp:253]     Train net output #0: loss = 1.44189 (* 1 = 1.44189 loss)
I0522 16:04:45.180902 27620 sgd_solver.cpp:106] Iteration 205500, lr = 0.0045
I0522 16:04:55.794569 27620 solver.cpp:237] Iteration 206000, loss = 1.09884
I0522 16:04:55.794605 27620 solver.cpp:253]     Train net output #0: loss = 1.09884 (* 1 = 1.09884 loss)
I0522 16:04:55.794621 27620 sgd_solver.cpp:106] Iteration 206000, lr = 0.0045
I0522 16:05:06.402817 27620 solver.cpp:237] Iteration 206500, loss = 1.03234
I0522 16:05:06.403012 27620 solver.cpp:253]     Train net output #0: loss = 1.03234 (* 1 = 1.03234 loss)
I0522 16:05:06.403028 27620 sgd_solver.cpp:106] Iteration 206500, lr = 0.0045
I0522 16:05:37.927621 27620 solver.cpp:237] Iteration 207000, loss = 0.918052
I0522 16:05:37.927830 27620 solver.cpp:253]     Train net output #0: loss = 0.918052 (* 1 = 0.918052 loss)
I0522 16:05:37.927846 27620 sgd_solver.cpp:106] Iteration 207000, lr = 0.0045
I0522 16:05:48.552225 27620 solver.cpp:237] Iteration 207500, loss = 1.05393
I0522 16:05:48.552263 27620 solver.cpp:253]     Train net output #0: loss = 1.05394 (* 1 = 1.05394 loss)
I0522 16:05:48.552278 27620 sgd_solver.cpp:106] Iteration 207500, lr = 0.0045
I0522 16:05:59.113890 27620 solver.cpp:237] Iteration 208000, loss = 1.04078
I0522 16:05:59.113939 27620 solver.cpp:253]     Train net output #0: loss = 1.04078 (* 1 = 1.04078 loss)
I0522 16:05:59.113953 27620 sgd_solver.cpp:106] Iteration 208000, lr = 0.0045
I0522 16:06:09.664340 27620 solver.cpp:237] Iteration 208500, loss = 1.23907
I0522 16:06:09.664520 27620 solver.cpp:253]     Train net output #0: loss = 1.23907 (* 1 = 1.23907 loss)
I0522 16:06:09.664535 27620 sgd_solver.cpp:106] Iteration 208500, lr = 0.0045
I0522 16:06:20.211231 27620 solver.cpp:237] Iteration 209000, loss = 1.11572
I0522 16:06:20.211275 27620 solver.cpp:253]     Train net output #0: loss = 1.11572 (* 1 = 1.11572 loss)
I0522 16:06:20.211290 27620 sgd_solver.cpp:106] Iteration 209000, lr = 0.0045
I0522 16:06:30.755396 27620 solver.cpp:237] Iteration 209500, loss = 1.40897
I0522 16:06:30.755432 27620 solver.cpp:253]     Train net output #0: loss = 1.40897 (* 1 = 1.40897 loss)
I0522 16:06:30.755450 27620 sgd_solver.cpp:106] Iteration 209500, lr = 0.0045
I0522 16:06:41.275651 27620 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_210000.caffemodel
I0522 16:06:41.329475 27620 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_30_lr_0.0045_2016-05-20T15.49.00.352182_iter_210000.solverstate
I0522 16:06:41.354887 27620 solver.cpp:341] Iteration 210000, Testing net (#0)
I0522 16:07:31.037469 27620 solver.cpp:409]     Test net output #0: accuracy = 0.90019
I0522 16:07:31.037674 27620 solver.cpp:409]     Test net output #1: loss = 0.321174 (* 1 = 0.321174 loss)
I0522 16:07:51.887563 27620 solver.cpp:237] Iteration 210000, loss = 1.16219
I0522 16:07:51.887617 27620 solver.cpp:253]     Train net output #0: loss = 1.1622 (* 1 = 1.1622 loss)
I0522 16:07:51.887632 27620 sgd_solver.cpp:106] Iteration 210000, lr = 0.0045
I0522 16:08:02.450676 27620 solver.cpp:237] Iteration 210500, loss = 1.20096
I0522 16:08:02.450871 27620 solver.cpp:253]     Train net output #0: loss = 1.20096 (* 1 = 1.20096 loss)
I0522 16:08:02.450886 27620 sgd_solver.cpp:106] Iteration 210500, lr = 0.0045
I0522 16:08:13.016882 27620 solver.cpp:237] Iteration 211000, loss = 1.20768
I0522 16:08:13.016918 27620 solver.cpp:253]     Train net output #0: loss = 1.20768 (* 1 = 1.20768 loss)
I0522 16:08:13.016934 27620 sgd_solver.cpp:106] Iteration 211000, lr = 0.0045
I0522 16:08:23.600734 27620 solver.cpp:237] Iteration 211500, loss = 1.39499
I0522 16:08:23.600783 27620 solver.cpp:253]     Train net output #0: loss = 1.39499 (* 1 = 1.39499 loss)
I0522 16:08:23.600796 27620 sgd_solver.cpp:106] Iteration 211500, lr = 0.0045
I0522 16:08:34.179008 27620 solver.cpp:237] Iteration 212000, loss = 1.29872
I0522 16:08:34.179195 27620 solver.cpp:253]     Train net output #0: loss = 1.29872 (* 1 = 1.29872 loss)
I0522 16:08:34.179209 27620 sgd_solver.cpp:106] Iteration 212000, lr = 0.0045
I0522 16:08:44.754755 27620 solver.cpp:237] Iteration 212500, loss = 1.39007
I0522 16:08:44.754806 27620 solver.cpp:253]     Train net output #0: loss = 1.39007 (* 1 = 1.39007 loss)
I0522 16:08:44.754822 27620 sgd_solver.cpp:106] Iteration 212500, lr = 0.0045
I0522 16:08:55.331226 27620 solver.cpp:237] Iteration 213000, loss = 1.2778
I0522 16:08:55.331262 27620 solver.cpp:253]     Train net output #0: loss = 1.2778 (* 1 = 1.2778 loss)
I0522 16:08:55.331279 27620 sgd_solver.cpp:106] Iteration 213000, lr = 0.0045
I0522 16:09:26.763216 27620 solver.cpp:237] Iteration 213500, loss = 1.22803
I0522 16:09:26.763427 27620 solver.cpp:253]     Train net output #0: loss = 1.22803 (* 1 = 1.22803 loss)
I0522 16:09:26.763443 27620 sgd_solver.cpp:106] Iteration 213500, lr = 0.0045
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
*** Aborted at 1463947776 (unix time) try "date -d @1463947776" if you are using GNU date ***
PC: @     0x2aaaaaaca834 ([vdso]+0x833)
=>> PBS: job killed: walltime 7225 exceeded limit 7200
aprun: Apid 11248521: Caught signal Terminated, sending to application
*** SIGTERM (@0x6be1) received by PID 27620 (TID 0x2aaac746f900) from PID 27617; stack trace: ***
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @     0x2aaab7c78850 (unknown)
    @     0x2aaaaaaca834 ([vdso]+0x833)
    @     0x2aaab82072d0 maybe_syscall_gettime_cpu
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @     0x2aaab82074b0 __GI_clock_gettime
    @     0x2aaab9898f3e (unknown)
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @     0x2aaab928ec5b (unknown)
    @     0x2aaab926d723 (unknown)
    @     0x2aaab92655e1 (unknown)
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @     0x2aaab9266356 (unknown)
    @     0x2aaab91d5562 (unknown)
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @     0x2aaab91d56ba (unknown)
    @     0x2aaab91b8715 cuMemcpy
    @     0x2aaaaacf9e92 (unknown)
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @     0x2aaaaacde306 (unknown)
    @     0x2aaaaad00328 cudaMemcpy
    @           0x4d6a10 caffe::caffe_copy<>()
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @           0x626ea9 caffe::HDF5DataLayer<>::Forward_gpu()
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @           0x5ca109 caffe::Solver<>::Step()
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
aprun: Apid 11248521: Caught signal Terminated, sending to application
    @           0x438669 (unknown)
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
aprun: Apid 11248521: Caught signal Terminated, sending to application
