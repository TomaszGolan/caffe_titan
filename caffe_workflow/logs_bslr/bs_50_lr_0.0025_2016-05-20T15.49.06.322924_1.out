2808123
I0523 05:54:31.763106 32758 caffe.cpp:184] Using GPUs 0
I0523 05:54:32.188809 32758 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3000
test_interval: 6000
base_lr: 0.0025
display: 300
max_iter: 300000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 3000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924.prototxt"
I0523 05:54:32.190372 32758 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924.prototxt
I0523 05:54:32.201800 32758 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0523 05:54:32.201860 32758 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0523 05:54:32.202208 32758 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0523 05:54:32.202388 32758 layer_factory.hpp:77] Creating layer data_hdf5
I0523 05:54:32.202410 32758 net.cpp:106] Creating Layer data_hdf5
I0523 05:54:32.202425 32758 net.cpp:411] data_hdf5 -> data
I0523 05:54:32.202460 32758 net.cpp:411] data_hdf5 -> label
I0523 05:54:32.202491 32758 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0523 05:54:32.209460 32758 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0523 05:54:32.220652 32758 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0523 05:54:53.859091 32758 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0523 05:54:53.864246 32758 net.cpp:150] Setting up data_hdf5
I0523 05:54:53.864287 32758 net.cpp:157] Top shape: 50 1 127 50 (317500)
I0523 05:54:53.864301 32758 net.cpp:157] Top shape: 50 (50)
I0523 05:54:53.864312 32758 net.cpp:165] Memory required for data: 1270200
I0523 05:54:53.864326 32758 layer_factory.hpp:77] Creating layer conv1
I0523 05:54:53.864361 32758 net.cpp:106] Creating Layer conv1
I0523 05:54:53.864372 32758 net.cpp:454] conv1 <- data
I0523 05:54:53.864392 32758 net.cpp:411] conv1 -> conv1
I0523 05:54:56.837373 32758 net.cpp:150] Setting up conv1
I0523 05:54:56.837419 32758 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 05:54:56.837430 32758 net.cpp:165] Memory required for data: 15094200
I0523 05:54:56.837460 32758 layer_factory.hpp:77] Creating layer relu1
I0523 05:54:56.837481 32758 net.cpp:106] Creating Layer relu1
I0523 05:54:56.837491 32758 net.cpp:454] relu1 <- conv1
I0523 05:54:56.837505 32758 net.cpp:397] relu1 -> conv1 (in-place)
I0523 05:54:56.838021 32758 net.cpp:150] Setting up relu1
I0523 05:54:56.838037 32758 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 05:54:56.838047 32758 net.cpp:165] Memory required for data: 28918200
I0523 05:54:56.838057 32758 layer_factory.hpp:77] Creating layer pool1
I0523 05:54:56.838074 32758 net.cpp:106] Creating Layer pool1
I0523 05:54:56.838084 32758 net.cpp:454] pool1 <- conv1
I0523 05:54:56.838098 32758 net.cpp:411] pool1 -> pool1
I0523 05:54:56.838177 32758 net.cpp:150] Setting up pool1
I0523 05:54:56.838192 32758 net.cpp:157] Top shape: 50 12 60 48 (1728000)
I0523 05:54:56.838202 32758 net.cpp:165] Memory required for data: 35830200
I0523 05:54:56.838212 32758 layer_factory.hpp:77] Creating layer conv2
I0523 05:54:56.838235 32758 net.cpp:106] Creating Layer conv2
I0523 05:54:56.838245 32758 net.cpp:454] conv2 <- pool1
I0523 05:54:56.838259 32758 net.cpp:411] conv2 -> conv2
I0523 05:54:56.840945 32758 net.cpp:150] Setting up conv2
I0523 05:54:56.840972 32758 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 05:54:56.840983 32758 net.cpp:165] Memory required for data: 45766200
I0523 05:54:56.841002 32758 layer_factory.hpp:77] Creating layer relu2
I0523 05:54:56.841038 32758 net.cpp:106] Creating Layer relu2
I0523 05:54:56.841050 32758 net.cpp:454] relu2 <- conv2
I0523 05:54:56.841061 32758 net.cpp:397] relu2 -> conv2 (in-place)
I0523 05:54:56.841395 32758 net.cpp:150] Setting up relu2
I0523 05:54:56.841410 32758 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 05:54:56.841420 32758 net.cpp:165] Memory required for data: 55702200
I0523 05:54:56.841430 32758 layer_factory.hpp:77] Creating layer pool2
I0523 05:54:56.841444 32758 net.cpp:106] Creating Layer pool2
I0523 05:54:56.841454 32758 net.cpp:454] pool2 <- conv2
I0523 05:54:56.841466 32758 net.cpp:411] pool2 -> pool2
I0523 05:54:56.841547 32758 net.cpp:150] Setting up pool2
I0523 05:54:56.841560 32758 net.cpp:157] Top shape: 50 20 27 46 (1242000)
I0523 05:54:56.841570 32758 net.cpp:165] Memory required for data: 60670200
I0523 05:54:56.841580 32758 layer_factory.hpp:77] Creating layer conv3
I0523 05:54:56.841598 32758 net.cpp:106] Creating Layer conv3
I0523 05:54:56.841609 32758 net.cpp:454] conv3 <- pool2
I0523 05:54:56.841622 32758 net.cpp:411] conv3 -> conv3
I0523 05:54:56.843565 32758 net.cpp:150] Setting up conv3
I0523 05:54:56.843588 32758 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 05:54:56.843601 32758 net.cpp:165] Memory required for data: 66091000
I0523 05:54:56.843619 32758 layer_factory.hpp:77] Creating layer relu3
I0523 05:54:56.843636 32758 net.cpp:106] Creating Layer relu3
I0523 05:54:56.843646 32758 net.cpp:454] relu3 <- conv3
I0523 05:54:56.843658 32758 net.cpp:397] relu3 -> conv3 (in-place)
I0523 05:54:56.844130 32758 net.cpp:150] Setting up relu3
I0523 05:54:56.844146 32758 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 05:54:56.844157 32758 net.cpp:165] Memory required for data: 71511800
I0523 05:54:56.844167 32758 layer_factory.hpp:77] Creating layer pool3
I0523 05:54:56.844180 32758 net.cpp:106] Creating Layer pool3
I0523 05:54:56.844190 32758 net.cpp:454] pool3 <- conv3
I0523 05:54:56.844202 32758 net.cpp:411] pool3 -> pool3
I0523 05:54:56.844270 32758 net.cpp:150] Setting up pool3
I0523 05:54:56.844283 32758 net.cpp:157] Top shape: 50 28 11 44 (677600)
I0523 05:54:56.844292 32758 net.cpp:165] Memory required for data: 74222200
I0523 05:54:56.844302 32758 layer_factory.hpp:77] Creating layer conv4
I0523 05:54:56.844317 32758 net.cpp:106] Creating Layer conv4
I0523 05:54:56.844327 32758 net.cpp:454] conv4 <- pool3
I0523 05:54:56.844341 32758 net.cpp:411] conv4 -> conv4
I0523 05:54:56.847101 32758 net.cpp:150] Setting up conv4
I0523 05:54:56.847137 32758 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 05:54:56.847147 32758 net.cpp:165] Memory required for data: 76036600
I0523 05:54:56.847165 32758 layer_factory.hpp:77] Creating layer relu4
I0523 05:54:56.847179 32758 net.cpp:106] Creating Layer relu4
I0523 05:54:56.847190 32758 net.cpp:454] relu4 <- conv4
I0523 05:54:56.847203 32758 net.cpp:397] relu4 -> conv4 (in-place)
I0523 05:54:56.847677 32758 net.cpp:150] Setting up relu4
I0523 05:54:56.847693 32758 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 05:54:56.847704 32758 net.cpp:165] Memory required for data: 77851000
I0523 05:54:56.847715 32758 layer_factory.hpp:77] Creating layer pool4
I0523 05:54:56.847728 32758 net.cpp:106] Creating Layer pool4
I0523 05:54:56.847738 32758 net.cpp:454] pool4 <- conv4
I0523 05:54:56.847750 32758 net.cpp:411] pool4 -> pool4
I0523 05:54:56.847818 32758 net.cpp:150] Setting up pool4
I0523 05:54:56.847831 32758 net.cpp:157] Top shape: 50 36 3 42 (226800)
I0523 05:54:56.847842 32758 net.cpp:165] Memory required for data: 78758200
I0523 05:54:56.847852 32758 layer_factory.hpp:77] Creating layer ip1
I0523 05:54:56.847872 32758 net.cpp:106] Creating Layer ip1
I0523 05:54:56.847882 32758 net.cpp:454] ip1 <- pool4
I0523 05:54:56.847895 32758 net.cpp:411] ip1 -> ip1
I0523 05:54:56.863312 32758 net.cpp:150] Setting up ip1
I0523 05:54:56.863340 32758 net.cpp:157] Top shape: 50 196 (9800)
I0523 05:54:56.863353 32758 net.cpp:165] Memory required for data: 78797400
I0523 05:54:56.863375 32758 layer_factory.hpp:77] Creating layer relu5
I0523 05:54:56.863390 32758 net.cpp:106] Creating Layer relu5
I0523 05:54:56.863400 32758 net.cpp:454] relu5 <- ip1
I0523 05:54:56.863414 32758 net.cpp:397] relu5 -> ip1 (in-place)
I0523 05:54:56.863757 32758 net.cpp:150] Setting up relu5
I0523 05:54:56.863771 32758 net.cpp:157] Top shape: 50 196 (9800)
I0523 05:54:56.863782 32758 net.cpp:165] Memory required for data: 78836600
I0523 05:54:56.863792 32758 layer_factory.hpp:77] Creating layer drop1
I0523 05:54:56.863813 32758 net.cpp:106] Creating Layer drop1
I0523 05:54:56.863823 32758 net.cpp:454] drop1 <- ip1
I0523 05:54:56.863837 32758 net.cpp:397] drop1 -> ip1 (in-place)
I0523 05:54:56.863894 32758 net.cpp:150] Setting up drop1
I0523 05:54:56.863909 32758 net.cpp:157] Top shape: 50 196 (9800)
I0523 05:54:56.863919 32758 net.cpp:165] Memory required for data: 78875800
I0523 05:54:56.863929 32758 layer_factory.hpp:77] Creating layer ip2
I0523 05:54:56.863947 32758 net.cpp:106] Creating Layer ip2
I0523 05:54:56.863957 32758 net.cpp:454] ip2 <- ip1
I0523 05:54:56.863971 32758 net.cpp:411] ip2 -> ip2
I0523 05:54:56.864434 32758 net.cpp:150] Setting up ip2
I0523 05:54:56.864446 32758 net.cpp:157] Top shape: 50 98 (4900)
I0523 05:54:56.864456 32758 net.cpp:165] Memory required for data: 78895400
I0523 05:54:56.864471 32758 layer_factory.hpp:77] Creating layer relu6
I0523 05:54:56.864483 32758 net.cpp:106] Creating Layer relu6
I0523 05:54:56.864493 32758 net.cpp:454] relu6 <- ip2
I0523 05:54:56.864506 32758 net.cpp:397] relu6 -> ip2 (in-place)
I0523 05:54:56.865020 32758 net.cpp:150] Setting up relu6
I0523 05:54:56.865036 32758 net.cpp:157] Top shape: 50 98 (4900)
I0523 05:54:56.865046 32758 net.cpp:165] Memory required for data: 78915000
I0523 05:54:56.865058 32758 layer_factory.hpp:77] Creating layer drop2
I0523 05:54:56.865072 32758 net.cpp:106] Creating Layer drop2
I0523 05:54:56.865082 32758 net.cpp:454] drop2 <- ip2
I0523 05:54:56.865093 32758 net.cpp:397] drop2 -> ip2 (in-place)
I0523 05:54:56.865135 32758 net.cpp:150] Setting up drop2
I0523 05:54:56.865149 32758 net.cpp:157] Top shape: 50 98 (4900)
I0523 05:54:56.865159 32758 net.cpp:165] Memory required for data: 78934600
I0523 05:54:56.865170 32758 layer_factory.hpp:77] Creating layer ip3
I0523 05:54:56.865182 32758 net.cpp:106] Creating Layer ip3
I0523 05:54:56.865192 32758 net.cpp:454] ip3 <- ip2
I0523 05:54:56.865203 32758 net.cpp:411] ip3 -> ip3
I0523 05:54:56.865413 32758 net.cpp:150] Setting up ip3
I0523 05:54:56.865427 32758 net.cpp:157] Top shape: 50 11 (550)
I0523 05:54:56.865437 32758 net.cpp:165] Memory required for data: 78936800
I0523 05:54:56.865453 32758 layer_factory.hpp:77] Creating layer drop3
I0523 05:54:56.865464 32758 net.cpp:106] Creating Layer drop3
I0523 05:54:56.865474 32758 net.cpp:454] drop3 <- ip3
I0523 05:54:56.865486 32758 net.cpp:397] drop3 -> ip3 (in-place)
I0523 05:54:56.865525 32758 net.cpp:150] Setting up drop3
I0523 05:54:56.865538 32758 net.cpp:157] Top shape: 50 11 (550)
I0523 05:54:56.865548 32758 net.cpp:165] Memory required for data: 78939000
I0523 05:54:56.865558 32758 layer_factory.hpp:77] Creating layer loss
I0523 05:54:56.865577 32758 net.cpp:106] Creating Layer loss
I0523 05:54:56.865586 32758 net.cpp:454] loss <- ip3
I0523 05:54:56.865597 32758 net.cpp:454] loss <- label
I0523 05:54:56.865609 32758 net.cpp:411] loss -> loss
I0523 05:54:56.865628 32758 layer_factory.hpp:77] Creating layer loss
I0523 05:54:56.866268 32758 net.cpp:150] Setting up loss
I0523 05:54:56.866289 32758 net.cpp:157] Top shape: (1)
I0523 05:54:56.866302 32758 net.cpp:160]     with loss weight 1
I0523 05:54:56.866344 32758 net.cpp:165] Memory required for data: 78939004
I0523 05:54:56.866354 32758 net.cpp:226] loss needs backward computation.
I0523 05:54:56.866365 32758 net.cpp:226] drop3 needs backward computation.
I0523 05:54:56.866375 32758 net.cpp:226] ip3 needs backward computation.
I0523 05:54:56.866386 32758 net.cpp:226] drop2 needs backward computation.
I0523 05:54:56.866396 32758 net.cpp:226] relu6 needs backward computation.
I0523 05:54:56.866406 32758 net.cpp:226] ip2 needs backward computation.
I0523 05:54:56.866416 32758 net.cpp:226] drop1 needs backward computation.
I0523 05:54:56.866426 32758 net.cpp:226] relu5 needs backward computation.
I0523 05:54:56.866436 32758 net.cpp:226] ip1 needs backward computation.
I0523 05:54:56.866446 32758 net.cpp:226] pool4 needs backward computation.
I0523 05:54:56.866456 32758 net.cpp:226] relu4 needs backward computation.
I0523 05:54:56.866466 32758 net.cpp:226] conv4 needs backward computation.
I0523 05:54:56.866475 32758 net.cpp:226] pool3 needs backward computation.
I0523 05:54:56.866487 32758 net.cpp:226] relu3 needs backward computation.
I0523 05:54:56.866497 32758 net.cpp:226] conv3 needs backward computation.
I0523 05:54:56.866515 32758 net.cpp:226] pool2 needs backward computation.
I0523 05:54:56.866526 32758 net.cpp:226] relu2 needs backward computation.
I0523 05:54:56.866538 32758 net.cpp:226] conv2 needs backward computation.
I0523 05:54:56.866547 32758 net.cpp:226] pool1 needs backward computation.
I0523 05:54:56.866559 32758 net.cpp:226] relu1 needs backward computation.
I0523 05:54:56.866569 32758 net.cpp:226] conv1 needs backward computation.
I0523 05:54:56.866580 32758 net.cpp:228] data_hdf5 does not need backward computation.
I0523 05:54:56.866590 32758 net.cpp:270] This network produces output loss
I0523 05:54:56.866612 32758 net.cpp:283] Network initialization done.
I0523 05:54:56.868239 32758 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924.prototxt
I0523 05:54:56.868311 32758 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0523 05:54:56.868669 32758 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0523 05:54:56.868855 32758 layer_factory.hpp:77] Creating layer data_hdf5
I0523 05:54:56.868870 32758 net.cpp:106] Creating Layer data_hdf5
I0523 05:54:56.868882 32758 net.cpp:411] data_hdf5 -> data
I0523 05:54:56.868898 32758 net.cpp:411] data_hdf5 -> label
I0523 05:54:56.868914 32758 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0523 05:54:56.884582 32758 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0523 05:55:18.265663 32758 net.cpp:150] Setting up data_hdf5
I0523 05:55:18.265827 32758 net.cpp:157] Top shape: 50 1 127 50 (317500)
I0523 05:55:18.265841 32758 net.cpp:157] Top shape: 50 (50)
I0523 05:55:18.265853 32758 net.cpp:165] Memory required for data: 1270200
I0523 05:55:18.265866 32758 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0523 05:55:18.265894 32758 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0523 05:55:18.265905 32758 net.cpp:454] label_data_hdf5_1_split <- label
I0523 05:55:18.265920 32758 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0523 05:55:18.265943 32758 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0523 05:55:18.266015 32758 net.cpp:150] Setting up label_data_hdf5_1_split
I0523 05:55:18.266028 32758 net.cpp:157] Top shape: 50 (50)
I0523 05:55:18.266039 32758 net.cpp:157] Top shape: 50 (50)
I0523 05:55:18.266049 32758 net.cpp:165] Memory required for data: 1270600
I0523 05:55:18.266059 32758 layer_factory.hpp:77] Creating layer conv1
I0523 05:55:18.266082 32758 net.cpp:106] Creating Layer conv1
I0523 05:55:18.266091 32758 net.cpp:454] conv1 <- data
I0523 05:55:18.266104 32758 net.cpp:411] conv1 -> conv1
I0523 05:55:18.268023 32758 net.cpp:150] Setting up conv1
I0523 05:55:18.268049 32758 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 05:55:18.268060 32758 net.cpp:165] Memory required for data: 15094600
I0523 05:55:18.268079 32758 layer_factory.hpp:77] Creating layer relu1
I0523 05:55:18.268095 32758 net.cpp:106] Creating Layer relu1
I0523 05:55:18.268105 32758 net.cpp:454] relu1 <- conv1
I0523 05:55:18.268118 32758 net.cpp:397] relu1 -> conv1 (in-place)
I0523 05:55:18.268612 32758 net.cpp:150] Setting up relu1
I0523 05:55:18.268628 32758 net.cpp:157] Top shape: 50 12 120 48 (3456000)
I0523 05:55:18.268640 32758 net.cpp:165] Memory required for data: 28918600
I0523 05:55:18.268649 32758 layer_factory.hpp:77] Creating layer pool1
I0523 05:55:18.268666 32758 net.cpp:106] Creating Layer pool1
I0523 05:55:18.268676 32758 net.cpp:454] pool1 <- conv1
I0523 05:55:18.268688 32758 net.cpp:411] pool1 -> pool1
I0523 05:55:18.268762 32758 net.cpp:150] Setting up pool1
I0523 05:55:18.268776 32758 net.cpp:157] Top shape: 50 12 60 48 (1728000)
I0523 05:55:18.268785 32758 net.cpp:165] Memory required for data: 35830600
I0523 05:55:18.268796 32758 layer_factory.hpp:77] Creating layer conv2
I0523 05:55:18.268815 32758 net.cpp:106] Creating Layer conv2
I0523 05:55:18.268824 32758 net.cpp:454] conv2 <- pool1
I0523 05:55:18.268839 32758 net.cpp:411] conv2 -> conv2
I0523 05:55:18.270747 32758 net.cpp:150] Setting up conv2
I0523 05:55:18.270769 32758 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 05:55:18.270782 32758 net.cpp:165] Memory required for data: 45766600
I0523 05:55:18.270799 32758 layer_factory.hpp:77] Creating layer relu2
I0523 05:55:18.270813 32758 net.cpp:106] Creating Layer relu2
I0523 05:55:18.270823 32758 net.cpp:454] relu2 <- conv2
I0523 05:55:18.270835 32758 net.cpp:397] relu2 -> conv2 (in-place)
I0523 05:55:18.271176 32758 net.cpp:150] Setting up relu2
I0523 05:55:18.271189 32758 net.cpp:157] Top shape: 50 20 54 46 (2484000)
I0523 05:55:18.271199 32758 net.cpp:165] Memory required for data: 55702600
I0523 05:55:18.271209 32758 layer_factory.hpp:77] Creating layer pool2
I0523 05:55:18.271222 32758 net.cpp:106] Creating Layer pool2
I0523 05:55:18.271232 32758 net.cpp:454] pool2 <- conv2
I0523 05:55:18.271245 32758 net.cpp:411] pool2 -> pool2
I0523 05:55:18.271317 32758 net.cpp:150] Setting up pool2
I0523 05:55:18.271332 32758 net.cpp:157] Top shape: 50 20 27 46 (1242000)
I0523 05:55:18.271340 32758 net.cpp:165] Memory required for data: 60670600
I0523 05:55:18.271350 32758 layer_factory.hpp:77] Creating layer conv3
I0523 05:55:18.271368 32758 net.cpp:106] Creating Layer conv3
I0523 05:55:18.271378 32758 net.cpp:454] conv3 <- pool2
I0523 05:55:18.271391 32758 net.cpp:411] conv3 -> conv3
I0523 05:55:18.273363 32758 net.cpp:150] Setting up conv3
I0523 05:55:18.273381 32758 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 05:55:18.273392 32758 net.cpp:165] Memory required for data: 66091400
I0523 05:55:18.273424 32758 layer_factory.hpp:77] Creating layer relu3
I0523 05:55:18.273438 32758 net.cpp:106] Creating Layer relu3
I0523 05:55:18.273448 32758 net.cpp:454] relu3 <- conv3
I0523 05:55:18.273463 32758 net.cpp:397] relu3 -> conv3 (in-place)
I0523 05:55:18.273932 32758 net.cpp:150] Setting up relu3
I0523 05:55:18.273948 32758 net.cpp:157] Top shape: 50 28 22 44 (1355200)
I0523 05:55:18.273958 32758 net.cpp:165] Memory required for data: 71512200
I0523 05:55:18.273968 32758 layer_factory.hpp:77] Creating layer pool3
I0523 05:55:18.273982 32758 net.cpp:106] Creating Layer pool3
I0523 05:55:18.273991 32758 net.cpp:454] pool3 <- conv3
I0523 05:55:18.274003 32758 net.cpp:411] pool3 -> pool3
I0523 05:55:18.274075 32758 net.cpp:150] Setting up pool3
I0523 05:55:18.274087 32758 net.cpp:157] Top shape: 50 28 11 44 (677600)
I0523 05:55:18.274097 32758 net.cpp:165] Memory required for data: 74222600
I0523 05:55:18.274107 32758 layer_factory.hpp:77] Creating layer conv4
I0523 05:55:18.274123 32758 net.cpp:106] Creating Layer conv4
I0523 05:55:18.274134 32758 net.cpp:454] conv4 <- pool3
I0523 05:55:18.274148 32758 net.cpp:411] conv4 -> conv4
I0523 05:55:18.276209 32758 net.cpp:150] Setting up conv4
I0523 05:55:18.276226 32758 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 05:55:18.276237 32758 net.cpp:165] Memory required for data: 76037000
I0523 05:55:18.276255 32758 layer_factory.hpp:77] Creating layer relu4
I0523 05:55:18.276268 32758 net.cpp:106] Creating Layer relu4
I0523 05:55:18.276279 32758 net.cpp:454] relu4 <- conv4
I0523 05:55:18.276291 32758 net.cpp:397] relu4 -> conv4 (in-place)
I0523 05:55:18.276762 32758 net.cpp:150] Setting up relu4
I0523 05:55:18.276778 32758 net.cpp:157] Top shape: 50 36 6 42 (453600)
I0523 05:55:18.276788 32758 net.cpp:165] Memory required for data: 77851400
I0523 05:55:18.276799 32758 layer_factory.hpp:77] Creating layer pool4
I0523 05:55:18.276813 32758 net.cpp:106] Creating Layer pool4
I0523 05:55:18.276823 32758 net.cpp:454] pool4 <- conv4
I0523 05:55:18.276835 32758 net.cpp:411] pool4 -> pool4
I0523 05:55:18.276906 32758 net.cpp:150] Setting up pool4
I0523 05:55:18.276921 32758 net.cpp:157] Top shape: 50 36 3 42 (226800)
I0523 05:55:18.276929 32758 net.cpp:165] Memory required for data: 78758600
I0523 05:55:18.276939 32758 layer_factory.hpp:77] Creating layer ip1
I0523 05:55:18.276953 32758 net.cpp:106] Creating Layer ip1
I0523 05:55:18.276964 32758 net.cpp:454] ip1 <- pool4
I0523 05:55:18.276978 32758 net.cpp:411] ip1 -> ip1
I0523 05:55:18.292474 32758 net.cpp:150] Setting up ip1
I0523 05:55:18.292503 32758 net.cpp:157] Top shape: 50 196 (9800)
I0523 05:55:18.292518 32758 net.cpp:165] Memory required for data: 78797800
I0523 05:55:18.292541 32758 layer_factory.hpp:77] Creating layer relu5
I0523 05:55:18.292556 32758 net.cpp:106] Creating Layer relu5
I0523 05:55:18.292565 32758 net.cpp:454] relu5 <- ip1
I0523 05:55:18.292580 32758 net.cpp:397] relu5 -> ip1 (in-place)
I0523 05:55:18.292929 32758 net.cpp:150] Setting up relu5
I0523 05:55:18.292943 32758 net.cpp:157] Top shape: 50 196 (9800)
I0523 05:55:18.292953 32758 net.cpp:165] Memory required for data: 78837000
I0523 05:55:18.292963 32758 layer_factory.hpp:77] Creating layer drop1
I0523 05:55:18.292982 32758 net.cpp:106] Creating Layer drop1
I0523 05:55:18.292992 32758 net.cpp:454] drop1 <- ip1
I0523 05:55:18.293006 32758 net.cpp:397] drop1 -> ip1 (in-place)
I0523 05:55:18.293051 32758 net.cpp:150] Setting up drop1
I0523 05:55:18.293063 32758 net.cpp:157] Top shape: 50 196 (9800)
I0523 05:55:18.293073 32758 net.cpp:165] Memory required for data: 78876200
I0523 05:55:18.293083 32758 layer_factory.hpp:77] Creating layer ip2
I0523 05:55:18.293097 32758 net.cpp:106] Creating Layer ip2
I0523 05:55:18.293107 32758 net.cpp:454] ip2 <- ip1
I0523 05:55:18.293120 32758 net.cpp:411] ip2 -> ip2
I0523 05:55:18.293598 32758 net.cpp:150] Setting up ip2
I0523 05:55:18.293612 32758 net.cpp:157] Top shape: 50 98 (4900)
I0523 05:55:18.293622 32758 net.cpp:165] Memory required for data: 78895800
I0523 05:55:18.293637 32758 layer_factory.hpp:77] Creating layer relu6
I0523 05:55:18.293660 32758 net.cpp:106] Creating Layer relu6
I0523 05:55:18.293670 32758 net.cpp:454] relu6 <- ip2
I0523 05:55:18.293684 32758 net.cpp:397] relu6 -> ip2 (in-place)
I0523 05:55:18.294216 32758 net.cpp:150] Setting up relu6
I0523 05:55:18.294232 32758 net.cpp:157] Top shape: 50 98 (4900)
I0523 05:55:18.294244 32758 net.cpp:165] Memory required for data: 78915400
I0523 05:55:18.294256 32758 layer_factory.hpp:77] Creating layer drop2
I0523 05:55:18.294270 32758 net.cpp:106] Creating Layer drop2
I0523 05:55:18.294280 32758 net.cpp:454] drop2 <- ip2
I0523 05:55:18.294292 32758 net.cpp:397] drop2 -> ip2 (in-place)
I0523 05:55:18.294337 32758 net.cpp:150] Setting up drop2
I0523 05:55:18.294349 32758 net.cpp:157] Top shape: 50 98 (4900)
I0523 05:55:18.294358 32758 net.cpp:165] Memory required for data: 78935000
I0523 05:55:18.294368 32758 layer_factory.hpp:77] Creating layer ip3
I0523 05:55:18.294381 32758 net.cpp:106] Creating Layer ip3
I0523 05:55:18.294391 32758 net.cpp:454] ip3 <- ip2
I0523 05:55:18.294405 32758 net.cpp:411] ip3 -> ip3
I0523 05:55:18.294625 32758 net.cpp:150] Setting up ip3
I0523 05:55:18.294638 32758 net.cpp:157] Top shape: 50 11 (550)
I0523 05:55:18.294648 32758 net.cpp:165] Memory required for data: 78937200
I0523 05:55:18.294662 32758 layer_factory.hpp:77] Creating layer drop3
I0523 05:55:18.294674 32758 net.cpp:106] Creating Layer drop3
I0523 05:55:18.294684 32758 net.cpp:454] drop3 <- ip3
I0523 05:55:18.294697 32758 net.cpp:397] drop3 -> ip3 (in-place)
I0523 05:55:18.294737 32758 net.cpp:150] Setting up drop3
I0523 05:55:18.294750 32758 net.cpp:157] Top shape: 50 11 (550)
I0523 05:55:18.294760 32758 net.cpp:165] Memory required for data: 78939400
I0523 05:55:18.294770 32758 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0523 05:55:18.294782 32758 net.cpp:106] Creating Layer ip3_drop3_0_split
I0523 05:55:18.294791 32758 net.cpp:454] ip3_drop3_0_split <- ip3
I0523 05:55:18.294805 32758 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0523 05:55:18.294819 32758 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0523 05:55:18.294891 32758 net.cpp:150] Setting up ip3_drop3_0_split
I0523 05:55:18.294904 32758 net.cpp:157] Top shape: 50 11 (550)
I0523 05:55:18.294917 32758 net.cpp:157] Top shape: 50 11 (550)
I0523 05:55:18.294927 32758 net.cpp:165] Memory required for data: 78943800
I0523 05:55:18.294937 32758 layer_factory.hpp:77] Creating layer accuracy
I0523 05:55:18.294958 32758 net.cpp:106] Creating Layer accuracy
I0523 05:55:18.294967 32758 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0523 05:55:18.294978 32758 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0523 05:55:18.294992 32758 net.cpp:411] accuracy -> accuracy
I0523 05:55:18.295017 32758 net.cpp:150] Setting up accuracy
I0523 05:55:18.295029 32758 net.cpp:157] Top shape: (1)
I0523 05:55:18.295039 32758 net.cpp:165] Memory required for data: 78943804
I0523 05:55:18.295049 32758 layer_factory.hpp:77] Creating layer loss
I0523 05:55:18.295063 32758 net.cpp:106] Creating Layer loss
I0523 05:55:18.295071 32758 net.cpp:454] loss <- ip3_drop3_0_split_1
I0523 05:55:18.295083 32758 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0523 05:55:18.295095 32758 net.cpp:411] loss -> loss
I0523 05:55:18.295120 32758 layer_factory.hpp:77] Creating layer loss
I0523 05:55:18.295603 32758 net.cpp:150] Setting up loss
I0523 05:55:18.295617 32758 net.cpp:157] Top shape: (1)
I0523 05:55:18.295627 32758 net.cpp:160]     with loss weight 1
I0523 05:55:18.295645 32758 net.cpp:165] Memory required for data: 78943808
I0523 05:55:18.295655 32758 net.cpp:226] loss needs backward computation.
I0523 05:55:18.295666 32758 net.cpp:228] accuracy does not need backward computation.
I0523 05:55:18.295677 32758 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0523 05:55:18.295687 32758 net.cpp:226] drop3 needs backward computation.
I0523 05:55:18.295697 32758 net.cpp:226] ip3 needs backward computation.
I0523 05:55:18.295706 32758 net.cpp:226] drop2 needs backward computation.
I0523 05:55:18.295716 32758 net.cpp:226] relu6 needs backward computation.
I0523 05:55:18.295732 32758 net.cpp:226] ip2 needs backward computation.
I0523 05:55:18.295743 32758 net.cpp:226] drop1 needs backward computation.
I0523 05:55:18.295753 32758 net.cpp:226] relu5 needs backward computation.
I0523 05:55:18.295763 32758 net.cpp:226] ip1 needs backward computation.
I0523 05:55:18.295773 32758 net.cpp:226] pool4 needs backward computation.
I0523 05:55:18.295783 32758 net.cpp:226] relu4 needs backward computation.
I0523 05:55:18.295792 32758 net.cpp:226] conv4 needs backward computation.
I0523 05:55:18.295802 32758 net.cpp:226] pool3 needs backward computation.
I0523 05:55:18.295812 32758 net.cpp:226] relu3 needs backward computation.
I0523 05:55:18.295821 32758 net.cpp:226] conv3 needs backward computation.
I0523 05:55:18.295831 32758 net.cpp:226] pool2 needs backward computation.
I0523 05:55:18.295841 32758 net.cpp:226] relu2 needs backward computation.
I0523 05:55:18.295855 32758 net.cpp:226] conv2 needs backward computation.
I0523 05:55:18.295864 32758 net.cpp:226] pool1 needs backward computation.
I0523 05:55:18.295872 32758 net.cpp:226] relu1 needs backward computation.
I0523 05:55:18.295882 32758 net.cpp:226] conv1 needs backward computation.
I0523 05:55:18.295894 32758 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0523 05:55:18.295907 32758 net.cpp:228] data_hdf5 does not need backward computation.
I0523 05:55:18.295917 32758 net.cpp:270] This network produces output accuracy
I0523 05:55:18.295928 32758 net.cpp:270] This network produces output loss
I0523 05:55:18.295955 32758 net.cpp:283] Network initialization done.
I0523 05:55:18.296087 32758 solver.cpp:60] Solver scaffolding done.
I0523 05:55:18.297215 32758 caffe.cpp:212] Starting Optimization
I0523 05:55:18.297229 32758 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0523 05:55:18.297238 32758 solver.cpp:289] Learning Rate Policy: fixed
I0523 05:55:18.298452 32758 solver.cpp:341] Iteration 0, Testing net (#0)
I0523 05:56:07.088222 32758 solver.cpp:409]     Test net output #0: accuracy = 0.0753932
I0523 05:56:07.088384 32758 solver.cpp:409]     Test net output #1: loss = 2.39885 (* 1 = 2.39885 loss)
I0523 05:56:07.112460 32758 solver.cpp:237] Iteration 0, loss = 2.40138
I0523 05:56:07.112498 32758 solver.cpp:253]     Train net output #0: loss = 2.40138 (* 1 = 2.40138 loss)
I0523 05:56:07.112519 32758 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0523 05:56:16.392030 32758 solver.cpp:237] Iteration 300, loss = 2.20924
I0523 05:56:16.392066 32758 solver.cpp:253]     Train net output #0: loss = 2.20924 (* 1 = 2.20924 loss)
I0523 05:56:16.392081 32758 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0523 05:56:25.671316 32758 solver.cpp:237] Iteration 600, loss = 2.09003
I0523 05:56:25.671365 32758 solver.cpp:253]     Train net output #0: loss = 2.09003 (* 1 = 2.09003 loss)
I0523 05:56:25.671378 32758 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0523 05:56:34.947223 32758 solver.cpp:237] Iteration 900, loss = 1.92134
I0523 05:56:34.947259 32758 solver.cpp:253]     Train net output #0: loss = 1.92134 (* 1 = 1.92134 loss)
I0523 05:56:34.947273 32758 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0523 05:56:44.225185 32758 solver.cpp:237] Iteration 1200, loss = 1.84702
I0523 05:56:44.225340 32758 solver.cpp:253]     Train net output #0: loss = 1.84702 (* 1 = 1.84702 loss)
I0523 05:56:44.225354 32758 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0523 05:56:53.504322 32758 solver.cpp:237] Iteration 1500, loss = 1.7877
I0523 05:56:53.504356 32758 solver.cpp:253]     Train net output #0: loss = 1.7877 (* 1 = 1.7877 loss)
I0523 05:56:53.504371 32758 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0523 05:57:02.782858 32758 solver.cpp:237] Iteration 1800, loss = 1.94818
I0523 05:57:02.782894 32758 solver.cpp:253]     Train net output #0: loss = 1.94818 (* 1 = 1.94818 loss)
I0523 05:57:02.782907 32758 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0523 05:57:34.197536 32758 solver.cpp:237] Iteration 2100, loss = 1.67721
I0523 05:57:34.197697 32758 solver.cpp:253]     Train net output #0: loss = 1.67721 (* 1 = 1.67721 loss)
I0523 05:57:34.197711 32758 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0523 05:57:43.478986 32758 solver.cpp:237] Iteration 2400, loss = 1.53068
I0523 05:57:43.479022 32758 solver.cpp:253]     Train net output #0: loss = 1.53068 (* 1 = 1.53068 loss)
I0523 05:57:43.479038 32758 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0523 05:57:52.760829 32758 solver.cpp:237] Iteration 2700, loss = 1.8052
I0523 05:57:52.760865 32758 solver.cpp:253]     Train net output #0: loss = 1.8052 (* 1 = 1.8052 loss)
I0523 05:57:52.760880 32758 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0523 05:58:02.013463 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_3000.caffemodel
I0523 05:58:02.076138 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_3000.solverstate
I0523 05:58:02.113477 32758 solver.cpp:237] Iteration 3000, loss = 1.62993
I0523 05:58:02.113528 32758 solver.cpp:253]     Train net output #0: loss = 1.62993 (* 1 = 1.62993 loss)
I0523 05:58:02.113541 32758 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0523 05:58:11.393925 32758 solver.cpp:237] Iteration 3300, loss = 1.66761
I0523 05:58:11.394068 32758 solver.cpp:253]     Train net output #0: loss = 1.66761 (* 1 = 1.66761 loss)
I0523 05:58:11.394081 32758 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0523 05:58:20.675451 32758 solver.cpp:237] Iteration 3600, loss = 1.50598
I0523 05:58:20.675485 32758 solver.cpp:253]     Train net output #0: loss = 1.50598 (* 1 = 1.50598 loss)
I0523 05:58:20.675500 32758 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0523 05:58:29.961071 32758 solver.cpp:237] Iteration 3900, loss = 1.61377
I0523 05:58:29.961112 32758 solver.cpp:253]     Train net output #0: loss = 1.61377 (* 1 = 1.61377 loss)
I0523 05:58:29.961127 32758 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0523 05:59:01.420017 32758 solver.cpp:237] Iteration 4200, loss = 1.70984
I0523 05:59:01.420174 32758 solver.cpp:253]     Train net output #0: loss = 1.70984 (* 1 = 1.70984 loss)
I0523 05:59:01.420188 32758 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0523 05:59:10.705483 32758 solver.cpp:237] Iteration 4500, loss = 1.52755
I0523 05:59:10.705519 32758 solver.cpp:253]     Train net output #0: loss = 1.52755 (* 1 = 1.52755 loss)
I0523 05:59:10.705533 32758 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0523 05:59:19.986938 32758 solver.cpp:237] Iteration 4800, loss = 1.85825
I0523 05:59:19.986979 32758 solver.cpp:253]     Train net output #0: loss = 1.85825 (* 1 = 1.85825 loss)
I0523 05:59:19.986996 32758 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0523 05:59:29.267410 32758 solver.cpp:237] Iteration 5100, loss = 1.59433
I0523 05:59:29.267444 32758 solver.cpp:253]     Train net output #0: loss = 1.59433 (* 1 = 1.59433 loss)
I0523 05:59:29.267462 32758 sgd_solver.cpp:106] Iteration 5100, lr = 0.0025
I0523 05:59:38.550277 32758 solver.cpp:237] Iteration 5400, loss = 1.32592
I0523 05:59:38.550443 32758 solver.cpp:253]     Train net output #0: loss = 1.32592 (* 1 = 1.32592 loss)
I0523 05:59:38.550457 32758 sgd_solver.cpp:106] Iteration 5400, lr = 0.0025
I0523 05:59:47.832967 32758 solver.cpp:237] Iteration 5700, loss = 1.31718
I0523 05:59:47.833003 32758 solver.cpp:253]     Train net output #0: loss = 1.31718 (* 1 = 1.31718 loss)
I0523 05:59:47.833017 32758 sgd_solver.cpp:106] Iteration 5700, lr = 0.0025
I0523 05:59:57.087546 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_6000.caffemodel
I0523 05:59:57.148250 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_6000.solverstate
I0523 05:59:57.174963 32758 solver.cpp:341] Iteration 6000, Testing net (#0)
I0523 06:00:44.950945 32758 solver.cpp:409]     Test net output #0: accuracy = 0.78143
I0523 06:00:44.951102 32758 solver.cpp:409]     Test net output #1: loss = 0.7763 (* 1 = 0.7763 loss)
I0523 06:01:07.125720 32758 solver.cpp:237] Iteration 6000, loss = 1.4399
I0523 06:01:07.125773 32758 solver.cpp:253]     Train net output #0: loss = 1.4399 (* 1 = 1.4399 loss)
I0523 06:01:07.125788 32758 sgd_solver.cpp:106] Iteration 6000, lr = 0.0025
I0523 06:01:16.402456 32758 solver.cpp:237] Iteration 6300, loss = 1.48144
I0523 06:01:16.402614 32758 solver.cpp:253]     Train net output #0: loss = 1.48144 (* 1 = 1.48144 loss)
I0523 06:01:16.402628 32758 sgd_solver.cpp:106] Iteration 6300, lr = 0.0025
I0523 06:01:25.681149 32758 solver.cpp:237] Iteration 6600, loss = 1.32281
I0523 06:01:25.681193 32758 solver.cpp:253]     Train net output #0: loss = 1.32281 (* 1 = 1.32281 loss)
I0523 06:01:25.681207 32758 sgd_solver.cpp:106] Iteration 6600, lr = 0.0025
I0523 06:01:34.962013 32758 solver.cpp:237] Iteration 6900, loss = 1.5973
I0523 06:01:34.962047 32758 solver.cpp:253]     Train net output #0: loss = 1.5973 (* 1 = 1.5973 loss)
I0523 06:01:34.962062 32758 sgd_solver.cpp:106] Iteration 6900, lr = 0.0025
I0523 06:01:44.239934 32758 solver.cpp:237] Iteration 7200, loss = 1.33542
I0523 06:01:44.239976 32758 solver.cpp:253]     Train net output #0: loss = 1.33542 (* 1 = 1.33542 loss)
I0523 06:01:44.239991 32758 sgd_solver.cpp:106] Iteration 7200, lr = 0.0025
I0523 06:01:53.519620 32758 solver.cpp:237] Iteration 7500, loss = 1.3771
I0523 06:01:53.519753 32758 solver.cpp:253]     Train net output #0: loss = 1.3771 (* 1 = 1.3771 loss)
I0523 06:01:53.519767 32758 sgd_solver.cpp:106] Iteration 7500, lr = 0.0025
I0523 06:02:02.796747 32758 solver.cpp:237] Iteration 7800, loss = 1.3038
I0523 06:02:02.796782 32758 solver.cpp:253]     Train net output #0: loss = 1.3038 (* 1 = 1.3038 loss)
I0523 06:02:02.796797 32758 sgd_solver.cpp:106] Iteration 7800, lr = 0.0025
I0523 06:02:34.220060 32758 solver.cpp:237] Iteration 8100, loss = 1.60021
I0523 06:02:34.220221 32758 solver.cpp:253]     Train net output #0: loss = 1.60021 (* 1 = 1.60021 loss)
I0523 06:02:34.220238 32758 sgd_solver.cpp:106] Iteration 8100, lr = 0.0025
I0523 06:02:43.496553 32758 solver.cpp:237] Iteration 8400, loss = 1.21879
I0523 06:02:43.496588 32758 solver.cpp:253]     Train net output #0: loss = 1.21879 (* 1 = 1.21879 loss)
I0523 06:02:43.496603 32758 sgd_solver.cpp:106] Iteration 8400, lr = 0.0025
I0523 06:02:52.775837 32758 solver.cpp:237] Iteration 8700, loss = 1.34305
I0523 06:02:52.775873 32758 solver.cpp:253]     Train net output #0: loss = 1.34305 (* 1 = 1.34305 loss)
I0523 06:02:52.775887 32758 sgd_solver.cpp:106] Iteration 8700, lr = 0.0025
I0523 06:03:02.027976 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_9000.caffemodel
I0523 06:03:02.088829 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_9000.solverstate
I0523 06:03:02.125587 32758 solver.cpp:237] Iteration 9000, loss = 1.53957
I0523 06:03:02.125728 32758 solver.cpp:253]     Train net output #0: loss = 1.53957 (* 1 = 1.53957 loss)
I0523 06:03:02.125744 32758 sgd_solver.cpp:106] Iteration 9000, lr = 0.0025
I0523 06:03:11.401695 32758 solver.cpp:237] Iteration 9300, loss = 1.22202
I0523 06:03:11.401847 32758 solver.cpp:253]     Train net output #0: loss = 1.22202 (* 1 = 1.22202 loss)
I0523 06:03:11.401861 32758 sgd_solver.cpp:106] Iteration 9300, lr = 0.0025
I0523 06:03:20.679577 32758 solver.cpp:237] Iteration 9600, loss = 1.35305
I0523 06:03:20.679611 32758 solver.cpp:253]     Train net output #0: loss = 1.35305 (* 1 = 1.35305 loss)
I0523 06:03:20.679627 32758 sgd_solver.cpp:106] Iteration 9600, lr = 0.0025
I0523 06:03:29.955842 32758 solver.cpp:237] Iteration 9900, loss = 1.34935
I0523 06:03:29.955890 32758 solver.cpp:253]     Train net output #0: loss = 1.34935 (* 1 = 1.34935 loss)
I0523 06:03:29.955906 32758 sgd_solver.cpp:106] Iteration 9900, lr = 0.0025
I0523 06:04:01.363829 32758 solver.cpp:237] Iteration 10200, loss = 1.25091
I0523 06:04:01.363991 32758 solver.cpp:253]     Train net output #0: loss = 1.25091 (* 1 = 1.25091 loss)
I0523 06:04:01.364004 32758 sgd_solver.cpp:106] Iteration 10200, lr = 0.0025
I0523 06:04:10.641618 32758 solver.cpp:237] Iteration 10500, loss = 1.49037
I0523 06:04:10.641652 32758 solver.cpp:253]     Train net output #0: loss = 1.49037 (* 1 = 1.49037 loss)
I0523 06:04:10.641666 32758 sgd_solver.cpp:106] Iteration 10500, lr = 0.0025
I0523 06:04:19.923069 32758 solver.cpp:237] Iteration 10800, loss = 1.27472
I0523 06:04:19.923118 32758 solver.cpp:253]     Train net output #0: loss = 1.27472 (* 1 = 1.27472 loss)
I0523 06:04:19.923132 32758 sgd_solver.cpp:106] Iteration 10800, lr = 0.0025
I0523 06:04:29.205236 32758 solver.cpp:237] Iteration 11100, loss = 1.27881
I0523 06:04:29.205271 32758 solver.cpp:253]     Train net output #0: loss = 1.27881 (* 1 = 1.27881 loss)
I0523 06:04:29.205287 32758 sgd_solver.cpp:106] Iteration 11100, lr = 0.0025
I0523 06:04:38.486268 32758 solver.cpp:237] Iteration 11400, loss = 1.45526
I0523 06:04:38.486416 32758 solver.cpp:253]     Train net output #0: loss = 1.45526 (* 1 = 1.45526 loss)
I0523 06:04:38.486431 32758 sgd_solver.cpp:106] Iteration 11400, lr = 0.0025
I0523 06:04:47.764981 32758 solver.cpp:237] Iteration 11700, loss = 1.24922
I0523 06:04:47.765017 32758 solver.cpp:253]     Train net output #0: loss = 1.24922 (* 1 = 1.24922 loss)
I0523 06:04:47.765030 32758 sgd_solver.cpp:106] Iteration 11700, lr = 0.0025
I0523 06:04:57.012915 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_12000.caffemodel
I0523 06:04:57.074437 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_12000.solverstate
I0523 06:04:57.101764 32758 solver.cpp:341] Iteration 12000, Testing net (#0)
I0523 06:06:05.785112 32758 solver.cpp:409]     Test net output #0: accuracy = 0.82433
I0523 06:06:05.785269 32758 solver.cpp:409]     Test net output #1: loss = 0.615296 (* 1 = 0.615296 loss)
I0523 06:06:27.991156 32758 solver.cpp:237] Iteration 12000, loss = 1.19118
I0523 06:06:27.991210 32758 solver.cpp:253]     Train net output #0: loss = 1.19118 (* 1 = 1.19118 loss)
I0523 06:06:27.991225 32758 sgd_solver.cpp:106] Iteration 12000, lr = 0.0025
I0523 06:06:37.288074 32758 solver.cpp:237] Iteration 12300, loss = 1.26462
I0523 06:06:37.288250 32758 solver.cpp:253]     Train net output #0: loss = 1.26462 (* 1 = 1.26462 loss)
I0523 06:06:37.288264 32758 sgd_solver.cpp:106] Iteration 12300, lr = 0.0025
I0523 06:06:46.587321 32758 solver.cpp:237] Iteration 12600, loss = 1.64583
I0523 06:06:46.587364 32758 solver.cpp:253]     Train net output #0: loss = 1.64583 (* 1 = 1.64583 loss)
I0523 06:06:46.587379 32758 sgd_solver.cpp:106] Iteration 12600, lr = 0.0025
I0523 06:06:55.889348 32758 solver.cpp:237] Iteration 12900, loss = 1.22078
I0523 06:06:55.889384 32758 solver.cpp:253]     Train net output #0: loss = 1.22078 (* 1 = 1.22078 loss)
I0523 06:06:55.889397 32758 sgd_solver.cpp:106] Iteration 12900, lr = 0.0025
I0523 06:07:05.188385 32758 solver.cpp:237] Iteration 13200, loss = 1.45323
I0523 06:07:05.188419 32758 solver.cpp:253]     Train net output #0: loss = 1.45323 (* 1 = 1.45323 loss)
I0523 06:07:05.188433 32758 sgd_solver.cpp:106] Iteration 13200, lr = 0.0025
I0523 06:07:14.491955 32758 solver.cpp:237] Iteration 13500, loss = 1.42771
I0523 06:07:14.492126 32758 solver.cpp:253]     Train net output #0: loss = 1.42771 (* 1 = 1.42771 loss)
I0523 06:07:14.492141 32758 sgd_solver.cpp:106] Iteration 13500, lr = 0.0025
I0523 06:07:23.792333 32758 solver.cpp:237] Iteration 13800, loss = 1.50041
I0523 06:07:23.792367 32758 solver.cpp:253]     Train net output #0: loss = 1.50041 (* 1 = 1.50041 loss)
I0523 06:07:23.792383 32758 sgd_solver.cpp:106] Iteration 13800, lr = 0.0025
I0523 06:07:55.241677 32758 solver.cpp:237] Iteration 14100, loss = 1.1978
I0523 06:07:55.241835 32758 solver.cpp:253]     Train net output #0: loss = 1.1978 (* 1 = 1.1978 loss)
I0523 06:07:55.241849 32758 sgd_solver.cpp:106] Iteration 14100, lr = 0.0025
I0523 06:08:04.541618 32758 solver.cpp:237] Iteration 14400, loss = 1.17327
I0523 06:08:04.541654 32758 solver.cpp:253]     Train net output #0: loss = 1.17327 (* 1 = 1.17327 loss)
I0523 06:08:04.541666 32758 sgd_solver.cpp:106] Iteration 14400, lr = 0.0025
I0523 06:08:13.840999 32758 solver.cpp:237] Iteration 14700, loss = 1.61168
I0523 06:08:13.841030 32758 solver.cpp:253]     Train net output #0: loss = 1.61168 (* 1 = 1.61168 loss)
I0523 06:08:13.841043 32758 sgd_solver.cpp:106] Iteration 14700, lr = 0.0025
I0523 06:08:23.107681 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_15000.caffemodel
I0523 06:08:23.168367 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_15000.solverstate
I0523 06:08:23.205337 32758 solver.cpp:237] Iteration 15000, loss = 1.2615
I0523 06:08:23.205387 32758 solver.cpp:253]     Train net output #0: loss = 1.2615 (* 1 = 1.2615 loss)
I0523 06:08:23.205401 32758 sgd_solver.cpp:106] Iteration 15000, lr = 0.0025
I0523 06:08:32.506269 32758 solver.cpp:237] Iteration 15300, loss = 1.30221
I0523 06:08:32.506415 32758 solver.cpp:253]     Train net output #0: loss = 1.30221 (* 1 = 1.30221 loss)
I0523 06:08:32.506428 32758 sgd_solver.cpp:106] Iteration 15300, lr = 0.0025
I0523 06:08:41.801190 32758 solver.cpp:237] Iteration 15600, loss = 1.42236
I0523 06:08:41.801221 32758 solver.cpp:253]     Train net output #0: loss = 1.42236 (* 1 = 1.42236 loss)
I0523 06:08:41.801234 32758 sgd_solver.cpp:106] Iteration 15600, lr = 0.0025
I0523 06:08:51.099308 32758 solver.cpp:237] Iteration 15900, loss = 1.47814
I0523 06:08:51.099346 32758 solver.cpp:253]     Train net output #0: loss = 1.47814 (* 1 = 1.47814 loss)
I0523 06:08:51.099359 32758 sgd_solver.cpp:106] Iteration 15900, lr = 0.0025
I0523 06:09:22.607255 32758 solver.cpp:237] Iteration 16200, loss = 1.20335
I0523 06:09:22.607434 32758 solver.cpp:253]     Train net output #0: loss = 1.20335 (* 1 = 1.20335 loss)
I0523 06:09:22.607447 32758 sgd_solver.cpp:106] Iteration 16200, lr = 0.0025
I0523 06:09:31.909046 32758 solver.cpp:237] Iteration 16500, loss = 1.37701
I0523 06:09:31.909081 32758 solver.cpp:253]     Train net output #0: loss = 1.37701 (* 1 = 1.37701 loss)
I0523 06:09:31.909097 32758 sgd_solver.cpp:106] Iteration 16500, lr = 0.0025
I0523 06:09:41.216761 32758 solver.cpp:237] Iteration 16800, loss = 1.11737
I0523 06:09:41.216805 32758 solver.cpp:253]     Train net output #0: loss = 1.11737 (* 1 = 1.11737 loss)
I0523 06:09:41.216820 32758 sgd_solver.cpp:106] Iteration 16800, lr = 0.0025
I0523 06:09:50.520747 32758 solver.cpp:237] Iteration 17100, loss = 1.34243
I0523 06:09:50.520782 32758 solver.cpp:253]     Train net output #0: loss = 1.34243 (* 1 = 1.34243 loss)
I0523 06:09:50.520797 32758 sgd_solver.cpp:106] Iteration 17100, lr = 0.0025
I0523 06:09:59.821147 32758 solver.cpp:237] Iteration 17400, loss = 1.21136
I0523 06:09:59.821286 32758 solver.cpp:253]     Train net output #0: loss = 1.21136 (* 1 = 1.21136 loss)
I0523 06:09:59.821300 32758 sgd_solver.cpp:106] Iteration 17400, lr = 0.0025
I0523 06:10:09.121892 32758 solver.cpp:237] Iteration 17700, loss = 1.01346
I0523 06:10:09.121928 32758 solver.cpp:253]     Train net output #0: loss = 1.01346 (* 1 = 1.01346 loss)
I0523 06:10:09.121949 32758 sgd_solver.cpp:106] Iteration 17700, lr = 0.0025
I0523 06:10:18.387712 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_18000.caffemodel
I0523 06:10:18.447237 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_18000.solverstate
I0523 06:10:18.473325 32758 solver.cpp:341] Iteration 18000, Testing net (#0)
I0523 06:11:05.999513 32758 solver.cpp:409]     Test net output #0: accuracy = 0.848309
I0523 06:11:05.999673 32758 solver.cpp:409]     Test net output #1: loss = 0.498608 (* 1 = 0.498608 loss)
I0523 06:11:28.162868 32758 solver.cpp:237] Iteration 18000, loss = 0.976477
I0523 06:11:28.162921 32758 solver.cpp:253]     Train net output #0: loss = 0.976477 (* 1 = 0.976477 loss)
I0523 06:11:28.162936 32758 sgd_solver.cpp:106] Iteration 18000, lr = 0.0025
I0523 06:11:37.449792 32758 solver.cpp:237] Iteration 18300, loss = 1.34198
I0523 06:11:37.449944 32758 solver.cpp:253]     Train net output #0: loss = 1.34198 (* 1 = 1.34198 loss)
I0523 06:11:37.449956 32758 sgd_solver.cpp:106] Iteration 18300, lr = 0.0025
I0523 06:11:46.738988 32758 solver.cpp:237] Iteration 18600, loss = 1.15388
I0523 06:11:46.739037 32758 solver.cpp:253]     Train net output #0: loss = 1.15388 (* 1 = 1.15388 loss)
I0523 06:11:46.739050 32758 sgd_solver.cpp:106] Iteration 18600, lr = 0.0025
I0523 06:11:56.025027 32758 solver.cpp:237] Iteration 18900, loss = 1.54214
I0523 06:11:56.025061 32758 solver.cpp:253]     Train net output #0: loss = 1.54214 (* 1 = 1.54214 loss)
I0523 06:11:56.025076 32758 sgd_solver.cpp:106] Iteration 18900, lr = 0.0025
I0523 06:12:05.312317 32758 solver.cpp:237] Iteration 19200, loss = 1.50825
I0523 06:12:05.312353 32758 solver.cpp:253]     Train net output #0: loss = 1.50825 (* 1 = 1.50825 loss)
I0523 06:12:05.312368 32758 sgd_solver.cpp:106] Iteration 19200, lr = 0.0025
I0523 06:12:14.600615 32758 solver.cpp:237] Iteration 19500, loss = 1.31239
I0523 06:12:14.600769 32758 solver.cpp:253]     Train net output #0: loss = 1.31239 (* 1 = 1.31239 loss)
I0523 06:12:14.600782 32758 sgd_solver.cpp:106] Iteration 19500, lr = 0.0025
I0523 06:12:23.885912 32758 solver.cpp:237] Iteration 19800, loss = 1.13307
I0523 06:12:23.885946 32758 solver.cpp:253]     Train net output #0: loss = 1.13307 (* 1 = 1.13307 loss)
I0523 06:12:23.885962 32758 sgd_solver.cpp:106] Iteration 19800, lr = 0.0025
I0523 06:12:55.336516 32758 solver.cpp:237] Iteration 20100, loss = 1.25533
I0523 06:12:55.336693 32758 solver.cpp:253]     Train net output #0: loss = 1.25533 (* 1 = 1.25533 loss)
I0523 06:12:55.336707 32758 sgd_solver.cpp:106] Iteration 20100, lr = 0.0025
I0523 06:13:04.625201 32758 solver.cpp:237] Iteration 20400, loss = 1.12639
I0523 06:13:04.625248 32758 solver.cpp:253]     Train net output #0: loss = 1.12639 (* 1 = 1.12639 loss)
I0523 06:13:04.625264 32758 sgd_solver.cpp:106] Iteration 20400, lr = 0.0025
I0523 06:13:13.914784 32758 solver.cpp:237] Iteration 20700, loss = 1.13724
I0523 06:13:13.914820 32758 solver.cpp:253]     Train net output #0: loss = 1.13724 (* 1 = 1.13724 loss)
I0523 06:13:13.914835 32758 sgd_solver.cpp:106] Iteration 20700, lr = 0.0025
I0523 06:13:23.168203 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_21000.caffemodel
I0523 06:13:23.227279 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_21000.solverstate
I0523 06:13:23.263141 32758 solver.cpp:237] Iteration 21000, loss = 1.43729
I0523 06:13:23.263186 32758 solver.cpp:253]     Train net output #0: loss = 1.43729 (* 1 = 1.43729 loss)
I0523 06:13:23.263202 32758 sgd_solver.cpp:106] Iteration 21000, lr = 0.0025
I0523 06:13:32.549876 32758 solver.cpp:237] Iteration 21300, loss = 1.28952
I0523 06:13:32.550022 32758 solver.cpp:253]     Train net output #0: loss = 1.28952 (* 1 = 1.28952 loss)
I0523 06:13:32.550036 32758 sgd_solver.cpp:106] Iteration 21300, lr = 0.0025
I0523 06:13:41.838130 32758 solver.cpp:237] Iteration 21600, loss = 1.31851
I0523 06:13:41.838165 32758 solver.cpp:253]     Train net output #0: loss = 1.31851 (* 1 = 1.31851 loss)
I0523 06:13:41.838181 32758 sgd_solver.cpp:106] Iteration 21600, lr = 0.0025
I0523 06:13:51.123834 32758 solver.cpp:237] Iteration 21900, loss = 1.0459
I0523 06:13:51.123883 32758 solver.cpp:253]     Train net output #0: loss = 1.0459 (* 1 = 1.0459 loss)
I0523 06:13:51.123895 32758 sgd_solver.cpp:106] Iteration 21900, lr = 0.0025
I0523 06:14:22.620808 32758 solver.cpp:237] Iteration 22200, loss = 0.822027
I0523 06:14:22.620977 32758 solver.cpp:253]     Train net output #0: loss = 0.822027 (* 1 = 0.822027 loss)
I0523 06:14:22.620992 32758 sgd_solver.cpp:106] Iteration 22200, lr = 0.0025
I0523 06:14:31.911422 32758 solver.cpp:237] Iteration 22500, loss = 1.08288
I0523 06:14:31.911456 32758 solver.cpp:253]     Train net output #0: loss = 1.08288 (* 1 = 1.08288 loss)
I0523 06:14:31.911473 32758 sgd_solver.cpp:106] Iteration 22500, lr = 0.0025
I0523 06:14:41.200932 32758 solver.cpp:237] Iteration 22800, loss = 1.32565
I0523 06:14:41.200978 32758 solver.cpp:253]     Train net output #0: loss = 1.32565 (* 1 = 1.32565 loss)
I0523 06:14:41.200994 32758 sgd_solver.cpp:106] Iteration 22800, lr = 0.0025
I0523 06:14:50.485518 32758 solver.cpp:237] Iteration 23100, loss = 1.4697
I0523 06:14:50.485554 32758 solver.cpp:253]     Train net output #0: loss = 1.4697 (* 1 = 1.4697 loss)
I0523 06:14:50.485569 32758 sgd_solver.cpp:106] Iteration 23100, lr = 0.0025
I0523 06:14:59.773622 32758 solver.cpp:237] Iteration 23400, loss = 1.23535
I0523 06:14:59.773778 32758 solver.cpp:253]     Train net output #0: loss = 1.23535 (* 1 = 1.23535 loss)
I0523 06:14:59.773792 32758 sgd_solver.cpp:106] Iteration 23400, lr = 0.0025
I0523 06:15:09.062443 32758 solver.cpp:237] Iteration 23700, loss = 0.972789
I0523 06:15:09.062489 32758 solver.cpp:253]     Train net output #0: loss = 0.97279 (* 1 = 0.97279 loss)
I0523 06:15:09.062502 32758 sgd_solver.cpp:106] Iteration 23700, lr = 0.0025
I0523 06:15:18.315542 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_24000.caffemodel
I0523 06:15:18.374502 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_24000.solverstate
I0523 06:15:18.400563 32758 solver.cpp:341] Iteration 24000, Testing net (#0)
I0523 06:16:27.097337 32758 solver.cpp:409]     Test net output #0: accuracy = 0.859801
I0523 06:16:27.097508 32758 solver.cpp:409]     Test net output #1: loss = 0.468698 (* 1 = 0.468698 loss)
I0523 06:16:49.307605 32758 solver.cpp:237] Iteration 24000, loss = 1.13835
I0523 06:16:49.307657 32758 solver.cpp:253]     Train net output #0: loss = 1.13835 (* 1 = 1.13835 loss)
I0523 06:16:49.307672 32758 sgd_solver.cpp:106] Iteration 24000, lr = 0.0025
I0523 06:16:58.597975 32758 solver.cpp:237] Iteration 24300, loss = 1.23103
I0523 06:16:58.598131 32758 solver.cpp:253]     Train net output #0: loss = 1.23103 (* 1 = 1.23103 loss)
I0523 06:16:58.598145 32758 sgd_solver.cpp:106] Iteration 24300, lr = 0.0025
I0523 06:17:07.887789 32758 solver.cpp:237] Iteration 24600, loss = 1.39913
I0523 06:17:07.887835 32758 solver.cpp:253]     Train net output #0: loss = 1.39913 (* 1 = 1.39913 loss)
I0523 06:17:07.887850 32758 sgd_solver.cpp:106] Iteration 24600, lr = 0.0025
I0523 06:17:17.176025 32758 solver.cpp:237] Iteration 24900, loss = 1.08264
I0523 06:17:17.176059 32758 solver.cpp:253]     Train net output #0: loss = 1.08264 (* 1 = 1.08264 loss)
I0523 06:17:17.176074 32758 sgd_solver.cpp:106] Iteration 24900, lr = 0.0025
I0523 06:17:26.469595 32758 solver.cpp:237] Iteration 25200, loss = 1.28374
I0523 06:17:26.469631 32758 solver.cpp:253]     Train net output #0: loss = 1.28374 (* 1 = 1.28374 loss)
I0523 06:17:26.469645 32758 sgd_solver.cpp:106] Iteration 25200, lr = 0.0025
I0523 06:17:35.757216 32758 solver.cpp:237] Iteration 25500, loss = 1.00379
I0523 06:17:35.757375 32758 solver.cpp:253]     Train net output #0: loss = 1.00379 (* 1 = 1.00379 loss)
I0523 06:17:35.757390 32758 sgd_solver.cpp:106] Iteration 25500, lr = 0.0025
I0523 06:17:45.048151 32758 solver.cpp:237] Iteration 25800, loss = 1.26988
I0523 06:17:45.048187 32758 solver.cpp:253]     Train net output #0: loss = 1.26988 (* 1 = 1.26988 loss)
I0523 06:17:45.048200 32758 sgd_solver.cpp:106] Iteration 25800, lr = 0.0025
I0523 06:18:16.500578 32758 solver.cpp:237] Iteration 26100, loss = 1.37909
I0523 06:18:16.500751 32758 solver.cpp:253]     Train net output #0: loss = 1.37909 (* 1 = 1.37909 loss)
I0523 06:18:16.500766 32758 sgd_solver.cpp:106] Iteration 26100, lr = 0.0025
I0523 06:18:25.790474 32758 solver.cpp:237] Iteration 26400, loss = 1.1314
I0523 06:18:25.790519 32758 solver.cpp:253]     Train net output #0: loss = 1.1314 (* 1 = 1.1314 loss)
I0523 06:18:25.790534 32758 sgd_solver.cpp:106] Iteration 26400, lr = 0.0025
I0523 06:18:35.082317 32758 solver.cpp:237] Iteration 26700, loss = 1.19726
I0523 06:18:35.082352 32758 solver.cpp:253]     Train net output #0: loss = 1.19726 (* 1 = 1.19726 loss)
I0523 06:18:35.082366 32758 sgd_solver.cpp:106] Iteration 26700, lr = 0.0025
I0523 06:18:44.342483 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_27000.caffemodel
I0523 06:18:44.408721 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_27000.solverstate
I0523 06:18:44.446610 32758 solver.cpp:237] Iteration 27000, loss = 1.47045
I0523 06:18:44.446660 32758 solver.cpp:253]     Train net output #0: loss = 1.47045 (* 1 = 1.47045 loss)
I0523 06:18:44.446674 32758 sgd_solver.cpp:106] Iteration 27000, lr = 0.0025
I0523 06:18:53.739428 32758 solver.cpp:237] Iteration 27300, loss = 1.31593
I0523 06:18:53.739588 32758 solver.cpp:253]     Train net output #0: loss = 1.31593 (* 1 = 1.31593 loss)
I0523 06:18:53.739603 32758 sgd_solver.cpp:106] Iteration 27300, lr = 0.0025
I0523 06:19:03.030589 32758 solver.cpp:237] Iteration 27600, loss = 1.22224
I0523 06:19:03.030623 32758 solver.cpp:253]     Train net output #0: loss = 1.22224 (* 1 = 1.22224 loss)
I0523 06:19:03.030639 32758 sgd_solver.cpp:106] Iteration 27600, lr = 0.0025
I0523 06:19:12.317507 32758 solver.cpp:237] Iteration 27900, loss = 0.89128
I0523 06:19:12.317558 32758 solver.cpp:253]     Train net output #0: loss = 0.89128 (* 1 = 0.89128 loss)
I0523 06:19:12.317574 32758 sgd_solver.cpp:106] Iteration 27900, lr = 0.0025
I0523 06:19:43.744176 32758 solver.cpp:237] Iteration 28200, loss = 1.17316
I0523 06:19:43.744359 32758 solver.cpp:253]     Train net output #0: loss = 1.17316 (* 1 = 1.17316 loss)
I0523 06:19:43.744374 32758 sgd_solver.cpp:106] Iteration 28200, lr = 0.0025
I0523 06:19:53.040078 32758 solver.cpp:237] Iteration 28500, loss = 1.28644
I0523 06:19:53.040113 32758 solver.cpp:253]     Train net output #0: loss = 1.28644 (* 1 = 1.28644 loss)
I0523 06:19:53.040128 32758 sgd_solver.cpp:106] Iteration 28500, lr = 0.0025
I0523 06:20:02.331192 32758 solver.cpp:237] Iteration 28800, loss = 1.20981
I0523 06:20:02.331236 32758 solver.cpp:253]     Train net output #0: loss = 1.20981 (* 1 = 1.20981 loss)
I0523 06:20:02.331254 32758 sgd_solver.cpp:106] Iteration 28800, lr = 0.0025
I0523 06:20:11.622962 32758 solver.cpp:237] Iteration 29100, loss = 1.33694
I0523 06:20:11.622998 32758 solver.cpp:253]     Train net output #0: loss = 1.33694 (* 1 = 1.33694 loss)
I0523 06:20:11.623013 32758 sgd_solver.cpp:106] Iteration 29100, lr = 0.0025
I0523 06:20:20.918201 32758 solver.cpp:237] Iteration 29400, loss = 1.24685
I0523 06:20:20.918349 32758 solver.cpp:253]     Train net output #0: loss = 1.24685 (* 1 = 1.24685 loss)
I0523 06:20:20.918364 32758 sgd_solver.cpp:106] Iteration 29400, lr = 0.0025
I0523 06:20:30.207846 32758 solver.cpp:237] Iteration 29700, loss = 1.19083
I0523 06:20:30.207885 32758 solver.cpp:253]     Train net output #0: loss = 1.19083 (* 1 = 1.19083 loss)
I0523 06:20:30.207898 32758 sgd_solver.cpp:106] Iteration 29700, lr = 0.0025
I0523 06:20:39.470422 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_30000.caffemodel
I0523 06:20:39.532022 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_30000.solverstate
I0523 06:20:39.560463 32758 solver.cpp:341] Iteration 30000, Testing net (#0)
I0523 06:21:27.372241 32758 solver.cpp:409]     Test net output #0: accuracy = 0.868064
I0523 06:21:27.372414 32758 solver.cpp:409]     Test net output #1: loss = 0.439301 (* 1 = 0.439301 loss)
I0523 06:21:48.247376 32758 solver.cpp:237] Iteration 30000, loss = 1.12203
I0523 06:21:48.247429 32758 solver.cpp:253]     Train net output #0: loss = 1.12203 (* 1 = 1.12203 loss)
I0523 06:21:48.247444 32758 sgd_solver.cpp:106] Iteration 30000, lr = 0.0025
I0523 06:21:57.529709 32758 solver.cpp:237] Iteration 30300, loss = 1.18627
I0523 06:21:57.529860 32758 solver.cpp:253]     Train net output #0: loss = 1.18627 (* 1 = 1.18627 loss)
I0523 06:21:57.529872 32758 sgd_solver.cpp:106] Iteration 30300, lr = 0.0025
I0523 06:22:06.809940 32758 solver.cpp:237] Iteration 30600, loss = 1.42777
I0523 06:22:06.809975 32758 solver.cpp:253]     Train net output #0: loss = 1.42777 (* 1 = 1.42777 loss)
I0523 06:22:06.809990 32758 sgd_solver.cpp:106] Iteration 30600, lr = 0.0025
I0523 06:22:16.093437 32758 solver.cpp:237] Iteration 30900, loss = 1.27079
I0523 06:22:16.093485 32758 solver.cpp:253]     Train net output #0: loss = 1.27079 (* 1 = 1.27079 loss)
I0523 06:22:16.093498 32758 sgd_solver.cpp:106] Iteration 30900, lr = 0.0025
I0523 06:22:25.378728 32758 solver.cpp:237] Iteration 31200, loss = 1.19937
I0523 06:22:25.378763 32758 solver.cpp:253]     Train net output #0: loss = 1.19937 (* 1 = 1.19937 loss)
I0523 06:22:25.378777 32758 sgd_solver.cpp:106] Iteration 31200, lr = 0.0025
I0523 06:22:34.662132 32758 solver.cpp:237] Iteration 31500, loss = 1.23683
I0523 06:22:34.662293 32758 solver.cpp:253]     Train net output #0: loss = 1.23683 (* 1 = 1.23683 loss)
I0523 06:22:34.662307 32758 sgd_solver.cpp:106] Iteration 31500, lr = 0.0025
I0523 06:22:43.944962 32758 solver.cpp:237] Iteration 31800, loss = 1.39828
I0523 06:22:43.944998 32758 solver.cpp:253]     Train net output #0: loss = 1.39828 (* 1 = 1.39828 loss)
I0523 06:22:43.945013 32758 sgd_solver.cpp:106] Iteration 31800, lr = 0.0025
I0523 06:23:14.105084 32758 solver.cpp:237] Iteration 32100, loss = 1.25672
I0523 06:23:14.105264 32758 solver.cpp:253]     Train net output #0: loss = 1.25672 (* 1 = 1.25672 loss)
I0523 06:23:14.105280 32758 sgd_solver.cpp:106] Iteration 32100, lr = 0.0025
I0523 06:23:23.388667 32758 solver.cpp:237] Iteration 32400, loss = 1.22119
I0523 06:23:23.388712 32758 solver.cpp:253]     Train net output #0: loss = 1.22119 (* 1 = 1.22119 loss)
I0523 06:23:23.388728 32758 sgd_solver.cpp:106] Iteration 32400, lr = 0.0025
I0523 06:23:32.670610 32758 solver.cpp:237] Iteration 32700, loss = 1.5181
I0523 06:23:32.670646 32758 solver.cpp:253]     Train net output #0: loss = 1.5181 (* 1 = 1.5181 loss)
I0523 06:23:32.670660 32758 sgd_solver.cpp:106] Iteration 32700, lr = 0.0025
I0523 06:23:41.921092 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_33000.caffemodel
I0523 06:23:41.980103 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_33000.solverstate
I0523 06:23:42.015874 32758 solver.cpp:237] Iteration 33000, loss = 1.06066
I0523 06:23:42.015918 32758 solver.cpp:253]     Train net output #0: loss = 1.06066 (* 1 = 1.06066 loss)
I0523 06:23:42.015935 32758 sgd_solver.cpp:106] Iteration 33000, lr = 0.0025
I0523 06:23:51.298341 32758 solver.cpp:237] Iteration 33300, loss = 1.02878
I0523 06:23:51.298519 32758 solver.cpp:253]     Train net output #0: loss = 1.02878 (* 1 = 1.02878 loss)
I0523 06:23:51.298533 32758 sgd_solver.cpp:106] Iteration 33300, lr = 0.0025
I0523 06:24:00.580395 32758 solver.cpp:237] Iteration 33600, loss = 1.21069
I0523 06:24:00.580430 32758 solver.cpp:253]     Train net output #0: loss = 1.21069 (* 1 = 1.21069 loss)
I0523 06:24:00.580446 32758 sgd_solver.cpp:106] Iteration 33600, lr = 0.0025
I0523 06:24:09.862535 32758 solver.cpp:237] Iteration 33900, loss = 1.18204
I0523 06:24:09.862571 32758 solver.cpp:253]     Train net output #0: loss = 1.18204 (* 1 = 1.18204 loss)
I0523 06:24:09.862584 32758 sgd_solver.cpp:106] Iteration 33900, lr = 0.0025
I0523 06:24:39.995538 32758 solver.cpp:237] Iteration 34200, loss = 1.15105
I0523 06:24:39.995708 32758 solver.cpp:253]     Train net output #0: loss = 1.15105 (* 1 = 1.15105 loss)
I0523 06:24:39.995724 32758 sgd_solver.cpp:106] Iteration 34200, lr = 0.0025
I0523 06:24:49.280637 32758 solver.cpp:237] Iteration 34500, loss = 0.990306
I0523 06:24:49.280673 32758 solver.cpp:253]     Train net output #0: loss = 0.990306 (* 1 = 0.990306 loss)
I0523 06:24:49.280688 32758 sgd_solver.cpp:106] Iteration 34500, lr = 0.0025
I0523 06:24:58.565588 32758 solver.cpp:237] Iteration 34800, loss = 1.19547
I0523 06:24:58.565623 32758 solver.cpp:253]     Train net output #0: loss = 1.19547 (* 1 = 1.19547 loss)
I0523 06:24:58.565639 32758 sgd_solver.cpp:106] Iteration 34800, lr = 0.0025
I0523 06:25:07.848253 32758 solver.cpp:237] Iteration 35100, loss = 1.542
I0523 06:25:07.848296 32758 solver.cpp:253]     Train net output #0: loss = 1.542 (* 1 = 1.542 loss)
I0523 06:25:07.848311 32758 sgd_solver.cpp:106] Iteration 35100, lr = 0.0025
I0523 06:25:17.128815 32758 solver.cpp:237] Iteration 35400, loss = 1.2806
I0523 06:25:17.128962 32758 solver.cpp:253]     Train net output #0: loss = 1.2806 (* 1 = 1.2806 loss)
I0523 06:25:17.128976 32758 sgd_solver.cpp:106] Iteration 35400, lr = 0.0025
I0523 06:25:26.412364 32758 solver.cpp:237] Iteration 35700, loss = 1.48908
I0523 06:25:26.412397 32758 solver.cpp:253]     Train net output #0: loss = 1.48908 (* 1 = 1.48908 loss)
I0523 06:25:26.412413 32758 sgd_solver.cpp:106] Iteration 35700, lr = 0.0025
I0523 06:25:35.665715 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_36000.caffemodel
I0523 06:25:35.725394 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_36000.solverstate
I0523 06:25:35.751854 32758 solver.cpp:341] Iteration 36000, Testing net (#0)
I0523 06:26:44.368386 32758 solver.cpp:409]     Test net output #0: accuracy = 0.866804
I0523 06:26:44.368561 32758 solver.cpp:409]     Test net output #1: loss = 0.415483 (* 1 = 0.415483 loss)
I0523 06:27:05.232161 32758 solver.cpp:237] Iteration 36000, loss = 1.39421
I0523 06:27:05.232213 32758 solver.cpp:253]     Train net output #0: loss = 1.39421 (* 1 = 1.39421 loss)
I0523 06:27:05.232228 32758 sgd_solver.cpp:106] Iteration 36000, lr = 0.0025
I0523 06:27:14.518496 32758 solver.cpp:237] Iteration 36300, loss = 1.05246
I0523 06:27:14.518662 32758 solver.cpp:253]     Train net output #0: loss = 1.05246 (* 1 = 1.05246 loss)
I0523 06:27:14.518676 32758 sgd_solver.cpp:106] Iteration 36300, lr = 0.0025
I0523 06:27:23.807011 32758 solver.cpp:237] Iteration 36600, loss = 1.16981
I0523 06:27:23.807045 32758 solver.cpp:253]     Train net output #0: loss = 1.16981 (* 1 = 1.16981 loss)
I0523 06:27:23.807060 32758 sgd_solver.cpp:106] Iteration 36600, lr = 0.0025
I0523 06:27:33.096097 32758 solver.cpp:237] Iteration 36900, loss = 1.19847
I0523 06:27:33.096146 32758 solver.cpp:253]     Train net output #0: loss = 1.19847 (* 1 = 1.19847 loss)
I0523 06:27:33.096159 32758 sgd_solver.cpp:106] Iteration 36900, lr = 0.0025
I0523 06:27:42.388206 32758 solver.cpp:237] Iteration 37200, loss = 1.3043
I0523 06:27:42.388241 32758 solver.cpp:253]     Train net output #0: loss = 1.3043 (* 1 = 1.3043 loss)
I0523 06:27:42.388257 32758 sgd_solver.cpp:106] Iteration 37200, lr = 0.0025
I0523 06:27:51.678917 32758 solver.cpp:237] Iteration 37500, loss = 1.16841
I0523 06:27:51.679062 32758 solver.cpp:253]     Train net output #0: loss = 1.16841 (* 1 = 1.16841 loss)
I0523 06:27:51.679076 32758 sgd_solver.cpp:106] Iteration 37500, lr = 0.0025
I0523 06:28:00.967236 32758 solver.cpp:237] Iteration 37800, loss = 1.21507
I0523 06:28:00.967283 32758 solver.cpp:253]     Train net output #0: loss = 1.21507 (* 1 = 1.21507 loss)
I0523 06:28:00.967298 32758 sgd_solver.cpp:106] Iteration 37800, lr = 0.0025
I0523 06:28:31.112313 32758 solver.cpp:237] Iteration 38100, loss = 1.07726
I0523 06:28:31.112484 32758 solver.cpp:253]     Train net output #0: loss = 1.07726 (* 1 = 1.07726 loss)
I0523 06:28:31.112500 32758 sgd_solver.cpp:106] Iteration 38100, lr = 0.0025
I0523 06:28:40.400707 32758 solver.cpp:237] Iteration 38400, loss = 1.20522
I0523 06:28:40.400741 32758 solver.cpp:253]     Train net output #0: loss = 1.20522 (* 1 = 1.20522 loss)
I0523 06:28:40.400758 32758 sgd_solver.cpp:106] Iteration 38400, lr = 0.0025
I0523 06:28:49.694594 32758 solver.cpp:237] Iteration 38700, loss = 1.34602
I0523 06:28:49.694640 32758 solver.cpp:253]     Train net output #0: loss = 1.34602 (* 1 = 1.34602 loss)
I0523 06:28:49.694654 32758 sgd_solver.cpp:106] Iteration 38700, lr = 0.0025
I0523 06:28:58.958564 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_39000.caffemodel
I0523 06:28:59.018309 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_39000.solverstate
I0523 06:28:59.057299 32758 solver.cpp:237] Iteration 39000, loss = 1.23729
I0523 06:28:59.057349 32758 solver.cpp:253]     Train net output #0: loss = 1.23729 (* 1 = 1.23729 loss)
I0523 06:28:59.057364 32758 sgd_solver.cpp:106] Iteration 39000, lr = 0.0025
I0523 06:29:08.347482 32758 solver.cpp:237] Iteration 39300, loss = 1.22195
I0523 06:29:08.347645 32758 solver.cpp:253]     Train net output #0: loss = 1.22195 (* 1 = 1.22195 loss)
I0523 06:29:08.347659 32758 sgd_solver.cpp:106] Iteration 39300, lr = 0.0025
I0523 06:29:17.637621 32758 solver.cpp:237] Iteration 39600, loss = 1.51226
I0523 06:29:17.637668 32758 solver.cpp:253]     Train net output #0: loss = 1.51226 (* 1 = 1.51226 loss)
I0523 06:29:17.637683 32758 sgd_solver.cpp:106] Iteration 39600, lr = 0.0025
I0523 06:29:26.927312 32758 solver.cpp:237] Iteration 39900, loss = 1.42635
I0523 06:29:26.927343 32758 solver.cpp:253]     Train net output #0: loss = 1.42635 (* 1 = 1.42635 loss)
I0523 06:29:26.927356 32758 sgd_solver.cpp:106] Iteration 39900, lr = 0.0025
I0523 06:29:57.100008 32758 solver.cpp:237] Iteration 40200, loss = 1.34193
I0523 06:29:57.100179 32758 solver.cpp:253]     Train net output #0: loss = 1.34193 (* 1 = 1.34193 loss)
I0523 06:29:57.100195 32758 sgd_solver.cpp:106] Iteration 40200, lr = 0.0025
I0523 06:30:06.392992 32758 solver.cpp:237] Iteration 40500, loss = 1.07479
I0523 06:30:06.393036 32758 solver.cpp:253]     Train net output #0: loss = 1.07479 (* 1 = 1.07479 loss)
I0523 06:30:06.393052 32758 sgd_solver.cpp:106] Iteration 40500, lr = 0.0025
I0523 06:30:15.680809 32758 solver.cpp:237] Iteration 40800, loss = 1.25081
I0523 06:30:15.680845 32758 solver.cpp:253]     Train net output #0: loss = 1.25081 (* 1 = 1.25081 loss)
I0523 06:30:15.680858 32758 sgd_solver.cpp:106] Iteration 40800, lr = 0.0025
I0523 06:30:24.969992 32758 solver.cpp:237] Iteration 41100, loss = 0.971142
I0523 06:30:24.970028 32758 solver.cpp:253]     Train net output #0: loss = 0.971142 (* 1 = 0.971142 loss)
I0523 06:30:24.970042 32758 sgd_solver.cpp:106] Iteration 41100, lr = 0.0025
I0523 06:30:34.259970 32758 solver.cpp:237] Iteration 41400, loss = 1.39807
I0523 06:30:34.260145 32758 solver.cpp:253]     Train net output #0: loss = 1.39807 (* 1 = 1.39807 loss)
I0523 06:30:34.260159 32758 sgd_solver.cpp:106] Iteration 41400, lr = 0.0025
I0523 06:30:43.549012 32758 solver.cpp:237] Iteration 41700, loss = 1.079
I0523 06:30:43.549046 32758 solver.cpp:253]     Train net output #0: loss = 1.079 (* 1 = 1.079 loss)
I0523 06:30:43.549062 32758 sgd_solver.cpp:106] Iteration 41700, lr = 0.0025
I0523 06:30:52.811000 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_42000.caffemodel
I0523 06:30:52.871495 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_42000.solverstate
I0523 06:30:52.899359 32758 solver.cpp:341] Iteration 42000, Testing net (#0)
I0523 06:31:40.374600 32758 solver.cpp:409]     Test net output #0: accuracy = 0.872976
I0523 06:31:40.374765 32758 solver.cpp:409]     Test net output #1: loss = 0.425375 (* 1 = 0.425375 loss)
I0523 06:32:01.257414 32758 solver.cpp:237] Iteration 42000, loss = 1.10885
I0523 06:32:01.257468 32758 solver.cpp:253]     Train net output #0: loss = 1.10885 (* 1 = 1.10885 loss)
I0523 06:32:01.257484 32758 sgd_solver.cpp:106] Iteration 42000, lr = 0.0025
I0523 06:32:10.543054 32758 solver.cpp:237] Iteration 42300, loss = 1.24669
I0523 06:32:10.543222 32758 solver.cpp:253]     Train net output #0: loss = 1.24669 (* 1 = 1.24669 loss)
I0523 06:32:10.543236 32758 sgd_solver.cpp:106] Iteration 42300, lr = 0.0025
I0523 06:32:19.827989 32758 solver.cpp:237] Iteration 42600, loss = 1.42573
I0523 06:32:19.828024 32758 solver.cpp:253]     Train net output #0: loss = 1.42573 (* 1 = 1.42573 loss)
I0523 06:32:19.828039 32758 sgd_solver.cpp:106] Iteration 42600, lr = 0.0025
I0523 06:32:29.113502 32758 solver.cpp:237] Iteration 42900, loss = 1.12728
I0523 06:32:29.113538 32758 solver.cpp:253]     Train net output #0: loss = 1.12728 (* 1 = 1.12728 loss)
I0523 06:32:29.113553 32758 sgd_solver.cpp:106] Iteration 42900, lr = 0.0025
I0523 06:32:38.399459 32758 solver.cpp:237] Iteration 43200, loss = 1.25906
I0523 06:32:38.399504 32758 solver.cpp:253]     Train net output #0: loss = 1.25906 (* 1 = 1.25906 loss)
I0523 06:32:38.399519 32758 sgd_solver.cpp:106] Iteration 43200, lr = 0.0025
I0523 06:32:47.685652 32758 solver.cpp:237] Iteration 43500, loss = 1.17154
I0523 06:32:47.685812 32758 solver.cpp:253]     Train net output #0: loss = 1.17154 (* 1 = 1.17154 loss)
I0523 06:32:47.685825 32758 sgd_solver.cpp:106] Iteration 43500, lr = 0.0025
I0523 06:32:56.972005 32758 solver.cpp:237] Iteration 43800, loss = 1.24606
I0523 06:32:56.972039 32758 solver.cpp:253]     Train net output #0: loss = 1.24606 (* 1 = 1.24606 loss)
I0523 06:32:56.972054 32758 sgd_solver.cpp:106] Iteration 43800, lr = 0.0025
I0523 06:33:27.172893 32758 solver.cpp:237] Iteration 44100, loss = 1.2035
I0523 06:33:27.173074 32758 solver.cpp:253]     Train net output #0: loss = 1.2035 (* 1 = 1.2035 loss)
I0523 06:33:27.173087 32758 sgd_solver.cpp:106] Iteration 44100, lr = 0.0025
I0523 06:33:36.457906 32758 solver.cpp:237] Iteration 44400, loss = 1.01368
I0523 06:33:36.457940 32758 solver.cpp:253]     Train net output #0: loss = 1.01368 (* 1 = 1.01368 loss)
I0523 06:33:36.457957 32758 sgd_solver.cpp:106] Iteration 44400, lr = 0.0025
I0523 06:33:45.743408 32758 solver.cpp:237] Iteration 44700, loss = 1.2822
I0523 06:33:45.743444 32758 solver.cpp:253]     Train net output #0: loss = 1.2822 (* 1 = 1.2822 loss)
I0523 06:33:45.743459 32758 sgd_solver.cpp:106] Iteration 44700, lr = 0.0025
I0523 06:33:55.001889 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_45000.caffemodel
I0523 06:33:55.063580 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_45000.solverstate
I0523 06:33:55.101661 32758 solver.cpp:237] Iteration 45000, loss = 1.1723
I0523 06:33:55.101711 32758 solver.cpp:253]     Train net output #0: loss = 1.1723 (* 1 = 1.1723 loss)
I0523 06:33:55.101727 32758 sgd_solver.cpp:106] Iteration 45000, lr = 0.0025
I0523 06:34:04.390954 32758 solver.cpp:237] Iteration 45300, loss = 1.1117
I0523 06:34:04.391119 32758 solver.cpp:253]     Train net output #0: loss = 1.1117 (* 1 = 1.1117 loss)
I0523 06:34:04.391132 32758 sgd_solver.cpp:106] Iteration 45300, lr = 0.0025
I0523 06:34:13.679782 32758 solver.cpp:237] Iteration 45600, loss = 1.52898
I0523 06:34:13.679832 32758 solver.cpp:253]     Train net output #0: loss = 1.52898 (* 1 = 1.52898 loss)
I0523 06:34:13.679847 32758 sgd_solver.cpp:106] Iteration 45600, lr = 0.0025
I0523 06:34:22.964613 32758 solver.cpp:237] Iteration 45900, loss = 1.27799
I0523 06:34:22.964649 32758 solver.cpp:253]     Train net output #0: loss = 1.27799 (* 1 = 1.27799 loss)
I0523 06:34:22.964663 32758 sgd_solver.cpp:106] Iteration 45900, lr = 0.0025
I0523 06:34:53.128659 32758 solver.cpp:237] Iteration 46200, loss = 1.23582
I0523 06:34:53.128834 32758 solver.cpp:253]     Train net output #0: loss = 1.23582 (* 1 = 1.23582 loss)
I0523 06:34:53.128849 32758 sgd_solver.cpp:106] Iteration 46200, lr = 0.0025
I0523 06:35:02.417671 32758 solver.cpp:237] Iteration 46500, loss = 1.22175
I0523 06:35:02.417719 32758 solver.cpp:253]     Train net output #0: loss = 1.22175 (* 1 = 1.22175 loss)
I0523 06:35:02.417734 32758 sgd_solver.cpp:106] Iteration 46500, lr = 0.0025
I0523 06:35:11.704186 32758 solver.cpp:237] Iteration 46800, loss = 1.1365
I0523 06:35:11.704222 32758 solver.cpp:253]     Train net output #0: loss = 1.1365 (* 1 = 1.1365 loss)
I0523 06:35:11.704236 32758 sgd_solver.cpp:106] Iteration 46800, lr = 0.0025
I0523 06:35:20.989797 32758 solver.cpp:237] Iteration 47100, loss = 1.21088
I0523 06:35:20.989832 32758 solver.cpp:253]     Train net output #0: loss = 1.21088 (* 1 = 1.21088 loss)
I0523 06:35:20.989847 32758 sgd_solver.cpp:106] Iteration 47100, lr = 0.0025
I0523 06:35:30.275205 32758 solver.cpp:237] Iteration 47400, loss = 1.18598
I0523 06:35:30.275388 32758 solver.cpp:253]     Train net output #0: loss = 1.18598 (* 1 = 1.18598 loss)
I0523 06:35:30.275403 32758 sgd_solver.cpp:106] Iteration 47400, lr = 0.0025
I0523 06:35:39.561813 32758 solver.cpp:237] Iteration 47700, loss = 0.937748
I0523 06:35:39.561848 32758 solver.cpp:253]     Train net output #0: loss = 0.937748 (* 1 = 0.937748 loss)
I0523 06:35:39.561864 32758 sgd_solver.cpp:106] Iteration 47700, lr = 0.0025
I0523 06:35:48.816886 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_48000.caffemodel
I0523 06:35:48.875969 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_48000.solverstate
I0523 06:35:48.902030 32758 solver.cpp:341] Iteration 48000, Testing net (#0)
I0523 06:36:57.521240 32758 solver.cpp:409]     Test net output #0: accuracy = 0.876183
I0523 06:36:57.521412 32758 solver.cpp:409]     Test net output #1: loss = 0.395451 (* 1 = 0.395451 loss)
I0523 06:37:18.385293 32758 solver.cpp:237] Iteration 48000, loss = 1.077
I0523 06:37:18.385344 32758 solver.cpp:253]     Train net output #0: loss = 1.077 (* 1 = 1.077 loss)
I0523 06:37:18.385360 32758 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0523 06:37:27.674443 32758 solver.cpp:237] Iteration 48300, loss = 0.988065
I0523 06:37:27.674602 32758 solver.cpp:253]     Train net output #0: loss = 0.988066 (* 1 = 0.988066 loss)
I0523 06:37:27.674615 32758 sgd_solver.cpp:106] Iteration 48300, lr = 0.0025
I0523 06:37:36.961983 32758 solver.cpp:237] Iteration 48600, loss = 1.40934
I0523 06:37:36.962030 32758 solver.cpp:253]     Train net output #0: loss = 1.40934 (* 1 = 1.40934 loss)
I0523 06:37:36.962045 32758 sgd_solver.cpp:106] Iteration 48600, lr = 0.0025
I0523 06:37:46.247977 32758 solver.cpp:237] Iteration 48900, loss = 1.09755
I0523 06:37:46.248013 32758 solver.cpp:253]     Train net output #0: loss = 1.09755 (* 1 = 1.09755 loss)
I0523 06:37:46.248028 32758 sgd_solver.cpp:106] Iteration 48900, lr = 0.0025
I0523 06:37:55.539348 32758 solver.cpp:237] Iteration 49200, loss = 1.6541
I0523 06:37:55.539383 32758 solver.cpp:253]     Train net output #0: loss = 1.6541 (* 1 = 1.6541 loss)
I0523 06:37:55.539398 32758 sgd_solver.cpp:106] Iteration 49200, lr = 0.0025
I0523 06:38:04.828084 32758 solver.cpp:237] Iteration 49500, loss = 1.27082
I0523 06:38:04.828248 32758 solver.cpp:253]     Train net output #0: loss = 1.27083 (* 1 = 1.27083 loss)
I0523 06:38:04.828263 32758 sgd_solver.cpp:106] Iteration 49500, lr = 0.0025
I0523 06:38:14.121323 32758 solver.cpp:237] Iteration 49800, loss = 1.19118
I0523 06:38:14.121357 32758 solver.cpp:253]     Train net output #0: loss = 1.19118 (* 1 = 1.19118 loss)
I0523 06:38:14.121373 32758 sgd_solver.cpp:106] Iteration 49800, lr = 0.0025
I0523 06:38:44.273028 32758 solver.cpp:237] Iteration 50100, loss = 1.22559
I0523 06:38:44.273200 32758 solver.cpp:253]     Train net output #0: loss = 1.22559 (* 1 = 1.22559 loss)
I0523 06:38:44.273216 32758 sgd_solver.cpp:106] Iteration 50100, lr = 0.0025
I0523 06:38:53.560279 32758 solver.cpp:237] Iteration 50400, loss = 1.07089
I0523 06:38:53.560328 32758 solver.cpp:253]     Train net output #0: loss = 1.07089 (* 1 = 1.07089 loss)
I0523 06:38:53.560343 32758 sgd_solver.cpp:106] Iteration 50400, lr = 0.0025
I0523 06:39:02.848915 32758 solver.cpp:237] Iteration 50700, loss = 1.18268
I0523 06:39:02.848951 32758 solver.cpp:253]     Train net output #0: loss = 1.18268 (* 1 = 1.18268 loss)
I0523 06:39:02.848966 32758 sgd_solver.cpp:106] Iteration 50700, lr = 0.0025
I0523 06:39:12.110694 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_51000.caffemodel
I0523 06:39:12.180635 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_51000.solverstate
I0523 06:39:12.229768 32758 solver.cpp:237] Iteration 51000, loss = 1.07965
I0523 06:39:12.229815 32758 solver.cpp:253]     Train net output #0: loss = 1.07966 (* 1 = 1.07966 loss)
I0523 06:39:12.229832 32758 sgd_solver.cpp:106] Iteration 51000, lr = 0.0025
I0523 06:39:21.519949 32758 solver.cpp:237] Iteration 51300, loss = 1.09127
I0523 06:39:21.520118 32758 solver.cpp:253]     Train net output #0: loss = 1.09127 (* 1 = 1.09127 loss)
I0523 06:39:21.520133 32758 sgd_solver.cpp:106] Iteration 51300, lr = 0.0025
I0523 06:39:30.807821 32758 solver.cpp:237] Iteration 51600, loss = 1.07623
I0523 06:39:30.807855 32758 solver.cpp:253]     Train net output #0: loss = 1.07623 (* 1 = 1.07623 loss)
I0523 06:39:30.807871 32758 sgd_solver.cpp:106] Iteration 51600, lr = 0.0025
I0523 06:39:40.098178 32758 solver.cpp:237] Iteration 51900, loss = 0.839633
I0523 06:39:40.098223 32758 solver.cpp:253]     Train net output #0: loss = 0.839634 (* 1 = 0.839634 loss)
I0523 06:39:40.098239 32758 sgd_solver.cpp:106] Iteration 51900, lr = 0.0025
I0523 06:40:10.213923 32758 solver.cpp:237] Iteration 52200, loss = 1.12319
I0523 06:40:10.214102 32758 solver.cpp:253]     Train net output #0: loss = 1.12319 (* 1 = 1.12319 loss)
I0523 06:40:10.214115 32758 sgd_solver.cpp:106] Iteration 52200, lr = 0.0025
I0523 06:40:19.507972 32758 solver.cpp:237] Iteration 52500, loss = 1.05072
I0523 06:40:19.508005 32758 solver.cpp:253]     Train net output #0: loss = 1.05072 (* 1 = 1.05072 loss)
I0523 06:40:19.508021 32758 sgd_solver.cpp:106] Iteration 52500, lr = 0.0025
I0523 06:40:28.801724 32758 solver.cpp:237] Iteration 52800, loss = 1.28425
I0523 06:40:28.801770 32758 solver.cpp:253]     Train net output #0: loss = 1.28425 (* 1 = 1.28425 loss)
I0523 06:40:28.801784 32758 sgd_solver.cpp:106] Iteration 52800, lr = 0.0025
I0523 06:40:38.093243 32758 solver.cpp:237] Iteration 53100, loss = 1.33305
I0523 06:40:38.093278 32758 solver.cpp:253]     Train net output #0: loss = 1.33305 (* 1 = 1.33305 loss)
I0523 06:40:38.093293 32758 sgd_solver.cpp:106] Iteration 53100, lr = 0.0025
I0523 06:40:47.382339 32758 solver.cpp:237] Iteration 53400, loss = 1.15475
I0523 06:40:47.382493 32758 solver.cpp:253]     Train net output #0: loss = 1.15475 (* 1 = 1.15475 loss)
I0523 06:40:47.382506 32758 sgd_solver.cpp:106] Iteration 53400, lr = 0.0025
I0523 06:40:56.671257 32758 solver.cpp:237] Iteration 53700, loss = 0.83939
I0523 06:40:56.671303 32758 solver.cpp:253]     Train net output #0: loss = 0.83939 (* 1 = 0.83939 loss)
I0523 06:40:56.671316 32758 sgd_solver.cpp:106] Iteration 53700, lr = 0.0025
I0523 06:41:05.933681 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_54000.caffemodel
I0523 06:41:05.992871 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_54000.solverstate
I0523 06:41:06.019968 32758 solver.cpp:341] Iteration 54000, Testing net (#0)
I0523 06:41:53.835571 32758 solver.cpp:409]     Test net output #0: accuracy = 0.880635
I0523 06:41:53.835742 32758 solver.cpp:409]     Test net output #1: loss = 0.385671 (* 1 = 0.385671 loss)
I0523 06:42:14.769387 32758 solver.cpp:237] Iteration 54000, loss = 1.17418
I0523 06:42:14.769435 32758 solver.cpp:253]     Train net output #0: loss = 1.17418 (* 1 = 1.17418 loss)
I0523 06:42:14.769450 32758 sgd_solver.cpp:106] Iteration 54000, lr = 0.0025
I0523 06:42:24.049406 32758 solver.cpp:237] Iteration 54300, loss = 1.26235
I0523 06:42:24.049566 32758 solver.cpp:253]     Train net output #0: loss = 1.26235 (* 1 = 1.26235 loss)
I0523 06:42:24.049579 32758 sgd_solver.cpp:106] Iteration 54300, lr = 0.0025
I0523 06:42:33.330906 32758 solver.cpp:237] Iteration 54600, loss = 1.39938
I0523 06:42:33.330950 32758 solver.cpp:253]     Train net output #0: loss = 1.39938 (* 1 = 1.39938 loss)
I0523 06:42:33.330966 32758 sgd_solver.cpp:106] Iteration 54600, lr = 0.0025
I0523 06:42:42.611994 32758 solver.cpp:237] Iteration 54900, loss = 0.847119
I0523 06:42:42.612030 32758 solver.cpp:253]     Train net output #0: loss = 0.847119 (* 1 = 0.847119 loss)
I0523 06:42:42.612045 32758 sgd_solver.cpp:106] Iteration 54900, lr = 0.0025
I0523 06:42:51.893550 32758 solver.cpp:237] Iteration 55200, loss = 1.2422
I0523 06:42:51.893585 32758 solver.cpp:253]     Train net output #0: loss = 1.2422 (* 1 = 1.2422 loss)
I0523 06:42:51.893599 32758 sgd_solver.cpp:106] Iteration 55200, lr = 0.0025
I0523 06:43:01.175129 32758 solver.cpp:237] Iteration 55500, loss = 1.01001
I0523 06:43:01.175312 32758 solver.cpp:253]     Train net output #0: loss = 1.01001 (* 1 = 1.01001 loss)
I0523 06:43:01.175325 32758 sgd_solver.cpp:106] Iteration 55500, lr = 0.0025
I0523 06:43:10.457070 32758 solver.cpp:237] Iteration 55800, loss = 1.27413
I0523 06:43:10.457105 32758 solver.cpp:253]     Train net output #0: loss = 1.27413 (* 1 = 1.27413 loss)
I0523 06:43:10.457121 32758 sgd_solver.cpp:106] Iteration 55800, lr = 0.0025
I0523 06:43:40.643154 32758 solver.cpp:237] Iteration 56100, loss = 1.22871
I0523 06:43:40.643337 32758 solver.cpp:253]     Train net output #0: loss = 1.22871 (* 1 = 1.22871 loss)
I0523 06:43:40.643353 32758 sgd_solver.cpp:106] Iteration 56100, lr = 0.0025
I0523 06:43:49.922863 32758 solver.cpp:237] Iteration 56400, loss = 1.42936
I0523 06:43:49.922902 32758 solver.cpp:253]     Train net output #0: loss = 1.42936 (* 1 = 1.42936 loss)
I0523 06:43:49.922922 32758 sgd_solver.cpp:106] Iteration 56400, lr = 0.0025
I0523 06:43:59.206933 32758 solver.cpp:237] Iteration 56700, loss = 1.23491
I0523 06:43:59.206969 32758 solver.cpp:253]     Train net output #0: loss = 1.23491 (* 1 = 1.23491 loss)
I0523 06:43:59.206984 32758 sgd_solver.cpp:106] Iteration 56700, lr = 0.0025
I0523 06:44:08.456542 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_57000.caffemodel
I0523 06:44:08.518618 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_57000.solverstate
I0523 06:44:08.556529 32758 solver.cpp:237] Iteration 57000, loss = 1.43299
I0523 06:44:08.556578 32758 solver.cpp:253]     Train net output #0: loss = 1.43299 (* 1 = 1.43299 loss)
I0523 06:44:08.556594 32758 sgd_solver.cpp:106] Iteration 57000, lr = 0.0025
I0523 06:44:17.838652 32758 solver.cpp:237] Iteration 57300, loss = 1.20361
I0523 06:44:17.838824 32758 solver.cpp:253]     Train net output #0: loss = 1.20361 (* 1 = 1.20361 loss)
I0523 06:44:17.838837 32758 sgd_solver.cpp:106] Iteration 57300, lr = 0.0025
I0523 06:44:27.120004 32758 solver.cpp:237] Iteration 57600, loss = 1.3502
I0523 06:44:27.120039 32758 solver.cpp:253]     Train net output #0: loss = 1.3502 (* 1 = 1.3502 loss)
I0523 06:44:27.120055 32758 sgd_solver.cpp:106] Iteration 57600, lr = 0.0025
I0523 06:44:36.400997 32758 solver.cpp:237] Iteration 57900, loss = 1.20828
I0523 06:44:36.401032 32758 solver.cpp:253]     Train net output #0: loss = 1.20828 (* 1 = 1.20828 loss)
I0523 06:44:36.401048 32758 sgd_solver.cpp:106] Iteration 57900, lr = 0.0025
I0523 06:45:06.558408 32758 solver.cpp:237] Iteration 58200, loss = 1.2919
I0523 06:45:06.558584 32758 solver.cpp:253]     Train net output #0: loss = 1.2919 (* 1 = 1.2919 loss)
I0523 06:45:06.558601 32758 sgd_solver.cpp:106] Iteration 58200, lr = 0.0025
I0523 06:45:15.839126 32758 solver.cpp:237] Iteration 58500, loss = 1.29263
I0523 06:45:15.839161 32758 solver.cpp:253]     Train net output #0: loss = 1.29263 (* 1 = 1.29263 loss)
I0523 06:45:15.839176 32758 sgd_solver.cpp:106] Iteration 58500, lr = 0.0025
I0523 06:45:25.124585 32758 solver.cpp:237] Iteration 58800, loss = 1.25545
I0523 06:45:25.124621 32758 solver.cpp:253]     Train net output #0: loss = 1.25545 (* 1 = 1.25545 loss)
I0523 06:45:25.124635 32758 sgd_solver.cpp:106] Iteration 58800, lr = 0.0025
I0523 06:45:34.406395 32758 solver.cpp:237] Iteration 59100, loss = 1.15691
I0523 06:45:34.406443 32758 solver.cpp:253]     Train net output #0: loss = 1.15691 (* 1 = 1.15691 loss)
I0523 06:45:34.406457 32758 sgd_solver.cpp:106] Iteration 59100, lr = 0.0025
I0523 06:45:43.695425 32758 solver.cpp:237] Iteration 59400, loss = 1.21516
I0523 06:45:43.695593 32758 solver.cpp:253]     Train net output #0: loss = 1.21516 (* 1 = 1.21516 loss)
I0523 06:45:43.695606 32758 sgd_solver.cpp:106] Iteration 59400, lr = 0.0025
I0523 06:45:52.981112 32758 solver.cpp:237] Iteration 59700, loss = 1.16884
I0523 06:45:52.981156 32758 solver.cpp:253]     Train net output #0: loss = 1.16884 (* 1 = 1.16884 loss)
I0523 06:45:52.981171 32758 sgd_solver.cpp:106] Iteration 59700, lr = 0.0025
I0523 06:46:02.235076 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_60000.caffemodel
I0523 06:46:02.296624 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_60000.solverstate
I0523 06:46:02.324865 32758 solver.cpp:341] Iteration 60000, Testing net (#0)
I0523 06:47:11.023484 32758 solver.cpp:409]     Test net output #0: accuracy = 0.884941
I0523 06:47:11.023656 32758 solver.cpp:409]     Test net output #1: loss = 0.368039 (* 1 = 0.368039 loss)
I0523 06:47:31.914536 32758 solver.cpp:237] Iteration 60000, loss = 1.19886
I0523 06:47:31.914588 32758 solver.cpp:253]     Train net output #0: loss = 1.19886 (* 1 = 1.19886 loss)
I0523 06:47:31.914603 32758 sgd_solver.cpp:106] Iteration 60000, lr = 0.0025
I0523 06:47:41.204582 32758 solver.cpp:237] Iteration 60300, loss = 0.953795
I0523 06:47:41.204756 32758 solver.cpp:253]     Train net output #0: loss = 0.953795 (* 1 = 0.953795 loss)
I0523 06:47:41.204769 32758 sgd_solver.cpp:106] Iteration 60300, lr = 0.0025
I0523 06:47:50.494287 32758 solver.cpp:237] Iteration 60600, loss = 1.37357
I0523 06:47:50.494321 32758 solver.cpp:253]     Train net output #0: loss = 1.37357 (* 1 = 1.37357 loss)
I0523 06:47:50.494336 32758 sgd_solver.cpp:106] Iteration 60600, lr = 0.0025
I0523 06:47:59.786871 32758 solver.cpp:237] Iteration 60900, loss = 1.42528
I0523 06:47:59.786921 32758 solver.cpp:253]     Train net output #0: loss = 1.42528 (* 1 = 1.42528 loss)
I0523 06:47:59.786936 32758 sgd_solver.cpp:106] Iteration 60900, lr = 0.0025
I0523 06:48:09.075999 32758 solver.cpp:237] Iteration 61200, loss = 1.42078
I0523 06:48:09.076035 32758 solver.cpp:253]     Train net output #0: loss = 1.42078 (* 1 = 1.42078 loss)
I0523 06:48:09.076050 32758 sgd_solver.cpp:106] Iteration 61200, lr = 0.0025
I0523 06:48:18.366874 32758 solver.cpp:237] Iteration 61500, loss = 1.22376
I0523 06:48:18.367043 32758 solver.cpp:253]     Train net output #0: loss = 1.22376 (* 1 = 1.22376 loss)
I0523 06:48:18.367055 32758 sgd_solver.cpp:106] Iteration 61500, lr = 0.0025
I0523 06:48:27.653152 32758 solver.cpp:237] Iteration 61800, loss = 1.08131
I0523 06:48:27.653197 32758 solver.cpp:253]     Train net output #0: loss = 1.08131 (* 1 = 1.08131 loss)
I0523 06:48:27.653213 32758 sgd_solver.cpp:106] Iteration 61800, lr = 0.0025
I0523 06:48:57.820754 32758 solver.cpp:237] Iteration 62100, loss = 0.991366
I0523 06:48:57.820930 32758 solver.cpp:253]     Train net output #0: loss = 0.991366 (* 1 = 0.991366 loss)
I0523 06:48:57.820947 32758 sgd_solver.cpp:106] Iteration 62100, lr = 0.0025
I0523 06:49:07.112869 32758 solver.cpp:237] Iteration 62400, loss = 1.24538
I0523 06:49:07.112902 32758 solver.cpp:253]     Train net output #0: loss = 1.24538 (* 1 = 1.24538 loss)
I0523 06:49:07.112917 32758 sgd_solver.cpp:106] Iteration 62400, lr = 0.0025
I0523 06:49:16.405421 32758 solver.cpp:237] Iteration 62700, loss = 1.54285
I0523 06:49:16.405462 32758 solver.cpp:253]     Train net output #0: loss = 1.54285 (* 1 = 1.54285 loss)
I0523 06:49:16.405481 32758 sgd_solver.cpp:106] Iteration 62700, lr = 0.0025
I0523 06:49:25.661119 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_63000.caffemodel
I0523 06:49:25.719952 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_63000.solverstate
I0523 06:49:25.756002 32758 solver.cpp:237] Iteration 63000, loss = 1.28654
I0523 06:49:25.756048 32758 solver.cpp:253]     Train net output #0: loss = 1.28654 (* 1 = 1.28654 loss)
I0523 06:49:25.756062 32758 sgd_solver.cpp:106] Iteration 63000, lr = 0.0025
I0523 06:49:35.049054 32758 solver.cpp:237] Iteration 63300, loss = 1.17376
I0523 06:49:35.049224 32758 solver.cpp:253]     Train net output #0: loss = 1.17376 (* 1 = 1.17376 loss)
I0523 06:49:35.049238 32758 sgd_solver.cpp:106] Iteration 63300, lr = 0.0025
I0523 06:49:44.335237 32758 solver.cpp:237] Iteration 63600, loss = 1.32836
I0523 06:49:44.335280 32758 solver.cpp:253]     Train net output #0: loss = 1.32836 (* 1 = 1.32836 loss)
I0523 06:49:44.335297 32758 sgd_solver.cpp:106] Iteration 63600, lr = 0.0025
I0523 06:49:53.628284 32758 solver.cpp:237] Iteration 63900, loss = 1.1039
I0523 06:49:53.628320 32758 solver.cpp:253]     Train net output #0: loss = 1.1039 (* 1 = 1.1039 loss)
I0523 06:49:53.628334 32758 sgd_solver.cpp:106] Iteration 63900, lr = 0.0025
I0523 06:50:23.812348 32758 solver.cpp:237] Iteration 64200, loss = 1.16882
I0523 06:50:23.812525 32758 solver.cpp:253]     Train net output #0: loss = 1.16882 (* 1 = 1.16882 loss)
I0523 06:50:23.812541 32758 sgd_solver.cpp:106] Iteration 64200, lr = 0.0025
I0523 06:50:33.102022 32758 solver.cpp:237] Iteration 64500, loss = 1.07918
I0523 06:50:33.102061 32758 solver.cpp:253]     Train net output #0: loss = 1.07918 (* 1 = 1.07918 loss)
I0523 06:50:33.102073 32758 sgd_solver.cpp:106] Iteration 64500, lr = 0.0025
I0523 06:50:42.393635 32758 solver.cpp:237] Iteration 64800, loss = 1.29597
I0523 06:50:42.393669 32758 solver.cpp:253]     Train net output #0: loss = 1.29597 (* 1 = 1.29597 loss)
I0523 06:50:42.393687 32758 sgd_solver.cpp:106] Iteration 64800, lr = 0.0025
I0523 06:50:51.686327 32758 solver.cpp:237] Iteration 65100, loss = 1.26353
I0523 06:50:51.686431 32758 solver.cpp:253]     Train net output #0: loss = 1.26353 (* 1 = 1.26353 loss)
I0523 06:50:51.686445 32758 sgd_solver.cpp:106] Iteration 65100, lr = 0.0025
I0523 06:51:00.976747 32758 solver.cpp:237] Iteration 65400, loss = 1.43489
I0523 06:51:00.976917 32758 solver.cpp:253]     Train net output #0: loss = 1.43489 (* 1 = 1.43489 loss)
I0523 06:51:00.976930 32758 sgd_solver.cpp:106] Iteration 65400, lr = 0.0025
I0523 06:51:10.265223 32758 solver.cpp:237] Iteration 65700, loss = 0.973222
I0523 06:51:10.265259 32758 solver.cpp:253]     Train net output #0: loss = 0.973222 (* 1 = 0.973222 loss)
I0523 06:51:10.265275 32758 sgd_solver.cpp:106] Iteration 65700, lr = 0.0025
I0523 06:51:19.526259 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_66000.caffemodel
I0523 06:51:19.585474 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_66000.solverstate
I0523 06:51:19.611893 32758 solver.cpp:341] Iteration 66000, Testing net (#0)
I0523 06:52:07.111737 32758 solver.cpp:409]     Test net output #0: accuracy = 0.883366
I0523 06:52:07.111914 32758 solver.cpp:409]     Test net output #1: loss = 0.364363 (* 1 = 0.364363 loss)
I0523 06:52:27.969415 32758 solver.cpp:237] Iteration 66000, loss = 0.975679
I0523 06:52:27.969468 32758 solver.cpp:253]     Train net output #0: loss = 0.97568 (* 1 = 0.97568 loss)
I0523 06:52:27.969483 32758 sgd_solver.cpp:106] Iteration 66000, lr = 0.0025
I0523 06:52:37.256917 32758 solver.cpp:237] Iteration 66300, loss = 1.19033
I0523 06:52:37.257143 32758 solver.cpp:253]     Train net output #0: loss = 1.19033 (* 1 = 1.19033 loss)
I0523 06:52:37.257158 32758 sgd_solver.cpp:106] Iteration 66300, lr = 0.0025
I0523 06:52:46.544693 32758 solver.cpp:237] Iteration 66600, loss = 1.05319
I0523 06:52:46.544728 32758 solver.cpp:253]     Train net output #0: loss = 1.05319 (* 1 = 1.05319 loss)
I0523 06:52:46.544744 32758 sgd_solver.cpp:106] Iteration 66600, lr = 0.0025
I0523 06:52:55.830145 32758 solver.cpp:237] Iteration 66900, loss = 1.23801
I0523 06:52:55.830180 32758 solver.cpp:253]     Train net output #0: loss = 1.23801 (* 1 = 1.23801 loss)
I0523 06:52:55.830196 32758 sgd_solver.cpp:106] Iteration 66900, lr = 0.0025
I0523 06:53:05.113486 32758 solver.cpp:237] Iteration 67200, loss = 1.04867
I0523 06:53:05.113524 32758 solver.cpp:253]     Train net output #0: loss = 1.04867 (* 1 = 1.04867 loss)
I0523 06:53:05.113541 32758 sgd_solver.cpp:106] Iteration 67200, lr = 0.0025
I0523 06:53:14.400593 32758 solver.cpp:237] Iteration 67500, loss = 1.1356
I0523 06:53:14.400748 32758 solver.cpp:253]     Train net output #0: loss = 1.1356 (* 1 = 1.1356 loss)
I0523 06:53:14.400760 32758 sgd_solver.cpp:106] Iteration 67500, lr = 0.0025
I0523 06:53:23.687748 32758 solver.cpp:237] Iteration 67800, loss = 0.907068
I0523 06:53:23.687796 32758 solver.cpp:253]     Train net output #0: loss = 0.907068 (* 1 = 0.907068 loss)
I0523 06:53:23.687811 32758 sgd_solver.cpp:106] Iteration 67800, lr = 0.0025
I0523 06:53:53.888228 32758 solver.cpp:237] Iteration 68100, loss = 0.959757
I0523 06:53:53.888422 32758 solver.cpp:253]     Train net output #0: loss = 0.959757 (* 1 = 0.959757 loss)
I0523 06:53:53.888437 32758 sgd_solver.cpp:106] Iteration 68100, lr = 0.0025
I0523 06:54:03.173146 32758 solver.cpp:237] Iteration 68400, loss = 1.02044
I0523 06:54:03.173177 32758 solver.cpp:253]     Train net output #0: loss = 1.02044 (* 1 = 1.02044 loss)
I0523 06:54:03.173190 32758 sgd_solver.cpp:106] Iteration 68400, lr = 0.0025
I0523 06:54:12.459075 32758 solver.cpp:237] Iteration 68700, loss = 1.16736
I0523 06:54:12.459130 32758 solver.cpp:253]     Train net output #0: loss = 1.16736 (* 1 = 1.16736 loss)
I0523 06:54:12.459143 32758 sgd_solver.cpp:106] Iteration 68700, lr = 0.0025
I0523 06:54:21.713209 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_69000.caffemodel
I0523 06:54:21.774175 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_69000.solverstate
I0523 06:54:21.810346 32758 solver.cpp:237] Iteration 69000, loss = 1.23485
I0523 06:54:21.810391 32758 solver.cpp:253]     Train net output #0: loss = 1.23485 (* 1 = 1.23485 loss)
I0523 06:54:21.810407 32758 sgd_solver.cpp:106] Iteration 69000, lr = 0.0025
I0523 06:54:31.097523 32758 solver.cpp:237] Iteration 69300, loss = 0.99665
I0523 06:54:31.097687 32758 solver.cpp:253]     Train net output #0: loss = 0.99665 (* 1 = 0.99665 loss)
I0523 06:54:31.097702 32758 sgd_solver.cpp:106] Iteration 69300, lr = 0.0025
I0523 06:54:40.385190 32758 solver.cpp:237] Iteration 69600, loss = 1.14534
I0523 06:54:40.385234 32758 solver.cpp:253]     Train net output #0: loss = 1.14534 (* 1 = 1.14534 loss)
I0523 06:54:40.385252 32758 sgd_solver.cpp:106] Iteration 69600, lr = 0.0025
I0523 06:54:49.671567 32758 solver.cpp:237] Iteration 69900, loss = 0.813052
I0523 06:54:49.671603 32758 solver.cpp:253]     Train net output #0: loss = 0.813052 (* 1 = 0.813052 loss)
I0523 06:54:49.671617 32758 sgd_solver.cpp:106] Iteration 69900, lr = 0.0025
I0523 06:55:19.838922 32758 solver.cpp:237] Iteration 70200, loss = 1.18706
I0523 06:55:19.839100 32758 solver.cpp:253]     Train net output #0: loss = 1.18706 (* 1 = 1.18706 loss)
I0523 06:55:19.839123 32758 sgd_solver.cpp:106] Iteration 70200, lr = 0.0025
I0523 06:55:29.123661 32758 solver.cpp:237] Iteration 70500, loss = 1.39667
I0523 06:55:29.123708 32758 solver.cpp:253]     Train net output #0: loss = 1.39667 (* 1 = 1.39667 loss)
I0523 06:55:29.123723 32758 sgd_solver.cpp:106] Iteration 70500, lr = 0.0025
I0523 06:55:38.406745 32758 solver.cpp:237] Iteration 70800, loss = 1.13496
I0523 06:55:38.406781 32758 solver.cpp:253]     Train net output #0: loss = 1.13497 (* 1 = 1.13497 loss)
I0523 06:55:38.406796 32758 sgd_solver.cpp:106] Iteration 70800, lr = 0.0025
I0523 06:55:47.691561 32758 solver.cpp:237] Iteration 71100, loss = 1.04592
I0523 06:55:47.691596 32758 solver.cpp:253]     Train net output #0: loss = 1.04592 (* 1 = 1.04592 loss)
I0523 06:55:47.691611 32758 sgd_solver.cpp:106] Iteration 71100, lr = 0.0025
I0523 06:55:56.979544 32758 solver.cpp:237] Iteration 71400, loss = 1.20553
I0523 06:55:56.979723 32758 solver.cpp:253]     Train net output #0: loss = 1.20553 (* 1 = 1.20553 loss)
I0523 06:55:56.979738 32758 sgd_solver.cpp:106] Iteration 71400, lr = 0.0025
I0523 06:56:06.264263 32758 solver.cpp:237] Iteration 71700, loss = 1.03905
I0523 06:56:06.264298 32758 solver.cpp:253]     Train net output #0: loss = 1.03905 (* 1 = 1.03905 loss)
I0523 06:56:06.264314 32758 sgd_solver.cpp:106] Iteration 71700, lr = 0.0025
I0523 06:56:15.519170 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_72000.caffemodel
I0523 06:56:15.581671 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_72000.solverstate
I0523 06:56:15.609407 32758 solver.cpp:341] Iteration 72000, Testing net (#0)
I0523 06:57:24.333266 32758 solver.cpp:409]     Test net output #0: accuracy = 0.882222
I0523 06:57:24.333444 32758 solver.cpp:409]     Test net output #1: loss = 0.379595 (* 1 = 0.379595 loss)
I0523 06:57:45.241422 32758 solver.cpp:237] Iteration 72000, loss = 1.03207
I0523 06:57:45.241477 32758 solver.cpp:253]     Train net output #0: loss = 1.03207 (* 1 = 1.03207 loss)
I0523 06:57:45.241492 32758 sgd_solver.cpp:106] Iteration 72000, lr = 0.0025
I0523 06:57:54.531927 32758 solver.cpp:237] Iteration 72300, loss = 1.37703
I0523 06:57:54.532094 32758 solver.cpp:253]     Train net output #0: loss = 1.37703 (* 1 = 1.37703 loss)
I0523 06:57:54.532107 32758 sgd_solver.cpp:106] Iteration 72300, lr = 0.0025
I0523 06:58:03.818627 32758 solver.cpp:237] Iteration 72600, loss = 1.33882
I0523 06:58:03.818670 32758 solver.cpp:253]     Train net output #0: loss = 1.33882 (* 1 = 1.33882 loss)
I0523 06:58:03.818686 32758 sgd_solver.cpp:106] Iteration 72600, lr = 0.0025
I0523 06:58:13.105737 32758 solver.cpp:237] Iteration 72900, loss = 1.06442
I0523 06:58:13.105773 32758 solver.cpp:253]     Train net output #0: loss = 1.06442 (* 1 = 1.06442 loss)
I0523 06:58:13.105788 32758 sgd_solver.cpp:106] Iteration 72900, lr = 0.0025
I0523 06:58:22.399624 32758 solver.cpp:237] Iteration 73200, loss = 1.25142
I0523 06:58:22.399670 32758 solver.cpp:253]     Train net output #0: loss = 1.25142 (* 1 = 1.25142 loss)
I0523 06:58:22.399685 32758 sgd_solver.cpp:106] Iteration 73200, lr = 0.0025
I0523 06:58:31.694612 32758 solver.cpp:237] Iteration 73500, loss = 1.12866
I0523 06:58:31.694771 32758 solver.cpp:253]     Train net output #0: loss = 1.12866 (* 1 = 1.12866 loss)
I0523 06:58:31.694784 32758 sgd_solver.cpp:106] Iteration 73500, lr = 0.0025
I0523 06:58:40.989512 32758 solver.cpp:237] Iteration 73800, loss = 1.18049
I0523 06:58:40.989547 32758 solver.cpp:253]     Train net output #0: loss = 1.18049 (* 1 = 1.18049 loss)
I0523 06:58:40.989562 32758 sgd_solver.cpp:106] Iteration 73800, lr = 0.0025
I0523 06:59:11.185736 32758 solver.cpp:237] Iteration 74100, loss = 1.06532
I0523 06:59:11.185919 32758 solver.cpp:253]     Train net output #0: loss = 1.06532 (* 1 = 1.06532 loss)
I0523 06:59:11.185933 32758 sgd_solver.cpp:106] Iteration 74100, lr = 0.0025
I0523 06:59:20.480769 32758 solver.cpp:237] Iteration 74400, loss = 1.01834
I0523 06:59:20.480813 32758 solver.cpp:253]     Train net output #0: loss = 1.01834 (* 1 = 1.01834 loss)
I0523 06:59:20.480830 32758 sgd_solver.cpp:106] Iteration 74400, lr = 0.0025
I0523 06:59:29.770562 32758 solver.cpp:237] Iteration 74700, loss = 1.24577
I0523 06:59:29.770597 32758 solver.cpp:253]     Train net output #0: loss = 1.24577 (* 1 = 1.24577 loss)
I0523 06:59:29.770612 32758 sgd_solver.cpp:106] Iteration 74700, lr = 0.0025
I0523 06:59:39.033465 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_75000.caffemodel
I0523 06:59:39.094920 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_75000.solverstate
I0523 06:59:39.133044 32758 solver.cpp:237] Iteration 75000, loss = 1.19088
I0523 06:59:39.133088 32758 solver.cpp:253]     Train net output #0: loss = 1.19088 (* 1 = 1.19088 loss)
I0523 06:59:39.133103 32758 sgd_solver.cpp:106] Iteration 75000, lr = 0.0025
I0523 06:59:48.428050 32758 solver.cpp:237] Iteration 75300, loss = 1.54965
I0523 06:59:48.428225 32758 solver.cpp:253]     Train net output #0: loss = 1.54965 (* 1 = 1.54965 loss)
I0523 06:59:48.428237 32758 sgd_solver.cpp:106] Iteration 75300, lr = 0.0025
I0523 06:59:57.721902 32758 solver.cpp:237] Iteration 75600, loss = 1.35506
I0523 06:59:57.721937 32758 solver.cpp:253]     Train net output #0: loss = 1.35506 (* 1 = 1.35506 loss)
I0523 06:59:57.721952 32758 sgd_solver.cpp:106] Iteration 75600, lr = 0.0025
I0523 07:00:07.017616 32758 solver.cpp:237] Iteration 75900, loss = 1.06051
I0523 07:00:07.017662 32758 solver.cpp:253]     Train net output #0: loss = 1.06051 (* 1 = 1.06051 loss)
I0523 07:00:07.017678 32758 sgd_solver.cpp:106] Iteration 75900, lr = 0.0025
I0523 07:00:37.140444 32758 solver.cpp:237] Iteration 76200, loss = 1.24622
I0523 07:00:37.140628 32758 solver.cpp:253]     Train net output #0: loss = 1.24622 (* 1 = 1.24622 loss)
I0523 07:00:37.140642 32758 sgd_solver.cpp:106] Iteration 76200, lr = 0.0025
I0523 07:00:46.430238 32758 solver.cpp:237] Iteration 76500, loss = 1.41213
I0523 07:00:46.430274 32758 solver.cpp:253]     Train net output #0: loss = 1.41213 (* 1 = 1.41213 loss)
I0523 07:00:46.430289 32758 sgd_solver.cpp:106] Iteration 76500, lr = 0.0025
I0523 07:00:55.719254 32758 solver.cpp:237] Iteration 76800, loss = 1.08738
I0523 07:00:55.719298 32758 solver.cpp:253]     Train net output #0: loss = 1.08738 (* 1 = 1.08738 loss)
I0523 07:00:55.719315 32758 sgd_solver.cpp:106] Iteration 76800, lr = 0.0025
I0523 07:01:05.013455 32758 solver.cpp:237] Iteration 77100, loss = 1.17106
I0523 07:01:05.013490 32758 solver.cpp:253]     Train net output #0: loss = 1.17106 (* 1 = 1.17106 loss)
I0523 07:01:05.013505 32758 sgd_solver.cpp:106] Iteration 77100, lr = 0.0025
I0523 07:01:14.302927 32758 solver.cpp:237] Iteration 77400, loss = 1.32218
I0523 07:01:14.303086 32758 solver.cpp:253]     Train net output #0: loss = 1.32219 (* 1 = 1.32219 loss)
I0523 07:01:14.303099 32758 sgd_solver.cpp:106] Iteration 77400, lr = 0.0025
I0523 07:01:23.596638 32758 solver.cpp:237] Iteration 77700, loss = 0.767682
I0523 07:01:23.596678 32758 solver.cpp:253]     Train net output #0: loss = 0.767682 (* 1 = 0.767682 loss)
I0523 07:01:23.596698 32758 sgd_solver.cpp:106] Iteration 77700, lr = 0.0025
I0523 07:01:32.857781 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_78000.caffemodel
I0523 07:01:32.917255 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_78000.solverstate
I0523 07:01:32.944205 32758 solver.cpp:341] Iteration 78000, Testing net (#0)
I0523 07:02:20.782356 32758 solver.cpp:409]     Test net output #0: accuracy = 0.879543
I0523 07:02:20.782546 32758 solver.cpp:409]     Test net output #1: loss = 0.367817 (* 1 = 0.367817 loss)
I0523 07:02:41.642869 32758 solver.cpp:237] Iteration 78000, loss = 1.10523
I0523 07:02:41.642921 32758 solver.cpp:253]     Train net output #0: loss = 1.10523 (* 1 = 1.10523 loss)
I0523 07:02:41.642936 32758 sgd_solver.cpp:106] Iteration 78000, lr = 0.0025
I0523 07:02:50.927376 32758 solver.cpp:237] Iteration 78300, loss = 1.13251
I0523 07:02:50.927533 32758 solver.cpp:253]     Train net output #0: loss = 1.13251 (* 1 = 1.13251 loss)
I0523 07:02:50.927547 32758 sgd_solver.cpp:106] Iteration 78300, lr = 0.0025
I0523 07:03:00.210062 32758 solver.cpp:237] Iteration 78600, loss = 1.35837
I0523 07:03:00.210110 32758 solver.cpp:253]     Train net output #0: loss = 1.35837 (* 1 = 1.35837 loss)
I0523 07:03:00.210124 32758 sgd_solver.cpp:106] Iteration 78600, lr = 0.0025
I0523 07:03:09.488018 32758 solver.cpp:237] Iteration 78900, loss = 1.21945
I0523 07:03:09.488054 32758 solver.cpp:253]     Train net output #0: loss = 1.21945 (* 1 = 1.21945 loss)
I0523 07:03:09.488070 32758 sgd_solver.cpp:106] Iteration 78900, lr = 0.0025
I0523 07:03:18.767961 32758 solver.cpp:237] Iteration 79200, loss = 1.38067
I0523 07:03:18.767997 32758 solver.cpp:253]     Train net output #0: loss = 1.38067 (* 1 = 1.38067 loss)
I0523 07:03:18.768009 32758 sgd_solver.cpp:106] Iteration 79200, lr = 0.0025
I0523 07:03:28.049166 32758 solver.cpp:237] Iteration 79500, loss = 1.14483
I0523 07:03:28.049338 32758 solver.cpp:253]     Train net output #0: loss = 1.14483 (* 1 = 1.14483 loss)
I0523 07:03:28.049352 32758 sgd_solver.cpp:106] Iteration 79500, lr = 0.0025
I0523 07:03:37.330929 32758 solver.cpp:237] Iteration 79800, loss = 0.914782
I0523 07:03:37.330965 32758 solver.cpp:253]     Train net output #0: loss = 0.914782 (* 1 = 0.914782 loss)
I0523 07:03:37.330979 32758 sgd_solver.cpp:106] Iteration 79800, lr = 0.0025
I0523 07:04:07.493083 32758 solver.cpp:237] Iteration 80100, loss = 1.11933
I0523 07:04:07.493264 32758 solver.cpp:253]     Train net output #0: loss = 1.11933 (* 1 = 1.11933 loss)
I0523 07:04:07.493281 32758 sgd_solver.cpp:106] Iteration 80100, lr = 0.0025
I0523 07:04:16.776310 32758 solver.cpp:237] Iteration 80400, loss = 0.945815
I0523 07:04:16.776355 32758 solver.cpp:253]     Train net output #0: loss = 0.945815 (* 1 = 0.945815 loss)
I0523 07:04:16.776371 32758 sgd_solver.cpp:106] Iteration 80400, lr = 0.0025
I0523 07:04:26.061185 32758 solver.cpp:237] Iteration 80700, loss = 1.23799
I0523 07:04:26.061221 32758 solver.cpp:253]     Train net output #0: loss = 1.23799 (* 1 = 1.23799 loss)
I0523 07:04:26.061236 32758 sgd_solver.cpp:106] Iteration 80700, lr = 0.0025
I0523 07:04:35.311084 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_81000.caffemodel
I0523 07:04:35.370976 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_81000.solverstate
I0523 07:04:35.407111 32758 solver.cpp:237] Iteration 81000, loss = 1.07909
I0523 07:04:35.407158 32758 solver.cpp:253]     Train net output #0: loss = 1.07909 (* 1 = 1.07909 loss)
I0523 07:04:35.407178 32758 sgd_solver.cpp:106] Iteration 81000, lr = 0.0025
I0523 07:04:44.693131 32758 solver.cpp:237] Iteration 81300, loss = 1.06289
I0523 07:04:44.693310 32758 solver.cpp:253]     Train net output #0: loss = 1.06289 (* 1 = 1.06289 loss)
I0523 07:04:44.693323 32758 sgd_solver.cpp:106] Iteration 81300, lr = 0.0025
I0523 07:04:53.980269 32758 solver.cpp:237] Iteration 81600, loss = 1.22683
I0523 07:04:53.980304 32758 solver.cpp:253]     Train net output #0: loss = 1.22683 (* 1 = 1.22683 loss)
I0523 07:04:53.980320 32758 sgd_solver.cpp:106] Iteration 81600, lr = 0.0025
I0523 07:05:03.261021 32758 solver.cpp:237] Iteration 81900, loss = 0.99989
I0523 07:05:03.261067 32758 solver.cpp:253]     Train net output #0: loss = 0.99989 (* 1 = 0.99989 loss)
I0523 07:05:03.261083 32758 sgd_solver.cpp:106] Iteration 81900, lr = 0.0025
I0523 07:05:33.416772 32758 solver.cpp:237] Iteration 82200, loss = 1.01007
I0523 07:05:33.416967 32758 solver.cpp:253]     Train net output #0: loss = 1.01007 (* 1 = 1.01007 loss)
I0523 07:05:33.416985 32758 sgd_solver.cpp:106] Iteration 82200, lr = 0.0025
I0523 07:05:42.693472 32758 solver.cpp:237] Iteration 82500, loss = 1.35347
I0523 07:05:42.693506 32758 solver.cpp:253]     Train net output #0: loss = 1.35347 (* 1 = 1.35347 loss)
I0523 07:05:42.693522 32758 sgd_solver.cpp:106] Iteration 82500, lr = 0.0025
I0523 07:05:51.977502 32758 solver.cpp:237] Iteration 82800, loss = 0.947842
I0523 07:05:51.977538 32758 solver.cpp:253]     Train net output #0: loss = 0.947842 (* 1 = 0.947842 loss)
I0523 07:05:51.977555 32758 sgd_solver.cpp:106] Iteration 82800, lr = 0.0025
I0523 07:06:01.263013 32758 solver.cpp:237] Iteration 83100, loss = 1.23139
I0523 07:06:01.263056 32758 solver.cpp:253]     Train net output #0: loss = 1.23139 (* 1 = 1.23139 loss)
I0523 07:06:01.263072 32758 sgd_solver.cpp:106] Iteration 83100, lr = 0.0025
I0523 07:06:10.543565 32758 solver.cpp:237] Iteration 83400, loss = 1.02229
I0523 07:06:10.543730 32758 solver.cpp:253]     Train net output #0: loss = 1.02229 (* 1 = 1.02229 loss)
I0523 07:06:10.543742 32758 sgd_solver.cpp:106] Iteration 83400, lr = 0.0025
I0523 07:06:19.824265 32758 solver.cpp:237] Iteration 83700, loss = 0.99783
I0523 07:06:19.824312 32758 solver.cpp:253]     Train net output #0: loss = 0.997831 (* 1 = 0.997831 loss)
I0523 07:06:19.824327 32758 sgd_solver.cpp:106] Iteration 83700, lr = 0.0025
I0523 07:06:29.076915 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_84000.caffemodel
I0523 07:06:29.136555 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_84000.solverstate
I0523 07:06:29.163045 32758 solver.cpp:341] Iteration 84000, Testing net (#0)
I0523 07:07:37.851034 32758 solver.cpp:409]     Test net output #0: accuracy = 0.888126
I0523 07:07:37.851222 32758 solver.cpp:409]     Test net output #1: loss = 0.354128 (* 1 = 0.354128 loss)
I0523 07:07:58.790591 32758 solver.cpp:237] Iteration 84000, loss = 1.22111
I0523 07:07:58.790644 32758 solver.cpp:253]     Train net output #0: loss = 1.22111 (* 1 = 1.22111 loss)
I0523 07:07:58.790660 32758 sgd_solver.cpp:106] Iteration 84000, lr = 0.0025
I0523 07:08:08.079742 32758 solver.cpp:237] Iteration 84300, loss = 1.4224
I0523 07:08:08.079911 32758 solver.cpp:253]     Train net output #0: loss = 1.4224 (* 1 = 1.4224 loss)
I0523 07:08:08.079926 32758 sgd_solver.cpp:106] Iteration 84300, lr = 0.0025
I0523 07:08:17.370062 32758 solver.cpp:237] Iteration 84600, loss = 0.952876
I0523 07:08:17.370098 32758 solver.cpp:253]     Train net output #0: loss = 0.952877 (* 1 = 0.952877 loss)
I0523 07:08:17.370115 32758 sgd_solver.cpp:106] Iteration 84600, lr = 0.0025
I0523 07:08:26.661432 32758 solver.cpp:237] Iteration 84900, loss = 1.01068
I0523 07:08:26.661473 32758 solver.cpp:253]     Train net output #0: loss = 1.01068 (* 1 = 1.01068 loss)
I0523 07:08:26.661492 32758 sgd_solver.cpp:106] Iteration 84900, lr = 0.0025
I0523 07:08:35.955809 32758 solver.cpp:237] Iteration 85200, loss = 1.10977
I0523 07:08:35.955845 32758 solver.cpp:253]     Train net output #0: loss = 1.10977 (* 1 = 1.10977 loss)
I0523 07:08:35.955860 32758 sgd_solver.cpp:106] Iteration 85200, lr = 0.0025
I0523 07:08:45.246119 32758 solver.cpp:237] Iteration 85500, loss = 1.25031
I0523 07:08:45.246284 32758 solver.cpp:253]     Train net output #0: loss = 1.25031 (* 1 = 1.25031 loss)
I0523 07:08:45.246299 32758 sgd_solver.cpp:106] Iteration 85500, lr = 0.0025
I0523 07:08:54.536480 32758 solver.cpp:237] Iteration 85800, loss = 1.16662
I0523 07:08:54.536523 32758 solver.cpp:253]     Train net output #0: loss = 1.16662 (* 1 = 1.16662 loss)
I0523 07:08:54.536540 32758 sgd_solver.cpp:106] Iteration 85800, lr = 0.0025
I0523 07:09:24.734194 32758 solver.cpp:237] Iteration 86100, loss = 1.11361
I0523 07:09:24.734390 32758 solver.cpp:253]     Train net output #0: loss = 1.11361 (* 1 = 1.11361 loss)
I0523 07:09:24.734406 32758 sgd_solver.cpp:106] Iteration 86100, lr = 0.0025
I0523 07:09:34.026558 32758 solver.cpp:237] Iteration 86400, loss = 1.0447
I0523 07:09:34.026594 32758 solver.cpp:253]     Train net output #0: loss = 1.0447 (* 1 = 1.0447 loss)
I0523 07:09:34.026609 32758 sgd_solver.cpp:106] Iteration 86400, lr = 0.0025
I0523 07:09:43.319393 32758 solver.cpp:237] Iteration 86700, loss = 1.23239
I0523 07:09:43.319442 32758 solver.cpp:253]     Train net output #0: loss = 1.23239 (* 1 = 1.23239 loss)
I0523 07:09:43.319456 32758 sgd_solver.cpp:106] Iteration 86700, lr = 0.0025
I0523 07:09:52.578249 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_87000.caffemodel
I0523 07:09:52.640058 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_87000.solverstate
I0523 07:09:52.678689 32758 solver.cpp:237] Iteration 87000, loss = 1.13265
I0523 07:09:52.678740 32758 solver.cpp:253]     Train net output #0: loss = 1.13265 (* 1 = 1.13265 loss)
I0523 07:09:52.678755 32758 sgd_solver.cpp:106] Iteration 87000, lr = 0.0025
I0523 07:10:01.972169 32758 solver.cpp:237] Iteration 87300, loss = 0.880508
I0523 07:10:01.972342 32758 solver.cpp:253]     Train net output #0: loss = 0.880508 (* 1 = 0.880508 loss)
I0523 07:10:01.972354 32758 sgd_solver.cpp:106] Iteration 87300, lr = 0.0025
I0523 07:10:11.262238 32758 solver.cpp:237] Iteration 87600, loss = 1.01616
I0523 07:10:11.262277 32758 solver.cpp:253]     Train net output #0: loss = 1.01616 (* 1 = 1.01616 loss)
I0523 07:10:11.262295 32758 sgd_solver.cpp:106] Iteration 87600, lr = 0.0025
I0523 07:10:20.555712 32758 solver.cpp:237] Iteration 87900, loss = 0.976481
I0523 07:10:20.555748 32758 solver.cpp:253]     Train net output #0: loss = 0.976481 (* 1 = 0.976481 loss)
I0523 07:10:20.555763 32758 sgd_solver.cpp:106] Iteration 87900, lr = 0.0025
I0523 07:10:50.737942 32758 solver.cpp:237] Iteration 88200, loss = 0.800326
I0523 07:10:50.738129 32758 solver.cpp:253]     Train net output #0: loss = 0.800327 (* 1 = 0.800327 loss)
I0523 07:10:50.738144 32758 sgd_solver.cpp:106] Iteration 88200, lr = 0.0025
I0523 07:11:00.026187 32758 solver.cpp:237] Iteration 88500, loss = 1.11716
I0523 07:11:00.026232 32758 solver.cpp:253]     Train net output #0: loss = 1.11716 (* 1 = 1.11716 loss)
I0523 07:11:00.026248 32758 sgd_solver.cpp:106] Iteration 88500, lr = 0.0025
I0523 07:11:09.311208 32758 solver.cpp:237] Iteration 88800, loss = 1.13227
I0523 07:11:09.311244 32758 solver.cpp:253]     Train net output #0: loss = 1.13227 (* 1 = 1.13227 loss)
I0523 07:11:09.311259 32758 sgd_solver.cpp:106] Iteration 88800, lr = 0.0025
I0523 07:11:18.601548 32758 solver.cpp:237] Iteration 89100, loss = 1.08473
I0523 07:11:18.601594 32758 solver.cpp:253]     Train net output #0: loss = 1.08473 (* 1 = 1.08473 loss)
I0523 07:11:18.601608 32758 sgd_solver.cpp:106] Iteration 89100, lr = 0.0025
I0523 07:11:27.896075 32758 solver.cpp:237] Iteration 89400, loss = 1.09341
I0523 07:11:27.896241 32758 solver.cpp:253]     Train net output #0: loss = 1.09341 (* 1 = 1.09341 loss)
I0523 07:11:27.896255 32758 sgd_solver.cpp:106] Iteration 89400, lr = 0.0025
I0523 07:11:37.187531 32758 solver.cpp:237] Iteration 89700, loss = 1.27192
I0523 07:11:37.187566 32758 solver.cpp:253]     Train net output #0: loss = 1.27192 (* 1 = 1.27192 loss)
I0523 07:11:37.187579 32758 sgd_solver.cpp:106] Iteration 89700, lr = 0.0025
I0523 07:11:46.445744 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_90000.caffemodel
I0523 07:11:46.507189 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_90000.solverstate
I0523 07:11:46.535611 32758 solver.cpp:341] Iteration 90000, Testing net (#0)
I0523 07:12:34.009356 32758 solver.cpp:409]     Test net output #0: accuracy = 0.884139
I0523 07:12:34.009551 32758 solver.cpp:409]     Test net output #1: loss = 0.398832 (* 1 = 0.398832 loss)
I0523 07:12:54.912726 32758 solver.cpp:237] Iteration 90000, loss = 1.10286
I0523 07:12:54.912780 32758 solver.cpp:253]     Train net output #0: loss = 1.10286 (* 1 = 1.10286 loss)
I0523 07:12:54.912794 32758 sgd_solver.cpp:106] Iteration 90000, lr = 0.0025
I0523 07:13:04.198524 32758 solver.cpp:237] Iteration 90300, loss = 0.937668
I0523 07:13:04.198709 32758 solver.cpp:253]     Train net output #0: loss = 0.937668 (* 1 = 0.937668 loss)
I0523 07:13:04.198724 32758 sgd_solver.cpp:106] Iteration 90300, lr = 0.0025
I0523 07:13:13.481384 32758 solver.cpp:237] Iteration 90600, loss = 1.72098
I0523 07:13:13.481418 32758 solver.cpp:253]     Train net output #0: loss = 1.72098 (* 1 = 1.72098 loss)
I0523 07:13:13.481434 32758 sgd_solver.cpp:106] Iteration 90600, lr = 0.0025
I0523 07:13:22.770448 32758 solver.cpp:237] Iteration 90900, loss = 1.14325
I0523 07:13:22.770496 32758 solver.cpp:253]     Train net output #0: loss = 1.14325 (* 1 = 1.14325 loss)
I0523 07:13:22.770510 32758 sgd_solver.cpp:106] Iteration 90900, lr = 0.0025
I0523 07:13:32.070574 32758 solver.cpp:237] Iteration 91200, loss = 1.09785
I0523 07:13:32.070608 32758 solver.cpp:253]     Train net output #0: loss = 1.09785 (* 1 = 1.09785 loss)
I0523 07:13:32.070624 32758 sgd_solver.cpp:106] Iteration 91200, lr = 0.0025
I0523 07:13:41.366574 32758 solver.cpp:237] Iteration 91500, loss = 1.02712
I0523 07:13:41.366740 32758 solver.cpp:253]     Train net output #0: loss = 1.02712 (* 1 = 1.02712 loss)
I0523 07:13:41.366755 32758 sgd_solver.cpp:106] Iteration 91500, lr = 0.0025
I0523 07:13:50.659138 32758 solver.cpp:237] Iteration 91800, loss = 1.1328
I0523 07:13:50.659184 32758 solver.cpp:253]     Train net output #0: loss = 1.1328 (* 1 = 1.1328 loss)
I0523 07:13:50.659199 32758 sgd_solver.cpp:106] Iteration 91800, lr = 0.0025
I0523 07:14:20.842082 32758 solver.cpp:237] Iteration 92100, loss = 1.28403
I0523 07:14:20.842269 32758 solver.cpp:253]     Train net output #0: loss = 1.28403 (* 1 = 1.28403 loss)
I0523 07:14:20.842285 32758 sgd_solver.cpp:106] Iteration 92100, lr = 0.0025
I0523 07:14:30.126075 32758 solver.cpp:237] Iteration 92400, loss = 0.964139
I0523 07:14:30.126111 32758 solver.cpp:253]     Train net output #0: loss = 0.96414 (* 1 = 0.96414 loss)
I0523 07:14:30.126127 32758 sgd_solver.cpp:106] Iteration 92400, lr = 0.0025
I0523 07:14:39.408215 32758 solver.cpp:237] Iteration 92700, loss = 1.77814
I0523 07:14:39.408254 32758 solver.cpp:253]     Train net output #0: loss = 1.77814 (* 1 = 1.77814 loss)
I0523 07:14:39.408273 32758 sgd_solver.cpp:106] Iteration 92700, lr = 0.0025
I0523 07:14:48.656088 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_93000.caffemodel
I0523 07:14:48.715569 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_93000.solverstate
I0523 07:14:48.751672 32758 solver.cpp:237] Iteration 93000, loss = 1.22776
I0523 07:14:48.751716 32758 solver.cpp:253]     Train net output #0: loss = 1.22776 (* 1 = 1.22776 loss)
I0523 07:14:48.751734 32758 sgd_solver.cpp:106] Iteration 93000, lr = 0.0025
I0523 07:14:58.037353 32758 solver.cpp:237] Iteration 93300, loss = 1.11498
I0523 07:14:58.037524 32758 solver.cpp:253]     Train net output #0: loss = 1.11498 (* 1 = 1.11498 loss)
I0523 07:14:58.037539 32758 sgd_solver.cpp:106] Iteration 93300, lr = 0.0025
I0523 07:15:07.317920 32758 solver.cpp:237] Iteration 93600, loss = 1.21648
I0523 07:15:07.317968 32758 solver.cpp:253]     Train net output #0: loss = 1.21648 (* 1 = 1.21648 loss)
I0523 07:15:07.317984 32758 sgd_solver.cpp:106] Iteration 93600, lr = 0.0025
I0523 07:15:16.596026 32758 solver.cpp:237] Iteration 93900, loss = 1.01702
I0523 07:15:16.596063 32758 solver.cpp:253]     Train net output #0: loss = 1.01702 (* 1 = 1.01702 loss)
I0523 07:15:16.596077 32758 sgd_solver.cpp:106] Iteration 93900, lr = 0.0025
I0523 07:15:46.789857 32758 solver.cpp:237] Iteration 94200, loss = 1.12191
I0523 07:15:46.790056 32758 solver.cpp:253]     Train net output #0: loss = 1.12191 (* 1 = 1.12191 loss)
I0523 07:15:46.790073 32758 sgd_solver.cpp:106] Iteration 94200, lr = 0.0025
I0523 07:15:56.070011 32758 solver.cpp:237] Iteration 94500, loss = 0.907682
I0523 07:15:56.070060 32758 solver.cpp:253]     Train net output #0: loss = 0.907683 (* 1 = 0.907683 loss)
I0523 07:15:56.070073 32758 sgd_solver.cpp:106] Iteration 94500, lr = 0.0025
I0523 07:16:05.348191 32758 solver.cpp:237] Iteration 94800, loss = 1.31507
I0523 07:16:05.348228 32758 solver.cpp:253]     Train net output #0: loss = 1.31507 (* 1 = 1.31507 loss)
I0523 07:16:05.348242 32758 sgd_solver.cpp:106] Iteration 94800, lr = 0.0025
I0523 07:16:14.627616 32758 solver.cpp:237] Iteration 95100, loss = 1.22289
I0523 07:16:14.627651 32758 solver.cpp:253]     Train net output #0: loss = 1.22289 (* 1 = 1.22289 loss)
I0523 07:16:14.627666 32758 sgd_solver.cpp:106] Iteration 95100, lr = 0.0025
I0523 07:16:23.907665 32758 solver.cpp:237] Iteration 95400, loss = 1.34365
I0523 07:16:23.907847 32758 solver.cpp:253]     Train net output #0: loss = 1.34365 (* 1 = 1.34365 loss)
I0523 07:16:23.907862 32758 sgd_solver.cpp:106] Iteration 95400, lr = 0.0025
I0523 07:16:33.189592 32758 solver.cpp:237] Iteration 95700, loss = 1.38934
I0523 07:16:33.189628 32758 solver.cpp:253]     Train net output #0: loss = 1.38934 (* 1 = 1.38934 loss)
I0523 07:16:33.189641 32758 sgd_solver.cpp:106] Iteration 95700, lr = 0.0025
I0523 07:16:42.440546 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_96000.caffemodel
I0523 07:16:42.499831 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_96000.solverstate
I0523 07:16:42.526165 32758 solver.cpp:341] Iteration 96000, Testing net (#0)
I0523 07:17:51.233606 32758 solver.cpp:409]     Test net output #0: accuracy = 0.890105
I0523 07:17:51.233791 32758 solver.cpp:409]     Test net output #1: loss = 0.349965 (* 1 = 0.349965 loss)
I0523 07:18:12.128399 32758 solver.cpp:237] Iteration 96000, loss = 1.12625
I0523 07:18:12.128451 32758 solver.cpp:253]     Train net output #0: loss = 1.12625 (* 1 = 1.12625 loss)
I0523 07:18:12.128466 32758 sgd_solver.cpp:106] Iteration 96000, lr = 0.0025
I0523 07:18:21.408540 32758 solver.cpp:237] Iteration 96300, loss = 1.10114
I0523 07:18:21.408710 32758 solver.cpp:253]     Train net output #0: loss = 1.10114 (* 1 = 1.10114 loss)
I0523 07:18:21.408723 32758 sgd_solver.cpp:106] Iteration 96300, lr = 0.0025
I0523 07:18:30.691784 32758 solver.cpp:237] Iteration 96600, loss = 1.08381
I0523 07:18:30.691824 32758 solver.cpp:253]     Train net output #0: loss = 1.08381 (* 1 = 1.08381 loss)
I0523 07:18:30.691840 32758 sgd_solver.cpp:106] Iteration 96600, lr = 0.0025
I0523 07:18:39.974452 32758 solver.cpp:237] Iteration 96900, loss = 1.15713
I0523 07:18:39.974488 32758 solver.cpp:253]     Train net output #0: loss = 1.15713 (* 1 = 1.15713 loss)
I0523 07:18:39.974503 32758 sgd_solver.cpp:106] Iteration 96900, lr = 0.0025
I0523 07:18:49.255882 32758 solver.cpp:237] Iteration 97200, loss = 1.15203
I0523 07:18:49.255923 32758 solver.cpp:253]     Train net output #0: loss = 1.15204 (* 1 = 1.15204 loss)
I0523 07:18:49.255937 32758 sgd_solver.cpp:106] Iteration 97200, lr = 0.0025
I0523 07:18:58.539338 32758 solver.cpp:237] Iteration 97500, loss = 1.33783
I0523 07:18:58.539527 32758 solver.cpp:253]     Train net output #0: loss = 1.33784 (* 1 = 1.33784 loss)
I0523 07:18:58.539541 32758 sgd_solver.cpp:106] Iteration 97500, lr = 0.0025
I0523 07:19:07.816439 32758 solver.cpp:237] Iteration 97800, loss = 1.09468
I0523 07:19:07.816474 32758 solver.cpp:253]     Train net output #0: loss = 1.09468 (* 1 = 1.09468 loss)
I0523 07:19:07.816489 32758 sgd_solver.cpp:106] Iteration 97800, lr = 0.0025
I0523 07:19:37.999313 32758 solver.cpp:237] Iteration 98100, loss = 1.09979
I0523 07:19:37.999501 32758 solver.cpp:253]     Train net output #0: loss = 1.09979 (* 1 = 1.09979 loss)
I0523 07:19:37.999516 32758 sgd_solver.cpp:106] Iteration 98100, lr = 0.0025
I0523 07:19:47.276768 32758 solver.cpp:237] Iteration 98400, loss = 1.25202
I0523 07:19:47.276803 32758 solver.cpp:253]     Train net output #0: loss = 1.25202 (* 1 = 1.25202 loss)
I0523 07:19:47.276819 32758 sgd_solver.cpp:106] Iteration 98400, lr = 0.0025
I0523 07:19:56.558198 32758 solver.cpp:237] Iteration 98700, loss = 1.20244
I0523 07:19:56.558233 32758 solver.cpp:253]     Train net output #0: loss = 1.20244 (* 1 = 1.20244 loss)
I0523 07:19:56.558248 32758 sgd_solver.cpp:106] Iteration 98700, lr = 0.0025
I0523 07:20:05.808708 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_99000.caffemodel
I0523 07:20:05.868394 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_99000.solverstate
I0523 07:20:05.907420 32758 solver.cpp:237] Iteration 99000, loss = 1.21063
I0523 07:20:05.907466 32758 solver.cpp:253]     Train net output #0: loss = 1.21063 (* 1 = 1.21063 loss)
I0523 07:20:05.907485 32758 sgd_solver.cpp:106] Iteration 99000, lr = 0.0025
I0523 07:20:15.188093 32758 solver.cpp:237] Iteration 99300, loss = 0.851489
I0523 07:20:15.188269 32758 solver.cpp:253]     Train net output #0: loss = 0.85149 (* 1 = 0.85149 loss)
I0523 07:20:15.188282 32758 sgd_solver.cpp:106] Iteration 99300, lr = 0.0025
I0523 07:20:24.463644 32758 solver.cpp:237] Iteration 99600, loss = 1.16216
I0523 07:20:24.463678 32758 solver.cpp:253]     Train net output #0: loss = 1.16216 (* 1 = 1.16216 loss)
I0523 07:20:24.463695 32758 sgd_solver.cpp:106] Iteration 99600, lr = 0.0025
I0523 07:20:33.742182 32758 solver.cpp:237] Iteration 99900, loss = 1.22663
I0523 07:20:33.742223 32758 solver.cpp:253]     Train net output #0: loss = 1.22663 (* 1 = 1.22663 loss)
I0523 07:20:33.742240 32758 sgd_solver.cpp:106] Iteration 99900, lr = 0.0025
I0523 07:21:03.897387 32758 solver.cpp:237] Iteration 100200, loss = 1.32389
I0523 07:21:03.897578 32758 solver.cpp:253]     Train net output #0: loss = 1.32389 (* 1 = 1.32389 loss)
I0523 07:21:03.897591 32758 sgd_solver.cpp:106] Iteration 100200, lr = 0.0025
I0523 07:21:13.175680 32758 solver.cpp:237] Iteration 100500, loss = 1.2457
I0523 07:21:13.175715 32758 solver.cpp:253]     Train net output #0: loss = 1.2457 (* 1 = 1.2457 loss)
I0523 07:21:13.175732 32758 sgd_solver.cpp:106] Iteration 100500, lr = 0.0025
I0523 07:21:22.455899 32758 solver.cpp:237] Iteration 100800, loss = 1.17381
I0523 07:21:22.455948 32758 solver.cpp:253]     Train net output #0: loss = 1.17381 (* 1 = 1.17381 loss)
I0523 07:21:22.455962 32758 sgd_solver.cpp:106] Iteration 100800, lr = 0.0025
I0523 07:21:31.737164 32758 solver.cpp:237] Iteration 101100, loss = 1.20137
I0523 07:21:31.737195 32758 solver.cpp:253]     Train net output #0: loss = 1.20137 (* 1 = 1.20137 loss)
I0523 07:21:31.737210 32758 sgd_solver.cpp:106] Iteration 101100, lr = 0.0025
I0523 07:21:41.013633 32758 solver.cpp:237] Iteration 101400, loss = 1.13673
I0523 07:21:41.013803 32758 solver.cpp:253]     Train net output #0: loss = 1.13673 (* 1 = 1.13673 loss)
I0523 07:21:41.013816 32758 sgd_solver.cpp:106] Iteration 101400, lr = 0.0025
I0523 07:21:50.291402 32758 solver.cpp:237] Iteration 101700, loss = 0.866365
I0523 07:21:50.291450 32758 solver.cpp:253]     Train net output #0: loss = 0.866365 (* 1 = 0.866365 loss)
I0523 07:21:50.291465 32758 sgd_solver.cpp:106] Iteration 101700, lr = 0.0025
I0523 07:21:59.543121 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_102000.caffemodel
I0523 07:21:59.604199 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_102000.solverstate
I0523 07:21:59.631258 32758 solver.cpp:341] Iteration 102000, Testing net (#0)
I0523 07:22:47.474161 32758 solver.cpp:409]     Test net output #0: accuracy = 0.889193
I0523 07:22:47.474370 32758 solver.cpp:409]     Test net output #1: loss = 0.358939 (* 1 = 0.358939 loss)
I0523 07:23:08.360363 32758 solver.cpp:237] Iteration 102000, loss = 1.25289
I0523 07:23:08.360414 32758 solver.cpp:253]     Train net output #0: loss = 1.25289 (* 1 = 1.25289 loss)
I0523 07:23:08.360430 32758 sgd_solver.cpp:106] Iteration 102000, lr = 0.0025
I0523 07:23:17.643216 32758 solver.cpp:237] Iteration 102300, loss = 1.27649
I0523 07:23:17.643391 32758 solver.cpp:253]     Train net output #0: loss = 1.27649 (* 1 = 1.27649 loss)
I0523 07:23:17.643405 32758 sgd_solver.cpp:106] Iteration 102300, lr = 0.0025
I0523 07:23:26.924721 32758 solver.cpp:237] Iteration 102600, loss = 1.34795
I0523 07:23:26.924769 32758 solver.cpp:253]     Train net output #0: loss = 1.34796 (* 1 = 1.34796 loss)
I0523 07:23:26.924783 32758 sgd_solver.cpp:106] Iteration 102600, lr = 0.0025
I0523 07:23:36.203761 32758 solver.cpp:237] Iteration 102900, loss = 0.885569
I0523 07:23:36.203796 32758 solver.cpp:253]     Train net output #0: loss = 0.885569 (* 1 = 0.885569 loss)
I0523 07:23:36.203812 32758 sgd_solver.cpp:106] Iteration 102900, lr = 0.0025
I0523 07:23:45.488206 32758 solver.cpp:237] Iteration 103200, loss = 1.26697
I0523 07:23:45.488241 32758 solver.cpp:253]     Train net output #0: loss = 1.26697 (* 1 = 1.26697 loss)
I0523 07:23:45.488257 32758 sgd_solver.cpp:106] Iteration 103200, lr = 0.0025
I0523 07:23:54.771884 32758 solver.cpp:237] Iteration 103500, loss = 0.922153
I0523 07:23:54.772068 32758 solver.cpp:253]     Train net output #0: loss = 0.922153 (* 1 = 0.922153 loss)
I0523 07:23:54.772081 32758 sgd_solver.cpp:106] Iteration 103500, lr = 0.0025
I0523 07:24:04.053691 32758 solver.cpp:237] Iteration 103800, loss = 1.15414
I0523 07:24:04.053725 32758 solver.cpp:253]     Train net output #0: loss = 1.15414 (* 1 = 1.15414 loss)
I0523 07:24:04.053740 32758 sgd_solver.cpp:106] Iteration 103800, lr = 0.0025
I0523 07:24:34.197820 32758 solver.cpp:237] Iteration 104100, loss = 0.923391
I0523 07:24:34.198011 32758 solver.cpp:253]     Train net output #0: loss = 0.923391 (* 1 = 0.923391 loss)
I0523 07:24:34.198029 32758 sgd_solver.cpp:106] Iteration 104100, lr = 0.0025
I0523 07:24:43.474234 32758 solver.cpp:237] Iteration 104400, loss = 0.910601
I0523 07:24:43.474277 32758 solver.cpp:253]     Train net output #0: loss = 0.910601 (* 1 = 0.910601 loss)
I0523 07:24:43.474292 32758 sgd_solver.cpp:106] Iteration 104400, lr = 0.0025
I0523 07:24:52.753865 32758 solver.cpp:237] Iteration 104700, loss = 1.19156
I0523 07:24:52.753901 32758 solver.cpp:253]     Train net output #0: loss = 1.19156 (* 1 = 1.19156 loss)
I0523 07:24:52.753913 32758 sgd_solver.cpp:106] Iteration 104700, lr = 0.0025
I0523 07:25:02.002991 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_105000.caffemodel
I0523 07:25:02.063966 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_105000.solverstate
I0523 07:25:02.101644 32758 solver.cpp:237] Iteration 105000, loss = 1.23296
I0523 07:25:02.101689 32758 solver.cpp:253]     Train net output #0: loss = 1.23296 (* 1 = 1.23296 loss)
I0523 07:25:02.101708 32758 sgd_solver.cpp:106] Iteration 105000, lr = 0.0025
I0523 07:25:11.384394 32758 solver.cpp:237] Iteration 105300, loss = 1.17208
I0523 07:25:11.384588 32758 solver.cpp:253]     Train net output #0: loss = 1.17208 (* 1 = 1.17208 loss)
I0523 07:25:11.384603 32758 sgd_solver.cpp:106] Iteration 105300, lr = 0.0025
I0523 07:25:20.665530 32758 solver.cpp:237] Iteration 105600, loss = 1.05177
I0523 07:25:20.665565 32758 solver.cpp:253]     Train net output #0: loss = 1.05177 (* 1 = 1.05177 loss)
I0523 07:25:20.665580 32758 sgd_solver.cpp:106] Iteration 105600, lr = 0.0025
I0523 07:25:29.945076 32758 solver.cpp:237] Iteration 105900, loss = 1.07744
I0523 07:25:29.945127 32758 solver.cpp:253]     Train net output #0: loss = 1.07745 (* 1 = 1.07745 loss)
I0523 07:25:29.945142 32758 sgd_solver.cpp:106] Iteration 105900, lr = 0.0025
I0523 07:26:00.139201 32758 solver.cpp:237] Iteration 106200, loss = 1.17
I0523 07:26:00.139389 32758 solver.cpp:253]     Train net output #0: loss = 1.17 (* 1 = 1.17 loss)
I0523 07:26:00.139405 32758 sgd_solver.cpp:106] Iteration 106200, lr = 0.0025
I0523 07:26:09.419682 32758 solver.cpp:237] Iteration 106500, loss = 1.13423
I0523 07:26:09.419716 32758 solver.cpp:253]     Train net output #0: loss = 1.13423 (* 1 = 1.13423 loss)
I0523 07:26:09.419734 32758 sgd_solver.cpp:106] Iteration 106500, lr = 0.0025
I0523 07:26:18.696635 32758 solver.cpp:237] Iteration 106800, loss = 1.15931
I0523 07:26:18.696678 32758 solver.cpp:253]     Train net output #0: loss = 1.15931 (* 1 = 1.15931 loss)
I0523 07:26:18.696696 32758 sgd_solver.cpp:106] Iteration 106800, lr = 0.0025
I0523 07:26:27.978606 32758 solver.cpp:237] Iteration 107100, loss = 1.08567
I0523 07:26:27.978642 32758 solver.cpp:253]     Train net output #0: loss = 1.08567 (* 1 = 1.08567 loss)
I0523 07:26:27.978657 32758 sgd_solver.cpp:106] Iteration 107100, lr = 0.0025
I0523 07:26:37.259610 32758 solver.cpp:237] Iteration 107400, loss = 1.20531
I0523 07:26:37.259788 32758 solver.cpp:253]     Train net output #0: loss = 1.20531 (* 1 = 1.20531 loss)
I0523 07:26:37.259801 32758 sgd_solver.cpp:106] Iteration 107400, lr = 0.0025
I0523 07:26:46.539469 32758 solver.cpp:237] Iteration 107700, loss = 0.935032
I0523 07:26:46.539513 32758 solver.cpp:253]     Train net output #0: loss = 0.935033 (* 1 = 0.935033 loss)
I0523 07:26:46.539528 32758 sgd_solver.cpp:106] Iteration 107700, lr = 0.0025
I0523 07:26:55.789368 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_108000.caffemodel
I0523 07:26:55.850391 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_108000.solverstate
I0523 07:26:55.877349 32758 solver.cpp:341] Iteration 108000, Testing net (#0)
I0523 07:28:04.524757 32758 solver.cpp:409]     Test net output #0: accuracy = 0.887613
I0523 07:28:04.524947 32758 solver.cpp:409]     Test net output #1: loss = 0.348186 (* 1 = 0.348186 loss)
I0523 07:28:25.413614 32758 solver.cpp:237] Iteration 108000, loss = 0.935573
I0523 07:28:25.413667 32758 solver.cpp:253]     Train net output #0: loss = 0.935574 (* 1 = 0.935574 loss)
I0523 07:28:25.413683 32758 sgd_solver.cpp:106] Iteration 108000, lr = 0.0025
I0523 07:28:34.693792 32758 solver.cpp:237] Iteration 108300, loss = 1.12981
I0523 07:28:34.693970 32758 solver.cpp:253]     Train net output #0: loss = 1.12981 (* 1 = 1.12981 loss)
I0523 07:28:34.693984 32758 sgd_solver.cpp:106] Iteration 108300, lr = 0.0025
I0523 07:28:43.973770 32758 solver.cpp:237] Iteration 108600, loss = 1.42755
I0523 07:28:43.973804 32758 solver.cpp:253]     Train net output #0: loss = 1.42755 (* 1 = 1.42755 loss)
I0523 07:28:43.973820 32758 sgd_solver.cpp:106] Iteration 108600, lr = 0.0025
I0523 07:28:53.253738 32758 solver.cpp:237] Iteration 108900, loss = 1.11352
I0523 07:28:53.253783 32758 solver.cpp:253]     Train net output #0: loss = 1.11352 (* 1 = 1.11352 loss)
I0523 07:28:53.253799 32758 sgd_solver.cpp:106] Iteration 108900, lr = 0.0025
I0523 07:29:02.534634 32758 solver.cpp:237] Iteration 109200, loss = 1.25392
I0523 07:29:02.534669 32758 solver.cpp:253]     Train net output #0: loss = 1.25392 (* 1 = 1.25392 loss)
I0523 07:29:02.534684 32758 sgd_solver.cpp:106] Iteration 109200, lr = 0.0025
I0523 07:29:11.817831 32758 solver.cpp:237] Iteration 109500, loss = 1.28371
I0523 07:29:11.818013 32758 solver.cpp:253]     Train net output #0: loss = 1.28371 (* 1 = 1.28371 loss)
I0523 07:29:11.818027 32758 sgd_solver.cpp:106] Iteration 109500, lr = 0.0025
I0523 07:29:21.097901 32758 solver.cpp:237] Iteration 109800, loss = 1.1216
I0523 07:29:21.097942 32758 solver.cpp:253]     Train net output #0: loss = 1.1216 (* 1 = 1.1216 loss)
I0523 07:29:21.097959 32758 sgd_solver.cpp:106] Iteration 109800, lr = 0.0025
I0523 07:29:51.233654 32758 solver.cpp:237] Iteration 110100, loss = 0.896389
I0523 07:29:51.233850 32758 solver.cpp:253]     Train net output #0: loss = 0.896389 (* 1 = 0.896389 loss)
I0523 07:29:51.233865 32758 sgd_solver.cpp:106] Iteration 110100, lr = 0.0025
I0523 07:30:00.514091 32758 solver.cpp:237] Iteration 110400, loss = 1.03293
I0523 07:30:00.514125 32758 solver.cpp:253]     Train net output #0: loss = 1.03293 (* 1 = 1.03293 loss)
I0523 07:30:00.514142 32758 sgd_solver.cpp:106] Iteration 110400, lr = 0.0025
I0523 07:30:09.794978 32758 solver.cpp:237] Iteration 110700, loss = 1.16367
I0523 07:30:09.795025 32758 solver.cpp:253]     Train net output #0: loss = 1.16367 (* 1 = 1.16367 loss)
I0523 07:30:09.795038 32758 sgd_solver.cpp:106] Iteration 110700, lr = 0.0025
I0523 07:30:19.046823 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_111000.caffemodel
I0523 07:30:19.106068 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_111000.solverstate
I0523 07:30:19.141252 32758 solver.cpp:237] Iteration 111000, loss = 1.35109
I0523 07:30:19.141294 32758 solver.cpp:253]     Train net output #0: loss = 1.35109 (* 1 = 1.35109 loss)
I0523 07:30:19.141309 32758 sgd_solver.cpp:106] Iteration 111000, lr = 0.0025
I0523 07:30:28.423969 32758 solver.cpp:237] Iteration 111300, loss = 1.0746
I0523 07:30:28.424149 32758 solver.cpp:253]     Train net output #0: loss = 1.0746 (* 1 = 1.0746 loss)
I0523 07:30:28.424163 32758 sgd_solver.cpp:106] Iteration 111300, lr = 0.0025
I0523 07:30:37.706658 32758 solver.cpp:237] Iteration 111600, loss = 1.56959
I0523 07:30:37.706694 32758 solver.cpp:253]     Train net output #0: loss = 1.56959 (* 1 = 1.56959 loss)
I0523 07:30:37.706709 32758 sgd_solver.cpp:106] Iteration 111600, lr = 0.0025
I0523 07:30:46.988734 32758 solver.cpp:237] Iteration 111900, loss = 1.33282
I0523 07:30:46.988770 32758 solver.cpp:253]     Train net output #0: loss = 1.33282 (* 1 = 1.33282 loss)
I0523 07:30:46.988785 32758 sgd_solver.cpp:106] Iteration 111900, lr = 0.0025
I0523 07:31:17.166797 32758 solver.cpp:237] Iteration 112200, loss = 1.21626
I0523 07:31:17.195433 32758 solver.cpp:253]     Train net output #0: loss = 1.21626 (* 1 = 1.21626 loss)
I0523 07:31:17.195451 32758 sgd_solver.cpp:106] Iteration 112200, lr = 0.0025
I0523 07:31:26.454851 32758 solver.cpp:237] Iteration 112500, loss = 1.05714
I0523 07:31:26.454888 32758 solver.cpp:253]     Train net output #0: loss = 1.05714 (* 1 = 1.05714 loss)
I0523 07:31:26.454901 32758 sgd_solver.cpp:106] Iteration 112500, lr = 0.0025
I0523 07:31:35.737084 32758 solver.cpp:237] Iteration 112800, loss = 1.21229
I0523 07:31:35.737120 32758 solver.cpp:253]     Train net output #0: loss = 1.21229 (* 1 = 1.21229 loss)
I0523 07:31:35.737133 32758 sgd_solver.cpp:106] Iteration 112800, lr = 0.0025
I0523 07:31:45.020231 32758 solver.cpp:237] Iteration 113100, loss = 1.37638
I0523 07:31:45.020273 32758 solver.cpp:253]     Train net output #0: loss = 1.37638 (* 1 = 1.37638 loss)
I0523 07:31:45.020290 32758 sgd_solver.cpp:106] Iteration 113100, lr = 0.0025
I0523 07:31:54.301893 32758 solver.cpp:237] Iteration 113400, loss = 1.13207
I0523 07:31:54.302075 32758 solver.cpp:253]     Train net output #0: loss = 1.13207 (* 1 = 1.13207 loss)
I0523 07:31:54.302089 32758 sgd_solver.cpp:106] Iteration 113400, lr = 0.0025
I0523 07:32:03.581758 32758 solver.cpp:237] Iteration 113700, loss = 1.31944
I0523 07:32:03.581794 32758 solver.cpp:253]     Train net output #0: loss = 1.31945 (* 1 = 1.31945 loss)
I0523 07:32:03.581809 32758 sgd_solver.cpp:106] Iteration 113700, lr = 0.0025
I0523 07:32:12.831720 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_114000.caffemodel
I0523 07:32:12.891667 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_114000.solverstate
I0523 07:32:12.917062 32758 solver.cpp:341] Iteration 114000, Testing net (#0)
I0523 07:33:00.409972 32758 solver.cpp:409]     Test net output #0: accuracy = 0.892997
I0523 07:33:00.410163 32758 solver.cpp:409]     Test net output #1: loss = 0.358275 (* 1 = 0.358275 loss)
I0523 07:33:21.342500 32758 solver.cpp:237] Iteration 114000, loss = 1.16511
I0523 07:33:21.342553 32758 solver.cpp:253]     Train net output #0: loss = 1.16511 (* 1 = 1.16511 loss)
I0523 07:33:21.342569 32758 sgd_solver.cpp:106] Iteration 114000, lr = 0.0025
I0523 07:33:30.625418 32758 solver.cpp:237] Iteration 114300, loss = 1.48783
I0523 07:33:30.625600 32758 solver.cpp:253]     Train net output #0: loss = 1.48783 (* 1 = 1.48783 loss)
I0523 07:33:30.625614 32758 sgd_solver.cpp:106] Iteration 114300, lr = 0.0025
I0523 07:33:39.905617 32758 solver.cpp:237] Iteration 114600, loss = 1.09264
I0523 07:33:39.905653 32758 solver.cpp:253]     Train net output #0: loss = 1.09264 (* 1 = 1.09264 loss)
I0523 07:33:39.905668 32758 sgd_solver.cpp:106] Iteration 114600, lr = 0.0025
I0523 07:33:49.189291 32758 solver.cpp:237] Iteration 114900, loss = 1.1427
I0523 07:33:49.189333 32758 solver.cpp:253]     Train net output #0: loss = 1.1427 (* 1 = 1.1427 loss)
I0523 07:33:49.189348 32758 sgd_solver.cpp:106] Iteration 114900, lr = 0.0025
I0523 07:33:58.469070 32758 solver.cpp:237] Iteration 115200, loss = 1.37825
I0523 07:33:58.469106 32758 solver.cpp:253]     Train net output #0: loss = 1.37825 (* 1 = 1.37825 loss)
I0523 07:33:58.469121 32758 sgd_solver.cpp:106] Iteration 115200, lr = 0.0025
I0523 07:34:07.747972 32758 solver.cpp:237] Iteration 115500, loss = 1.02471
I0523 07:34:07.748148 32758 solver.cpp:253]     Train net output #0: loss = 1.02471 (* 1 = 1.02471 loss)
I0523 07:34:07.748162 32758 sgd_solver.cpp:106] Iteration 115500, lr = 0.0025
I0523 07:34:17.029920 32758 solver.cpp:237] Iteration 115800, loss = 1.11713
I0523 07:34:17.029966 32758 solver.cpp:253]     Train net output #0: loss = 1.11713 (* 1 = 1.11713 loss)
I0523 07:34:17.029980 32758 sgd_solver.cpp:106] Iteration 115800, lr = 0.0025
I0523 07:34:47.218130 32758 solver.cpp:237] Iteration 116100, loss = 1.22727
I0523 07:34:47.218322 32758 solver.cpp:253]     Train net output #0: loss = 1.22727 (* 1 = 1.22727 loss)
I0523 07:34:47.218338 32758 sgd_solver.cpp:106] Iteration 116100, lr = 0.0025
I0523 07:34:56.503149 32758 solver.cpp:237] Iteration 116400, loss = 1.13272
I0523 07:34:56.503185 32758 solver.cpp:253]     Train net output #0: loss = 1.13272 (* 1 = 1.13272 loss)
I0523 07:34:56.503201 32758 sgd_solver.cpp:106] Iteration 116400, lr = 0.0025
I0523 07:35:05.785401 32758 solver.cpp:237] Iteration 116700, loss = 1.18701
I0523 07:35:05.785447 32758 solver.cpp:253]     Train net output #0: loss = 1.18701 (* 1 = 1.18701 loss)
I0523 07:35:05.785465 32758 sgd_solver.cpp:106] Iteration 116700, lr = 0.0025
I0523 07:35:15.036425 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_117000.caffemodel
I0523 07:35:15.097345 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_117000.solverstate
I0523 07:35:15.133780 32758 solver.cpp:237] Iteration 117000, loss = 1.06671
I0523 07:35:15.133828 32758 solver.cpp:253]     Train net output #0: loss = 1.06671 (* 1 = 1.06671 loss)
I0523 07:35:15.133843 32758 sgd_solver.cpp:106] Iteration 117000, lr = 0.0025
I0523 07:35:24.415727 32758 solver.cpp:237] Iteration 117300, loss = 0.882529
I0523 07:35:24.415915 32758 solver.cpp:253]     Train net output #0: loss = 0.882529 (* 1 = 0.882529 loss)
I0523 07:35:24.415930 32758 sgd_solver.cpp:106] Iteration 117300, lr = 0.0025
I0523 07:35:33.696372 32758 solver.cpp:237] Iteration 117600, loss = 1.13665
I0523 07:35:33.696420 32758 solver.cpp:253]     Train net output #0: loss = 1.13665 (* 1 = 1.13665 loss)
I0523 07:35:33.696435 32758 sgd_solver.cpp:106] Iteration 117600, lr = 0.0025
I0523 07:35:42.976891 32758 solver.cpp:237] Iteration 117900, loss = 1.04073
I0523 07:35:42.976927 32758 solver.cpp:253]     Train net output #0: loss = 1.04073 (* 1 = 1.04073 loss)
I0523 07:35:42.976941 32758 sgd_solver.cpp:106] Iteration 117900, lr = 0.0025
I0523 07:36:13.145126 32758 solver.cpp:237] Iteration 118200, loss = 1.13386
I0523 07:36:13.145324 32758 solver.cpp:253]     Train net output #0: loss = 1.13386 (* 1 = 1.13386 loss)
I0523 07:36:13.145341 32758 sgd_solver.cpp:106] Iteration 118200, lr = 0.0025
I0523 07:36:22.427842 32758 solver.cpp:237] Iteration 118500, loss = 1.10163
I0523 07:36:22.427889 32758 solver.cpp:253]     Train net output #0: loss = 1.10163 (* 1 = 1.10163 loss)
I0523 07:36:22.427906 32758 sgd_solver.cpp:106] Iteration 118500, lr = 0.0025
I0523 07:36:31.705790 32758 solver.cpp:237] Iteration 118800, loss = 1.19866
I0523 07:36:31.705826 32758 solver.cpp:253]     Train net output #0: loss = 1.19866 (* 1 = 1.19866 loss)
I0523 07:36:31.705842 32758 sgd_solver.cpp:106] Iteration 118800, lr = 0.0025
I0523 07:36:40.988649 32758 solver.cpp:237] Iteration 119100, loss = 1.33258
I0523 07:36:40.988685 32758 solver.cpp:253]     Train net output #0: loss = 1.33258 (* 1 = 1.33258 loss)
I0523 07:36:40.988699 32758 sgd_solver.cpp:106] Iteration 119100, lr = 0.0025
I0523 07:36:50.267762 32758 solver.cpp:237] Iteration 119400, loss = 1.10497
I0523 07:36:50.267942 32758 solver.cpp:253]     Train net output #0: loss = 1.10497 (* 1 = 1.10497 loss)
I0523 07:36:50.267956 32758 sgd_solver.cpp:106] Iteration 119400, lr = 0.0025
I0523 07:36:59.550216 32758 solver.cpp:237] Iteration 119700, loss = 0.986831
I0523 07:36:59.550252 32758 solver.cpp:253]     Train net output #0: loss = 0.986831 (* 1 = 0.986831 loss)
I0523 07:36:59.550267 32758 sgd_solver.cpp:106] Iteration 119700, lr = 0.0025
I0523 07:37:08.804412 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_120000.caffemodel
I0523 07:37:08.866225 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_120000.solverstate
I0523 07:37:08.893681 32758 solver.cpp:341] Iteration 120000, Testing net (#0)
I0523 07:38:17.600183 32758 solver.cpp:409]     Test net output #0: accuracy = 0.884187
I0523 07:38:17.600373 32758 solver.cpp:409]     Test net output #1: loss = 0.380004 (* 1 = 0.380004 loss)
I0523 07:38:38.471940 32758 solver.cpp:237] Iteration 120000, loss = 1.28804
I0523 07:38:38.471992 32758 solver.cpp:253]     Train net output #0: loss = 1.28804 (* 1 = 1.28804 loss)
I0523 07:38:38.472007 32758 sgd_solver.cpp:106] Iteration 120000, lr = 0.0025
I0523 07:38:47.751869 32758 solver.cpp:237] Iteration 120300, loss = 0.855303
I0523 07:38:47.752053 32758 solver.cpp:253]     Train net output #0: loss = 0.855304 (* 1 = 0.855304 loss)
I0523 07:38:47.752066 32758 sgd_solver.cpp:106] Iteration 120300, lr = 0.0025
I0523 07:38:57.034250 32758 solver.cpp:237] Iteration 120600, loss = 1.15874
I0523 07:38:57.034284 32758 solver.cpp:253]     Train net output #0: loss = 1.15874 (* 1 = 1.15874 loss)
I0523 07:38:57.034301 32758 sgd_solver.cpp:106] Iteration 120600, lr = 0.0025
I0523 07:39:06.315448 32758 solver.cpp:237] Iteration 120900, loss = 1.01978
I0523 07:39:06.315481 32758 solver.cpp:253]     Train net output #0: loss = 1.01978 (* 1 = 1.01978 loss)
I0523 07:39:06.315498 32758 sgd_solver.cpp:106] Iteration 120900, lr = 0.0025
I0523 07:39:15.593720 32758 solver.cpp:237] Iteration 121200, loss = 1.17454
I0523 07:39:15.593758 32758 solver.cpp:253]     Train net output #0: loss = 1.17454 (* 1 = 1.17454 loss)
I0523 07:39:15.593778 32758 sgd_solver.cpp:106] Iteration 121200, lr = 0.0025
I0523 07:39:24.872635 32758 solver.cpp:237] Iteration 121500, loss = 1.11328
I0523 07:39:24.872819 32758 solver.cpp:253]     Train net output #0: loss = 1.11328 (* 1 = 1.11328 loss)
I0523 07:39:24.872833 32758 sgd_solver.cpp:106] Iteration 121500, lr = 0.0025
I0523 07:39:34.154713 32758 solver.cpp:237] Iteration 121800, loss = 1.13125
I0523 07:39:34.154747 32758 solver.cpp:253]     Train net output #0: loss = 1.13125 (* 1 = 1.13125 loss)
I0523 07:39:34.154763 32758 sgd_solver.cpp:106] Iteration 121800, lr = 0.0025
I0523 07:40:04.325353 32758 solver.cpp:237] Iteration 122100, loss = 1.38106
I0523 07:40:04.325557 32758 solver.cpp:253]     Train net output #0: loss = 1.38106 (* 1 = 1.38106 loss)
I0523 07:40:04.325574 32758 sgd_solver.cpp:106] Iteration 122100, lr = 0.0025
I0523 07:40:13.604027 32758 solver.cpp:237] Iteration 122400, loss = 1.33443
I0523 07:40:13.604061 32758 solver.cpp:253]     Train net output #0: loss = 1.33444 (* 1 = 1.33444 loss)
I0523 07:40:13.604077 32758 sgd_solver.cpp:106] Iteration 122400, lr = 0.0025
I0523 07:40:22.882738 32758 solver.cpp:237] Iteration 122700, loss = 1.81496
I0523 07:40:22.882774 32758 solver.cpp:253]     Train net output #0: loss = 1.81496 (* 1 = 1.81496 loss)
I0523 07:40:22.882789 32758 sgd_solver.cpp:106] Iteration 122700, lr = 0.0025
I0523 07:40:32.133287 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_123000.caffemodel
I0523 07:40:32.192718 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_123000.solverstate
I0523 07:40:32.227744 32758 solver.cpp:237] Iteration 123000, loss = 0.996633
I0523 07:40:32.227792 32758 solver.cpp:253]     Train net output #0: loss = 0.996633 (* 1 = 0.996633 loss)
I0523 07:40:32.227807 32758 sgd_solver.cpp:106] Iteration 123000, lr = 0.0025
I0523 07:40:41.508287 32758 solver.cpp:237] Iteration 123300, loss = 1.20182
I0523 07:40:41.508466 32758 solver.cpp:253]     Train net output #0: loss = 1.20182 (* 1 = 1.20182 loss)
I0523 07:40:41.508479 32758 sgd_solver.cpp:106] Iteration 123300, lr = 0.0025
I0523 07:40:50.788815 32758 solver.cpp:237] Iteration 123600, loss = 0.929612
I0523 07:40:50.788849 32758 solver.cpp:253]     Train net output #0: loss = 0.929612 (* 1 = 0.929612 loss)
I0523 07:40:50.788866 32758 sgd_solver.cpp:106] Iteration 123600, lr = 0.0025
I0523 07:41:00.066596 32758 solver.cpp:237] Iteration 123900, loss = 0.955353
I0523 07:41:00.066642 32758 solver.cpp:253]     Train net output #0: loss = 0.955353 (* 1 = 0.955353 loss)
I0523 07:41:00.066658 32758 sgd_solver.cpp:106] Iteration 123900, lr = 0.0025
I0523 07:41:30.231521 32758 solver.cpp:237] Iteration 124200, loss = 1.00272
I0523 07:41:30.231717 32758 solver.cpp:253]     Train net output #0: loss = 1.00272 (* 1 = 1.00272 loss)
I0523 07:41:30.231732 32758 sgd_solver.cpp:106] Iteration 124200, lr = 0.0025
I0523 07:41:39.507196 32758 solver.cpp:237] Iteration 124500, loss = 1.18781
I0523 07:41:39.507231 32758 solver.cpp:253]     Train net output #0: loss = 1.18781 (* 1 = 1.18781 loss)
I0523 07:41:39.507247 32758 sgd_solver.cpp:106] Iteration 124500, lr = 0.0025
I0523 07:41:48.785873 32758 solver.cpp:237] Iteration 124800, loss = 1.2288
I0523 07:41:48.785918 32758 solver.cpp:253]     Train net output #0: loss = 1.2288 (* 1 = 1.2288 loss)
I0523 07:41:48.785935 32758 sgd_solver.cpp:106] Iteration 124800, lr = 0.0025
I0523 07:41:58.065951 32758 solver.cpp:237] Iteration 125100, loss = 0.91263
I0523 07:41:58.065987 32758 solver.cpp:253]     Train net output #0: loss = 0.91263 (* 1 = 0.91263 loss)
I0523 07:41:58.066002 32758 sgd_solver.cpp:106] Iteration 125100, lr = 0.0025
I0523 07:42:07.348922 32758 solver.cpp:237] Iteration 125400, loss = 0.942534
I0523 07:42:07.349107 32758 solver.cpp:253]     Train net output #0: loss = 0.942535 (* 1 = 0.942535 loss)
I0523 07:42:07.349123 32758 sgd_solver.cpp:106] Iteration 125400, lr = 0.0025
I0523 07:42:16.632510 32758 solver.cpp:237] Iteration 125700, loss = 1.16735
I0523 07:42:16.632545 32758 solver.cpp:253]     Train net output #0: loss = 1.16735 (* 1 = 1.16735 loss)
I0523 07:42:16.632560 32758 sgd_solver.cpp:106] Iteration 125700, lr = 0.0025
I0523 07:42:25.881592 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_126000.caffemodel
I0523 07:42:25.941282 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_126000.solverstate
I0523 07:42:25.967103 32758 solver.cpp:341] Iteration 126000, Testing net (#0)
I0523 07:43:13.784875 32758 solver.cpp:409]     Test net output #0: accuracy = 0.895306
I0523 07:43:13.785073 32758 solver.cpp:409]     Test net output #1: loss = 0.346928 (* 1 = 0.346928 loss)
I0523 07:43:34.649865 32758 solver.cpp:237] Iteration 126000, loss = 1.44991
I0523 07:43:34.649917 32758 solver.cpp:253]     Train net output #0: loss = 1.44991 (* 1 = 1.44991 loss)
I0523 07:43:34.649933 32758 sgd_solver.cpp:106] Iteration 126000, lr = 0.0025
I0523 07:43:43.926751 32758 solver.cpp:237] Iteration 126300, loss = 1.14313
I0523 07:43:43.926930 32758 solver.cpp:253]     Train net output #0: loss = 1.14313 (* 1 = 1.14313 loss)
I0523 07:43:43.926944 32758 sgd_solver.cpp:106] Iteration 126300, lr = 0.0025
I0523 07:43:53.208230 32758 solver.cpp:237] Iteration 126600, loss = 0.940592
I0523 07:43:53.208274 32758 solver.cpp:253]     Train net output #0: loss = 0.940592 (* 1 = 0.940592 loss)
I0523 07:43:53.208289 32758 sgd_solver.cpp:106] Iteration 126600, lr = 0.0025
I0523 07:44:02.489573 32758 solver.cpp:237] Iteration 126900, loss = 1.19008
I0523 07:44:02.489609 32758 solver.cpp:253]     Train net output #0: loss = 1.19008 (* 1 = 1.19008 loss)
I0523 07:44:02.489624 32758 sgd_solver.cpp:106] Iteration 126900, lr = 0.0025
I0523 07:44:11.767277 32758 solver.cpp:237] Iteration 127200, loss = 1.49021
I0523 07:44:11.767313 32758 solver.cpp:253]     Train net output #0: loss = 1.49021 (* 1 = 1.49021 loss)
I0523 07:44:11.767326 32758 sgd_solver.cpp:106] Iteration 127200, lr = 0.0025
I0523 07:44:21.049705 32758 solver.cpp:237] Iteration 127500, loss = 1.25774
I0523 07:44:21.049895 32758 solver.cpp:253]     Train net output #0: loss = 1.25775 (* 1 = 1.25775 loss)
I0523 07:44:21.049909 32758 sgd_solver.cpp:106] Iteration 127500, lr = 0.0025
I0523 07:44:30.329816 32758 solver.cpp:237] Iteration 127800, loss = 1.15088
I0523 07:44:30.329851 32758 solver.cpp:253]     Train net output #0: loss = 1.15088 (* 1 = 1.15088 loss)
I0523 07:44:30.329867 32758 sgd_solver.cpp:106] Iteration 127800, lr = 0.0025
I0523 07:45:00.511335 32758 solver.cpp:237] Iteration 128100, loss = 1.38272
I0523 07:45:00.511531 32758 solver.cpp:253]     Train net output #0: loss = 1.38272 (* 1 = 1.38272 loss)
I0523 07:45:00.511548 32758 sgd_solver.cpp:106] Iteration 128100, lr = 0.0025
I0523 07:45:09.793303 32758 solver.cpp:237] Iteration 128400, loss = 1.06271
I0523 07:45:09.793346 32758 solver.cpp:253]     Train net output #0: loss = 1.06271 (* 1 = 1.06271 loss)
I0523 07:45:09.793362 32758 sgd_solver.cpp:106] Iteration 128400, lr = 0.0025
I0523 07:45:19.073215 32758 solver.cpp:237] Iteration 128700, loss = 1.23822
I0523 07:45:19.073251 32758 solver.cpp:253]     Train net output #0: loss = 1.23822 (* 1 = 1.23822 loss)
I0523 07:45:19.073266 32758 sgd_solver.cpp:106] Iteration 128700, lr = 0.0025
I0523 07:45:28.323614 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_129000.caffemodel
I0523 07:45:28.382936 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_129000.solverstate
I0523 07:45:28.418248 32758 solver.cpp:237] Iteration 129000, loss = 1.02638
I0523 07:45:28.418292 32758 solver.cpp:253]     Train net output #0: loss = 1.02638 (* 1 = 1.02638 loss)
I0523 07:45:28.418309 32758 sgd_solver.cpp:106] Iteration 129000, lr = 0.0025
I0523 07:45:37.699149 32758 solver.cpp:237] Iteration 129300, loss = 1.05534
I0523 07:45:37.699342 32758 solver.cpp:253]     Train net output #0: loss = 1.05534 (* 1 = 1.05534 loss)
I0523 07:45:37.699355 32758 sgd_solver.cpp:106] Iteration 129300, lr = 0.0025
I0523 07:45:46.980219 32758 solver.cpp:237] Iteration 129600, loss = 1.25597
I0523 07:45:46.980254 32758 solver.cpp:253]     Train net output #0: loss = 1.25597 (* 1 = 1.25597 loss)
I0523 07:45:46.980270 32758 sgd_solver.cpp:106] Iteration 129600, lr = 0.0025
I0523 07:45:56.256763 32758 solver.cpp:237] Iteration 129900, loss = 1.05729
I0523 07:45:56.256811 32758 solver.cpp:253]     Train net output #0: loss = 1.05729 (* 1 = 1.05729 loss)
I0523 07:45:56.256826 32758 sgd_solver.cpp:106] Iteration 129900, lr = 0.0025
I0523 07:46:26.415336 32758 solver.cpp:237] Iteration 130200, loss = 1.11669
I0523 07:46:26.415539 32758 solver.cpp:253]     Train net output #0: loss = 1.11669 (* 1 = 1.11669 loss)
I0523 07:46:26.415552 32758 sgd_solver.cpp:106] Iteration 130200, lr = 0.0025
I0523 07:46:35.694313 32758 solver.cpp:237] Iteration 130500, loss = 1.35018
I0523 07:46:35.694346 32758 solver.cpp:253]     Train net output #0: loss = 1.35018 (* 1 = 1.35018 loss)
I0523 07:46:35.694361 32758 sgd_solver.cpp:106] Iteration 130500, lr = 0.0025
I0523 07:46:44.976752 32758 solver.cpp:237] Iteration 130800, loss = 1.20583
I0523 07:46:44.976795 32758 solver.cpp:253]     Train net output #0: loss = 1.20583 (* 1 = 1.20583 loss)
I0523 07:46:44.976812 32758 sgd_solver.cpp:106] Iteration 130800, lr = 0.0025
I0523 07:46:54.262754 32758 solver.cpp:237] Iteration 131100, loss = 1.10088
I0523 07:46:54.262789 32758 solver.cpp:253]     Train net output #0: loss = 1.10088 (* 1 = 1.10088 loss)
I0523 07:46:54.262805 32758 sgd_solver.cpp:106] Iteration 131100, lr = 0.0025
I0523 07:47:03.537767 32758 solver.cpp:237] Iteration 131400, loss = 1.17552
I0523 07:47:03.537945 32758 solver.cpp:253]     Train net output #0: loss = 1.17552 (* 1 = 1.17552 loss)
I0523 07:47:03.537958 32758 sgd_solver.cpp:106] Iteration 131400, lr = 0.0025
I0523 07:47:12.818615 32758 solver.cpp:237] Iteration 131700, loss = 1.10223
I0523 07:47:12.818663 32758 solver.cpp:253]     Train net output #0: loss = 1.10224 (* 1 = 1.10224 loss)
I0523 07:47:12.818677 32758 sgd_solver.cpp:106] Iteration 131700, lr = 0.0025
I0523 07:47:22.066987 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_132000.caffemodel
I0523 07:47:22.127842 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_132000.solverstate
I0523 07:47:22.154808 32758 solver.cpp:341] Iteration 132000, Testing net (#0)
I0523 07:48:30.843070 32758 solver.cpp:409]     Test net output #0: accuracy = 0.893826
I0523 07:48:30.843282 32758 solver.cpp:409]     Test net output #1: loss = 0.342649 (* 1 = 0.342649 loss)
I0523 07:48:51.730617 32758 solver.cpp:237] Iteration 132000, loss = 1.04193
I0523 07:48:51.730669 32758 solver.cpp:253]     Train net output #0: loss = 1.04193 (* 1 = 1.04193 loss)
I0523 07:48:51.730684 32758 sgd_solver.cpp:106] Iteration 132000, lr = 0.0025
I0523 07:49:01.013314 32758 solver.cpp:237] Iteration 132300, loss = 1.13463
I0523 07:49:01.013499 32758 solver.cpp:253]     Train net output #0: loss = 1.13463 (* 1 = 1.13463 loss)
I0523 07:49:01.013511 32758 sgd_solver.cpp:106] Iteration 132300, lr = 0.0025
I0523 07:49:10.294764 32758 solver.cpp:237] Iteration 132600, loss = 1.16319
I0523 07:49:10.294798 32758 solver.cpp:253]     Train net output #0: loss = 1.16319 (* 1 = 1.16319 loss)
I0523 07:49:10.294814 32758 sgd_solver.cpp:106] Iteration 132600, lr = 0.0025
I0523 07:49:19.577551 32758 solver.cpp:237] Iteration 132900, loss = 1.4022
I0523 07:49:19.577589 32758 solver.cpp:253]     Train net output #0: loss = 1.4022 (* 1 = 1.4022 loss)
I0523 07:49:19.577601 32758 sgd_solver.cpp:106] Iteration 132900, lr = 0.0025
I0523 07:49:28.860033 32758 solver.cpp:237] Iteration 133200, loss = 1.23952
I0523 07:49:28.860069 32758 solver.cpp:253]     Train net output #0: loss = 1.23952 (* 1 = 1.23952 loss)
I0523 07:49:28.860083 32758 sgd_solver.cpp:106] Iteration 133200, lr = 0.0025
I0523 07:49:38.142052 32758 solver.cpp:237] Iteration 133500, loss = 1.07177
I0523 07:49:38.142242 32758 solver.cpp:253]     Train net output #0: loss = 1.07177 (* 1 = 1.07177 loss)
I0523 07:49:38.142256 32758 sgd_solver.cpp:106] Iteration 133500, lr = 0.0025
I0523 07:49:47.424108 32758 solver.cpp:237] Iteration 133800, loss = 1.21117
I0523 07:49:47.424142 32758 solver.cpp:253]     Train net output #0: loss = 1.21117 (* 1 = 1.21117 loss)
I0523 07:49:47.424159 32758 sgd_solver.cpp:106] Iteration 133800, lr = 0.0025
I0523 07:50:17.572242 32758 solver.cpp:237] Iteration 134100, loss = 1.00799
I0523 07:50:17.572441 32758 solver.cpp:253]     Train net output #0: loss = 1.00799 (* 1 = 1.00799 loss)
I0523 07:50:17.572456 32758 sgd_solver.cpp:106] Iteration 134100, lr = 0.0025
I0523 07:50:26.850412 32758 solver.cpp:237] Iteration 134400, loss = 0.890264
I0523 07:50:26.850447 32758 solver.cpp:253]     Train net output #0: loss = 0.890265 (* 1 = 0.890265 loss)
I0523 07:50:26.850462 32758 sgd_solver.cpp:106] Iteration 134400, lr = 0.0025
I0523 07:50:36.131549 32758 solver.cpp:237] Iteration 134700, loss = 1.22095
I0523 07:50:36.131595 32758 solver.cpp:253]     Train net output #0: loss = 1.22095 (* 1 = 1.22095 loss)
I0523 07:50:36.131610 32758 sgd_solver.cpp:106] Iteration 134700, lr = 0.0025
I0523 07:50:45.383994 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_135000.caffemodel
I0523 07:50:45.445099 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_135000.solverstate
I0523 07:50:45.482313 32758 solver.cpp:237] Iteration 135000, loss = 1.24462
I0523 07:50:45.482363 32758 solver.cpp:253]     Train net output #0: loss = 1.24462 (* 1 = 1.24462 loss)
I0523 07:50:45.482380 32758 sgd_solver.cpp:106] Iteration 135000, lr = 0.0025
I0523 07:50:54.763391 32758 solver.cpp:237] Iteration 135300, loss = 1.23201
I0523 07:50:54.763581 32758 solver.cpp:253]     Train net output #0: loss = 1.23201 (* 1 = 1.23201 loss)
I0523 07:50:54.763595 32758 sgd_solver.cpp:106] Iteration 135300, lr = 0.0025
I0523 07:51:04.043854 32758 solver.cpp:237] Iteration 135600, loss = 1.37528
I0523 07:51:04.043889 32758 solver.cpp:253]     Train net output #0: loss = 1.37528 (* 1 = 1.37528 loss)
I0523 07:51:04.043905 32758 sgd_solver.cpp:106] Iteration 135600, lr = 0.0025
I0523 07:51:13.322891 32758 solver.cpp:237] Iteration 135900, loss = 1.49011
I0523 07:51:13.322926 32758 solver.cpp:253]     Train net output #0: loss = 1.49011 (* 1 = 1.49011 loss)
I0523 07:51:13.322942 32758 sgd_solver.cpp:106] Iteration 135900, lr = 0.0025
I0523 07:51:43.510475 32758 solver.cpp:237] Iteration 136200, loss = 0.998356
I0523 07:51:43.510686 32758 solver.cpp:253]     Train net output #0: loss = 0.998357 (* 1 = 0.998357 loss)
I0523 07:51:43.510704 32758 sgd_solver.cpp:106] Iteration 136200, lr = 0.0025
I0523 07:51:52.793113 32758 solver.cpp:237] Iteration 136500, loss = 0.981045
I0523 07:51:52.793149 32758 solver.cpp:253]     Train net output #0: loss = 0.981046 (* 1 = 0.981046 loss)
I0523 07:51:52.793164 32758 sgd_solver.cpp:106] Iteration 136500, lr = 0.0025
I0523 07:52:02.076633 32758 solver.cpp:237] Iteration 136800, loss = 1.18677
I0523 07:52:02.076670 32758 solver.cpp:253]     Train net output #0: loss = 1.18677 (* 1 = 1.18677 loss)
I0523 07:52:02.076685 32758 sgd_solver.cpp:106] Iteration 136800, lr = 0.0025
I0523 07:52:11.357694 32758 solver.cpp:237] Iteration 137100, loss = 1.05451
I0523 07:52:11.357738 32758 solver.cpp:253]     Train net output #0: loss = 1.05451 (* 1 = 1.05451 loss)
I0523 07:52:11.357755 32758 sgd_solver.cpp:106] Iteration 137100, lr = 0.0025
I0523 07:52:20.638888 32758 solver.cpp:237] Iteration 137400, loss = 1.23554
I0523 07:52:20.639068 32758 solver.cpp:253]     Train net output #0: loss = 1.23554 (* 1 = 1.23554 loss)
I0523 07:52:20.639081 32758 sgd_solver.cpp:106] Iteration 137400, lr = 0.0025
I0523 07:52:29.917389 32758 solver.cpp:237] Iteration 137700, loss = 0.912276
I0523 07:52:29.917425 32758 solver.cpp:253]     Train net output #0: loss = 0.912276 (* 1 = 0.912276 loss)
I0523 07:52:29.917439 32758 sgd_solver.cpp:106] Iteration 137700, lr = 0.0025
I0523 07:52:39.174052 32758 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_138000.caffemodel
I0523 07:52:39.233736 32758 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_50_lr_0.0025_2016-05-20T15.49.06.322924_iter_138000.solverstate
I0523 07:52:39.259642 32758 solver.cpp:341] Iteration 138000, Testing net (#0)
I0523 07:53:26.739313 32758 solver.cpp:409]     Test net output #0: accuracy = 0.894271
I0523 07:53:26.739522 32758 solver.cpp:409]     Test net output #1: loss = 0.329549 (* 1 = 0.329549 loss)
I0523 07:53:47.633038 32758 solver.cpp:237] Iteration 138000, loss = 1.09683
I0523 07:53:47.633092 32758 solver.cpp:253]     Train net output #0: loss = 1.09683 (* 1 = 1.09683 loss)
I0523 07:53:47.633106 32758 sgd_solver.cpp:106] Iteration 138000, lr = 0.0025
I0523 07:53:56.912645 32758 solver.cpp:237] Iteration 138300, loss = 0.905937
I0523 07:53:56.912833 32758 solver.cpp:253]     Train net output #0: loss = 0.905937 (* 1 = 0.905937 loss)
I0523 07:53:56.912847 32758 sgd_solver.cpp:106] Iteration 138300, lr = 0.0025
I0523 07:54:06.191035 32758 solver.cpp:237] Iteration 138600, loss = 0.932295
I0523 07:54:06.191069 32758 solver.cpp:253]     Train net output #0: loss = 0.932295 (* 1 = 0.932295 loss)
I0523 07:54:06.191084 32758 sgd_solver.cpp:106] Iteration 138600, lr = 0.0025
I0523 07:54:15.474655 32758 solver.cpp:237] Iteration 138900, loss = 0.955853
I0523 07:54:15.474706 32758 solver.cpp:253]     Train net output #0: loss = 0.955854 (* 1 = 0.955854 loss)
I0523 07:54:15.474720 32758 sgd_solver.cpp:106] Iteration 138900, lr = 0.0025
I0523 07:54:24.754292 32758 solver.cpp:237] Iteration 139200, loss = 1.05809
I0523 07:54:24.754328 32758 solver.cpp:253]     Train net output #0: loss = 1.05809 (* 1 = 1.05809 loss)
I0523 07:54:24.754343 32758 sgd_solver.cpp:106] Iteration 139200, lr = 0.0025
aprun: Apid 11253180: Caught signal Terminated, sending to application
*** Aborted at 1464004467 (unix time) try "date -d @1464004467" if you are using GNU date ***
PC: @     0x2aaaaaaca834 ([vdso]+0x833)
*** SIGTERM (@0x7ff3) received by PID 32758 (TID 0x2aaac746f900) from PID 32755; stack trace: ***
    @     0x2aaab7c78850 (unknown)
    @     0x2aaaaaaca834 ([vdso]+0x833)
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @     0x2aaab82072d0 maybe_syscall_gettime_cpu
=>> PBS: job killed: walltime 7208 exceeded limit 7200
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @     0x2aaab82074b0 __GI_clock_gettime
    @     0x2aaab9898f3e (unknown)
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @     0x2aaab928ec5b (unknown)
    @     0x2aaab926d723 (unknown)
    @     0x2aaab92655e1 (unknown)
    @     0x2aaab9266356 (unknown)
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @     0x2aaab91d5562 (unknown)
    @     0x2aaab91d56ba (unknown)
    @     0x2aaab91b8715 cuMemcpy
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @     0x2aaaaacf9e92 (unknown)
    @     0x2aaaaacde306 (unknown)
    @     0x2aaaaad00328 cudaMemcpy
    @           0x4d6a10 caffe::caffe_copy<>()
    @           0x626ea9 caffe::HDF5DataLayer<>::Forward_gpu()
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
    @           0x5ca109 caffe::Solver<>::Step()
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
    @           0x43020c main
aprun: Apid 11253180: Caught signal Terminated, sending to application
    @     0x2aaab7ea4c36 __libc_start_main
    @           0x438669 (unknown)
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
aprun: Apid 11253180: Caught signal Terminated, sending to application
