2813287
I0527 11:16:39.188870 21372 caffe.cpp:184] Using GPUs 0
I0527 11:16:39.614483 21372 solver.cpp:48] Initializing solver from parameters: 
test_iter: 3750
test_interval: 7500
base_lr: 0.003
display: 375
max_iter: 375000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 3750
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511.prototxt"
I0527 11:16:39.616598 21372 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511.prototxt
I0527 11:16:39.628574 21372 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0527 11:16:39.628633 21372 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0527 11:16:39.628979 21372 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 40
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0527 11:16:39.629160 21372 layer_factory.hpp:77] Creating layer data_hdf5
I0527 11:16:39.629184 21372 net.cpp:106] Creating Layer data_hdf5
I0527 11:16:39.629199 21372 net.cpp:411] data_hdf5 -> data
I0527 11:16:39.629232 21372 net.cpp:411] data_hdf5 -> label
I0527 11:16:39.629266 21372 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0527 11:16:39.642973 21372 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0527 11:16:39.656337 21372 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0527 11:17:01.267233 21372 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0527 11:17:01.272435 21372 net.cpp:150] Setting up data_hdf5
I0527 11:17:01.272480 21372 net.cpp:157] Top shape: 40 1 127 50 (254000)
I0527 11:17:01.272495 21372 net.cpp:157] Top shape: 40 (40)
I0527 11:17:01.272505 21372 net.cpp:165] Memory required for data: 1016160
I0527 11:17:01.272518 21372 layer_factory.hpp:77] Creating layer conv1
I0527 11:17:01.272552 21372 net.cpp:106] Creating Layer conv1
I0527 11:17:01.272563 21372 net.cpp:454] conv1 <- data
I0527 11:17:01.272585 21372 net.cpp:411] conv1 -> conv1
I0527 11:17:02.993803 21372 net.cpp:150] Setting up conv1
I0527 11:17:02.993851 21372 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0527 11:17:02.993862 21372 net.cpp:165] Memory required for data: 12075360
I0527 11:17:02.993892 21372 layer_factory.hpp:77] Creating layer relu1
I0527 11:17:02.993914 21372 net.cpp:106] Creating Layer relu1
I0527 11:17:02.993926 21372 net.cpp:454] relu1 <- conv1
I0527 11:17:02.993939 21372 net.cpp:397] relu1 -> conv1 (in-place)
I0527 11:17:02.994480 21372 net.cpp:150] Setting up relu1
I0527 11:17:02.994498 21372 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0527 11:17:02.994508 21372 net.cpp:165] Memory required for data: 23134560
I0527 11:17:02.994519 21372 layer_factory.hpp:77] Creating layer pool1
I0527 11:17:02.994536 21372 net.cpp:106] Creating Layer pool1
I0527 11:17:02.994546 21372 net.cpp:454] pool1 <- conv1
I0527 11:17:02.994560 21372 net.cpp:411] pool1 -> pool1
I0527 11:17:02.994642 21372 net.cpp:150] Setting up pool1
I0527 11:17:02.994655 21372 net.cpp:157] Top shape: 40 12 60 48 (1382400)
I0527 11:17:02.994665 21372 net.cpp:165] Memory required for data: 28664160
I0527 11:17:02.994676 21372 layer_factory.hpp:77] Creating layer conv2
I0527 11:17:02.994699 21372 net.cpp:106] Creating Layer conv2
I0527 11:17:02.994709 21372 net.cpp:454] conv2 <- pool1
I0527 11:17:02.994721 21372 net.cpp:411] conv2 -> conv2
I0527 11:17:02.997403 21372 net.cpp:150] Setting up conv2
I0527 11:17:02.997432 21372 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0527 11:17:02.997442 21372 net.cpp:165] Memory required for data: 36612960
I0527 11:17:02.997462 21372 layer_factory.hpp:77] Creating layer relu2
I0527 11:17:02.997478 21372 net.cpp:106] Creating Layer relu2
I0527 11:17:02.997488 21372 net.cpp:454] relu2 <- conv2
I0527 11:17:02.997500 21372 net.cpp:397] relu2 -> conv2 (in-place)
I0527 11:17:02.997831 21372 net.cpp:150] Setting up relu2
I0527 11:17:02.997845 21372 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0527 11:17:02.997855 21372 net.cpp:165] Memory required for data: 44561760
I0527 11:17:02.997866 21372 layer_factory.hpp:77] Creating layer pool2
I0527 11:17:02.997879 21372 net.cpp:106] Creating Layer pool2
I0527 11:17:02.997889 21372 net.cpp:454] pool2 <- conv2
I0527 11:17:02.997901 21372 net.cpp:411] pool2 -> pool2
I0527 11:17:02.997983 21372 net.cpp:150] Setting up pool2
I0527 11:17:02.997997 21372 net.cpp:157] Top shape: 40 20 27 46 (993600)
I0527 11:17:02.998006 21372 net.cpp:165] Memory required for data: 48536160
I0527 11:17:02.998014 21372 layer_factory.hpp:77] Creating layer conv3
I0527 11:17:02.998034 21372 net.cpp:106] Creating Layer conv3
I0527 11:17:02.998044 21372 net.cpp:454] conv3 <- pool2
I0527 11:17:02.998056 21372 net.cpp:411] conv3 -> conv3
I0527 11:17:02.999994 21372 net.cpp:150] Setting up conv3
I0527 11:17:03.000017 21372 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0527 11:17:03.000030 21372 net.cpp:165] Memory required for data: 52872800
I0527 11:17:03.000048 21372 layer_factory.hpp:77] Creating layer relu3
I0527 11:17:03.000064 21372 net.cpp:106] Creating Layer relu3
I0527 11:17:03.000074 21372 net.cpp:454] relu3 <- conv3
I0527 11:17:03.000097 21372 net.cpp:397] relu3 -> conv3 (in-place)
I0527 11:17:03.000567 21372 net.cpp:150] Setting up relu3
I0527 11:17:03.000584 21372 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0527 11:17:03.000596 21372 net.cpp:165] Memory required for data: 57209440
I0527 11:17:03.000605 21372 layer_factory.hpp:77] Creating layer pool3
I0527 11:17:03.000618 21372 net.cpp:106] Creating Layer pool3
I0527 11:17:03.000628 21372 net.cpp:454] pool3 <- conv3
I0527 11:17:03.000641 21372 net.cpp:411] pool3 -> pool3
I0527 11:17:03.000710 21372 net.cpp:150] Setting up pool3
I0527 11:17:03.000723 21372 net.cpp:157] Top shape: 40 28 11 44 (542080)
I0527 11:17:03.000733 21372 net.cpp:165] Memory required for data: 59377760
I0527 11:17:03.000741 21372 layer_factory.hpp:77] Creating layer conv4
I0527 11:17:03.000758 21372 net.cpp:106] Creating Layer conv4
I0527 11:17:03.000769 21372 net.cpp:454] conv4 <- pool3
I0527 11:17:03.000783 21372 net.cpp:411] conv4 -> conv4
I0527 11:17:03.003559 21372 net.cpp:150] Setting up conv4
I0527 11:17:03.003587 21372 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0527 11:17:03.003598 21372 net.cpp:165] Memory required for data: 60829280
I0527 11:17:03.003614 21372 layer_factory.hpp:77] Creating layer relu4
I0527 11:17:03.003628 21372 net.cpp:106] Creating Layer relu4
I0527 11:17:03.003638 21372 net.cpp:454] relu4 <- conv4
I0527 11:17:03.003653 21372 net.cpp:397] relu4 -> conv4 (in-place)
I0527 11:17:03.004140 21372 net.cpp:150] Setting up relu4
I0527 11:17:03.004156 21372 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0527 11:17:03.004168 21372 net.cpp:165] Memory required for data: 62280800
I0527 11:17:03.004179 21372 layer_factory.hpp:77] Creating layer pool4
I0527 11:17:03.004191 21372 net.cpp:106] Creating Layer pool4
I0527 11:17:03.004201 21372 net.cpp:454] pool4 <- conv4
I0527 11:17:03.004215 21372 net.cpp:411] pool4 -> pool4
I0527 11:17:03.004284 21372 net.cpp:150] Setting up pool4
I0527 11:17:03.004298 21372 net.cpp:157] Top shape: 40 36 3 42 (181440)
I0527 11:17:03.004308 21372 net.cpp:165] Memory required for data: 63006560
I0527 11:17:03.004319 21372 layer_factory.hpp:77] Creating layer ip1
I0527 11:17:03.004339 21372 net.cpp:106] Creating Layer ip1
I0527 11:17:03.004350 21372 net.cpp:454] ip1 <- pool4
I0527 11:17:03.004364 21372 net.cpp:411] ip1 -> ip1
I0527 11:17:03.019824 21372 net.cpp:150] Setting up ip1
I0527 11:17:03.019853 21372 net.cpp:157] Top shape: 40 196 (7840)
I0527 11:17:03.019870 21372 net.cpp:165] Memory required for data: 63037920
I0527 11:17:03.019896 21372 layer_factory.hpp:77] Creating layer relu5
I0527 11:17:03.019911 21372 net.cpp:106] Creating Layer relu5
I0527 11:17:03.019922 21372 net.cpp:454] relu5 <- ip1
I0527 11:17:03.019934 21372 net.cpp:397] relu5 -> ip1 (in-place)
I0527 11:17:03.020287 21372 net.cpp:150] Setting up relu5
I0527 11:17:03.020300 21372 net.cpp:157] Top shape: 40 196 (7840)
I0527 11:17:03.020311 21372 net.cpp:165] Memory required for data: 63069280
I0527 11:17:03.020321 21372 layer_factory.hpp:77] Creating layer drop1
I0527 11:17:03.020344 21372 net.cpp:106] Creating Layer drop1
I0527 11:17:03.020354 21372 net.cpp:454] drop1 <- ip1
I0527 11:17:03.020367 21372 net.cpp:397] drop1 -> ip1 (in-place)
I0527 11:17:03.020428 21372 net.cpp:150] Setting up drop1
I0527 11:17:03.020442 21372 net.cpp:157] Top shape: 40 196 (7840)
I0527 11:17:03.020452 21372 net.cpp:165] Memory required for data: 63100640
I0527 11:17:03.020462 21372 layer_factory.hpp:77] Creating layer ip2
I0527 11:17:03.020481 21372 net.cpp:106] Creating Layer ip2
I0527 11:17:03.020491 21372 net.cpp:454] ip2 <- ip1
I0527 11:17:03.020505 21372 net.cpp:411] ip2 -> ip2
I0527 11:17:03.020968 21372 net.cpp:150] Setting up ip2
I0527 11:17:03.020982 21372 net.cpp:157] Top shape: 40 98 (3920)
I0527 11:17:03.020992 21372 net.cpp:165] Memory required for data: 63116320
I0527 11:17:03.021006 21372 layer_factory.hpp:77] Creating layer relu6
I0527 11:17:03.021019 21372 net.cpp:106] Creating Layer relu6
I0527 11:17:03.021029 21372 net.cpp:454] relu6 <- ip2
I0527 11:17:03.021041 21372 net.cpp:397] relu6 -> ip2 (in-place)
I0527 11:17:03.021558 21372 net.cpp:150] Setting up relu6
I0527 11:17:03.021574 21372 net.cpp:157] Top shape: 40 98 (3920)
I0527 11:17:03.021585 21372 net.cpp:165] Memory required for data: 63132000
I0527 11:17:03.021595 21372 layer_factory.hpp:77] Creating layer drop2
I0527 11:17:03.021610 21372 net.cpp:106] Creating Layer drop2
I0527 11:17:03.021618 21372 net.cpp:454] drop2 <- ip2
I0527 11:17:03.021631 21372 net.cpp:397] drop2 -> ip2 (in-place)
I0527 11:17:03.021673 21372 net.cpp:150] Setting up drop2
I0527 11:17:03.021687 21372 net.cpp:157] Top shape: 40 98 (3920)
I0527 11:17:03.021697 21372 net.cpp:165] Memory required for data: 63147680
I0527 11:17:03.021708 21372 layer_factory.hpp:77] Creating layer ip3
I0527 11:17:03.021721 21372 net.cpp:106] Creating Layer ip3
I0527 11:17:03.021730 21372 net.cpp:454] ip3 <- ip2
I0527 11:17:03.021744 21372 net.cpp:411] ip3 -> ip3
I0527 11:17:03.021955 21372 net.cpp:150] Setting up ip3
I0527 11:17:03.021968 21372 net.cpp:157] Top shape: 40 11 (440)
I0527 11:17:03.021978 21372 net.cpp:165] Memory required for data: 63149440
I0527 11:17:03.021993 21372 layer_factory.hpp:77] Creating layer drop3
I0527 11:17:03.022006 21372 net.cpp:106] Creating Layer drop3
I0527 11:17:03.022016 21372 net.cpp:454] drop3 <- ip3
I0527 11:17:03.022028 21372 net.cpp:397] drop3 -> ip3 (in-place)
I0527 11:17:03.022068 21372 net.cpp:150] Setting up drop3
I0527 11:17:03.022080 21372 net.cpp:157] Top shape: 40 11 (440)
I0527 11:17:03.022090 21372 net.cpp:165] Memory required for data: 63151200
I0527 11:17:03.022100 21372 layer_factory.hpp:77] Creating layer loss
I0527 11:17:03.022120 21372 net.cpp:106] Creating Layer loss
I0527 11:17:03.022130 21372 net.cpp:454] loss <- ip3
I0527 11:17:03.022141 21372 net.cpp:454] loss <- label
I0527 11:17:03.022155 21372 net.cpp:411] loss -> loss
I0527 11:17:03.022171 21372 layer_factory.hpp:77] Creating layer loss
I0527 11:17:03.022814 21372 net.cpp:150] Setting up loss
I0527 11:17:03.022835 21372 net.cpp:157] Top shape: (1)
I0527 11:17:03.022847 21372 net.cpp:160]     with loss weight 1
I0527 11:17:03.022892 21372 net.cpp:165] Memory required for data: 63151204
I0527 11:17:03.022904 21372 net.cpp:226] loss needs backward computation.
I0527 11:17:03.022915 21372 net.cpp:226] drop3 needs backward computation.
I0527 11:17:03.022923 21372 net.cpp:226] ip3 needs backward computation.
I0527 11:17:03.022934 21372 net.cpp:226] drop2 needs backward computation.
I0527 11:17:03.022944 21372 net.cpp:226] relu6 needs backward computation.
I0527 11:17:03.022954 21372 net.cpp:226] ip2 needs backward computation.
I0527 11:17:03.022965 21372 net.cpp:226] drop1 needs backward computation.
I0527 11:17:03.022975 21372 net.cpp:226] relu5 needs backward computation.
I0527 11:17:03.022984 21372 net.cpp:226] ip1 needs backward computation.
I0527 11:17:03.022994 21372 net.cpp:226] pool4 needs backward computation.
I0527 11:17:03.023005 21372 net.cpp:226] relu4 needs backward computation.
I0527 11:17:03.023015 21372 net.cpp:226] conv4 needs backward computation.
I0527 11:17:03.023025 21372 net.cpp:226] pool3 needs backward computation.
I0527 11:17:03.023036 21372 net.cpp:226] relu3 needs backward computation.
I0527 11:17:03.023046 21372 net.cpp:226] conv3 needs backward computation.
I0527 11:17:03.023064 21372 net.cpp:226] pool2 needs backward computation.
I0527 11:17:03.023075 21372 net.cpp:226] relu2 needs backward computation.
I0527 11:17:03.023087 21372 net.cpp:226] conv2 needs backward computation.
I0527 11:17:03.023097 21372 net.cpp:226] pool1 needs backward computation.
I0527 11:17:03.023107 21372 net.cpp:226] relu1 needs backward computation.
I0527 11:17:03.023118 21372 net.cpp:226] conv1 needs backward computation.
I0527 11:17:03.023128 21372 net.cpp:228] data_hdf5 does not need backward computation.
I0527 11:17:03.023138 21372 net.cpp:270] This network produces output loss
I0527 11:17:03.023161 21372 net.cpp:283] Network initialization done.
I0527 11:17:03.024762 21372 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511.prototxt
I0527 11:17:03.024834 21372 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0527 11:17:03.025188 21372 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 40
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0527 11:17:03.025385 21372 layer_factory.hpp:77] Creating layer data_hdf5
I0527 11:17:03.025400 21372 net.cpp:106] Creating Layer data_hdf5
I0527 11:17:03.025413 21372 net.cpp:411] data_hdf5 -> data
I0527 11:17:03.025429 21372 net.cpp:411] data_hdf5 -> label
I0527 11:17:03.025445 21372 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0527 11:17:03.037443 21372 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0527 11:17:24.406633 21372 net.cpp:150] Setting up data_hdf5
I0527 11:17:24.406801 21372 net.cpp:157] Top shape: 40 1 127 50 (254000)
I0527 11:17:24.406816 21372 net.cpp:157] Top shape: 40 (40)
I0527 11:17:24.406827 21372 net.cpp:165] Memory required for data: 1016160
I0527 11:17:24.406841 21372 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0527 11:17:24.406868 21372 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0527 11:17:24.406879 21372 net.cpp:454] label_data_hdf5_1_split <- label
I0527 11:17:24.406894 21372 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0527 11:17:24.406915 21372 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0527 11:17:24.406987 21372 net.cpp:150] Setting up label_data_hdf5_1_split
I0527 11:17:24.407001 21372 net.cpp:157] Top shape: 40 (40)
I0527 11:17:24.407013 21372 net.cpp:157] Top shape: 40 (40)
I0527 11:17:24.407023 21372 net.cpp:165] Memory required for data: 1016480
I0527 11:17:24.407033 21372 layer_factory.hpp:77] Creating layer conv1
I0527 11:17:24.407055 21372 net.cpp:106] Creating Layer conv1
I0527 11:17:24.407066 21372 net.cpp:454] conv1 <- data
I0527 11:17:24.407081 21372 net.cpp:411] conv1 -> conv1
I0527 11:17:24.409009 21372 net.cpp:150] Setting up conv1
I0527 11:17:24.409034 21372 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0527 11:17:24.409046 21372 net.cpp:165] Memory required for data: 12075680
I0527 11:17:24.409066 21372 layer_factory.hpp:77] Creating layer relu1
I0527 11:17:24.409081 21372 net.cpp:106] Creating Layer relu1
I0527 11:17:24.409090 21372 net.cpp:454] relu1 <- conv1
I0527 11:17:24.409103 21372 net.cpp:397] relu1 -> conv1 (in-place)
I0527 11:17:24.409598 21372 net.cpp:150] Setting up relu1
I0527 11:17:24.409615 21372 net.cpp:157] Top shape: 40 12 120 48 (2764800)
I0527 11:17:24.409626 21372 net.cpp:165] Memory required for data: 23134880
I0527 11:17:24.409636 21372 layer_factory.hpp:77] Creating layer pool1
I0527 11:17:24.409651 21372 net.cpp:106] Creating Layer pool1
I0527 11:17:24.409662 21372 net.cpp:454] pool1 <- conv1
I0527 11:17:24.409674 21372 net.cpp:411] pool1 -> pool1
I0527 11:17:24.409749 21372 net.cpp:150] Setting up pool1
I0527 11:17:24.409764 21372 net.cpp:157] Top shape: 40 12 60 48 (1382400)
I0527 11:17:24.409773 21372 net.cpp:165] Memory required for data: 28664480
I0527 11:17:24.409783 21372 layer_factory.hpp:77] Creating layer conv2
I0527 11:17:24.409801 21372 net.cpp:106] Creating Layer conv2
I0527 11:17:24.409811 21372 net.cpp:454] conv2 <- pool1
I0527 11:17:24.409826 21372 net.cpp:411] conv2 -> conv2
I0527 11:17:24.411727 21372 net.cpp:150] Setting up conv2
I0527 11:17:24.411749 21372 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0527 11:17:24.411762 21372 net.cpp:165] Memory required for data: 36613280
I0527 11:17:24.411780 21372 layer_factory.hpp:77] Creating layer relu2
I0527 11:17:24.411794 21372 net.cpp:106] Creating Layer relu2
I0527 11:17:24.411804 21372 net.cpp:454] relu2 <- conv2
I0527 11:17:24.411816 21372 net.cpp:397] relu2 -> conv2 (in-place)
I0527 11:17:24.412159 21372 net.cpp:150] Setting up relu2
I0527 11:17:24.412173 21372 net.cpp:157] Top shape: 40 20 54 46 (1987200)
I0527 11:17:24.412184 21372 net.cpp:165] Memory required for data: 44562080
I0527 11:17:24.412194 21372 layer_factory.hpp:77] Creating layer pool2
I0527 11:17:24.412207 21372 net.cpp:106] Creating Layer pool2
I0527 11:17:24.412217 21372 net.cpp:454] pool2 <- conv2
I0527 11:17:24.412230 21372 net.cpp:411] pool2 -> pool2
I0527 11:17:24.412302 21372 net.cpp:150] Setting up pool2
I0527 11:17:24.412315 21372 net.cpp:157] Top shape: 40 20 27 46 (993600)
I0527 11:17:24.412325 21372 net.cpp:165] Memory required for data: 48536480
I0527 11:17:24.412335 21372 layer_factory.hpp:77] Creating layer conv3
I0527 11:17:24.412353 21372 net.cpp:106] Creating Layer conv3
I0527 11:17:24.412364 21372 net.cpp:454] conv3 <- pool2
I0527 11:17:24.412377 21372 net.cpp:411] conv3 -> conv3
I0527 11:17:24.414356 21372 net.cpp:150] Setting up conv3
I0527 11:17:24.414376 21372 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0527 11:17:24.414386 21372 net.cpp:165] Memory required for data: 52873120
I0527 11:17:24.414418 21372 layer_factory.hpp:77] Creating layer relu3
I0527 11:17:24.414433 21372 net.cpp:106] Creating Layer relu3
I0527 11:17:24.414443 21372 net.cpp:454] relu3 <- conv3
I0527 11:17:24.414456 21372 net.cpp:397] relu3 -> conv3 (in-place)
I0527 11:17:24.414934 21372 net.cpp:150] Setting up relu3
I0527 11:17:24.414952 21372 net.cpp:157] Top shape: 40 28 22 44 (1084160)
I0527 11:17:24.414961 21372 net.cpp:165] Memory required for data: 57209760
I0527 11:17:24.414971 21372 layer_factory.hpp:77] Creating layer pool3
I0527 11:17:24.414984 21372 net.cpp:106] Creating Layer pool3
I0527 11:17:24.414995 21372 net.cpp:454] pool3 <- conv3
I0527 11:17:24.415009 21372 net.cpp:411] pool3 -> pool3
I0527 11:17:24.415079 21372 net.cpp:150] Setting up pool3
I0527 11:17:24.415093 21372 net.cpp:157] Top shape: 40 28 11 44 (542080)
I0527 11:17:24.415103 21372 net.cpp:165] Memory required for data: 59378080
I0527 11:17:24.415112 21372 layer_factory.hpp:77] Creating layer conv4
I0527 11:17:24.415129 21372 net.cpp:106] Creating Layer conv4
I0527 11:17:24.415140 21372 net.cpp:454] conv4 <- pool3
I0527 11:17:24.415154 21372 net.cpp:411] conv4 -> conv4
I0527 11:17:24.417217 21372 net.cpp:150] Setting up conv4
I0527 11:17:24.417240 21372 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0527 11:17:24.417253 21372 net.cpp:165] Memory required for data: 60829600
I0527 11:17:24.417268 21372 layer_factory.hpp:77] Creating layer relu4
I0527 11:17:24.417281 21372 net.cpp:106] Creating Layer relu4
I0527 11:17:24.417292 21372 net.cpp:454] relu4 <- conv4
I0527 11:17:24.417305 21372 net.cpp:397] relu4 -> conv4 (in-place)
I0527 11:17:24.417773 21372 net.cpp:150] Setting up relu4
I0527 11:17:24.417788 21372 net.cpp:157] Top shape: 40 36 6 42 (362880)
I0527 11:17:24.417799 21372 net.cpp:165] Memory required for data: 62281120
I0527 11:17:24.417809 21372 layer_factory.hpp:77] Creating layer pool4
I0527 11:17:24.417822 21372 net.cpp:106] Creating Layer pool4
I0527 11:17:24.417832 21372 net.cpp:454] pool4 <- conv4
I0527 11:17:24.417845 21372 net.cpp:411] pool4 -> pool4
I0527 11:17:24.417917 21372 net.cpp:150] Setting up pool4
I0527 11:17:24.417930 21372 net.cpp:157] Top shape: 40 36 3 42 (181440)
I0527 11:17:24.417940 21372 net.cpp:165] Memory required for data: 63006880
I0527 11:17:24.417948 21372 layer_factory.hpp:77] Creating layer ip1
I0527 11:17:24.417964 21372 net.cpp:106] Creating Layer ip1
I0527 11:17:24.417975 21372 net.cpp:454] ip1 <- pool4
I0527 11:17:24.417989 21372 net.cpp:411] ip1 -> ip1
I0527 11:17:24.433382 21372 net.cpp:150] Setting up ip1
I0527 11:17:24.433411 21372 net.cpp:157] Top shape: 40 196 (7840)
I0527 11:17:24.433423 21372 net.cpp:165] Memory required for data: 63038240
I0527 11:17:24.433445 21372 layer_factory.hpp:77] Creating layer relu5
I0527 11:17:24.433460 21372 net.cpp:106] Creating Layer relu5
I0527 11:17:24.433470 21372 net.cpp:454] relu5 <- ip1
I0527 11:17:24.433483 21372 net.cpp:397] relu5 -> ip1 (in-place)
I0527 11:17:24.433832 21372 net.cpp:150] Setting up relu5
I0527 11:17:24.433846 21372 net.cpp:157] Top shape: 40 196 (7840)
I0527 11:17:24.433856 21372 net.cpp:165] Memory required for data: 63069600
I0527 11:17:24.433867 21372 layer_factory.hpp:77] Creating layer drop1
I0527 11:17:24.433887 21372 net.cpp:106] Creating Layer drop1
I0527 11:17:24.433897 21372 net.cpp:454] drop1 <- ip1
I0527 11:17:24.433909 21372 net.cpp:397] drop1 -> ip1 (in-place)
I0527 11:17:24.433954 21372 net.cpp:150] Setting up drop1
I0527 11:17:24.433967 21372 net.cpp:157] Top shape: 40 196 (7840)
I0527 11:17:24.433976 21372 net.cpp:165] Memory required for data: 63100960
I0527 11:17:24.433987 21372 layer_factory.hpp:77] Creating layer ip2
I0527 11:17:24.434002 21372 net.cpp:106] Creating Layer ip2
I0527 11:17:24.434012 21372 net.cpp:454] ip2 <- ip1
I0527 11:17:24.434027 21372 net.cpp:411] ip2 -> ip2
I0527 11:17:24.434510 21372 net.cpp:150] Setting up ip2
I0527 11:17:24.434523 21372 net.cpp:157] Top shape: 40 98 (3920)
I0527 11:17:24.434533 21372 net.cpp:165] Memory required for data: 63116640
I0527 11:17:24.434550 21372 layer_factory.hpp:77] Creating layer relu6
I0527 11:17:24.434576 21372 net.cpp:106] Creating Layer relu6
I0527 11:17:24.434587 21372 net.cpp:454] relu6 <- ip2
I0527 11:17:24.434598 21372 net.cpp:397] relu6 -> ip2 (in-place)
I0527 11:17:24.435129 21372 net.cpp:150] Setting up relu6
I0527 11:17:24.435147 21372 net.cpp:157] Top shape: 40 98 (3920)
I0527 11:17:24.435159 21372 net.cpp:165] Memory required for data: 63132320
I0527 11:17:24.435170 21372 layer_factory.hpp:77] Creating layer drop2
I0527 11:17:24.435184 21372 net.cpp:106] Creating Layer drop2
I0527 11:17:24.435194 21372 net.cpp:454] drop2 <- ip2
I0527 11:17:24.435207 21372 net.cpp:397] drop2 -> ip2 (in-place)
I0527 11:17:24.435250 21372 net.cpp:150] Setting up drop2
I0527 11:17:24.435263 21372 net.cpp:157] Top shape: 40 98 (3920)
I0527 11:17:24.435273 21372 net.cpp:165] Memory required for data: 63148000
I0527 11:17:24.435283 21372 layer_factory.hpp:77] Creating layer ip3
I0527 11:17:24.435297 21372 net.cpp:106] Creating Layer ip3
I0527 11:17:24.435307 21372 net.cpp:454] ip3 <- ip2
I0527 11:17:24.435322 21372 net.cpp:411] ip3 -> ip3
I0527 11:17:24.435544 21372 net.cpp:150] Setting up ip3
I0527 11:17:24.435557 21372 net.cpp:157] Top shape: 40 11 (440)
I0527 11:17:24.435569 21372 net.cpp:165] Memory required for data: 63149760
I0527 11:17:24.435583 21372 layer_factory.hpp:77] Creating layer drop3
I0527 11:17:24.435596 21372 net.cpp:106] Creating Layer drop3
I0527 11:17:24.435606 21372 net.cpp:454] drop3 <- ip3
I0527 11:17:24.435619 21372 net.cpp:397] drop3 -> ip3 (in-place)
I0527 11:17:24.435660 21372 net.cpp:150] Setting up drop3
I0527 11:17:24.435673 21372 net.cpp:157] Top shape: 40 11 (440)
I0527 11:17:24.435683 21372 net.cpp:165] Memory required for data: 63151520
I0527 11:17:24.435693 21372 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0527 11:17:24.435705 21372 net.cpp:106] Creating Layer ip3_drop3_0_split
I0527 11:17:24.435715 21372 net.cpp:454] ip3_drop3_0_split <- ip3
I0527 11:17:24.435729 21372 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0527 11:17:24.435744 21372 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0527 11:17:24.435817 21372 net.cpp:150] Setting up ip3_drop3_0_split
I0527 11:17:24.435830 21372 net.cpp:157] Top shape: 40 11 (440)
I0527 11:17:24.435842 21372 net.cpp:157] Top shape: 40 11 (440)
I0527 11:17:24.435853 21372 net.cpp:165] Memory required for data: 63155040
I0527 11:17:24.435863 21372 layer_factory.hpp:77] Creating layer accuracy
I0527 11:17:24.435885 21372 net.cpp:106] Creating Layer accuracy
I0527 11:17:24.435895 21372 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0527 11:17:24.435907 21372 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0527 11:17:24.435921 21372 net.cpp:411] accuracy -> accuracy
I0527 11:17:24.435945 21372 net.cpp:150] Setting up accuracy
I0527 11:17:24.435957 21372 net.cpp:157] Top shape: (1)
I0527 11:17:24.435967 21372 net.cpp:165] Memory required for data: 63155044
I0527 11:17:24.435977 21372 layer_factory.hpp:77] Creating layer loss
I0527 11:17:24.435991 21372 net.cpp:106] Creating Layer loss
I0527 11:17:24.436000 21372 net.cpp:454] loss <- ip3_drop3_0_split_1
I0527 11:17:24.436012 21372 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0527 11:17:24.436024 21372 net.cpp:411] loss -> loss
I0527 11:17:24.436043 21372 layer_factory.hpp:77] Creating layer loss
I0527 11:17:24.436537 21372 net.cpp:150] Setting up loss
I0527 11:17:24.436552 21372 net.cpp:157] Top shape: (1)
I0527 11:17:24.436561 21372 net.cpp:160]     with loss weight 1
I0527 11:17:24.436583 21372 net.cpp:165] Memory required for data: 63155048
I0527 11:17:24.436592 21372 net.cpp:226] loss needs backward computation.
I0527 11:17:24.436604 21372 net.cpp:228] accuracy does not need backward computation.
I0527 11:17:24.436614 21372 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0527 11:17:24.436625 21372 net.cpp:226] drop3 needs backward computation.
I0527 11:17:24.436636 21372 net.cpp:226] ip3 needs backward computation.
I0527 11:17:24.436646 21372 net.cpp:226] drop2 needs backward computation.
I0527 11:17:24.436656 21372 net.cpp:226] relu6 needs backward computation.
I0527 11:17:24.436674 21372 net.cpp:226] ip2 needs backward computation.
I0527 11:17:24.436686 21372 net.cpp:226] drop1 needs backward computation.
I0527 11:17:24.436696 21372 net.cpp:226] relu5 needs backward computation.
I0527 11:17:24.436704 21372 net.cpp:226] ip1 needs backward computation.
I0527 11:17:24.436714 21372 net.cpp:226] pool4 needs backward computation.
I0527 11:17:24.436725 21372 net.cpp:226] relu4 needs backward computation.
I0527 11:17:24.436735 21372 net.cpp:226] conv4 needs backward computation.
I0527 11:17:24.436744 21372 net.cpp:226] pool3 needs backward computation.
I0527 11:17:24.436755 21372 net.cpp:226] relu3 needs backward computation.
I0527 11:17:24.436765 21372 net.cpp:226] conv3 needs backward computation.
I0527 11:17:24.436777 21372 net.cpp:226] pool2 needs backward computation.
I0527 11:17:24.436787 21372 net.cpp:226] relu2 needs backward computation.
I0527 11:17:24.436797 21372 net.cpp:226] conv2 needs backward computation.
I0527 11:17:24.436808 21372 net.cpp:226] pool1 needs backward computation.
I0527 11:17:24.436818 21372 net.cpp:226] relu1 needs backward computation.
I0527 11:17:24.436828 21372 net.cpp:226] conv1 needs backward computation.
I0527 11:17:24.436841 21372 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0527 11:17:24.436851 21372 net.cpp:228] data_hdf5 does not need backward computation.
I0527 11:17:24.436862 21372 net.cpp:270] This network produces output accuracy
I0527 11:17:24.436873 21372 net.cpp:270] This network produces output loss
I0527 11:17:24.436902 21372 net.cpp:283] Network initialization done.
I0527 11:17:24.437034 21372 solver.cpp:60] Solver scaffolding done.
I0527 11:17:24.438166 21372 caffe.cpp:202] Resuming from /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_168750.solverstate
I0527 11:17:24.660774 21372 sgd_solver.cpp:318] SGDSolver: restoring history
I0527 11:17:24.666246 21372 caffe.cpp:212] Starting Optimization
I0527 11:17:24.666286 21372 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0527 11:17:24.666297 21372 solver.cpp:289] Learning Rate Policy: fixed
I0527 11:17:24.693353 21372 solver.cpp:237] Iteration 168750, loss = 1.093
I0527 11:17:24.693399 21372 solver.cpp:253]     Train net output #0: loss = 1.093 (* 1 = 1.093 loss)
I0527 11:17:24.693418 21372 sgd_solver.cpp:106] Iteration 168750, lr = 0.003
I0527 11:17:34.430644 21372 solver.cpp:237] Iteration 169125, loss = 0.83987
I0527 11:17:34.430682 21372 solver.cpp:253]     Train net output #0: loss = 0.83987 (* 1 = 0.83987 loss)
I0527 11:17:34.430696 21372 sgd_solver.cpp:106] Iteration 169125, lr = 0.003
I0527 11:17:44.147552 21372 solver.cpp:237] Iteration 169500, loss = 1.28349
I0527 11:17:44.147600 21372 solver.cpp:253]     Train net output #0: loss = 1.28349 (* 1 = 1.28349 loss)
I0527 11:17:44.147614 21372 sgd_solver.cpp:106] Iteration 169500, lr = 0.003
I0527 11:17:53.861016 21372 solver.cpp:237] Iteration 169875, loss = 1.28804
I0527 11:17:53.861052 21372 solver.cpp:253]     Train net output #0: loss = 1.28804 (* 1 = 1.28804 loss)
I0527 11:17:53.861065 21372 sgd_solver.cpp:106] Iteration 169875, lr = 0.003
I0527 11:18:03.577235 21372 solver.cpp:237] Iteration 170250, loss = 1.15652
I0527 11:18:03.577400 21372 solver.cpp:253]     Train net output #0: loss = 1.15652 (* 1 = 1.15652 loss)
I0527 11:18:03.577415 21372 sgd_solver.cpp:106] Iteration 170250, lr = 0.003
I0527 11:18:13.293859 21372 solver.cpp:237] Iteration 170625, loss = 1.26839
I0527 11:18:13.293895 21372 solver.cpp:253]     Train net output #0: loss = 1.26839 (* 1 = 1.26839 loss)
I0527 11:18:13.293910 21372 sgd_solver.cpp:106] Iteration 170625, lr = 0.003
I0527 11:18:23.019402 21372 solver.cpp:237] Iteration 171000, loss = 1.32658
I0527 11:18:23.019438 21372 solver.cpp:253]     Train net output #0: loss = 1.32658 (* 1 = 1.32658 loss)
I0527 11:18:23.019451 21372 sgd_solver.cpp:106] Iteration 171000, lr = 0.003
I0527 11:18:54.918429 21372 solver.cpp:237] Iteration 171375, loss = 1.12917
I0527 11:18:54.918596 21372 solver.cpp:253]     Train net output #0: loss = 1.12917 (* 1 = 1.12917 loss)
I0527 11:18:54.918611 21372 sgd_solver.cpp:106] Iteration 171375, lr = 0.003
I0527 11:19:04.651800 21372 solver.cpp:237] Iteration 171750, loss = 1.09887
I0527 11:19:04.651836 21372 solver.cpp:253]     Train net output #0: loss = 1.09887 (* 1 = 1.09887 loss)
I0527 11:19:04.651854 21372 sgd_solver.cpp:106] Iteration 171750, lr = 0.003
I0527 11:19:14.380239 21372 solver.cpp:237] Iteration 172125, loss = 1.55886
I0527 11:19:14.380275 21372 solver.cpp:253]     Train net output #0: loss = 1.55886 (* 1 = 1.55886 loss)
I0527 11:19:14.380292 21372 sgd_solver.cpp:106] Iteration 172125, lr = 0.003
I0527 11:19:24.084189 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_172500.caffemodel
I0527 11:19:24.140720 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_172500.solverstate
I0527 11:19:24.169075 21372 solver.cpp:341] Iteration 172500, Testing net (#0)
I0527 11:20:13.779623 21372 solver.cpp:409]     Test net output #0: accuracy = 0.900013
I0527 11:20:13.779799 21372 solver.cpp:409]     Test net output #1: loss = 0.298132 (* 1 = 0.298132 loss)
I0527 11:20:13.787907 21372 solver.cpp:237] Iteration 172500, loss = 1.16372
I0527 11:20:13.787935 21372 solver.cpp:253]     Train net output #0: loss = 1.16372 (* 1 = 1.16372 loss)
I0527 11:20:13.787950 21372 sgd_solver.cpp:106] Iteration 172500, lr = 0.003
I0527 11:20:23.681310 21372 solver.cpp:237] Iteration 172875, loss = 1.32687
I0527 11:20:23.681345 21372 solver.cpp:253]     Train net output #0: loss = 1.32687 (* 1 = 1.32687 loss)
I0527 11:20:23.681363 21372 sgd_solver.cpp:106] Iteration 172875, lr = 0.003
I0527 11:20:33.556417 21372 solver.cpp:237] Iteration 173250, loss = 0.84971
I0527 11:20:33.556468 21372 solver.cpp:253]     Train net output #0: loss = 0.84971 (* 1 = 0.84971 loss)
I0527 11:20:33.556485 21372 sgd_solver.cpp:106] Iteration 173250, lr = 0.003
I0527 11:20:43.438827 21372 solver.cpp:237] Iteration 173625, loss = 1.00551
I0527 11:20:43.438863 21372 solver.cpp:253]     Train net output #0: loss = 1.00551 (* 1 = 1.00551 loss)
I0527 11:20:43.438876 21372 sgd_solver.cpp:106] Iteration 173625, lr = 0.003
I0527 11:21:15.495240 21372 solver.cpp:237] Iteration 174000, loss = 0.888464
I0527 11:21:15.495419 21372 solver.cpp:253]     Train net output #0: loss = 0.888464 (* 1 = 0.888464 loss)
I0527 11:21:15.495435 21372 sgd_solver.cpp:106] Iteration 174000, lr = 0.003
I0527 11:21:25.374395 21372 solver.cpp:237] Iteration 174375, loss = 1.26723
I0527 11:21:25.374447 21372 solver.cpp:253]     Train net output #0: loss = 1.26723 (* 1 = 1.26723 loss)
I0527 11:21:25.374460 21372 sgd_solver.cpp:106] Iteration 174375, lr = 0.003
I0527 11:21:35.254974 21372 solver.cpp:237] Iteration 174750, loss = 1.22239
I0527 11:21:35.255012 21372 solver.cpp:253]     Train net output #0: loss = 1.22239 (* 1 = 1.22239 loss)
I0527 11:21:35.255024 21372 sgd_solver.cpp:106] Iteration 174750, lr = 0.003
I0527 11:21:45.138046 21372 solver.cpp:237] Iteration 175125, loss = 0.979882
I0527 11:21:45.138082 21372 solver.cpp:253]     Train net output #0: loss = 0.979882 (* 1 = 0.979882 loss)
I0527 11:21:45.138097 21372 sgd_solver.cpp:106] Iteration 175125, lr = 0.003
I0527 11:21:55.014714 21372 solver.cpp:237] Iteration 175500, loss = 1.2821
I0527 11:21:55.014879 21372 solver.cpp:253]     Train net output #0: loss = 1.2821 (* 1 = 1.2821 loss)
I0527 11:21:55.014894 21372 sgd_solver.cpp:106] Iteration 175500, lr = 0.003
I0527 11:22:04.893900 21372 solver.cpp:237] Iteration 175875, loss = 1.36215
I0527 11:22:04.893935 21372 solver.cpp:253]     Train net output #0: loss = 1.36215 (* 1 = 1.36215 loss)
I0527 11:22:04.893954 21372 sgd_solver.cpp:106] Iteration 175875, lr = 0.003
I0527 11:22:14.753527 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_176250.caffemodel
I0527 11:22:14.812042 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_176250.solverstate
I0527 11:22:37.051867 21372 solver.cpp:237] Iteration 176250, loss = 1.00649
I0527 11:22:37.052040 21372 solver.cpp:253]     Train net output #0: loss = 1.00649 (* 1 = 1.00649 loss)
I0527 11:22:37.052055 21372 sgd_solver.cpp:106] Iteration 176250, lr = 0.003
I0527 11:22:46.928724 21372 solver.cpp:237] Iteration 176625, loss = 1.00797
I0527 11:22:46.928771 21372 solver.cpp:253]     Train net output #0: loss = 1.00797 (* 1 = 1.00797 loss)
I0527 11:22:46.928786 21372 sgd_solver.cpp:106] Iteration 176625, lr = 0.003
I0527 11:22:56.802530 21372 solver.cpp:237] Iteration 177000, loss = 1.23819
I0527 11:22:56.802567 21372 solver.cpp:253]     Train net output #0: loss = 1.23819 (* 1 = 1.23819 loss)
I0527 11:22:56.802579 21372 sgd_solver.cpp:106] Iteration 177000, lr = 0.003
I0527 11:23:06.680956 21372 solver.cpp:237] Iteration 177375, loss = 1.03856
I0527 11:23:06.681007 21372 solver.cpp:253]     Train net output #0: loss = 1.03856 (* 1 = 1.03856 loss)
I0527 11:23:06.681021 21372 sgd_solver.cpp:106] Iteration 177375, lr = 0.003
I0527 11:23:16.549247 21372 solver.cpp:237] Iteration 177750, loss = 1.20025
I0527 11:23:16.549387 21372 solver.cpp:253]     Train net output #0: loss = 1.20025 (* 1 = 1.20025 loss)
I0527 11:23:16.549401 21372 sgd_solver.cpp:106] Iteration 177750, lr = 0.003
I0527 11:23:26.434855 21372 solver.cpp:237] Iteration 178125, loss = 1.37082
I0527 11:23:26.434890 21372 solver.cpp:253]     Train net output #0: loss = 1.37082 (* 1 = 1.37082 loss)
I0527 11:23:26.434905 21372 sgd_solver.cpp:106] Iteration 178125, lr = 0.003
I0527 11:23:36.309236 21372 solver.cpp:237] Iteration 178500, loss = 0.9328
I0527 11:23:36.309286 21372 solver.cpp:253]     Train net output #0: loss = 0.9328 (* 1 = 0.9328 loss)
I0527 11:23:36.309300 21372 sgd_solver.cpp:106] Iteration 178500, lr = 0.003
I0527 11:24:08.388402 21372 solver.cpp:237] Iteration 178875, loss = 1.1952
I0527 11:24:08.388573 21372 solver.cpp:253]     Train net output #0: loss = 1.1952 (* 1 = 1.1952 loss)
I0527 11:24:08.388589 21372 sgd_solver.cpp:106] Iteration 178875, lr = 0.003
I0527 11:24:18.266644 21372 solver.cpp:237] Iteration 179250, loss = 0.878983
I0527 11:24:18.266680 21372 solver.cpp:253]     Train net output #0: loss = 0.878983 (* 1 = 0.878983 loss)
I0527 11:24:18.266693 21372 sgd_solver.cpp:106] Iteration 179250, lr = 0.003
I0527 11:24:28.141144 21372 solver.cpp:237] Iteration 179625, loss = 1.20018
I0527 11:24:28.141188 21372 solver.cpp:253]     Train net output #0: loss = 1.20018 (* 1 = 1.20018 loss)
I0527 11:24:28.141204 21372 sgd_solver.cpp:106] Iteration 179625, lr = 0.003
I0527 11:24:37.990279 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_180000.caffemodel
I0527 11:24:38.049722 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_180000.solverstate
I0527 11:24:38.077566 21372 solver.cpp:341] Iteration 180000, Testing net (#0)
I0527 11:25:26.730229 21372 solver.cpp:409]     Test net output #0: accuracy = 0.902607
I0527 11:25:26.730420 21372 solver.cpp:409]     Test net output #1: loss = 0.315153 (* 1 = 0.315153 loss)
I0527 11:25:26.738500 21372 solver.cpp:237] Iteration 180000, loss = 1.33242
I0527 11:25:26.738528 21372 solver.cpp:253]     Train net output #0: loss = 1.33242 (* 1 = 1.33242 loss)
I0527 11:25:26.738543 21372 sgd_solver.cpp:106] Iteration 180000, lr = 0.003
I0527 11:25:36.505844 21372 solver.cpp:237] Iteration 180375, loss = 0.992823
I0527 11:25:36.505887 21372 solver.cpp:253]     Train net output #0: loss = 0.992823 (* 1 = 0.992823 loss)
I0527 11:25:36.505905 21372 sgd_solver.cpp:106] Iteration 180375, lr = 0.003
I0527 11:25:46.265135 21372 solver.cpp:237] Iteration 180750, loss = 0.772206
I0527 11:25:46.265172 21372 solver.cpp:253]     Train net output #0: loss = 0.772206 (* 1 = 0.772206 loss)
I0527 11:25:46.265184 21372 sgd_solver.cpp:106] Iteration 180750, lr = 0.003
I0527 11:25:56.027470 21372 solver.cpp:237] Iteration 181125, loss = 1.21242
I0527 11:25:56.027505 21372 solver.cpp:253]     Train net output #0: loss = 1.21242 (* 1 = 1.21242 loss)
I0527 11:25:56.027519 21372 sgd_solver.cpp:106] Iteration 181125, lr = 0.003
I0527 11:26:28.025038 21372 solver.cpp:237] Iteration 181500, loss = 1.10701
I0527 11:26:28.025218 21372 solver.cpp:253]     Train net output #0: loss = 1.10701 (* 1 = 1.10701 loss)
I0527 11:26:28.025234 21372 sgd_solver.cpp:106] Iteration 181500, lr = 0.003
I0527 11:26:37.787421 21372 solver.cpp:237] Iteration 181875, loss = 1.26765
I0527 11:26:37.787456 21372 solver.cpp:253]     Train net output #0: loss = 1.26765 (* 1 = 1.26765 loss)
I0527 11:26:37.787470 21372 sgd_solver.cpp:106] Iteration 181875, lr = 0.003
I0527 11:26:47.546205 21372 solver.cpp:237] Iteration 182250, loss = 1.1043
I0527 11:26:47.546241 21372 solver.cpp:253]     Train net output #0: loss = 1.1043 (* 1 = 1.1043 loss)
I0527 11:26:47.546257 21372 sgd_solver.cpp:106] Iteration 182250, lr = 0.003
I0527 11:26:57.303354 21372 solver.cpp:237] Iteration 182625, loss = 1.14327
I0527 11:26:57.303402 21372 solver.cpp:253]     Train net output #0: loss = 1.14327 (* 1 = 1.14327 loss)
I0527 11:26:57.303416 21372 sgd_solver.cpp:106] Iteration 182625, lr = 0.003
I0527 11:27:07.068317 21372 solver.cpp:237] Iteration 183000, loss = 0.946988
I0527 11:27:07.068457 21372 solver.cpp:253]     Train net output #0: loss = 0.946988 (* 1 = 0.946988 loss)
I0527 11:27:07.068472 21372 sgd_solver.cpp:106] Iteration 183000, lr = 0.003
I0527 11:27:16.830585 21372 solver.cpp:237] Iteration 183375, loss = 1.1361
I0527 11:27:16.830633 21372 solver.cpp:253]     Train net output #0: loss = 1.1361 (* 1 = 1.1361 loss)
I0527 11:27:16.830647 21372 sgd_solver.cpp:106] Iteration 183375, lr = 0.003
I0527 11:27:26.571765 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_183750.caffemodel
I0527 11:27:26.629796 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_183750.solverstate
I0527 11:27:48.936353 21372 solver.cpp:237] Iteration 183750, loss = 1.01253
I0527 11:27:48.936542 21372 solver.cpp:253]     Train net output #0: loss = 1.01253 (* 1 = 1.01253 loss)
I0527 11:27:48.936558 21372 sgd_solver.cpp:106] Iteration 183750, lr = 0.003
I0527 11:27:58.697834 21372 solver.cpp:237] Iteration 184125, loss = 1.12797
I0527 11:27:58.697868 21372 solver.cpp:253]     Train net output #0: loss = 1.12797 (* 1 = 1.12797 loss)
I0527 11:27:58.697885 21372 sgd_solver.cpp:106] Iteration 184125, lr = 0.003
I0527 11:28:08.461603 21372 solver.cpp:237] Iteration 184500, loss = 1.45981
I0527 11:28:08.461652 21372 solver.cpp:253]     Train net output #0: loss = 1.45981 (* 1 = 1.45981 loss)
I0527 11:28:08.461673 21372 sgd_solver.cpp:106] Iteration 184500, lr = 0.003
I0527 11:28:18.223573 21372 solver.cpp:237] Iteration 184875, loss = 0.973339
I0527 11:28:18.223609 21372 solver.cpp:253]     Train net output #0: loss = 0.973339 (* 1 = 0.973339 loss)
I0527 11:28:18.223623 21372 sgd_solver.cpp:106] Iteration 184875, lr = 0.003
I0527 11:28:27.988512 21372 solver.cpp:237] Iteration 185250, loss = 1.0161
I0527 11:28:27.988656 21372 solver.cpp:253]     Train net output #0: loss = 1.0161 (* 1 = 1.0161 loss)
I0527 11:28:27.988668 21372 sgd_solver.cpp:106] Iteration 185250, lr = 0.003
I0527 11:28:37.755548 21372 solver.cpp:237] Iteration 185625, loss = 1.36581
I0527 11:28:37.755595 21372 solver.cpp:253]     Train net output #0: loss = 1.36581 (* 1 = 1.36581 loss)
I0527 11:28:37.755612 21372 sgd_solver.cpp:106] Iteration 185625, lr = 0.003
I0527 11:28:47.511533 21372 solver.cpp:237] Iteration 186000, loss = 1.21679
I0527 11:28:47.511567 21372 solver.cpp:253]     Train net output #0: loss = 1.21679 (* 1 = 1.21679 loss)
I0527 11:28:47.511581 21372 sgd_solver.cpp:106] Iteration 186000, lr = 0.003
I0527 11:29:19.427378 21372 solver.cpp:237] Iteration 186375, loss = 0.944679
I0527 11:29:19.427543 21372 solver.cpp:253]     Train net output #0: loss = 0.944679 (* 1 = 0.944679 loss)
I0527 11:29:19.427558 21372 sgd_solver.cpp:106] Iteration 186375, lr = 0.003
I0527 11:29:29.186908 21372 solver.cpp:237] Iteration 186750, loss = 1.08562
I0527 11:29:29.186944 21372 solver.cpp:253]     Train net output #0: loss = 1.08562 (* 1 = 1.08562 loss)
I0527 11:29:29.186959 21372 sgd_solver.cpp:106] Iteration 186750, lr = 0.003
I0527 11:29:38.940377 21372 solver.cpp:237] Iteration 187125, loss = 0.966142
I0527 11:29:38.940412 21372 solver.cpp:253]     Train net output #0: loss = 0.966142 (* 1 = 0.966142 loss)
I0527 11:29:38.940428 21372 sgd_solver.cpp:106] Iteration 187125, lr = 0.003
I0527 11:29:48.683362 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_187500.caffemodel
I0527 11:29:48.741575 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_187500.solverstate
I0527 11:29:48.769673 21372 solver.cpp:341] Iteration 187500, Testing net (#0)
I0527 11:30:58.125351 21372 solver.cpp:409]     Test net output #0: accuracy = 0.902073
I0527 11:30:58.125519 21372 solver.cpp:409]     Test net output #1: loss = 0.33213 (* 1 = 0.33213 loss)
I0527 11:30:58.133553 21372 solver.cpp:237] Iteration 187500, loss = 0.889051
I0527 11:30:58.133581 21372 solver.cpp:253]     Train net output #0: loss = 0.889051 (* 1 = 0.889051 loss)
I0527 11:30:58.133595 21372 sgd_solver.cpp:106] Iteration 187500, lr = 0.003
I0527 11:31:07.869273 21372 solver.cpp:237] Iteration 187875, loss = 1.22918
I0527 11:31:07.869313 21372 solver.cpp:253]     Train net output #0: loss = 1.22918 (* 1 = 1.22918 loss)
I0527 11:31:07.869333 21372 sgd_solver.cpp:106] Iteration 187875, lr = 0.003
I0527 11:31:17.606418 21372 solver.cpp:237] Iteration 188250, loss = 1.09897
I0527 11:31:17.606454 21372 solver.cpp:253]     Train net output #0: loss = 1.09897 (* 1 = 1.09897 loss)
I0527 11:31:17.606467 21372 sgd_solver.cpp:106] Iteration 188250, lr = 0.003
I0527 11:31:27.341712 21372 solver.cpp:237] Iteration 188625, loss = 1.25978
I0527 11:31:27.341766 21372 solver.cpp:253]     Train net output #0: loss = 1.25978 (* 1 = 1.25978 loss)
I0527 11:31:27.341780 21372 sgd_solver.cpp:106] Iteration 188625, lr = 0.003
I0527 11:31:59.259659 21372 solver.cpp:237] Iteration 189000, loss = 1.04354
I0527 11:31:59.259829 21372 solver.cpp:253]     Train net output #0: loss = 1.04354 (* 1 = 1.04354 loss)
I0527 11:31:59.259845 21372 sgd_solver.cpp:106] Iteration 189000, lr = 0.003
I0527 11:32:08.987449 21372 solver.cpp:237] Iteration 189375, loss = 1.13715
I0527 11:32:08.987483 21372 solver.cpp:253]     Train net output #0: loss = 1.13715 (* 1 = 1.13715 loss)
I0527 11:32:08.987501 21372 sgd_solver.cpp:106] Iteration 189375, lr = 0.003
I0527 11:32:18.740734 21372 solver.cpp:237] Iteration 189750, loss = 1.06794
I0527 11:32:18.740778 21372 solver.cpp:253]     Train net output #0: loss = 1.06794 (* 1 = 1.06794 loss)
I0527 11:32:18.740793 21372 sgd_solver.cpp:106] Iteration 189750, lr = 0.003
I0527 11:32:28.504793 21372 solver.cpp:237] Iteration 190125, loss = 0.934515
I0527 11:32:28.504829 21372 solver.cpp:253]     Train net output #0: loss = 0.934515 (* 1 = 0.934515 loss)
I0527 11:32:28.504843 21372 sgd_solver.cpp:106] Iteration 190125, lr = 0.003
I0527 11:32:38.279564 21372 solver.cpp:237] Iteration 190500, loss = 1.12073
I0527 11:32:38.279706 21372 solver.cpp:253]     Train net output #0: loss = 1.12073 (* 1 = 1.12073 loss)
I0527 11:32:38.279721 21372 sgd_solver.cpp:106] Iteration 190500, lr = 0.003
I0527 11:32:48.037483 21372 solver.cpp:237] Iteration 190875, loss = 0.814501
I0527 11:32:48.037533 21372 solver.cpp:253]     Train net output #0: loss = 0.814501 (* 1 = 0.814501 loss)
I0527 11:32:48.037545 21372 sgd_solver.cpp:106] Iteration 190875, lr = 0.003
I0527 11:32:57.784116 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_191250.caffemodel
I0527 11:32:57.841142 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_191250.solverstate
I0527 11:33:20.155283 21372 solver.cpp:237] Iteration 191250, loss = 1.03414
I0527 11:33:20.155467 21372 solver.cpp:253]     Train net output #0: loss = 1.03414 (* 1 = 1.03414 loss)
I0527 11:33:20.155483 21372 sgd_solver.cpp:106] Iteration 191250, lr = 0.003
I0527 11:33:29.925016 21372 solver.cpp:237] Iteration 191625, loss = 1.11766
I0527 11:33:29.925067 21372 solver.cpp:253]     Train net output #0: loss = 1.11766 (* 1 = 1.11766 loss)
I0527 11:33:29.925081 21372 sgd_solver.cpp:106] Iteration 191625, lr = 0.003
I0527 11:33:39.684931 21372 solver.cpp:237] Iteration 192000, loss = 1.57933
I0527 11:33:39.684967 21372 solver.cpp:253]     Train net output #0: loss = 1.57933 (* 1 = 1.57933 loss)
I0527 11:33:39.684984 21372 sgd_solver.cpp:106] Iteration 192000, lr = 0.003
I0527 11:33:49.452184 21372 solver.cpp:237] Iteration 192375, loss = 1.10522
I0527 11:33:49.452220 21372 solver.cpp:253]     Train net output #0: loss = 1.10522 (* 1 = 1.10522 loss)
I0527 11:33:49.452234 21372 sgd_solver.cpp:106] Iteration 192375, lr = 0.003
I0527 11:33:59.217648 21372 solver.cpp:237] Iteration 192750, loss = 0.97978
I0527 11:33:59.217805 21372 solver.cpp:253]     Train net output #0: loss = 0.97978 (* 1 = 0.97978 loss)
I0527 11:33:59.217819 21372 sgd_solver.cpp:106] Iteration 192750, lr = 0.003
I0527 11:34:08.985250 21372 solver.cpp:237] Iteration 193125, loss = 1.10069
I0527 11:34:08.985285 21372 solver.cpp:253]     Train net output #0: loss = 1.10069 (* 1 = 1.10069 loss)
I0527 11:34:08.985303 21372 sgd_solver.cpp:106] Iteration 193125, lr = 0.003
I0527 11:34:18.749691 21372 solver.cpp:237] Iteration 193500, loss = 1.00175
I0527 11:34:18.749743 21372 solver.cpp:253]     Train net output #0: loss = 1.00175 (* 1 = 1.00175 loss)
I0527 11:34:18.749758 21372 sgd_solver.cpp:106] Iteration 193500, lr = 0.003
I0527 11:34:50.759630 21372 solver.cpp:237] Iteration 193875, loss = 1.01809
I0527 11:34:50.759804 21372 solver.cpp:253]     Train net output #0: loss = 1.01809 (* 1 = 1.01809 loss)
I0527 11:34:50.759820 21372 sgd_solver.cpp:106] Iteration 193875, lr = 0.003
I0527 11:35:00.523087 21372 solver.cpp:237] Iteration 194250, loss = 1.04486
I0527 11:35:00.523123 21372 solver.cpp:253]     Train net output #0: loss = 1.04486 (* 1 = 1.04486 loss)
I0527 11:35:00.523140 21372 sgd_solver.cpp:106] Iteration 194250, lr = 0.003
I0527 11:35:10.293515 21372 solver.cpp:237] Iteration 194625, loss = 1.47079
I0527 11:35:10.293561 21372 solver.cpp:253]     Train net output #0: loss = 1.47079 (* 1 = 1.47079 loss)
I0527 11:35:10.293576 21372 sgd_solver.cpp:106] Iteration 194625, lr = 0.003
I0527 11:35:20.022933 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_195000.caffemodel
I0527 11:35:20.079246 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_195000.solverstate
I0527 11:35:20.104794 21372 solver.cpp:341] Iteration 195000, Testing net (#0)
I0527 11:36:08.377025 21372 solver.cpp:409]     Test net output #0: accuracy = 0.903718
I0527 11:36:08.377205 21372 solver.cpp:409]     Test net output #1: loss = 0.304241 (* 1 = 0.304241 loss)
I0527 11:36:08.385262 21372 solver.cpp:237] Iteration 195000, loss = 1.14546
I0527 11:36:08.385289 21372 solver.cpp:253]     Train net output #0: loss = 1.14546 (* 1 = 1.14546 loss)
I0527 11:36:08.385303 21372 sgd_solver.cpp:106] Iteration 195000, lr = 0.003
I0527 11:36:18.217849 21372 solver.cpp:237] Iteration 195375, loss = 1.03729
I0527 11:36:18.217882 21372 solver.cpp:253]     Train net output #0: loss = 1.03729 (* 1 = 1.03729 loss)
I0527 11:36:18.217897 21372 sgd_solver.cpp:106] Iteration 195375, lr = 0.003
I0527 11:36:28.050515 21372 solver.cpp:237] Iteration 195750, loss = 1.03719
I0527 11:36:28.050562 21372 solver.cpp:253]     Train net output #0: loss = 1.03719 (* 1 = 1.03719 loss)
I0527 11:36:28.050581 21372 sgd_solver.cpp:106] Iteration 195750, lr = 0.003
I0527 11:36:37.875012 21372 solver.cpp:237] Iteration 196125, loss = 1.08831
I0527 11:36:37.875049 21372 solver.cpp:253]     Train net output #0: loss = 1.08831 (* 1 = 1.08831 loss)
I0527 11:36:37.875062 21372 sgd_solver.cpp:106] Iteration 196125, lr = 0.003
I0527 11:37:09.906472 21372 solver.cpp:237] Iteration 196500, loss = 1.30454
I0527 11:37:09.906638 21372 solver.cpp:253]     Train net output #0: loss = 1.30454 (* 1 = 1.30454 loss)
I0527 11:37:09.906656 21372 sgd_solver.cpp:106] Iteration 196500, lr = 0.003
I0527 11:37:19.736798 21372 solver.cpp:237] Iteration 196875, loss = 1.16018
I0527 11:37:19.736845 21372 solver.cpp:253]     Train net output #0: loss = 1.16018 (* 1 = 1.16018 loss)
I0527 11:37:19.736865 21372 sgd_solver.cpp:106] Iteration 196875, lr = 0.003
I0527 11:37:29.561604 21372 solver.cpp:237] Iteration 197250, loss = 1.22317
I0527 11:37:29.561640 21372 solver.cpp:253]     Train net output #0: loss = 1.22317 (* 1 = 1.22317 loss)
I0527 11:37:29.561655 21372 sgd_solver.cpp:106] Iteration 197250, lr = 0.003
I0527 11:37:39.391491 21372 solver.cpp:237] Iteration 197625, loss = 1.34501
I0527 11:37:39.391543 21372 solver.cpp:253]     Train net output #0: loss = 1.34501 (* 1 = 1.34501 loss)
I0527 11:37:39.391559 21372 sgd_solver.cpp:106] Iteration 197625, lr = 0.003
I0527 11:37:49.220012 21372 solver.cpp:237] Iteration 198000, loss = 1.11556
I0527 11:37:49.220162 21372 solver.cpp:253]     Train net output #0: loss = 1.11556 (* 1 = 1.11556 loss)
I0527 11:37:49.220176 21372 sgd_solver.cpp:106] Iteration 198000, lr = 0.003
I0527 11:37:59.044963 21372 solver.cpp:237] Iteration 198375, loss = 1.20462
I0527 11:37:59.044997 21372 solver.cpp:253]     Train net output #0: loss = 1.20462 (* 1 = 1.20462 loss)
I0527 11:37:59.045016 21372 sgd_solver.cpp:106] Iteration 198375, lr = 0.003
I0527 11:38:08.844231 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_198750.caffemodel
I0527 11:38:08.901039 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_198750.solverstate
I0527 11:38:31.232703 21372 solver.cpp:237] Iteration 198750, loss = 1.17935
I0527 11:38:31.232890 21372 solver.cpp:253]     Train net output #0: loss = 1.17935 (* 1 = 1.17935 loss)
I0527 11:38:31.232905 21372 sgd_solver.cpp:106] Iteration 198750, lr = 0.003
I0527 11:38:41.059695 21372 solver.cpp:237] Iteration 199125, loss = 1.16831
I0527 11:38:41.059731 21372 solver.cpp:253]     Train net output #0: loss = 1.16831 (* 1 = 1.16831 loss)
I0527 11:38:41.059746 21372 sgd_solver.cpp:106] Iteration 199125, lr = 0.003
I0527 11:38:50.886178 21372 solver.cpp:237] Iteration 199500, loss = 0.784944
I0527 11:38:50.886212 21372 solver.cpp:253]     Train net output #0: loss = 0.784944 (* 1 = 0.784944 loss)
I0527 11:38:50.886226 21372 sgd_solver.cpp:106] Iteration 199500, lr = 0.003
I0527 11:39:00.716760 21372 solver.cpp:237] Iteration 199875, loss = 1.08576
I0527 11:39:00.716804 21372 solver.cpp:253]     Train net output #0: loss = 1.08576 (* 1 = 1.08576 loss)
I0527 11:39:00.716821 21372 sgd_solver.cpp:106] Iteration 199875, lr = 0.003
I0527 11:39:10.546238 21372 solver.cpp:237] Iteration 200250, loss = 1.02522
I0527 11:39:10.546380 21372 solver.cpp:253]     Train net output #0: loss = 1.02522 (* 1 = 1.02522 loss)
I0527 11:39:10.546394 21372 sgd_solver.cpp:106] Iteration 200250, lr = 0.003
I0527 11:39:20.376196 21372 solver.cpp:237] Iteration 200625, loss = 1.02045
I0527 11:39:20.376240 21372 solver.cpp:253]     Train net output #0: loss = 1.02045 (* 1 = 1.02045 loss)
I0527 11:39:20.376258 21372 sgd_solver.cpp:106] Iteration 200625, lr = 0.003
I0527 11:39:30.204905 21372 solver.cpp:237] Iteration 201000, loss = 1.12434
I0527 11:39:30.204939 21372 solver.cpp:253]     Train net output #0: loss = 1.12434 (* 1 = 1.12434 loss)
I0527 11:39:30.204953 21372 sgd_solver.cpp:106] Iteration 201000, lr = 0.003
I0527 11:40:02.285935 21372 solver.cpp:237] Iteration 201375, loss = 1.13863
I0527 11:40:02.286099 21372 solver.cpp:253]     Train net output #0: loss = 1.13863 (* 1 = 1.13863 loss)
I0527 11:40:02.286114 21372 sgd_solver.cpp:106] Iteration 201375, lr = 0.003
I0527 11:40:12.113970 21372 solver.cpp:237] Iteration 201750, loss = 1.35384
I0527 11:40:12.114019 21372 solver.cpp:253]     Train net output #0: loss = 1.35384 (* 1 = 1.35384 loss)
I0527 11:40:12.114033 21372 sgd_solver.cpp:106] Iteration 201750, lr = 0.003
I0527 11:40:21.941557 21372 solver.cpp:237] Iteration 202125, loss = 1.04006
I0527 11:40:21.941592 21372 solver.cpp:253]     Train net output #0: loss = 1.04006 (* 1 = 1.04006 loss)
I0527 11:40:21.941609 21372 sgd_solver.cpp:106] Iteration 202125, lr = 0.003
I0527 11:40:31.742908 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_202500.caffemodel
I0527 11:40:31.801360 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_202500.solverstate
I0527 11:40:31.828547 21372 solver.cpp:341] Iteration 202500, Testing net (#0)
I0527 11:41:41.236716 21372 solver.cpp:409]     Test net output #0: accuracy = 0.901979
I0527 11:41:41.236886 21372 solver.cpp:409]     Test net output #1: loss = 0.320155 (* 1 = 0.320155 loss)
I0527 11:41:41.244966 21372 solver.cpp:237] Iteration 202500, loss = 1.13962
I0527 11:41:41.244993 21372 solver.cpp:253]     Train net output #0: loss = 1.13962 (* 1 = 1.13962 loss)
I0527 11:41:41.245008 21372 sgd_solver.cpp:106] Iteration 202500, lr = 0.003
I0527 11:41:51.148427 21372 solver.cpp:237] Iteration 202875, loss = 1.42316
I0527 11:41:51.148474 21372 solver.cpp:253]     Train net output #0: loss = 1.42316 (* 1 = 1.42316 loss)
I0527 11:41:51.148491 21372 sgd_solver.cpp:106] Iteration 202875, lr = 0.003
I0527 11:42:01.049337 21372 solver.cpp:237] Iteration 203250, loss = 1.3028
I0527 11:42:01.049373 21372 solver.cpp:253]     Train net output #0: loss = 1.3028 (* 1 = 1.3028 loss)
I0527 11:42:01.049389 21372 sgd_solver.cpp:106] Iteration 203250, lr = 0.003
I0527 11:42:10.952296 21372 solver.cpp:237] Iteration 203625, loss = 1.13463
I0527 11:42:10.952329 21372 solver.cpp:253]     Train net output #0: loss = 1.13463 (* 1 = 1.13463 loss)
I0527 11:42:10.952344 21372 sgd_solver.cpp:106] Iteration 203625, lr = 0.003
I0527 11:42:43.072206 21372 solver.cpp:237] Iteration 204000, loss = 0.94618
I0527 11:42:43.072394 21372 solver.cpp:253]     Train net output #0: loss = 0.94618 (* 1 = 0.94618 loss)
I0527 11:42:43.072412 21372 sgd_solver.cpp:106] Iteration 204000, lr = 0.003
I0527 11:42:52.974114 21372 solver.cpp:237] Iteration 204375, loss = 1.19726
I0527 11:42:52.974149 21372 solver.cpp:253]     Train net output #0: loss = 1.19726 (* 1 = 1.19726 loss)
I0527 11:42:52.974164 21372 sgd_solver.cpp:106] Iteration 204375, lr = 0.003
I0527 11:43:02.874220 21372 solver.cpp:237] Iteration 204750, loss = 1.18803
I0527 11:43:02.874255 21372 solver.cpp:253]     Train net output #0: loss = 1.18803 (* 1 = 1.18803 loss)
I0527 11:43:02.874269 21372 sgd_solver.cpp:106] Iteration 204750, lr = 0.003
I0527 11:43:12.771090 21372 solver.cpp:237] Iteration 205125, loss = 0.892124
I0527 11:43:12.771133 21372 solver.cpp:253]     Train net output #0: loss = 0.892124 (* 1 = 0.892124 loss)
I0527 11:43:12.771148 21372 sgd_solver.cpp:106] Iteration 205125, lr = 0.003
I0527 11:43:22.674373 21372 solver.cpp:237] Iteration 205500, loss = 1.11792
I0527 11:43:22.674520 21372 solver.cpp:253]     Train net output #0: loss = 1.11792 (* 1 = 1.11792 loss)
I0527 11:43:22.674532 21372 sgd_solver.cpp:106] Iteration 205500, lr = 0.003
I0527 11:43:32.573951 21372 solver.cpp:237] Iteration 205875, loss = 1.21738
I0527 11:43:32.573989 21372 solver.cpp:253]     Train net output #0: loss = 1.21738 (* 1 = 1.21738 loss)
I0527 11:43:32.574012 21372 sgd_solver.cpp:106] Iteration 205875, lr = 0.003
I0527 11:43:42.445431 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_206250.caffemodel
I0527 11:43:42.503944 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_206250.solverstate
I0527 11:44:03.487802 21372 solver.cpp:237] Iteration 206250, loss = 1.10006
I0527 11:44:03.487988 21372 solver.cpp:253]     Train net output #0: loss = 1.10006 (* 1 = 1.10006 loss)
I0527 11:44:03.488005 21372 sgd_solver.cpp:106] Iteration 206250, lr = 0.003
I0527 11:44:13.390281 21372 solver.cpp:237] Iteration 206625, loss = 0.73491
I0527 11:44:13.390316 21372 solver.cpp:253]     Train net output #0: loss = 0.73491 (* 1 = 0.73491 loss)
I0527 11:44:13.390336 21372 sgd_solver.cpp:106] Iteration 206625, lr = 0.003
I0527 11:44:23.286146 21372 solver.cpp:237] Iteration 207000, loss = 1.42669
I0527 11:44:23.286185 21372 solver.cpp:253]     Train net output #0: loss = 1.42669 (* 1 = 1.42669 loss)
I0527 11:44:23.286198 21372 sgd_solver.cpp:106] Iteration 207000, lr = 0.003
I0527 11:44:33.190721 21372 solver.cpp:237] Iteration 207375, loss = 1.20322
I0527 11:44:33.190755 21372 solver.cpp:253]     Train net output #0: loss = 1.20322 (* 1 = 1.20322 loss)
I0527 11:44:33.190769 21372 sgd_solver.cpp:106] Iteration 207375, lr = 0.003
I0527 11:44:43.098580 21372 solver.cpp:237] Iteration 207750, loss = 1.35674
I0527 11:44:43.098724 21372 solver.cpp:253]     Train net output #0: loss = 1.35674 (* 1 = 1.35674 loss)
I0527 11:44:43.098737 21372 sgd_solver.cpp:106] Iteration 207750, lr = 0.003
I0527 11:44:53.004140 21372 solver.cpp:237] Iteration 208125, loss = 1.13933
I0527 11:44:53.004178 21372 solver.cpp:253]     Train net output #0: loss = 1.13933 (* 1 = 1.13933 loss)
I0527 11:44:53.004194 21372 sgd_solver.cpp:106] Iteration 208125, lr = 0.003
I0527 11:45:02.906998 21372 solver.cpp:237] Iteration 208500, loss = 1.35064
I0527 11:45:02.907033 21372 solver.cpp:253]     Train net output #0: loss = 1.35064 (* 1 = 1.35064 loss)
I0527 11:45:02.907047 21372 sgd_solver.cpp:106] Iteration 208500, lr = 0.003
I0527 11:45:33.679142 21372 solver.cpp:237] Iteration 208875, loss = 0.903731
I0527 11:45:33.679322 21372 solver.cpp:253]     Train net output #0: loss = 0.903731 (* 1 = 0.903731 loss)
I0527 11:45:33.679335 21372 sgd_solver.cpp:106] Iteration 208875, lr = 0.003
I0527 11:45:43.571766 21372 solver.cpp:237] Iteration 209250, loss = 0.58001
I0527 11:45:43.571801 21372 solver.cpp:253]     Train net output #0: loss = 0.58001 (* 1 = 0.58001 loss)
I0527 11:45:43.571815 21372 sgd_solver.cpp:106] Iteration 209250, lr = 0.003
I0527 11:45:53.464809 21372 solver.cpp:237] Iteration 209625, loss = 1.56255
I0527 11:45:53.464844 21372 solver.cpp:253]     Train net output #0: loss = 1.56255 (* 1 = 1.56255 loss)
I0527 11:45:53.464859 21372 sgd_solver.cpp:106] Iteration 209625, lr = 0.003
I0527 11:46:03.342331 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_210000.caffemodel
I0527 11:46:03.399055 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_210000.solverstate
I0527 11:46:03.424626 21372 solver.cpp:341] Iteration 210000, Testing net (#0)
I0527 11:46:51.986754 21372 solver.cpp:409]     Test net output #0: accuracy = 0.900671
I0527 11:46:51.986937 21372 solver.cpp:409]     Test net output #1: loss = 0.299179 (* 1 = 0.299179 loss)
I0527 11:46:51.994995 21372 solver.cpp:237] Iteration 210000, loss = 0.940407
I0527 11:46:51.995023 21372 solver.cpp:253]     Train net output #0: loss = 0.940407 (* 1 = 0.940407 loss)
I0527 11:46:51.995038 21372 sgd_solver.cpp:106] Iteration 210000, lr = 0.003
I0527 11:47:01.841697 21372 solver.cpp:237] Iteration 210375, loss = 1.09963
I0527 11:47:01.841732 21372 solver.cpp:253]     Train net output #0: loss = 1.09963 (* 1 = 1.09963 loss)
I0527 11:47:01.841747 21372 sgd_solver.cpp:106] Iteration 210375, lr = 0.003
I0527 11:47:11.696470 21372 solver.cpp:237] Iteration 210750, loss = 0.734629
I0527 11:47:11.696506 21372 solver.cpp:253]     Train net output #0: loss = 0.734629 (* 1 = 0.734629 loss)
I0527 11:47:11.696522 21372 sgd_solver.cpp:106] Iteration 210750, lr = 0.003
I0527 11:47:21.547049 21372 solver.cpp:237] Iteration 211125, loss = 1.12617
I0527 11:47:21.547093 21372 solver.cpp:253]     Train net output #0: loss = 1.12617 (* 1 = 1.12617 loss)
I0527 11:47:21.547111 21372 sgd_solver.cpp:106] Iteration 211125, lr = 0.003
I0527 11:47:52.255111 21372 solver.cpp:237] Iteration 211500, loss = 0.974224
I0527 11:47:52.255286 21372 solver.cpp:253]     Train net output #0: loss = 0.974224 (* 1 = 0.974224 loss)
I0527 11:47:52.255302 21372 sgd_solver.cpp:106] Iteration 211500, lr = 0.003
I0527 11:48:02.103104 21372 solver.cpp:237] Iteration 211875, loss = 0.907267
I0527 11:48:02.103140 21372 solver.cpp:253]     Train net output #0: loss = 0.907267 (* 1 = 0.907267 loss)
I0527 11:48:02.103154 21372 sgd_solver.cpp:106] Iteration 211875, lr = 0.003
I0527 11:48:11.948864 21372 solver.cpp:237] Iteration 212250, loss = 1.24105
I0527 11:48:11.948914 21372 solver.cpp:253]     Train net output #0: loss = 1.24105 (* 1 = 1.24105 loss)
I0527 11:48:11.948927 21372 sgd_solver.cpp:106] Iteration 212250, lr = 0.003
I0527 11:48:21.796248 21372 solver.cpp:237] Iteration 212625, loss = 1.17696
I0527 11:48:21.796279 21372 solver.cpp:253]     Train net output #0: loss = 1.17696 (* 1 = 1.17696 loss)
I0527 11:48:21.796293 21372 sgd_solver.cpp:106] Iteration 212625, lr = 0.003
I0527 11:48:31.649642 21372 solver.cpp:237] Iteration 213000, loss = 1.19751
I0527 11:48:31.649816 21372 solver.cpp:253]     Train net output #0: loss = 1.19751 (* 1 = 1.19751 loss)
I0527 11:48:31.649830 21372 sgd_solver.cpp:106] Iteration 213000, lr = 0.003
I0527 11:48:41.489961 21372 solver.cpp:237] Iteration 213375, loss = 0.995132
I0527 11:48:41.489997 21372 solver.cpp:253]     Train net output #0: loss = 0.995132 (* 1 = 0.995132 loss)
I0527 11:48:41.490015 21372 sgd_solver.cpp:106] Iteration 213375, lr = 0.003
I0527 11:48:51.311422 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_213750.caffemodel
I0527 11:48:51.367841 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_213750.solverstate
I0527 11:49:12.277262 21372 solver.cpp:237] Iteration 213750, loss = 1.01668
I0527 11:49:12.277452 21372 solver.cpp:253]     Train net output #0: loss = 1.01668 (* 1 = 1.01668 loss)
I0527 11:49:12.277469 21372 sgd_solver.cpp:106] Iteration 213750, lr = 0.003
I0527 11:49:22.122761 21372 solver.cpp:237] Iteration 214125, loss = 0.998433
I0527 11:49:22.122812 21372 solver.cpp:253]     Train net output #0: loss = 0.998433 (* 1 = 0.998433 loss)
I0527 11:49:22.122825 21372 sgd_solver.cpp:106] Iteration 214125, lr = 0.003
I0527 11:49:31.966015 21372 solver.cpp:237] Iteration 214500, loss = 1.19973
I0527 11:49:31.966051 21372 solver.cpp:253]     Train net output #0: loss = 1.19973 (* 1 = 1.19973 loss)
I0527 11:49:31.966064 21372 sgd_solver.cpp:106] Iteration 214500, lr = 0.003
I0527 11:49:41.816149 21372 solver.cpp:237] Iteration 214875, loss = 1.36886
I0527 11:49:41.816184 21372 solver.cpp:253]     Train net output #0: loss = 1.36886 (* 1 = 1.36886 loss)
I0527 11:49:41.816201 21372 sgd_solver.cpp:106] Iteration 214875, lr = 0.003
I0527 11:49:51.665407 21372 solver.cpp:237] Iteration 215250, loss = 0.784854
I0527 11:49:51.665563 21372 solver.cpp:253]     Train net output #0: loss = 0.784854 (* 1 = 0.784854 loss)
I0527 11:49:51.665578 21372 sgd_solver.cpp:106] Iteration 215250, lr = 0.003
I0527 11:50:01.513031 21372 solver.cpp:237] Iteration 215625, loss = 1.26963
I0527 11:50:01.513067 21372 solver.cpp:253]     Train net output #0: loss = 1.26963 (* 1 = 1.26963 loss)
I0527 11:50:01.513080 21372 sgd_solver.cpp:106] Iteration 215625, lr = 0.003
I0527 11:50:11.324627 21372 solver.cpp:237] Iteration 216000, loss = 1.273
I0527 11:50:11.324666 21372 solver.cpp:253]     Train net output #0: loss = 1.273 (* 1 = 1.273 loss)
I0527 11:50:11.324683 21372 sgd_solver.cpp:106] Iteration 216000, lr = 0.003
I0527 11:50:42.033588 21372 solver.cpp:237] Iteration 216375, loss = 1.18132
I0527 11:50:42.033763 21372 solver.cpp:253]     Train net output #0: loss = 1.18132 (* 1 = 1.18132 loss)
I0527 11:50:42.033779 21372 sgd_solver.cpp:106] Iteration 216375, lr = 0.003
I0527 11:50:51.843082 21372 solver.cpp:237] Iteration 216750, loss = 1.14006
I0527 11:50:51.843116 21372 solver.cpp:253]     Train net output #0: loss = 1.14006 (* 1 = 1.14006 loss)
I0527 11:50:51.843132 21372 sgd_solver.cpp:106] Iteration 216750, lr = 0.003
I0527 11:51:01.662701 21372 solver.cpp:237] Iteration 217125, loss = 1.3528
I0527 11:51:01.662746 21372 solver.cpp:253]     Train net output #0: loss = 1.3528 (* 1 = 1.3528 loss)
I0527 11:51:01.662765 21372 sgd_solver.cpp:106] Iteration 217125, lr = 0.003
I0527 11:51:11.456718 21372 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_217500.caffemodel
I0527 11:51:11.515446 21372 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_40_lr_0.0030_2016-05-20T15.49.02.997511_iter_217500.solverstate
I0527 11:51:11.540959 21372 solver.cpp:341] Iteration 217500, Testing net (#0)
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
aprun: Apid 11273500: Caught signal Terminated, sending to application
