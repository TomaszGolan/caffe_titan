2811508
I0526 09:32:48.592380 22046 caffe.cpp:184] Using GPUs 0
I0526 09:32:49.018826 22046 solver.cpp:48] Initializing solver from parameters: 
test_iter: 15000
test_interval: 30000
base_lr: 0.0045
display: 1500
max_iter: 1500000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 15000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324.prototxt"
I0526 09:32:49.020442 22046 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324.prototxt
I0526 09:32:49.032598 22046 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0526 09:32:49.032660 22046 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0526 09:32:49.033036 22046 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 10
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0526 09:32:49.033243 22046 layer_factory.hpp:77] Creating layer data_hdf5
I0526 09:32:49.033272 22046 net.cpp:106] Creating Layer data_hdf5
I0526 09:32:49.033296 22046 net.cpp:411] data_hdf5 -> data
I0526 09:32:49.033329 22046 net.cpp:411] data_hdf5 -> label
I0526 09:32:49.033366 22046 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0526 09:32:49.034657 22046 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0526 09:32:49.036859 22046 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0526 09:33:10.592357 22046 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0526 09:33:10.597676 22046 net.cpp:150] Setting up data_hdf5
I0526 09:33:10.597718 22046 net.cpp:157] Top shape: 10 1 127 50 (63500)
I0526 09:33:10.597735 22046 net.cpp:157] Top shape: 10 (10)
I0526 09:33:10.597748 22046 net.cpp:165] Memory required for data: 254040
I0526 09:33:10.597766 22046 layer_factory.hpp:77] Creating layer conv1
I0526 09:33:10.597812 22046 net.cpp:106] Creating Layer conv1
I0526 09:33:10.597826 22046 net.cpp:454] conv1 <- data
I0526 09:33:10.597851 22046 net.cpp:411] conv1 -> conv1
I0526 09:33:10.964447 22046 net.cpp:150] Setting up conv1
I0526 09:33:10.964498 22046 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 09:33:10.964520 22046 net.cpp:165] Memory required for data: 3018840
I0526 09:33:10.964551 22046 layer_factory.hpp:77] Creating layer relu1
I0526 09:33:10.964573 22046 net.cpp:106] Creating Layer relu1
I0526 09:33:10.964612 22046 net.cpp:454] relu1 <- conv1
I0526 09:33:10.964628 22046 net.cpp:397] relu1 -> conv1 (in-place)
I0526 09:33:10.965167 22046 net.cpp:150] Setting up relu1
I0526 09:33:10.965190 22046 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 09:33:10.965204 22046 net.cpp:165] Memory required for data: 5783640
I0526 09:33:10.965216 22046 layer_factory.hpp:77] Creating layer pool1
I0526 09:33:10.965247 22046 net.cpp:106] Creating Layer pool1
I0526 09:33:10.965261 22046 net.cpp:454] pool1 <- conv1
I0526 09:33:10.965277 22046 net.cpp:411] pool1 -> pool1
I0526 09:33:10.965370 22046 net.cpp:150] Setting up pool1
I0526 09:33:10.965389 22046 net.cpp:157] Top shape: 10 12 60 48 (345600)
I0526 09:33:10.965409 22046 net.cpp:165] Memory required for data: 7166040
I0526 09:33:10.965428 22046 layer_factory.hpp:77] Creating layer conv2
I0526 09:33:10.965452 22046 net.cpp:106] Creating Layer conv2
I0526 09:33:10.965466 22046 net.cpp:454] conv2 <- pool1
I0526 09:33:10.965486 22046 net.cpp:411] conv2 -> conv2
I0526 09:33:10.968196 22046 net.cpp:150] Setting up conv2
I0526 09:33:10.968226 22046 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 09:33:10.968240 22046 net.cpp:165] Memory required for data: 9153240
I0526 09:33:10.968267 22046 layer_factory.hpp:77] Creating layer relu2
I0526 09:33:10.968296 22046 net.cpp:106] Creating Layer relu2
I0526 09:33:10.968308 22046 net.cpp:454] relu2 <- conv2
I0526 09:33:10.968324 22046 net.cpp:397] relu2 -> conv2 (in-place)
I0526 09:33:10.968682 22046 net.cpp:150] Setting up relu2
I0526 09:33:10.968701 22046 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 09:33:10.968714 22046 net.cpp:165] Memory required for data: 11140440
I0526 09:33:10.968726 22046 layer_factory.hpp:77] Creating layer pool2
I0526 09:33:10.968751 22046 net.cpp:106] Creating Layer pool2
I0526 09:33:10.968765 22046 net.cpp:454] pool2 <- conv2
I0526 09:33:10.968780 22046 net.cpp:411] pool2 -> pool2
I0526 09:33:10.968874 22046 net.cpp:150] Setting up pool2
I0526 09:33:10.968891 22046 net.cpp:157] Top shape: 10 20 27 46 (248400)
I0526 09:33:10.968906 22046 net.cpp:165] Memory required for data: 12134040
I0526 09:33:10.968926 22046 layer_factory.hpp:77] Creating layer conv3
I0526 09:33:10.968947 22046 net.cpp:106] Creating Layer conv3
I0526 09:33:10.968967 22046 net.cpp:454] conv3 <- pool2
I0526 09:33:10.968983 22046 net.cpp:411] conv3 -> conv3
I0526 09:33:10.971108 22046 net.cpp:150] Setting up conv3
I0526 09:33:10.971133 22046 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 09:33:10.971154 22046 net.cpp:165] Memory required for data: 13218200
I0526 09:33:10.971176 22046 layer_factory.hpp:77] Creating layer relu3
I0526 09:33:10.971207 22046 net.cpp:106] Creating Layer relu3
I0526 09:33:10.971221 22046 net.cpp:454] relu3 <- conv3
I0526 09:33:10.971237 22046 net.cpp:397] relu3 -> conv3 (in-place)
I0526 09:33:10.971730 22046 net.cpp:150] Setting up relu3
I0526 09:33:10.971755 22046 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 09:33:10.971768 22046 net.cpp:165] Memory required for data: 14302360
I0526 09:33:10.971784 22046 layer_factory.hpp:77] Creating layer pool3
I0526 09:33:10.971807 22046 net.cpp:106] Creating Layer pool3
I0526 09:33:10.971822 22046 net.cpp:454] pool3 <- conv3
I0526 09:33:10.971837 22046 net.cpp:411] pool3 -> pool3
I0526 09:33:10.971918 22046 net.cpp:150] Setting up pool3
I0526 09:33:10.971942 22046 net.cpp:157] Top shape: 10 28 11 44 (135520)
I0526 09:33:10.971954 22046 net.cpp:165] Memory required for data: 14844440
I0526 09:33:10.971969 22046 layer_factory.hpp:77] Creating layer conv4
I0526 09:33:10.971997 22046 net.cpp:106] Creating Layer conv4
I0526 09:33:10.972009 22046 net.cpp:454] conv4 <- pool3
I0526 09:33:10.972025 22046 net.cpp:411] conv4 -> conv4
I0526 09:33:10.974750 22046 net.cpp:150] Setting up conv4
I0526 09:33:10.974781 22046 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 09:33:10.974797 22046 net.cpp:165] Memory required for data: 15207320
I0526 09:33:10.974815 22046 layer_factory.hpp:77] Creating layer relu4
I0526 09:33:10.974848 22046 net.cpp:106] Creating Layer relu4
I0526 09:33:10.974863 22046 net.cpp:454] relu4 <- conv4
I0526 09:33:10.974879 22046 net.cpp:397] relu4 -> conv4 (in-place)
I0526 09:33:10.975370 22046 net.cpp:150] Setting up relu4
I0526 09:33:10.975394 22046 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 09:33:10.975407 22046 net.cpp:165] Memory required for data: 15570200
I0526 09:33:10.975424 22046 layer_factory.hpp:77] Creating layer pool4
I0526 09:33:10.975446 22046 net.cpp:106] Creating Layer pool4
I0526 09:33:10.975461 22046 net.cpp:454] pool4 <- conv4
I0526 09:33:10.975476 22046 net.cpp:411] pool4 -> pool4
I0526 09:33:10.975559 22046 net.cpp:150] Setting up pool4
I0526 09:33:10.975582 22046 net.cpp:157] Top shape: 10 36 3 42 (45360)
I0526 09:33:10.975594 22046 net.cpp:165] Memory required for data: 15751640
I0526 09:33:10.975610 22046 layer_factory.hpp:77] Creating layer ip1
I0526 09:33:10.975636 22046 net.cpp:106] Creating Layer ip1
I0526 09:33:10.975649 22046 net.cpp:454] ip1 <- pool4
I0526 09:33:10.975666 22046 net.cpp:411] ip1 -> ip1
I0526 09:33:10.991062 22046 net.cpp:150] Setting up ip1
I0526 09:33:10.991118 22046 net.cpp:157] Top shape: 10 196 (1960)
I0526 09:33:10.991132 22046 net.cpp:165] Memory required for data: 15759480
I0526 09:33:10.991174 22046 layer_factory.hpp:77] Creating layer relu5
I0526 09:33:10.991192 22046 net.cpp:106] Creating Layer relu5
I0526 09:33:10.991205 22046 net.cpp:454] relu5 <- ip1
I0526 09:33:10.991222 22046 net.cpp:397] relu5 -> ip1 (in-place)
I0526 09:33:10.991590 22046 net.cpp:150] Setting up relu5
I0526 09:33:10.991611 22046 net.cpp:157] Top shape: 10 196 (1960)
I0526 09:33:10.991624 22046 net.cpp:165] Memory required for data: 15767320
I0526 09:33:10.991636 22046 layer_factory.hpp:77] Creating layer drop1
I0526 09:33:10.991674 22046 net.cpp:106] Creating Layer drop1
I0526 09:33:10.991688 22046 net.cpp:454] drop1 <- ip1
I0526 09:33:10.991710 22046 net.cpp:397] drop1 -> ip1 (in-place)
I0526 09:33:10.991783 22046 net.cpp:150] Setting up drop1
I0526 09:33:10.991801 22046 net.cpp:157] Top shape: 10 196 (1960)
I0526 09:33:10.991812 22046 net.cpp:165] Memory required for data: 15775160
I0526 09:33:10.991827 22046 layer_factory.hpp:77] Creating layer ip2
I0526 09:33:10.991848 22046 net.cpp:106] Creating Layer ip2
I0526 09:33:10.991861 22046 net.cpp:454] ip2 <- ip1
I0526 09:33:10.991884 22046 net.cpp:411] ip2 -> ip2
I0526 09:33:10.992377 22046 net.cpp:150] Setting up ip2
I0526 09:33:10.992396 22046 net.cpp:157] Top shape: 10 98 (980)
I0526 09:33:10.992409 22046 net.cpp:165] Memory required for data: 15779080
I0526 09:33:10.992430 22046 layer_factory.hpp:77] Creating layer relu6
I0526 09:33:10.992451 22046 net.cpp:106] Creating Layer relu6
I0526 09:33:10.992465 22046 net.cpp:454] relu6 <- ip2
I0526 09:33:10.992480 22046 net.cpp:397] relu6 -> ip2 (in-place)
I0526 09:33:10.993029 22046 net.cpp:150] Setting up relu6
I0526 09:33:10.993052 22046 net.cpp:157] Top shape: 10 98 (980)
I0526 09:33:10.993065 22046 net.cpp:165] Memory required for data: 15783000
I0526 09:33:10.993082 22046 layer_factory.hpp:77] Creating layer drop2
I0526 09:33:10.993104 22046 net.cpp:106] Creating Layer drop2
I0526 09:33:10.993118 22046 net.cpp:454] drop2 <- ip2
I0526 09:33:10.993134 22046 net.cpp:397] drop2 -> ip2 (in-place)
I0526 09:33:10.993191 22046 net.cpp:150] Setting up drop2
I0526 09:33:10.993207 22046 net.cpp:157] Top shape: 10 98 (980)
I0526 09:33:10.993219 22046 net.cpp:165] Memory required for data: 15786920
I0526 09:33:10.993234 22046 layer_factory.hpp:77] Creating layer ip3
I0526 09:33:10.993250 22046 net.cpp:106] Creating Layer ip3
I0526 09:33:10.993266 22046 net.cpp:454] ip3 <- ip2
I0526 09:33:10.993288 22046 net.cpp:411] ip3 -> ip3
I0526 09:33:10.993515 22046 net.cpp:150] Setting up ip3
I0526 09:33:10.993533 22046 net.cpp:157] Top shape: 10 11 (110)
I0526 09:33:10.993546 22046 net.cpp:165] Memory required for data: 15787360
I0526 09:33:10.993566 22046 layer_factory.hpp:77] Creating layer drop3
I0526 09:33:10.993588 22046 net.cpp:106] Creating Layer drop3
I0526 09:33:10.993600 22046 net.cpp:454] drop3 <- ip3
I0526 09:33:10.993616 22046 net.cpp:397] drop3 -> ip3 (in-place)
I0526 09:33:10.993662 22046 net.cpp:150] Setting up drop3
I0526 09:33:10.993685 22046 net.cpp:157] Top shape: 10 11 (110)
I0526 09:33:10.993698 22046 net.cpp:165] Memory required for data: 15787800
I0526 09:33:10.993719 22046 layer_factory.hpp:77] Creating layer loss
I0526 09:33:10.993741 22046 net.cpp:106] Creating Layer loss
I0526 09:33:10.993755 22046 net.cpp:454] loss <- ip3
I0526 09:33:10.993775 22046 net.cpp:454] loss <- label
I0526 09:33:10.993791 22046 net.cpp:411] loss -> loss
I0526 09:33:10.993811 22046 layer_factory.hpp:77] Creating layer loss
I0526 09:33:10.994478 22046 net.cpp:150] Setting up loss
I0526 09:33:10.994499 22046 net.cpp:157] Top shape: (1)
I0526 09:33:10.994521 22046 net.cpp:160]     with loss weight 1
I0526 09:33:10.994581 22046 net.cpp:165] Memory required for data: 15787804
I0526 09:33:10.994596 22046 net.cpp:226] loss needs backward computation.
I0526 09:33:10.994609 22046 net.cpp:226] drop3 needs backward computation.
I0526 09:33:10.994622 22046 net.cpp:226] ip3 needs backward computation.
I0526 09:33:10.994635 22046 net.cpp:226] drop2 needs backward computation.
I0526 09:33:10.994647 22046 net.cpp:226] relu6 needs backward computation.
I0526 09:33:10.994662 22046 net.cpp:226] ip2 needs backward computation.
I0526 09:33:10.994680 22046 net.cpp:226] drop1 needs backward computation.
I0526 09:33:10.994693 22046 net.cpp:226] relu5 needs backward computation.
I0526 09:33:10.994705 22046 net.cpp:226] ip1 needs backward computation.
I0526 09:33:10.994722 22046 net.cpp:226] pool4 needs backward computation.
I0526 09:33:10.994735 22046 net.cpp:226] relu4 needs backward computation.
I0526 09:33:10.994747 22046 net.cpp:226] conv4 needs backward computation.
I0526 09:33:10.994760 22046 net.cpp:226] pool3 needs backward computation.
I0526 09:33:10.994776 22046 net.cpp:226] relu3 needs backward computation.
I0526 09:33:10.994797 22046 net.cpp:226] conv3 needs backward computation.
I0526 09:33:10.994818 22046 net.cpp:226] pool2 needs backward computation.
I0526 09:33:10.994834 22046 net.cpp:226] relu2 needs backward computation.
I0526 09:33:10.994848 22046 net.cpp:226] conv2 needs backward computation.
I0526 09:33:10.994860 22046 net.cpp:226] pool1 needs backward computation.
I0526 09:33:10.994873 22046 net.cpp:226] relu1 needs backward computation.
I0526 09:33:10.994887 22046 net.cpp:226] conv1 needs backward computation.
I0526 09:33:10.994910 22046 net.cpp:228] data_hdf5 does not need backward computation.
I0526 09:33:10.994921 22046 net.cpp:270] This network produces output loss
I0526 09:33:10.994949 22046 net.cpp:283] Network initialization done.
I0526 09:33:10.996610 22046 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324.prototxt
I0526 09:33:10.996690 22046 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0526 09:33:10.997066 22046 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 10
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0526 09:33:10.997292 22046 layer_factory.hpp:77] Creating layer data_hdf5
I0526 09:33:10.997311 22046 net.cpp:106] Creating Layer data_hdf5
I0526 09:33:10.997330 22046 net.cpp:411] data_hdf5 -> data
I0526 09:33:10.997349 22046 net.cpp:411] data_hdf5 -> label
I0526 09:33:10.997371 22046 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0526 09:33:10.998606 22046 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0526 09:33:32.326251 22046 net.cpp:150] Setting up data_hdf5
I0526 09:33:32.326421 22046 net.cpp:157] Top shape: 10 1 127 50 (63500)
I0526 09:33:32.326441 22046 net.cpp:157] Top shape: 10 (10)
I0526 09:33:32.326453 22046 net.cpp:165] Memory required for data: 254040
I0526 09:33:32.326468 22046 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0526 09:33:32.326503 22046 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0526 09:33:32.326535 22046 net.cpp:454] label_data_hdf5_1_split <- label
I0526 09:33:32.326552 22046 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0526 09:33:32.326575 22046 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0526 09:33:32.326663 22046 net.cpp:150] Setting up label_data_hdf5_1_split
I0526 09:33:32.326679 22046 net.cpp:157] Top shape: 10 (10)
I0526 09:33:32.326694 22046 net.cpp:157] Top shape: 10 (10)
I0526 09:33:32.326707 22046 net.cpp:165] Memory required for data: 254120
I0526 09:33:32.326719 22046 layer_factory.hpp:77] Creating layer conv1
I0526 09:33:32.326751 22046 net.cpp:106] Creating Layer conv1
I0526 09:33:32.326766 22046 net.cpp:454] conv1 <- data
I0526 09:33:32.326782 22046 net.cpp:411] conv1 -> conv1
I0526 09:33:32.328788 22046 net.cpp:150] Setting up conv1
I0526 09:33:32.328814 22046 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 09:33:32.328835 22046 net.cpp:165] Memory required for data: 3018920
I0526 09:33:32.328860 22046 layer_factory.hpp:77] Creating layer relu1
I0526 09:33:32.328889 22046 net.cpp:106] Creating Layer relu1
I0526 09:33:32.328903 22046 net.cpp:454] relu1 <- conv1
I0526 09:33:32.328919 22046 net.cpp:397] relu1 -> conv1 (in-place)
I0526 09:33:32.329447 22046 net.cpp:150] Setting up relu1
I0526 09:33:32.329469 22046 net.cpp:157] Top shape: 10 12 120 48 (691200)
I0526 09:33:32.329483 22046 net.cpp:165] Memory required for data: 5783720
I0526 09:33:32.329495 22046 layer_factory.hpp:77] Creating layer pool1
I0526 09:33:32.329524 22046 net.cpp:106] Creating Layer pool1
I0526 09:33:32.329538 22046 net.cpp:454] pool1 <- conv1
I0526 09:33:32.329561 22046 net.cpp:411] pool1 -> pool1
I0526 09:33:32.329660 22046 net.cpp:150] Setting up pool1
I0526 09:33:32.329677 22046 net.cpp:157] Top shape: 10 12 60 48 (345600)
I0526 09:33:32.329690 22046 net.cpp:165] Memory required for data: 7166120
I0526 09:33:32.329702 22046 layer_factory.hpp:77] Creating layer conv2
I0526 09:33:32.329730 22046 net.cpp:106] Creating Layer conv2
I0526 09:33:32.329744 22046 net.cpp:454] conv2 <- pool1
I0526 09:33:32.329761 22046 net.cpp:411] conv2 -> conv2
I0526 09:33:32.331710 22046 net.cpp:150] Setting up conv2
I0526 09:33:32.331734 22046 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 09:33:32.331755 22046 net.cpp:165] Memory required for data: 9153320
I0526 09:33:32.331776 22046 layer_factory.hpp:77] Creating layer relu2
I0526 09:33:32.331796 22046 net.cpp:106] Creating Layer relu2
I0526 09:33:32.331817 22046 net.cpp:454] relu2 <- conv2
I0526 09:33:32.331833 22046 net.cpp:397] relu2 -> conv2 (in-place)
I0526 09:33:32.332182 22046 net.cpp:150] Setting up relu2
I0526 09:33:32.332202 22046 net.cpp:157] Top shape: 10 20 54 46 (496800)
I0526 09:33:32.332216 22046 net.cpp:165] Memory required for data: 11140520
I0526 09:33:32.332231 22046 layer_factory.hpp:77] Creating layer pool2
I0526 09:33:32.332252 22046 net.cpp:106] Creating Layer pool2
I0526 09:33:32.332267 22046 net.cpp:454] pool2 <- conv2
I0526 09:33:32.332281 22046 net.cpp:411] pool2 -> pool2
I0526 09:33:32.332376 22046 net.cpp:150] Setting up pool2
I0526 09:33:32.332391 22046 net.cpp:157] Top shape: 10 20 27 46 (248400)
I0526 09:33:32.332404 22046 net.cpp:165] Memory required for data: 12134120
I0526 09:33:32.332419 22046 layer_factory.hpp:77] Creating layer conv3
I0526 09:33:32.332442 22046 net.cpp:106] Creating Layer conv3
I0526 09:33:32.332463 22046 net.cpp:454] conv3 <- pool2
I0526 09:33:32.332479 22046 net.cpp:411] conv3 -> conv3
I0526 09:33:32.334601 22046 net.cpp:150] Setting up conv3
I0526 09:33:32.334627 22046 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 09:33:32.334646 22046 net.cpp:165] Memory required for data: 13218280
I0526 09:33:32.334669 22046 layer_factory.hpp:77] Creating layer relu3
I0526 09:33:32.334710 22046 net.cpp:106] Creating Layer relu3
I0526 09:33:32.334724 22046 net.cpp:454] relu3 <- conv3
I0526 09:33:32.334740 22046 net.cpp:397] relu3 -> conv3 (in-place)
I0526 09:33:32.335242 22046 net.cpp:150] Setting up relu3
I0526 09:33:32.335265 22046 net.cpp:157] Top shape: 10 28 22 44 (271040)
I0526 09:33:32.335278 22046 net.cpp:165] Memory required for data: 14302440
I0526 09:33:32.335294 22046 layer_factory.hpp:77] Creating layer pool3
I0526 09:33:32.335319 22046 net.cpp:106] Creating Layer pool3
I0526 09:33:32.335332 22046 net.cpp:454] pool3 <- conv3
I0526 09:33:32.335350 22046 net.cpp:411] pool3 -> pool3
I0526 09:33:32.335434 22046 net.cpp:150] Setting up pool3
I0526 09:33:32.335451 22046 net.cpp:157] Top shape: 10 28 11 44 (135520)
I0526 09:33:32.335466 22046 net.cpp:165] Memory required for data: 14844520
I0526 09:33:32.335479 22046 layer_factory.hpp:77] Creating layer conv4
I0526 09:33:32.335506 22046 net.cpp:106] Creating Layer conv4
I0526 09:33:32.335520 22046 net.cpp:454] conv4 <- pool3
I0526 09:33:32.335536 22046 net.cpp:411] conv4 -> conv4
I0526 09:33:32.337631 22046 net.cpp:150] Setting up conv4
I0526 09:33:32.337656 22046 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 09:33:32.337677 22046 net.cpp:165] Memory required for data: 15207400
I0526 09:33:32.337695 22046 layer_factory.hpp:77] Creating layer relu4
I0526 09:33:32.337715 22046 net.cpp:106] Creating Layer relu4
I0526 09:33:32.337736 22046 net.cpp:454] relu4 <- conv4
I0526 09:33:32.337754 22046 net.cpp:397] relu4 -> conv4 (in-place)
I0526 09:33:32.338239 22046 net.cpp:150] Setting up relu4
I0526 09:33:32.338263 22046 net.cpp:157] Top shape: 10 36 6 42 (90720)
I0526 09:33:32.338276 22046 net.cpp:165] Memory required for data: 15570280
I0526 09:33:32.338292 22046 layer_factory.hpp:77] Creating layer pool4
I0526 09:33:32.338317 22046 net.cpp:106] Creating Layer pool4
I0526 09:33:32.338330 22046 net.cpp:454] pool4 <- conv4
I0526 09:33:32.338347 22046 net.cpp:411] pool4 -> pool4
I0526 09:33:32.338433 22046 net.cpp:150] Setting up pool4
I0526 09:33:32.338449 22046 net.cpp:157] Top shape: 10 36 3 42 (45360)
I0526 09:33:32.338464 22046 net.cpp:165] Memory required for data: 15751720
I0526 09:33:32.338476 22046 layer_factory.hpp:77] Creating layer ip1
I0526 09:33:32.338500 22046 net.cpp:106] Creating Layer ip1
I0526 09:33:32.338513 22046 net.cpp:454] ip1 <- pool4
I0526 09:33:32.338536 22046 net.cpp:411] ip1 -> ip1
I0526 09:33:32.353876 22046 net.cpp:150] Setting up ip1
I0526 09:33:32.353909 22046 net.cpp:157] Top shape: 10 196 (1960)
I0526 09:33:32.353930 22046 net.cpp:165] Memory required for data: 15759560
I0526 09:33:32.353956 22046 layer_factory.hpp:77] Creating layer relu5
I0526 09:33:32.353979 22046 net.cpp:106] Creating Layer relu5
I0526 09:33:32.354003 22046 net.cpp:454] relu5 <- ip1
I0526 09:33:32.354019 22046 net.cpp:397] relu5 -> ip1 (in-place)
I0526 09:33:32.354387 22046 net.cpp:150] Setting up relu5
I0526 09:33:32.354406 22046 net.cpp:157] Top shape: 10 196 (1960)
I0526 09:33:32.354420 22046 net.cpp:165] Memory required for data: 15767400
I0526 09:33:32.354435 22046 layer_factory.hpp:77] Creating layer drop1
I0526 09:33:32.354465 22046 net.cpp:106] Creating Layer drop1
I0526 09:33:32.354478 22046 net.cpp:454] drop1 <- ip1
I0526 09:33:32.354495 22046 net.cpp:397] drop1 -> ip1 (in-place)
I0526 09:33:32.354552 22046 net.cpp:150] Setting up drop1
I0526 09:33:32.354568 22046 net.cpp:157] Top shape: 10 196 (1960)
I0526 09:33:32.354581 22046 net.cpp:165] Memory required for data: 15775240
I0526 09:33:32.354593 22046 layer_factory.hpp:77] Creating layer ip2
I0526 09:33:32.354614 22046 net.cpp:106] Creating Layer ip2
I0526 09:33:32.354626 22046 net.cpp:454] ip2 <- ip1
I0526 09:33:32.354650 22046 net.cpp:411] ip2 -> ip2
I0526 09:33:32.355147 22046 net.cpp:150] Setting up ip2
I0526 09:33:32.355166 22046 net.cpp:157] Top shape: 10 98 (980)
I0526 09:33:32.355178 22046 net.cpp:165] Memory required for data: 15779160
I0526 09:33:32.355200 22046 layer_factory.hpp:77] Creating layer relu6
I0526 09:33:32.355234 22046 net.cpp:106] Creating Layer relu6
I0526 09:33:32.355247 22046 net.cpp:454] relu6 <- ip2
I0526 09:33:32.355263 22046 net.cpp:397] relu6 -> ip2 (in-place)
I0526 09:33:32.355836 22046 net.cpp:150] Setting up relu6
I0526 09:33:32.355859 22046 net.cpp:157] Top shape: 10 98 (980)
I0526 09:33:32.355873 22046 net.cpp:165] Memory required for data: 15783080
I0526 09:33:32.355886 22046 layer_factory.hpp:77] Creating layer drop2
I0526 09:33:32.355906 22046 net.cpp:106] Creating Layer drop2
I0526 09:33:32.355927 22046 net.cpp:454] drop2 <- ip2
I0526 09:33:32.355944 22046 net.cpp:397] drop2 -> ip2 (in-place)
I0526 09:33:32.355998 22046 net.cpp:150] Setting up drop2
I0526 09:33:32.356020 22046 net.cpp:157] Top shape: 10 98 (980)
I0526 09:33:32.356034 22046 net.cpp:165] Memory required for data: 15787000
I0526 09:33:32.356045 22046 layer_factory.hpp:77] Creating layer ip3
I0526 09:33:32.356072 22046 net.cpp:106] Creating Layer ip3
I0526 09:33:32.356086 22046 net.cpp:454] ip3 <- ip2
I0526 09:33:32.356104 22046 net.cpp:411] ip3 -> ip3
I0526 09:33:32.356346 22046 net.cpp:150] Setting up ip3
I0526 09:33:32.356365 22046 net.cpp:157] Top shape: 10 11 (110)
I0526 09:33:32.356379 22046 net.cpp:165] Memory required for data: 15787440
I0526 09:33:32.356400 22046 layer_factory.hpp:77] Creating layer drop3
I0526 09:33:32.356415 22046 net.cpp:106] Creating Layer drop3
I0526 09:33:32.356434 22046 net.cpp:454] drop3 <- ip3
I0526 09:33:32.356451 22046 net.cpp:397] drop3 -> ip3 (in-place)
I0526 09:33:32.356498 22046 net.cpp:150] Setting up drop3
I0526 09:33:32.356515 22046 net.cpp:157] Top shape: 10 11 (110)
I0526 09:33:32.356528 22046 net.cpp:165] Memory required for data: 15787880
I0526 09:33:32.356547 22046 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0526 09:33:32.356564 22046 net.cpp:106] Creating Layer ip3_drop3_0_split
I0526 09:33:32.356578 22046 net.cpp:454] ip3_drop3_0_split <- ip3
I0526 09:33:32.356595 22046 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0526 09:33:32.356612 22046 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0526 09:33:32.356699 22046 net.cpp:150] Setting up ip3_drop3_0_split
I0526 09:33:32.356727 22046 net.cpp:157] Top shape: 10 11 (110)
I0526 09:33:32.356744 22046 net.cpp:157] Top shape: 10 11 (110)
I0526 09:33:32.356757 22046 net.cpp:165] Memory required for data: 15788760
I0526 09:33:32.356770 22046 layer_factory.hpp:77] Creating layer accuracy
I0526 09:33:32.356799 22046 net.cpp:106] Creating Layer accuracy
I0526 09:33:32.356813 22046 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0526 09:33:32.356827 22046 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0526 09:33:32.356847 22046 net.cpp:411] accuracy -> accuracy
I0526 09:33:32.356878 22046 net.cpp:150] Setting up accuracy
I0526 09:33:32.356894 22046 net.cpp:157] Top shape: (1)
I0526 09:33:32.356909 22046 net.cpp:165] Memory required for data: 15788764
I0526 09:33:32.356922 22046 layer_factory.hpp:77] Creating layer loss
I0526 09:33:32.356937 22046 net.cpp:106] Creating Layer loss
I0526 09:33:32.356952 22046 net.cpp:454] loss <- ip3_drop3_0_split_1
I0526 09:33:32.356973 22046 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0526 09:33:32.356989 22046 net.cpp:411] loss -> loss
I0526 09:33:32.357012 22046 layer_factory.hpp:77] Creating layer loss
I0526 09:33:32.357522 22046 net.cpp:150] Setting up loss
I0526 09:33:32.357543 22046 net.cpp:157] Top shape: (1)
I0526 09:33:32.357555 22046 net.cpp:160]     with loss weight 1
I0526 09:33:32.357583 22046 net.cpp:165] Memory required for data: 15788768
I0526 09:33:32.357602 22046 net.cpp:226] loss needs backward computation.
I0526 09:33:32.357617 22046 net.cpp:228] accuracy does not need backward computation.
I0526 09:33:32.357633 22046 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0526 09:33:32.357647 22046 net.cpp:226] drop3 needs backward computation.
I0526 09:33:32.357659 22046 net.cpp:226] ip3 needs backward computation.
I0526 09:33:32.357676 22046 net.cpp:226] drop2 needs backward computation.
I0526 09:33:32.357694 22046 net.cpp:226] relu6 needs backward computation.
I0526 09:33:32.357717 22046 net.cpp:226] ip2 needs backward computation.
I0526 09:33:32.357733 22046 net.cpp:226] drop1 needs backward computation.
I0526 09:33:32.357745 22046 net.cpp:226] relu5 needs backward computation.
I0526 09:33:32.357758 22046 net.cpp:226] ip1 needs backward computation.
I0526 09:33:32.357771 22046 net.cpp:226] pool4 needs backward computation.
I0526 09:33:32.357784 22046 net.cpp:226] relu4 needs backward computation.
I0526 09:33:32.357805 22046 net.cpp:226] conv4 needs backward computation.
I0526 09:33:32.357818 22046 net.cpp:226] pool3 needs backward computation.
I0526 09:33:32.357834 22046 net.cpp:226] relu3 needs backward computation.
I0526 09:33:32.357847 22046 net.cpp:226] conv3 needs backward computation.
I0526 09:33:32.357859 22046 net.cpp:226] pool2 needs backward computation.
I0526 09:33:32.357872 22046 net.cpp:226] relu2 needs backward computation.
I0526 09:33:32.357887 22046 net.cpp:226] conv2 needs backward computation.
I0526 09:33:32.357906 22046 net.cpp:226] pool1 needs backward computation.
I0526 09:33:32.357920 22046 net.cpp:226] relu1 needs backward computation.
I0526 09:33:32.357933 22046 net.cpp:226] conv1 needs backward computation.
I0526 09:33:32.357949 22046 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0526 09:33:32.357964 22046 net.cpp:228] data_hdf5 does not need backward computation.
I0526 09:33:32.357975 22046 net.cpp:270] This network produces output accuracy
I0526 09:33:32.357987 22046 net.cpp:270] This network produces output loss
I0526 09:33:32.358027 22046 net.cpp:283] Network initialization done.
I0526 09:33:32.358175 22046 solver.cpp:60] Solver scaffolding done.
I0526 09:33:32.359344 22046 caffe.cpp:202] Resuming from /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_450000.solverstate
I0526 09:33:32.579764 22046 sgd_solver.cpp:318] SGDSolver: restoring history
I0526 09:33:32.585201 22046 caffe.cpp:212] Starting Optimization
I0526 09:33:32.585247 22046 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0526 09:33:32.585268 22046 solver.cpp:289] Learning Rate Policy: fixed
I0526 09:33:32.586522 22046 solver.cpp:341] Iteration 450000, Testing net (#0)
I0526 09:34:33.191699 22046 solver.cpp:409]     Test net output #0: accuracy = 0.883626
I0526 09:34:33.191864 22046 solver.cpp:409]     Test net output #1: loss = 0.368824 (* 1 = 0.368824 loss)
I0526 09:34:33.209899 22046 solver.cpp:237] Iteration 450000, loss = 1.05024
I0526 09:34:33.209938 22046 solver.cpp:253]     Train net output #0: loss = 1.05024 (* 1 = 1.05024 loss)
I0526 09:34:33.209959 22046 sgd_solver.cpp:106] Iteration 450000, lr = 0.0045
I0526 09:34:50.001474 22046 solver.cpp:237] Iteration 451500, loss = 1.31794
I0526 09:34:50.001514 22046 solver.cpp:253]     Train net output #0: loss = 1.31794 (* 1 = 1.31794 loss)
I0526 09:34:50.001533 22046 sgd_solver.cpp:106] Iteration 451500, lr = 0.0045
I0526 09:35:06.863126 22046 solver.cpp:237] Iteration 453000, loss = 2.2103
I0526 09:35:06.863294 22046 solver.cpp:253]     Train net output #0: loss = 2.2103 (* 1 = 2.2103 loss)
I0526 09:35:06.863312 22046 sgd_solver.cpp:106] Iteration 453000, lr = 0.0045
I0526 09:35:23.761335 22046 solver.cpp:237] Iteration 454500, loss = 1.86697
I0526 09:35:23.761392 22046 solver.cpp:253]     Train net output #0: loss = 1.86697 (* 1 = 1.86697 loss)
I0526 09:35:23.761426 22046 sgd_solver.cpp:106] Iteration 454500, lr = 0.0045
I0526 09:35:40.833644 22046 solver.cpp:237] Iteration 456000, loss = 1.46381
I0526 09:35:40.833787 22046 solver.cpp:253]     Train net output #0: loss = 1.46381 (* 1 = 1.46381 loss)
I0526 09:35:40.833804 22046 sgd_solver.cpp:106] Iteration 456000, lr = 0.0045
I0526 09:35:57.708585 22046 solver.cpp:237] Iteration 457500, loss = 0.838753
I0526 09:35:57.708643 22046 solver.cpp:253]     Train net output #0: loss = 0.838752 (* 1 = 0.838752 loss)
I0526 09:35:57.708660 22046 sgd_solver.cpp:106] Iteration 457500, lr = 0.0045
I0526 09:36:14.494101 22046 solver.cpp:237] Iteration 459000, loss = 1.23097
I0526 09:36:14.494257 22046 solver.cpp:253]     Train net output #0: loss = 1.23097 (* 1 = 1.23097 loss)
I0526 09:36:14.494274 22046 sgd_solver.cpp:106] Iteration 459000, lr = 0.0045
I0526 09:36:53.285722 22046 solver.cpp:237] Iteration 460500, loss = 0.883653
I0526 09:36:53.285893 22046 solver.cpp:253]     Train net output #0: loss = 0.883653 (* 1 = 0.883653 loss)
I0526 09:36:53.285912 22046 sgd_solver.cpp:106] Iteration 460500, lr = 0.0045
I0526 09:37:09.931288 22046 solver.cpp:237] Iteration 462000, loss = 1.6002
I0526 09:37:09.931345 22046 solver.cpp:253]     Train net output #0: loss = 1.6002 (* 1 = 1.6002 loss)
I0526 09:37:09.931363 22046 sgd_solver.cpp:106] Iteration 462000, lr = 0.0045
I0526 09:37:26.526480 22046 solver.cpp:237] Iteration 463500, loss = 1.80723
I0526 09:37:26.526625 22046 solver.cpp:253]     Train net output #0: loss = 1.80723 (* 1 = 1.80723 loss)
I0526 09:37:26.526643 22046 sgd_solver.cpp:106] Iteration 463500, lr = 0.0045
I0526 09:37:43.438451 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_465000.caffemodel
I0526 09:37:43.485060 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_465000.solverstate
I0526 09:37:43.514667 22046 solver.cpp:237] Iteration 465000, loss = 0.678847
I0526 09:37:43.514724 22046 solver.cpp:253]     Train net output #0: loss = 0.678846 (* 1 = 0.678846 loss)
I0526 09:37:43.514742 22046 sgd_solver.cpp:106] Iteration 465000, lr = 0.0045
I0526 09:38:00.432490 22046 solver.cpp:237] Iteration 466500, loss = 1.46601
I0526 09:38:00.432656 22046 solver.cpp:253]     Train net output #0: loss = 1.46601 (* 1 = 1.46601 loss)
I0526 09:38:00.432673 22046 sgd_solver.cpp:106] Iteration 466500, lr = 0.0045
I0526 09:38:17.294322 22046 solver.cpp:237] Iteration 468000, loss = 0.979567
I0526 09:38:17.294363 22046 solver.cpp:253]     Train net output #0: loss = 0.979567 (* 1 = 0.979567 loss)
I0526 09:38:17.294383 22046 sgd_solver.cpp:106] Iteration 468000, lr = 0.0045
I0526 09:38:33.908251 22046 solver.cpp:237] Iteration 469500, loss = 0.833046
I0526 09:38:33.908412 22046 solver.cpp:253]     Train net output #0: loss = 0.833045 (* 1 = 0.833045 loss)
I0526 09:38:33.908430 22046 sgd_solver.cpp:106] Iteration 469500, lr = 0.0045
I0526 09:39:12.796262 22046 solver.cpp:237] Iteration 471000, loss = 1.08731
I0526 09:39:12.796443 22046 solver.cpp:253]     Train net output #0: loss = 1.08731 (* 1 = 1.08731 loss)
I0526 09:39:12.796460 22046 sgd_solver.cpp:106] Iteration 471000, lr = 0.0045
I0526 09:39:29.757161 22046 solver.cpp:237] Iteration 472500, loss = 1.11403
I0526 09:39:29.757200 22046 solver.cpp:253]     Train net output #0: loss = 1.11403 (* 1 = 1.11403 loss)
I0526 09:39:29.757218 22046 sgd_solver.cpp:106] Iteration 472500, lr = 0.0045
I0526 09:39:46.577451 22046 solver.cpp:237] Iteration 474000, loss = 0.868428
I0526 09:39:46.577615 22046 solver.cpp:253]     Train net output #0: loss = 0.868429 (* 1 = 0.868429 loss)
I0526 09:39:46.577632 22046 sgd_solver.cpp:106] Iteration 474000, lr = 0.0045
I0526 09:40:03.330540 22046 solver.cpp:237] Iteration 475500, loss = 0.721876
I0526 09:40:03.330598 22046 solver.cpp:253]     Train net output #0: loss = 0.721877 (* 1 = 0.721877 loss)
I0526 09:40:03.330616 22046 sgd_solver.cpp:106] Iteration 475500, lr = 0.0045
I0526 09:40:20.330482 22046 solver.cpp:237] Iteration 477000, loss = 1.59964
I0526 09:40:20.330627 22046 solver.cpp:253]     Train net output #0: loss = 1.59964 (* 1 = 1.59964 loss)
I0526 09:40:20.330644 22046 sgd_solver.cpp:106] Iteration 477000, lr = 0.0045
I0526 09:40:36.976323 22046 solver.cpp:237] Iteration 478500, loss = 1.38074
I0526 09:40:36.976377 22046 solver.cpp:253]     Train net output #0: loss = 1.38074 (* 1 = 1.38074 loss)
I0526 09:40:36.976395 22046 sgd_solver.cpp:106] Iteration 478500, lr = 0.0045
I0526 09:40:53.633416 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_480000.caffemodel
I0526 09:40:53.679694 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_480000.solverstate
I0526 09:40:53.710224 22046 solver.cpp:341] Iteration 480000, Testing net (#0)
I0526 09:41:53.029796 22046 solver.cpp:409]     Test net output #0: accuracy = 0.892348
I0526 09:41:53.029958 22046 solver.cpp:409]     Test net output #1: loss = 0.358269 (* 1 = 0.358269 loss)
I0526 09:42:15.239428 22046 solver.cpp:237] Iteration 480000, loss = 0.95218
I0526 09:42:15.239492 22046 solver.cpp:253]     Train net output #0: loss = 0.952181 (* 1 = 0.952181 loss)
I0526 09:42:15.239518 22046 sgd_solver.cpp:106] Iteration 480000, lr = 0.0045
I0526 09:42:32.101812 22046 solver.cpp:237] Iteration 481500, loss = 1.46854
I0526 09:42:32.101982 22046 solver.cpp:253]     Train net output #0: loss = 1.46854 (* 1 = 1.46854 loss)
I0526 09:42:32.102007 22046 sgd_solver.cpp:106] Iteration 481500, lr = 0.0045
I0526 09:42:48.738451 22046 solver.cpp:237] Iteration 483000, loss = 0.948036
I0526 09:42:48.738490 22046 solver.cpp:253]     Train net output #0: loss = 0.948036 (* 1 = 0.948036 loss)
I0526 09:42:48.738508 22046 sgd_solver.cpp:106] Iteration 483000, lr = 0.0045
I0526 09:43:05.827285 22046 solver.cpp:237] Iteration 484500, loss = 1.38583
I0526 09:43:05.827443 22046 solver.cpp:253]     Train net output #0: loss = 1.38583 (* 1 = 1.38583 loss)
I0526 09:43:05.827461 22046 sgd_solver.cpp:106] Iteration 484500, lr = 0.0045
I0526 09:43:22.942335 22046 solver.cpp:237] Iteration 486000, loss = 0.951532
I0526 09:43:22.942396 22046 solver.cpp:253]     Train net output #0: loss = 0.951532 (* 1 = 0.951532 loss)
I0526 09:43:22.942420 22046 sgd_solver.cpp:106] Iteration 486000, lr = 0.0045
I0526 09:43:39.831918 22046 solver.cpp:237] Iteration 487500, loss = 1.26348
I0526 09:43:39.832061 22046 solver.cpp:253]     Train net output #0: loss = 1.26348 (* 1 = 1.26348 loss)
I0526 09:43:39.832077 22046 sgd_solver.cpp:106] Iteration 487500, lr = 0.0045
I0526 09:43:56.775207 22046 solver.cpp:237] Iteration 489000, loss = 0.924429
I0526 09:43:56.775264 22046 solver.cpp:253]     Train net output #0: loss = 0.924431 (* 1 = 0.924431 loss)
I0526 09:43:56.775282 22046 sgd_solver.cpp:106] Iteration 489000, lr = 0.0045
I0526 09:44:35.936233 22046 solver.cpp:237] Iteration 490500, loss = 1.1264
I0526 09:44:35.936408 22046 solver.cpp:253]     Train net output #0: loss = 1.1264 (* 1 = 1.1264 loss)
I0526 09:44:35.936426 22046 sgd_solver.cpp:106] Iteration 490500, lr = 0.0045
I0526 09:44:52.789408 22046 solver.cpp:237] Iteration 492000, loss = 1.23523
I0526 09:44:52.789448 22046 solver.cpp:253]     Train net output #0: loss = 1.23523 (* 1 = 1.23523 loss)
I0526 09:44:52.789465 22046 sgd_solver.cpp:106] Iteration 492000, lr = 0.0045
I0526 09:45:09.816064 22046 solver.cpp:237] Iteration 493500, loss = 0.842012
I0526 09:45:09.816232 22046 solver.cpp:253]     Train net output #0: loss = 0.842015 (* 1 = 0.842015 loss)
I0526 09:45:09.816251 22046 sgd_solver.cpp:106] Iteration 493500, lr = 0.0045
I0526 09:45:26.787235 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_495000.caffemodel
I0526 09:45:26.835304 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_495000.solverstate
I0526 09:45:26.866670 22046 solver.cpp:237] Iteration 495000, loss = 1.27966
I0526 09:45:26.866734 22046 solver.cpp:253]     Train net output #0: loss = 1.27966 (* 1 = 1.27966 loss)
I0526 09:45:26.866752 22046 sgd_solver.cpp:106] Iteration 495000, lr = 0.0045
I0526 09:45:43.676736 22046 solver.cpp:237] Iteration 496500, loss = 3.0232
I0526 09:45:43.676885 22046 solver.cpp:253]     Train net output #0: loss = 3.0232 (* 1 = 3.0232 loss)
I0526 09:45:43.676903 22046 sgd_solver.cpp:106] Iteration 496500, lr = 0.0045
I0526 09:46:00.664866 22046 solver.cpp:237] Iteration 498000, loss = 0.984687
I0526 09:46:00.664924 22046 solver.cpp:253]     Train net output #0: loss = 0.98469 (* 1 = 0.98469 loss)
I0526 09:46:00.664942 22046 sgd_solver.cpp:106] Iteration 498000, lr = 0.0045
I0526 09:46:17.738232 22046 solver.cpp:237] Iteration 499500, loss = 1.75303
I0526 09:46:17.738394 22046 solver.cpp:253]     Train net output #0: loss = 1.75303 (* 1 = 1.75303 loss)
I0526 09:46:17.738414 22046 sgd_solver.cpp:106] Iteration 499500, lr = 0.0045
I0526 09:46:57.116103 22046 solver.cpp:237] Iteration 501000, loss = 0.723139
I0526 09:46:57.116272 22046 solver.cpp:253]     Train net output #0: loss = 0.723142 (* 1 = 0.723142 loss)
I0526 09:46:57.116289 22046 sgd_solver.cpp:106] Iteration 501000, lr = 0.0045
I0526 09:47:14.099994 22046 solver.cpp:237] Iteration 502500, loss = 0.633826
I0526 09:47:14.100051 22046 solver.cpp:253]     Train net output #0: loss = 0.633829 (* 1 = 0.633829 loss)
I0526 09:47:14.100069 22046 sgd_solver.cpp:106] Iteration 502500, lr = 0.0045
I0526 09:47:30.889037 22046 solver.cpp:237] Iteration 504000, loss = 1.93399
I0526 09:47:30.889200 22046 solver.cpp:253]     Train net output #0: loss = 1.934 (* 1 = 1.934 loss)
I0526 09:47:30.889219 22046 sgd_solver.cpp:106] Iteration 504000, lr = 0.0045
I0526 09:47:47.531445 22046 solver.cpp:237] Iteration 505500, loss = 1.36617
I0526 09:47:47.531482 22046 solver.cpp:253]     Train net output #0: loss = 1.36617 (* 1 = 1.36617 loss)
I0526 09:47:47.531502 22046 sgd_solver.cpp:106] Iteration 505500, lr = 0.0045
I0526 09:48:04.299175 22046 solver.cpp:237] Iteration 507000, loss = 0.979936
I0526 09:48:04.299340 22046 solver.cpp:253]     Train net output #0: loss = 0.979937 (* 1 = 0.979937 loss)
I0526 09:48:04.299357 22046 sgd_solver.cpp:106] Iteration 507000, lr = 0.0045
I0526 09:48:21.254794 22046 solver.cpp:237] Iteration 508500, loss = 0.816196
I0526 09:48:21.254853 22046 solver.cpp:253]     Train net output #0: loss = 0.816197 (* 1 = 0.816197 loss)
I0526 09:48:21.254880 22046 sgd_solver.cpp:106] Iteration 508500, lr = 0.0045
I0526 09:48:37.868155 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_510000.caffemodel
I0526 09:48:37.916169 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_510000.solverstate
I0526 09:48:37.943822 22046 solver.cpp:341] Iteration 510000, Testing net (#0)
I0526 09:49:58.101305 22046 solver.cpp:409]     Test net output #0: accuracy = 0.886298
I0526 09:49:58.101477 22046 solver.cpp:409]     Test net output #1: loss = 0.387679 (* 1 = 0.387679 loss)
I0526 09:50:20.304111 22046 solver.cpp:237] Iteration 510000, loss = 2.20044
I0526 09:50:20.304173 22046 solver.cpp:253]     Train net output #0: loss = 2.20044 (* 1 = 2.20044 loss)
I0526 09:50:20.304193 22046 sgd_solver.cpp:106] Iteration 510000, lr = 0.0045
I0526 09:50:37.494781 22046 solver.cpp:237] Iteration 511500, loss = 1.04121
I0526 09:50:37.494951 22046 solver.cpp:253]     Train net output #0: loss = 1.04121 (* 1 = 1.04121 loss)
I0526 09:50:37.494968 22046 sgd_solver.cpp:106] Iteration 511500, lr = 0.0045
I0526 09:50:54.517222 22046 solver.cpp:237] Iteration 513000, loss = 1.25676
I0526 09:50:54.517277 22046 solver.cpp:253]     Train net output #0: loss = 1.25676 (* 1 = 1.25676 loss)
I0526 09:50:54.517295 22046 sgd_solver.cpp:106] Iteration 513000, lr = 0.0045
I0526 09:51:11.313495 22046 solver.cpp:237] Iteration 514500, loss = 1.13304
I0526 09:51:11.313642 22046 solver.cpp:253]     Train net output #0: loss = 1.13304 (* 1 = 1.13304 loss)
I0526 09:51:11.313659 22046 sgd_solver.cpp:106] Iteration 514500, lr = 0.0045
I0526 09:51:28.477396 22046 solver.cpp:237] Iteration 516000, loss = 1.18033
I0526 09:51:28.477453 22046 solver.cpp:253]     Train net output #0: loss = 1.18033 (* 1 = 1.18033 loss)
I0526 09:51:28.477470 22046 sgd_solver.cpp:106] Iteration 516000, lr = 0.0045
I0526 09:51:45.549536 22046 solver.cpp:237] Iteration 517500, loss = 0.473822
I0526 09:51:45.549701 22046 solver.cpp:253]     Train net output #0: loss = 0.473823 (* 1 = 0.473823 loss)
I0526 09:51:45.549718 22046 sgd_solver.cpp:106] Iteration 517500, lr = 0.0045
I0526 09:52:02.510361 22046 solver.cpp:237] Iteration 519000, loss = 1.11122
I0526 09:52:02.510398 22046 solver.cpp:253]     Train net output #0: loss = 1.11122 (* 1 = 1.11122 loss)
I0526 09:52:02.510417 22046 sgd_solver.cpp:106] Iteration 519000, lr = 0.0045
I0526 09:52:41.929697 22046 solver.cpp:237] Iteration 520500, loss = 1.12427
I0526 09:52:41.929872 22046 solver.cpp:253]     Train net output #0: loss = 1.12427 (* 1 = 1.12427 loss)
I0526 09:52:41.929890 22046 sgd_solver.cpp:106] Iteration 520500, lr = 0.0045
I0526 09:52:58.850158 22046 solver.cpp:237] Iteration 522000, loss = 1.30605
I0526 09:52:58.850214 22046 solver.cpp:253]     Train net output #0: loss = 1.30605 (* 1 = 1.30605 loss)
I0526 09:52:58.850239 22046 sgd_solver.cpp:106] Iteration 522000, lr = 0.0045
I0526 09:53:16.081986 22046 solver.cpp:237] Iteration 523500, loss = 0.373176
I0526 09:53:16.082132 22046 solver.cpp:253]     Train net output #0: loss = 0.373177 (* 1 = 0.373177 loss)
I0526 09:53:16.082149 22046 sgd_solver.cpp:106] Iteration 523500, lr = 0.0045
I0526 09:53:32.775280 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_525000.caffemodel
I0526 09:53:32.823338 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_525000.solverstate
I0526 09:53:32.854779 22046 solver.cpp:237] Iteration 525000, loss = 1.57625
I0526 09:53:32.854842 22046 solver.cpp:253]     Train net output #0: loss = 1.57625 (* 1 = 1.57625 loss)
I0526 09:53:32.854861 22046 sgd_solver.cpp:106] Iteration 525000, lr = 0.0045
I0526 09:53:49.611531 22046 solver.cpp:237] Iteration 526500, loss = 1.878
I0526 09:53:49.611703 22046 solver.cpp:253]     Train net output #0: loss = 1.878 (* 1 = 1.878 loss)
I0526 09:53:49.611722 22046 sgd_solver.cpp:106] Iteration 526500, lr = 0.0045
I0526 09:54:06.586097 22046 solver.cpp:237] Iteration 528000, loss = 0.662405
I0526 09:54:06.586139 22046 solver.cpp:253]     Train net output #0: loss = 0.662406 (* 1 = 0.662406 loss)
I0526 09:54:06.586156 22046 sgd_solver.cpp:106] Iteration 528000, lr = 0.0045
I0526 09:54:23.393571 22046 solver.cpp:237] Iteration 529500, loss = 0.795
I0526 09:54:23.393748 22046 solver.cpp:253]     Train net output #0: loss = 0.795002 (* 1 = 0.795002 loss)
I0526 09:54:23.393765 22046 sgd_solver.cpp:106] Iteration 529500, lr = 0.0045
I0526 09:55:02.426645 22046 solver.cpp:237] Iteration 531000, loss = 0.689978
I0526 09:55:02.426823 22046 solver.cpp:253]     Train net output #0: loss = 0.68998 (* 1 = 0.68998 loss)
I0526 09:55:02.426842 22046 sgd_solver.cpp:106] Iteration 531000, lr = 0.0045
I0526 09:55:19.477988 22046 solver.cpp:237] Iteration 532500, loss = 0.601385
I0526 09:55:19.478026 22046 solver.cpp:253]     Train net output #0: loss = 0.601387 (* 1 = 0.601387 loss)
I0526 09:55:19.478044 22046 sgd_solver.cpp:106] Iteration 532500, lr = 0.0045
I0526 09:55:36.650830 22046 solver.cpp:237] Iteration 534000, loss = 1.34106
I0526 09:55:36.650991 22046 solver.cpp:253]     Train net output #0: loss = 1.34106 (* 1 = 1.34106 loss)
I0526 09:55:36.651008 22046 sgd_solver.cpp:106] Iteration 534000, lr = 0.0045
I0526 09:55:53.667906 22046 solver.cpp:237] Iteration 535500, loss = 1.45137
I0526 09:55:53.667958 22046 solver.cpp:253]     Train net output #0: loss = 1.45137 (* 1 = 1.45137 loss)
I0526 09:55:53.667975 22046 sgd_solver.cpp:106] Iteration 535500, lr = 0.0045
I0526 09:56:10.308529 22046 solver.cpp:237] Iteration 537000, loss = 0.875532
I0526 09:56:10.308677 22046 solver.cpp:253]     Train net output #0: loss = 0.875533 (* 1 = 0.875533 loss)
I0526 09:56:10.308694 22046 sgd_solver.cpp:106] Iteration 537000, lr = 0.0045
I0526 09:56:27.064612 22046 solver.cpp:237] Iteration 538500, loss = 1.02313
I0526 09:56:27.064664 22046 solver.cpp:253]     Train net output #0: loss = 1.02313 (* 1 = 1.02313 loss)
I0526 09:56:27.064682 22046 sgd_solver.cpp:106] Iteration 538500, lr = 0.0045
I0526 09:56:43.862911 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_540000.caffemodel
I0526 09:56:43.927422 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_540000.solverstate
I0526 09:56:43.954125 22046 solver.cpp:341] Iteration 540000, Testing net (#0)
I0526 09:57:49.743651 22046 solver.cpp:409]     Test net output #0: accuracy = 0.889865
I0526 09:57:49.743819 22046 solver.cpp:409]     Test net output #1: loss = 0.410719 (* 1 = 0.410719 loss)
I0526 09:58:11.866912 22046 solver.cpp:237] Iteration 540000, loss = 1.12495
I0526 09:58:11.866974 22046 solver.cpp:253]     Train net output #0: loss = 1.12495 (* 1 = 1.12495 loss)
I0526 09:58:11.867000 22046 sgd_solver.cpp:106] Iteration 540000, lr = 0.0045
I0526 09:58:28.834916 22046 solver.cpp:237] Iteration 541500, loss = 1.05132
I0526 09:58:28.835089 22046 solver.cpp:253]     Train net output #0: loss = 1.05132 (* 1 = 1.05132 loss)
I0526 09:58:28.835106 22046 sgd_solver.cpp:106] Iteration 541500, lr = 0.0045
I0526 09:58:45.894723 22046 solver.cpp:237] Iteration 543000, loss = 1.51934
I0526 09:58:45.894780 22046 solver.cpp:253]     Train net output #0: loss = 1.51934 (* 1 = 1.51934 loss)
I0526 09:58:45.894807 22046 sgd_solver.cpp:106] Iteration 543000, lr = 0.0045
I0526 09:59:03.091114 22046 solver.cpp:237] Iteration 544500, loss = 2.07701
I0526 09:59:03.091262 22046 solver.cpp:253]     Train net output #0: loss = 2.07701 (* 1 = 2.07701 loss)
I0526 09:59:03.091279 22046 sgd_solver.cpp:106] Iteration 544500, lr = 0.0045
I0526 09:59:19.888020 22046 solver.cpp:237] Iteration 546000, loss = 1.41124
I0526 09:59:19.888079 22046 solver.cpp:253]     Train net output #0: loss = 1.41124 (* 1 = 1.41124 loss)
I0526 09:59:19.888098 22046 sgd_solver.cpp:106] Iteration 546000, lr = 0.0045
I0526 09:59:36.571239 22046 solver.cpp:237] Iteration 547500, loss = 1.27758
I0526 09:59:36.571419 22046 solver.cpp:253]     Train net output #0: loss = 1.27758 (* 1 = 1.27758 loss)
I0526 09:59:36.571436 22046 sgd_solver.cpp:106] Iteration 547500, lr = 0.0045
I0526 09:59:53.375267 22046 solver.cpp:237] Iteration 549000, loss = 0.944077
I0526 09:59:53.375308 22046 solver.cpp:253]     Train net output #0: loss = 0.944076 (* 1 = 0.944076 loss)
I0526 09:59:53.375325 22046 sgd_solver.cpp:106] Iteration 549000, lr = 0.0045
I0526 10:00:32.482553 22046 solver.cpp:237] Iteration 550500, loss = 1.4267
I0526 10:00:32.482723 22046 solver.cpp:253]     Train net output #0: loss = 1.4267 (* 1 = 1.4267 loss)
I0526 10:00:32.482740 22046 sgd_solver.cpp:106] Iteration 550500, lr = 0.0045
I0526 10:00:49.397518 22046 solver.cpp:237] Iteration 552000, loss = 1.14043
I0526 10:00:49.397577 22046 solver.cpp:253]     Train net output #0: loss = 1.14043 (* 1 = 1.14043 loss)
I0526 10:00:49.397600 22046 sgd_solver.cpp:106] Iteration 552000, lr = 0.0045
I0526 10:01:06.458436 22046 solver.cpp:237] Iteration 553500, loss = 0.706125
I0526 10:01:06.458587 22046 solver.cpp:253]     Train net output #0: loss = 0.706123 (* 1 = 0.706123 loss)
I0526 10:01:06.458605 22046 sgd_solver.cpp:106] Iteration 553500, lr = 0.0045
I0526 10:01:23.400162 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_555000.caffemodel
I0526 10:01:23.465148 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_555000.solverstate
I0526 10:01:23.556421 22046 solver.cpp:237] Iteration 555000, loss = 1.12117
I0526 10:01:23.556478 22046 solver.cpp:253]     Train net output #0: loss = 1.12117 (* 1 = 1.12117 loss)
I0526 10:01:23.556505 22046 sgd_solver.cpp:106] Iteration 555000, lr = 0.0045
I0526 10:01:40.467005 22046 solver.cpp:237] Iteration 556500, loss = 1.13555
I0526 10:01:40.467175 22046 solver.cpp:253]     Train net output #0: loss = 1.13555 (* 1 = 1.13555 loss)
I0526 10:01:40.467192 22046 sgd_solver.cpp:106] Iteration 556500, lr = 0.0045
I0526 10:01:57.672449 22046 solver.cpp:237] Iteration 558000, loss = 1.0109
I0526 10:01:57.672488 22046 solver.cpp:253]     Train net output #0: loss = 1.01089 (* 1 = 1.01089 loss)
I0526 10:01:57.672508 22046 sgd_solver.cpp:106] Iteration 558000, lr = 0.0045
I0526 10:02:14.455417 22046 solver.cpp:237] Iteration 559500, loss = 1.15911
I0526 10:02:14.455585 22046 solver.cpp:253]     Train net output #0: loss = 1.15911 (* 1 = 1.15911 loss)
I0526 10:02:14.455601 22046 sgd_solver.cpp:106] Iteration 559500, lr = 0.0045
I0526 10:02:53.356467 22046 solver.cpp:237] Iteration 561000, loss = 1.00481
I0526 10:02:53.356637 22046 solver.cpp:253]     Train net output #0: loss = 1.00481 (* 1 = 1.00481 loss)
I0526 10:02:53.356657 22046 sgd_solver.cpp:106] Iteration 561000, lr = 0.0045
I0526 10:03:10.145099 22046 solver.cpp:237] Iteration 562500, loss = 3.17092
I0526 10:03:10.145139 22046 solver.cpp:253]     Train net output #0: loss = 3.17091 (* 1 = 3.17091 loss)
I0526 10:03:10.145156 22046 sgd_solver.cpp:106] Iteration 562500, lr = 0.0045
I0526 10:03:26.841913 22046 solver.cpp:237] Iteration 564000, loss = 1.78173
I0526 10:03:26.842073 22046 solver.cpp:253]     Train net output #0: loss = 1.78173 (* 1 = 1.78173 loss)
I0526 10:03:26.842090 22046 sgd_solver.cpp:106] Iteration 564000, lr = 0.0045
I0526 10:03:43.447468 22046 solver.cpp:237] Iteration 565500, loss = 1.1117
I0526 10:03:43.447525 22046 solver.cpp:253]     Train net output #0: loss = 1.1117 (* 1 = 1.1117 loss)
I0526 10:03:43.447551 22046 sgd_solver.cpp:106] Iteration 565500, lr = 0.0045
I0526 10:04:00.095659 22046 solver.cpp:237] Iteration 567000, loss = 1.44732
I0526 10:04:00.095813 22046 solver.cpp:253]     Train net output #0: loss = 1.44732 (* 1 = 1.44732 loss)
I0526 10:04:00.095829 22046 sgd_solver.cpp:106] Iteration 567000, lr = 0.0045
I0526 10:04:16.959751 22046 solver.cpp:237] Iteration 568500, loss = 1.82972
I0526 10:04:16.959808 22046 solver.cpp:253]     Train net output #0: loss = 1.82972 (* 1 = 1.82972 loss)
I0526 10:04:16.959825 22046 sgd_solver.cpp:106] Iteration 568500, lr = 0.0045
I0526 10:04:34.033996 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_570000.caffemodel
I0526 10:04:34.132119 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_570000.solverstate
I0526 10:04:34.210150 22046 solver.cpp:341] Iteration 570000, Testing net (#0)
I0526 10:05:54.780726 22046 solver.cpp:409]     Test net output #0: accuracy = 0.888404
I0526 10:05:54.780899 22046 solver.cpp:409]     Test net output #1: loss = 0.357531 (* 1 = 0.357531 loss)
I0526 10:06:16.963008 22046 solver.cpp:237] Iteration 570000, loss = 1.54951
I0526 10:06:16.963069 22046 solver.cpp:253]     Train net output #0: loss = 1.54951 (* 1 = 1.54951 loss)
I0526 10:06:16.963095 22046 sgd_solver.cpp:106] Iteration 570000, lr = 0.0045
I0526 10:06:33.565940 22046 solver.cpp:237] Iteration 571500, loss = 1.14366
I0526 10:06:33.566090 22046 solver.cpp:253]     Train net output #0: loss = 1.14366 (* 1 = 1.14366 loss)
I0526 10:06:33.566107 22046 sgd_solver.cpp:106] Iteration 571500, lr = 0.0045
I0526 10:06:50.345353 22046 solver.cpp:237] Iteration 573000, loss = 1.21215
I0526 10:06:50.345407 22046 solver.cpp:253]     Train net output #0: loss = 1.21215 (* 1 = 1.21215 loss)
I0526 10:06:50.345427 22046 sgd_solver.cpp:106] Iteration 573000, lr = 0.0045
I0526 10:07:07.230008 22046 solver.cpp:237] Iteration 574500, loss = 1.01775
I0526 10:07:07.230175 22046 solver.cpp:253]     Train net output #0: loss = 1.01774 (* 1 = 1.01774 loss)
I0526 10:07:07.230195 22046 sgd_solver.cpp:106] Iteration 574500, lr = 0.0045
I0526 10:07:24.179679 22046 solver.cpp:237] Iteration 576000, loss = 1.08511
I0526 10:07:24.179718 22046 solver.cpp:253]     Train net output #0: loss = 1.08511 (* 1 = 1.08511 loss)
I0526 10:07:24.179735 22046 sgd_solver.cpp:106] Iteration 576000, lr = 0.0045
I0526 10:07:41.323927 22046 solver.cpp:237] Iteration 577500, loss = 0.990775
I0526 10:07:41.324096 22046 solver.cpp:253]     Train net output #0: loss = 0.990773 (* 1 = 0.990773 loss)
I0526 10:07:41.324113 22046 sgd_solver.cpp:106] Iteration 577500, lr = 0.0045
I0526 10:07:58.345144 22046 solver.cpp:237] Iteration 579000, loss = 1.22447
I0526 10:07:58.345197 22046 solver.cpp:253]     Train net output #0: loss = 1.22447 (* 1 = 1.22447 loss)
I0526 10:07:58.345216 22046 sgd_solver.cpp:106] Iteration 579000, lr = 0.0045
I0526 10:08:37.296489 22046 solver.cpp:237] Iteration 580500, loss = 1.31108
I0526 10:08:37.296660 22046 solver.cpp:253]     Train net output #0: loss = 1.31108 (* 1 = 1.31108 loss)
I0526 10:08:37.296679 22046 sgd_solver.cpp:106] Iteration 580500, lr = 0.0045
I0526 10:08:54.232818 22046 solver.cpp:237] Iteration 582000, loss = 0.432683
I0526 10:08:54.232874 22046 solver.cpp:253]     Train net output #0: loss = 0.432682 (* 1 = 0.432682 loss)
I0526 10:08:54.232892 22046 sgd_solver.cpp:106] Iteration 582000, lr = 0.0045
I0526 10:09:11.249516 22046 solver.cpp:237] Iteration 583500, loss = 0.915101
I0526 10:09:11.249687 22046 solver.cpp:253]     Train net output #0: loss = 0.915101 (* 1 = 0.915101 loss)
I0526 10:09:11.249704 22046 sgd_solver.cpp:106] Iteration 583500, lr = 0.0045
I0526 10:09:28.129228 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_585000.caffemodel
I0526 10:09:31.535845 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_585000.solverstate
I0526 10:09:31.635151 22046 solver.cpp:237] Iteration 585000, loss = 3.45282
I0526 10:09:31.635206 22046 solver.cpp:253]     Train net output #0: loss = 3.45282 (* 1 = 3.45282 loss)
I0526 10:09:31.635236 22046 sgd_solver.cpp:106] Iteration 585000, lr = 0.0045
I0526 10:09:48.610801 22046 solver.cpp:237] Iteration 586500, loss = 0.85722
I0526 10:09:48.610983 22046 solver.cpp:253]     Train net output #0: loss = 0.857219 (* 1 = 0.857219 loss)
I0526 10:09:48.611001 22046 sgd_solver.cpp:106] Iteration 586500, lr = 0.0045
I0526 10:10:05.425204 22046 solver.cpp:237] Iteration 588000, loss = 2.03058
I0526 10:10:05.425259 22046 solver.cpp:253]     Train net output #0: loss = 2.03058 (* 1 = 2.03058 loss)
I0526 10:10:05.425276 22046 sgd_solver.cpp:106] Iteration 588000, lr = 0.0045
I0526 10:10:22.074977 22046 solver.cpp:237] Iteration 589500, loss = 1.26469
I0526 10:10:22.075131 22046 solver.cpp:253]     Train net output #0: loss = 1.26469 (* 1 = 1.26469 loss)
I0526 10:10:22.075148 22046 sgd_solver.cpp:106] Iteration 589500, lr = 0.0045
I0526 10:11:01.080234 22046 solver.cpp:237] Iteration 591000, loss = 0.797553
I0526 10:11:01.080412 22046 solver.cpp:253]     Train net output #0: loss = 0.797553 (* 1 = 0.797553 loss)
I0526 10:11:01.080430 22046 sgd_solver.cpp:106] Iteration 591000, lr = 0.0045
I0526 10:11:17.799101 22046 solver.cpp:237] Iteration 592500, loss = 1.35706
I0526 10:11:17.799156 22046 solver.cpp:253]     Train net output #0: loss = 1.35706 (* 1 = 1.35706 loss)
I0526 10:11:17.799173 22046 sgd_solver.cpp:106] Iteration 592500, lr = 0.0045
I0526 10:11:34.443728 22046 solver.cpp:237] Iteration 594000, loss = 1.08285
I0526 10:11:34.443883 22046 solver.cpp:253]     Train net output #0: loss = 1.08285 (* 1 = 1.08285 loss)
I0526 10:11:34.443900 22046 sgd_solver.cpp:106] Iteration 594000, lr = 0.0045
I0526 10:11:51.058508 22046 solver.cpp:237] Iteration 595500, loss = 0.99532
I0526 10:11:51.058567 22046 solver.cpp:253]     Train net output #0: loss = 0.99532 (* 1 = 0.99532 loss)
I0526 10:11:51.058584 22046 sgd_solver.cpp:106] Iteration 595500, lr = 0.0045
I0526 10:12:07.749073 22046 solver.cpp:237] Iteration 597000, loss = 0.855686
I0526 10:12:07.749245 22046 solver.cpp:253]     Train net output #0: loss = 0.855686 (* 1 = 0.855686 loss)
I0526 10:12:07.749264 22046 sgd_solver.cpp:106] Iteration 597000, lr = 0.0045
I0526 10:12:24.519181 22046 solver.cpp:237] Iteration 598500, loss = 1.23086
I0526 10:12:24.519219 22046 solver.cpp:253]     Train net output #0: loss = 1.23086 (* 1 = 1.23086 loss)
I0526 10:12:24.519237 22046 sgd_solver.cpp:106] Iteration 598500, lr = 0.0045
I0526 10:12:41.383034 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_600000.caffemodel
I0526 10:12:41.432149 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_600000.solverstate
I0526 10:12:41.466938 22046 solver.cpp:341] Iteration 600000, Testing net (#0)
I0526 10:13:47.735062 22046 solver.cpp:409]     Test net output #0: accuracy = 0.888349
I0526 10:13:47.735234 22046 solver.cpp:409]     Test net output #1: loss = 0.377976 (* 1 = 0.377976 loss)
I0526 10:14:13.925206 22046 solver.cpp:237] Iteration 600000, loss = 0.83873
I0526 10:14:13.925266 22046 solver.cpp:253]     Train net output #0: loss = 0.838729 (* 1 = 0.838729 loss)
I0526 10:14:13.925295 22046 sgd_solver.cpp:106] Iteration 600000, lr = 0.0045
I0526 10:14:30.664911 22046 solver.cpp:237] Iteration 601500, loss = 1.12074
I0526 10:14:30.665065 22046 solver.cpp:253]     Train net output #0: loss = 1.12074 (* 1 = 1.12074 loss)
I0526 10:14:30.665081 22046 sgd_solver.cpp:106] Iteration 601500, lr = 0.0045
I0526 10:14:47.694092 22046 solver.cpp:237] Iteration 603000, loss = 1.74934
I0526 10:14:47.694145 22046 solver.cpp:253]     Train net output #0: loss = 1.74934 (* 1 = 1.74934 loss)
I0526 10:14:47.694164 22046 sgd_solver.cpp:106] Iteration 603000, lr = 0.0045
I0526 10:15:04.678201 22046 solver.cpp:237] Iteration 604500, loss = 0.923617
I0526 10:15:04.678385 22046 solver.cpp:253]     Train net output #0: loss = 0.923616 (* 1 = 0.923616 loss)
I0526 10:15:04.678401 22046 sgd_solver.cpp:106] Iteration 604500, lr = 0.0045
I0526 10:15:21.582916 22046 solver.cpp:237] Iteration 606000, loss = 1.48927
I0526 10:15:21.582953 22046 solver.cpp:253]     Train net output #0: loss = 1.48927 (* 1 = 1.48927 loss)
I0526 10:15:21.582972 22046 sgd_solver.cpp:106] Iteration 606000, lr = 0.0045
I0526 10:15:38.235008 22046 solver.cpp:237] Iteration 607500, loss = 0.676856
I0526 10:15:38.235180 22046 solver.cpp:253]     Train net output #0: loss = 0.676854 (* 1 = 0.676854 loss)
I0526 10:15:38.235198 22046 sgd_solver.cpp:106] Iteration 607500, lr = 0.0045
I0526 10:15:54.857122 22046 solver.cpp:237] Iteration 609000, loss = 0.999291
I0526 10:15:54.857179 22046 solver.cpp:253]     Train net output #0: loss = 0.999288 (* 1 = 0.999288 loss)
I0526 10:15:54.857197 22046 sgd_solver.cpp:106] Iteration 609000, lr = 0.0045
I0526 10:16:32.347321 22046 solver.cpp:237] Iteration 610500, loss = 0.86887
I0526 10:16:32.347503 22046 solver.cpp:253]     Train net output #0: loss = 0.868868 (* 1 = 0.868868 loss)
I0526 10:16:32.347522 22046 sgd_solver.cpp:106] Iteration 610500, lr = 0.0045
I0526 10:16:49.145400 22046 solver.cpp:237] Iteration 612000, loss = 1.12777
I0526 10:16:49.145457 22046 solver.cpp:253]     Train net output #0: loss = 1.12776 (* 1 = 1.12776 loss)
I0526 10:16:49.145474 22046 sgd_solver.cpp:106] Iteration 612000, lr = 0.0045
I0526 10:17:05.956789 22046 solver.cpp:237] Iteration 613500, loss = 1.79709
I0526 10:17:05.956961 22046 solver.cpp:253]     Train net output #0: loss = 1.79709 (* 1 = 1.79709 loss)
I0526 10:17:05.956979 22046 sgd_solver.cpp:106] Iteration 613500, lr = 0.0045
I0526 10:17:22.564363 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_615000.caffemodel
I0526 10:17:22.615151 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_615000.solverstate
I0526 10:17:22.648344 22046 solver.cpp:237] Iteration 615000, loss = 0.574692
I0526 10:17:22.648402 22046 solver.cpp:253]     Train net output #0: loss = 0.574691 (* 1 = 0.574691 loss)
I0526 10:17:22.648421 22046 sgd_solver.cpp:106] Iteration 615000, lr = 0.0045
I0526 10:17:39.565784 22046 solver.cpp:237] Iteration 616500, loss = 0.987626
I0526 10:17:39.565959 22046 solver.cpp:253]     Train net output #0: loss = 0.987624 (* 1 = 0.987624 loss)
I0526 10:17:39.565976 22046 sgd_solver.cpp:106] Iteration 616500, lr = 0.0045
I0526 10:17:56.585000 22046 solver.cpp:237] Iteration 618000, loss = 1.03223
I0526 10:17:56.585054 22046 solver.cpp:253]     Train net output #0: loss = 1.03223 (* 1 = 1.03223 loss)
I0526 10:17:56.585072 22046 sgd_solver.cpp:106] Iteration 618000, lr = 0.0045
I0526 10:18:13.550670 22046 solver.cpp:237] Iteration 619500, loss = 0.649792
I0526 10:18:13.550824 22046 solver.cpp:253]     Train net output #0: loss = 0.64979 (* 1 = 0.64979 loss)
I0526 10:18:13.550842 22046 sgd_solver.cpp:106] Iteration 619500, lr = 0.0045
I0526 10:18:54.650842 22046 solver.cpp:237] Iteration 621000, loss = 1.22474
I0526 10:18:54.651021 22046 solver.cpp:253]     Train net output #0: loss = 1.22474 (* 1 = 1.22474 loss)
I0526 10:18:54.651038 22046 sgd_solver.cpp:106] Iteration 621000, lr = 0.0045
I0526 10:19:11.736641 22046 solver.cpp:237] Iteration 622500, loss = 0.718801
I0526 10:19:11.736701 22046 solver.cpp:253]     Train net output #0: loss = 0.7188 (* 1 = 0.7188 loss)
I0526 10:19:11.736724 22046 sgd_solver.cpp:106] Iteration 622500, lr = 0.0045
I0526 10:19:28.350167 22046 solver.cpp:237] Iteration 624000, loss = 1.03408
I0526 10:19:28.350319 22046 solver.cpp:253]     Train net output #0: loss = 1.03408 (* 1 = 1.03408 loss)
I0526 10:19:28.350335 22046 sgd_solver.cpp:106] Iteration 624000, lr = 0.0045
I0526 10:19:44.984572 22046 solver.cpp:237] Iteration 625500, loss = 1.02816
I0526 10:19:44.984629 22046 solver.cpp:253]     Train net output #0: loss = 1.02816 (* 1 = 1.02816 loss)
I0526 10:19:44.984647 22046 sgd_solver.cpp:106] Iteration 625500, lr = 0.0045
I0526 10:20:01.652835 22046 solver.cpp:237] Iteration 627000, loss = 0.99664
I0526 10:20:01.653018 22046 solver.cpp:253]     Train net output #0: loss = 0.996638 (* 1 = 0.996638 loss)
I0526 10:20:01.653036 22046 sgd_solver.cpp:106] Iteration 627000, lr = 0.0045
I0526 10:20:18.533416 22046 solver.cpp:237] Iteration 628500, loss = 1.27678
I0526 10:20:18.533453 22046 solver.cpp:253]     Train net output #0: loss = 1.27678 (* 1 = 1.27678 loss)
I0526 10:20:18.533471 22046 sgd_solver.cpp:106] Iteration 628500, lr = 0.0045
I0526 10:20:35.236101 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_630000.caffemodel
I0526 10:20:35.288386 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_630000.solverstate
I0526 10:20:35.434077 22046 solver.cpp:341] Iteration 630000, Testing net (#0)
I0526 10:21:55.957711 22046 solver.cpp:409]     Test net output #0: accuracy = 0.889922
I0526 10:21:55.957880 22046 solver.cpp:409]     Test net output #1: loss = 0.36872 (* 1 = 0.36872 loss)
I0526 10:22:16.982548 22046 solver.cpp:237] Iteration 630000, loss = 0.839985
I0526 10:22:16.982611 22046 solver.cpp:253]     Train net output #0: loss = 0.839983 (* 1 = 0.839983 loss)
I0526 10:22:16.982631 22046 sgd_solver.cpp:106] Iteration 630000, lr = 0.0045
I0526 10:22:33.731964 22046 solver.cpp:237] Iteration 631500, loss = 1.83209
I0526 10:22:33.732138 22046 solver.cpp:253]     Train net output #0: loss = 1.83209 (* 1 = 1.83209 loss)
I0526 10:22:33.732156 22046 sgd_solver.cpp:106] Iteration 631500, lr = 0.0045
I0526 10:22:50.770665 22046 solver.cpp:237] Iteration 633000, loss = 0.888601
I0526 10:22:50.770704 22046 solver.cpp:253]     Train net output #0: loss = 0.888601 (* 1 = 0.888601 loss)
I0526 10:22:50.770725 22046 sgd_solver.cpp:106] Iteration 633000, lr = 0.0045
I0526 10:23:07.580037 22046 solver.cpp:237] Iteration 634500, loss = 1.37991
I0526 10:23:07.580209 22046 solver.cpp:253]     Train net output #0: loss = 1.37991 (* 1 = 1.37991 loss)
I0526 10:23:07.580225 22046 sgd_solver.cpp:106] Iteration 634500, lr = 0.0045
I0526 10:23:24.445291 22046 solver.cpp:237] Iteration 636000, loss = 1.19336
I0526 10:23:24.445346 22046 solver.cpp:253]     Train net output #0: loss = 1.19336 (* 1 = 1.19336 loss)
I0526 10:23:24.445371 22046 sgd_solver.cpp:106] Iteration 636000, lr = 0.0045
I0526 10:23:41.486884 22046 solver.cpp:237] Iteration 637500, loss = 1.50962
I0526 10:23:41.487040 22046 solver.cpp:253]     Train net output #0: loss = 1.50962 (* 1 = 1.50962 loss)
I0526 10:23:41.487056 22046 sgd_solver.cpp:106] Iteration 637500, lr = 0.0045
I0526 10:23:58.165254 22046 solver.cpp:237] Iteration 639000, loss = 0.965227
I0526 10:23:58.165310 22046 solver.cpp:253]     Train net output #0: loss = 0.965225 (* 1 = 0.965225 loss)
I0526 10:23:58.165328 22046 sgd_solver.cpp:106] Iteration 639000, lr = 0.0045
I0526 10:24:46.298180 22046 solver.cpp:237] Iteration 640500, loss = 1.47995
I0526 10:24:46.298359 22046 solver.cpp:253]     Train net output #0: loss = 1.47995 (* 1 = 1.47995 loss)
I0526 10:24:46.298377 22046 sgd_solver.cpp:106] Iteration 640500, lr = 0.0045
I0526 10:25:03.307873 22046 solver.cpp:237] Iteration 642000, loss = 0.718518
I0526 10:25:03.307934 22046 solver.cpp:253]     Train net output #0: loss = 0.718516 (* 1 = 0.718516 loss)
I0526 10:25:03.307958 22046 sgd_solver.cpp:106] Iteration 642000, lr = 0.0045
I0526 10:25:20.160550 22046 solver.cpp:237] Iteration 643500, loss = 1.59951
I0526 10:25:20.160704 22046 solver.cpp:253]     Train net output #0: loss = 1.59951 (* 1 = 1.59951 loss)
I0526 10:25:20.160722 22046 sgd_solver.cpp:106] Iteration 643500, lr = 0.0045
I0526 10:25:37.106734 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_645000.caffemodel
I0526 10:25:40.461194 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_645000.solverstate
I0526 10:25:40.524991 22046 solver.cpp:237] Iteration 645000, loss = 0.577978
I0526 10:25:40.525048 22046 solver.cpp:253]     Train net output #0: loss = 0.577975 (* 1 = 0.577975 loss)
I0526 10:25:40.525069 22046 sgd_solver.cpp:106] Iteration 645000, lr = 0.0045
I0526 10:25:57.297813 22046 solver.cpp:237] Iteration 646500, loss = 1.01428
I0526 10:25:57.297999 22046 solver.cpp:253]     Train net output #0: loss = 1.01428 (* 1 = 1.01428 loss)
I0526 10:25:57.298017 22046 sgd_solver.cpp:106] Iteration 646500, lr = 0.0045
I0526 10:26:13.935915 22046 solver.cpp:237] Iteration 648000, loss = 1.79195
I0526 10:26:13.935974 22046 solver.cpp:253]     Train net output #0: loss = 1.79195 (* 1 = 1.79195 loss)
I0526 10:26:13.935999 22046 sgd_solver.cpp:106] Iteration 648000, lr = 0.0045
I0526 10:26:30.578739 22046 solver.cpp:237] Iteration 649500, loss = 1.54459
I0526 10:26:30.578894 22046 solver.cpp:253]     Train net output #0: loss = 1.54458 (* 1 = 1.54458 loss)
I0526 10:26:30.578912 22046 sgd_solver.cpp:106] Iteration 649500, lr = 0.0045
I0526 10:27:08.282063 22046 solver.cpp:237] Iteration 651000, loss = 0.712829
I0526 10:27:08.282245 22046 solver.cpp:253]     Train net output #0: loss = 0.712825 (* 1 = 0.712825 loss)
I0526 10:27:08.282264 22046 sgd_solver.cpp:106] Iteration 651000, lr = 0.0045
I0526 10:27:25.175602 22046 solver.cpp:237] Iteration 652500, loss = 0.684648
I0526 10:27:25.175640 22046 solver.cpp:253]     Train net output #0: loss = 0.684644 (* 1 = 0.684644 loss)
I0526 10:27:25.175658 22046 sgd_solver.cpp:106] Iteration 652500, lr = 0.0045
I0526 10:27:42.179088 22046 solver.cpp:237] Iteration 654000, loss = 1.48931
I0526 10:27:42.179262 22046 solver.cpp:253]     Train net output #0: loss = 1.4893 (* 1 = 1.4893 loss)
I0526 10:27:42.179280 22046 sgd_solver.cpp:106] Iteration 654000, lr = 0.0045
I0526 10:27:59.078574 22046 solver.cpp:237] Iteration 655500, loss = 1.29274
I0526 10:27:59.078631 22046 solver.cpp:253]     Train net output #0: loss = 1.29274 (* 1 = 1.29274 loss)
I0526 10:27:59.078656 22046 sgd_solver.cpp:106] Iteration 655500, lr = 0.0045
I0526 10:28:15.730075 22046 solver.cpp:237] Iteration 657000, loss = 1.91782
I0526 10:28:15.730232 22046 solver.cpp:253]     Train net output #0: loss = 1.91781 (* 1 = 1.91781 loss)
I0526 10:28:15.730247 22046 sgd_solver.cpp:106] Iteration 657000, lr = 0.0045
I0526 10:28:32.580086 22046 solver.cpp:237] Iteration 658500, loss = 0.754123
I0526 10:28:32.580143 22046 solver.cpp:253]     Train net output #0: loss = 0.754118 (* 1 = 0.754118 loss)
I0526 10:28:32.580160 22046 sgd_solver.cpp:106] Iteration 658500, lr = 0.0045
I0526 10:28:49.493845 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_660000.caffemodel
I0526 10:28:49.612304 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_660000.solverstate
I0526 10:28:49.648918 22046 solver.cpp:341] Iteration 660000, Testing net (#0)
I0526 10:29:48.922899 22046 solver.cpp:409]     Test net output #0: accuracy = 0.887976
I0526 10:29:48.923072 22046 solver.cpp:409]     Test net output #1: loss = 0.35552 (* 1 = 0.35552 loss)
I0526 10:30:09.790982 22046 solver.cpp:237] Iteration 660000, loss = 1.53944
I0526 10:30:09.791048 22046 solver.cpp:253]     Train net output #0: loss = 1.53943 (* 1 = 1.53943 loss)
I0526 10:30:09.791065 22046 sgd_solver.cpp:106] Iteration 660000, lr = 0.0045
I0526 10:30:26.674850 22046 solver.cpp:237] Iteration 661500, loss = 0.808345
I0526 10:30:26.675041 22046 solver.cpp:253]     Train net output #0: loss = 0.808341 (* 1 = 0.808341 loss)
I0526 10:30:26.675058 22046 sgd_solver.cpp:106] Iteration 661500, lr = 0.0045
I0526 10:30:43.887217 22046 solver.cpp:237] Iteration 663000, loss = 0.947477
I0526 10:30:43.887254 22046 solver.cpp:253]     Train net output #0: loss = 0.947473 (* 1 = 0.947473 loss)
I0526 10:30:43.887274 22046 sgd_solver.cpp:106] Iteration 663000, lr = 0.0045
I0526 10:31:00.916782 22046 solver.cpp:237] Iteration 664500, loss = 0.901232
I0526 10:31:00.916957 22046 solver.cpp:253]     Train net output #0: loss = 0.901228 (* 1 = 0.901228 loss)
I0526 10:31:00.916975 22046 sgd_solver.cpp:106] Iteration 664500, lr = 0.0045
I0526 10:31:17.815251 22046 solver.cpp:237] Iteration 666000, loss = 0.71409
I0526 10:31:17.815309 22046 solver.cpp:253]     Train net output #0: loss = 0.714086 (* 1 = 0.714086 loss)
I0526 10:31:17.815336 22046 sgd_solver.cpp:106] Iteration 666000, lr = 0.0045
I0526 10:31:34.411713 22046 solver.cpp:237] Iteration 667500, loss = 1.16662
I0526 10:31:34.411870 22046 solver.cpp:253]     Train net output #0: loss = 1.16661 (* 1 = 1.16661 loss)
I0526 10:31:34.411887 22046 sgd_solver.cpp:106] Iteration 667500, lr = 0.0045
I0526 10:31:51.517290 22046 solver.cpp:237] Iteration 669000, loss = 1.12274
I0526 10:31:51.517348 22046 solver.cpp:253]     Train net output #0: loss = 1.12274 (* 1 = 1.12274 loss)
I0526 10:31:51.517364 22046 sgd_solver.cpp:106] Iteration 669000, lr = 0.0045
I0526 10:32:29.482543 22046 solver.cpp:237] Iteration 670500, loss = 1.06109
I0526 10:32:29.482728 22046 solver.cpp:253]     Train net output #0: loss = 1.06109 (* 1 = 1.06109 loss)
I0526 10:32:29.482746 22046 sgd_solver.cpp:106] Iteration 670500, lr = 0.0045
I0526 10:32:46.455760 22046 solver.cpp:237] Iteration 672000, loss = 1.19148
I0526 10:32:46.455798 22046 solver.cpp:253]     Train net output #0: loss = 1.19148 (* 1 = 1.19148 loss)
I0526 10:32:46.455816 22046 sgd_solver.cpp:106] Iteration 672000, lr = 0.0045
I0526 10:33:03.378428 22046 solver.cpp:237] Iteration 673500, loss = 1.82044
I0526 10:33:03.378602 22046 solver.cpp:253]     Train net output #0: loss = 1.82044 (* 1 = 1.82044 loss)
I0526 10:33:03.378619 22046 sgd_solver.cpp:106] Iteration 673500, lr = 0.0045
I0526 10:33:20.229583 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_675000.caffemodel
I0526 10:33:20.278008 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_675000.solverstate
I0526 10:33:20.309175 22046 solver.cpp:237] Iteration 675000, loss = 1.57992
I0526 10:33:20.309234 22046 solver.cpp:253]     Train net output #0: loss = 1.57992 (* 1 = 1.57992 loss)
I0526 10:33:20.309259 22046 sgd_solver.cpp:106] Iteration 675000, lr = 0.0045
I0526 10:33:37.168418 22046 solver.cpp:237] Iteration 676500, loss = 1.07986
I0526 10:33:37.168579 22046 solver.cpp:253]     Train net output #0: loss = 1.07986 (* 1 = 1.07986 loss)
I0526 10:33:37.168596 22046 sgd_solver.cpp:106] Iteration 676500, lr = 0.0045
I0526 10:33:54.205366 22046 solver.cpp:237] Iteration 678000, loss = 1.29039
I0526 10:33:54.205422 22046 solver.cpp:253]     Train net output #0: loss = 1.29039 (* 1 = 1.29039 loss)
I0526 10:33:54.205441 22046 sgd_solver.cpp:106] Iteration 678000, lr = 0.0045
I0526 10:34:11.347842 22046 solver.cpp:237] Iteration 679500, loss = 1.22694
I0526 10:34:11.348016 22046 solver.cpp:253]     Train net output #0: loss = 1.22694 (* 1 = 1.22694 loss)
I0526 10:34:11.348034 22046 sgd_solver.cpp:106] Iteration 679500, lr = 0.0045
I0526 10:34:49.177112 22046 solver.cpp:237] Iteration 681000, loss = 1.22412
I0526 10:34:49.177291 22046 solver.cpp:253]     Train net output #0: loss = 1.22412 (* 1 = 1.22412 loss)
I0526 10:34:49.177309 22046 sgd_solver.cpp:106] Iteration 681000, lr = 0.0045
I0526 10:35:06.200557 22046 solver.cpp:237] Iteration 682500, loss = 1.35128
I0526 10:35:06.200611 22046 solver.cpp:253]     Train net output #0: loss = 1.35128 (* 1 = 1.35128 loss)
I0526 10:35:06.200631 22046 sgd_solver.cpp:106] Iteration 682500, lr = 0.0045
I0526 10:35:23.234925 22046 solver.cpp:237] Iteration 684000, loss = 1.30452
I0526 10:35:23.235112 22046 solver.cpp:253]     Train net output #0: loss = 1.30452 (* 1 = 1.30452 loss)
I0526 10:35:23.235129 22046 sgd_solver.cpp:106] Iteration 684000, lr = 0.0045
I0526 10:35:39.844310 22046 solver.cpp:237] Iteration 685500, loss = 1.52272
I0526 10:35:39.844348 22046 solver.cpp:253]     Train net output #0: loss = 1.52272 (* 1 = 1.52272 loss)
I0526 10:35:39.844365 22046 sgd_solver.cpp:106] Iteration 685500, lr = 0.0045
I0526 10:35:56.789666 22046 solver.cpp:237] Iteration 687000, loss = 0.875919
I0526 10:35:56.789844 22046 solver.cpp:253]     Train net output #0: loss = 0.875916 (* 1 = 0.875916 loss)
I0526 10:35:56.789861 22046 sgd_solver.cpp:106] Iteration 687000, lr = 0.0045
I0526 10:36:13.960093 22046 solver.cpp:237] Iteration 688500, loss = 1.02282
I0526 10:36:13.960150 22046 solver.cpp:253]     Train net output #0: loss = 1.02282 (* 1 = 1.02282 loss)
I0526 10:36:13.960168 22046 sgd_solver.cpp:106] Iteration 688500, lr = 0.0045
I0526 10:36:30.748658 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_690000.caffemodel
I0526 10:36:30.795585 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_690000.solverstate
I0526 10:36:30.821682 22046 solver.cpp:341] Iteration 690000, Testing net (#0)
I0526 10:37:50.984170 22046 solver.cpp:409]     Test net output #0: accuracy = 0.884161
I0526 10:37:50.984344 22046 solver.cpp:409]     Test net output #1: loss = 0.386605 (* 1 = 0.386605 loss)
I0526 10:38:11.816931 22046 solver.cpp:237] Iteration 690000, loss = 1.17301
I0526 10:38:11.816990 22046 solver.cpp:253]     Train net output #0: loss = 1.17301 (* 1 = 1.17301 loss)
I0526 10:38:11.817016 22046 sgd_solver.cpp:106] Iteration 690000, lr = 0.0045
I0526 10:38:28.723687 22046 solver.cpp:237] Iteration 691500, loss = 0.486061
I0526 10:38:28.723870 22046 solver.cpp:253]     Train net output #0: loss = 0.486059 (* 1 = 0.486059 loss)
I0526 10:38:28.723887 22046 sgd_solver.cpp:106] Iteration 691500, lr = 0.0045
I0526 10:38:45.550160 22046 solver.cpp:237] Iteration 693000, loss = 1.38857
I0526 10:38:45.550215 22046 solver.cpp:253]     Train net output #0: loss = 1.38856 (* 1 = 1.38856 loss)
I0526 10:38:45.550243 22046 sgd_solver.cpp:106] Iteration 693000, lr = 0.0045
I0526 10:39:02.507908 22046 solver.cpp:237] Iteration 694500, loss = 1.19832
I0526 10:39:02.508069 22046 solver.cpp:253]     Train net output #0: loss = 1.19831 (* 1 = 1.19831 loss)
I0526 10:39:02.508085 22046 sgd_solver.cpp:106] Iteration 694500, lr = 0.0045
I0526 10:39:19.250586 22046 solver.cpp:237] Iteration 696000, loss = 1.38312
I0526 10:39:19.250641 22046 solver.cpp:253]     Train net output #0: loss = 1.38312 (* 1 = 1.38312 loss)
I0526 10:39:19.250659 22046 sgd_solver.cpp:106] Iteration 696000, lr = 0.0045
I0526 10:39:35.942997 22046 solver.cpp:237] Iteration 697500, loss = 0.977019
I0526 10:39:35.943176 22046 solver.cpp:253]     Train net output #0: loss = 0.977017 (* 1 = 0.977017 loss)
I0526 10:39:35.943193 22046 sgd_solver.cpp:106] Iteration 697500, lr = 0.0045
I0526 10:39:53.004904 22046 solver.cpp:237] Iteration 699000, loss = 1.3159
I0526 10:39:53.004942 22046 solver.cpp:253]     Train net output #0: loss = 1.3159 (* 1 = 1.3159 loss)
I0526 10:39:53.004959 22046 sgd_solver.cpp:106] Iteration 699000, lr = 0.0045
I0526 10:40:30.608119 22046 solver.cpp:237] Iteration 700500, loss = 1.47398
I0526 10:40:30.608302 22046 solver.cpp:253]     Train net output #0: loss = 1.47398 (* 1 = 1.47398 loss)
I0526 10:40:30.608320 22046 sgd_solver.cpp:106] Iteration 700500, lr = 0.0045
I0526 10:40:47.260783 22046 solver.cpp:237] Iteration 702000, loss = 1.07919
I0526 10:40:47.260823 22046 solver.cpp:253]     Train net output #0: loss = 1.07918 (* 1 = 1.07918 loss)
I0526 10:40:47.260846 22046 sgd_solver.cpp:106] Iteration 702000, lr = 0.0045
I0526 10:41:03.886452 22046 solver.cpp:237] Iteration 703500, loss = 1.34074
I0526 10:41:03.886636 22046 solver.cpp:253]     Train net output #0: loss = 1.34074 (* 1 = 1.34074 loss)
I0526 10:41:03.886653 22046 sgd_solver.cpp:106] Iteration 703500, lr = 0.0045
I0526 10:41:20.782358 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_705000.caffemodel
I0526 10:41:20.828929 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_705000.solverstate
I0526 10:41:20.858006 22046 solver.cpp:237] Iteration 705000, loss = 2.25827
I0526 10:41:20.858063 22046 solver.cpp:253]     Train net output #0: loss = 2.25827 (* 1 = 2.25827 loss)
I0526 10:41:20.858088 22046 sgd_solver.cpp:106] Iteration 705000, lr = 0.0045
I0526 10:41:38.053037 22046 solver.cpp:237] Iteration 706500, loss = 0.669589
I0526 10:41:38.053215 22046 solver.cpp:253]     Train net output #0: loss = 0.669586 (* 1 = 0.669586 loss)
I0526 10:41:38.053233 22046 sgd_solver.cpp:106] Iteration 706500, lr = 0.0045
I0526 10:41:54.710762 22046 solver.cpp:237] Iteration 708000, loss = 1.15729
I0526 10:41:54.710804 22046 solver.cpp:253]     Train net output #0: loss = 1.15729 (* 1 = 1.15729 loss)
I0526 10:41:54.710822 22046 sgd_solver.cpp:106] Iteration 708000, lr = 0.0045
I0526 10:42:11.622922 22046 solver.cpp:237] Iteration 709500, loss = 1.02573
I0526 10:42:11.623098 22046 solver.cpp:253]     Train net output #0: loss = 1.02573 (* 1 = 1.02573 loss)
I0526 10:42:11.623116 22046 sgd_solver.cpp:106] Iteration 709500, lr = 0.0045
I0526 10:42:49.680807 22046 solver.cpp:237] Iteration 711000, loss = 1.13677
I0526 10:42:49.680986 22046 solver.cpp:253]     Train net output #0: loss = 1.13677 (* 1 = 1.13677 loss)
I0526 10:42:49.681005 22046 sgd_solver.cpp:106] Iteration 711000, lr = 0.0045
I0526 10:43:06.702687 22046 solver.cpp:237] Iteration 712500, loss = 1.88303
I0526 10:43:06.702744 22046 solver.cpp:253]     Train net output #0: loss = 1.88303 (* 1 = 1.88303 loss)
I0526 10:43:06.702760 22046 sgd_solver.cpp:106] Iteration 712500, lr = 0.0045
I0526 10:43:23.553632 22046 solver.cpp:237] Iteration 714000, loss = 1.25814
I0526 10:43:23.553812 22046 solver.cpp:253]     Train net output #0: loss = 1.25813 (* 1 = 1.25813 loss)
I0526 10:43:23.553829 22046 sgd_solver.cpp:106] Iteration 714000, lr = 0.0045
I0526 10:43:40.166174 22046 solver.cpp:237] Iteration 715500, loss = 1.67302
I0526 10:43:40.166213 22046 solver.cpp:253]     Train net output #0: loss = 1.67301 (* 1 = 1.67301 loss)
I0526 10:43:40.166230 22046 sgd_solver.cpp:106] Iteration 715500, lr = 0.0045
I0526 10:43:57.130656 22046 solver.cpp:237] Iteration 717000, loss = 0.834391
I0526 10:43:57.130828 22046 solver.cpp:253]     Train net output #0: loss = 0.834387 (* 1 = 0.834387 loss)
I0526 10:43:57.130846 22046 sgd_solver.cpp:106] Iteration 717000, lr = 0.0045
I0526 10:44:14.143187 22046 solver.cpp:237] Iteration 718500, loss = 1.15417
I0526 10:44:14.143241 22046 solver.cpp:253]     Train net output #0: loss = 1.15417 (* 1 = 1.15417 loss)
I0526 10:44:14.143266 22046 sgd_solver.cpp:106] Iteration 718500, lr = 0.0045
I0526 10:44:31.094509 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_720000.caffemodel
I0526 10:44:31.141577 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_720000.solverstate
I0526 10:44:31.167502 22046 solver.cpp:341] Iteration 720000, Testing net (#0)
I0526 10:45:30.621671 22046 solver.cpp:409]     Test net output #0: accuracy = 0.886895
I0526 10:45:30.621863 22046 solver.cpp:409]     Test net output #1: loss = 0.356943 (* 1 = 0.356943 loss)
I0526 10:45:51.474781 22046 solver.cpp:237] Iteration 720000, loss = 1.24162
I0526 10:45:51.474843 22046 solver.cpp:253]     Train net output #0: loss = 1.24162 (* 1 = 1.24162 loss)
I0526 10:45:51.474869 22046 sgd_solver.cpp:106] Iteration 720000, lr = 0.0045
I0526 10:46:08.139173 22046 solver.cpp:237] Iteration 721500, loss = 1.4432
I0526 10:46:08.139349 22046 solver.cpp:253]     Train net output #0: loss = 1.4432 (* 1 = 1.4432 loss)
I0526 10:46:08.139366 22046 sgd_solver.cpp:106] Iteration 721500, lr = 0.0045
I0526 10:46:24.746876 22046 solver.cpp:237] Iteration 723000, loss = 0.84009
I0526 10:46:24.746934 22046 solver.cpp:253]     Train net output #0: loss = 0.840085 (* 1 = 0.840085 loss)
I0526 10:46:24.746953 22046 sgd_solver.cpp:106] Iteration 723000, lr = 0.0045
I0526 10:46:41.367205 22046 solver.cpp:237] Iteration 724500, loss = 0.748531
I0526 10:46:41.367386 22046 solver.cpp:253]     Train net output #0: loss = 0.748526 (* 1 = 0.748526 loss)
I0526 10:46:41.367405 22046 sgd_solver.cpp:106] Iteration 724500, lr = 0.0045
I0526 10:46:58.025748 22046 solver.cpp:237] Iteration 726000, loss = 0.989623
I0526 10:46:58.025787 22046 solver.cpp:253]     Train net output #0: loss = 0.989619 (* 1 = 0.989619 loss)
I0526 10:46:58.025804 22046 sgd_solver.cpp:106] Iteration 726000, lr = 0.0045
I0526 10:47:15.076167 22046 solver.cpp:237] Iteration 727500, loss = 1.32895
I0526 10:47:15.076344 22046 solver.cpp:253]     Train net output #0: loss = 1.32894 (* 1 = 1.32894 loss)
I0526 10:47:15.076362 22046 sgd_solver.cpp:106] Iteration 727500, lr = 0.0045
I0526 10:47:32.228931 22046 solver.cpp:237] Iteration 729000, loss = 1.50408
I0526 10:47:32.228991 22046 solver.cpp:253]     Train net output #0: loss = 1.50407 (* 1 = 1.50407 loss)
I0526 10:47:32.229010 22046 sgd_solver.cpp:106] Iteration 729000, lr = 0.0045
I0526 10:48:10.002806 22046 solver.cpp:237] Iteration 730500, loss = 0.776261
I0526 10:48:10.002990 22046 solver.cpp:253]     Train net output #0: loss = 0.776256 (* 1 = 0.776256 loss)
I0526 10:48:10.003008 22046 sgd_solver.cpp:106] Iteration 730500, lr = 0.0045
I0526 10:48:27.070197 22046 solver.cpp:237] Iteration 732000, loss = 0.818436
I0526 10:48:27.070252 22046 solver.cpp:253]     Train net output #0: loss = 0.818433 (* 1 = 0.818433 loss)
I0526 10:48:27.070271 22046 sgd_solver.cpp:106] Iteration 732000, lr = 0.0045
I0526 10:48:44.252251 22046 solver.cpp:237] Iteration 733500, loss = 0.924527
I0526 10:48:44.252432 22046 solver.cpp:253]     Train net output #0: loss = 0.924522 (* 1 = 0.924522 loss)
I0526 10:48:44.252450 22046 sgd_solver.cpp:106] Iteration 733500, lr = 0.0045
I0526 10:49:01.016129 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_735000.caffemodel
I0526 10:49:01.064721 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_735000.solverstate
I0526 10:49:01.095855 22046 solver.cpp:237] Iteration 735000, loss = 2.22044
I0526 10:49:01.095914 22046 solver.cpp:253]     Train net output #0: loss = 2.22044 (* 1 = 2.22044 loss)
I0526 10:49:01.095932 22046 sgd_solver.cpp:106] Iteration 735000, lr = 0.0045
I0526 10:49:17.901998 22046 solver.cpp:237] Iteration 736500, loss = 0.807494
I0526 10:49:17.902178 22046 solver.cpp:253]     Train net output #0: loss = 0.80749 (* 1 = 0.80749 loss)
I0526 10:49:17.902195 22046 sgd_solver.cpp:106] Iteration 736500, lr = 0.0045
I0526 10:49:34.790568 22046 solver.cpp:237] Iteration 738000, loss = 1.62018
I0526 10:49:34.790627 22046 solver.cpp:253]     Train net output #0: loss = 1.62018 (* 1 = 1.62018 loss)
I0526 10:49:34.790644 22046 sgd_solver.cpp:106] Iteration 738000, lr = 0.0045
I0526 10:49:51.552695 22046 solver.cpp:237] Iteration 739500, loss = 0.960074
I0526 10:49:51.552868 22046 solver.cpp:253]     Train net output #0: loss = 0.960071 (* 1 = 0.960071 loss)
I0526 10:49:51.552886 22046 sgd_solver.cpp:106] Iteration 739500, lr = 0.0045
I0526 10:50:29.132236 22046 solver.cpp:237] Iteration 741000, loss = 1.44231
I0526 10:50:29.132423 22046 solver.cpp:253]     Train net output #0: loss = 1.44231 (* 1 = 1.44231 loss)
I0526 10:50:29.132441 22046 sgd_solver.cpp:106] Iteration 741000, lr = 0.0045
I0526 10:50:45.757544 22046 solver.cpp:237] Iteration 742500, loss = 1.57738
I0526 10:50:45.757583 22046 solver.cpp:253]     Train net output #0: loss = 1.57738 (* 1 = 1.57738 loss)
I0526 10:50:45.757599 22046 sgd_solver.cpp:106] Iteration 742500, lr = 0.0045
I0526 10:51:02.729897 22046 solver.cpp:237] Iteration 744000, loss = 1.48949
I0526 10:51:02.730072 22046 solver.cpp:253]     Train net output #0: loss = 1.48949 (* 1 = 1.48949 loss)
I0526 10:51:02.730088 22046 sgd_solver.cpp:106] Iteration 744000, lr = 0.0045
I0526 10:51:19.691370 22046 solver.cpp:237] Iteration 745500, loss = 1.31731
I0526 10:51:19.691427 22046 solver.cpp:253]     Train net output #0: loss = 1.31731 (* 1 = 1.31731 loss)
I0526 10:51:19.691453 22046 sgd_solver.cpp:106] Iteration 745500, lr = 0.0045
I0526 10:51:36.454668 22046 solver.cpp:237] Iteration 747000, loss = 1.11028
I0526 10:51:36.454829 22046 solver.cpp:253]     Train net output #0: loss = 1.11027 (* 1 = 1.11027 loss)
I0526 10:51:36.454845 22046 sgd_solver.cpp:106] Iteration 747000, lr = 0.0045
I0526 10:51:53.450700 22046 solver.cpp:237] Iteration 748500, loss = 1.29284
I0526 10:51:53.450757 22046 solver.cpp:253]     Train net output #0: loss = 1.29284 (* 1 = 1.29284 loss)
I0526 10:51:53.450774 22046 sgd_solver.cpp:106] Iteration 748500, lr = 0.0045
I0526 10:52:10.572726 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_750000.caffemodel
I0526 10:52:10.621306 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_750000.solverstate
I0526 10:52:10.649536 22046 solver.cpp:341] Iteration 750000, Testing net (#0)
I0526 10:53:31.101222 22046 solver.cpp:409]     Test net output #0: accuracy = 0.88825
I0526 10:53:31.101404 22046 solver.cpp:409]     Test net output #1: loss = 0.36056 (* 1 = 0.36056 loss)
I0526 10:53:51.974087 22046 solver.cpp:237] Iteration 750000, loss = 0.940348
I0526 10:53:51.974148 22046 solver.cpp:253]     Train net output #0: loss = 0.940343 (* 1 = 0.940343 loss)
I0526 10:53:51.974174 22046 sgd_solver.cpp:106] Iteration 750000, lr = 0.0045
I0526 10:54:09.006191 22046 solver.cpp:237] Iteration 751500, loss = 0.749581
I0526 10:54:09.006371 22046 solver.cpp:253]     Train net output #0: loss = 0.749576 (* 1 = 0.749576 loss)
I0526 10:54:09.006388 22046 sgd_solver.cpp:106] Iteration 751500, lr = 0.0045
I0526 10:54:25.812881 22046 solver.cpp:237] Iteration 753000, loss = 1.63135
I0526 10:54:25.812919 22046 solver.cpp:253]     Train net output #0: loss = 1.63135 (* 1 = 1.63135 loss)
I0526 10:54:25.812937 22046 sgd_solver.cpp:106] Iteration 753000, lr = 0.0045
I0526 10:54:42.541328 22046 solver.cpp:237] Iteration 754500, loss = 0.850358
I0526 10:54:42.541504 22046 solver.cpp:253]     Train net output #0: loss = 0.850355 (* 1 = 0.850355 loss)
I0526 10:54:42.541522 22046 sgd_solver.cpp:106] Iteration 754500, lr = 0.0045
I0526 10:54:59.540109 22046 solver.cpp:237] Iteration 756000, loss = 1.10317
I0526 10:54:59.540166 22046 solver.cpp:253]     Train net output #0: loss = 1.10316 (* 1 = 1.10316 loss)
I0526 10:54:59.540194 22046 sgd_solver.cpp:106] Iteration 756000, lr = 0.0045
I0526 10:55:16.502192 22046 solver.cpp:237] Iteration 757500, loss = 0.33124
I0526 10:55:16.502352 22046 solver.cpp:253]     Train net output #0: loss = 0.331236 (* 1 = 0.331236 loss)
I0526 10:55:16.502369 22046 sgd_solver.cpp:106] Iteration 757500, lr = 0.0045
I0526 10:55:33.345952 22046 solver.cpp:237] Iteration 759000, loss = 1.51829
I0526 10:55:33.346007 22046 solver.cpp:253]     Train net output #0: loss = 1.51829 (* 1 = 1.51829 loss)
I0526 10:55:33.346024 22046 sgd_solver.cpp:106] Iteration 759000, lr = 0.0045
I0526 10:56:11.006696 22046 solver.cpp:237] Iteration 760500, loss = 0.77676
I0526 10:56:11.006892 22046 solver.cpp:253]     Train net output #0: loss = 0.776757 (* 1 = 0.776757 loss)
I0526 10:56:11.006911 22046 sgd_solver.cpp:106] Iteration 760500, lr = 0.0045
I0526 10:56:27.670653 22046 solver.cpp:237] Iteration 762000, loss = 1.21537
I0526 10:56:27.670708 22046 solver.cpp:253]     Train net output #0: loss = 1.21537 (* 1 = 1.21537 loss)
I0526 10:56:27.670727 22046 sgd_solver.cpp:106] Iteration 762000, lr = 0.0045
I0526 10:56:44.446979 22046 solver.cpp:237] Iteration 763500, loss = 1.36254
I0526 10:56:44.447161 22046 solver.cpp:253]     Train net output #0: loss = 1.36254 (* 1 = 1.36254 loss)
I0526 10:56:44.447180 22046 sgd_solver.cpp:106] Iteration 763500, lr = 0.0045
I0526 10:57:01.472101 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_765000.caffemodel
I0526 10:57:01.518626 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_765000.solverstate
I0526 10:57:01.547886 22046 solver.cpp:237] Iteration 765000, loss = 0.766303
I0526 10:57:01.547945 22046 solver.cpp:253]     Train net output #0: loss = 0.7663 (* 1 = 0.7663 loss)
I0526 10:57:01.547962 22046 sgd_solver.cpp:106] Iteration 765000, lr = 0.0045
I0526 10:57:18.600584 22046 solver.cpp:237] Iteration 766500, loss = 1.05618
I0526 10:57:18.600769 22046 solver.cpp:253]     Train net output #0: loss = 1.05617 (* 1 = 1.05617 loss)
I0526 10:57:18.600786 22046 sgd_solver.cpp:106] Iteration 766500, lr = 0.0045
I0526 10:57:35.539675 22046 solver.cpp:237] Iteration 768000, loss = 1.10266
I0526 10:57:35.539732 22046 solver.cpp:253]     Train net output #0: loss = 1.10266 (* 1 = 1.10266 loss)
I0526 10:57:35.539749 22046 sgd_solver.cpp:106] Iteration 768000, lr = 0.0045
I0526 10:57:52.425617 22046 solver.cpp:237] Iteration 769500, loss = 0.912123
I0526 10:57:52.425781 22046 solver.cpp:253]     Train net output #0: loss = 0.912121 (* 1 = 0.912121 loss)
I0526 10:57:52.425798 22046 sgd_solver.cpp:106] Iteration 769500, lr = 0.0045
I0526 10:58:30.445804 22046 solver.cpp:237] Iteration 771000, loss = 0.964679
I0526 10:58:30.445991 22046 solver.cpp:253]     Train net output #0: loss = 0.964676 (* 1 = 0.964676 loss)
I0526 10:58:30.446009 22046 sgd_solver.cpp:106] Iteration 771000, lr = 0.0045
I0526 10:58:47.582182 22046 solver.cpp:237] Iteration 772500, loss = 0.655488
I0526 10:58:47.582237 22046 solver.cpp:253]     Train net output #0: loss = 0.655484 (* 1 = 0.655484 loss)
I0526 10:58:47.582263 22046 sgd_solver.cpp:106] Iteration 772500, lr = 0.0045
I0526 10:59:04.534723 22046 solver.cpp:237] Iteration 774000, loss = 1.05364
I0526 10:59:04.534885 22046 solver.cpp:253]     Train net output #0: loss = 1.05364 (* 1 = 1.05364 loss)
I0526 10:59:04.534903 22046 sgd_solver.cpp:106] Iteration 774000, lr = 0.0045
I0526 10:59:21.488576 22046 solver.cpp:237] Iteration 775500, loss = 0.908319
I0526 10:59:21.488631 22046 solver.cpp:253]     Train net output #0: loss = 0.908315 (* 1 = 0.908315 loss)
I0526 10:59:21.488648 22046 sgd_solver.cpp:106] Iteration 775500, lr = 0.0045
I0526 10:59:38.434407 22046 solver.cpp:237] Iteration 777000, loss = 1.52235
I0526 10:59:38.434589 22046 solver.cpp:253]     Train net output #0: loss = 1.52235 (* 1 = 1.52235 loss)
I0526 10:59:38.434607 22046 sgd_solver.cpp:106] Iteration 777000, lr = 0.0045
I0526 10:59:55.205466 22046 solver.cpp:237] Iteration 778500, loss = 1.57891
I0526 10:59:55.205505 22046 solver.cpp:253]     Train net output #0: loss = 1.5789 (* 1 = 1.5789 loss)
I0526 10:59:55.205523 22046 sgd_solver.cpp:106] Iteration 778500, lr = 0.0045
I0526 11:00:11.842242 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_780000.caffemodel
I0526 11:00:11.888106 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_780000.solverstate
I0526 11:00:11.913691 22046 solver.cpp:341] Iteration 780000, Testing net (#0)
I0526 11:01:10.790009 22046 solver.cpp:409]     Test net output #0: accuracy = 0.88975
I0526 11:01:10.790196 22046 solver.cpp:409]     Test net output #1: loss = 0.397601 (* 1 = 0.397601 loss)
I0526 11:01:31.627869 22046 solver.cpp:237] Iteration 780000, loss = 0.976193
I0526 11:01:31.627933 22046 solver.cpp:253]     Train net output #0: loss = 0.976188 (* 1 = 0.976188 loss)
I0526 11:01:31.627954 22046 sgd_solver.cpp:106] Iteration 780000, lr = 0.0045
I0526 11:01:48.659531 22046 solver.cpp:237] Iteration 781500, loss = 1.78403
I0526 11:01:48.659720 22046 solver.cpp:253]     Train net output #0: loss = 1.78403 (* 1 = 1.78403 loss)
I0526 11:01:48.659737 22046 sgd_solver.cpp:106] Iteration 781500, lr = 0.0045
I0526 11:02:05.720499 22046 solver.cpp:237] Iteration 783000, loss = 0.917029
I0526 11:02:05.720556 22046 solver.cpp:253]     Train net output #0: loss = 0.917025 (* 1 = 0.917025 loss)
I0526 11:02:05.720583 22046 sgd_solver.cpp:106] Iteration 783000, lr = 0.0045
I0526 11:02:22.911532 22046 solver.cpp:237] Iteration 784500, loss = 2.0629
I0526 11:02:22.911702 22046 solver.cpp:253]     Train net output #0: loss = 2.06289 (* 1 = 2.06289 loss)
I0526 11:02:22.911720 22046 sgd_solver.cpp:106] Iteration 784500, lr = 0.0045
I0526 11:02:39.756788 22046 solver.cpp:237] Iteration 786000, loss = 1.06065
I0526 11:02:39.756842 22046 solver.cpp:253]     Train net output #0: loss = 1.06065 (* 1 = 1.06065 loss)
I0526 11:02:39.756860 22046 sgd_solver.cpp:106] Iteration 786000, lr = 0.0045
I0526 11:02:56.430512 22046 solver.cpp:237] Iteration 787500, loss = 1.21547
I0526 11:02:56.430693 22046 solver.cpp:253]     Train net output #0: loss = 1.21547 (* 1 = 1.21547 loss)
I0526 11:02:56.430712 22046 sgd_solver.cpp:106] Iteration 787500, lr = 0.0045
I0526 11:03:13.209869 22046 solver.cpp:237] Iteration 789000, loss = 0.479373
I0526 11:03:13.209908 22046 solver.cpp:253]     Train net output #0: loss = 0.479369 (* 1 = 0.479369 loss)
I0526 11:03:13.209925 22046 sgd_solver.cpp:106] Iteration 789000, lr = 0.0045
I0526 11:03:50.905798 22046 solver.cpp:237] Iteration 790500, loss = 1.97019
I0526 11:03:50.905984 22046 solver.cpp:253]     Train net output #0: loss = 1.97019 (* 1 = 1.97019 loss)
I0526 11:03:50.906002 22046 sgd_solver.cpp:106] Iteration 790500, lr = 0.0045
I0526 11:04:07.778326 22046 solver.cpp:237] Iteration 792000, loss = 0.797429
I0526 11:04:07.778365 22046 solver.cpp:253]     Train net output #0: loss = 0.797426 (* 1 = 0.797426 loss)
I0526 11:04:07.778383 22046 sgd_solver.cpp:106] Iteration 792000, lr = 0.0045
I0526 11:04:24.966720 22046 solver.cpp:237] Iteration 793500, loss = 1.04819
I0526 11:04:24.966894 22046 solver.cpp:253]     Train net output #0: loss = 1.04819 (* 1 = 1.04819 loss)
I0526 11:04:24.966912 22046 sgd_solver.cpp:106] Iteration 793500, lr = 0.0045
I0526 11:04:42.072706 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_795000.caffemodel
I0526 11:04:42.118736 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_795000.solverstate
I0526 11:04:42.148057 22046 solver.cpp:237] Iteration 795000, loss = 0.743046
I0526 11:04:42.148114 22046 solver.cpp:253]     Train net output #0: loss = 0.743042 (* 1 = 0.743042 loss)
I0526 11:04:42.148133 22046 sgd_solver.cpp:106] Iteration 795000, lr = 0.0045
I0526 11:04:59.185626 22046 solver.cpp:237] Iteration 796500, loss = 1.07516
I0526 11:04:59.185820 22046 solver.cpp:253]     Train net output #0: loss = 1.07516 (* 1 = 1.07516 loss)
I0526 11:04:59.185838 22046 sgd_solver.cpp:106] Iteration 796500, lr = 0.0045
I0526 11:05:15.974622 22046 solver.cpp:237] Iteration 798000, loss = 1.01299
I0526 11:05:15.974663 22046 solver.cpp:253]     Train net output #0: loss = 1.01298 (* 1 = 1.01298 loss)
I0526 11:05:15.974681 22046 sgd_solver.cpp:106] Iteration 798000, lr = 0.0045
I0526 11:05:32.687357 22046 solver.cpp:237] Iteration 799500, loss = 1.24262
I0526 11:05:32.687541 22046 solver.cpp:253]     Train net output #0: loss = 1.24262 (* 1 = 1.24262 loss)
I0526 11:05:32.687558 22046 sgd_solver.cpp:106] Iteration 799500, lr = 0.0045
I0526 11:06:10.219300 22046 solver.cpp:237] Iteration 801000, loss = 0.540389
I0526 11:06:10.219497 22046 solver.cpp:253]     Train net output #0: loss = 0.540386 (* 1 = 0.540386 loss)
I0526 11:06:10.219516 22046 sgd_solver.cpp:106] Iteration 801000, lr = 0.0045
I0526 11:06:27.326807 22046 solver.cpp:237] Iteration 802500, loss = 1.05612
I0526 11:06:27.326864 22046 solver.cpp:253]     Train net output #0: loss = 1.05612 (* 1 = 1.05612 loss)
I0526 11:06:27.326881 22046 sgd_solver.cpp:106] Iteration 802500, lr = 0.0045
I0526 11:06:44.406468 22046 solver.cpp:237] Iteration 804000, loss = 1.40213
I0526 11:06:44.406653 22046 solver.cpp:253]     Train net output #0: loss = 1.40212 (* 1 = 1.40212 loss)
I0526 11:06:44.406672 22046 sgd_solver.cpp:106] Iteration 804000, lr = 0.0045
I0526 11:07:01.209337 22046 solver.cpp:237] Iteration 805500, loss = 1.29899
I0526 11:07:01.209378 22046 solver.cpp:253]     Train net output #0: loss = 1.29899 (* 1 = 1.29899 loss)
I0526 11:07:01.209394 22046 sgd_solver.cpp:106] Iteration 805500, lr = 0.0045
I0526 11:07:17.860913 22046 solver.cpp:237] Iteration 807000, loss = 2.48428
I0526 11:07:17.861093 22046 solver.cpp:253]     Train net output #0: loss = 2.48428 (* 1 = 2.48428 loss)
I0526 11:07:17.861109 22046 sgd_solver.cpp:106] Iteration 807000, lr = 0.0045
I0526 11:07:34.609223 22046 solver.cpp:237] Iteration 808500, loss = 0.895109
I0526 11:07:34.609280 22046 solver.cpp:253]     Train net output #0: loss = 0.895105 (* 1 = 0.895105 loss)
I0526 11:07:34.609305 22046 sgd_solver.cpp:106] Iteration 808500, lr = 0.0045
I0526 11:07:51.549190 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_810000.caffemodel
I0526 11:07:51.595161 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_810000.solverstate
I0526 11:07:51.620827 22046 solver.cpp:341] Iteration 810000, Testing net (#0)
I0526 11:09:11.942140 22046 solver.cpp:409]     Test net output #0: accuracy = 0.888674
I0526 11:09:11.942333 22046 solver.cpp:409]     Test net output #1: loss = 0.380226 (* 1 = 0.380226 loss)
I0526 11:09:32.772274 22046 solver.cpp:237] Iteration 810000, loss = 1.51319
I0526 11:09:32.772339 22046 solver.cpp:253]     Train net output #0: loss = 1.51319 (* 1 = 1.51319 loss)
I0526 11:09:32.772367 22046 sgd_solver.cpp:106] Iteration 810000, lr = 0.0045
I0526 11:09:49.559962 22046 solver.cpp:237] Iteration 811500, loss = 0.482871
I0526 11:09:49.560148 22046 solver.cpp:253]     Train net output #0: loss = 0.482866 (* 1 = 0.482866 loss)
I0526 11:09:49.560164 22046 sgd_solver.cpp:106] Iteration 811500, lr = 0.0045
I0526 11:10:06.291888 22046 solver.cpp:237] Iteration 813000, loss = 0.904558
I0526 11:10:06.291947 22046 solver.cpp:253]     Train net output #0: loss = 0.904554 (* 1 = 0.904554 loss)
I0526 11:10:06.291965 22046 sgd_solver.cpp:106] Iteration 813000, lr = 0.0045
I0526 11:10:22.892196 22046 solver.cpp:237] Iteration 814500, loss = 1.54341
I0526 11:10:22.892361 22046 solver.cpp:253]     Train net output #0: loss = 1.5434 (* 1 = 1.5434 loss)
I0526 11:10:22.892377 22046 sgd_solver.cpp:106] Iteration 814500, lr = 0.0045
I0526 11:10:39.763600 22046 solver.cpp:237] Iteration 816000, loss = 1.00534
I0526 11:10:39.763650 22046 solver.cpp:253]     Train net output #0: loss = 1.00534 (* 1 = 1.00534 loss)
I0526 11:10:39.763669 22046 sgd_solver.cpp:106] Iteration 816000, lr = 0.0045
I0526 11:10:56.376649 22046 solver.cpp:237] Iteration 817500, loss = 1.08658
I0526 11:10:56.376844 22046 solver.cpp:253]     Train net output #0: loss = 1.08657 (* 1 = 1.08657 loss)
I0526 11:10:56.376863 22046 sgd_solver.cpp:106] Iteration 817500, lr = 0.0045
I0526 11:11:12.992971 22046 solver.cpp:237] Iteration 819000, loss = 1.02686
I0526 11:11:12.993028 22046 solver.cpp:253]     Train net output #0: loss = 1.02686 (* 1 = 1.02686 loss)
I0526 11:11:12.993057 22046 sgd_solver.cpp:106] Iteration 819000, lr = 0.0045
I0526 11:11:50.745045 22046 solver.cpp:237] Iteration 820500, loss = 1.2642
I0526 11:11:50.745239 22046 solver.cpp:253]     Train net output #0: loss = 1.26419 (* 1 = 1.26419 loss)
I0526 11:11:50.745255 22046 sgd_solver.cpp:106] Iteration 820500, lr = 0.0045
I0526 11:12:07.536079 22046 solver.cpp:237] Iteration 822000, loss = 1.20118
I0526 11:12:07.536134 22046 solver.cpp:253]     Train net output #0: loss = 1.20117 (* 1 = 1.20117 loss)
I0526 11:12:07.536160 22046 sgd_solver.cpp:106] Iteration 822000, lr = 0.0045
I0526 11:12:24.184669 22046 solver.cpp:237] Iteration 823500, loss = 0.978921
I0526 11:12:24.184835 22046 solver.cpp:253]     Train net output #0: loss = 0.978915 (* 1 = 0.978915 loss)
I0526 11:12:24.184852 22046 sgd_solver.cpp:106] Iteration 823500, lr = 0.0045
I0526 11:12:41.278781 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_825000.caffemodel
I0526 11:12:41.326853 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_825000.solverstate
I0526 11:12:41.358223 22046 solver.cpp:237] Iteration 825000, loss = 1.09666
I0526 11:12:41.358289 22046 solver.cpp:253]     Train net output #0: loss = 1.09665 (* 1 = 1.09665 loss)
I0526 11:12:41.358306 22046 sgd_solver.cpp:106] Iteration 825000, lr = 0.0045
I0526 11:12:58.458955 22046 solver.cpp:237] Iteration 826500, loss = 1.69111
I0526 11:12:58.459143 22046 solver.cpp:253]     Train net output #0: loss = 1.6911 (* 1 = 1.6911 loss)
I0526 11:12:58.459161 22046 sgd_solver.cpp:106] Iteration 826500, lr = 0.0045
I0526 11:13:15.346653 22046 solver.cpp:237] Iteration 828000, loss = 1.01794
I0526 11:13:15.346693 22046 solver.cpp:253]     Train net output #0: loss = 1.01793 (* 1 = 1.01793 loss)
I0526 11:13:15.346711 22046 sgd_solver.cpp:106] Iteration 828000, lr = 0.0045
I0526 11:13:32.279773 22046 solver.cpp:237] Iteration 829500, loss = 1.24163
I0526 11:13:32.279958 22046 solver.cpp:253]     Train net output #0: loss = 1.24162 (* 1 = 1.24162 loss)
I0526 11:13:32.279974 22046 sgd_solver.cpp:106] Iteration 829500, lr = 0.0045
I0526 11:14:10.050428 22046 solver.cpp:237] Iteration 831000, loss = 1.07747
I0526 11:14:10.050616 22046 solver.cpp:253]     Train net output #0: loss = 1.07746 (* 1 = 1.07746 loss)
I0526 11:14:10.050633 22046 sgd_solver.cpp:106] Iteration 831000, lr = 0.0045
I0526 11:14:26.672277 22046 solver.cpp:237] Iteration 832500, loss = 0.845944
I0526 11:14:26.672317 22046 solver.cpp:253]     Train net output #0: loss = 0.845935 (* 1 = 0.845935 loss)
I0526 11:14:26.672333 22046 sgd_solver.cpp:106] Iteration 832500, lr = 0.0045
I0526 11:14:43.532922 22046 solver.cpp:237] Iteration 834000, loss = 1.33429
I0526 11:14:43.533112 22046 solver.cpp:253]     Train net output #0: loss = 1.33428 (* 1 = 1.33428 loss)
I0526 11:14:43.533128 22046 sgd_solver.cpp:106] Iteration 834000, lr = 0.0045
I0526 11:15:00.569010 22046 solver.cpp:237] Iteration 835500, loss = 1.88439
I0526 11:15:00.569061 22046 solver.cpp:253]     Train net output #0: loss = 1.88438 (* 1 = 1.88438 loss)
I0526 11:15:00.569082 22046 sgd_solver.cpp:106] Iteration 835500, lr = 0.0045
I0526 11:15:17.549535 22046 solver.cpp:237] Iteration 837000, loss = 1.48806
I0526 11:15:17.549722 22046 solver.cpp:253]     Train net output #0: loss = 1.48805 (* 1 = 1.48805 loss)
I0526 11:15:17.549739 22046 sgd_solver.cpp:106] Iteration 837000, lr = 0.0045
I0526 11:15:34.572300 22046 solver.cpp:237] Iteration 838500, loss = 1.17952
I0526 11:15:34.572350 22046 solver.cpp:253]     Train net output #0: loss = 1.17951 (* 1 = 1.17951 loss)
I0526 11:15:34.572366 22046 sgd_solver.cpp:106] Iteration 838500, lr = 0.0045
I0526 11:15:51.635928 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_840000.caffemodel
I0526 11:15:51.682730 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_840000.solverstate
I0526 11:15:51.708627 22046 solver.cpp:341] Iteration 840000, Testing net (#0)
I0526 11:16:51.052495 22046 solver.cpp:409]     Test net output #0: accuracy = 0.887806
I0526 11:16:51.052691 22046 solver.cpp:409]     Test net output #1: loss = 0.363242 (* 1 = 0.363242 loss)
I0526 11:17:11.876950 22046 solver.cpp:237] Iteration 840000, loss = 1.57667
I0526 11:17:11.877012 22046 solver.cpp:253]     Train net output #0: loss = 1.57666 (* 1 = 1.57666 loss)
I0526 11:17:11.877038 22046 sgd_solver.cpp:106] Iteration 840000, lr = 0.0045
I0526 11:17:28.659871 22046 solver.cpp:237] Iteration 841500, loss = 1.10224
I0526 11:17:28.660058 22046 solver.cpp:253]     Train net output #0: loss = 1.10224 (* 1 = 1.10224 loss)
I0526 11:17:28.660076 22046 sgd_solver.cpp:106] Iteration 841500, lr = 0.0045
I0526 11:17:45.734606 22046 solver.cpp:237] Iteration 843000, loss = 2.15759
I0526 11:17:45.734644 22046 solver.cpp:253]     Train net output #0: loss = 2.15758 (* 1 = 2.15758 loss)
I0526 11:17:45.734661 22046 sgd_solver.cpp:106] Iteration 843000, lr = 0.0045
I0526 11:18:02.702648 22046 solver.cpp:237] Iteration 844500, loss = 1.20449
I0526 11:18:02.702833 22046 solver.cpp:253]     Train net output #0: loss = 1.20449 (* 1 = 1.20449 loss)
I0526 11:18:02.702850 22046 sgd_solver.cpp:106] Iteration 844500, lr = 0.0045
I0526 11:18:19.650714 22046 solver.cpp:237] Iteration 846000, loss = 1.56327
I0526 11:18:19.650774 22046 solver.cpp:253]     Train net output #0: loss = 1.56327 (* 1 = 1.56327 loss)
I0526 11:18:19.650799 22046 sgd_solver.cpp:106] Iteration 846000, lr = 0.0045
I0526 11:18:36.437201 22046 solver.cpp:237] Iteration 847500, loss = 1.28258
I0526 11:18:36.437371 22046 solver.cpp:253]     Train net output #0: loss = 1.28258 (* 1 = 1.28258 loss)
I0526 11:18:36.437388 22046 sgd_solver.cpp:106] Iteration 847500, lr = 0.0045
I0526 11:18:53.504690 22046 solver.cpp:237] Iteration 849000, loss = 0.636599
I0526 11:18:53.504747 22046 solver.cpp:253]     Train net output #0: loss = 0.636591 (* 1 = 0.636591 loss)
I0526 11:18:53.504765 22046 sgd_solver.cpp:106] Iteration 849000, lr = 0.0045
I0526 11:19:31.544040 22046 solver.cpp:237] Iteration 850500, loss = 1.33795
I0526 11:19:31.544232 22046 solver.cpp:253]     Train net output #0: loss = 1.33794 (* 1 = 1.33794 loss)
I0526 11:19:31.544250 22046 sgd_solver.cpp:106] Iteration 850500, lr = 0.0045
I0526 11:19:48.206670 22046 solver.cpp:237] Iteration 852000, loss = 1.00938
I0526 11:19:48.206727 22046 solver.cpp:253]     Train net output #0: loss = 1.00937 (* 1 = 1.00937 loss)
I0526 11:19:48.206744 22046 sgd_solver.cpp:106] Iteration 852000, lr = 0.0045
I0526 11:20:04.999315 22046 solver.cpp:237] Iteration 853500, loss = 0.899515
I0526 11:20:04.999501 22046 solver.cpp:253]     Train net output #0: loss = 0.899508 (* 1 = 0.899508 loss)
I0526 11:20:04.999519 22046 sgd_solver.cpp:106] Iteration 853500, lr = 0.0045
I0526 11:20:21.957267 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_855000.caffemodel
I0526 11:20:22.002770 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_855000.solverstate
I0526 11:20:22.031711 22046 solver.cpp:237] Iteration 855000, loss = 1.77047
I0526 11:20:22.031767 22046 solver.cpp:253]     Train net output #0: loss = 1.77047 (* 1 = 1.77047 loss)
I0526 11:20:22.031793 22046 sgd_solver.cpp:106] Iteration 855000, lr = 0.0045
I0526 11:20:38.951603 22046 solver.cpp:237] Iteration 856500, loss = 0.9533
I0526 11:20:38.951808 22046 solver.cpp:253]     Train net output #0: loss = 0.953293 (* 1 = 0.953293 loss)
I0526 11:20:38.951827 22046 sgd_solver.cpp:106] Iteration 856500, lr = 0.0045
I0526 11:20:55.850291 22046 solver.cpp:237] Iteration 858000, loss = 0.902804
I0526 11:20:55.850347 22046 solver.cpp:253]     Train net output #0: loss = 0.902798 (* 1 = 0.902798 loss)
I0526 11:20:55.850368 22046 sgd_solver.cpp:106] Iteration 858000, lr = 0.0045
I0526 11:21:12.816344 22046 solver.cpp:237] Iteration 859500, loss = 0.66944
I0526 11:21:12.816517 22046 solver.cpp:253]     Train net output #0: loss = 0.669434 (* 1 = 0.669434 loss)
I0526 11:21:12.816534 22046 sgd_solver.cpp:106] Iteration 859500, lr = 0.0045
I0526 11:21:50.867609 22046 solver.cpp:237] Iteration 861000, loss = 0.799125
I0526 11:21:50.867802 22046 solver.cpp:253]     Train net output #0: loss = 0.799119 (* 1 = 0.799119 loss)
I0526 11:21:50.867820 22046 sgd_solver.cpp:106] Iteration 861000, lr = 0.0045
I0526 11:22:07.987547 22046 solver.cpp:237] Iteration 862500, loss = 1.79409
I0526 11:22:07.987604 22046 solver.cpp:253]     Train net output #0: loss = 1.79408 (* 1 = 1.79408 loss)
I0526 11:22:07.987632 22046 sgd_solver.cpp:106] Iteration 862500, lr = 0.0045
I0526 11:22:24.832422 22046 solver.cpp:237] Iteration 864000, loss = 1.59398
I0526 11:22:24.832592 22046 solver.cpp:253]     Train net output #0: loss = 1.59397 (* 1 = 1.59397 loss)
I0526 11:22:24.832609 22046 sgd_solver.cpp:106] Iteration 864000, lr = 0.0045
I0526 11:22:41.599617 22046 solver.cpp:237] Iteration 865500, loss = 1.60941
I0526 11:22:41.599676 22046 solver.cpp:253]     Train net output #0: loss = 1.6094 (* 1 = 1.6094 loss)
I0526 11:22:41.599694 22046 sgd_solver.cpp:106] Iteration 865500, lr = 0.0045
I0526 11:22:58.407815 22046 solver.cpp:237] Iteration 867000, loss = 1.30557
I0526 11:22:58.408002 22046 solver.cpp:253]     Train net output #0: loss = 1.30557 (* 1 = 1.30557 loss)
I0526 11:22:58.408020 22046 sgd_solver.cpp:106] Iteration 867000, lr = 0.0045
I0526 11:23:15.185947 22046 solver.cpp:237] Iteration 868500, loss = 0.936697
I0526 11:23:15.185989 22046 solver.cpp:253]     Train net output #0: loss = 0.936694 (* 1 = 0.936694 loss)
I0526 11:23:15.186007 22046 sgd_solver.cpp:106] Iteration 868500, lr = 0.0045
I0526 11:23:31.854593 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_870000.caffemodel
I0526 11:23:31.900449 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_870000.solverstate
I0526 11:23:31.926101 22046 solver.cpp:341] Iteration 870000, Testing net (#0)
I0526 11:24:52.273370 22046 solver.cpp:409]     Test net output #0: accuracy = 0.888223
I0526 11:24:52.273561 22046 solver.cpp:409]     Test net output #1: loss = 0.372299 (* 1 = 0.372299 loss)
I0526 11:25:13.090714 22046 solver.cpp:237] Iteration 870000, loss = 1.98768
I0526 11:25:13.090776 22046 solver.cpp:253]     Train net output #0: loss = 1.98768 (* 1 = 1.98768 loss)
I0526 11:25:13.090797 22046 sgd_solver.cpp:106] Iteration 870000, lr = 0.0045
I0526 11:25:29.922695 22046 solver.cpp:237] Iteration 871500, loss = 1.48404
I0526 11:25:29.922884 22046 solver.cpp:253]     Train net output #0: loss = 1.48404 (* 1 = 1.48404 loss)
I0526 11:25:29.922902 22046 sgd_solver.cpp:106] Iteration 871500, lr = 0.0045
I0526 11:25:46.706265 22046 solver.cpp:237] Iteration 873000, loss = 0.846325
I0526 11:25:46.706305 22046 solver.cpp:253]     Train net output #0: loss = 0.846323 (* 1 = 0.846323 loss)
I0526 11:25:46.706322 22046 sgd_solver.cpp:106] Iteration 873000, lr = 0.0045
I0526 11:26:03.351358 22046 solver.cpp:237] Iteration 874500, loss = 1.33784
I0526 11:26:03.351557 22046 solver.cpp:253]     Train net output #0: loss = 1.33784 (* 1 = 1.33784 loss)
I0526 11:26:03.351574 22046 sgd_solver.cpp:106] Iteration 874500, lr = 0.0045
I0526 11:26:19.975664 22046 solver.cpp:237] Iteration 876000, loss = 1.2238
I0526 11:26:19.975726 22046 solver.cpp:253]     Train net output #0: loss = 1.2238 (* 1 = 1.2238 loss)
I0526 11:26:19.975744 22046 sgd_solver.cpp:106] Iteration 876000, lr = 0.0045
I0526 11:26:36.613404 22046 solver.cpp:237] Iteration 877500, loss = 1.6263
I0526 11:26:36.613572 22046 solver.cpp:253]     Train net output #0: loss = 1.6263 (* 1 = 1.6263 loss)
I0526 11:26:36.613590 22046 sgd_solver.cpp:106] Iteration 877500, lr = 0.0045
I0526 11:26:53.373431 22046 solver.cpp:237] Iteration 879000, loss = 1.39713
I0526 11:26:53.373487 22046 solver.cpp:253]     Train net output #0: loss = 1.39713 (* 1 = 1.39713 loss)
I0526 11:26:53.373505 22046 sgd_solver.cpp:106] Iteration 879000, lr = 0.0045
I0526 11:27:31.152629 22046 solver.cpp:237] Iteration 880500, loss = 0.761993
I0526 11:27:31.152825 22046 solver.cpp:253]     Train net output #0: loss = 0.761991 (* 1 = 0.761991 loss)
I0526 11:27:31.152843 22046 sgd_solver.cpp:106] Iteration 880500, lr = 0.0045
I0526 11:27:48.365867 22046 solver.cpp:237] Iteration 882000, loss = 0.569247
I0526 11:27:48.365907 22046 solver.cpp:253]     Train net output #0: loss = 0.569243 (* 1 = 0.569243 loss)
I0526 11:27:48.365924 22046 sgd_solver.cpp:106] Iteration 882000, lr = 0.0045
I0526 11:28:05.488600 22046 solver.cpp:237] Iteration 883500, loss = 1.4248
I0526 11:28:05.488787 22046 solver.cpp:253]     Train net output #0: loss = 1.42479 (* 1 = 1.42479 loss)
I0526 11:28:05.488804 22046 sgd_solver.cpp:106] Iteration 883500, lr = 0.0045
I0526 11:28:22.462440 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_885000.caffemodel
I0526 11:28:22.511438 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_885000.solverstate
I0526 11:28:22.543066 22046 solver.cpp:237] Iteration 885000, loss = 2.13563
I0526 11:28:22.543125 22046 solver.cpp:253]     Train net output #0: loss = 2.13563 (* 1 = 2.13563 loss)
I0526 11:28:22.543143 22046 sgd_solver.cpp:106] Iteration 885000, lr = 0.0045
I0526 11:28:39.464867 22046 solver.cpp:237] Iteration 886500, loss = 1.12991
I0526 11:28:39.465042 22046 solver.cpp:253]     Train net output #0: loss = 1.12991 (* 1 = 1.12991 loss)
I0526 11:28:39.465060 22046 sgd_solver.cpp:106] Iteration 886500, lr = 0.0045
I0526 11:28:56.582000 22046 solver.cpp:237] Iteration 888000, loss = 1.8122
I0526 11:28:56.582056 22046 solver.cpp:253]     Train net output #0: loss = 1.81219 (* 1 = 1.81219 loss)
I0526 11:28:56.582075 22046 sgd_solver.cpp:106] Iteration 888000, lr = 0.0045
I0526 11:29:13.711999 22046 solver.cpp:237] Iteration 889500, loss = 1.66813
I0526 11:29:13.712188 22046 solver.cpp:253]     Train net output #0: loss = 1.66813 (* 1 = 1.66813 loss)
I0526 11:29:13.712206 22046 sgd_solver.cpp:106] Iteration 889500, lr = 0.0045
I0526 11:29:51.644011 22046 solver.cpp:237] Iteration 891000, loss = 0.448788
I0526 11:29:51.644207 22046 solver.cpp:253]     Train net output #0: loss = 0.448785 (* 1 = 0.448785 loss)
I0526 11:29:51.644223 22046 sgd_solver.cpp:106] Iteration 891000, lr = 0.0045
I0526 11:30:08.609143 22046 solver.cpp:237] Iteration 892500, loss = 1.15961
I0526 11:30:08.609201 22046 solver.cpp:253]     Train net output #0: loss = 1.1596 (* 1 = 1.1596 loss)
I0526 11:30:08.609218 22046 sgd_solver.cpp:106] Iteration 892500, lr = 0.0045
I0526 11:30:25.527174 22046 solver.cpp:237] Iteration 894000, loss = 1.01087
I0526 11:30:25.527376 22046 solver.cpp:253]     Train net output #0: loss = 1.01087 (* 1 = 1.01087 loss)
I0526 11:30:25.527395 22046 sgd_solver.cpp:106] Iteration 894000, lr = 0.0045
I0526 11:30:42.173663 22046 solver.cpp:237] Iteration 895500, loss = 0.673499
I0526 11:30:42.173702 22046 solver.cpp:253]     Train net output #0: loss = 0.673497 (* 1 = 0.673497 loss)
I0526 11:30:42.173718 22046 sgd_solver.cpp:106] Iteration 895500, lr = 0.0045
I0526 11:30:58.837687 22046 solver.cpp:237] Iteration 897000, loss = 0.765123
I0526 11:30:58.837878 22046 solver.cpp:253]     Train net output #0: loss = 0.765121 (* 1 = 0.765121 loss)
I0526 11:30:58.837896 22046 sgd_solver.cpp:106] Iteration 897000, lr = 0.0045
I0526 11:31:15.534570 22046 solver.cpp:237] Iteration 898500, loss = 0.690334
I0526 11:31:15.534627 22046 solver.cpp:253]     Train net output #0: loss = 0.690332 (* 1 = 0.690332 loss)
I0526 11:31:15.534654 22046 sgd_solver.cpp:106] Iteration 898500, lr = 0.0045
I0526 11:31:32.575387 22046 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_900000.caffemodel
I0526 11:31:32.624970 22046 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_10_lr_0.0045_2016-05-20T15.48.52.465324_iter_900000.solverstate
I0526 11:31:32.652855 22046 solver.cpp:341] Iteration 900000, Testing net (#0)
I0526 11:32:31.955382 22046 solver.cpp:409]     Test net output #0: accuracy = 0.891468
I0526 11:32:31.955576 22046 solver.cpp:409]     Test net output #1: loss = 0.360318 (* 1 = 0.360318 loss)
aprun: Apid 11267768: Caught signal Terminated, sending to application
*** Aborted at 1464276770 (unix time) try "date -d @1464276770" if you are using GNU date ***
PC: @     0x2aaab138cbd1 H5S_hyper_get_seq_list
*** SIGTERM (@0x561b) received by PID 22046 (TID 0x2aaac746f900) from PID 22043; stack trace: ***
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @     0x2aaab7c78850 (unknown)
=>> PBS: job killed: walltime 7211 exceeded limit 7200
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @     0x2aaab138cbd1 H5S_hyper_get_seq_list
    @     0x2aaab12a3307 H5D__select_io
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @     0x2aaab12a38cd H5D__select_read
    @     0x2aaab128be3d H5D__chunk_read
    @     0x2aaab129e5ec H5D__read
    @     0x2aaab129ec5c H5Dread
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @     0x2aaab0ff545c H5LTread_dataset_float
    @           0x4cd99a caffe::hdf5_load_nd_dataset<>()
    @           0x5b8d0e caffe::HDF5DataLayer<>::LoadHDF5FileData()
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @           0x626f33 caffe::HDF5DataLayer<>::Forward_gpu()
    @           0x5efe82 caffe::Net<>::ForwardFromTo()
    @           0x5eff97 caffe::Net<>::ForwardPrefilled()
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @           0x5ca109 caffe::Solver<>::Step()
    @           0x5caba5 caffe::Solver<>::Solve()
    @           0x43b3b8 train()
    @           0x43020c main
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @     0x2aaab7ea4c36 __libc_start_main
aprun: Apid 11267768: Caught signal Terminated, sending to application
    @           0x438669 (unknown)
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
aprun: Apid 11267768: Caught signal Terminated, sending to application
_pmiu_daemon(SIGCHLD): [NID 02200] [c4-3c0s3n2] [Thu May 26 11:32:52 2016] PE RANK 0 exit signal Terminated
Application 11267768 exit codes: 143
Application 11267768 resources: utime ~6270s, stime ~891s, Rss ~5329788, inblocks ~10476062, outblocks ~474916
