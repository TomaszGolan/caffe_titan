2807126
I0522 01:43:15.902329 25646 caffe.cpp:184] Using GPUs 0
I0522 01:43:16.326710 25646 solver.cpp:48] Initializing solver from parameters: 
test_iter: 7500
test_interval: 15000
base_lr: 0.003
display: 750
max_iter: 750000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 7500
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925.prototxt"
I0522 01:43:16.328668 25646 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925.prototxt
I0522 01:43:16.342450 25646 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0522 01:43:16.342520 25646 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0522 01:43:16.342897 25646 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 20
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0522 01:43:16.343101 25646 layer_factory.hpp:77] Creating layer data_hdf5
I0522 01:43:16.343127 25646 net.cpp:106] Creating Layer data_hdf5
I0522 01:43:16.343155 25646 net.cpp:411] data_hdf5 -> data
I0522 01:43:16.343190 25646 net.cpp:411] data_hdf5 -> label
I0522 01:43:16.343225 25646 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0522 01:43:16.352568 25646 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0522 01:43:16.367849 25646 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0522 01:43:37.891615 25646 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0522 01:43:37.896898 25646 net.cpp:150] Setting up data_hdf5
I0522 01:43:37.896939 25646 net.cpp:157] Top shape: 20 1 127 50 (127000)
I0522 01:43:37.896956 25646 net.cpp:157] Top shape: 20 (20)
I0522 01:43:37.896970 25646 net.cpp:165] Memory required for data: 508080
I0522 01:43:37.896988 25646 layer_factory.hpp:77] Creating layer conv1
I0522 01:43:37.897034 25646 net.cpp:106] Creating Layer conv1
I0522 01:43:37.897048 25646 net.cpp:454] conv1 <- data
I0522 01:43:37.897074 25646 net.cpp:411] conv1 -> conv1
I0522 01:43:41.121686 25646 net.cpp:150] Setting up conv1
I0522 01:43:41.121740 25646 net.cpp:157] Top shape: 20 12 120 48 (1382400)
I0522 01:43:41.121764 25646 net.cpp:165] Memory required for data: 6037680
I0522 01:43:41.121794 25646 layer_factory.hpp:77] Creating layer relu1
I0522 01:43:41.121816 25646 net.cpp:106] Creating Layer relu1
I0522 01:43:41.121829 25646 net.cpp:454] relu1 <- conv1
I0522 01:43:41.121851 25646 net.cpp:397] relu1 -> conv1 (in-place)
I0522 01:43:41.122401 25646 net.cpp:150] Setting up relu1
I0522 01:43:41.122426 25646 net.cpp:157] Top shape: 20 12 120 48 (1382400)
I0522 01:43:41.122438 25646 net.cpp:165] Memory required for data: 11567280
I0522 01:43:41.122454 25646 layer_factory.hpp:77] Creating layer pool1
I0522 01:43:41.122481 25646 net.cpp:106] Creating Layer pool1
I0522 01:43:41.122494 25646 net.cpp:454] pool1 <- conv1
I0522 01:43:41.122511 25646 net.cpp:411] pool1 -> pool1
I0522 01:43:41.122604 25646 net.cpp:150] Setting up pool1
I0522 01:43:41.122623 25646 net.cpp:157] Top shape: 20 12 60 48 (691200)
I0522 01:43:41.122644 25646 net.cpp:165] Memory required for data: 14332080
I0522 01:43:41.122659 25646 layer_factory.hpp:77] Creating layer conv2
I0522 01:43:41.122684 25646 net.cpp:106] Creating Layer conv2
I0522 01:43:41.122697 25646 net.cpp:454] conv2 <- pool1
I0522 01:43:41.122716 25646 net.cpp:411] conv2 -> conv2
I0522 01:43:41.125428 25646 net.cpp:150] Setting up conv2
I0522 01:43:41.125459 25646 net.cpp:157] Top shape: 20 20 54 46 (993600)
I0522 01:43:41.125475 25646 net.cpp:165] Memory required for data: 18306480
I0522 01:43:41.125504 25646 layer_factory.hpp:77] Creating layer relu2
I0522 01:43:41.125530 25646 net.cpp:106] Creating Layer relu2
I0522 01:43:41.125545 25646 net.cpp:454] relu2 <- conv2
I0522 01:43:41.125561 25646 net.cpp:397] relu2 -> conv2 (in-place)
I0522 01:43:41.125929 25646 net.cpp:150] Setting up relu2
I0522 01:43:41.125949 25646 net.cpp:157] Top shape: 20 20 54 46 (993600)
I0522 01:43:41.125962 25646 net.cpp:165] Memory required for data: 22280880
I0522 01:43:41.125974 25646 layer_factory.hpp:77] Creating layer pool2
I0522 01:43:41.125993 25646 net.cpp:106] Creating Layer pool2
I0522 01:43:41.126008 25646 net.cpp:454] pool2 <- conv2
I0522 01:43:41.126030 25646 net.cpp:411] pool2 -> pool2
I0522 01:43:41.126123 25646 net.cpp:150] Setting up pool2
I0522 01:43:41.126142 25646 net.cpp:157] Top shape: 20 20 27 46 (496800)
I0522 01:43:41.126157 25646 net.cpp:165] Memory required for data: 24268080
I0522 01:43:41.126176 25646 layer_factory.hpp:77] Creating layer conv3
I0522 01:43:41.126197 25646 net.cpp:106] Creating Layer conv3
I0522 01:43:41.126217 25646 net.cpp:454] conv3 <- pool2
I0522 01:43:41.126235 25646 net.cpp:411] conv3 -> conv3
I0522 01:43:41.128199 25646 net.cpp:150] Setting up conv3
I0522 01:43:41.128226 25646 net.cpp:157] Top shape: 20 28 22 44 (542080)
I0522 01:43:41.128245 25646 net.cpp:165] Memory required for data: 26436400
I0522 01:43:41.128268 25646 layer_factory.hpp:77] Creating layer relu3
I0522 01:43:41.128290 25646 net.cpp:106] Creating Layer relu3
I0522 01:43:41.128314 25646 net.cpp:454] relu3 <- conv3
I0522 01:43:41.128329 25646 net.cpp:397] relu3 -> conv3 (in-place)
I0522 01:43:41.128816 25646 net.cpp:150] Setting up relu3
I0522 01:43:41.128840 25646 net.cpp:157] Top shape: 20 28 22 44 (542080)
I0522 01:43:41.128854 25646 net.cpp:165] Memory required for data: 28604720
I0522 01:43:41.128870 25646 layer_factory.hpp:77] Creating layer pool3
I0522 01:43:41.128893 25646 net.cpp:106] Creating Layer pool3
I0522 01:43:41.128907 25646 net.cpp:454] pool3 <- conv3
I0522 01:43:41.128923 25646 net.cpp:411] pool3 -> pool3
I0522 01:43:41.129004 25646 net.cpp:150] Setting up pool3
I0522 01:43:41.129027 25646 net.cpp:157] Top shape: 20 28 11 44 (271040)
I0522 01:43:41.129040 25646 net.cpp:165] Memory required for data: 29688880
I0522 01:43:41.129055 25646 layer_factory.hpp:77] Creating layer conv4
I0522 01:43:41.129081 25646 net.cpp:106] Creating Layer conv4
I0522 01:43:41.129096 25646 net.cpp:454] conv4 <- pool3
I0522 01:43:41.129112 25646 net.cpp:411] conv4 -> conv4
I0522 01:43:41.131870 25646 net.cpp:150] Setting up conv4
I0522 01:43:41.131901 25646 net.cpp:157] Top shape: 20 36 6 42 (181440)
I0522 01:43:41.131917 25646 net.cpp:165] Memory required for data: 30414640
I0522 01:43:41.131937 25646 layer_factory.hpp:77] Creating layer relu4
I0522 01:43:41.131958 25646 net.cpp:106] Creating Layer relu4
I0522 01:43:41.131983 25646 net.cpp:454] relu4 <- conv4
I0522 01:43:41.131999 25646 net.cpp:397] relu4 -> conv4 (in-place)
I0522 01:43:41.132488 25646 net.cpp:150] Setting up relu4
I0522 01:43:41.132511 25646 net.cpp:157] Top shape: 20 36 6 42 (181440)
I0522 01:43:41.132524 25646 net.cpp:165] Memory required for data: 31140400
I0522 01:43:41.132540 25646 layer_factory.hpp:77] Creating layer pool4
I0522 01:43:41.132565 25646 net.cpp:106] Creating Layer pool4
I0522 01:43:41.132578 25646 net.cpp:454] pool4 <- conv4
I0522 01:43:41.132594 25646 net.cpp:411] pool4 -> pool4
I0522 01:43:41.132676 25646 net.cpp:150] Setting up pool4
I0522 01:43:41.132699 25646 net.cpp:157] Top shape: 20 36 3 42 (90720)
I0522 01:43:41.132711 25646 net.cpp:165] Memory required for data: 31503280
I0522 01:43:41.132726 25646 layer_factory.hpp:77] Creating layer ip1
I0522 01:43:41.132755 25646 net.cpp:106] Creating Layer ip1
I0522 01:43:41.132768 25646 net.cpp:454] ip1 <- pool4
I0522 01:43:41.132784 25646 net.cpp:411] ip1 -> ip1
I0522 01:43:41.148183 25646 net.cpp:150] Setting up ip1
I0522 01:43:41.148216 25646 net.cpp:157] Top shape: 20 196 (3920)
I0522 01:43:41.148237 25646 net.cpp:165] Memory required for data: 31518960
I0522 01:43:41.148263 25646 layer_factory.hpp:77] Creating layer relu5
I0522 01:43:41.148285 25646 net.cpp:106] Creating Layer relu5
I0522 01:43:41.148309 25646 net.cpp:454] relu5 <- ip1
I0522 01:43:41.148327 25646 net.cpp:397] relu5 -> ip1 (in-place)
I0522 01:43:41.148684 25646 net.cpp:150] Setting up relu5
I0522 01:43:41.148705 25646 net.cpp:157] Top shape: 20 196 (3920)
I0522 01:43:41.148717 25646 net.cpp:165] Memory required for data: 31534640
I0522 01:43:41.148732 25646 layer_factory.hpp:77] Creating layer drop1
I0522 01:43:41.148762 25646 net.cpp:106] Creating Layer drop1
I0522 01:43:41.148777 25646 net.cpp:454] drop1 <- ip1
I0522 01:43:41.148792 25646 net.cpp:397] drop1 -> ip1 (in-place)
I0522 01:43:41.148864 25646 net.cpp:150] Setting up drop1
I0522 01:43:41.148881 25646 net.cpp:157] Top shape: 20 196 (3920)
I0522 01:43:41.148895 25646 net.cpp:165] Memory required for data: 31550320
I0522 01:43:41.148910 25646 layer_factory.hpp:77] Creating layer ip2
I0522 01:43:41.148931 25646 net.cpp:106] Creating Layer ip2
I0522 01:43:41.148950 25646 net.cpp:454] ip2 <- ip1
I0522 01:43:41.148965 25646 net.cpp:411] ip2 -> ip2
I0522 01:43:41.149453 25646 net.cpp:150] Setting up ip2
I0522 01:43:41.149472 25646 net.cpp:157] Top shape: 20 98 (1960)
I0522 01:43:41.149485 25646 net.cpp:165] Memory required for data: 31558160
I0522 01:43:41.149507 25646 layer_factory.hpp:77] Creating layer relu6
I0522 01:43:41.149521 25646 net.cpp:106] Creating Layer relu6
I0522 01:43:41.149535 25646 net.cpp:454] relu6 <- ip2
I0522 01:43:41.149550 25646 net.cpp:397] relu6 -> ip2 (in-place)
I0522 01:43:41.150116 25646 net.cpp:150] Setting up relu6
I0522 01:43:41.150140 25646 net.cpp:157] Top shape: 20 98 (1960)
I0522 01:43:41.150153 25646 net.cpp:165] Memory required for data: 31566000
I0522 01:43:41.150169 25646 layer_factory.hpp:77] Creating layer drop2
I0522 01:43:41.150184 25646 net.cpp:106] Creating Layer drop2
I0522 01:43:41.150204 25646 net.cpp:454] drop2 <- ip2
I0522 01:43:41.150221 25646 net.cpp:397] drop2 -> ip2 (in-place)
I0522 01:43:41.150271 25646 net.cpp:150] Setting up drop2
I0522 01:43:41.150290 25646 net.cpp:157] Top shape: 20 98 (1960)
I0522 01:43:41.150308 25646 net.cpp:165] Memory required for data: 31573840
I0522 01:43:41.150326 25646 layer_factory.hpp:77] Creating layer ip3
I0522 01:43:41.150343 25646 net.cpp:106] Creating Layer ip3
I0522 01:43:41.150355 25646 net.cpp:454] ip3 <- ip2
I0522 01:43:41.150373 25646 net.cpp:411] ip3 -> ip3
I0522 01:43:41.150605 25646 net.cpp:150] Setting up ip3
I0522 01:43:41.150624 25646 net.cpp:157] Top shape: 20 11 (220)
I0522 01:43:41.150636 25646 net.cpp:165] Memory required for data: 31574720
I0522 01:43:41.150657 25646 layer_factory.hpp:77] Creating layer drop3
I0522 01:43:41.150672 25646 net.cpp:106] Creating Layer drop3
I0522 01:43:41.150692 25646 net.cpp:454] drop3 <- ip3
I0522 01:43:41.150708 25646 net.cpp:397] drop3 -> ip3 (in-place)
I0522 01:43:41.150754 25646 net.cpp:150] Setting up drop3
I0522 01:43:41.150777 25646 net.cpp:157] Top shape: 20 11 (220)
I0522 01:43:41.150790 25646 net.cpp:165] Memory required for data: 31575600
I0522 01:43:41.150802 25646 layer_factory.hpp:77] Creating layer loss
I0522 01:43:41.150823 25646 net.cpp:106] Creating Layer loss
I0522 01:43:41.150838 25646 net.cpp:454] loss <- ip3
I0522 01:43:41.150858 25646 net.cpp:454] loss <- label
I0522 01:43:41.150874 25646 net.cpp:411] loss -> loss
I0522 01:43:41.150894 25646 layer_factory.hpp:77] Creating layer loss
I0522 01:43:41.151561 25646 net.cpp:150] Setting up loss
I0522 01:43:41.151582 25646 net.cpp:157] Top shape: (1)
I0522 01:43:41.151600 25646 net.cpp:160]     with loss weight 1
I0522 01:43:41.151648 25646 net.cpp:165] Memory required for data: 31575604
I0522 01:43:41.151671 25646 net.cpp:226] loss needs backward computation.
I0522 01:43:41.151685 25646 net.cpp:226] drop3 needs backward computation.
I0522 01:43:41.151698 25646 net.cpp:226] ip3 needs backward computation.
I0522 01:43:41.151712 25646 net.cpp:226] drop2 needs backward computation.
I0522 01:43:41.151724 25646 net.cpp:226] relu6 needs backward computation.
I0522 01:43:41.151738 25646 net.cpp:226] ip2 needs backward computation.
I0522 01:43:41.151751 25646 net.cpp:226] drop1 needs backward computation.
I0522 01:43:41.151772 25646 net.cpp:226] relu5 needs backward computation.
I0522 01:43:41.151784 25646 net.cpp:226] ip1 needs backward computation.
I0522 01:43:41.151801 25646 net.cpp:226] pool4 needs backward computation.
I0522 01:43:41.151815 25646 net.cpp:226] relu4 needs backward computation.
I0522 01:43:41.151828 25646 net.cpp:226] conv4 needs backward computation.
I0522 01:43:41.151839 25646 net.cpp:226] pool3 needs backward computation.
I0522 01:43:41.151855 25646 net.cpp:226] relu3 needs backward computation.
I0522 01:43:41.151876 25646 net.cpp:226] conv3 needs backward computation.
I0522 01:43:41.151899 25646 net.cpp:226] pool2 needs backward computation.
I0522 01:43:41.151916 25646 net.cpp:226] relu2 needs backward computation.
I0522 01:43:41.151928 25646 net.cpp:226] conv2 needs backward computation.
I0522 01:43:41.151942 25646 net.cpp:226] pool1 needs backward computation.
I0522 01:43:41.151953 25646 net.cpp:226] relu1 needs backward computation.
I0522 01:43:41.151969 25646 net.cpp:226] conv1 needs backward computation.
I0522 01:43:41.151990 25646 net.cpp:228] data_hdf5 does not need backward computation.
I0522 01:43:41.152004 25646 net.cpp:270] This network produces output loss
I0522 01:43:41.152030 25646 net.cpp:283] Network initialization done.
I0522 01:43:41.153797 25646 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/logs_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925.prototxt
I0522 01:43:41.153875 25646 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0522 01:43:41.154256 25646 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 20
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0522 01:43:41.154479 25646 layer_factory.hpp:77] Creating layer data_hdf5
I0522 01:43:41.154498 25646 net.cpp:106] Creating Layer data_hdf5
I0522 01:43:41.154515 25646 net.cpp:411] data_hdf5 -> data
I0522 01:43:41.154534 25646 net.cpp:411] data_hdf5 -> label
I0522 01:43:41.154556 25646 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0522 01:43:41.166818 25646 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0522 01:44:02.511692 25646 net.cpp:150] Setting up data_hdf5
I0522 01:44:02.511858 25646 net.cpp:157] Top shape: 20 1 127 50 (127000)
I0522 01:44:02.511884 25646 net.cpp:157] Top shape: 20 (20)
I0522 01:44:02.511898 25646 net.cpp:165] Memory required for data: 508080
I0522 01:44:02.511912 25646 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0522 01:44:02.511940 25646 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0522 01:44:02.511960 25646 net.cpp:454] label_data_hdf5_1_split <- label
I0522 01:44:02.511996 25646 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0522 01:44:02.512017 25646 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0522 01:44:02.512096 25646 net.cpp:150] Setting up label_data_hdf5_1_split
I0522 01:44:02.512120 25646 net.cpp:157] Top shape: 20 (20)
I0522 01:44:02.512136 25646 net.cpp:157] Top shape: 20 (20)
I0522 01:44:02.512148 25646 net.cpp:165] Memory required for data: 508240
I0522 01:44:02.512167 25646 layer_factory.hpp:77] Creating layer conv1
I0522 01:44:02.512192 25646 net.cpp:106] Creating Layer conv1
I0522 01:44:02.512207 25646 net.cpp:454] conv1 <- data
I0522 01:44:02.512229 25646 net.cpp:411] conv1 -> conv1
I0522 01:44:02.514176 25646 net.cpp:150] Setting up conv1
I0522 01:44:02.514202 25646 net.cpp:157] Top shape: 20 12 120 48 (1382400)
I0522 01:44:02.514215 25646 net.cpp:165] Memory required for data: 6037840
I0522 01:44:02.514242 25646 layer_factory.hpp:77] Creating layer relu1
I0522 01:44:02.514269 25646 net.cpp:106] Creating Layer relu1
I0522 01:44:02.514283 25646 net.cpp:454] relu1 <- conv1
I0522 01:44:02.514300 25646 net.cpp:397] relu1 -> conv1 (in-place)
I0522 01:44:02.514821 25646 net.cpp:150] Setting up relu1
I0522 01:44:02.514844 25646 net.cpp:157] Top shape: 20 12 120 48 (1382400)
I0522 01:44:02.514858 25646 net.cpp:165] Memory required for data: 11567440
I0522 01:44:02.514869 25646 layer_factory.hpp:77] Creating layer pool1
I0522 01:44:02.514900 25646 net.cpp:106] Creating Layer pool1
I0522 01:44:02.514914 25646 net.cpp:454] pool1 <- conv1
I0522 01:44:02.514930 25646 net.cpp:411] pool1 -> pool1
I0522 01:44:02.515019 25646 net.cpp:150] Setting up pool1
I0522 01:44:02.515036 25646 net.cpp:157] Top shape: 20 12 60 48 (691200)
I0522 01:44:02.515051 25646 net.cpp:165] Memory required for data: 14332240
I0522 01:44:02.515070 25646 layer_factory.hpp:77] Creating layer conv2
I0522 01:44:02.515092 25646 net.cpp:106] Creating Layer conv2
I0522 01:44:02.515105 25646 net.cpp:454] conv2 <- pool1
I0522 01:44:02.515121 25646 net.cpp:411] conv2 -> conv2
I0522 01:44:02.517067 25646 net.cpp:150] Setting up conv2
I0522 01:44:02.517091 25646 net.cpp:157] Top shape: 20 20 54 46 (993600)
I0522 01:44:02.517112 25646 net.cpp:165] Memory required for data: 18306640
I0522 01:44:02.517134 25646 layer_factory.hpp:77] Creating layer relu2
I0522 01:44:02.517153 25646 net.cpp:106] Creating Layer relu2
I0522 01:44:02.517175 25646 net.cpp:454] relu2 <- conv2
I0522 01:44:02.517191 25646 net.cpp:397] relu2 -> conv2 (in-place)
I0522 01:44:02.517540 25646 net.cpp:150] Setting up relu2
I0522 01:44:02.517560 25646 net.cpp:157] Top shape: 20 20 54 46 (993600)
I0522 01:44:02.517573 25646 net.cpp:165] Memory required for data: 22281040
I0522 01:44:02.517588 25646 layer_factory.hpp:77] Creating layer pool2
I0522 01:44:02.517611 25646 net.cpp:106] Creating Layer pool2
I0522 01:44:02.517626 25646 net.cpp:454] pool2 <- conv2
I0522 01:44:02.517642 25646 net.cpp:411] pool2 -> pool2
I0522 01:44:02.517737 25646 net.cpp:150] Setting up pool2
I0522 01:44:02.517755 25646 net.cpp:157] Top shape: 20 20 27 46 (496800)
I0522 01:44:02.517776 25646 net.cpp:165] Memory required for data: 24268240
I0522 01:44:02.517791 25646 layer_factory.hpp:77] Creating layer conv3
I0522 01:44:02.517812 25646 net.cpp:106] Creating Layer conv3
I0522 01:44:02.517827 25646 net.cpp:454] conv3 <- pool2
I0522 01:44:02.517843 25646 net.cpp:411] conv3 -> conv3
I0522 01:44:02.519843 25646 net.cpp:150] Setting up conv3
I0522 01:44:02.519872 25646 net.cpp:157] Top shape: 20 28 22 44 (542080)
I0522 01:44:02.519886 25646 net.cpp:165] Memory required for data: 26436560
I0522 01:44:02.519912 25646 layer_factory.hpp:77] Creating layer relu3
I0522 01:44:02.519950 25646 net.cpp:106] Creating Layer relu3
I0522 01:44:02.519964 25646 net.cpp:454] relu3 <- conv3
I0522 01:44:02.519989 25646 net.cpp:397] relu3 -> conv3 (in-place)
I0522 01:44:02.520488 25646 net.cpp:150] Setting up relu3
I0522 01:44:02.520511 25646 net.cpp:157] Top shape: 20 28 22 44 (542080)
I0522 01:44:02.520524 25646 net.cpp:165] Memory required for data: 28604880
I0522 01:44:02.520540 25646 layer_factory.hpp:77] Creating layer pool3
I0522 01:44:02.520565 25646 net.cpp:106] Creating Layer pool3
I0522 01:44:02.520578 25646 net.cpp:454] pool3 <- conv3
I0522 01:44:02.520594 25646 net.cpp:411] pool3 -> pool3
I0522 01:44:02.520679 25646 net.cpp:150] Setting up pool3
I0522 01:44:02.520697 25646 net.cpp:157] Top shape: 20 28 11 44 (271040)
I0522 01:44:02.520711 25646 net.cpp:165] Memory required for data: 29689040
I0522 01:44:02.520725 25646 layer_factory.hpp:77] Creating layer conv4
I0522 01:44:02.520751 25646 net.cpp:106] Creating Layer conv4
I0522 01:44:02.520764 25646 net.cpp:454] conv4 <- pool3
I0522 01:44:02.520781 25646 net.cpp:411] conv4 -> conv4
I0522 01:44:02.522871 25646 net.cpp:150] Setting up conv4
I0522 01:44:02.522896 25646 net.cpp:157] Top shape: 20 36 6 42 (181440)
I0522 01:44:02.522915 25646 net.cpp:165] Memory required for data: 30414800
I0522 01:44:02.522934 25646 layer_factory.hpp:77] Creating layer relu4
I0522 01:44:02.522954 25646 net.cpp:106] Creating Layer relu4
I0522 01:44:02.522966 25646 net.cpp:454] relu4 <- conv4
I0522 01:44:02.522992 25646 net.cpp:397] relu4 -> conv4 (in-place)
I0522 01:44:02.523483 25646 net.cpp:150] Setting up relu4
I0522 01:44:02.523505 25646 net.cpp:157] Top shape: 20 36 6 42 (181440)
I0522 01:44:02.523519 25646 net.cpp:165] Memory required for data: 31140560
I0522 01:44:02.523535 25646 layer_factory.hpp:77] Creating layer pool4
I0522 01:44:02.523558 25646 net.cpp:106] Creating Layer pool4
I0522 01:44:02.523572 25646 net.cpp:454] pool4 <- conv4
I0522 01:44:02.523588 25646 net.cpp:411] pool4 -> pool4
I0522 01:44:02.523674 25646 net.cpp:150] Setting up pool4
I0522 01:44:02.523692 25646 net.cpp:157] Top shape: 20 36 3 42 (90720)
I0522 01:44:02.523707 25646 net.cpp:165] Memory required for data: 31503440
I0522 01:44:02.523720 25646 layer_factory.hpp:77] Creating layer ip1
I0522 01:44:02.523743 25646 net.cpp:106] Creating Layer ip1
I0522 01:44:02.523757 25646 net.cpp:454] ip1 <- pool4
I0522 01:44:02.523777 25646 net.cpp:411] ip1 -> ip1
I0522 01:44:02.539275 25646 net.cpp:150] Setting up ip1
I0522 01:44:02.539314 25646 net.cpp:157] Top shape: 20 196 (3920)
I0522 01:44:02.539327 25646 net.cpp:165] Memory required for data: 31519120
I0522 01:44:02.539352 25646 layer_factory.hpp:77] Creating layer relu5
I0522 01:44:02.539374 25646 net.cpp:106] Creating Layer relu5
I0522 01:44:02.539400 25646 net.cpp:454] relu5 <- ip1
I0522 01:44:02.539417 25646 net.cpp:397] relu5 -> ip1 (in-place)
I0522 01:44:02.539778 25646 net.cpp:150] Setting up relu5
I0522 01:44:02.539798 25646 net.cpp:157] Top shape: 20 196 (3920)
I0522 01:44:02.539811 25646 net.cpp:165] Memory required for data: 31534800
I0522 01:44:02.539826 25646 layer_factory.hpp:77] Creating layer drop1
I0522 01:44:02.539855 25646 net.cpp:106] Creating Layer drop1
I0522 01:44:02.539870 25646 net.cpp:454] drop1 <- ip1
I0522 01:44:02.539886 25646 net.cpp:397] drop1 -> ip1 (in-place)
I0522 01:44:02.539947 25646 net.cpp:150] Setting up drop1
I0522 01:44:02.539963 25646 net.cpp:157] Top shape: 20 196 (3920)
I0522 01:44:02.539976 25646 net.cpp:165] Memory required for data: 31550480
I0522 01:44:02.539988 25646 layer_factory.hpp:77] Creating layer ip2
I0522 01:44:02.540009 25646 net.cpp:106] Creating Layer ip2
I0522 01:44:02.540021 25646 net.cpp:454] ip2 <- ip1
I0522 01:44:02.540045 25646 net.cpp:411] ip2 -> ip2
I0522 01:44:02.540539 25646 net.cpp:150] Setting up ip2
I0522 01:44:02.540560 25646 net.cpp:157] Top shape: 20 98 (1960)
I0522 01:44:02.540572 25646 net.cpp:165] Memory required for data: 31558320
I0522 01:44:02.540593 25646 layer_factory.hpp:77] Creating layer relu6
I0522 01:44:02.540627 25646 net.cpp:106] Creating Layer relu6
I0522 01:44:02.540640 25646 net.cpp:454] relu6 <- ip2
I0522 01:44:02.540657 25646 net.cpp:397] relu6 -> ip2 (in-place)
I0522 01:44:02.541216 25646 net.cpp:150] Setting up relu6
I0522 01:44:02.541239 25646 net.cpp:157] Top shape: 20 98 (1960)
I0522 01:44:02.541254 25646 net.cpp:165] Memory required for data: 31566160
I0522 01:44:02.541265 25646 layer_factory.hpp:77] Creating layer drop2
I0522 01:44:02.541285 25646 net.cpp:106] Creating Layer drop2
I0522 01:44:02.541306 25646 net.cpp:454] drop2 <- ip2
I0522 01:44:02.541322 25646 net.cpp:397] drop2 -> ip2 (in-place)
I0522 01:44:02.541374 25646 net.cpp:150] Setting up drop2
I0522 01:44:02.541399 25646 net.cpp:157] Top shape: 20 98 (1960)
I0522 01:44:02.541411 25646 net.cpp:165] Memory required for data: 31574000
I0522 01:44:02.541426 25646 layer_factory.hpp:77] Creating layer ip3
I0522 01:44:02.541443 25646 net.cpp:106] Creating Layer ip3
I0522 01:44:02.541460 25646 net.cpp:454] ip3 <- ip2
I0522 01:44:02.541482 25646 net.cpp:411] ip3 -> ip3
I0522 01:44:02.541728 25646 net.cpp:150] Setting up ip3
I0522 01:44:02.541746 25646 net.cpp:157] Top shape: 20 11 (220)
I0522 01:44:02.541759 25646 net.cpp:165] Memory required for data: 31574880
I0522 01:44:02.541780 25646 layer_factory.hpp:77] Creating layer drop3
I0522 01:44:02.541802 25646 net.cpp:106] Creating Layer drop3
I0522 01:44:02.541816 25646 net.cpp:454] drop3 <- ip3
I0522 01:44:02.541831 25646 net.cpp:397] drop3 -> ip3 (in-place)
I0522 01:44:02.541887 25646 net.cpp:150] Setting up drop3
I0522 01:44:02.541903 25646 net.cpp:157] Top shape: 20 11 (220)
I0522 01:44:02.541914 25646 net.cpp:165] Memory required for data: 31575760
I0522 01:44:02.541929 25646 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0522 01:44:02.541945 25646 net.cpp:106] Creating Layer ip3_drop3_0_split
I0522 01:44:02.541961 25646 net.cpp:454] ip3_drop3_0_split <- ip3
I0522 01:44:02.541983 25646 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0522 01:44:02.542002 25646 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0522 01:44:02.542089 25646 net.cpp:150] Setting up ip3_drop3_0_split
I0522 01:44:02.542106 25646 net.cpp:157] Top shape: 20 11 (220)
I0522 01:44:02.542122 25646 net.cpp:157] Top shape: 20 11 (220)
I0522 01:44:02.542135 25646 net.cpp:165] Memory required for data: 31577520
I0522 01:44:02.542150 25646 layer_factory.hpp:77] Creating layer accuracy
I0522 01:44:02.542177 25646 net.cpp:106] Creating Layer accuracy
I0522 01:44:02.542191 25646 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0522 01:44:02.542215 25646 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0522 01:44:02.542232 25646 net.cpp:411] accuracy -> accuracy
I0522 01:44:02.542260 25646 net.cpp:150] Setting up accuracy
I0522 01:44:02.542281 25646 net.cpp:157] Top shape: (1)
I0522 01:44:02.542294 25646 net.cpp:165] Memory required for data: 31577524
I0522 01:44:02.542306 25646 layer_factory.hpp:77] Creating layer loss
I0522 01:44:02.542323 25646 net.cpp:106] Creating Layer loss
I0522 01:44:02.542336 25646 net.cpp:454] loss <- ip3_drop3_0_split_1
I0522 01:44:02.542351 25646 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0522 01:44:02.542367 25646 net.cpp:411] loss -> loss
I0522 01:44:02.542395 25646 layer_factory.hpp:77] Creating layer loss
I0522 01:44:02.542901 25646 net.cpp:150] Setting up loss
I0522 01:44:02.542922 25646 net.cpp:157] Top shape: (1)
I0522 01:44:02.542934 25646 net.cpp:160]     with loss weight 1
I0522 01:44:02.542958 25646 net.cpp:165] Memory required for data: 31577528
I0522 01:44:02.542979 25646 net.cpp:226] loss needs backward computation.
I0522 01:44:02.542994 25646 net.cpp:228] accuracy does not need backward computation.
I0522 01:44:02.543007 25646 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0522 01:44:02.543021 25646 net.cpp:226] drop3 needs backward computation.
I0522 01:44:02.543033 25646 net.cpp:226] ip3 needs backward computation.
I0522 01:44:02.543048 25646 net.cpp:226] drop2 needs backward computation.
I0522 01:44:02.543066 25646 net.cpp:226] relu6 needs backward computation.
I0522 01:44:02.543089 25646 net.cpp:226] ip2 needs backward computation.
I0522 01:44:02.543109 25646 net.cpp:226] drop1 needs backward computation.
I0522 01:44:02.543123 25646 net.cpp:226] relu5 needs backward computation.
I0522 01:44:02.543134 25646 net.cpp:226] ip1 needs backward computation.
I0522 01:44:02.543149 25646 net.cpp:226] pool4 needs backward computation.
I0522 01:44:02.543162 25646 net.cpp:226] relu4 needs backward computation.
I0522 01:44:02.543182 25646 net.cpp:226] conv4 needs backward computation.
I0522 01:44:02.543197 25646 net.cpp:226] pool3 needs backward computation.
I0522 01:44:02.543213 25646 net.cpp:226] relu3 needs backward computation.
I0522 01:44:02.543226 25646 net.cpp:226] conv3 needs backward computation.
I0522 01:44:02.543238 25646 net.cpp:226] pool2 needs backward computation.
I0522 01:44:02.543251 25646 net.cpp:226] relu2 needs backward computation.
I0522 01:44:02.543265 25646 net.cpp:226] conv2 needs backward computation.
I0522 01:44:02.543284 25646 net.cpp:226] pool1 needs backward computation.
I0522 01:44:02.543298 25646 net.cpp:226] relu1 needs backward computation.
I0522 01:44:02.543311 25646 net.cpp:226] conv1 needs backward computation.
I0522 01:44:02.543325 25646 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0522 01:44:02.543339 25646 net.cpp:228] data_hdf5 does not need backward computation.
I0522 01:44:02.543351 25646 net.cpp:270] This network produces output accuracy
I0522 01:44:02.543366 25646 net.cpp:270] This network produces output loss
I0522 01:44:02.543396 25646 net.cpp:283] Network initialization done.
I0522 01:44:02.543532 25646 solver.cpp:60] Solver scaffolding done.
I0522 01:44:02.544666 25646 caffe.cpp:212] Starting Optimization
I0522 01:44:02.544682 25646 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0522 01:44:02.544697 25646 solver.cpp:289] Learning Rate Policy: fixed
I0522 01:44:02.545933 25646 solver.cpp:341] Iteration 0, Testing net (#0)
I0522 01:44:55.437855 25646 solver.cpp:409]     Test net output #0: accuracy = 0.076052
I0522 01:44:55.438014 25646 solver.cpp:409]     Test net output #1: loss = 2.398 (* 1 = 2.398 loss)
I0522 01:44:55.457141 25646 solver.cpp:237] Iteration 0, loss = 2.38936
I0522 01:44:55.457181 25646 solver.cpp:253]     Train net output #0: loss = 2.38936 (* 1 = 2.38936 loss)
I0522 01:44:55.457202 25646 sgd_solver.cpp:106] Iteration 0, lr = 0.003
I0522 01:45:07.567322 25646 solver.cpp:237] Iteration 750, loss = 2.02024
I0522 01:45:07.567376 25646 solver.cpp:253]     Train net output #0: loss = 2.02024 (* 1 = 2.02024 loss)
I0522 01:45:07.567394 25646 sgd_solver.cpp:106] Iteration 750, lr = 0.003
I0522 01:45:19.649430 25646 solver.cpp:237] Iteration 1500, loss = 2.14785
I0522 01:45:19.649469 25646 solver.cpp:253]     Train net output #0: loss = 2.14785 (* 1 = 2.14785 loss)
I0522 01:45:19.649487 25646 sgd_solver.cpp:106] Iteration 1500, lr = 0.003
I0522 01:45:31.793850 25646 solver.cpp:237] Iteration 2250, loss = 2.05501
I0522 01:45:31.794016 25646 solver.cpp:253]     Train net output #0: loss = 2.05501 (* 1 = 2.05501 loss)
I0522 01:45:31.794034 25646 sgd_solver.cpp:106] Iteration 2250, lr = 0.003
I0522 01:45:43.971520 25646 solver.cpp:237] Iteration 3000, loss = 1.09126
I0522 01:45:43.971560 25646 solver.cpp:253]     Train net output #0: loss = 1.09126 (* 1 = 1.09126 loss)
I0522 01:45:43.971577 25646 sgd_solver.cpp:106] Iteration 3000, lr = 0.003
I0522 01:45:56.140952 25646 solver.cpp:237] Iteration 3750, loss = 1.68691
I0522 01:45:56.141007 25646 solver.cpp:253]     Train net output #0: loss = 1.68691 (* 1 = 1.68691 loss)
I0522 01:45:56.141026 25646 sgd_solver.cpp:106] Iteration 3750, lr = 0.003
I0522 01:46:08.293493 25646 solver.cpp:237] Iteration 4500, loss = 1.52446
I0522 01:46:08.293638 25646 solver.cpp:253]     Train net output #0: loss = 1.52446 (* 1 = 1.52446 loss)
I0522 01:46:08.293655 25646 sgd_solver.cpp:106] Iteration 4500, lr = 0.003
I0522 01:46:42.631543 25646 solver.cpp:237] Iteration 5250, loss = 1.35079
I0522 01:46:42.631708 25646 solver.cpp:253]     Train net output #0: loss = 1.35079 (* 1 = 1.35079 loss)
I0522 01:46:42.631726 25646 sgd_solver.cpp:106] Iteration 5250, lr = 0.003
I0522 01:46:54.751154 25646 solver.cpp:237] Iteration 6000, loss = 1.13945
I0522 01:46:54.751207 25646 solver.cpp:253]     Train net output #0: loss = 1.13945 (* 1 = 1.13945 loss)
I0522 01:46:54.751225 25646 sgd_solver.cpp:106] Iteration 6000, lr = 0.003
I0522 01:47:06.860121 25646 solver.cpp:237] Iteration 6750, loss = 1.56029
I0522 01:47:06.860157 25646 solver.cpp:253]     Train net output #0: loss = 1.56029 (* 1 = 1.56029 loss)
I0522 01:47:06.860177 25646 sgd_solver.cpp:106] Iteration 6750, lr = 0.003
I0522 01:47:18.929131 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_7500.caffemodel
I0522 01:47:18.981955 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_7500.solverstate
I0522 01:47:19.012341 25646 solver.cpp:237] Iteration 7500, loss = 1.355
I0522 01:47:19.012394 25646 solver.cpp:253]     Train net output #0: loss = 1.355 (* 1 = 1.355 loss)
I0522 01:47:19.012413 25646 sgd_solver.cpp:106] Iteration 7500, lr = 0.003
I0522 01:47:31.171203 25646 solver.cpp:237] Iteration 8250, loss = 1.90843
I0522 01:47:31.171242 25646 solver.cpp:253]     Train net output #0: loss = 1.90843 (* 1 = 1.90843 loss)
I0522 01:47:31.171260 25646 sgd_solver.cpp:106] Iteration 8250, lr = 0.003
I0522 01:47:43.284510 25646 solver.cpp:237] Iteration 9000, loss = 1.39746
I0522 01:47:43.284562 25646 solver.cpp:253]     Train net output #0: loss = 1.39746 (* 1 = 1.39746 loss)
I0522 01:47:43.284581 25646 sgd_solver.cpp:106] Iteration 9000, lr = 0.003
I0522 01:47:55.341563 25646 solver.cpp:237] Iteration 9750, loss = 1.25978
I0522 01:47:55.341727 25646 solver.cpp:253]     Train net output #0: loss = 1.25978 (* 1 = 1.25978 loss)
I0522 01:47:55.341744 25646 sgd_solver.cpp:106] Iteration 9750, lr = 0.003
I0522 01:48:29.607471 25646 solver.cpp:237] Iteration 10500, loss = 1.18494
I0522 01:48:29.607632 25646 solver.cpp:253]     Train net output #0: loss = 1.18494 (* 1 = 1.18494 loss)
I0522 01:48:29.607650 25646 sgd_solver.cpp:106] Iteration 10500, lr = 0.003
I0522 01:48:41.740077 25646 solver.cpp:237] Iteration 11250, loss = 0.934877
I0522 01:48:41.740115 25646 solver.cpp:253]     Train net output #0: loss = 0.934877 (* 1 = 0.934877 loss)
I0522 01:48:41.740133 25646 sgd_solver.cpp:106] Iteration 11250, lr = 0.003
I0522 01:48:53.891628 25646 solver.cpp:237] Iteration 12000, loss = 1.31021
I0522 01:48:53.891676 25646 solver.cpp:253]     Train net output #0: loss = 1.31021 (* 1 = 1.31021 loss)
I0522 01:48:53.891695 25646 sgd_solver.cpp:106] Iteration 12000, lr = 0.003
I0522 01:49:06.013347 25646 solver.cpp:237] Iteration 12750, loss = 1.46454
I0522 01:49:06.013502 25646 solver.cpp:253]     Train net output #0: loss = 1.46454 (* 1 = 1.46454 loss)
I0522 01:49:06.013520 25646 sgd_solver.cpp:106] Iteration 12750, lr = 0.003
I0522 01:49:18.128656 25646 solver.cpp:237] Iteration 13500, loss = 1.7198
I0522 01:49:18.128707 25646 solver.cpp:253]     Train net output #0: loss = 1.71979 (* 1 = 1.71979 loss)
I0522 01:49:18.128725 25646 sgd_solver.cpp:106] Iteration 13500, lr = 0.003
I0522 01:49:30.268055 25646 solver.cpp:237] Iteration 14250, loss = 1.52542
I0522 01:49:30.268095 25646 solver.cpp:253]     Train net output #0: loss = 1.52542 (* 1 = 1.52542 loss)
I0522 01:49:30.268111 25646 sgd_solver.cpp:106] Iteration 14250, lr = 0.003
I0522 01:49:42.376123 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_15000.caffemodel
I0522 01:49:42.426380 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_15000.solverstate
I0522 01:49:42.451545 25646 solver.cpp:341] Iteration 15000, Testing net (#0)
I0522 01:50:34.325816 25646 solver.cpp:409]     Test net output #0: accuracy = 0.839673
I0522 01:50:34.325975 25646 solver.cpp:409]     Test net output #1: loss = 0.531097 (* 1 = 0.531097 loss)
I0522 01:50:56.540871 25646 solver.cpp:237] Iteration 15000, loss = 1.81707
I0522 01:50:56.540928 25646 solver.cpp:253]     Train net output #0: loss = 1.81707 (* 1 = 1.81707 loss)
I0522 01:50:56.540949 25646 sgd_solver.cpp:106] Iteration 15000, lr = 0.003
I0522 01:51:08.656282 25646 solver.cpp:237] Iteration 15750, loss = 0.930099
I0522 01:51:08.656431 25646 solver.cpp:253]     Train net output #0: loss = 0.930098 (* 1 = 0.930098 loss)
I0522 01:51:08.656448 25646 sgd_solver.cpp:106] Iteration 15750, lr = 0.003
I0522 01:51:20.754245 25646 solver.cpp:237] Iteration 16500, loss = 1.52191
I0522 01:51:20.754297 25646 solver.cpp:253]     Train net output #0: loss = 1.52191 (* 1 = 1.52191 loss)
I0522 01:51:20.754315 25646 sgd_solver.cpp:106] Iteration 16500, lr = 0.003
I0522 01:51:32.819931 25646 solver.cpp:237] Iteration 17250, loss = 1.41934
I0522 01:51:32.819969 25646 solver.cpp:253]     Train net output #0: loss = 1.41934 (* 1 = 1.41934 loss)
I0522 01:51:32.819988 25646 sgd_solver.cpp:106] Iteration 17250, lr = 0.003
I0522 01:51:44.913533 25646 solver.cpp:237] Iteration 18000, loss = 1.31943
I0522 01:51:44.913700 25646 solver.cpp:253]     Train net output #0: loss = 1.31943 (* 1 = 1.31943 loss)
I0522 01:51:44.913722 25646 sgd_solver.cpp:106] Iteration 18000, lr = 0.003
I0522 01:51:57.070248 25646 solver.cpp:237] Iteration 18750, loss = 1.93311
I0522 01:51:57.070286 25646 solver.cpp:253]     Train net output #0: loss = 1.93311 (* 1 = 1.93311 loss)
I0522 01:51:57.070304 25646 sgd_solver.cpp:106] Iteration 18750, lr = 0.003
I0522 01:52:09.224619 25646 solver.cpp:237] Iteration 19500, loss = 1.30862
I0522 01:52:09.224670 25646 solver.cpp:253]     Train net output #0: loss = 1.30862 (* 1 = 1.30862 loss)
I0522 01:52:09.224695 25646 sgd_solver.cpp:106] Iteration 19500, lr = 0.003
I0522 01:52:43.487038 25646 solver.cpp:237] Iteration 20250, loss = 1.19531
I0522 01:52:43.487205 25646 solver.cpp:253]     Train net output #0: loss = 1.19531 (* 1 = 1.19531 loss)
I0522 01:52:43.487222 25646 sgd_solver.cpp:106] Iteration 20250, lr = 0.003
I0522 01:52:55.602123 25646 solver.cpp:237] Iteration 21000, loss = 1.31427
I0522 01:52:55.602159 25646 solver.cpp:253]     Train net output #0: loss = 1.31427 (* 1 = 1.31427 loss)
I0522 01:52:55.602176 25646 sgd_solver.cpp:106] Iteration 21000, lr = 0.003
I0522 01:53:07.764020 25646 solver.cpp:237] Iteration 21750, loss = 0.95012
I0522 01:53:07.764075 25646 solver.cpp:253]     Train net output #0: loss = 0.950119 (* 1 = 0.950119 loss)
I0522 01:53:07.764091 25646 sgd_solver.cpp:106] Iteration 21750, lr = 0.003
I0522 01:53:19.982257 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_22500.caffemodel
I0522 01:53:20.033725 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_22500.solverstate
I0522 01:53:20.067322 25646 solver.cpp:237] Iteration 22500, loss = 1.18179
I0522 01:53:20.067379 25646 solver.cpp:253]     Train net output #0: loss = 1.18179 (* 1 = 1.18179 loss)
I0522 01:53:20.067397 25646 sgd_solver.cpp:106] Iteration 22500, lr = 0.003
I0522 01:53:32.221202 25646 solver.cpp:237] Iteration 23250, loss = 1.09289
I0522 01:53:32.221256 25646 solver.cpp:253]     Train net output #0: loss = 1.09289 (* 1 = 1.09289 loss)
I0522 01:53:32.221273 25646 sgd_solver.cpp:106] Iteration 23250, lr = 0.003
I0522 01:53:44.365926 25646 solver.cpp:237] Iteration 24000, loss = 1.23812
I0522 01:53:44.365965 25646 solver.cpp:253]     Train net output #0: loss = 1.23812 (* 1 = 1.23812 loss)
I0522 01:53:44.365983 25646 sgd_solver.cpp:106] Iteration 24000, lr = 0.003
I0522 01:53:56.516311 25646 solver.cpp:237] Iteration 24750, loss = 1.17243
I0522 01:53:56.516470 25646 solver.cpp:253]     Train net output #0: loss = 1.17243 (* 1 = 1.17243 loss)
I0522 01:53:56.516489 25646 sgd_solver.cpp:106] Iteration 24750, lr = 0.003
I0522 01:54:30.842330 25646 solver.cpp:237] Iteration 25500, loss = 1.23018
I0522 01:54:30.842497 25646 solver.cpp:253]     Train net output #0: loss = 1.23018 (* 1 = 1.23018 loss)
I0522 01:54:30.842514 25646 sgd_solver.cpp:106] Iteration 25500, lr = 0.003
I0522 01:54:42.988075 25646 solver.cpp:237] Iteration 26250, loss = 1.18236
I0522 01:54:42.988127 25646 solver.cpp:253]     Train net output #0: loss = 1.18236 (* 1 = 1.18236 loss)
I0522 01:54:42.988145 25646 sgd_solver.cpp:106] Iteration 26250, lr = 0.003
I0522 01:54:55.069126 25646 solver.cpp:237] Iteration 27000, loss = 1.22182
I0522 01:54:55.069165 25646 solver.cpp:253]     Train net output #0: loss = 1.22182 (* 1 = 1.22182 loss)
I0522 01:54:55.069182 25646 sgd_solver.cpp:106] Iteration 27000, lr = 0.003
I0522 01:55:07.153602 25646 solver.cpp:237] Iteration 27750, loss = 1.27473
I0522 01:55:07.153764 25646 solver.cpp:253]     Train net output #0: loss = 1.27473 (* 1 = 1.27473 loss)
I0522 01:55:07.153782 25646 sgd_solver.cpp:106] Iteration 27750, lr = 0.003
I0522 01:55:19.271986 25646 solver.cpp:237] Iteration 28500, loss = 1.34853
I0522 01:55:19.272030 25646 solver.cpp:253]     Train net output #0: loss = 1.34853 (* 1 = 1.34853 loss)
I0522 01:55:19.272047 25646 sgd_solver.cpp:106] Iteration 28500, lr = 0.003
I0522 01:55:31.402487 25646 solver.cpp:237] Iteration 29250, loss = 1.23418
I0522 01:55:31.402542 25646 solver.cpp:253]     Train net output #0: loss = 1.23418 (* 1 = 1.23418 loss)
I0522 01:55:31.402559 25646 sgd_solver.cpp:106] Iteration 29250, lr = 0.003
I0522 01:55:43.566627 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_30000.caffemodel
I0522 01:55:43.617784 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_30000.solverstate
I0522 01:55:43.646347 25646 solver.cpp:341] Iteration 30000, Testing net (#0)
I0522 01:56:56.410192 25646 solver.cpp:409]     Test net output #0: accuracy = 0.852635
I0522 01:56:56.410351 25646 solver.cpp:409]     Test net output #1: loss = 0.563828 (* 1 = 0.563828 loss)
I0522 01:57:18.591205 25646 solver.cpp:237] Iteration 30000, loss = 1.50042
I0522 01:57:18.591269 25646 solver.cpp:253]     Train net output #0: loss = 1.50042 (* 1 = 1.50042 loss)
I0522 01:57:18.591297 25646 sgd_solver.cpp:106] Iteration 30000, lr = 0.003
I0522 01:57:30.698093 25646 solver.cpp:237] Iteration 30750, loss = 1.31078
I0522 01:57:30.698256 25646 solver.cpp:253]     Train net output #0: loss = 1.31078 (* 1 = 1.31078 loss)
I0522 01:57:30.698273 25646 sgd_solver.cpp:106] Iteration 30750, lr = 0.003
I0522 01:57:42.776767 25646 solver.cpp:237] Iteration 31500, loss = 0.894232
I0522 01:57:42.776821 25646 solver.cpp:253]     Train net output #0: loss = 0.894231 (* 1 = 0.894231 loss)
I0522 01:57:42.776839 25646 sgd_solver.cpp:106] Iteration 31500, lr = 0.003
I0522 01:57:54.854405 25646 solver.cpp:237] Iteration 32250, loss = 1.37473
I0522 01:57:54.854444 25646 solver.cpp:253]     Train net output #0: loss = 1.37473 (* 1 = 1.37473 loss)
I0522 01:57:54.854461 25646 sgd_solver.cpp:106] Iteration 32250, lr = 0.003
I0522 01:58:06.946374 25646 solver.cpp:237] Iteration 33000, loss = 1.49612
I0522 01:58:06.946534 25646 solver.cpp:253]     Train net output #0: loss = 1.49612 (* 1 = 1.49612 loss)
I0522 01:58:06.946552 25646 sgd_solver.cpp:106] Iteration 33000, lr = 0.003
I0522 01:58:19.098245 25646 solver.cpp:237] Iteration 33750, loss = 1.18002
I0522 01:58:19.098283 25646 solver.cpp:253]     Train net output #0: loss = 1.18002 (* 1 = 1.18002 loss)
I0522 01:58:19.098301 25646 sgd_solver.cpp:106] Iteration 33750, lr = 0.003
I0522 01:58:31.272402 25646 solver.cpp:237] Iteration 34500, loss = 0.810409
I0522 01:58:31.272456 25646 solver.cpp:253]     Train net output #0: loss = 0.810408 (* 1 = 0.810408 loss)
I0522 01:58:31.272474 25646 sgd_solver.cpp:106] Iteration 34500, lr = 0.003
I0522 01:59:05.670059 25646 solver.cpp:237] Iteration 35250, loss = 0.994586
I0522 01:59:05.670228 25646 solver.cpp:253]     Train net output #0: loss = 0.994586 (* 1 = 0.994586 loss)
I0522 01:59:05.670245 25646 sgd_solver.cpp:106] Iteration 35250, lr = 0.003
I0522 01:59:17.839510 25646 solver.cpp:237] Iteration 36000, loss = 1.46258
I0522 01:59:17.839562 25646 solver.cpp:253]     Train net output #0: loss = 1.46258 (* 1 = 1.46258 loss)
I0522 01:59:17.839579 25646 sgd_solver.cpp:106] Iteration 36000, lr = 0.003
I0522 01:59:30.017735 25646 solver.cpp:237] Iteration 36750, loss = 1.61921
I0522 01:59:30.017774 25646 solver.cpp:253]     Train net output #0: loss = 1.61921 (* 1 = 1.61921 loss)
I0522 01:59:30.017791 25646 sgd_solver.cpp:106] Iteration 36750, lr = 0.003
I0522 01:59:42.196050 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_37500.caffemodel
I0522 01:59:42.247153 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_37500.solverstate
I0522 01:59:42.280689 25646 solver.cpp:237] Iteration 37500, loss = 1.70425
I0522 01:59:42.280750 25646 solver.cpp:253]     Train net output #0: loss = 1.70425 (* 1 = 1.70425 loss)
I0522 01:59:42.280766 25646 sgd_solver.cpp:106] Iteration 37500, lr = 0.003
I0522 01:59:54.454078 25646 solver.cpp:237] Iteration 38250, loss = 1.02189
I0522 01:59:54.454118 25646 solver.cpp:253]     Train net output #0: loss = 1.02189 (* 1 = 1.02189 loss)
I0522 01:59:54.454135 25646 sgd_solver.cpp:106] Iteration 38250, lr = 0.003
I0522 02:00:06.636351 25646 solver.cpp:237] Iteration 39000, loss = 1.21714
I0522 02:00:06.636404 25646 solver.cpp:253]     Train net output #0: loss = 1.21714 (* 1 = 1.21714 loss)
I0522 02:00:06.636420 25646 sgd_solver.cpp:106] Iteration 39000, lr = 0.003
I0522 02:00:18.776635 25646 solver.cpp:237] Iteration 39750, loss = 1.16782
I0522 02:00:18.776787 25646 solver.cpp:253]     Train net output #0: loss = 1.16782 (* 1 = 1.16782 loss)
I0522 02:00:18.776804 25646 sgd_solver.cpp:106] Iteration 39750, lr = 0.003
I0522 02:00:53.110941 25646 solver.cpp:237] Iteration 40500, loss = 1.06724
I0522 02:00:53.111114 25646 solver.cpp:253]     Train net output #0: loss = 1.06724 (* 1 = 1.06724 loss)
I0522 02:00:53.111131 25646 sgd_solver.cpp:106] Iteration 40500, lr = 0.003
I0522 02:01:05.249146 25646 solver.cpp:237] Iteration 41250, loss = 1.43078
I0522 02:01:05.249187 25646 solver.cpp:253]     Train net output #0: loss = 1.43078 (* 1 = 1.43078 loss)
I0522 02:01:05.249205 25646 sgd_solver.cpp:106] Iteration 41250, lr = 0.003
I0522 02:01:17.393468 25646 solver.cpp:237] Iteration 42000, loss = 1.53802
I0522 02:01:17.393507 25646 solver.cpp:253]     Train net output #0: loss = 1.53802 (* 1 = 1.53802 loss)
I0522 02:01:17.393524 25646 sgd_solver.cpp:106] Iteration 42000, lr = 0.003
I0522 02:01:29.558434 25646 solver.cpp:237] Iteration 42750, loss = 1.28329
I0522 02:01:29.558586 25646 solver.cpp:253]     Train net output #0: loss = 1.28329 (* 1 = 1.28329 loss)
I0522 02:01:29.558604 25646 sgd_solver.cpp:106] Iteration 42750, lr = 0.003
I0522 02:01:41.741438 25646 solver.cpp:237] Iteration 43500, loss = 0.952423
I0522 02:01:41.741477 25646 solver.cpp:253]     Train net output #0: loss = 0.952422 (* 1 = 0.952422 loss)
I0522 02:01:41.741495 25646 sgd_solver.cpp:106] Iteration 43500, lr = 0.003
I0522 02:01:53.874500 25646 solver.cpp:237] Iteration 44250, loss = 1.40363
I0522 02:01:53.874547 25646 solver.cpp:253]     Train net output #0: loss = 1.40362 (* 1 = 1.40362 loss)
I0522 02:01:53.874565 25646 sgd_solver.cpp:106] Iteration 44250, lr = 0.003
I0522 02:02:05.968561 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_45000.caffemodel
I0522 02:02:06.018048 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_45000.solverstate
I0522 02:02:06.044703 25646 solver.cpp:341] Iteration 45000, Testing net (#0)
I0522 02:02:57.712788 25646 solver.cpp:409]     Test net output #0: accuracy = 0.872315
I0522 02:02:57.712950 25646 solver.cpp:409]     Test net output #1: loss = 0.417131 (* 1 = 0.417131 loss)
I0522 02:03:19.890525 25646 solver.cpp:237] Iteration 45000, loss = 0.868831
I0522 02:03:19.890584 25646 solver.cpp:253]     Train net output #0: loss = 0.868831 (* 1 = 0.868831 loss)
I0522 02:03:19.890605 25646 sgd_solver.cpp:106] Iteration 45000, lr = 0.003
I0522 02:03:32.069596 25646 solver.cpp:237] Iteration 45750, loss = 1.0946
I0522 02:03:32.069768 25646 solver.cpp:253]     Train net output #0: loss = 1.0946 (* 1 = 1.0946 loss)
I0522 02:03:32.069785 25646 sgd_solver.cpp:106] Iteration 45750, lr = 0.003
I0522 02:03:44.263883 25646 solver.cpp:237] Iteration 46500, loss = 1.4509
I0522 02:03:44.263921 25646 solver.cpp:253]     Train net output #0: loss = 1.4509 (* 1 = 1.4509 loss)
I0522 02:03:44.263941 25646 sgd_solver.cpp:106] Iteration 46500, lr = 0.003
I0522 02:03:56.404062 25646 solver.cpp:237] Iteration 47250, loss = 1.6121
I0522 02:03:56.404119 25646 solver.cpp:253]     Train net output #0: loss = 1.6121 (* 1 = 1.6121 loss)
I0522 02:03:56.404137 25646 sgd_solver.cpp:106] Iteration 47250, lr = 0.003
I0522 02:04:08.573232 25646 solver.cpp:237] Iteration 48000, loss = 1.17043
I0522 02:04:08.573379 25646 solver.cpp:253]     Train net output #0: loss = 1.17043 (* 1 = 1.17043 loss)
I0522 02:04:08.573395 25646 sgd_solver.cpp:106] Iteration 48000, lr = 0.003
I0522 02:04:20.751324 25646 solver.cpp:237] Iteration 48750, loss = 1.26837
I0522 02:04:20.751375 25646 solver.cpp:253]     Train net output #0: loss = 1.26837 (* 1 = 1.26837 loss)
I0522 02:04:20.751392 25646 sgd_solver.cpp:106] Iteration 48750, lr = 0.003
I0522 02:04:32.933228 25646 solver.cpp:237] Iteration 49500, loss = 1.0554
I0522 02:04:32.933266 25646 solver.cpp:253]     Train net output #0: loss = 1.0554 (* 1 = 1.0554 loss)
I0522 02:04:32.933284 25646 sgd_solver.cpp:106] Iteration 49500, lr = 0.003
I0522 02:05:07.259623 25646 solver.cpp:237] Iteration 50250, loss = 1.07237
I0522 02:05:07.259799 25646 solver.cpp:253]     Train net output #0: loss = 1.07237 (* 1 = 1.07237 loss)
I0522 02:05:07.259824 25646 sgd_solver.cpp:106] Iteration 50250, lr = 0.003
I0522 02:05:19.383224 25646 solver.cpp:237] Iteration 51000, loss = 1.08106
I0522 02:05:19.383262 25646 solver.cpp:253]     Train net output #0: loss = 1.08106 (* 1 = 1.08106 loss)
I0522 02:05:19.383280 25646 sgd_solver.cpp:106] Iteration 51000, lr = 0.003
I0522 02:05:31.529407 25646 solver.cpp:237] Iteration 51750, loss = 1.22666
I0522 02:05:31.529460 25646 solver.cpp:253]     Train net output #0: loss = 1.22666 (* 1 = 1.22666 loss)
I0522 02:05:31.529479 25646 sgd_solver.cpp:106] Iteration 51750, lr = 0.003
I0522 02:05:43.703855 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_52500.caffemodel
I0522 02:05:43.753521 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_52500.solverstate
I0522 02:05:43.785115 25646 solver.cpp:237] Iteration 52500, loss = 1.68088
I0522 02:05:43.785171 25646 solver.cpp:253]     Train net output #0: loss = 1.68088 (* 1 = 1.68088 loss)
I0522 02:05:43.785190 25646 sgd_solver.cpp:106] Iteration 52500, lr = 0.003
I0522 02:05:55.965762 25646 solver.cpp:237] Iteration 53250, loss = 1.22885
I0522 02:05:55.965816 25646 solver.cpp:253]     Train net output #0: loss = 1.22885 (* 1 = 1.22885 loss)
I0522 02:05:55.965832 25646 sgd_solver.cpp:106] Iteration 53250, lr = 0.003
I0522 02:06:08.083166 25646 solver.cpp:237] Iteration 54000, loss = 1.19559
I0522 02:06:08.083204 25646 solver.cpp:253]     Train net output #0: loss = 1.19559 (* 1 = 1.19559 loss)
I0522 02:06:08.083222 25646 sgd_solver.cpp:106] Iteration 54000, lr = 0.003
I0522 02:06:20.262015 25646 solver.cpp:237] Iteration 54750, loss = 0.727734
I0522 02:06:20.262178 25646 solver.cpp:253]     Train net output #0: loss = 0.727734 (* 1 = 0.727734 loss)
I0522 02:06:20.262197 25646 sgd_solver.cpp:106] Iteration 54750, lr = 0.003
I0522 02:06:54.601724 25646 solver.cpp:237] Iteration 55500, loss = 1.44349
I0522 02:06:54.601897 25646 solver.cpp:253]     Train net output #0: loss = 1.44349 (* 1 = 1.44349 loss)
I0522 02:06:54.601914 25646 sgd_solver.cpp:106] Iteration 55500, lr = 0.003
I0522 02:07:06.745746 25646 solver.cpp:237] Iteration 56250, loss = 1.0636
I0522 02:07:06.745784 25646 solver.cpp:253]     Train net output #0: loss = 1.0636 (* 1 = 1.0636 loss)
I0522 02:07:06.745802 25646 sgd_solver.cpp:106] Iteration 56250, lr = 0.003
I0522 02:07:18.974478 25646 solver.cpp:237] Iteration 57000, loss = 1.42348
I0522 02:07:18.974526 25646 solver.cpp:253]     Train net output #0: loss = 1.42348 (* 1 = 1.42348 loss)
I0522 02:07:18.974545 25646 sgd_solver.cpp:106] Iteration 57000, lr = 0.003
I0522 02:07:31.218544 25646 solver.cpp:237] Iteration 57750, loss = 1.3698
I0522 02:07:31.218688 25646 solver.cpp:253]     Train net output #0: loss = 1.3698 (* 1 = 1.3698 loss)
I0522 02:07:31.218704 25646 sgd_solver.cpp:106] Iteration 57750, lr = 0.003
I0522 02:07:43.438563 25646 solver.cpp:237] Iteration 58500, loss = 1.1839
I0522 02:07:43.438616 25646 solver.cpp:253]     Train net output #0: loss = 1.1839 (* 1 = 1.1839 loss)
I0522 02:07:43.438632 25646 sgd_solver.cpp:106] Iteration 58500, lr = 0.003
I0522 02:07:55.652771 25646 solver.cpp:237] Iteration 59250, loss = 1.36864
I0522 02:07:55.652808 25646 solver.cpp:253]     Train net output #0: loss = 1.36864 (* 1 = 1.36864 loss)
I0522 02:07:55.652827 25646 sgd_solver.cpp:106] Iteration 59250, lr = 0.003
I0522 02:08:07.823055 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_60000.caffemodel
I0522 02:08:07.872504 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_60000.solverstate
I0522 02:08:07.899216 25646 solver.cpp:341] Iteration 60000, Testing net (#0)
I0522 02:09:20.705809 25646 solver.cpp:409]     Test net output #0: accuracy = 0.871513
I0522 02:09:20.705983 25646 solver.cpp:409]     Test net output #1: loss = 0.400299 (* 1 = 0.400299 loss)
I0522 02:09:42.904168 25646 solver.cpp:237] Iteration 60000, loss = 1.84377
I0522 02:09:42.904232 25646 solver.cpp:253]     Train net output #0: loss = 1.84377 (* 1 = 1.84377 loss)
I0522 02:09:42.904253 25646 sgd_solver.cpp:106] Iteration 60000, lr = 0.003
I0522 02:09:55.065723 25646 solver.cpp:237] Iteration 60750, loss = 1.49825
I0522 02:09:55.065886 25646 solver.cpp:253]     Train net output #0: loss = 1.49825 (* 1 = 1.49825 loss)
I0522 02:09:55.065902 25646 sgd_solver.cpp:106] Iteration 60750, lr = 0.003
I0522 02:10:07.227701 25646 solver.cpp:237] Iteration 61500, loss = 0.868693
I0522 02:10:07.227752 25646 solver.cpp:253]     Train net output #0: loss = 0.868692 (* 1 = 0.868692 loss)
I0522 02:10:07.227771 25646 sgd_solver.cpp:106] Iteration 61500, lr = 0.003
I0522 02:10:19.409508 25646 solver.cpp:237] Iteration 62250, loss = 1.19479
I0522 02:10:19.409546 25646 solver.cpp:253]     Train net output #0: loss = 1.19479 (* 1 = 1.19479 loss)
I0522 02:10:19.409564 25646 sgd_solver.cpp:106] Iteration 62250, lr = 0.003
I0522 02:10:31.584813 25646 solver.cpp:237] Iteration 63000, loss = 1.18359
I0522 02:10:31.584976 25646 solver.cpp:253]     Train net output #0: loss = 1.18359 (* 1 = 1.18359 loss)
I0522 02:10:31.584993 25646 sgd_solver.cpp:106] Iteration 63000, lr = 0.003
I0522 02:10:43.714758 25646 solver.cpp:237] Iteration 63750, loss = 1.34952
I0522 02:10:43.714795 25646 solver.cpp:253]     Train net output #0: loss = 1.34952 (* 1 = 1.34952 loss)
I0522 02:10:43.714813 25646 sgd_solver.cpp:106] Iteration 63750, lr = 0.003
I0522 02:10:55.860770 25646 solver.cpp:237] Iteration 64500, loss = 1.2939
I0522 02:10:55.860822 25646 solver.cpp:253]     Train net output #0: loss = 1.2939 (* 1 = 1.2939 loss)
I0522 02:10:55.860839 25646 sgd_solver.cpp:106] Iteration 64500, lr = 0.003
I0522 02:11:30.168856 25646 solver.cpp:237] Iteration 65250, loss = 1.24522
I0522 02:11:30.169026 25646 solver.cpp:253]     Train net output #0: loss = 1.24522 (* 1 = 1.24522 loss)
I0522 02:11:30.169044 25646 sgd_solver.cpp:106] Iteration 65250, lr = 0.003
I0522 02:11:42.324923 25646 solver.cpp:237] Iteration 66000, loss = 1.33097
I0522 02:11:42.324959 25646 solver.cpp:253]     Train net output #0: loss = 1.33097 (* 1 = 1.33097 loss)
I0522 02:11:42.324978 25646 sgd_solver.cpp:106] Iteration 66000, lr = 0.003
I0522 02:11:54.479713 25646 solver.cpp:237] Iteration 66750, loss = 1.12225
I0522 02:11:54.479758 25646 solver.cpp:253]     Train net output #0: loss = 1.12225 (* 1 = 1.12225 loss)
I0522 02:11:54.479778 25646 sgd_solver.cpp:106] Iteration 66750, lr = 0.003
I0522 02:12:06.616514 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_67500.caffemodel
I0522 02:12:06.668090 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_67500.solverstate
I0522 02:12:06.701539 25646 solver.cpp:237] Iteration 67500, loss = 2.01689
I0522 02:12:06.701593 25646 solver.cpp:253]     Train net output #0: loss = 2.01689 (* 1 = 2.01689 loss)
I0522 02:12:06.701622 25646 sgd_solver.cpp:106] Iteration 67500, lr = 0.003
I0522 02:12:18.832355 25646 solver.cpp:237] Iteration 68250, loss = 1.00841
I0522 02:12:18.832404 25646 solver.cpp:253]     Train net output #0: loss = 1.00841 (* 1 = 1.00841 loss)
I0522 02:12:18.832422 25646 sgd_solver.cpp:106] Iteration 68250, lr = 0.003
I0522 02:12:30.948055 25646 solver.cpp:237] Iteration 69000, loss = 3.04524
I0522 02:12:30.948092 25646 solver.cpp:253]     Train net output #0: loss = 3.04524 (* 1 = 3.04524 loss)
I0522 02:12:30.948110 25646 sgd_solver.cpp:106] Iteration 69000, lr = 0.003
I0522 02:12:43.100137 25646 solver.cpp:237] Iteration 69750, loss = 1.13558
I0522 02:12:43.100313 25646 solver.cpp:253]     Train net output #0: loss = 1.13558 (* 1 = 1.13558 loss)
I0522 02:12:43.100332 25646 sgd_solver.cpp:106] Iteration 69750, lr = 0.003
I0522 02:13:17.433001 25646 solver.cpp:237] Iteration 70500, loss = 1.37914
I0522 02:13:17.433173 25646 solver.cpp:253]     Train net output #0: loss = 1.37914 (* 1 = 1.37914 loss)
I0522 02:13:17.433192 25646 sgd_solver.cpp:106] Iteration 70500, lr = 0.003
I0522 02:13:29.597810 25646 solver.cpp:237] Iteration 71250, loss = 1.22131
I0522 02:13:29.597861 25646 solver.cpp:253]     Train net output #0: loss = 1.22131 (* 1 = 1.22131 loss)
I0522 02:13:29.597877 25646 sgd_solver.cpp:106] Iteration 71250, lr = 0.003
I0522 02:13:41.769904 25646 solver.cpp:237] Iteration 72000, loss = 1.25446
I0522 02:13:41.769942 25646 solver.cpp:253]     Train net output #0: loss = 1.25446 (* 1 = 1.25446 loss)
I0522 02:13:41.769958 25646 sgd_solver.cpp:106] Iteration 72000, lr = 0.003
I0522 02:13:53.936141 25646 solver.cpp:237] Iteration 72750, loss = 1.18852
I0522 02:13:53.936303 25646 solver.cpp:253]     Train net output #0: loss = 1.18852 (* 1 = 1.18852 loss)
I0522 02:13:53.936321 25646 sgd_solver.cpp:106] Iteration 72750, lr = 0.003
I0522 02:14:06.094714 25646 solver.cpp:237] Iteration 73500, loss = 1.16412
I0522 02:14:06.094753 25646 solver.cpp:253]     Train net output #0: loss = 1.16412 (* 1 = 1.16412 loss)
I0522 02:14:06.094771 25646 sgd_solver.cpp:106] Iteration 73500, lr = 0.003
I0522 02:14:18.248522 25646 solver.cpp:237] Iteration 74250, loss = 1.43576
I0522 02:14:18.248575 25646 solver.cpp:253]     Train net output #0: loss = 1.43576 (* 1 = 1.43576 loss)
I0522 02:14:18.248594 25646 sgd_solver.cpp:106] Iteration 74250, lr = 0.003
I0522 02:14:30.387461 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_75000.caffemodel
I0522 02:14:30.442651 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_75000.solverstate
I0522 02:14:30.471359 25646 solver.cpp:341] Iteration 75000, Testing net (#0)
I0522 02:15:22.330467 25646 solver.cpp:409]     Test net output #0: accuracy = 0.877415
I0522 02:15:22.330632 25646 solver.cpp:409]     Test net output #1: loss = 0.397566 (* 1 = 0.397566 loss)
I0522 02:15:43.171687 25646 solver.cpp:237] Iteration 75000, loss = 1.78153
I0522 02:15:43.171747 25646 solver.cpp:253]     Train net output #0: loss = 1.78153 (* 1 = 1.78153 loss)
I0522 02:15:43.171766 25646 sgd_solver.cpp:106] Iteration 75000, lr = 0.003
I0522 02:15:55.351167 25646 solver.cpp:237] Iteration 75750, loss = 1.49217
I0522 02:15:55.351336 25646 solver.cpp:253]     Train net output #0: loss = 1.49218 (* 1 = 1.49218 loss)
I0522 02:15:55.351353 25646 sgd_solver.cpp:106] Iteration 75750, lr = 0.003
I0522 02:16:07.578776 25646 solver.cpp:237] Iteration 76500, loss = 1.38913
I0522 02:16:07.578814 25646 solver.cpp:253]     Train net output #0: loss = 1.38913 (* 1 = 1.38913 loss)
I0522 02:16:07.578832 25646 sgd_solver.cpp:106] Iteration 76500, lr = 0.003
I0522 02:16:19.769752 25646 solver.cpp:237] Iteration 77250, loss = 1.32692
I0522 02:16:19.769803 25646 solver.cpp:253]     Train net output #0: loss = 1.32692 (* 1 = 1.32692 loss)
I0522 02:16:19.769819 25646 sgd_solver.cpp:106] Iteration 77250, lr = 0.003
I0522 02:16:31.954614 25646 solver.cpp:237] Iteration 78000, loss = 1.18869
I0522 02:16:31.954766 25646 solver.cpp:253]     Train net output #0: loss = 1.18869 (* 1 = 1.18869 loss)
I0522 02:16:31.954782 25646 sgd_solver.cpp:106] Iteration 78000, lr = 0.003
I0522 02:16:44.135602 25646 solver.cpp:237] Iteration 78750, loss = 1.45352
I0522 02:16:44.135658 25646 solver.cpp:253]     Train net output #0: loss = 1.45352 (* 1 = 1.45352 loss)
I0522 02:16:44.135684 25646 sgd_solver.cpp:106] Iteration 78750, lr = 0.003
I0522 02:16:56.272253 25646 solver.cpp:237] Iteration 79500, loss = 1.48814
I0522 02:16:56.272290 25646 solver.cpp:253]     Train net output #0: loss = 1.48814 (* 1 = 1.48814 loss)
I0522 02:16:56.272310 25646 sgd_solver.cpp:106] Iteration 79500, lr = 0.003
I0522 02:17:29.232403 25646 solver.cpp:237] Iteration 80250, loss = 1.32726
I0522 02:17:29.232584 25646 solver.cpp:253]     Train net output #0: loss = 1.32726 (* 1 = 1.32726 loss)
I0522 02:17:29.232601 25646 sgd_solver.cpp:106] Iteration 80250, lr = 0.003
I0522 02:17:41.409943 25646 solver.cpp:237] Iteration 81000, loss = 0.896904
I0522 02:17:41.409996 25646 solver.cpp:253]     Train net output #0: loss = 0.896904 (* 1 = 0.896904 loss)
I0522 02:17:41.410014 25646 sgd_solver.cpp:106] Iteration 81000, lr = 0.003
I0522 02:17:53.594352 25646 solver.cpp:237] Iteration 81750, loss = 1.53729
I0522 02:17:53.594388 25646 solver.cpp:253]     Train net output #0: loss = 1.53729 (* 1 = 1.53729 loss)
I0522 02:17:53.594408 25646 sgd_solver.cpp:106] Iteration 81750, lr = 0.003
I0522 02:18:05.765650 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_82500.caffemodel
I0522 02:18:05.815606 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_82500.solverstate
I0522 02:18:05.847234 25646 solver.cpp:237] Iteration 82500, loss = 0.914372
I0522 02:18:05.847290 25646 solver.cpp:253]     Train net output #0: loss = 0.914372 (* 1 = 0.914372 loss)
I0522 02:18:05.847314 25646 sgd_solver.cpp:106] Iteration 82500, lr = 0.003
I0522 02:18:18.006525 25646 solver.cpp:237] Iteration 83250, loss = 1.52922
I0522 02:18:18.006562 25646 solver.cpp:253]     Train net output #0: loss = 1.52922 (* 1 = 1.52922 loss)
I0522 02:18:18.006582 25646 sgd_solver.cpp:106] Iteration 83250, lr = 0.003
I0522 02:18:30.164572 25646 solver.cpp:237] Iteration 84000, loss = 0.882617
I0522 02:18:30.164628 25646 solver.cpp:253]     Train net output #0: loss = 0.882618 (* 1 = 0.882618 loss)
I0522 02:18:30.164645 25646 sgd_solver.cpp:106] Iteration 84000, lr = 0.003
I0522 02:18:42.357702 25646 solver.cpp:237] Iteration 84750, loss = 0.887272
I0522 02:18:42.357862 25646 solver.cpp:253]     Train net output #0: loss = 0.887273 (* 1 = 0.887273 loss)
I0522 02:18:42.357879 25646 sgd_solver.cpp:106] Iteration 84750, lr = 0.003
I0522 02:19:15.367267 25646 solver.cpp:237] Iteration 85500, loss = 1.04071
I0522 02:19:15.367439 25646 solver.cpp:253]     Train net output #0: loss = 1.04071 (* 1 = 1.04071 loss)
I0522 02:19:15.367458 25646 sgd_solver.cpp:106] Iteration 85500, lr = 0.003
I0522 02:19:27.539773 25646 solver.cpp:237] Iteration 86250, loss = 1.06845
I0522 02:19:27.539811 25646 solver.cpp:253]     Train net output #0: loss = 1.06845 (* 1 = 1.06845 loss)
I0522 02:19:27.539829 25646 sgd_solver.cpp:106] Iteration 86250, lr = 0.003
I0522 02:19:39.708428 25646 solver.cpp:237] Iteration 87000, loss = 0.801942
I0522 02:19:39.708487 25646 solver.cpp:253]     Train net output #0: loss = 0.801943 (* 1 = 0.801943 loss)
I0522 02:19:39.708506 25646 sgd_solver.cpp:106] Iteration 87000, lr = 0.003
I0522 02:19:51.866189 25646 solver.cpp:237] Iteration 87750, loss = 1.00467
I0522 02:19:51.866343 25646 solver.cpp:253]     Train net output #0: loss = 1.00467 (* 1 = 1.00467 loss)
I0522 02:19:51.866359 25646 sgd_solver.cpp:106] Iteration 87750, lr = 0.003
I0522 02:20:04.027442 25646 solver.cpp:237] Iteration 88500, loss = 1.17199
I0522 02:20:04.027495 25646 solver.cpp:253]     Train net output #0: loss = 1.172 (* 1 = 1.172 loss)
I0522 02:20:04.027520 25646 sgd_solver.cpp:106] Iteration 88500, lr = 0.003
I0522 02:20:16.265581 25646 solver.cpp:237] Iteration 89250, loss = 1.24889
I0522 02:20:16.265619 25646 solver.cpp:253]     Train net output #0: loss = 1.24889 (* 1 = 1.24889 loss)
I0522 02:20:16.265637 25646 sgd_solver.cpp:106] Iteration 89250, lr = 0.003
I0522 02:20:28.468122 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_90000.caffemodel
I0522 02:20:28.517174 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_90000.solverstate
I0522 02:20:28.543583 25646 solver.cpp:341] Iteration 90000, Testing net (#0)
I0522 02:21:41.301398 25646 solver.cpp:409]     Test net output #0: accuracy = 0.887416
I0522 02:21:41.301566 25646 solver.cpp:409]     Test net output #1: loss = 0.363871 (* 1 = 0.363871 loss)
I0522 02:22:02.119204 25646 solver.cpp:237] Iteration 90000, loss = 1.36561
I0522 02:22:02.119263 25646 solver.cpp:253]     Train net output #0: loss = 1.36561 (* 1 = 1.36561 loss)
I0522 02:22:02.119284 25646 sgd_solver.cpp:106] Iteration 90000, lr = 0.003
I0522 02:22:14.278954 25646 solver.cpp:237] Iteration 90750, loss = 0.974629
I0522 02:22:14.279127 25646 solver.cpp:253]     Train net output #0: loss = 0.97463 (* 1 = 0.97463 loss)
I0522 02:22:14.279145 25646 sgd_solver.cpp:106] Iteration 90750, lr = 0.003
I0522 02:22:26.495673 25646 solver.cpp:237] Iteration 91500, loss = 1.13318
I0522 02:22:26.495712 25646 solver.cpp:253]     Train net output #0: loss = 1.13318 (* 1 = 1.13318 loss)
I0522 02:22:26.495728 25646 sgd_solver.cpp:106] Iteration 91500, lr = 0.003
I0522 02:22:38.676301 25646 solver.cpp:237] Iteration 92250, loss = 0.941942
I0522 02:22:38.676355 25646 solver.cpp:253]     Train net output #0: loss = 0.941943 (* 1 = 0.941943 loss)
I0522 02:22:38.676373 25646 sgd_solver.cpp:106] Iteration 92250, lr = 0.003
I0522 02:22:50.822573 25646 solver.cpp:237] Iteration 93000, loss = 0.893232
I0522 02:22:50.822728 25646 solver.cpp:253]     Train net output #0: loss = 0.893233 (* 1 = 0.893233 loss)
I0522 02:22:50.822746 25646 sgd_solver.cpp:106] Iteration 93000, lr = 0.003
I0522 02:23:02.966713 25646 solver.cpp:237] Iteration 93750, loss = 1.26853
I0522 02:23:02.966765 25646 solver.cpp:253]     Train net output #0: loss = 1.26853 (* 1 = 1.26853 loss)
I0522 02:23:02.966783 25646 sgd_solver.cpp:106] Iteration 93750, lr = 0.003
I0522 02:23:15.132375 25646 solver.cpp:237] Iteration 94500, loss = 1.57877
I0522 02:23:15.132411 25646 solver.cpp:253]     Train net output #0: loss = 1.57877 (* 1 = 1.57877 loss)
I0522 02:23:15.132429 25646 sgd_solver.cpp:106] Iteration 94500, lr = 0.003
I0522 02:23:48.141458 25646 solver.cpp:237] Iteration 95250, loss = 1.23535
I0522 02:23:48.141628 25646 solver.cpp:253]     Train net output #0: loss = 1.23535 (* 1 = 1.23535 loss)
I0522 02:23:48.141645 25646 sgd_solver.cpp:106] Iteration 95250, lr = 0.003
I0522 02:24:00.320745 25646 solver.cpp:237] Iteration 96000, loss = 0.640743
I0522 02:24:00.320787 25646 solver.cpp:253]     Train net output #0: loss = 0.640744 (* 1 = 0.640744 loss)
I0522 02:24:00.320806 25646 sgd_solver.cpp:106] Iteration 96000, lr = 0.003
I0522 02:24:12.488574 25646 solver.cpp:237] Iteration 96750, loss = 1.25163
I0522 02:24:12.488611 25646 solver.cpp:253]     Train net output #0: loss = 1.25163 (* 1 = 1.25163 loss)
I0522 02:24:12.488628 25646 sgd_solver.cpp:106] Iteration 96750, lr = 0.003
I0522 02:24:24.668351 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_97500.caffemodel
I0522 02:24:24.718207 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_97500.solverstate
I0522 02:24:24.749622 25646 solver.cpp:237] Iteration 97500, loss = 1.13209
I0522 02:24:24.749682 25646 solver.cpp:253]     Train net output #0: loss = 1.13209 (* 1 = 1.13209 loss)
I0522 02:24:24.749701 25646 sgd_solver.cpp:106] Iteration 97500, lr = 0.003
I0522 02:24:36.947109 25646 solver.cpp:237] Iteration 98250, loss = 1.13119
I0522 02:24:36.947147 25646 solver.cpp:253]     Train net output #0: loss = 1.13119 (* 1 = 1.13119 loss)
I0522 02:24:36.947166 25646 sgd_solver.cpp:106] Iteration 98250, lr = 0.003
I0522 02:24:49.131630 25646 solver.cpp:237] Iteration 99000, loss = 0.903924
I0522 02:24:49.131680 25646 solver.cpp:253]     Train net output #0: loss = 0.903925 (* 1 = 0.903925 loss)
I0522 02:24:49.131697 25646 sgd_solver.cpp:106] Iteration 99000, lr = 0.003
I0522 02:25:01.315770 25646 solver.cpp:237] Iteration 99750, loss = 1.31547
I0522 02:25:01.315948 25646 solver.cpp:253]     Train net output #0: loss = 1.31547 (* 1 = 1.31547 loss)
I0522 02:25:01.315964 25646 sgd_solver.cpp:106] Iteration 99750, lr = 0.003
I0522 02:25:34.273582 25646 solver.cpp:237] Iteration 100500, loss = 1.15245
I0522 02:25:34.273764 25646 solver.cpp:253]     Train net output #0: loss = 1.15245 (* 1 = 1.15245 loss)
I0522 02:25:34.273782 25646 sgd_solver.cpp:106] Iteration 100500, lr = 0.003
I0522 02:25:46.399106 25646 solver.cpp:237] Iteration 101250, loss = 1.69357
I0522 02:25:46.399143 25646 solver.cpp:253]     Train net output #0: loss = 1.69357 (* 1 = 1.69357 loss)
I0522 02:25:46.399161 25646 sgd_solver.cpp:106] Iteration 101250, lr = 0.003
I0522 02:25:58.549263 25646 solver.cpp:237] Iteration 102000, loss = 0.875127
I0522 02:25:58.549314 25646 solver.cpp:253]     Train net output #0: loss = 0.875129 (* 1 = 0.875129 loss)
I0522 02:25:58.549331 25646 sgd_solver.cpp:106] Iteration 102000, lr = 0.003
I0522 02:26:10.699326 25646 solver.cpp:237] Iteration 102750, loss = 0.658579
I0522 02:26:10.699481 25646 solver.cpp:253]     Train net output #0: loss = 0.65858 (* 1 = 0.65858 loss)
I0522 02:26:10.699498 25646 sgd_solver.cpp:106] Iteration 102750, lr = 0.003
I0522 02:26:22.867558 25646 solver.cpp:237] Iteration 103500, loss = 1.4198
I0522 02:26:22.867607 25646 solver.cpp:253]     Train net output #0: loss = 1.4198 (* 1 = 1.4198 loss)
I0522 02:26:22.867624 25646 sgd_solver.cpp:106] Iteration 103500, lr = 0.003
I0522 02:26:35.061211 25646 solver.cpp:237] Iteration 104250, loss = 1.38193
I0522 02:26:35.061247 25646 solver.cpp:253]     Train net output #0: loss = 1.38193 (* 1 = 1.38193 loss)
I0522 02:26:35.061266 25646 sgd_solver.cpp:106] Iteration 104250, lr = 0.003
I0522 02:26:47.249302 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_105000.caffemodel
I0522 02:26:47.298600 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_105000.solverstate
I0522 02:26:47.323899 25646 solver.cpp:341] Iteration 105000, Testing net (#0)
I0522 02:27:38.865044 25646 solver.cpp:409]     Test net output #0: accuracy = 0.880601
I0522 02:27:38.865211 25646 solver.cpp:409]     Test net output #1: loss = 0.389409 (* 1 = 0.389409 loss)
I0522 02:27:59.694753 25646 solver.cpp:237] Iteration 105000, loss = 1.1922
I0522 02:27:59.694816 25646 solver.cpp:253]     Train net output #0: loss = 1.1922 (* 1 = 1.1922 loss)
I0522 02:27:59.694835 25646 sgd_solver.cpp:106] Iteration 105000, lr = 0.003
I0522 02:28:11.895560 25646 solver.cpp:237] Iteration 105750, loss = 1.01607
I0522 02:28:11.895720 25646 solver.cpp:253]     Train net output #0: loss = 1.01607 (* 1 = 1.01607 loss)
I0522 02:28:11.895737 25646 sgd_solver.cpp:106] Iteration 105750, lr = 0.003
I0522 02:28:24.074300 25646 solver.cpp:237] Iteration 106500, loss = 1.09473
I0522 02:28:24.074355 25646 solver.cpp:253]     Train net output #0: loss = 1.09473 (* 1 = 1.09473 loss)
I0522 02:28:24.074374 25646 sgd_solver.cpp:106] Iteration 106500, lr = 0.003
I0522 02:28:36.260303 25646 solver.cpp:237] Iteration 107250, loss = 1.05235
I0522 02:28:36.260341 25646 solver.cpp:253]     Train net output #0: loss = 1.05235 (* 1 = 1.05235 loss)
I0522 02:28:36.260360 25646 sgd_solver.cpp:106] Iteration 107250, lr = 0.003
I0522 02:28:48.486549 25646 solver.cpp:237] Iteration 108000, loss = 1.27532
I0522 02:28:48.486719 25646 solver.cpp:253]     Train net output #0: loss = 1.27532 (* 1 = 1.27532 loss)
I0522 02:28:48.486738 25646 sgd_solver.cpp:106] Iteration 108000, lr = 0.003
I0522 02:29:00.746913 25646 solver.cpp:237] Iteration 108750, loss = 1.22818
I0522 02:29:00.746953 25646 solver.cpp:253]     Train net output #0: loss = 1.22818 (* 1 = 1.22818 loss)
I0522 02:29:00.746969 25646 sgd_solver.cpp:106] Iteration 108750, lr = 0.003
I0522 02:29:12.930279 25646 solver.cpp:237] Iteration 109500, loss = 0.851347
I0522 02:29:12.930317 25646 solver.cpp:253]     Train net output #0: loss = 0.851348 (* 1 = 0.851348 loss)
I0522 02:29:12.930335 25646 sgd_solver.cpp:106] Iteration 109500, lr = 0.003
I0522 02:29:45.920092 25646 solver.cpp:237] Iteration 110250, loss = 1.44775
I0522 02:29:45.920269 25646 solver.cpp:253]     Train net output #0: loss = 1.44775 (* 1 = 1.44775 loss)
I0522 02:29:45.920286 25646 sgd_solver.cpp:106] Iteration 110250, lr = 0.003
I0522 02:29:58.049872 25646 solver.cpp:237] Iteration 111000, loss = 1.44415
I0522 02:29:58.049911 25646 solver.cpp:253]     Train net output #0: loss = 1.44415 (* 1 = 1.44415 loss)
I0522 02:29:58.049929 25646 sgd_solver.cpp:106] Iteration 111000, lr = 0.003
I0522 02:30:10.194555 25646 solver.cpp:237] Iteration 111750, loss = 1.2593
I0522 02:30:10.194607 25646 solver.cpp:253]     Train net output #0: loss = 1.2593 (* 1 = 1.2593 loss)
I0522 02:30:10.194624 25646 sgd_solver.cpp:106] Iteration 111750, lr = 0.003
I0522 02:30:22.358392 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_112500.caffemodel
I0522 02:30:22.411284 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_112500.solverstate
I0522 02:30:22.443711 25646 solver.cpp:237] Iteration 112500, loss = 1.38286
I0522 02:30:22.443768 25646 solver.cpp:253]     Train net output #0: loss = 1.38286 (* 1 = 1.38286 loss)
I0522 02:30:22.443794 25646 sgd_solver.cpp:106] Iteration 112500, lr = 0.003
I0522 02:30:34.581318 25646 solver.cpp:237] Iteration 113250, loss = 1.23135
I0522 02:30:34.581372 25646 solver.cpp:253]     Train net output #0: loss = 1.23135 (* 1 = 1.23135 loss)
I0522 02:30:34.581389 25646 sgd_solver.cpp:106] Iteration 113250, lr = 0.003
I0522 02:30:46.678299 25646 solver.cpp:237] Iteration 114000, loss = 1.07806
I0522 02:30:46.678338 25646 solver.cpp:253]     Train net output #0: loss = 1.07807 (* 1 = 1.07807 loss)
I0522 02:30:46.678356 25646 sgd_solver.cpp:106] Iteration 114000, lr = 0.003
I0522 02:30:58.795678 25646 solver.cpp:237] Iteration 114750, loss = 0.842185
I0522 02:30:58.795850 25646 solver.cpp:253]     Train net output #0: loss = 0.842187 (* 1 = 0.842187 loss)
I0522 02:30:58.795868 25646 sgd_solver.cpp:106] Iteration 114750, lr = 0.003
I0522 02:31:31.740404 25646 solver.cpp:237] Iteration 115500, loss = 0.789647
I0522 02:31:31.740581 25646 solver.cpp:253]     Train net output #0: loss = 0.789648 (* 1 = 0.789648 loss)
I0522 02:31:31.740599 25646 sgd_solver.cpp:106] Iteration 115500, lr = 0.003
I0522 02:31:43.853480 25646 solver.cpp:237] Iteration 116250, loss = 0.844077
I0522 02:31:43.853533 25646 solver.cpp:253]     Train net output #0: loss = 0.844078 (* 1 = 0.844078 loss)
I0522 02:31:43.853564 25646 sgd_solver.cpp:106] Iteration 116250, lr = 0.003
I0522 02:31:55.986798 25646 solver.cpp:237] Iteration 117000, loss = 1.34628
I0522 02:31:55.986835 25646 solver.cpp:253]     Train net output #0: loss = 1.34628 (* 1 = 1.34628 loss)
I0522 02:31:55.986853 25646 sgd_solver.cpp:106] Iteration 117000, lr = 0.003
I0522 02:32:08.114262 25646 solver.cpp:237] Iteration 117750, loss = 0.761156
I0522 02:32:08.114418 25646 solver.cpp:253]     Train net output #0: loss = 0.761157 (* 1 = 0.761157 loss)
I0522 02:32:08.114434 25646 sgd_solver.cpp:106] Iteration 117750, lr = 0.003
I0522 02:32:20.224951 25646 solver.cpp:237] Iteration 118500, loss = 1.31429
I0522 02:32:20.224999 25646 solver.cpp:253]     Train net output #0: loss = 1.31429 (* 1 = 1.31429 loss)
I0522 02:32:20.225018 25646 sgd_solver.cpp:106] Iteration 118500, lr = 0.003
I0522 02:32:32.335551 25646 solver.cpp:237] Iteration 119250, loss = 1.09415
I0522 02:32:32.335590 25646 solver.cpp:253]     Train net output #0: loss = 1.09415 (* 1 = 1.09415 loss)
I0522 02:32:32.335608 25646 sgd_solver.cpp:106] Iteration 119250, lr = 0.003
I0522 02:32:44.483950 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_120000.caffemodel
I0522 02:32:44.532980 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_120000.solverstate
I0522 02:32:44.558503 25646 solver.cpp:341] Iteration 120000, Testing net (#0)
I0522 02:33:57.254812 25646 solver.cpp:409]     Test net output #0: accuracy = 0.889551
I0522 02:33:57.254982 25646 solver.cpp:409]     Test net output #1: loss = 0.358438 (* 1 = 0.358438 loss)
I0522 02:34:18.104212 25646 solver.cpp:237] Iteration 120000, loss = 0.865817
I0522 02:34:18.104272 25646 solver.cpp:253]     Train net output #0: loss = 0.865818 (* 1 = 0.865818 loss)
I0522 02:34:18.104292 25646 sgd_solver.cpp:106] Iteration 120000, lr = 0.003
I0522 02:34:30.194860 25646 solver.cpp:237] Iteration 120750, loss = 1.11937
I0522 02:34:30.195019 25646 solver.cpp:253]     Train net output #0: loss = 1.11937 (* 1 = 1.11937 loss)
I0522 02:34:30.195036 25646 sgd_solver.cpp:106] Iteration 120750, lr = 0.003
I0522 02:34:42.277701 25646 solver.cpp:237] Iteration 121500, loss = 1.23886
I0522 02:34:42.277755 25646 solver.cpp:253]     Train net output #0: loss = 1.23887 (* 1 = 1.23887 loss)
I0522 02:34:42.277772 25646 sgd_solver.cpp:106] Iteration 121500, lr = 0.003
I0522 02:34:54.425345 25646 solver.cpp:237] Iteration 122250, loss = 1.138
I0522 02:34:54.425384 25646 solver.cpp:253]     Train net output #0: loss = 1.138 (* 1 = 1.138 loss)
I0522 02:34:54.425401 25646 sgd_solver.cpp:106] Iteration 122250, lr = 0.003
I0522 02:35:06.591282 25646 solver.cpp:237] Iteration 123000, loss = 0.957277
I0522 02:35:06.591450 25646 solver.cpp:253]     Train net output #0: loss = 0.957279 (* 1 = 0.957279 loss)
I0522 02:35:06.591469 25646 sgd_solver.cpp:106] Iteration 123000, lr = 0.003
I0522 02:35:18.801748 25646 solver.cpp:237] Iteration 123750, loss = 1.24951
I0522 02:35:18.801785 25646 solver.cpp:253]     Train net output #0: loss = 1.24951 (* 1 = 1.24951 loss)
I0522 02:35:18.801803 25646 sgd_solver.cpp:106] Iteration 123750, lr = 0.003
I0522 02:35:30.993764 25646 solver.cpp:237] Iteration 124500, loss = 0.669171
I0522 02:35:30.993819 25646 solver.cpp:253]     Train net output #0: loss = 0.669173 (* 1 = 0.669173 loss)
I0522 02:35:30.993837 25646 sgd_solver.cpp:106] Iteration 124500, lr = 0.003
I0522 02:36:03.924736 25646 solver.cpp:237] Iteration 125250, loss = 1.39442
I0522 02:36:03.924911 25646 solver.cpp:253]     Train net output #0: loss = 1.39442 (* 1 = 1.39442 loss)
I0522 02:36:03.924928 25646 sgd_solver.cpp:106] Iteration 125250, lr = 0.003
I0522 02:36:16.105609 25646 solver.cpp:237] Iteration 126000, loss = 0.771387
I0522 02:36:16.105648 25646 solver.cpp:253]     Train net output #0: loss = 0.771389 (* 1 = 0.771389 loss)
I0522 02:36:16.105665 25646 sgd_solver.cpp:106] Iteration 126000, lr = 0.003
I0522 02:36:28.255919 25646 solver.cpp:237] Iteration 126750, loss = 1.61458
I0522 02:36:28.255972 25646 solver.cpp:253]     Train net output #0: loss = 1.61458 (* 1 = 1.61458 loss)
I0522 02:36:28.255990 25646 sgd_solver.cpp:106] Iteration 126750, lr = 0.003
I0522 02:36:40.392405 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_127500.caffemodel
I0522 02:36:40.442134 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_127500.solverstate
I0522 02:36:40.472360 25646 solver.cpp:237] Iteration 127500, loss = 1.45721
I0522 02:36:40.472414 25646 solver.cpp:253]     Train net output #0: loss = 1.45721 (* 1 = 1.45721 loss)
I0522 02:36:40.472440 25646 sgd_solver.cpp:106] Iteration 127500, lr = 0.003
I0522 02:36:52.679404 25646 solver.cpp:237] Iteration 128250, loss = 1.2989
I0522 02:36:52.679457 25646 solver.cpp:253]     Train net output #0: loss = 1.2989 (* 1 = 1.2989 loss)
I0522 02:36:52.679474 25646 sgd_solver.cpp:106] Iteration 128250, lr = 0.003
I0522 02:37:04.887796 25646 solver.cpp:237] Iteration 129000, loss = 1.06123
I0522 02:37:04.887833 25646 solver.cpp:253]     Train net output #0: loss = 1.06123 (* 1 = 1.06123 loss)
I0522 02:37:04.887851 25646 sgd_solver.cpp:106] Iteration 129000, lr = 0.003
I0522 02:37:17.071359 25646 solver.cpp:237] Iteration 129750, loss = 0.695247
I0522 02:37:17.071545 25646 solver.cpp:253]     Train net output #0: loss = 0.695248 (* 1 = 0.695248 loss)
I0522 02:37:17.071563 25646 sgd_solver.cpp:106] Iteration 129750, lr = 0.003
I0522 02:37:50.109968 25646 solver.cpp:237] Iteration 130500, loss = 0.83225
I0522 02:37:50.110139 25646 solver.cpp:253]     Train net output #0: loss = 0.832252 (* 1 = 0.832252 loss)
I0522 02:37:50.110157 25646 sgd_solver.cpp:106] Iteration 130500, lr = 0.003
I0522 02:38:02.290815 25646 solver.cpp:237] Iteration 131250, loss = 1.95927
I0522 02:38:02.290869 25646 solver.cpp:253]     Train net output #0: loss = 1.95927 (* 1 = 1.95927 loss)
I0522 02:38:02.290886 25646 sgd_solver.cpp:106] Iteration 131250, lr = 0.003
I0522 02:38:14.442288 25646 solver.cpp:237] Iteration 132000, loss = 1.4361
I0522 02:38:14.442324 25646 solver.cpp:253]     Train net output #0: loss = 1.4361 (* 1 = 1.4361 loss)
I0522 02:38:14.442343 25646 sgd_solver.cpp:106] Iteration 132000, lr = 0.003
I0522 02:38:26.619197 25646 solver.cpp:237] Iteration 132750, loss = 1.30009
I0522 02:38:26.619379 25646 solver.cpp:253]     Train net output #0: loss = 1.30009 (* 1 = 1.30009 loss)
I0522 02:38:26.619397 25646 sgd_solver.cpp:106] Iteration 132750, lr = 0.003
I0522 02:38:38.825919 25646 solver.cpp:237] Iteration 133500, loss = 1.042
I0522 02:38:38.825958 25646 solver.cpp:253]     Train net output #0: loss = 1.042 (* 1 = 1.042 loss)
I0522 02:38:38.825974 25646 sgd_solver.cpp:106] Iteration 133500, lr = 0.003
I0522 02:38:51.018491 25646 solver.cpp:237] Iteration 134250, loss = 1.09445
I0522 02:38:51.018544 25646 solver.cpp:253]     Train net output #0: loss = 1.09445 (* 1 = 1.09445 loss)
I0522 02:38:51.018563 25646 sgd_solver.cpp:106] Iteration 134250, lr = 0.003
I0522 02:39:03.186607 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_135000.caffemodel
I0522 02:39:03.235298 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_135000.solverstate
I0522 02:39:03.260357 25646 solver.cpp:341] Iteration 135000, Testing net (#0)
I0522 02:39:55.285747 25646 solver.cpp:409]     Test net output #0: accuracy = 0.891431
I0522 02:39:55.285918 25646 solver.cpp:409]     Test net output #1: loss = 0.366404 (* 1 = 0.366404 loss)
I0522 02:40:16.123594 25646 solver.cpp:237] Iteration 135000, loss = 1.20377
I0522 02:40:16.123654 25646 solver.cpp:253]     Train net output #0: loss = 1.20377 (* 1 = 1.20377 loss)
I0522 02:40:16.123675 25646 sgd_solver.cpp:106] Iteration 135000, lr = 0.003
I0522 02:40:28.252234 25646 solver.cpp:237] Iteration 135750, loss = 1.29739
I0522 02:40:28.273021 25646 solver.cpp:253]     Train net output #0: loss = 1.29739 (* 1 = 1.29739 loss)
I0522 02:40:28.273041 25646 sgd_solver.cpp:106] Iteration 135750, lr = 0.003
I0522 02:40:40.361259 25646 solver.cpp:237] Iteration 136500, loss = 0.816502
I0522 02:40:40.361299 25646 solver.cpp:253]     Train net output #0: loss = 0.816504 (* 1 = 0.816504 loss)
I0522 02:40:40.361316 25646 sgd_solver.cpp:106] Iteration 136500, lr = 0.003
I0522 02:40:52.471078 25646 solver.cpp:237] Iteration 137250, loss = 0.923713
I0522 02:40:52.471117 25646 solver.cpp:253]     Train net output #0: loss = 0.923715 (* 1 = 0.923715 loss)
I0522 02:40:52.471133 25646 sgd_solver.cpp:106] Iteration 137250, lr = 0.003
I0522 02:41:04.614280 25646 solver.cpp:237] Iteration 138000, loss = 0.901311
I0522 02:41:04.614470 25646 solver.cpp:253]     Train net output #0: loss = 0.901313 (* 1 = 0.901313 loss)
I0522 02:41:04.614487 25646 sgd_solver.cpp:106] Iteration 138000, lr = 0.003
I0522 02:41:16.731809 25646 solver.cpp:237] Iteration 138750, loss = 0.874337
I0522 02:41:16.731848 25646 solver.cpp:253]     Train net output #0: loss = 0.874339 (* 1 = 0.874339 loss)
I0522 02:41:16.731868 25646 sgd_solver.cpp:106] Iteration 138750, lr = 0.003
I0522 02:41:28.889781 25646 solver.cpp:237] Iteration 139500, loss = 0.92907
I0522 02:41:28.889835 25646 solver.cpp:253]     Train net output #0: loss = 0.929072 (* 1 = 0.929072 loss)
I0522 02:41:28.889853 25646 sgd_solver.cpp:106] Iteration 139500, lr = 0.003
I0522 02:42:01.919416 25646 solver.cpp:237] Iteration 140250, loss = 1.35086
I0522 02:42:01.919606 25646 solver.cpp:253]     Train net output #0: loss = 1.35086 (* 1 = 1.35086 loss)
I0522 02:42:01.919623 25646 sgd_solver.cpp:106] Iteration 140250, lr = 0.003
I0522 02:42:14.048604 25646 solver.cpp:237] Iteration 141000, loss = 1.18285
I0522 02:42:14.048655 25646 solver.cpp:253]     Train net output #0: loss = 1.18285 (* 1 = 1.18285 loss)
I0522 02:42:14.048673 25646 sgd_solver.cpp:106] Iteration 141000, lr = 0.003
I0522 02:42:26.180464 25646 solver.cpp:237] Iteration 141750, loss = 1.23876
I0522 02:42:26.180502 25646 solver.cpp:253]     Train net output #0: loss = 1.23876 (* 1 = 1.23876 loss)
I0522 02:42:26.180521 25646 sgd_solver.cpp:106] Iteration 141750, lr = 0.003
I0522 02:42:38.331121 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_142500.caffemodel
I0522 02:42:38.382987 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_142500.solverstate
I0522 02:42:38.416030 25646 solver.cpp:237] Iteration 142500, loss = 1.46089
I0522 02:42:38.416086 25646 solver.cpp:253]     Train net output #0: loss = 1.46089 (* 1 = 1.46089 loss)
I0522 02:42:38.416113 25646 sgd_solver.cpp:106] Iteration 142500, lr = 0.003
I0522 02:42:50.575172 25646 solver.cpp:237] Iteration 143250, loss = 1.3148
I0522 02:42:50.575211 25646 solver.cpp:253]     Train net output #0: loss = 1.3148 (* 1 = 1.3148 loss)
I0522 02:42:50.575229 25646 sgd_solver.cpp:106] Iteration 143250, lr = 0.003
I0522 02:43:02.777309 25646 solver.cpp:237] Iteration 144000, loss = 2.06503
I0522 02:43:02.777364 25646 solver.cpp:253]     Train net output #0: loss = 2.06504 (* 1 = 2.06504 loss)
I0522 02:43:02.777381 25646 sgd_solver.cpp:106] Iteration 144000, lr = 0.003
I0522 02:43:15.036936 25646 solver.cpp:237] Iteration 144750, loss = 0.918139
I0522 02:43:15.037102 25646 solver.cpp:253]     Train net output #0: loss = 0.918141 (* 1 = 0.918141 loss)
I0522 02:43:15.037120 25646 sgd_solver.cpp:106] Iteration 144750, lr = 0.003
I0522 02:43:48.125752 25646 solver.cpp:237] Iteration 145500, loss = 1.09084
I0522 02:43:48.125929 25646 solver.cpp:253]     Train net output #0: loss = 1.09084 (* 1 = 1.09084 loss)
I0522 02:43:48.125947 25646 sgd_solver.cpp:106] Iteration 145500, lr = 0.003
I0522 02:44:00.374586 25646 solver.cpp:237] Iteration 146250, loss = 0.710895
I0522 02:44:00.374639 25646 solver.cpp:253]     Train net output #0: loss = 0.710897 (* 1 = 0.710897 loss)
I0522 02:44:00.374656 25646 sgd_solver.cpp:106] Iteration 146250, lr = 0.003
I0522 02:44:12.624114 25646 solver.cpp:237] Iteration 147000, loss = 1.19252
I0522 02:44:12.624155 25646 solver.cpp:253]     Train net output #0: loss = 1.19252 (* 1 = 1.19252 loss)
I0522 02:44:12.624172 25646 sgd_solver.cpp:106] Iteration 147000, lr = 0.003
I0522 02:44:24.753173 25646 solver.cpp:237] Iteration 147750, loss = 1.38338
I0522 02:44:24.753355 25646 solver.cpp:253]     Train net output #0: loss = 1.38338 (* 1 = 1.38338 loss)
I0522 02:44:24.753371 25646 sgd_solver.cpp:106] Iteration 147750, lr = 0.003
I0522 02:44:36.855756 25646 solver.cpp:237] Iteration 148500, loss = 1.51791
I0522 02:44:36.855793 25646 solver.cpp:253]     Train net output #0: loss = 1.51791 (* 1 = 1.51791 loss)
I0522 02:44:36.855813 25646 sgd_solver.cpp:106] Iteration 148500, lr = 0.003
I0522 02:44:48.972265 25646 solver.cpp:237] Iteration 149250, loss = 2.03051
I0522 02:44:48.972319 25646 solver.cpp:253]     Train net output #0: loss = 2.03051 (* 1 = 2.03051 loss)
I0522 02:44:48.972337 25646 sgd_solver.cpp:106] Iteration 149250, lr = 0.003
I0522 02:45:01.084431 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_150000.caffemodel
I0522 02:45:01.135565 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_150000.solverstate
I0522 02:45:01.162606 25646 solver.cpp:341] Iteration 150000, Testing net (#0)
I0522 02:46:13.935201 25646 solver.cpp:409]     Test net output #0: accuracy = 0.893277
I0522 02:46:13.935386 25646 solver.cpp:409]     Test net output #1: loss = 0.357578 (* 1 = 0.357578 loss)
I0522 02:46:34.757298 25646 solver.cpp:237] Iteration 150000, loss = 0.841072
I0522 02:46:34.757361 25646 solver.cpp:253]     Train net output #0: loss = 0.841075 (* 1 = 0.841075 loss)
I0522 02:46:34.757390 25646 sgd_solver.cpp:106] Iteration 150000, lr = 0.003
I0522 02:46:46.950078 25646 solver.cpp:237] Iteration 150750, loss = 1.3886
I0522 02:46:46.950253 25646 solver.cpp:253]     Train net output #0: loss = 1.38861 (* 1 = 1.38861 loss)
I0522 02:46:46.950269 25646 sgd_solver.cpp:106] Iteration 150750, lr = 0.003
I0522 02:46:59.134768 25646 solver.cpp:237] Iteration 151500, loss = 1.2174
I0522 02:46:59.134805 25646 solver.cpp:253]     Train net output #0: loss = 1.2174 (* 1 = 1.2174 loss)
I0522 02:46:59.134824 25646 sgd_solver.cpp:106] Iteration 151500, lr = 0.003
I0522 02:47:11.307714 25646 solver.cpp:237] Iteration 152250, loss = 0.905415
I0522 02:47:11.307770 25646 solver.cpp:253]     Train net output #0: loss = 0.905417 (* 1 = 0.905417 loss)
I0522 02:47:11.307787 25646 sgd_solver.cpp:106] Iteration 152250, lr = 0.003
I0522 02:47:23.464092 25646 solver.cpp:237] Iteration 153000, loss = 0.804247
I0522 02:47:23.464252 25646 solver.cpp:253]     Train net output #0: loss = 0.804249 (* 1 = 0.804249 loss)
I0522 02:47:23.464269 25646 sgd_solver.cpp:106] Iteration 153000, lr = 0.003
I0522 02:47:35.649199 25646 solver.cpp:237] Iteration 153750, loss = 1.34268
I0522 02:47:35.649250 25646 solver.cpp:253]     Train net output #0: loss = 1.34268 (* 1 = 1.34268 loss)
I0522 02:47:35.649268 25646 sgd_solver.cpp:106] Iteration 153750, lr = 0.003
I0522 02:47:47.821831 25646 solver.cpp:237] Iteration 154500, loss = 1.70989
I0522 02:47:47.821869 25646 solver.cpp:253]     Train net output #0: loss = 1.70989 (* 1 = 1.70989 loss)
I0522 02:47:47.821887 25646 sgd_solver.cpp:106] Iteration 154500, lr = 0.003
I0522 02:48:20.904630 25646 solver.cpp:237] Iteration 155250, loss = 1.43842
I0522 02:48:20.904808 25646 solver.cpp:253]     Train net output #0: loss = 1.43842 (* 1 = 1.43842 loss)
I0522 02:48:20.904825 25646 sgd_solver.cpp:106] Iteration 155250, lr = 0.003
I0522 02:48:33.083784 25646 solver.cpp:237] Iteration 156000, loss = 1.08758
I0522 02:48:33.083838 25646 solver.cpp:253]     Train net output #0: loss = 1.08758 (* 1 = 1.08758 loss)
I0522 02:48:33.083855 25646 sgd_solver.cpp:106] Iteration 156000, lr = 0.003
I0522 02:48:45.256268 25646 solver.cpp:237] Iteration 156750, loss = 2.18786
I0522 02:48:45.256306 25646 solver.cpp:253]     Train net output #0: loss = 2.18787 (* 1 = 2.18787 loss)
I0522 02:48:45.256324 25646 sgd_solver.cpp:106] Iteration 156750, lr = 0.003
I0522 02:48:57.444785 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_157500.caffemodel
I0522 02:48:57.493818 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_157500.solverstate
I0522 02:48:57.524273 25646 solver.cpp:237] Iteration 157500, loss = 0.740414
I0522 02:48:57.524327 25646 solver.cpp:253]     Train net output #0: loss = 0.740417 (* 1 = 0.740417 loss)
I0522 02:48:57.524353 25646 sgd_solver.cpp:106] Iteration 157500, lr = 0.003
I0522 02:49:09.773763 25646 solver.cpp:237] Iteration 158250, loss = 1.19
I0522 02:49:09.773802 25646 solver.cpp:253]     Train net output #0: loss = 1.19 (* 1 = 1.19 loss)
I0522 02:49:09.773821 25646 sgd_solver.cpp:106] Iteration 158250, lr = 0.003
I0522 02:49:21.957165 25646 solver.cpp:237] Iteration 159000, loss = 0.82802
I0522 02:49:21.957216 25646 solver.cpp:253]     Train net output #0: loss = 0.828023 (* 1 = 0.828023 loss)
I0522 02:49:21.957234 25646 sgd_solver.cpp:106] Iteration 159000, lr = 0.003
I0522 02:49:34.138181 25646 solver.cpp:237] Iteration 159750, loss = 1.4398
I0522 02:49:34.138345 25646 solver.cpp:253]     Train net output #0: loss = 1.4398 (* 1 = 1.4398 loss)
I0522 02:49:34.138361 25646 sgd_solver.cpp:106] Iteration 159750, lr = 0.003
I0522 02:50:07.144572 25646 solver.cpp:237] Iteration 160500, loss = 1.45477
I0522 02:50:07.144748 25646 solver.cpp:253]     Train net output #0: loss = 1.45477 (* 1 = 1.45477 loss)
I0522 02:50:07.144765 25646 sgd_solver.cpp:106] Iteration 160500, lr = 0.003
I0522 02:50:19.261564 25646 solver.cpp:237] Iteration 161250, loss = 0.894817
I0522 02:50:19.261603 25646 solver.cpp:253]     Train net output #0: loss = 0.89482 (* 1 = 0.89482 loss)
I0522 02:50:19.261620 25646 sgd_solver.cpp:106] Iteration 161250, lr = 0.003
I0522 02:50:31.384191 25646 solver.cpp:237] Iteration 162000, loss = 0.752322
I0522 02:50:31.384239 25646 solver.cpp:253]     Train net output #0: loss = 0.752325 (* 1 = 0.752325 loss)
I0522 02:50:31.384256 25646 sgd_solver.cpp:106] Iteration 162000, lr = 0.003
I0522 02:50:43.504395 25646 solver.cpp:237] Iteration 162750, loss = 1.25477
I0522 02:50:43.504554 25646 solver.cpp:253]     Train net output #0: loss = 1.25477 (* 1 = 1.25477 loss)
I0522 02:50:43.504571 25646 sgd_solver.cpp:106] Iteration 162750, lr = 0.003
I0522 02:50:55.648540 25646 solver.cpp:237] Iteration 163500, loss = 1.35676
I0522 02:50:55.648592 25646 solver.cpp:253]     Train net output #0: loss = 1.35676 (* 1 = 1.35676 loss)
I0522 02:50:55.648610 25646 sgd_solver.cpp:106] Iteration 163500, lr = 0.003
I0522 02:51:07.784423 25646 solver.cpp:237] Iteration 164250, loss = 1.54703
I0522 02:51:07.784462 25646 solver.cpp:253]     Train net output #0: loss = 1.54703 (* 1 = 1.54703 loss)
I0522 02:51:07.784482 25646 sgd_solver.cpp:106] Iteration 164250, lr = 0.003
I0522 02:51:19.881279 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_165000.caffemodel
I0522 02:51:19.940374 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_165000.solverstate
I0522 02:51:19.965409 25646 solver.cpp:341] Iteration 165000, Testing net (#0)
I0522 02:52:11.563148 25646 solver.cpp:409]     Test net output #0: accuracy = 0.88987
I0522 02:52:11.563321 25646 solver.cpp:409]     Test net output #1: loss = 0.333125 (* 1 = 0.333125 loss)
I0522 02:52:32.398366 25646 solver.cpp:237] Iteration 165000, loss = 1.10517
I0522 02:52:32.398427 25646 solver.cpp:253]     Train net output #0: loss = 1.10518 (* 1 = 1.10518 loss)
I0522 02:52:32.398447 25646 sgd_solver.cpp:106] Iteration 165000, lr = 0.003
I0522 02:52:44.589771 25646 solver.cpp:237] Iteration 165750, loss = 0.83075
I0522 02:52:44.589958 25646 solver.cpp:253]     Train net output #0: loss = 0.830753 (* 1 = 0.830753 loss)
I0522 02:52:44.589977 25646 sgd_solver.cpp:106] Iteration 165750, lr = 0.003
I0522 02:52:56.767663 25646 solver.cpp:237] Iteration 166500, loss = 0.750678
I0522 02:52:56.767702 25646 solver.cpp:253]     Train net output #0: loss = 0.750681 (* 1 = 0.750681 loss)
I0522 02:52:56.767721 25646 sgd_solver.cpp:106] Iteration 166500, lr = 0.003
I0522 02:53:08.973196 25646 solver.cpp:237] Iteration 167250, loss = 1.12354
I0522 02:53:08.973248 25646 solver.cpp:253]     Train net output #0: loss = 1.12355 (* 1 = 1.12355 loss)
I0522 02:53:08.973266 25646 sgd_solver.cpp:106] Iteration 167250, lr = 0.003
I0522 02:53:21.177567 25646 solver.cpp:237] Iteration 168000, loss = 1.3801
I0522 02:53:21.177736 25646 solver.cpp:253]     Train net output #0: loss = 1.3801 (* 1 = 1.3801 loss)
I0522 02:53:21.177752 25646 sgd_solver.cpp:106] Iteration 168000, lr = 0.003
I0522 02:53:33.409390 25646 solver.cpp:237] Iteration 168750, loss = 1.50549
I0522 02:53:33.409443 25646 solver.cpp:253]     Train net output #0: loss = 1.50549 (* 1 = 1.50549 loss)
I0522 02:53:33.409461 25646 sgd_solver.cpp:106] Iteration 168750, lr = 0.003
I0522 02:53:45.623109 25646 solver.cpp:237] Iteration 169500, loss = 0.822122
I0522 02:53:45.623147 25646 solver.cpp:253]     Train net output #0: loss = 0.822125 (* 1 = 0.822125 loss)
I0522 02:53:45.623165 25646 sgd_solver.cpp:106] Iteration 169500, lr = 0.003
I0522 02:54:18.624261 25646 solver.cpp:237] Iteration 170250, loss = 1.34348
I0522 02:54:18.624439 25646 solver.cpp:253]     Train net output #0: loss = 1.34348 (* 1 = 1.34348 loss)
I0522 02:54:18.624456 25646 sgd_solver.cpp:106] Iteration 170250, lr = 0.003
I0522 02:54:30.738045 25646 solver.cpp:237] Iteration 171000, loss = 0.887064
I0522 02:54:30.738083 25646 solver.cpp:253]     Train net output #0: loss = 0.887067 (* 1 = 0.887067 loss)
I0522 02:54:30.738102 25646 sgd_solver.cpp:106] Iteration 171000, lr = 0.003
I0522 02:54:42.877835 25646 solver.cpp:237] Iteration 171750, loss = 1.03511
I0522 02:54:42.877889 25646 solver.cpp:253]     Train net output #0: loss = 1.03511 (* 1 = 1.03511 loss)
I0522 02:54:42.877907 25646 sgd_solver.cpp:106] Iteration 171750, lr = 0.003
I0522 02:54:55.020840 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_172500.caffemodel
I0522 02:54:55.070142 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_172500.solverstate
I0522 02:54:55.100219 25646 solver.cpp:237] Iteration 172500, loss = 1.42283
I0522 02:54:55.100270 25646 solver.cpp:253]     Train net output #0: loss = 1.42283 (* 1 = 1.42283 loss)
I0522 02:54:55.100298 25646 sgd_solver.cpp:106] Iteration 172500, lr = 0.003
I0522 02:55:07.276782 25646 solver.cpp:237] Iteration 173250, loss = 0.858394
I0522 02:55:07.276839 25646 solver.cpp:253]     Train net output #0: loss = 0.858397 (* 1 = 0.858397 loss)
I0522 02:55:07.276855 25646 sgd_solver.cpp:106] Iteration 173250, lr = 0.003
I0522 02:55:19.383030 25646 solver.cpp:237] Iteration 174000, loss = 0.970981
I0522 02:55:19.383069 25646 solver.cpp:253]     Train net output #0: loss = 0.970985 (* 1 = 0.970985 loss)
I0522 02:55:19.383087 25646 sgd_solver.cpp:106] Iteration 174000, lr = 0.003
I0522 02:55:31.508919 25646 solver.cpp:237] Iteration 174750, loss = 1.5301
I0522 02:55:31.509088 25646 solver.cpp:253]     Train net output #0: loss = 1.53011 (* 1 = 1.53011 loss)
I0522 02:55:31.509105 25646 sgd_solver.cpp:106] Iteration 174750, lr = 0.003
I0522 02:56:04.534402 25646 solver.cpp:237] Iteration 175500, loss = 1.05351
I0522 02:56:04.534579 25646 solver.cpp:253]     Train net output #0: loss = 1.05352 (* 1 = 1.05352 loss)
I0522 02:56:04.534596 25646 sgd_solver.cpp:106] Iteration 175500, lr = 0.003
I0522 02:56:16.657629 25646 solver.cpp:237] Iteration 176250, loss = 1.52496
I0522 02:56:16.657667 25646 solver.cpp:253]     Train net output #0: loss = 1.52496 (* 1 = 1.52496 loss)
I0522 02:56:16.657694 25646 sgd_solver.cpp:106] Iteration 176250, lr = 0.003
I0522 02:56:28.835572 25646 solver.cpp:237] Iteration 177000, loss = 0.870813
I0522 02:56:28.835626 25646 solver.cpp:253]     Train net output #0: loss = 0.870816 (* 1 = 0.870816 loss)
I0522 02:56:28.835644 25646 sgd_solver.cpp:106] Iteration 177000, lr = 0.003
I0522 02:56:41.011142 25646 solver.cpp:237] Iteration 177750, loss = 1.32964
I0522 02:56:41.011314 25646 solver.cpp:253]     Train net output #0: loss = 1.32964 (* 1 = 1.32964 loss)
I0522 02:56:41.011332 25646 sgd_solver.cpp:106] Iteration 177750, lr = 0.003
I0522 02:56:53.201267 25646 solver.cpp:237] Iteration 178500, loss = 1.3362
I0522 02:56:53.201320 25646 solver.cpp:253]     Train net output #0: loss = 1.3362 (* 1 = 1.3362 loss)
I0522 02:56:53.201338 25646 sgd_solver.cpp:106] Iteration 178500, lr = 0.003
I0522 02:57:05.384569 25646 solver.cpp:237] Iteration 179250, loss = 1.12233
I0522 02:57:05.384608 25646 solver.cpp:253]     Train net output #0: loss = 1.12234 (* 1 = 1.12234 loss)
I0522 02:57:05.384625 25646 sgd_solver.cpp:106] Iteration 179250, lr = 0.003
I0522 02:57:17.500721 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_180000.caffemodel
I0522 02:57:17.549962 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_180000.solverstate
I0522 02:57:17.575654 25646 solver.cpp:341] Iteration 180000, Testing net (#0)
I0522 02:58:30.289060 25646 solver.cpp:409]     Test net output #0: accuracy = 0.888735
I0522 02:58:30.289244 25646 solver.cpp:409]     Test net output #1: loss = 0.367034 (* 1 = 0.367034 loss)
I0522 02:58:51.117080 25646 solver.cpp:237] Iteration 180000, loss = 1.16545
I0522 02:58:51.117144 25646 solver.cpp:253]     Train net output #0: loss = 1.16545 (* 1 = 1.16545 loss)
I0522 02:58:51.117164 25646 sgd_solver.cpp:106] Iteration 180000, lr = 0.003
I0522 02:59:03.187173 25646 solver.cpp:237] Iteration 180750, loss = 1.025
I0522 02:59:03.187340 25646 solver.cpp:253]     Train net output #0: loss = 1.02501 (* 1 = 1.02501 loss)
I0522 02:59:03.187357 25646 sgd_solver.cpp:106] Iteration 180750, lr = 0.003
I0522 02:59:15.278930 25646 solver.cpp:237] Iteration 181500, loss = 0.836338
I0522 02:59:15.278983 25646 solver.cpp:253]     Train net output #0: loss = 0.836342 (* 1 = 0.836342 loss)
I0522 02:59:15.279009 25646 sgd_solver.cpp:106] Iteration 181500, lr = 0.003
I0522 02:59:27.497058 25646 solver.cpp:237] Iteration 182250, loss = 1.46999
I0522 02:59:27.497097 25646 solver.cpp:253]     Train net output #0: loss = 1.46999 (* 1 = 1.46999 loss)
I0522 02:59:27.497115 25646 sgd_solver.cpp:106] Iteration 182250, lr = 0.003
I0522 02:59:39.742050 25646 solver.cpp:237] Iteration 183000, loss = 1.5222
I0522 02:59:39.742226 25646 solver.cpp:253]     Train net output #0: loss = 1.5222 (* 1 = 1.5222 loss)
I0522 02:59:39.742244 25646 sgd_solver.cpp:106] Iteration 183000, lr = 0.003
I0522 02:59:51.922251 25646 solver.cpp:237] Iteration 183750, loss = 0.676809
I0522 02:59:51.922289 25646 solver.cpp:253]     Train net output #0: loss = 0.676813 (* 1 = 0.676813 loss)
I0522 02:59:51.922308 25646 sgd_solver.cpp:106] Iteration 183750, lr = 0.003
I0522 03:00:04.079128 25646 solver.cpp:237] Iteration 184500, loss = 1.23199
I0522 03:00:04.079186 25646 solver.cpp:253]     Train net output #0: loss = 1.232 (* 1 = 1.232 loss)
I0522 03:00:04.079205 25646 sgd_solver.cpp:106] Iteration 184500, lr = 0.003
I0522 03:00:37.061563 25646 solver.cpp:237] Iteration 185250, loss = 1.01655
I0522 03:00:37.061759 25646 solver.cpp:253]     Train net output #0: loss = 1.01655 (* 1 = 1.01655 loss)
I0522 03:00:37.061777 25646 sgd_solver.cpp:106] Iteration 185250, lr = 0.003
I0522 03:00:49.247321 25646 solver.cpp:237] Iteration 186000, loss = 1.20462
I0522 03:00:49.247359 25646 solver.cpp:253]     Train net output #0: loss = 1.20462 (* 1 = 1.20462 loss)
I0522 03:00:49.247378 25646 sgd_solver.cpp:106] Iteration 186000, lr = 0.003
I0522 03:01:01.385092 25646 solver.cpp:237] Iteration 186750, loss = 1.21343
I0522 03:01:01.385144 25646 solver.cpp:253]     Train net output #0: loss = 1.21344 (* 1 = 1.21344 loss)
I0522 03:01:01.385162 25646 sgd_solver.cpp:106] Iteration 186750, lr = 0.003
I0522 03:01:13.545826 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_187500.caffemodel
I0522 03:01:13.597105 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_187500.solverstate
I0522 03:01:13.629207 25646 solver.cpp:237] Iteration 187500, loss = 1.08028
I0522 03:01:13.629263 25646 solver.cpp:253]     Train net output #0: loss = 1.08028 (* 1 = 1.08028 loss)
I0522 03:01:13.629292 25646 sgd_solver.cpp:106] Iteration 187500, lr = 0.003
I0522 03:01:25.806373 25646 solver.cpp:237] Iteration 188250, loss = 1.11818
I0522 03:01:25.806427 25646 solver.cpp:253]     Train net output #0: loss = 1.11819 (* 1 = 1.11819 loss)
I0522 03:01:25.806444 25646 sgd_solver.cpp:106] Iteration 188250, lr = 0.003
I0522 03:01:38.012097 25646 solver.cpp:237] Iteration 189000, loss = 1.00249
I0522 03:01:38.012135 25646 solver.cpp:253]     Train net output #0: loss = 1.00249 (* 1 = 1.00249 loss)
I0522 03:01:38.012158 25646 sgd_solver.cpp:106] Iteration 189000, lr = 0.003
I0522 03:01:50.170698 25646 solver.cpp:237] Iteration 189750, loss = 1.29949
I0522 03:01:50.170878 25646 solver.cpp:253]     Train net output #0: loss = 1.29949 (* 1 = 1.29949 loss)
I0522 03:01:50.170895 25646 sgd_solver.cpp:106] Iteration 189750, lr = 0.003
I0522 03:02:23.158640 25646 solver.cpp:237] Iteration 190500, loss = 1.03932
I0522 03:02:23.158818 25646 solver.cpp:253]     Train net output #0: loss = 1.03933 (* 1 = 1.03933 loss)
I0522 03:02:23.158836 25646 sgd_solver.cpp:106] Iteration 190500, lr = 0.003
I0522 03:02:35.235563 25646 solver.cpp:237] Iteration 191250, loss = 1.22001
I0522 03:02:35.235616 25646 solver.cpp:253]     Train net output #0: loss = 1.22001 (* 1 = 1.22001 loss)
I0522 03:02:35.235635 25646 sgd_solver.cpp:106] Iteration 191250, lr = 0.003
I0522 03:02:47.338662 25646 solver.cpp:237] Iteration 192000, loss = 1.47891
I0522 03:02:47.338701 25646 solver.cpp:253]     Train net output #0: loss = 1.47892 (* 1 = 1.47892 loss)
I0522 03:02:47.338719 25646 sgd_solver.cpp:106] Iteration 192000, lr = 0.003
I0522 03:02:59.471637 25646 solver.cpp:237] Iteration 192750, loss = 1.14387
I0522 03:02:59.471814 25646 solver.cpp:253]     Train net output #0: loss = 1.14387 (* 1 = 1.14387 loss)
I0522 03:02:59.471832 25646 sgd_solver.cpp:106] Iteration 192750, lr = 0.003
I0522 03:03:11.606145 25646 solver.cpp:237] Iteration 193500, loss = 1.3798
I0522 03:03:11.606184 25646 solver.cpp:253]     Train net output #0: loss = 1.37981 (* 1 = 1.37981 loss)
I0522 03:03:11.606202 25646 sgd_solver.cpp:106] Iteration 193500, lr = 0.003
I0522 03:03:23.728592 25646 solver.cpp:237] Iteration 194250, loss = 1.02413
I0522 03:03:23.728646 25646 solver.cpp:253]     Train net output #0: loss = 1.02413 (* 1 = 1.02413 loss)
I0522 03:03:23.728672 25646 sgd_solver.cpp:106] Iteration 194250, lr = 0.003
I0522 03:03:35.874245 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_195000.caffemodel
I0522 03:03:35.923550 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_195000.solverstate
I0522 03:03:35.948633 25646 solver.cpp:341] Iteration 195000, Testing net (#0)
I0522 03:04:27.889662 25646 solver.cpp:409]     Test net output #0: accuracy = 0.896525
I0522 03:04:27.889856 25646 solver.cpp:409]     Test net output #1: loss = 0.329316 (* 1 = 0.329316 loss)
I0522 03:04:48.708566 25646 solver.cpp:237] Iteration 195000, loss = 0.928651
I0522 03:04:48.708629 25646 solver.cpp:253]     Train net output #0: loss = 0.928654 (* 1 = 0.928654 loss)
I0522 03:04:48.708659 25646 sgd_solver.cpp:106] Iteration 195000, lr = 0.003
I0522 03:05:00.863230 25646 solver.cpp:237] Iteration 195750, loss = 0.832173
I0522 03:05:00.863400 25646 solver.cpp:253]     Train net output #0: loss = 0.832176 (* 1 = 0.832176 loss)
I0522 03:05:00.863417 25646 sgd_solver.cpp:106] Iteration 195750, lr = 0.003
I0522 03:05:13.083271 25646 solver.cpp:237] Iteration 196500, loss = 1.33494
I0522 03:05:13.083324 25646 solver.cpp:253]     Train net output #0: loss = 1.33494 (* 1 = 1.33494 loss)
I0522 03:05:13.083341 25646 sgd_solver.cpp:106] Iteration 196500, lr = 0.003
I0522 03:05:25.331735 25646 solver.cpp:237] Iteration 197250, loss = 0.989243
I0522 03:05:25.331776 25646 solver.cpp:253]     Train net output #0: loss = 0.989246 (* 1 = 0.989246 loss)
I0522 03:05:25.331794 25646 sgd_solver.cpp:106] Iteration 197250, lr = 0.003
I0522 03:05:37.501435 25646 solver.cpp:237] Iteration 198000, loss = 1.05485
I0522 03:05:37.501616 25646 solver.cpp:253]     Train net output #0: loss = 1.05485 (* 1 = 1.05485 loss)
I0522 03:05:37.501632 25646 sgd_solver.cpp:106] Iteration 198000, lr = 0.003
I0522 03:05:49.645014 25646 solver.cpp:237] Iteration 198750, loss = 1.46145
I0522 03:05:49.645051 25646 solver.cpp:253]     Train net output #0: loss = 1.46145 (* 1 = 1.46145 loss)
I0522 03:05:49.645068 25646 sgd_solver.cpp:106] Iteration 198750, lr = 0.003
I0522 03:06:01.785526 25646 solver.cpp:237] Iteration 199500, loss = 1.21519
I0522 03:06:01.785580 25646 solver.cpp:253]     Train net output #0: loss = 1.2152 (* 1 = 1.2152 loss)
I0522 03:06:01.785598 25646 sgd_solver.cpp:106] Iteration 199500, lr = 0.003
I0522 03:06:34.764356 25646 solver.cpp:237] Iteration 200250, loss = 1.09802
I0522 03:06:34.764538 25646 solver.cpp:253]     Train net output #0: loss = 1.09802 (* 1 = 1.09802 loss)
I0522 03:06:34.764555 25646 sgd_solver.cpp:106] Iteration 200250, lr = 0.003
I0522 03:06:46.919519 25646 solver.cpp:237] Iteration 201000, loss = 0.831972
I0522 03:06:46.919574 25646 solver.cpp:253]     Train net output #0: loss = 0.831975 (* 1 = 0.831975 loss)
I0522 03:06:46.919591 25646 sgd_solver.cpp:106] Iteration 201000, lr = 0.003
I0522 03:06:59.076699 25646 solver.cpp:237] Iteration 201750, loss = 1.40218
I0522 03:06:59.076736 25646 solver.cpp:253]     Train net output #0: loss = 1.40218 (* 1 = 1.40218 loss)
I0522 03:06:59.076755 25646 sgd_solver.cpp:106] Iteration 201750, lr = 0.003
I0522 03:07:11.198554 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_202500.caffemodel
I0522 03:07:11.247963 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_202500.solverstate
I0522 03:07:11.278193 25646 solver.cpp:237] Iteration 202500, loss = 1.15563
I0522 03:07:11.278247 25646 solver.cpp:253]     Train net output #0: loss = 1.15564 (* 1 = 1.15564 loss)
I0522 03:07:11.278265 25646 sgd_solver.cpp:106] Iteration 202500, lr = 0.003
I0522 03:07:23.429155 25646 solver.cpp:237] Iteration 203250, loss = 1.1504
I0522 03:07:23.429194 25646 solver.cpp:253]     Train net output #0: loss = 1.15041 (* 1 = 1.15041 loss)
I0522 03:07:23.429211 25646 sgd_solver.cpp:106] Iteration 203250, lr = 0.003
I0522 03:07:35.608013 25646 solver.cpp:237] Iteration 204000, loss = 1.30134
I0522 03:07:35.608064 25646 solver.cpp:253]     Train net output #0: loss = 1.30135 (* 1 = 1.30135 loss)
I0522 03:07:35.608081 25646 sgd_solver.cpp:106] Iteration 204000, lr = 0.003
I0522 03:07:47.782035 25646 solver.cpp:237] Iteration 204750, loss = 0.823082
I0522 03:07:47.782217 25646 solver.cpp:253]     Train net output #0: loss = 0.823085 (* 1 = 0.823085 loss)
I0522 03:07:47.782233 25646 sgd_solver.cpp:106] Iteration 204750, lr = 0.003
I0522 03:08:20.788118 25646 solver.cpp:237] Iteration 205500, loss = 1.11643
I0522 03:08:20.788303 25646 solver.cpp:253]     Train net output #0: loss = 1.11644 (* 1 = 1.11644 loss)
I0522 03:08:20.788321 25646 sgd_solver.cpp:106] Iteration 205500, lr = 0.003
I0522 03:08:32.946162 25646 solver.cpp:237] Iteration 206250, loss = 1.45382
I0522 03:08:32.946213 25646 solver.cpp:253]     Train net output #0: loss = 1.45382 (* 1 = 1.45382 loss)
I0522 03:08:32.946230 25646 sgd_solver.cpp:106] Iteration 206250, lr = 0.003
I0522 03:08:45.101438 25646 solver.cpp:237] Iteration 207000, loss = 1.4942
I0522 03:08:45.101475 25646 solver.cpp:253]     Train net output #0: loss = 1.4942 (* 1 = 1.4942 loss)
I0522 03:08:45.101491 25646 sgd_solver.cpp:106] Iteration 207000, lr = 0.003
I0522 03:08:57.267277 25646 solver.cpp:237] Iteration 207750, loss = 0.950306
I0522 03:08:57.267454 25646 solver.cpp:253]     Train net output #0: loss = 0.950308 (* 1 = 0.950308 loss)
I0522 03:08:57.267472 25646 sgd_solver.cpp:106] Iteration 207750, lr = 0.003
I0522 03:09:09.372536 25646 solver.cpp:237] Iteration 208500, loss = 1.37689
I0522 03:09:09.372575 25646 solver.cpp:253]     Train net output #0: loss = 1.37689 (* 1 = 1.37689 loss)
I0522 03:09:09.372592 25646 sgd_solver.cpp:106] Iteration 208500, lr = 0.003
I0522 03:09:21.592397 25646 solver.cpp:237] Iteration 209250, loss = 1.35358
I0522 03:09:21.592452 25646 solver.cpp:253]     Train net output #0: loss = 1.35358 (* 1 = 1.35358 loss)
I0522 03:09:21.592468 25646 sgd_solver.cpp:106] Iteration 209250, lr = 0.003
I0522 03:09:33.765252 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_210000.caffemodel
I0522 03:09:33.814626 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_210000.solverstate
I0522 03:09:33.839843 25646 solver.cpp:341] Iteration 210000, Testing net (#0)
I0522 03:10:46.642709 25646 solver.cpp:409]     Test net output #0: accuracy = 0.889464
I0522 03:10:46.642887 25646 solver.cpp:409]     Test net output #1: loss = 0.345267 (* 1 = 0.345267 loss)
I0522 03:11:07.461774 25646 solver.cpp:237] Iteration 210000, loss = 1.58188
I0522 03:11:07.461835 25646 solver.cpp:253]     Train net output #0: loss = 1.58188 (* 1 = 1.58188 loss)
I0522 03:11:07.461863 25646 sgd_solver.cpp:106] Iteration 210000, lr = 0.003
I0522 03:11:19.662173 25646 solver.cpp:237] Iteration 210750, loss = 0.800228
I0522 03:11:19.662354 25646 solver.cpp:253]     Train net output #0: loss = 0.800231 (* 1 = 0.800231 loss)
I0522 03:11:19.662372 25646 sgd_solver.cpp:106] Iteration 210750, lr = 0.003
I0522 03:11:31.843005 25646 solver.cpp:237] Iteration 211500, loss = 0.86211
I0522 03:11:31.843044 25646 solver.cpp:253]     Train net output #0: loss = 0.862112 (* 1 = 0.862112 loss)
I0522 03:11:31.843061 25646 sgd_solver.cpp:106] Iteration 211500, lr = 0.003
I0522 03:11:44.001469 25646 solver.cpp:237] Iteration 212250, loss = 0.725425
I0522 03:11:44.001526 25646 solver.cpp:253]     Train net output #0: loss = 0.725428 (* 1 = 0.725428 loss)
I0522 03:11:44.001552 25646 sgd_solver.cpp:106] Iteration 212250, lr = 0.003
I0522 03:11:56.166359 25646 solver.cpp:237] Iteration 213000, loss = 1.13446
I0522 03:11:56.166522 25646 solver.cpp:253]     Train net output #0: loss = 1.13447 (* 1 = 1.13447 loss)
I0522 03:11:56.166538 25646 sgd_solver.cpp:106] Iteration 213000, lr = 0.003
I0522 03:12:08.353945 25646 solver.cpp:237] Iteration 213750, loss = 1.12053
I0522 03:12:08.353996 25646 solver.cpp:253]     Train net output #0: loss = 1.12053 (* 1 = 1.12053 loss)
I0522 03:12:08.354012 25646 sgd_solver.cpp:106] Iteration 213750, lr = 0.003
I0522 03:12:20.518625 25646 solver.cpp:237] Iteration 214500, loss = 1.0786
I0522 03:12:20.518667 25646 solver.cpp:253]     Train net output #0: loss = 1.07861 (* 1 = 1.07861 loss)
I0522 03:12:20.518684 25646 sgd_solver.cpp:106] Iteration 214500, lr = 0.003
I0522 03:12:53.513417 25646 solver.cpp:237] Iteration 215250, loss = 1.2646
I0522 03:12:53.513610 25646 solver.cpp:253]     Train net output #0: loss = 1.26461 (* 1 = 1.26461 loss)
I0522 03:12:53.513628 25646 sgd_solver.cpp:106] Iteration 215250, lr = 0.003
I0522 03:13:05.665242 25646 solver.cpp:237] Iteration 216000, loss = 1.50865
I0522 03:13:05.665297 25646 solver.cpp:253]     Train net output #0: loss = 1.50865 (* 1 = 1.50865 loss)
I0522 03:13:05.665314 25646 sgd_solver.cpp:106] Iteration 216000, lr = 0.003
I0522 03:13:17.802443 25646 solver.cpp:237] Iteration 216750, loss = 1.14619
I0522 03:13:17.802481 25646 solver.cpp:253]     Train net output #0: loss = 1.1462 (* 1 = 1.1462 loss)
I0522 03:13:17.802500 25646 sgd_solver.cpp:106] Iteration 216750, lr = 0.003
I0522 03:13:29.937608 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_217500.caffemodel
I0522 03:13:29.989238 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_217500.solverstate
I0522 03:13:30.021986 25646 solver.cpp:237] Iteration 217500, loss = 2.01018
I0522 03:13:30.022040 25646 solver.cpp:253]     Train net output #0: loss = 2.01018 (* 1 = 2.01018 loss)
I0522 03:13:30.022061 25646 sgd_solver.cpp:106] Iteration 217500, lr = 0.003
I0522 03:13:42.191380 25646 solver.cpp:237] Iteration 218250, loss = 0.915302
I0522 03:13:42.191419 25646 solver.cpp:253]     Train net output #0: loss = 0.915305 (* 1 = 0.915305 loss)
I0522 03:13:42.191437 25646 sgd_solver.cpp:106] Iteration 218250, lr = 0.003
I0522 03:13:54.345907 25646 solver.cpp:237] Iteration 219000, loss = 1.22279
I0522 03:13:54.345960 25646 solver.cpp:253]     Train net output #0: loss = 1.2228 (* 1 = 1.2228 loss)
I0522 03:13:54.345978 25646 sgd_solver.cpp:106] Iteration 219000, lr = 0.003
I0522 03:14:06.470832 25646 solver.cpp:237] Iteration 219750, loss = 0.936873
I0522 03:14:06.471004 25646 solver.cpp:253]     Train net output #0: loss = 0.936876 (* 1 = 0.936876 loss)
I0522 03:14:06.471021 25646 sgd_solver.cpp:106] Iteration 219750, lr = 0.003
I0522 03:14:39.420569 25646 solver.cpp:237] Iteration 220500, loss = 0.93499
I0522 03:14:39.420755 25646 solver.cpp:253]     Train net output #0: loss = 0.934993 (* 1 = 0.934993 loss)
I0522 03:14:39.420775 25646 sgd_solver.cpp:106] Iteration 220500, lr = 0.003
I0522 03:14:51.489251 25646 solver.cpp:237] Iteration 221250, loss = 1.55677
I0522 03:14:51.489290 25646 solver.cpp:253]     Train net output #0: loss = 1.55677 (* 1 = 1.55677 loss)
I0522 03:14:51.489307 25646 sgd_solver.cpp:106] Iteration 221250, lr = 0.003
I0522 03:15:03.633716 25646 solver.cpp:237] Iteration 222000, loss = 1.03045
I0522 03:15:03.633771 25646 solver.cpp:253]     Train net output #0: loss = 1.03045 (* 1 = 1.03045 loss)
I0522 03:15:03.633797 25646 sgd_solver.cpp:106] Iteration 222000, lr = 0.003
I0522 03:15:15.829814 25646 solver.cpp:237] Iteration 222750, loss = 1.36878
I0522 03:15:15.829982 25646 solver.cpp:253]     Train net output #0: loss = 1.36878 (* 1 = 1.36878 loss)
I0522 03:15:15.829998 25646 sgd_solver.cpp:106] Iteration 222750, lr = 0.003
I0522 03:15:28.027222 25646 solver.cpp:237] Iteration 223500, loss = 1.43333
I0522 03:15:28.027261 25646 solver.cpp:253]     Train net output #0: loss = 1.43333 (* 1 = 1.43333 loss)
I0522 03:15:28.027278 25646 sgd_solver.cpp:106] Iteration 223500, lr = 0.003
I0522 03:15:40.207121 25646 solver.cpp:237] Iteration 224250, loss = 1.64185
I0522 03:15:40.207172 25646 solver.cpp:253]     Train net output #0: loss = 1.64185 (* 1 = 1.64185 loss)
I0522 03:15:40.207190 25646 sgd_solver.cpp:106] Iteration 224250, lr = 0.003
I0522 03:15:52.382473 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_225000.caffemodel
I0522 03:15:52.433513 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_225000.solverstate
I0522 03:15:52.460860 25646 solver.cpp:341] Iteration 225000, Testing net (#0)
I0522 03:16:44.138416 25646 solver.cpp:409]     Test net output #0: accuracy = 0.895237
I0522 03:16:44.138593 25646 solver.cpp:409]     Test net output #1: loss = 0.341602 (* 1 = 0.341602 loss)
I0522 03:17:04.970712 25646 solver.cpp:237] Iteration 225000, loss = 1.03575
I0522 03:17:04.970772 25646 solver.cpp:253]     Train net output #0: loss = 1.03575 (* 1 = 1.03575 loss)
I0522 03:17:04.970793 25646 sgd_solver.cpp:106] Iteration 225000, lr = 0.003
I0522 03:17:17.065738 25646 solver.cpp:237] Iteration 225750, loss = 1.13742
I0522 03:17:17.065919 25646 solver.cpp:253]     Train net output #0: loss = 1.13742 (* 1 = 1.13742 loss)
I0522 03:17:17.065937 25646 sgd_solver.cpp:106] Iteration 225750, lr = 0.003
I0522 03:17:29.137387 25646 solver.cpp:237] Iteration 226500, loss = 1.20293
I0522 03:17:29.137424 25646 solver.cpp:253]     Train net output #0: loss = 1.20293 (* 1 = 1.20293 loss)
I0522 03:17:29.137444 25646 sgd_solver.cpp:106] Iteration 226500, lr = 0.003
I0522 03:17:41.285712 25646 solver.cpp:237] Iteration 227250, loss = 1.0053
I0522 03:17:41.285768 25646 solver.cpp:253]     Train net output #0: loss = 1.0053 (* 1 = 1.0053 loss)
I0522 03:17:41.285784 25646 sgd_solver.cpp:106] Iteration 227250, lr = 0.003
I0522 03:17:53.462739 25646 solver.cpp:237] Iteration 228000, loss = 0.861519
I0522 03:17:53.462905 25646 solver.cpp:253]     Train net output #0: loss = 0.861522 (* 1 = 0.861522 loss)
I0522 03:17:53.462923 25646 sgd_solver.cpp:106] Iteration 228000, lr = 0.003
I0522 03:18:05.658102 25646 solver.cpp:237] Iteration 228750, loss = 1.89892
I0522 03:18:05.658156 25646 solver.cpp:253]     Train net output #0: loss = 1.89892 (* 1 = 1.89892 loss)
I0522 03:18:05.658174 25646 sgd_solver.cpp:106] Iteration 228750, lr = 0.003
I0522 03:18:17.813388 25646 solver.cpp:237] Iteration 229500, loss = 1.44843
I0522 03:18:17.813427 25646 solver.cpp:253]     Train net output #0: loss = 1.44844 (* 1 = 1.44844 loss)
I0522 03:18:17.813446 25646 sgd_solver.cpp:106] Iteration 229500, lr = 0.003
I0522 03:18:50.754612 25646 solver.cpp:237] Iteration 230250, loss = 1.2079
I0522 03:18:50.754797 25646 solver.cpp:253]     Train net output #0: loss = 1.2079 (* 1 = 1.2079 loss)
I0522 03:18:50.754815 25646 sgd_solver.cpp:106] Iteration 230250, lr = 0.003
I0522 03:19:02.966938 25646 solver.cpp:237] Iteration 231000, loss = 1.08591
I0522 03:19:02.966974 25646 solver.cpp:253]     Train net output #0: loss = 1.08591 (* 1 = 1.08591 loss)
I0522 03:19:02.966992 25646 sgd_solver.cpp:106] Iteration 231000, lr = 0.003
I0522 03:19:15.147366 25646 solver.cpp:237] Iteration 231750, loss = 2.01903
I0522 03:19:15.147423 25646 solver.cpp:253]     Train net output #0: loss = 2.01904 (* 1 = 2.01904 loss)
I0522 03:19:15.147441 25646 sgd_solver.cpp:106] Iteration 231750, lr = 0.003
I0522 03:19:27.264946 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_232500.caffemodel
I0522 03:19:27.314093 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_232500.solverstate
I0522 03:19:27.343974 25646 solver.cpp:237] Iteration 232500, loss = 1.07747
I0522 03:19:27.344028 25646 solver.cpp:253]     Train net output #0: loss = 1.07747 (* 1 = 1.07747 loss)
I0522 03:19:27.344048 25646 sgd_solver.cpp:106] Iteration 232500, lr = 0.003
I0522 03:19:39.464126 25646 solver.cpp:237] Iteration 233250, loss = 1.33606
I0522 03:19:39.464181 25646 solver.cpp:253]     Train net output #0: loss = 1.33606 (* 1 = 1.33606 loss)
I0522 03:19:39.464210 25646 sgd_solver.cpp:106] Iteration 233250, lr = 0.003
I0522 03:19:51.637142 25646 solver.cpp:237] Iteration 234000, loss = 1.00494
I0522 03:19:51.637179 25646 solver.cpp:253]     Train net output #0: loss = 1.00494 (* 1 = 1.00494 loss)
I0522 03:19:51.637198 25646 sgd_solver.cpp:106] Iteration 234000, lr = 0.003
I0522 03:20:03.810403 25646 solver.cpp:237] Iteration 234750, loss = 1.2892
I0522 03:20:03.810590 25646 solver.cpp:253]     Train net output #0: loss = 1.2892 (* 1 = 1.2892 loss)
I0522 03:20:03.810607 25646 sgd_solver.cpp:106] Iteration 234750, lr = 0.003
I0522 03:20:36.856781 25646 solver.cpp:237] Iteration 235500, loss = 0.941528
I0522 03:20:36.856969 25646 solver.cpp:253]     Train net output #0: loss = 0.94153 (* 1 = 0.94153 loss)
I0522 03:20:36.856987 25646 sgd_solver.cpp:106] Iteration 235500, lr = 0.003
I0522 03:20:49.052577 25646 solver.cpp:237] Iteration 236250, loss = 1.38338
I0522 03:20:49.052615 25646 solver.cpp:253]     Train net output #0: loss = 1.38339 (* 1 = 1.38339 loss)
I0522 03:20:49.052634 25646 sgd_solver.cpp:106] Iteration 236250, lr = 0.003
I0522 03:21:01.240371 25646 solver.cpp:237] Iteration 237000, loss = 0.739369
I0522 03:21:01.240427 25646 solver.cpp:253]     Train net output #0: loss = 0.73937 (* 1 = 0.73937 loss)
I0522 03:21:01.240444 25646 sgd_solver.cpp:106] Iteration 237000, lr = 0.003
I0522 03:21:13.427284 25646 solver.cpp:237] Iteration 237750, loss = 1.25746
I0522 03:21:13.427453 25646 solver.cpp:253]     Train net output #0: loss = 1.25746 (* 1 = 1.25746 loss)
I0522 03:21:13.427469 25646 sgd_solver.cpp:106] Iteration 237750, lr = 0.003
I0522 03:21:25.597162 25646 solver.cpp:237] Iteration 238500, loss = 0.868245
I0522 03:21:25.597218 25646 solver.cpp:253]     Train net output #0: loss = 0.868247 (* 1 = 0.868247 loss)
I0522 03:21:25.597235 25646 sgd_solver.cpp:106] Iteration 238500, lr = 0.003
I0522 03:21:37.791781 25646 solver.cpp:237] Iteration 239250, loss = 1.40252
I0522 03:21:37.791820 25646 solver.cpp:253]     Train net output #0: loss = 1.40252 (* 1 = 1.40252 loss)
I0522 03:21:37.791838 25646 sgd_solver.cpp:106] Iteration 239250, lr = 0.003
I0522 03:21:49.944164 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_240000.caffemodel
I0522 03:21:49.993870 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_240000.solverstate
I0522 03:21:50.019300 25646 solver.cpp:341] Iteration 240000, Testing net (#0)
I0522 03:23:02.860363 25646 solver.cpp:409]     Test net output #0: accuracy = 0.898191
I0522 03:23:02.860548 25646 solver.cpp:409]     Test net output #1: loss = 0.322536 (* 1 = 0.322536 loss)
I0522 03:23:23.688621 25646 solver.cpp:237] Iteration 240000, loss = 1.15836
I0522 03:23:23.688683 25646 solver.cpp:253]     Train net output #0: loss = 1.15836 (* 1 = 1.15836 loss)
I0522 03:23:23.688700 25646 sgd_solver.cpp:106] Iteration 240000, lr = 0.003
I0522 03:23:35.831820 25646 solver.cpp:237] Iteration 240750, loss = 1.09235
I0522 03:23:35.831992 25646 solver.cpp:253]     Train net output #0: loss = 1.09236 (* 1 = 1.09236 loss)
I0522 03:23:35.832010 25646 sgd_solver.cpp:106] Iteration 240750, lr = 0.003
I0522 03:23:47.983798 25646 solver.cpp:237] Iteration 241500, loss = 0.880249
I0522 03:23:47.983836 25646 solver.cpp:253]     Train net output #0: loss = 0.88025 (* 1 = 0.88025 loss)
I0522 03:23:47.983855 25646 sgd_solver.cpp:106] Iteration 241500, lr = 0.003
I0522 03:24:00.207872 25646 solver.cpp:237] Iteration 242250, loss = 1.58747
I0522 03:24:00.207926 25646 solver.cpp:253]     Train net output #0: loss = 1.58747 (* 1 = 1.58747 loss)
I0522 03:24:00.207942 25646 sgd_solver.cpp:106] Iteration 242250, lr = 0.003
I0522 03:24:12.451935 25646 solver.cpp:237] Iteration 243000, loss = 1.21986
I0522 03:24:12.452114 25646 solver.cpp:253]     Train net output #0: loss = 1.21986 (* 1 = 1.21986 loss)
I0522 03:24:12.452131 25646 sgd_solver.cpp:106] Iteration 243000, lr = 0.003
I0522 03:24:24.616312 25646 solver.cpp:237] Iteration 243750, loss = 1.03454
I0522 03:24:24.616365 25646 solver.cpp:253]     Train net output #0: loss = 1.03454 (* 1 = 1.03454 loss)
I0522 03:24:24.616382 25646 sgd_solver.cpp:106] Iteration 243750, lr = 0.003
I0522 03:24:36.755796 25646 solver.cpp:237] Iteration 244500, loss = 1.36124
I0522 03:24:36.755834 25646 solver.cpp:253]     Train net output #0: loss = 1.36124 (* 1 = 1.36124 loss)
I0522 03:24:36.755851 25646 sgd_solver.cpp:106] Iteration 244500, lr = 0.003
I0522 03:25:09.763548 25646 solver.cpp:237] Iteration 245250, loss = 1.18037
I0522 03:25:09.763739 25646 solver.cpp:253]     Train net output #0: loss = 1.18037 (* 1 = 1.18037 loss)
I0522 03:25:09.763756 25646 sgd_solver.cpp:106] Iteration 245250, lr = 0.003
I0522 03:25:21.894578 25646 solver.cpp:237] Iteration 246000, loss = 0.746796
I0522 03:25:21.894618 25646 solver.cpp:253]     Train net output #0: loss = 0.746798 (* 1 = 0.746798 loss)
I0522 03:25:21.894634 25646 sgd_solver.cpp:106] Iteration 246000, lr = 0.003
I0522 03:25:34.090278 25646 solver.cpp:237] Iteration 246750, loss = 1.40772
I0522 03:25:34.090329 25646 solver.cpp:253]     Train net output #0: loss = 1.40772 (* 1 = 1.40772 loss)
I0522 03:25:34.090348 25646 sgd_solver.cpp:106] Iteration 246750, lr = 0.003
I0522 03:25:46.285445 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_247500.caffemodel
I0522 03:25:46.335688 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_247500.solverstate
I0522 03:25:46.365897 25646 solver.cpp:237] Iteration 247500, loss = 0.948674
I0522 03:25:46.365952 25646 solver.cpp:253]     Train net output #0: loss = 0.948676 (* 1 = 0.948676 loss)
I0522 03:25:46.365972 25646 sgd_solver.cpp:106] Iteration 247500, lr = 0.003
I0522 03:25:58.590976 25646 solver.cpp:237] Iteration 248250, loss = 1.28783
I0522 03:25:58.591029 25646 solver.cpp:253]     Train net output #0: loss = 1.28784 (* 1 = 1.28784 loss)
I0522 03:25:58.591048 25646 sgd_solver.cpp:106] Iteration 248250, lr = 0.003
I0522 03:26:10.832165 25646 solver.cpp:237] Iteration 249000, loss = 0.989851
I0522 03:26:10.832203 25646 solver.cpp:253]     Train net output #0: loss = 0.989853 (* 1 = 0.989853 loss)
I0522 03:26:10.832221 25646 sgd_solver.cpp:106] Iteration 249000, lr = 0.003
I0522 03:26:23.025003 25646 solver.cpp:237] Iteration 249750, loss = 0.793662
I0522 03:26:23.025192 25646 solver.cpp:253]     Train net output #0: loss = 0.793664 (* 1 = 0.793664 loss)
I0522 03:26:23.025208 25646 sgd_solver.cpp:106] Iteration 249750, lr = 0.003
I0522 03:26:55.994981 25646 solver.cpp:237] Iteration 250500, loss = 0.814946
I0522 03:26:55.995172 25646 solver.cpp:253]     Train net output #0: loss = 0.814948 (* 1 = 0.814948 loss)
I0522 03:26:55.995188 25646 sgd_solver.cpp:106] Iteration 250500, lr = 0.003
I0522 03:27:08.103088 25646 solver.cpp:237] Iteration 251250, loss = 1.29579
I0522 03:27:08.103126 25646 solver.cpp:253]     Train net output #0: loss = 1.29579 (* 1 = 1.29579 loss)
I0522 03:27:08.103144 25646 sgd_solver.cpp:106] Iteration 251250, lr = 0.003
I0522 03:27:20.225476 25646 solver.cpp:237] Iteration 252000, loss = 0.888426
I0522 03:27:20.225528 25646 solver.cpp:253]     Train net output #0: loss = 0.888428 (* 1 = 0.888428 loss)
I0522 03:27:20.225545 25646 sgd_solver.cpp:106] Iteration 252000, lr = 0.003
I0522 03:27:32.334728 25646 solver.cpp:237] Iteration 252750, loss = 0.787402
I0522 03:27:32.334900 25646 solver.cpp:253]     Train net output #0: loss = 0.787404 (* 1 = 0.787404 loss)
I0522 03:27:32.334918 25646 sgd_solver.cpp:106] Iteration 252750, lr = 0.003
I0522 03:27:44.565784 25646 solver.cpp:237] Iteration 253500, loss = 1.05288
I0522 03:27:44.565829 25646 solver.cpp:253]     Train net output #0: loss = 1.05289 (* 1 = 1.05289 loss)
I0522 03:27:44.565847 25646 sgd_solver.cpp:106] Iteration 253500, lr = 0.003
I0522 03:27:56.788096 25646 solver.cpp:237] Iteration 254250, loss = 0.889168
I0522 03:27:56.788136 25646 solver.cpp:253]     Train net output #0: loss = 0.88917 (* 1 = 0.88917 loss)
I0522 03:27:56.788152 25646 sgd_solver.cpp:106] Iteration 254250, lr = 0.003
I0522 03:28:08.981696 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_255000.caffemodel
I0522 03:28:09.031357 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_255000.solverstate
I0522 03:28:09.056874 25646 solver.cpp:341] Iteration 255000, Testing net (#0)
I0522 03:29:00.983719 25646 solver.cpp:409]     Test net output #0: accuracy = 0.898651
I0522 03:29:00.983906 25646 solver.cpp:409]     Test net output #1: loss = 0.328783 (* 1 = 0.328783 loss)
I0522 03:29:21.829131 25646 solver.cpp:237] Iteration 255000, loss = 0.781546
I0522 03:29:21.829195 25646 solver.cpp:253]     Train net output #0: loss = 0.781548 (* 1 = 0.781548 loss)
I0522 03:29:21.829215 25646 sgd_solver.cpp:106] Iteration 255000, lr = 0.003
I0522 03:29:34.025706 25646 solver.cpp:237] Iteration 255750, loss = 1.12866
I0522 03:29:34.025882 25646 solver.cpp:253]     Train net output #0: loss = 1.12866 (* 1 = 1.12866 loss)
I0522 03:29:34.025898 25646 sgd_solver.cpp:106] Iteration 255750, lr = 0.003
I0522 03:29:46.234941 25646 solver.cpp:237] Iteration 256500, loss = 0.928603
I0522 03:29:46.234993 25646 solver.cpp:253]     Train net output #0: loss = 0.928606 (* 1 = 0.928606 loss)
I0522 03:29:46.235010 25646 sgd_solver.cpp:106] Iteration 256500, lr = 0.003
I0522 03:29:58.446378 25646 solver.cpp:237] Iteration 257250, loss = 1.515
I0522 03:29:58.446416 25646 solver.cpp:253]     Train net output #0: loss = 1.515 (* 1 = 1.515 loss)
I0522 03:29:58.446434 25646 sgd_solver.cpp:106] Iteration 257250, lr = 0.003
I0522 03:30:10.623129 25646 solver.cpp:237] Iteration 258000, loss = 0.909105
I0522 03:30:10.623317 25646 solver.cpp:253]     Train net output #0: loss = 0.909107 (* 1 = 0.909107 loss)
I0522 03:30:10.623334 25646 sgd_solver.cpp:106] Iteration 258000, lr = 0.003
I0522 03:30:22.774436 25646 solver.cpp:237] Iteration 258750, loss = 1.12955
I0522 03:30:22.774473 25646 solver.cpp:253]     Train net output #0: loss = 1.12955 (* 1 = 1.12955 loss)
I0522 03:30:22.774492 25646 sgd_solver.cpp:106] Iteration 258750, lr = 0.003
I0522 03:30:34.948982 25646 solver.cpp:237] Iteration 259500, loss = 1.29964
I0522 03:30:34.949033 25646 solver.cpp:253]     Train net output #0: loss = 1.29964 (* 1 = 1.29964 loss)
I0522 03:30:34.949049 25646 sgd_solver.cpp:106] Iteration 259500, lr = 0.003
I0522 03:31:07.994999 25646 solver.cpp:237] Iteration 260250, loss = 1.01575
I0522 03:31:07.995189 25646 solver.cpp:253]     Train net output #0: loss = 1.01575 (* 1 = 1.01575 loss)
I0522 03:31:07.995208 25646 sgd_solver.cpp:106] Iteration 260250, lr = 0.003
I0522 03:31:20.125583 25646 solver.cpp:237] Iteration 261000, loss = 1.14985
I0522 03:31:20.125640 25646 solver.cpp:253]     Train net output #0: loss = 1.14985 (* 1 = 1.14985 loss)
I0522 03:31:20.125665 25646 sgd_solver.cpp:106] Iteration 261000, lr = 0.003
I0522 03:31:32.267382 25646 solver.cpp:237] Iteration 261750, loss = 1.14156
I0522 03:31:32.267419 25646 solver.cpp:253]     Train net output #0: loss = 1.14156 (* 1 = 1.14156 loss)
I0522 03:31:32.267437 25646 sgd_solver.cpp:106] Iteration 261750, lr = 0.003
I0522 03:31:44.389159 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_262500.caffemodel
I0522 03:31:44.448199 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_262500.solverstate
I0522 03:31:44.480640 25646 solver.cpp:237] Iteration 262500, loss = 1.64697
I0522 03:31:44.480700 25646 solver.cpp:253]     Train net output #0: loss = 1.64698 (* 1 = 1.64698 loss)
I0522 03:31:44.480718 25646 sgd_solver.cpp:106] Iteration 262500, lr = 0.003
I0522 03:31:56.619191 25646 solver.cpp:237] Iteration 263250, loss = 0.998514
I0522 03:31:56.619231 25646 solver.cpp:253]     Train net output #0: loss = 0.998517 (* 1 = 0.998517 loss)
I0522 03:31:56.619248 25646 sgd_solver.cpp:106] Iteration 263250, lr = 0.003
I0522 03:32:08.736023 25646 solver.cpp:237] Iteration 264000, loss = 1.12965
I0522 03:32:08.736078 25646 solver.cpp:253]     Train net output #0: loss = 1.12965 (* 1 = 1.12965 loss)
I0522 03:32:08.736104 25646 sgd_solver.cpp:106] Iteration 264000, lr = 0.003
I0522 03:32:20.911401 25646 solver.cpp:237] Iteration 264750, loss = 0.953632
I0522 03:32:20.911592 25646 solver.cpp:253]     Train net output #0: loss = 0.953635 (* 1 = 0.953635 loss)
I0522 03:32:20.911609 25646 sgd_solver.cpp:106] Iteration 264750, lr = 0.003
I0522 03:32:53.935556 25646 solver.cpp:237] Iteration 265500, loss = 0.868975
I0522 03:32:53.935748 25646 solver.cpp:253]     Train net output #0: loss = 0.868978 (* 1 = 0.868978 loss)
I0522 03:32:53.935765 25646 sgd_solver.cpp:106] Iteration 265500, lr = 0.003
I0522 03:33:06.150538 25646 solver.cpp:237] Iteration 266250, loss = 1.03959
I0522 03:33:06.150593 25646 solver.cpp:253]     Train net output #0: loss = 1.03959 (* 1 = 1.03959 loss)
I0522 03:33:06.150610 25646 sgd_solver.cpp:106] Iteration 266250, lr = 0.003
I0522 03:33:18.337435 25646 solver.cpp:237] Iteration 267000, loss = 1.58327
I0522 03:33:18.337473 25646 solver.cpp:253]     Train net output #0: loss = 1.58327 (* 1 = 1.58327 loss)
I0522 03:33:18.337491 25646 sgd_solver.cpp:106] Iteration 267000, lr = 0.003
I0522 03:33:30.515035 25646 solver.cpp:237] Iteration 267750, loss = 1.24419
I0522 03:33:30.515218 25646 solver.cpp:253]     Train net output #0: loss = 1.24419 (* 1 = 1.24419 loss)
I0522 03:33:30.515234 25646 sgd_solver.cpp:106] Iteration 267750, lr = 0.003
I0522 03:33:42.677790 25646 solver.cpp:237] Iteration 268500, loss = 1.48038
I0522 03:33:42.677829 25646 solver.cpp:253]     Train net output #0: loss = 1.48038 (* 1 = 1.48038 loss)
I0522 03:33:42.677846 25646 sgd_solver.cpp:106] Iteration 268500, lr = 0.003
I0522 03:33:54.782649 25646 solver.cpp:237] Iteration 269250, loss = 1.1434
I0522 03:33:54.782701 25646 solver.cpp:253]     Train net output #0: loss = 1.1434 (* 1 = 1.1434 loss)
I0522 03:33:54.782718 25646 sgd_solver.cpp:106] Iteration 269250, lr = 0.003
I0522 03:34:06.876518 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_270000.caffemodel
I0522 03:34:06.927963 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_270000.solverstate
I0522 03:34:06.955282 25646 solver.cpp:341] Iteration 270000, Testing net (#0)
I0522 03:35:19.608986 25646 solver.cpp:409]     Test net output #0: accuracy = 0.898389
I0522 03:35:19.609174 25646 solver.cpp:409]     Test net output #1: loss = 0.319726 (* 1 = 0.319726 loss)
I0522 03:35:40.401542 25646 solver.cpp:237] Iteration 270000, loss = 1.24447
I0522 03:35:40.401607 25646 solver.cpp:253]     Train net output #0: loss = 1.24447 (* 1 = 1.24447 loss)
I0522 03:35:40.401625 25646 sgd_solver.cpp:106] Iteration 270000, lr = 0.003
I0522 03:35:52.547283 25646 solver.cpp:237] Iteration 270750, loss = 0.902281
I0522 03:35:52.547461 25646 solver.cpp:253]     Train net output #0: loss = 0.902284 (* 1 = 0.902284 loss)
I0522 03:35:52.547477 25646 sgd_solver.cpp:106] Iteration 270750, lr = 0.003
I0522 03:36:04.712731 25646 solver.cpp:237] Iteration 271500, loss = 1.28519
I0522 03:36:04.712781 25646 solver.cpp:253]     Train net output #0: loss = 1.2852 (* 1 = 1.2852 loss)
I0522 03:36:04.712800 25646 sgd_solver.cpp:106] Iteration 271500, lr = 0.003
I0522 03:36:16.883981 25646 solver.cpp:237] Iteration 272250, loss = 1.07664
I0522 03:36:16.884019 25646 solver.cpp:253]     Train net output #0: loss = 1.07664 (* 1 = 1.07664 loss)
I0522 03:36:16.884037 25646 sgd_solver.cpp:106] Iteration 272250, lr = 0.003
I0522 03:36:29.102454 25646 solver.cpp:237] Iteration 273000, loss = 0.856505
I0522 03:36:29.102654 25646 solver.cpp:253]     Train net output #0: loss = 0.856507 (* 1 = 0.856507 loss)
I0522 03:36:29.102672 25646 sgd_solver.cpp:106] Iteration 273000, lr = 0.003
I0522 03:36:41.317364 25646 solver.cpp:237] Iteration 273750, loss = 0.681139
I0522 03:36:41.317409 25646 solver.cpp:253]     Train net output #0: loss = 0.681142 (* 1 = 0.681142 loss)
I0522 03:36:41.317425 25646 sgd_solver.cpp:106] Iteration 273750, lr = 0.003
I0522 03:36:53.500680 25646 solver.cpp:237] Iteration 274500, loss = 0.859773
I0522 03:36:53.500735 25646 solver.cpp:253]     Train net output #0: loss = 0.859776 (* 1 = 0.859776 loss)
I0522 03:36:53.500752 25646 sgd_solver.cpp:106] Iteration 274500, lr = 0.003
I0522 03:37:26.474779 25646 solver.cpp:237] Iteration 275250, loss = 1.1432
I0522 03:37:26.474972 25646 solver.cpp:253]     Train net output #0: loss = 1.1432 (* 1 = 1.1432 loss)
I0522 03:37:26.474990 25646 sgd_solver.cpp:106] Iteration 275250, lr = 0.003
I0522 03:37:38.653301 25646 solver.cpp:237] Iteration 276000, loss = 1.0373
I0522 03:37:38.653354 25646 solver.cpp:253]     Train net output #0: loss = 1.0373 (* 1 = 1.0373 loss)
I0522 03:37:38.653372 25646 sgd_solver.cpp:106] Iteration 276000, lr = 0.003
I0522 03:37:50.827970 25646 solver.cpp:237] Iteration 276750, loss = 1.38787
I0522 03:37:50.828009 25646 solver.cpp:253]     Train net output #0: loss = 1.38787 (* 1 = 1.38787 loss)
I0522 03:37:50.828027 25646 sgd_solver.cpp:106] Iteration 276750, lr = 0.003
I0522 03:38:03.016571 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_277500.caffemodel
I0522 03:38:03.065675 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_277500.solverstate
I0522 03:38:03.096385 25646 solver.cpp:237] Iteration 277500, loss = 0.914178
I0522 03:38:03.096441 25646 solver.cpp:253]     Train net output #0: loss = 0.914181 (* 1 = 0.914181 loss)
I0522 03:38:03.096468 25646 sgd_solver.cpp:106] Iteration 277500, lr = 0.003
I0522 03:38:15.260293 25646 solver.cpp:237] Iteration 278250, loss = 1.07455
I0522 03:38:15.260330 25646 solver.cpp:253]     Train net output #0: loss = 1.07455 (* 1 = 1.07455 loss)
I0522 03:38:15.260349 25646 sgd_solver.cpp:106] Iteration 278250, lr = 0.003
I0522 03:38:27.430480 25646 solver.cpp:237] Iteration 279000, loss = 1.2589
I0522 03:38:27.430533 25646 solver.cpp:253]     Train net output #0: loss = 1.2589 (* 1 = 1.2589 loss)
I0522 03:38:27.430551 25646 sgd_solver.cpp:106] Iteration 279000, lr = 0.003
I0522 03:38:39.553163 25646 solver.cpp:237] Iteration 279750, loss = 0.825943
I0522 03:38:39.553344 25646 solver.cpp:253]     Train net output #0: loss = 0.825945 (* 1 = 0.825945 loss)
I0522 03:38:39.553360 25646 sgd_solver.cpp:106] Iteration 279750, lr = 0.003
I0522 03:39:12.503325 25646 solver.cpp:237] Iteration 280500, loss = 1.14865
I0522 03:39:12.503517 25646 solver.cpp:253]     Train net output #0: loss = 1.14865 (* 1 = 1.14865 loss)
I0522 03:39:12.503535 25646 sgd_solver.cpp:106] Iteration 280500, lr = 0.003
I0522 03:39:24.621608 25646 solver.cpp:237] Iteration 281250, loss = 1.46712
I0522 03:39:24.621654 25646 solver.cpp:253]     Train net output #0: loss = 1.46712 (* 1 = 1.46712 loss)
I0522 03:39:24.621672 25646 sgd_solver.cpp:106] Iteration 281250, lr = 0.003
I0522 03:39:36.742298 25646 solver.cpp:237] Iteration 282000, loss = 1.10293
I0522 03:39:36.742336 25646 solver.cpp:253]     Train net output #0: loss = 1.10293 (* 1 = 1.10293 loss)
I0522 03:39:36.742354 25646 sgd_solver.cpp:106] Iteration 282000, lr = 0.003
I0522 03:39:48.901113 25646 solver.cpp:237] Iteration 282750, loss = 1.34331
I0522 03:39:48.901309 25646 solver.cpp:253]     Train net output #0: loss = 1.34331 (* 1 = 1.34331 loss)
I0522 03:39:48.901326 25646 sgd_solver.cpp:106] Iteration 282750, lr = 0.003
I0522 03:40:01.068627 25646 solver.cpp:237] Iteration 283500, loss = 1.16179
I0522 03:40:01.068666 25646 solver.cpp:253]     Train net output #0: loss = 1.16179 (* 1 = 1.16179 loss)
I0522 03:40:01.068684 25646 sgd_solver.cpp:106] Iteration 283500, lr = 0.003
I0522 03:40:13.220842 25646 solver.cpp:237] Iteration 284250, loss = 1.39638
I0522 03:40:13.220896 25646 solver.cpp:253]     Train net output #0: loss = 1.39639 (* 1 = 1.39639 loss)
I0522 03:40:13.220913 25646 sgd_solver.cpp:106] Iteration 284250, lr = 0.003
I0522 03:40:25.372222 25646 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_285000.caffemodel
I0522 03:40:25.421274 25646 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/caffe_workflow/snapshots_bslr/bs_20_lr_0.0030_2016-05-20T15.48.55.379925_iter_285000.solverstate
I0522 03:40:25.446787 25646 solver.cpp:341] Iteration 285000, Testing net (#0)
I0522 03:41:17.135673 25646 solver.cpp:409]     Test net output #0: accuracy = 0.891936
I0522 03:41:17.135862 25646 solver.cpp:409]     Test net output #1: loss = 0.353824 (* 1 = 0.353824 loss)
I0522 03:41:38.010869 25646 solver.cpp:237] Iteration 285000, loss = 1.14138
I0522 03:41:38.010931 25646 solver.cpp:253]     Train net output #0: loss = 1.14139 (* 1 = 1.14139 loss)
I0522 03:41:38.010960 25646 sgd_solver.cpp:106] Iteration 285000, lr = 0.003
I0522 03:41:50.238984 25646 solver.cpp:237] Iteration 285750, loss = 1.24513
I0522 03:41:50.239174 25646 solver.cpp:253]     Train net output #0: loss = 1.24514 (* 1 = 1.24514 loss)
I0522 03:41:50.239192 25646 sgd_solver.cpp:106] Iteration 285750, lr = 0.003
I0522 03:42:02.436594 25646 solver.cpp:237] Iteration 286500, loss = 1.33187
I0522 03:42:02.436630 25646 solver.cpp:253]     Train net output #0: loss = 1.33187 (* 1 = 1.33187 loss)
I0522 03:42:02.436650 25646 sgd_solver.cpp:106] Iteration 286500, lr = 0.003
I0522 03:42:14.622592 25646 solver.cpp:237] Iteration 287250, loss = 1.28783
I0522 03:42:14.622647 25646 solver.cpp:253]     Train net output #0: loss = 1.28783 (* 1 = 1.28783 loss)
I0522 03:42:14.622664 25646 sgd_solver.cpp:106] Iteration 287250, lr = 0.003
I0522 03:42:26.801056 25646 solver.cpp:237] Iteration 288000, loss = 0.959945
I0522 03:42:26.801234 25646 solver.cpp:253]     Train net output #0: loss = 0.959948 (* 1 = 0.959948 loss)
I0522 03:42:26.801250 25646 sgd_solver.cpp:106] Iteration 288000, lr = 0.003
I0522 03:42:38.977151 25646 solver.cpp:237] Iteration 288750, loss = 1.10868
I0522 03:42:38.977206 25646 solver.cpp:253]     Train net output #0: loss = 1.10868 (* 1 = 1.10868 loss)
I0522 03:42:38.977223 25646 sgd_solver.cpp:106] Iteration 288750, lr = 0.003
I0522 03:42:51.193063 25646 solver.cpp:237] Iteration 289500, loss = 1.23456
I0522 03:42:51.193100 25646 solver.cpp:253]     Train net output #0: loss = 1.23457 (* 1 = 1.23457 loss)
I0522 03:42:51.193119 25646 sgd_solver.cpp:106] Iteration 289500, lr = 0.003
I0522 03:43:24.276630 25646 solver.cpp:237] Iteration 290250, loss = 1.10803
I0522 03:43:24.276826 25646 solver.cpp:253]     Train net output #0: loss = 1.10803 (* 1 = 1.10803 loss)
I0522 03:43:24.276844 25646 sgd_solver.cpp:106] Iteration 290250, lr = 0.003
=>> PBS: job killed: walltime 7226 exceeded limit 7200
aprun: Apid 11244167: Caught signal Terminated, sending to application
*** Aborted at 1463903009 (unix time) try "date -d @1463903009" if you are using GNU date ***
PC: @     0x2aaab499da2c (unknown)
*** SIGTERM (@0x642b) received by PID 25646 (TID 0x2aaac746f900) from PID 25643; stack trace: ***
    @     0x2aaab7c78850 (unknown)
    @     0x2aaab499da2c (unknown)
    @     0x2aaab4985e33 (unknown)
    @     0x2aaab49c34fc (unknown)
    @     0x2aaab4882174 (unknown)
    @           0x62fa8d caffe::CuDNNConvolutionLayer<>::Backward_gpu()
    @           0x5f02f3 caffe::Net<>::BackwardFromTo()
    @           0x5f033f caffe::Net<>::Backward()
    @           0x5ca111 caffe::Solver<>::Step()
aprun: Apid 11244167: Caught signal Terminated, sending to application
    @           0x5caba5 caffe::Solver<>::Solve()
aprun: Apid 11244167: Caught signal Terminated, sending to application
    @           0x43b3b8 train()
    @           0x43020c main
    @     0x2aaab7ea4c36 __libc_start_main
    @           0x438669 (unknown)
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
aprun: Apid 11244167: Caught signal Terminated, sending to application
_pmiu_daemon(SIGCHLD): [NID 02172] [c4-3c2s1n2] [Sun May 22 03:43:31 2016] PE RANK 0 exit signal Terminated
Application 11244167 exit codes: 143
Application 11244167 resources: utime ~6287s, stime ~923s, Rss ~5333252, inblocks ~13315148, outblocks ~592958
