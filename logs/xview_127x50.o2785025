I0506 11:25:12.229321 11809 caffe.cpp:184] Using GPUs 0
I0506 11:25:12.670402 11809 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1500
test_interval: 1000
base_lr: 0.0025
display: 100
max_iter: 15000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0001
snapshot: 5000
snapshot_prefix: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model"
solver_mode: GPU
device_id: 0
net: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt"
I0506 11:25:12.672543 11809 solver.cpp:91] Creating training net from net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0506 11:25:12.676448 11809 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_hdf5
I0506 11:25:12.676504 11809 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0506 11:25:12.676892 11809 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TRAIN
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0506 11:25:12.677095 11809 layer_factory.hpp:77] Creating layer data_hdf5
I0506 11:25:12.677119 11809 net.cpp:106] Creating Layer data_hdf5
I0506 11:25:12.677141 11809 net.cpp:411] data_hdf5 -> data
I0506 11:25:12.677181 11809 net.cpp:411] data_hdf5 -> label
I0506 11:25:12.677213 11809 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.trainlist
I0506 11:25:12.679455 11809 hdf5_data_layer.cpp:93] Number of HDF5 files: 15
I0506 11:25:12.695644 11809 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0506 11:25:19.327793 11809 hdf5.cpp:35] Datatype class: H5T_INTEGER
I0506 11:25:19.331293 11809 net.cpp:150] Setting up data_hdf5
I0506 11:25:19.331342 11809 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0506 11:25:19.331360 11809 net.cpp:157] Top shape: 100 (100)
I0506 11:25:19.331377 11809 net.cpp:165] Memory required for data: 2540400
I0506 11:25:19.331405 11809 layer_factory.hpp:77] Creating layer conv1
I0506 11:25:19.331440 11809 net.cpp:106] Creating Layer conv1
I0506 11:25:19.331464 11809 net.cpp:454] conv1 <- data
I0506 11:25:19.331486 11809 net.cpp:411] conv1 -> conv1
I0506 11:25:22.657706 11809 net.cpp:150] Setting up conv1
I0506 11:25:22.657755 11809 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 11:25:22.657769 11809 net.cpp:165] Memory required for data: 30188400
I0506 11:25:22.657802 11809 layer_factory.hpp:77] Creating layer relu1
I0506 11:25:22.657824 11809 net.cpp:106] Creating Layer relu1
I0506 11:25:22.657838 11809 net.cpp:454] relu1 <- conv1
I0506 11:25:22.657879 11809 net.cpp:397] relu1 -> conv1 (in-place)
I0506 11:25:22.658401 11809 net.cpp:150] Setting up relu1
I0506 11:25:22.658423 11809 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 11:25:22.658437 11809 net.cpp:165] Memory required for data: 57836400
I0506 11:25:22.658449 11809 layer_factory.hpp:77] Creating layer pool1
I0506 11:25:22.658479 11809 net.cpp:106] Creating Layer pool1
I0506 11:25:22.658493 11809 net.cpp:454] pool1 <- conv1
I0506 11:25:22.658509 11809 net.cpp:411] pool1 -> pool1
I0506 11:25:22.658601 11809 net.cpp:150] Setting up pool1
I0506 11:25:22.658618 11809 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0506 11:25:22.658634 11809 net.cpp:165] Memory required for data: 71660400
I0506 11:25:22.658653 11809 layer_factory.hpp:77] Creating layer conv2
I0506 11:25:22.658676 11809 net.cpp:106] Creating Layer conv2
I0506 11:25:22.658697 11809 net.cpp:454] conv2 <- pool1
I0506 11:25:22.658713 11809 net.cpp:411] conv2 -> conv2
I0506 11:25:22.661551 11809 net.cpp:150] Setting up conv2
I0506 11:25:22.661584 11809 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 11:25:22.661598 11809 net.cpp:165] Memory required for data: 91532400
I0506 11:25:22.661625 11809 layer_factory.hpp:77] Creating layer relu2
I0506 11:25:22.661653 11809 net.cpp:106] Creating Layer relu2
I0506 11:25:22.661667 11809 net.cpp:454] relu2 <- conv2
I0506 11:25:22.661684 11809 net.cpp:397] relu2 -> conv2 (in-place)
I0506 11:25:22.662055 11809 net.cpp:150] Setting up relu2
I0506 11:25:22.662075 11809 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 11:25:22.662087 11809 net.cpp:165] Memory required for data: 111404400
I0506 11:25:22.662099 11809 layer_factory.hpp:77] Creating layer pool2
I0506 11:25:22.662125 11809 net.cpp:106] Creating Layer pool2
I0506 11:25:22.662138 11809 net.cpp:454] pool2 <- conv2
I0506 11:25:22.662158 11809 net.cpp:411] pool2 -> pool2
I0506 11:25:22.662245 11809 net.cpp:150] Setting up pool2
I0506 11:25:22.662279 11809 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0506 11:25:22.662292 11809 net.cpp:165] Memory required for data: 121340400
I0506 11:25:22.662307 11809 layer_factory.hpp:77] Creating layer conv3
I0506 11:25:22.662336 11809 net.cpp:106] Creating Layer conv3
I0506 11:25:22.662350 11809 net.cpp:454] conv3 <- pool2
I0506 11:25:22.662374 11809 net.cpp:411] conv3 -> conv3
I0506 11:25:22.664371 11809 net.cpp:150] Setting up conv3
I0506 11:25:22.664399 11809 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 11:25:22.664412 11809 net.cpp:165] Memory required for data: 132182000
I0506 11:25:22.664443 11809 layer_factory.hpp:77] Creating layer relu3
I0506 11:25:22.664460 11809 net.cpp:106] Creating Layer relu3
I0506 11:25:22.664475 11809 net.cpp:454] relu3 <- conv3
I0506 11:25:22.664490 11809 net.cpp:397] relu3 -> conv3 (in-place)
I0506 11:25:22.665012 11809 net.cpp:150] Setting up relu3
I0506 11:25:22.665035 11809 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 11:25:22.665048 11809 net.cpp:165] Memory required for data: 143023600
I0506 11:25:22.665060 11809 layer_factory.hpp:77] Creating layer pool3
I0506 11:25:22.665083 11809 net.cpp:106] Creating Layer pool3
I0506 11:25:22.665105 11809 net.cpp:454] pool3 <- conv3
I0506 11:25:22.665122 11809 net.cpp:411] pool3 -> pool3
I0506 11:25:22.665205 11809 net.cpp:150] Setting up pool3
I0506 11:25:22.665223 11809 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0506 11:25:22.665243 11809 net.cpp:165] Memory required for data: 148444400
I0506 11:25:22.665256 11809 layer_factory.hpp:77] Creating layer conv4
I0506 11:25:22.665282 11809 net.cpp:106] Creating Layer conv4
I0506 11:25:22.665293 11809 net.cpp:454] conv4 <- pool3
I0506 11:25:22.665309 11809 net.cpp:411] conv4 -> conv4
I0506 11:25:22.668344 11809 net.cpp:150] Setting up conv4
I0506 11:25:22.668378 11809 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 11:25:22.668392 11809 net.cpp:165] Memory required for data: 152073200
I0506 11:25:22.668412 11809 layer_factory.hpp:77] Creating layer relu4
I0506 11:25:22.668437 11809 net.cpp:106] Creating Layer relu4
I0506 11:25:22.668462 11809 net.cpp:454] relu4 <- conv4
I0506 11:25:22.668478 11809 net.cpp:397] relu4 -> conv4 (in-place)
I0506 11:25:22.668997 11809 net.cpp:150] Setting up relu4
I0506 11:25:22.669019 11809 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 11:25:22.669034 11809 net.cpp:165] Memory required for data: 155702000
I0506 11:25:22.669049 11809 layer_factory.hpp:77] Creating layer pool4
I0506 11:25:22.669078 11809 net.cpp:106] Creating Layer pool4
I0506 11:25:22.669092 11809 net.cpp:454] pool4 <- conv4
I0506 11:25:22.669108 11809 net.cpp:411] pool4 -> pool4
I0506 11:25:22.669195 11809 net.cpp:150] Setting up pool4
I0506 11:25:22.669222 11809 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0506 11:25:22.669234 11809 net.cpp:165] Memory required for data: 157516400
I0506 11:25:22.669247 11809 layer_factory.hpp:77] Creating layer ip1
I0506 11:25:22.669276 11809 net.cpp:106] Creating Layer ip1
I0506 11:25:22.669291 11809 net.cpp:454] ip1 <- pool4
I0506 11:25:22.669309 11809 net.cpp:411] ip1 -> ip1
I0506 11:25:22.685197 11809 net.cpp:150] Setting up ip1
I0506 11:25:22.685228 11809 net.cpp:157] Top shape: 100 196 (19600)
I0506 11:25:22.685241 11809 net.cpp:165] Memory required for data: 157594800
I0506 11:25:22.685276 11809 layer_factory.hpp:77] Creating layer relu5
I0506 11:25:22.685303 11809 net.cpp:106] Creating Layer relu5
I0506 11:25:22.685318 11809 net.cpp:454] relu5 <- ip1
I0506 11:25:22.685333 11809 net.cpp:397] relu5 -> ip1 (in-place)
I0506 11:25:22.685704 11809 net.cpp:150] Setting up relu5
I0506 11:25:22.685724 11809 net.cpp:157] Top shape: 100 196 (19600)
I0506 11:25:22.685737 11809 net.cpp:165] Memory required for data: 157673200
I0506 11:25:22.685752 11809 layer_factory.hpp:77] Creating layer drop1
I0506 11:25:22.685781 11809 net.cpp:106] Creating Layer drop1
I0506 11:25:22.685794 11809 net.cpp:454] drop1 <- ip1
I0506 11:25:22.685813 11809 net.cpp:397] drop1 -> ip1 (in-place)
I0506 11:25:22.685871 11809 net.cpp:150] Setting up drop1
I0506 11:25:22.685902 11809 net.cpp:157] Top shape: 100 196 (19600)
I0506 11:25:22.685915 11809 net.cpp:165] Memory required for data: 157751600
I0506 11:25:22.685930 11809 layer_factory.hpp:77] Creating layer ip2
I0506 11:25:22.685950 11809 net.cpp:106] Creating Layer ip2
I0506 11:25:22.685969 11809 net.cpp:454] ip2 <- ip1
I0506 11:25:22.685988 11809 net.cpp:411] ip2 -> ip2
I0506 11:25:22.686492 11809 net.cpp:150] Setting up ip2
I0506 11:25:22.686511 11809 net.cpp:157] Top shape: 100 98 (9800)
I0506 11:25:22.686524 11809 net.cpp:165] Memory required for data: 157790800
I0506 11:25:22.686544 11809 layer_factory.hpp:77] Creating layer relu6
I0506 11:25:22.686570 11809 net.cpp:106] Creating Layer relu6
I0506 11:25:22.686583 11809 net.cpp:454] relu6 <- ip2
I0506 11:25:22.686597 11809 net.cpp:397] relu6 -> ip2 (in-place)
I0506 11:25:22.687147 11809 net.cpp:150] Setting up relu6
I0506 11:25:22.687173 11809 net.cpp:157] Top shape: 100 98 (9800)
I0506 11:25:22.687186 11809 net.cpp:165] Memory required for data: 157830000
I0506 11:25:22.687203 11809 layer_factory.hpp:77] Creating layer drop2
I0506 11:25:22.687225 11809 net.cpp:106] Creating Layer drop2
I0506 11:25:22.687239 11809 net.cpp:454] drop2 <- ip2
I0506 11:25:22.687254 11809 net.cpp:397] drop2 -> ip2 (in-place)
I0506 11:25:22.687305 11809 net.cpp:150] Setting up drop2
I0506 11:25:22.687328 11809 net.cpp:157] Top shape: 100 98 (9800)
I0506 11:25:22.687340 11809 net.cpp:165] Memory required for data: 157869200
I0506 11:25:22.687353 11809 layer_factory.hpp:77] Creating layer ip3
I0506 11:25:22.687368 11809 net.cpp:106] Creating Layer ip3
I0506 11:25:22.687383 11809 net.cpp:454] ip3 <- ip2
I0506 11:25:22.687408 11809 net.cpp:411] ip3 -> ip3
I0506 11:25:22.687641 11809 net.cpp:150] Setting up ip3
I0506 11:25:22.687660 11809 net.cpp:157] Top shape: 100 11 (1100)
I0506 11:25:22.687672 11809 net.cpp:165] Memory required for data: 157873600
I0506 11:25:22.687693 11809 layer_factory.hpp:77] Creating layer drop3
I0506 11:25:22.687713 11809 net.cpp:106] Creating Layer drop3
I0506 11:25:22.687726 11809 net.cpp:454] drop3 <- ip3
I0506 11:25:22.687746 11809 net.cpp:397] drop3 -> ip3 (in-place)
I0506 11:25:22.687790 11809 net.cpp:150] Setting up drop3
I0506 11:25:22.687820 11809 net.cpp:157] Top shape: 100 11 (1100)
I0506 11:25:22.687834 11809 net.cpp:165] Memory required for data: 157878000
I0506 11:25:22.687846 11809 layer_factory.hpp:77] Creating layer loss
I0506 11:25:22.687867 11809 net.cpp:106] Creating Layer loss
I0506 11:25:22.687880 11809 net.cpp:454] loss <- ip3
I0506 11:25:22.687899 11809 net.cpp:454] loss <- label
I0506 11:25:22.687914 11809 net.cpp:411] loss -> loss
I0506 11:25:22.687947 11809 layer_factory.hpp:77] Creating layer loss
I0506 11:25:22.688621 11809 net.cpp:150] Setting up loss
I0506 11:25:22.688643 11809 net.cpp:157] Top shape: (1)
I0506 11:25:22.688665 11809 net.cpp:160]     with loss weight 1
I0506 11:25:22.688716 11809 net.cpp:165] Memory required for data: 157878004
I0506 11:25:22.688738 11809 net.cpp:226] loss needs backward computation.
I0506 11:25:22.688752 11809 net.cpp:226] drop3 needs backward computation.
I0506 11:25:22.688766 11809 net.cpp:226] ip3 needs backward computation.
I0506 11:25:22.688778 11809 net.cpp:226] drop2 needs backward computation.
I0506 11:25:22.688791 11809 net.cpp:226] relu6 needs backward computation.
I0506 11:25:22.688805 11809 net.cpp:226] ip2 needs backward computation.
I0506 11:25:22.688817 11809 net.cpp:226] drop1 needs backward computation.
I0506 11:25:22.688839 11809 net.cpp:226] relu5 needs backward computation.
I0506 11:25:22.688851 11809 net.cpp:226] ip1 needs backward computation.
I0506 11:25:22.688876 11809 net.cpp:226] pool4 needs backward computation.
I0506 11:25:22.688889 11809 net.cpp:226] relu4 needs backward computation.
I0506 11:25:22.688901 11809 net.cpp:226] conv4 needs backward computation.
I0506 11:25:22.688915 11809 net.cpp:226] pool3 needs backward computation.
I0506 11:25:22.688930 11809 net.cpp:226] relu3 needs backward computation.
I0506 11:25:22.688951 11809 net.cpp:226] conv3 needs backward computation.
I0506 11:25:22.688971 11809 net.cpp:226] pool2 needs backward computation.
I0506 11:25:22.688987 11809 net.cpp:226] relu2 needs backward computation.
I0506 11:25:22.688998 11809 net.cpp:226] conv2 needs backward computation.
I0506 11:25:22.689012 11809 net.cpp:226] pool1 needs backward computation.
I0506 11:25:22.689026 11809 net.cpp:226] relu1 needs backward computation.
I0506 11:25:22.689038 11809 net.cpp:226] conv1 needs backward computation.
I0506 11:25:22.689055 11809 net.cpp:228] data_hdf5 does not need backward computation.
I0506 11:25:22.689080 11809 net.cpp:270] This network produces output loss
I0506 11:25:22.689113 11809 net.cpp:283] Network initialization done.
I0506 11:25:22.691018 11809 solver.cpp:181] Creating test net (#0) specified by net file: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.prototxt
I0506 11:25:22.691087 11809 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_hdf5
I0506 11:25:22.691458 11809 net.cpp:49] Initializing net from parameters: 
name: "caffe_test_127x50_x_unshifted"
state {
  phase: TEST
}
layer {
  name: "data_hdf5"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 12
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 8
    kernel_w: 3
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 20
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 7
    kernel_w: 3
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 28
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  convolution_param {
    num_output: 36
    pad: 0
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
    kernel_h: 6
    kernel_w: 3
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_h: 2
    kernel_w: 1
    stride_h: 2
    stride_w: 1
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool4"
  top: "ip1"
  inner_product_param {
    num_output: 196
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 98
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  inner_product_param {
    num_output: 11
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "ip3"
  top: "ip3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0506 11:25:22.691690 11809 layer_factory.hpp:77] Creating layer data_hdf5
I0506 11:25:22.691710 11809 net.cpp:106] Creating Layer data_hdf5
I0506 11:25:22.691726 11809 net.cpp:411] data_hdf5 -> data
I0506 11:25:22.691748 11809 net.cpp:411] data_hdf5 -> label
I0506 11:25:22.691766 11809 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc.testlist
I0506 11:25:22.694321 11809 hdf5_data_layer.cpp:93] Number of HDF5 files: 3
I0506 11:25:29.460294 11809 net.cpp:150] Setting up data_hdf5
I0506 11:25:29.460341 11809 net.cpp:157] Top shape: 100 1 127 50 (635000)
I0506 11:25:29.460361 11809 net.cpp:157] Top shape: 100 (100)
I0506 11:25:29.460373 11809 net.cpp:165] Memory required for data: 2540400
I0506 11:25:29.460388 11809 layer_factory.hpp:77] Creating layer label_data_hdf5_1_split
I0506 11:25:29.460417 11809 net.cpp:106] Creating Layer label_data_hdf5_1_split
I0506 11:25:29.460430 11809 net.cpp:454] label_data_hdf5_1_split <- label
I0506 11:25:29.460453 11809 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_0
I0506 11:25:29.460491 11809 net.cpp:411] label_data_hdf5_1_split -> label_data_hdf5_1_split_1
I0506 11:25:29.460566 11809 net.cpp:150] Setting up label_data_hdf5_1_split
I0506 11:25:29.460582 11809 net.cpp:157] Top shape: 100 (100)
I0506 11:25:29.460597 11809 net.cpp:157] Top shape: 100 (100)
I0506 11:25:29.460611 11809 net.cpp:165] Memory required for data: 2541200
I0506 11:25:29.460631 11809 layer_factory.hpp:77] Creating layer conv1
I0506 11:25:29.460654 11809 net.cpp:106] Creating Layer conv1
I0506 11:25:29.460680 11809 net.cpp:454] conv1 <- data
I0506 11:25:29.460695 11809 net.cpp:411] conv1 -> conv1
I0506 11:25:29.462556 11809 net.cpp:150] Setting up conv1
I0506 11:25:29.462585 11809 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 11:25:29.462599 11809 net.cpp:165] Memory required for data: 30189200
I0506 11:25:29.462621 11809 layer_factory.hpp:77] Creating layer relu1
I0506 11:25:29.462656 11809 net.cpp:106] Creating Layer relu1
I0506 11:25:29.462671 11809 net.cpp:454] relu1 <- conv1
I0506 11:25:29.462687 11809 net.cpp:397] relu1 -> conv1 (in-place)
I0506 11:25:29.463201 11809 net.cpp:150] Setting up relu1
I0506 11:25:29.463223 11809 net.cpp:157] Top shape: 100 12 120 48 (6912000)
I0506 11:25:29.463237 11809 net.cpp:165] Memory required for data: 57837200
I0506 11:25:29.463253 11809 layer_factory.hpp:77] Creating layer pool1
I0506 11:25:29.463281 11809 net.cpp:106] Creating Layer pool1
I0506 11:25:29.463294 11809 net.cpp:454] pool1 <- conv1
I0506 11:25:29.463310 11809 net.cpp:411] pool1 -> pool1
I0506 11:25:29.463399 11809 net.cpp:150] Setting up pool1
I0506 11:25:29.463415 11809 net.cpp:157] Top shape: 100 12 60 48 (3456000)
I0506 11:25:29.463430 11809 net.cpp:165] Memory required for data: 71661200
I0506 11:25:29.463449 11809 layer_factory.hpp:77] Creating layer conv2
I0506 11:25:29.463471 11809 net.cpp:106] Creating Layer conv2
I0506 11:25:29.463493 11809 net.cpp:454] conv2 <- pool1
I0506 11:25:29.463510 11809 net.cpp:411] conv2 -> conv2
I0506 11:25:29.465447 11809 net.cpp:150] Setting up conv2
I0506 11:25:29.465471 11809 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 11:25:29.465497 11809 net.cpp:165] Memory required for data: 91533200
I0506 11:25:29.465529 11809 layer_factory.hpp:77] Creating layer relu2
I0506 11:25:29.465554 11809 net.cpp:106] Creating Layer relu2
I0506 11:25:29.465569 11809 net.cpp:454] relu2 <- conv2
I0506 11:25:29.465590 11809 net.cpp:397] relu2 -> conv2 (in-place)
I0506 11:25:29.465946 11809 net.cpp:150] Setting up relu2
I0506 11:25:29.465970 11809 net.cpp:157] Top shape: 100 20 54 46 (4968000)
I0506 11:25:29.465982 11809 net.cpp:165] Memory required for data: 111405200
I0506 11:25:29.465997 11809 layer_factory.hpp:77] Creating layer pool2
I0506 11:25:29.466019 11809 net.cpp:106] Creating Layer pool2
I0506 11:25:29.466032 11809 net.cpp:454] pool2 <- conv2
I0506 11:25:29.466048 11809 net.cpp:411] pool2 -> pool2
I0506 11:25:29.466140 11809 net.cpp:150] Setting up pool2
I0506 11:25:29.466157 11809 net.cpp:157] Top shape: 100 20 27 46 (2484000)
I0506 11:25:29.466171 11809 net.cpp:165] Memory required for data: 121341200
I0506 11:25:29.466183 11809 layer_factory.hpp:77] Creating layer conv3
I0506 11:25:29.466209 11809 net.cpp:106] Creating Layer conv3
I0506 11:25:29.466233 11809 net.cpp:454] conv3 <- pool2
I0506 11:25:29.466249 11809 net.cpp:411] conv3 -> conv3
I0506 11:25:29.468263 11809 net.cpp:150] Setting up conv3
I0506 11:25:29.468296 11809 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 11:25:29.468308 11809 net.cpp:165] Memory required for data: 132182800
I0506 11:25:29.468334 11809 layer_factory.hpp:77] Creating layer relu3
I0506 11:25:29.468358 11809 net.cpp:106] Creating Layer relu3
I0506 11:25:29.468371 11809 net.cpp:454] relu3 <- conv3
I0506 11:25:29.468387 11809 net.cpp:397] relu3 -> conv3 (in-place)
I0506 11:25:29.468896 11809 net.cpp:150] Setting up relu3
I0506 11:25:29.468919 11809 net.cpp:157] Top shape: 100 28 22 44 (2710400)
I0506 11:25:29.468932 11809 net.cpp:165] Memory required for data: 143024400
I0506 11:25:29.468945 11809 layer_factory.hpp:77] Creating layer pool3
I0506 11:25:29.468976 11809 net.cpp:106] Creating Layer pool3
I0506 11:25:29.468989 11809 net.cpp:454] pool3 <- conv3
I0506 11:25:29.469004 11809 net.cpp:411] pool3 -> pool3
I0506 11:25:29.469092 11809 net.cpp:150] Setting up pool3
I0506 11:25:29.469110 11809 net.cpp:157] Top shape: 100 28 11 44 (1355200)
I0506 11:25:29.469120 11809 net.cpp:165] Memory required for data: 148445200
I0506 11:25:29.469135 11809 layer_factory.hpp:77] Creating layer conv4
I0506 11:25:29.469164 11809 net.cpp:106] Creating Layer conv4
I0506 11:25:29.469178 11809 net.cpp:454] conv4 <- pool3
I0506 11:25:29.469202 11809 net.cpp:411] conv4 -> conv4
I0506 11:25:29.471372 11809 net.cpp:150] Setting up conv4
I0506 11:25:29.471400 11809 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 11:25:29.471415 11809 net.cpp:165] Memory required for data: 152074000
I0506 11:25:29.471433 11809 layer_factory.hpp:77] Creating layer relu4
I0506 11:25:29.471456 11809 net.cpp:106] Creating Layer relu4
I0506 11:25:29.471478 11809 net.cpp:454] relu4 <- conv4
I0506 11:25:29.471493 11809 net.cpp:397] relu4 -> conv4 (in-place)
I0506 11:25:29.471997 11809 net.cpp:150] Setting up relu4
I0506 11:25:29.472020 11809 net.cpp:157] Top shape: 100 36 6 42 (907200)
I0506 11:25:29.472033 11809 net.cpp:165] Memory required for data: 155702800
I0506 11:25:29.472049 11809 layer_factory.hpp:77] Creating layer pool4
I0506 11:25:29.472076 11809 net.cpp:106] Creating Layer pool4
I0506 11:25:29.472090 11809 net.cpp:454] pool4 <- conv4
I0506 11:25:29.472105 11809 net.cpp:411] pool4 -> pool4
I0506 11:25:29.472194 11809 net.cpp:150] Setting up pool4
I0506 11:25:29.472211 11809 net.cpp:157] Top shape: 100 36 3 42 (453600)
I0506 11:25:29.472226 11809 net.cpp:165] Memory required for data: 157517200
I0506 11:25:29.472239 11809 layer_factory.hpp:77] Creating layer ip1
I0506 11:25:29.472255 11809 net.cpp:106] Creating Layer ip1
I0506 11:25:29.472275 11809 net.cpp:454] ip1 <- pool4
I0506 11:25:29.472300 11809 net.cpp:411] ip1 -> ip1
I0506 11:25:29.488200 11809 net.cpp:150] Setting up ip1
I0506 11:25:29.488231 11809 net.cpp:157] Top shape: 100 196 (19600)
I0506 11:25:29.488255 11809 net.cpp:165] Memory required for data: 157595600
I0506 11:25:29.488289 11809 layer_factory.hpp:77] Creating layer relu5
I0506 11:25:29.488318 11809 net.cpp:106] Creating Layer relu5
I0506 11:25:29.488332 11809 net.cpp:454] relu5 <- ip1
I0506 11:25:29.488348 11809 net.cpp:397] relu5 -> ip1 (in-place)
I0506 11:25:29.488726 11809 net.cpp:150] Setting up relu5
I0506 11:25:29.488745 11809 net.cpp:157] Top shape: 100 196 (19600)
I0506 11:25:29.488759 11809 net.cpp:165] Memory required for data: 157674000
I0506 11:25:29.488775 11809 layer_factory.hpp:77] Creating layer drop1
I0506 11:25:29.488804 11809 net.cpp:106] Creating Layer drop1
I0506 11:25:29.488818 11809 net.cpp:454] drop1 <- ip1
I0506 11:25:29.488837 11809 net.cpp:397] drop1 -> ip1 (in-place)
I0506 11:25:29.488899 11809 net.cpp:150] Setting up drop1
I0506 11:25:29.488919 11809 net.cpp:157] Top shape: 100 196 (19600)
I0506 11:25:29.488934 11809 net.cpp:165] Memory required for data: 157752400
I0506 11:25:29.488945 11809 layer_factory.hpp:77] Creating layer ip2
I0506 11:25:29.488965 11809 net.cpp:106] Creating Layer ip2
I0506 11:25:29.488976 11809 net.cpp:454] ip2 <- ip1
I0506 11:25:29.489001 11809 net.cpp:411] ip2 -> ip2
I0506 11:25:29.489516 11809 net.cpp:150] Setting up ip2
I0506 11:25:29.489536 11809 net.cpp:157] Top shape: 100 98 (9800)
I0506 11:25:29.489547 11809 net.cpp:165] Memory required for data: 157791600
I0506 11:25:29.489568 11809 layer_factory.hpp:77] Creating layer relu6
I0506 11:25:29.489593 11809 net.cpp:106] Creating Layer relu6
I0506 11:25:29.489606 11809 net.cpp:454] relu6 <- ip2
I0506 11:25:29.489621 11809 net.cpp:397] relu6 -> ip2 (in-place)
I0506 11:25:29.490185 11809 net.cpp:150] Setting up relu6
I0506 11:25:29.490211 11809 net.cpp:157] Top shape: 100 98 (9800)
I0506 11:25:29.490224 11809 net.cpp:165] Memory required for data: 157830800
I0506 11:25:29.490242 11809 layer_factory.hpp:77] Creating layer drop2
I0506 11:25:29.490265 11809 net.cpp:106] Creating Layer drop2
I0506 11:25:29.490278 11809 net.cpp:454] drop2 <- ip2
I0506 11:25:29.490293 11809 net.cpp:397] drop2 -> ip2 (in-place)
I0506 11:25:29.490351 11809 net.cpp:150] Setting up drop2
I0506 11:25:29.490367 11809 net.cpp:157] Top shape: 100 98 (9800)
I0506 11:25:29.490380 11809 net.cpp:165] Memory required for data: 157870000
I0506 11:25:29.490392 11809 layer_factory.hpp:77] Creating layer ip3
I0506 11:25:29.490407 11809 net.cpp:106] Creating Layer ip3
I0506 11:25:29.490422 11809 net.cpp:454] ip3 <- ip2
I0506 11:25:29.490447 11809 net.cpp:411] ip3 -> ip3
I0506 11:25:29.490686 11809 net.cpp:150] Setting up ip3
I0506 11:25:29.490705 11809 net.cpp:157] Top shape: 100 11 (1100)
I0506 11:25:29.490718 11809 net.cpp:165] Memory required for data: 157874400
I0506 11:25:29.490738 11809 layer_factory.hpp:77] Creating layer drop3
I0506 11:25:29.490761 11809 net.cpp:106] Creating Layer drop3
I0506 11:25:29.490773 11809 net.cpp:454] drop3 <- ip3
I0506 11:25:29.490792 11809 net.cpp:397] drop3 -> ip3 (in-place)
I0506 11:25:29.490839 11809 net.cpp:150] Setting up drop3
I0506 11:25:29.490864 11809 net.cpp:157] Top shape: 100 11 (1100)
I0506 11:25:29.490876 11809 net.cpp:165] Memory required for data: 157878800
I0506 11:25:29.490890 11809 layer_factory.hpp:77] Creating layer ip3_drop3_0_split
I0506 11:25:29.490902 11809 net.cpp:106] Creating Layer ip3_drop3_0_split
I0506 11:25:29.490917 11809 net.cpp:454] ip3_drop3_0_split <- ip3
I0506 11:25:29.490932 11809 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_0
I0506 11:25:29.490955 11809 net.cpp:411] ip3_drop3_0_split -> ip3_drop3_0_split_1
I0506 11:25:29.491042 11809 net.cpp:150] Setting up ip3_drop3_0_split
I0506 11:25:29.491060 11809 net.cpp:157] Top shape: 100 11 (1100)
I0506 11:25:29.491075 11809 net.cpp:157] Top shape: 100 11 (1100)
I0506 11:25:29.491089 11809 net.cpp:165] Memory required for data: 157887600
I0506 11:25:29.491101 11809 layer_factory.hpp:77] Creating layer accuracy
I0506 11:25:29.491128 11809 net.cpp:106] Creating Layer accuracy
I0506 11:25:29.491142 11809 net.cpp:454] accuracy <- ip3_drop3_0_split_0
I0506 11:25:29.491163 11809 net.cpp:454] accuracy <- label_data_hdf5_1_split_0
I0506 11:25:29.491181 11809 net.cpp:411] accuracy -> accuracy
I0506 11:25:29.491211 11809 net.cpp:150] Setting up accuracy
I0506 11:25:29.491227 11809 net.cpp:157] Top shape: (1)
I0506 11:25:29.491245 11809 net.cpp:165] Memory required for data: 157887604
I0506 11:25:29.491258 11809 layer_factory.hpp:77] Creating layer loss
I0506 11:25:29.491281 11809 net.cpp:106] Creating Layer loss
I0506 11:25:29.491293 11809 net.cpp:454] loss <- ip3_drop3_0_split_1
I0506 11:25:29.491307 11809 net.cpp:454] loss <- label_data_hdf5_1_split_1
I0506 11:25:29.491329 11809 net.cpp:411] loss -> loss
I0506 11:25:29.491354 11809 layer_factory.hpp:77] Creating layer loss
I0506 11:25:29.491859 11809 net.cpp:150] Setting up loss
I0506 11:25:29.491884 11809 net.cpp:157] Top shape: (1)
I0506 11:25:29.491897 11809 net.cpp:160]     with loss weight 1
I0506 11:25:29.491924 11809 net.cpp:165] Memory required for data: 157887608
I0506 11:25:29.491945 11809 net.cpp:226] loss needs backward computation.
I0506 11:25:29.491958 11809 net.cpp:228] accuracy does not need backward computation.
I0506 11:25:29.491976 11809 net.cpp:226] ip3_drop3_0_split needs backward computation.
I0506 11:25:29.491988 11809 net.cpp:226] drop3 needs backward computation.
I0506 11:25:29.492000 11809 net.cpp:226] ip3 needs backward computation.
I0506 11:25:29.492015 11809 net.cpp:226] drop2 needs backward computation.
I0506 11:25:29.492034 11809 net.cpp:226] relu6 needs backward computation.
I0506 11:25:29.492046 11809 net.cpp:226] ip2 needs backward computation.
I0506 11:25:29.492059 11809 net.cpp:226] drop1 needs backward computation.
I0506 11:25:29.492074 11809 net.cpp:226] relu5 needs backward computation.
I0506 11:25:29.492086 11809 net.cpp:226] ip1 needs backward computation.
I0506 11:25:29.492099 11809 net.cpp:226] pool4 needs backward computation.
I0506 11:25:29.492110 11809 net.cpp:226] relu4 needs backward computation.
I0506 11:25:29.492125 11809 net.cpp:226] conv4 needs backward computation.
I0506 11:25:29.492138 11809 net.cpp:226] pool3 needs backward computation.
I0506 11:25:29.492158 11809 net.cpp:226] relu3 needs backward computation.
I0506 11:25:29.492172 11809 net.cpp:226] conv3 needs backward computation.
I0506 11:25:29.492187 11809 net.cpp:226] pool2 needs backward computation.
I0506 11:25:29.492198 11809 net.cpp:226] relu2 needs backward computation.
I0506 11:25:29.492210 11809 net.cpp:226] conv2 needs backward computation.
I0506 11:25:29.492225 11809 net.cpp:226] pool1 needs backward computation.
I0506 11:25:29.492238 11809 net.cpp:226] relu1 needs backward computation.
I0506 11:25:29.492259 11809 net.cpp:226] conv1 needs backward computation.
I0506 11:25:29.492272 11809 net.cpp:228] label_data_hdf5_1_split does not need backward computation.
I0506 11:25:29.492288 11809 net.cpp:228] data_hdf5 does not need backward computation.
I0506 11:25:29.492300 11809 net.cpp:270] This network produces output accuracy
I0506 11:25:29.492312 11809 net.cpp:270] This network produces output loss
I0506 11:25:29.492349 11809 net.cpp:283] Network initialization done.
I0506 11:25:29.492471 11809 solver.cpp:60] Solver scaffolding done.
I0506 11:25:29.493582 11809 caffe.cpp:212] Starting Optimization
I0506 11:25:29.493602 11809 solver.cpp:288] Solving caffe_test_127x50_x_unshifted
I0506 11:25:29.493618 11809 solver.cpp:289] Learning Rate Policy: fixed
I0506 11:25:29.494693 11809 solver.cpp:341] Iteration 0, Testing net (#0)
I0506 11:26:02.530189 11809 solver.cpp:409]     Test net output #0: accuracy = 0.0857666
I0506 11:26:02.530380 11809 solver.cpp:409]     Test net output #1: loss = 2.39841 (* 1 = 2.39841 loss)
I0506 11:26:02.563180 11809 solver.cpp:237] Iteration 0, loss = 2.40005
I0506 11:26:02.563220 11809 solver.cpp:253]     Train net output #0: loss = 2.40005 (* 1 = 2.40005 loss)
I0506 11:26:02.563241 11809 sgd_solver.cpp:106] Iteration 0, lr = 0.0025
I0506 11:26:08.376453 11809 solver.cpp:237] Iteration 100, loss = 2.34871
I0506 11:26:08.376502 11809 solver.cpp:253]     Train net output #0: loss = 2.34871 (* 1 = 2.34871 loss)
I0506 11:26:08.376528 11809 sgd_solver.cpp:106] Iteration 100, lr = 0.0025
I0506 11:26:14.189929 11809 solver.cpp:237] Iteration 200, loss = 2.36512
I0506 11:26:14.189963 11809 solver.cpp:253]     Train net output #0: loss = 2.36512 (* 1 = 2.36512 loss)
I0506 11:26:14.189981 11809 sgd_solver.cpp:106] Iteration 200, lr = 0.0025
I0506 11:26:20.003716 11809 solver.cpp:237] Iteration 300, loss = 2.22748
I0506 11:26:20.003749 11809 solver.cpp:253]     Train net output #0: loss = 2.22748 (* 1 = 2.22748 loss)
I0506 11:26:20.003768 11809 sgd_solver.cpp:106] Iteration 300, lr = 0.0025
I0506 11:26:25.817600 11809 solver.cpp:237] Iteration 400, loss = 2.13147
I0506 11:26:25.817636 11809 solver.cpp:253]     Train net output #0: loss = 2.13147 (* 1 = 2.13147 loss)
I0506 11:26:25.817654 11809 sgd_solver.cpp:106] Iteration 400, lr = 0.0025
I0506 11:26:31.627946 11809 solver.cpp:237] Iteration 500, loss = 2.06045
I0506 11:26:31.627981 11809 solver.cpp:253]     Train net output #0: loss = 2.06045 (* 1 = 2.06045 loss)
I0506 11:26:31.627998 11809 sgd_solver.cpp:106] Iteration 500, lr = 0.0025
I0506 11:26:37.443656 11809 solver.cpp:237] Iteration 600, loss = 1.99105
I0506 11:26:37.443812 11809 solver.cpp:253]     Train net output #0: loss = 1.99105 (* 1 = 1.99105 loss)
I0506 11:26:37.443830 11809 sgd_solver.cpp:106] Iteration 600, lr = 0.0025
I0506 11:26:43.260129 11809 solver.cpp:237] Iteration 700, loss = 2.08699
I0506 11:26:43.260167 11809 solver.cpp:253]     Train net output #0: loss = 2.08699 (* 1 = 2.08699 loss)
I0506 11:26:43.260184 11809 sgd_solver.cpp:106] Iteration 700, lr = 0.0025
I0506 11:26:49.075522 11809 solver.cpp:237] Iteration 800, loss = 1.88686
I0506 11:26:49.075563 11809 solver.cpp:253]     Train net output #0: loss = 1.88686 (* 1 = 1.88686 loss)
I0506 11:26:49.075580 11809 sgd_solver.cpp:106] Iteration 800, lr = 0.0025
I0506 11:26:54.888962 11809 solver.cpp:237] Iteration 900, loss = 1.84507
I0506 11:26:54.888998 11809 solver.cpp:253]     Train net output #0: loss = 1.84507 (* 1 = 1.84507 loss)
I0506 11:26:54.889014 11809 sgd_solver.cpp:106] Iteration 900, lr = 0.0025
I0506 11:27:00.645148 11809 solver.cpp:341] Iteration 1000, Testing net (#0)
I0506 11:27:30.263068 11809 solver.cpp:409]     Test net output #0: accuracy = 0.60464
I0506 11:27:30.263239 11809 solver.cpp:409]     Test net output #1: loss = 1.4553 (* 1 = 1.4553 loss)
I0506 11:27:38.689954 11809 solver.cpp:237] Iteration 1000, loss = 2.00141
I0506 11:27:38.690019 11809 solver.cpp:253]     Train net output #0: loss = 2.00141 (* 1 = 2.00141 loss)
I0506 11:27:38.690037 11809 sgd_solver.cpp:106] Iteration 1000, lr = 0.0025
I0506 11:27:44.511850 11809 solver.cpp:237] Iteration 1100, loss = 1.89357
I0506 11:27:44.511884 11809 solver.cpp:253]     Train net output #0: loss = 1.89357 (* 1 = 1.89357 loss)
I0506 11:27:44.511904 11809 sgd_solver.cpp:106] Iteration 1100, lr = 0.0025
I0506 11:27:50.339083 11809 solver.cpp:237] Iteration 1200, loss = 1.82352
I0506 11:27:50.339133 11809 solver.cpp:253]     Train net output #0: loss = 1.82352 (* 1 = 1.82352 loss)
I0506 11:27:50.339160 11809 sgd_solver.cpp:106] Iteration 1200, lr = 0.0025
I0506 11:27:56.162941 11809 solver.cpp:237] Iteration 1300, loss = 1.78598
I0506 11:27:56.162977 11809 solver.cpp:253]     Train net output #0: loss = 1.78598 (* 1 = 1.78598 loss)
I0506 11:27:56.162994 11809 sgd_solver.cpp:106] Iteration 1300, lr = 0.0025
I0506 11:28:01.985592 11809 solver.cpp:237] Iteration 1400, loss = 1.86382
I0506 11:28:01.985736 11809 solver.cpp:253]     Train net output #0: loss = 1.86382 (* 1 = 1.86382 loss)
I0506 11:28:01.985754 11809 sgd_solver.cpp:106] Iteration 1400, lr = 0.0025
I0506 11:28:07.810681 11809 solver.cpp:237] Iteration 1500, loss = 1.89481
I0506 11:28:07.810714 11809 solver.cpp:253]     Train net output #0: loss = 1.89481 (* 1 = 1.89481 loss)
I0506 11:28:07.810734 11809 sgd_solver.cpp:106] Iteration 1500, lr = 0.0025
I0506 11:28:13.633710 11809 solver.cpp:237] Iteration 1600, loss = 1.8899
I0506 11:28:13.633766 11809 solver.cpp:253]     Train net output #0: loss = 1.8899 (* 1 = 1.8899 loss)
I0506 11:28:13.633791 11809 sgd_solver.cpp:106] Iteration 1600, lr = 0.0025
I0506 11:28:19.454138 11809 solver.cpp:237] Iteration 1700, loss = 1.56274
I0506 11:28:19.454174 11809 solver.cpp:253]     Train net output #0: loss = 1.56274 (* 1 = 1.56274 loss)
I0506 11:28:19.454196 11809 sgd_solver.cpp:106] Iteration 1700, lr = 0.0025
I0506 11:28:25.274323 11809 solver.cpp:237] Iteration 1800, loss = 1.73264
I0506 11:28:25.274358 11809 solver.cpp:253]     Train net output #0: loss = 1.73264 (* 1 = 1.73264 loss)
I0506 11:28:25.274376 11809 sgd_solver.cpp:106] Iteration 1800, lr = 0.0025
I0506 11:28:31.098489 11809 solver.cpp:237] Iteration 1900, loss = 1.7289
I0506 11:28:31.098526 11809 solver.cpp:253]     Train net output #0: loss = 1.7289 (* 1 = 1.7289 loss)
I0506 11:28:31.098543 11809 sgd_solver.cpp:106] Iteration 1900, lr = 0.0025
I0506 11:28:36.859290 11809 solver.cpp:341] Iteration 2000, Testing net (#0)
I0506 11:29:10.192394 11809 solver.cpp:409]     Test net output #0: accuracy = 0.643793
I0506 11:29:10.192564 11809 solver.cpp:409]     Test net output #1: loss = 1.21262 (* 1 = 1.21262 loss)
I0506 11:29:18.857194 11809 solver.cpp:237] Iteration 2000, loss = 1.72685
I0506 11:29:18.857264 11809 solver.cpp:253]     Train net output #0: loss = 1.72685 (* 1 = 1.72685 loss)
I0506 11:29:18.857282 11809 sgd_solver.cpp:106] Iteration 2000, lr = 0.0025
I0506 11:29:24.680411 11809 solver.cpp:237] Iteration 2100, loss = 1.5267
I0506 11:29:24.680447 11809 solver.cpp:253]     Train net output #0: loss = 1.5267 (* 1 = 1.5267 loss)
I0506 11:29:24.680465 11809 sgd_solver.cpp:106] Iteration 2100, lr = 0.0025
I0506 11:29:30.507652 11809 solver.cpp:237] Iteration 2200, loss = 1.69194
I0506 11:29:30.507701 11809 solver.cpp:253]     Train net output #0: loss = 1.69194 (* 1 = 1.69194 loss)
I0506 11:29:30.507730 11809 sgd_solver.cpp:106] Iteration 2200, lr = 0.0025
I0506 11:29:36.339771 11809 solver.cpp:237] Iteration 2300, loss = 1.62951
I0506 11:29:36.339805 11809 solver.cpp:253]     Train net output #0: loss = 1.62951 (* 1 = 1.62951 loss)
I0506 11:29:36.339824 11809 sgd_solver.cpp:106] Iteration 2300, lr = 0.0025
I0506 11:29:42.173912 11809 solver.cpp:237] Iteration 2400, loss = 1.67597
I0506 11:29:42.174055 11809 solver.cpp:253]     Train net output #0: loss = 1.67597 (* 1 = 1.67597 loss)
I0506 11:29:42.174072 11809 sgd_solver.cpp:106] Iteration 2400, lr = 0.0025
I0506 11:29:48.001996 11809 solver.cpp:237] Iteration 2500, loss = 1.61486
I0506 11:29:48.002032 11809 solver.cpp:253]     Train net output #0: loss = 1.61486 (* 1 = 1.61486 loss)
I0506 11:29:48.002048 11809 sgd_solver.cpp:106] Iteration 2500, lr = 0.0025
I0506 11:29:53.830312 11809 solver.cpp:237] Iteration 2600, loss = 1.37881
I0506 11:29:53.830363 11809 solver.cpp:253]     Train net output #0: loss = 1.37881 (* 1 = 1.37881 loss)
I0506 11:29:53.830379 11809 sgd_solver.cpp:106] Iteration 2600, lr = 0.0025
I0506 11:29:59.659567 11809 solver.cpp:237] Iteration 2700, loss = 1.75373
I0506 11:29:59.659602 11809 solver.cpp:253]     Train net output #0: loss = 1.75373 (* 1 = 1.75373 loss)
I0506 11:29:59.659626 11809 sgd_solver.cpp:106] Iteration 2700, lr = 0.0025
I0506 11:30:05.486407 11809 solver.cpp:237] Iteration 2800, loss = 1.81395
I0506 11:30:05.486440 11809 solver.cpp:253]     Train net output #0: loss = 1.81395 (* 1 = 1.81395 loss)
I0506 11:30:05.486464 11809 sgd_solver.cpp:106] Iteration 2800, lr = 0.0025
I0506 11:30:11.315605 11809 solver.cpp:237] Iteration 2900, loss = 1.47014
I0506 11:30:11.315639 11809 solver.cpp:253]     Train net output #0: loss = 1.47014 (* 1 = 1.47014 loss)
I0506 11:30:11.315657 11809 sgd_solver.cpp:106] Iteration 2900, lr = 0.0025
I0506 11:30:17.088767 11809 solver.cpp:341] Iteration 3000, Testing net (#0)
I0506 11:30:46.430704 11809 solver.cpp:409]     Test net output #0: accuracy = 0.704239
I0506 11:30:46.430764 11809 solver.cpp:409]     Test net output #1: loss = 1.01899 (* 1 = 1.01899 loss)
I0506 11:30:55.075049 11809 solver.cpp:237] Iteration 3000, loss = 1.66018
I0506 11:30:55.075242 11809 solver.cpp:253]     Train net output #0: loss = 1.66018 (* 1 = 1.66018 loss)
I0506 11:30:55.075260 11809 sgd_solver.cpp:106] Iteration 3000, lr = 0.0025
I0506 11:31:00.892891 11809 solver.cpp:237] Iteration 3100, loss = 1.59865
I0506 11:31:00.892926 11809 solver.cpp:253]     Train net output #0: loss = 1.59865 (* 1 = 1.59865 loss)
I0506 11:31:00.892949 11809 sgd_solver.cpp:106] Iteration 3100, lr = 0.0025
I0506 11:31:06.709813 11809 solver.cpp:237] Iteration 3200, loss = 1.60969
I0506 11:31:06.709848 11809 solver.cpp:253]     Train net output #0: loss = 1.60969 (* 1 = 1.60969 loss)
I0506 11:31:06.709866 11809 sgd_solver.cpp:106] Iteration 3200, lr = 0.0025
I0506 11:31:12.528731 11809 solver.cpp:237] Iteration 3300, loss = 1.70337
I0506 11:31:12.528781 11809 solver.cpp:253]     Train net output #0: loss = 1.70337 (* 1 = 1.70337 loss)
I0506 11:31:12.528810 11809 sgd_solver.cpp:106] Iteration 3300, lr = 0.0025
I0506 11:31:18.345168 11809 solver.cpp:237] Iteration 3400, loss = 1.5846
I0506 11:31:18.345202 11809 solver.cpp:253]     Train net output #0: loss = 1.5846 (* 1 = 1.5846 loss)
I0506 11:31:18.345221 11809 sgd_solver.cpp:106] Iteration 3400, lr = 0.0025
I0506 11:31:24.164504 11809 solver.cpp:237] Iteration 3500, loss = 1.60196
I0506 11:31:24.164536 11809 solver.cpp:253]     Train net output #0: loss = 1.60196 (* 1 = 1.60196 loss)
I0506 11:31:24.164556 11809 sgd_solver.cpp:106] Iteration 3500, lr = 0.0025
I0506 11:31:29.982583 11809 solver.cpp:237] Iteration 3600, loss = 1.5572
I0506 11:31:29.982743 11809 solver.cpp:253]     Train net output #0: loss = 1.5572 (* 1 = 1.5572 loss)
I0506 11:31:29.982759 11809 sgd_solver.cpp:106] Iteration 3600, lr = 0.0025
I0506 11:31:35.805168 11809 solver.cpp:237] Iteration 3700, loss = 1.5265
I0506 11:31:35.805219 11809 solver.cpp:253]     Train net output #0: loss = 1.5265 (* 1 = 1.5265 loss)
I0506 11:31:35.805236 11809 sgd_solver.cpp:106] Iteration 3700, lr = 0.0025
I0506 11:31:41.625799 11809 solver.cpp:237] Iteration 3800, loss = 1.4549
I0506 11:31:41.625833 11809 solver.cpp:253]     Train net output #0: loss = 1.4549 (* 1 = 1.4549 loss)
I0506 11:31:41.625852 11809 sgd_solver.cpp:106] Iteration 3800, lr = 0.0025
I0506 11:31:47.442363 11809 solver.cpp:237] Iteration 3900, loss = 1.39894
I0506 11:31:47.442395 11809 solver.cpp:253]     Train net output #0: loss = 1.39894 (* 1 = 1.39894 loss)
I0506 11:31:47.442414 11809 sgd_solver.cpp:106] Iteration 3900, lr = 0.0025
I0506 11:31:53.202018 11809 solver.cpp:341] Iteration 4000, Testing net (#0)
I0506 11:32:26.360172 11809 solver.cpp:409]     Test net output #0: accuracy = 0.756314
I0506 11:32:26.360337 11809 solver.cpp:409]     Test net output #1: loss = 0.888019 (* 1 = 0.888019 loss)
I0506 13:03:58.133934 11809 solver.cpp:237] Iteration 4000, loss = 1.61281
I0506 13:03:58.134119 11809 solver.cpp:253]     Train net output #0: loss = 1.61281 (* 1 = 1.61281 loss)
I0506 13:03:58.134137 11809 sgd_solver.cpp:106] Iteration 4000, lr = 0.0025
I0506 13:04:03.953874 11809 solver.cpp:237] Iteration 4100, loss = 1.5932
I0506 13:04:03.953928 11809 solver.cpp:253]     Train net output #0: loss = 1.5932 (* 1 = 1.5932 loss)
I0506 13:04:03.953958 11809 sgd_solver.cpp:106] Iteration 4100, lr = 0.0025
I0506 13:04:09.776253 11809 solver.cpp:237] Iteration 4200, loss = 1.26871
I0506 13:04:09.776288 11809 solver.cpp:253]     Train net output #0: loss = 1.26871 (* 1 = 1.26871 loss)
I0506 13:04:09.776310 11809 sgd_solver.cpp:106] Iteration 4200, lr = 0.0025
I0506 13:04:15.598675 11809 solver.cpp:237] Iteration 4300, loss = 1.53052
I0506 13:04:15.598709 11809 solver.cpp:253]     Train net output #0: loss = 1.53052 (* 1 = 1.53052 loss)
I0506 13:04:15.598726 11809 sgd_solver.cpp:106] Iteration 4300, lr = 0.0025
I0506 13:04:21.417487 11809 solver.cpp:237] Iteration 4400, loss = 1.50612
I0506 13:04:21.417520 11809 solver.cpp:253]     Train net output #0: loss = 1.50612 (* 1 = 1.50612 loss)
I0506 13:04:21.417539 11809 sgd_solver.cpp:106] Iteration 4400, lr = 0.0025
I0506 13:04:27.239382 11809 solver.cpp:237] Iteration 4500, loss = 1.48316
I0506 13:04:27.239436 11809 solver.cpp:253]     Train net output #0: loss = 1.48316 (* 1 = 1.48316 loss)
I0506 13:04:27.239454 11809 sgd_solver.cpp:106] Iteration 4500, lr = 0.0025
I0506 13:04:33.053485 11809 solver.cpp:237] Iteration 4600, loss = 1.62423
I0506 13:04:33.053642 11809 solver.cpp:253]     Train net output #0: loss = 1.62423 (* 1 = 1.62423 loss)
I0506 13:04:33.053659 11809 sgd_solver.cpp:106] Iteration 4600, lr = 0.0025
I0506 13:04:38.871580 11809 solver.cpp:237] Iteration 4700, loss = 1.4121
I0506 13:04:38.871616 11809 solver.cpp:253]     Train net output #0: loss = 1.4121 (* 1 = 1.4121 loss)
I0506 13:04:38.871634 11809 sgd_solver.cpp:106] Iteration 4700, lr = 0.0025
I0506 13:04:44.693220 11809 solver.cpp:237] Iteration 4800, loss = 1.50337
I0506 13:04:44.693254 11809 solver.cpp:253]     Train net output #0: loss = 1.50337 (* 1 = 1.50337 loss)
I0506 13:04:44.693272 11809 sgd_solver.cpp:106] Iteration 4800, lr = 0.0025
I0506 13:04:50.515590 11809 solver.cpp:237] Iteration 4900, loss = 1.42879
I0506 13:04:50.515626 11809 solver.cpp:253]     Train net output #0: loss = 1.42879 (* 1 = 1.42879 loss)
I0506 13:04:50.515643 11809 sgd_solver.cpp:106] Iteration 4900, lr = 0.0025
I0506 13:04:56.280421 11809 solver.cpp:459] Snapshotting to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_5000.caffemodel
I0506 13:04:56.400096 11809 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /lustre/atlas/proj-shared/hep105/caffe_titan/minosmatch_nukecczdefs_127x50_x_unshifted_me1Bmc_model_iter_5000.solverstate
I0506 13:04:56.446744 11809 solver.cpp:341] Iteration 5000, Testing net (#0)
I0506 13:05:25.768329 11809 solver.cpp:409]     Test net output #0: accuracy = 0.787074
I0506 13:05:25.768502 11809 solver.cpp:409]     Test net output #1: loss = 0.799962 (* 1 = 0.799962 loss)
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
=>> PBS: job killed: walltime 7205 exceeded limit 7200
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
aprun: Apid 11146333: Caught signal Terminated, sending to application
